---
generated_at: 2026-02-09T21:27:48.074Z
content_hash: f3124a988e86b4fd955b0436cc2b3fc12e5bc5433442c4f37fcbc1d813dea324
purpose: src/discovery/filters/index.ts orchestrates filter chain execution for file discovery, applying multiple FileFilter p...
---
**src/discovery/filters/index.ts orchestrates filter chain execution for file discovery, applying multiple FileFilter predicates sequentially with bounded concurrency and tracking exclusion reasons per filter.**

## Exported Functions

**applyFilters(files: string[], filters: FileFilter[], options?: { tracer?: ITraceWriter; debug?: boolean }): Promise<FilterResult>** processes files through filter chain with short-circuit evaluation. Returns FilterResult containing `included: string[]` and `excluded: ExcludedFile[]` arrays. Each file runs through filters sequentially until one returns true from `shouldExclude()`, at which point remaining filters are skipped. Uses bounded concurrency of 30 workers sharing single iterator to prevent file descriptor exhaustion during I/O-heavy binary content detection.

## Re-Exported Filter Creators

Aggregates filter factory functions from sibling modules:
- **createGitignoreFilter** from `./gitignore.js`
- **createVendorFilter** and **DEFAULT_VENDOR_DIRS** from `./vendor.js`
- **createBinaryFilter**, **BINARY_EXTENSIONS**, **BinaryFilterOptions** from `./binary.js`
- **createCustomFilter** from `./custom.js`

## Concurrency Strategy

Uses iterator-based worker pool pattern with `CONCURRENCY = 30` to process files in parallel without exhausting file handles. Workers share single `files.entries()` iterator, each consuming items until exhausted. BinaryFilter calls `isBinaryFile()` which performs synchronous file reads, making concurrency bounds critical to prevent system resource limits (ulimit -n).

## Filter Execution Algorithm

1. Initialize `filterStats` map tracking `matched` and `rejected` counts per filter name
2. Spawn 30 workers (or `files.length` if fewer) each iterating over shared file entries
3. For each file, iterate through `filters[]` array sequentially
4. On first `shouldExclude()` returning true, push `ExcludedFile` with `{ path, reason, filter: filter.name }` and break inner loop (short-circuit)
5. If no filter excludes file, push path to `included[]` and increment `matched` count for all filters
6. Flatten worker results, sort by original index to preserve order
7. Emit `filter:applied` trace events with `filesMatched` and `filesRejected` counts per filter

## Trace Events

Emits `filter:applied` events via `options?.tracer?.emit()` containing:
- `type: 'filter:applied'`
- `filterName: string` — filter.name from FileFilter
- `filesMatched: number` — count of files passing through this filter
- `filesRejected: number` — count of files excluded by this filter

## Debug Output

When `options?.debug` is true and `stats.rejected > 0`, writes `pc.dim()` colored message to stderr: `[debug] Filter [${filter.name}]: ${stats.rejected} files rejected`.

## Return Type

FilterResult contains:
- `included: string[]` — absolute paths passing all filters
- `excluded: ExcludedFile[]` — objects with `{ path: string, reason: string, filter: string }` describing which filter excluded each file