<!-- Generated by agents-reverse-engineer -->

# ai/providers

SubprocessProvider implements AIProvider by spawning CLI child processes for AI backends (claude, gemini, opencode), enforcing timeouts with SIGTERM/SIGKILL escalation, detecting rate-limit patterns in stderr, parsing responses via AIBackend.parseResponse(), and optionally writing subprocess output logs with serialized promise-chain writes to prevent concurrent mkdir races.

## Contents

**[subprocess.ts](./subprocess.ts)** — Exports SubprocessProvider class wrapping AIBackend.buildArgs() + runSubprocess() execution, emitting subprocess:spawn/subprocess:exit trace events, throwing AIServiceError with codes TIMEOUT/RATE_LIMIT/SUBPROCESS_ERROR/PARSE_ERROR, maintaining activeCount for debug logging, queueing subprocess logs via enqueueSubprocessLog() promise chain.

## Behavioral Contracts

### Rate-Limit Detection
```typescript
const RATE_LIMIT_PATTERNS = ['rate limit', '429', 'too many requests', 'overloaded'];
isRateLimitStderr(stderr: string) = stderr.toLowerCase() contains any pattern
```
On match, call() throws AIServiceError('RATE_LIMIT', ...).

### Timeout Logic
```typescript
timeoutMs = options.timeoutMs ?? this.timeoutMs
if (result.timedOut === true) throw AIServiceError('TIMEOUT', 'Subprocess timed out')
```
SIGTERM sent at timeoutMs, SIGKILL at timeoutMs + 5000ms (implemented in runSubprocess from ../subprocess.js).

### Exit Code Handling
```typescript
if (exitCode !== 0) {
  if (isRateLimitStderr(stderr)) throw AIServiceError('RATE_LIMIT', ...)
  else throw AIServiceError('SUBPROCESS_ERROR', ..., truncatedStderr)
}
```
Stderr truncated to 500 chars max in error messages.

### Subprocess Log Filename
```typescript
sanitized = taskLabel.replace(/\//g, '--').replace(/[^a-zA-Z0-9._-]/g, '_')
filename = `${sanitized}_pid${childPid}.log`
```
Written to `path.join(subprocessLogDir, filename)` via serialized logWriteQueue.

### Debug Logging Format
```typescript
`[SubprocessProvider] About to spawn: activeCount=${activeCount} heapUsed=${formatBytes(heapUsed)} rss=${formatBytes(rss)} timeoutMs=${timeoutMs}`
`[SubprocessProvider] Done: pid=${childPid} exitCode=${exitCode} duration=${durationMs}ms activeCount=${activeCount}`
```

### formatBytes Output
```typescript
bytes < 1024 → `${bytes}B`
bytes < 1024*1024 → `${(bytes/1024).toFixed(1)}KB`
else → `${(bytes/(1024*1024)).toFixed(1)}MB`
```

## Integration

SubprocessProvider wraps AIBackend instances (ClaudeBackend, GeminiBackend, OpencodeBackend from `src/ai/backends/`). Consumed by AIService in `src/ai/service.ts` which layers withRetry() exponential backoff from `src/ai/retry.ts`. Instantiated in GenerationOrchestrator.run() (`src/generation/orchestrator.ts`) for concurrent pool execution via runPool() (`src/orchestration/pool.ts`).