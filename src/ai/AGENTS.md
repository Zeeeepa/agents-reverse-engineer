<!-- Generated by agents-reverse-engineer -->

# src/ai

AI service layer implementing backend-agnostic subprocess orchestration with exponential backoff retry, timeout enforcement (SIGTERM→SIGKILL escalation), telemetry recording (per-call tokens/latency/cost), and registry-based runtime selection for Claude/Gemini/OpenCode CLI backends.

## Contents

**[index.ts](./index.ts)** — Barrel export enforcing architectural boundary: re-exports `AIService`, `AIServiceOptions` from `./service.js`, `BackendRegistry`, `createBackendRegistry()`, `resolveBackend()`, `detectBackend()`, `getInstallInstructions()` from `./registry.js`, `withRetry()`, `DEFAULT_RETRY_OPTIONS` from `./retry.js`, `runSubprocess()` from `./subprocess.js`, `isCommandOnPath()` from `./backends/claude.js`, all types from `./types.js`. Prohibits direct imports from `backends/` or `telemetry/` subdirectories.

**[registry.ts](./registry.ts)** — `BackendRegistry` stores `AIBackend` instances in insertion-order Map (determines auto-detection priority), `createBackendRegistry()` pre-populates ClaudeBackend→GeminiBackend→OpenCodeBackend, `detectBackend()` returns first `isAvailable()` backend or null, `resolveBackend(requested)` handles `'auto'` via `detectBackend()` or explicit name lookup with `isAvailable()` validation, throws `AIServiceError` code `CLI_NOT_FOUND` with `getInstallInstructions()` on failure.

**[retry.ts](./retry.ts)** — `withRetry<T>(fn, options)` executes fn with exponential backoff (`min(baseDelayMs * multiplier^attempt, maxDelayMs) + random(500)`), retries only when `options.isRetryable(error) === true`, invokes `options.onRetry?.(attempt, error)` before sleep, throws immediately on non-retryable errors or exhausted retries. `DEFAULT_RETRY_OPTIONS` provides `maxRetries: 3, baseDelayMs: 1_000, maxDelayMs: 8_000, multiplier: 2`.

**[service.ts](./service.ts)** — `AIService` orchestrates AI calls: `call(options)` wraps `backend.buildArgs()` + `runSubprocess()` in `withRetry()` (retries only `RATE_LIMIT` errors, not `TIMEOUT`), parses response via `backend.parseResponse()`, records `TelemetryLogger.addEntry()` with tokens/latency/error, emits `subprocess:spawn`/`subprocess:exit` trace events, optionally writes subprocess logs via `enqueueSubprocessLog()` promise chain. `finalize(projectRoot)` persists `RunLog` and invokes `cleanupOldLogs()` retention. Maintains `activeSubprocesses` count for debug output (heap/RSS/PID/duration).

**[subprocess.ts](./subprocess.ts)** — `runSubprocess(command, args, options)` spawns child via `execFile` with `maxBuffer: 10MB`, pipes `options.input` to stdin, enforces `options.timeoutMs` with SIGTERM, escalates to `process.kill(-childPid, 'SIGKILL')` after `SIGKILL_GRACE_MS` (5000ms), always resolves with `SubprocessResult` (never rejects), tracks active processes in Map for debugging, fires `options.onSpawn(childPid)` callback. Exports `getActiveSubprocessCount()`, `getActiveSubprocesses()`.

**[types.ts](./types.ts)** — Defines `AIBackend` interface (`isAvailable`, `buildArgs`, `parseResponse`, `getInstallInstructions`), `AICallOptions` (`prompt`, `systemPrompt?`, `model?`, `timeoutMs?`, `maxTurns?`, `taskLabel?`), `AIResponse` normalized shape (`text`, `model`, `inputTokens`, `outputTokens`, `cacheReadTokens`, `cacheCreationTokens`, `durationMs`, `exitCode`, `raw`), `SubprocessResult` (`stdout`, `stderr`, `exitCode`, `signal`, `durationMs`, `timedOut`, `childPid?`), `RetryOptions`, `TelemetryEntry`, `RunLog`, `AIServiceErrorCode` union (`CLI_NOT_FOUND | TIMEOUT | PARSE_ERROR | SUBPROCESS_ERROR | RATE_LIMIT`), `AIServiceError` class.

## Subdirectories

**[backends/](./backends/)** — Concrete `AIBackend` implementations: `ClaudeBackend` (full JSON/NDJSON parsing via `ClaudeResponseSchema`, `--allowedTools Read,Write`, `--model`/`--system-prompt`/`--max-turns` flags), `GeminiBackend` + `OpenCodeBackend` stubs (both throw `SUBPROCESS_ERROR` in `parseResponse()` pending CLI stabilization). Exports shared `isCommandOnPath()` for PATH detection.

**[telemetry/](./telemetry/)** — Telemetry accumulation (`TelemetryLogger.addEntry()`, `getSummary()` totals, `toRunLog()`), persistence (`writeRunLog()` to `.agents-reverse-engineer/logs/run-${timestamp}.json`), retention (`cleanupOldLogs()` deletes oldest beyond `keepRuns` config).

## Architecture

### Three-Layer Call Flow

1. **Registry Layer** (`registry.ts`): `createBackendRegistry()` → `resolveBackend('auto')` → `detectBackend()` iterates backends calling `isAvailable()` → returns first available or throws `CLI_NOT_FOUND`
2. **Service Layer** (`service.ts`): `AIService.call(options)` → `withRetry(() => ...)` wraps `backend.buildArgs()` + `runSubprocess()` → `backend.parseResponse()` → `logger.addEntry()` telemetry → returns `AIResponse`
3. **Subprocess Layer** (`subprocess.ts`): `runSubprocess()` spawns `execFile`, pipes stdin, enforces timeout, tracks active processes, returns `SubprocessResult` (never throws)

### Retry Strategy

`AIService.call()` configures `withRetry()` with `isRetryable: (error) => error instanceof AIServiceError && error.code === 'RATE_LIMIT'`. Timeouts are NOT retried (prevents resource exhaustion). Rate-limit detection: `RATE_LIMIT_PATTERNS = ['rate limit', '429', 'too many requests', 'overloaded']` matched case-insensitively in stderr. Retry callback increments `retryCount`, logs `[warn]` with attempt number, emits `tracer.emit({ type: 'retry', ... })`.

### Timeout Escalation

`subprocess.ts` sets `timeout: options.timeoutMs` with `killSignal: 'SIGTERM'` in `execFile`. Parallel `setTimeout` schedules SIGKILL at `timeoutMs + SIGKILL_GRACE_MS`. On exit, clears timer. SIGKILL targets process group (`process.kill(-childPid, 'SIGKILL')`) to kill child and descendants, falls back to single-process kill if group kill fails. Timeout detection: `timedOut = error?.killed === true`.

### Telemetry Recording

`TelemetryLogger` accumulates `TelemetryEntry[]` in memory with fields: `timestamp`, `prompt`, `systemPrompt`, `response`, `model`, `inputTokens`, `outputTokens`, `cacheReadTokens`, `cacheCreationTokens`, `latencyMs`, `exitCode`, `error`, `retryCount`, `thinking`, `filesRead`. `getSummary()` totals tokens/duration/errors, counts unique file paths. `toRunLog()` produces `RunLog` with `runId`, `startTime`, `endTime`, `entries`, `summary`. `AIService.finalize()` writes JSON and prunes old logs.

## Behavioral Contracts

### Concurrency Calculation (Not in `src/ai/`)
```javascript
// From src/orchestration/runner.ts (referenced for context)
concurrency = min(20, floor(totalMemGB * 0.5 / 0.512), max(2, os.availableParallelism() * 5))
```

### Exponential Backoff Formula
```javascript
delay = min(baseDelayMs * multiplier^attempt, maxDelayMs) + random(500)
// Defaults: baseDelayMs=1000, multiplier=2, maxDelayMs=8000, jitter=[0,500ms]
```

### Rate Limit Detection Regex (Prose Pattern)
```javascript
stderr.toLowerCase().includes('rate limit') || 
stderr.toLowerCase().includes('429') || 
stderr.toLowerCase().includes('too many requests') || 
stderr.toLowerCase().includes('overloaded')
```

### Timeout Detection
```typescript
timedOut = error !== null && 'killed' in error && error.killed === true
```

### SIGKILL Escalation Delay
```typescript
SIGKILL_GRACE_MS = 5000  // milliseconds after SIGTERM
```

### Subprocess Log Filename Sanitization
```typescript
sanitized = taskLabel.replace(/\//g, '--').replace(/[^a-zA-Z0-9._-]/g, '_')
filename = `${sanitized}_pid${childPid}.log`
```

### Telemetry Log Retention
```javascript
cleanupOldLogs(projectRoot, keepCount)
// Filters: run-*.json sorted lexicographically, deletes oldest beyond keepCount
```

### Backend Priority Order
```typescript
// From createBackendRegistry() insertion order
['ClaudeBackend', 'GeminiBackend', 'OpenCodeBackend']
```

### Exit Code Extraction
```typescript
exitCode = error === null ? 0 :
           typeof error.code === 'number' ? error.code :
           child.exitCode !== null ? child.exitCode : 1
```

### ClaudeBackend CLI Flags
```javascript
['-p', '--output-format', 'json', '--no-session-persistence', '--allowedTools', 'Read', 'Write']
// Optional: --model, --system-prompt, --max-turns
```

### Subprocess maxBuffer
```typescript
maxBuffer: 10 * 1024 * 1024  // 10MB for large AI responses
```

## Integration Points

`src/generation/orchestrator.ts` calls `resolveBackend()` + `new AIService()`, invokes `service.call()` in worker pool (`runPool()` from `src/orchestration/pool.ts`), attaches filesRead metadata via `service.addFilesReadToLastEntry()`, finalizes telemetry via `service.finalize()`. `src/orchestration/trace.ts` provides `ITraceWriter` injected via `service.setTracer()`. CLI commands (`src/cli/generate.ts`, `src/cli/update.ts`) instantiate orchestrator with `config.ai.backend` and `config.ai.concurrency` settings.