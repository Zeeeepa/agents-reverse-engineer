<!-- Generated by agents-reverse-engineer v1.0.1 -->

# src/ai/backends

The `src/ai/backends` directory implements CLI adapters for AI coding assistants (Claude Code, Gemini, Codex, OpenCode), each parsing tool-specific output formats (JSON, NDJSON, JSONL) to normalize responses into `AIResponse` with token metrics, cost tracking, and CLI availability detection.

## Contents

- [`claude.ts`](./claude.ts) — `ClaudeBackend` implements `AIBackend` for Claude Code CLI, parsing three output formats (JSON array ≥2.1.38, NDJSON, legacy ≤2.1.31) via `extractResultJson`, validating with `ClaudeResponseSchema` (Zod), extracting `usage.{input_tokens, cache_creation_input_tokens, cache_read_input_tokens, output_tokens}` and per-model `modelUsage` with `costUSD`, building args `['-p', '--output-format', 'json', '--no-session-persistence', '--allowedTools', 'Read', 'Write']` plus optional `--model`, `--system-prompt`, `--max-turns`
- [`codex.ts`](./codex.ts) — `CodexBackend` implements `AIBackend` for Codex CLI (`codex exec --json`), parsing JSONL event streams with `extractAssistantTextFromItem` (filters `type='agent_message'`, skips `type='reasoning'` via `shouldSkipTextObject`), aggregating `turn.completed` usage events via `extractUsageFromTurnCompleted` (maps `cached_input_tokens` → `cacheReadTokens`, `cache_creation_input_tokens` → `cacheCreationTokens`, computes `inputTokens = rawInput - cacheRead`), falling back to `collectText` recursion over all events then raw stdout, building args `['-a', 'never', 'exec', '--json', '--skip-git-repo-check', '--ephemeral', '--color', 'never', '--model', model?, '-']` with system prompt wrapped in `<system-instructions>` tags via `composePromptWithSystem`
- [`common.ts`](./common.ts) — exports `isCommandOnPath(command: string): Promise<boolean>` checking `process.env.PATH` directories with Windows `PATHEXT` handling (`.exe`, `.cmd`, `.bat`), using `fs.stat` for cross-platform compatibility
- [`gemini.ts`](./gemini.ts) — `GeminiBackend` implements `AIBackend` for Gemini CLI, parsing JSON output via `GeminiResponseSchema` (extracts `response`, `stats.models[modelName].tokens.{prompt, response}`), mapping ARE model aliases (`sonnet` → `gemini-3-flash-preview`, `opus` → `gemini-3-pro-preview`, `haiku` → `gemini-2.5-flash`) via `resolveModelForGemini`, building args `['-p', prompt, '--output-format', 'json', '-m', model]` with system prompt folded into prompt as `<system-instructions>` XML tags
- [`opencode.ts`](./opencode.ts) — `OpenCodeBackend` implements `AIBackend` for OpenCode CLI, parsing NDJSON via `parseNdjson` (aggregates `tokens.{input, output, reasoning, cache.read, cache.write}` and `part.cost` from `OpenCodeStepFinishSchema`, joins `part.text` from `OpenCodeTextSchema`), calling `calculateCostFromTokens` with Anthropic pricing constants (`INPUT_COST_PER_MTOK: 15`, `OUTPUT_COST_PER_MTOK: 75`, `CACHE_WRITE_COST_PER_MTOK: 18.75`, `CACHE_READ_COST_PER_MTOK: 1.50`) when `totalCost === 0`, writing `.opencode/agents/are-summarizer.md` with `steps: 5`, `tools: {"*": false}` via `ensureProjectConfig`, building args `['run', '--format', 'json', '--agent', 'are-summarizer']` with system prompt wrapped in `<system-instructions>` XML tags, mapping model aliases (`sonnet` → `anthropic/claude-sonnet-4-5`) via `resolveModelForOpenCode`

## Architecture

Each backend class implements `AIBackend` interface (from `../types.js`) with methods: `isAvailable(): Promise<boolean>` (PATH detection via `isCommandOnPath`), `buildArgs(options: AICallOptions): string[]` (CLI argument construction), `composeStdinInput(options: AICallOptions): string` (prompt formatting), `parseResponse(stdout: string, durationMs: number, exitCode: number): AIResponse` (output parsing), `getInstallInstructions(): string` (install command). All throw `AIServiceError` with codes `PARSE_ERROR`, `SUBPROCESS_ERROR` on failure.

## Output Format Handling

- **ClaudeBackend**: `extractResultJson` handles JSON array (finds `type: "result"` element), NDJSON (parses lines, finds `type: "result"`), legacy (extracts JSON starting at first `{`); validates with `ClaudeResponseSchema` (requires `subtype: 'success' | 'error'`, `is_error`, `duration_ms`, `num_turns`, `result`, `usage`, `modelUsage`)
- **CodexBackend**: parses JSONL lines, extracts text from `type='agent_message'` events (checks `item.text`, `item.content[].text` where `part.type='text'|'output_text'`), filters reasoning tokens via `shouldSkipTextObject(obj.type='reasoning')`, falls back to `collectText` recursive walk then raw stdout if no assistant items found, sets `AIResponse.raw.format` to `'jsonl'|'jsonl-fallback'|'text'`
- **GeminiBackend**: `extractJson` fast-paths if starts with `{`, else finds first `{` index; `extractMetrics` aggregates `prompt`/`response` tokens across `stats.models` entries
- **OpenCodeBackend**: splits stdout by newlines, filters lines starting with `{`, skips malformed JSON, aggregates tokens/cost from `OpenCodeStepFinishSchema` events (`event.type === 'step_finish'` or `event.part?.type === 'step-finish'`), strips `MAXIMUM STEPS REACHED` marker if content length > 100 chars

## System Prompt Integration

- **ClaudeBackend**: passes `--system-prompt` arg (native support)
- **CodexBackend**: wraps in `<system-instructions>` tags via `composePromptWithSystem` (merged into prompt arg)
- **GeminiBackend**: wraps in `<system-instructions>` tags, folded into `-p` arg
- **OpenCodeBackend**: wraps in `<system-instructions>` tags via `composeStdinInput` (no native flag)

## Model Alias Resolution

- **GeminiBackend**: `MODEL_ALIASES` maps `sonnet`, `opus`, `haiku`, `flash`, `pro`; `resolveModelForGemini` returns input if starts with `gemini-`, else lookups alias, else passthrough
- **OpenCodeBackend**: `MODEL_ALIASES` maps `sonnet`, `opus`, `haiku` to `anthropic/claude-*-*-*`; `resolveModelForOpenCode` returns input if contains `/`, else lookups alias

## Behavioral Contracts

- **ClaudeBackend**: `ClaudeResponseSchema` requires `type: 'result'`, `subtype: 'success' | 'error'`, `is_error: boolean`, `duration_ms`, `duration_api_ms`, `num_turns`, `result`, `session_id`, `total_cost_usd`, `usage: { input_tokens, cache_creation_input_tokens, cache_read_input_tokens, output_tokens }`, `modelUsage: Record<string, { inputTokens, outputTokens, cacheReadInputTokens, cacheCreationInputTokens, costUSD }>`
- **CodexBackend**: `extractAssistantTextFromItem` checks `type='agent_message'`, `shouldSkipTextObject` skips `type='reasoning'` or `type.endsWith('.reasoning')`, `extractUsageFromTurnCompleted` maps `input_tokens`, `cached_input_tokens`, `cache_creation_input_tokens`, `output_tokens` (and camelCase variants) with `inputTokens = rawInput - cacheRead`
- **GeminiBackend**: `GeminiResponseSchema` defines `{ response: string | null, stats?: { session?: { duration?: number }, models?: Record<string, { tokens?: { prompt?: number, response?: number, total?: number } }> }, error?: unknown }`
- **OpenCodeBackend**: `OpenCodeTokensSchema` has `total`, `input`, `output`, `reasoning`, `cache.read`, `cache.write` (all optional, defaults 0); `OpenCodeStepFinishPartSchema` has `type: 'step-finish'`, `cost`, `tokens`; `OpenCodeTextPartSchema` has `type: 'text'`, `text`; Anthropic pricing constants: `INPUT_COST_PER_MTOK: 15`, `OUTPUT_COST_PER_MTOK: 75`, `CACHE_WRITE_COST_PER_MTOK: 18.75`, `CACHE_READ_COST_PER_MTOK: 1.50`; `OPENCODE_AGENT_NAME: 'are-summarizer'` with YAML frontmatter `steps: 5`, `tools: {"*": false}`