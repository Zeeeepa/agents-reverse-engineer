---
generated_at: 2026-02-09T14:08:25.660Z
content_hash: 8b123030a9ae9b650e3148fccfc8bcfe2bc0e1b2bb0bfa9db2d6a0d7901c3178
purpose: index.ts exports the public API surface for the AI service layer, enforcing encapsulation by providing a single impor...
---
**index.ts exports the public API surface for the AI service layer, enforcing encapsulation by providing a single import point for AIService, BackendRegistry, retry utilities, subprocess execution, and all related types.**

## Exported Types

- `AIBackend` — Interface defining backend contract (name, availabilityCheck, buildCommand, parseResponse, pricing)
- `AIResponse` — Parsed response from AI CLI containing content, usage tokens (input/output/cacheRead/cacheWrite), model, stopReason
- `AICallOptions` — Parameters for AIService.call(): prompt, traceEmitter, fileMetadata, retryCount
- `SubprocessResult` — Raw result from runSubprocess(): stdout, stderr, exitCode, duration, killed, signal
- `RetryOptions` — Configuration for exponential backoff: maxRetries, initialDelayMs, maxDelayMs, backoffFactor
- `TelemetryEntry` — Single AI call record with timestamps, tokens, cost, duration, error, filesRead
- `RunLog` — Aggregated session log with backend, totalInputTokens, totalCacheReadTokens, errorCount, uniqueFilesRead
- `FileRead` — Metadata tracking file access: path, sizeBytes, linesRead
- `AIServiceOptions` — Configuration for AIService constructor: timeoutMs, maxRetries, telemetry (enabled/keepRuns/costThresholdUsd), concurrency

## Exported Classes and Functions

- `AIServiceError` — Custom error class for AI layer failures
- `AIService` — Main orchestrator calling AI backends via subprocess spawning, retry logic, telemetry logging, trace emission
- `BackendRegistry` — Registry managing available AIBackend implementations with lazy loading
- `createBackendRegistry()` — Factory instantiating registry with claude/gemini/opencode backends
- `resolveBackend(registry, nameOrAuto)` — Resolves backend name ('auto'|'claude'|'gemini'|'opencode') to AIBackend instance via availability check
- `detectBackend(registry)` — Iterates registry backends, returns first available via availabilityCheck()
- `getInstallInstructions(backendName)` — Returns user-facing installation guide for unavailable backend
- `withRetry(fn, options)` — Exponential backoff wrapper detecting rate limits via stderr pattern matching ("rate limit", "429", "too many requests", "overloaded")
- `DEFAULT_RETRY_OPTIONS` — Default retry config: maxRetries=3, initialDelayMs=1000, maxDelayMs=32000, backoffFactor=2
- `runSubprocess(command, args, options)` — Spawns child process via execFile() with resource limits (NODE_OPTIONS, UV_THREADPOOL_SIZE, CLAUDE_CODE_DISABLE_BACKGROUND_TASKS), timeout enforcement (SIGTERM→SIGKILL escalation), process group killing (kill(-pid))
- `isCommandOnPath(command)` — Checks command availability via `which` on Unix or `where` on Windows

## Module Boundaries

Enforces strict encapsulation: consumers must import from `src/ai/index.ts` rather than reaching into `src/ai/backends/` or `src/ai/telemetry/` subdirectories. Backend implementations (ClaudeBackend, GeminiBackend, OpenCodeBackend) and telemetry internals (TelemetryLogger, cleanupOldLogs) remain private to the AI layer.

## Integration Pattern

Example workflow: call createBackendRegistry(), resolve backend via 'auto' detection or explicit name, instantiate AIService with backend and options (timeout/retry/telemetry config), invoke service.call() with prompt and optional trace emitter. AIService handles subprocess spawning, retry on rate limits, NDJSON telemetry logging to `.agents-reverse-engineer/logs/`, and trace event emission for orchestration layer consumption.