<!-- Generated by agents-reverse-engineer -->

# src/orchestration

Concurrent task execution subsystem implementing iterator-based worker pools with shared task queues, serialized progress/plan/trace file writes via promise-chain queueing, ETA-aware streaming console output mirroring to `.agents-reverse-engineer/progress.log`, NDJSON trace event emission with phase/worker/task/subprocess lifecycle tracking, and three-phase pipeline coordination integrating AIService, ExecutionPlan, quality validators, and telemetry aggregation.

## Contents

### Core Execution

**[pool.ts](./pool.ts)** — Iterator-based worker pool (`runPool<T>()`) executing async task factories with shared-iterator concurrency limiting where N workers pull from single `tasks.entries()` iterator, each task executes exactly once with immediate next-task pickup upon completion avoiding batch-chunking idle time. Returns `TaskResult<T>[]` indexed by original task position (sparse if `failFast` aborts early). Emits trace events: `worker:start/end` with `tasksExecuted` count, `task:pickup/done` with `durationMs`, `success`, `activeTasks` snapshot. Uses `aborted` flag checked before iterator pull to stop work on first error. PoolOptions configures `concurrency`, `failFast`, `tracer`, `phaseLabel`, `taskLabels` for human-readable event tagging.

**[runner.ts](./runner.ts)** — CommandRunner orchestrates three-phase pipeline via `executeGenerate()` (full: file analysis → directory aggregation → root synthesis) and `executeUpdate()` (incremental file re-analysis only). Phase 1 maps ExecutionPlan.fileTasks to `buildFilePrompt()` + `AIService.call()` + `writeSumFile()` via pool with concurrency limiting, computes `contentHash` from in-memory `sourceContentCache`, emits `phase:start/end` trace events, updates PlanTracker via `markDone()`, integrates ProgressReporter callbacks (`onFileStart/onFileDone/onFileError`). Post-Phase 1 runs directory-scoped quality checks via throttled pool (concurrency=10): stale doc comparison against `oldSumCache`, fresh doc validation via `checkCodeVsDoc()`, duplicate export detection via `checkCodeVsCode()`, builds `InconsistencyReport` with `inconsistenciesCodeVsDoc`, `inconsistenciesCodeVsCode` counters (non-throwing). Phase 2 processes directoryTasks grouped by `metadata.depth` descending (deepest first = post-order traversal), per depth level runs `buildDirectoryPrompt()` + `aiService.call()` + `writeAgentsMd()` via pool with `phaseLabel: 'phase-2-dirs-depth-${depth}'`, injects child `.sum` content, subdirectory AGENTS.md files, import maps filtered via `knownDirs`. Post-Phase 2 runs `checkPhantomPaths()` extracting path-like strings via regex, resolving against project root, updates `phantomPathCount`. Phase 3 iterates rootTasks sequentially (concurrency=1), calls `buildRootPrompt()` injecting all AGENTS.md via `collectAgentsDocs()`, strips conversational preamble (finds first `'# '` header), writes to `rootTask.outputPath`, emits `task:start/done` events. Returns RunSummary aggregating `aiService.getSummary()` metrics: `version`, `filesProcessed`, `filesFailed`, token counts (`totalInputTokens`, `totalOutputTokens`, `totalCacheReadTokens`, `totalCacheCreationTokens`), `totalDurationMs`, `errorCount`, `retryCount`, telemetry fields (`totalFilesRead`, `uniqueFilesRead`), quality counters, optional `inconsistencyReport`. Integrates PlanTracker via `flush()`, ProgressReporter via `printSummary()`. Helper functions: `stripPreamble()` detects LLM preamble via `\n---\n` separator or bold text patterns, `extractPurpose()` iterates lines skipping headers/separators/preamble prefixes (13 patterns: 'now i', 'perfect', 'based on', etc.), strips bold markdown, truncates to 120 chars.

### Progress Tracking

**[progress.ts](./progress.ts)** — ProgressReporter streams real-time console output with ETA calculation via moving-average completion times (sliding window size=10), emitting colored lines (`onFileStart`, `onFileDone`, `onFileError`, `onDirectoryStart`, `onDirectoryDone`, `onRootDone`) with format `[X/Y] STATUS path duration tokens model ~ETA`, using picocolors (`pc.cyan('ANALYZING')`, `pc.green('DONE')`, `pc.blue('DONE')`, `pc.red('FAIL')`). Token counts formatted as `totalIn/out tok` where `totalIn = tokensIn + cacheReadTokens + cacheCreationTokens`. `printSummary(summary: RunSummary)` emits multi-line block showing `version`, `filesProcessed`, `filesFailed`, `filesSkipped`, `totalCalls`, token totals (in/out), `totalFilesRead`, `uniqueFilesRead`, `totalTime` (elapsed from `startTime`), `errorCount`, `retryCount` with conditional coloring (green=success, red=errors, yellow=warnings). ProgressLog provides file-backed mirroring to `.agents-reverse-engineer/progress.log` for real-time monitoring via `tail -f` in buffered environments (Claude Code Bash tool), strips ANSI via `stripAnsi()` regex `/\x1b\[[0-9;]*m/g`, uses promise-chain serialization (`writeQueue = writeQueue.then(...)`) to handle concurrent writes from pool workers safely, opens FileHandle in truncate mode ('w') on first write, swallows write errors (non-critical telemetry). ETA formatters (`formatETA()`, `formatDirectoryETA()`) compute remaining time via `average(completionTimes) * (total - completed - failed)`, require minimum 2 samples, format as `~12s remaining` or `~2m 30s remaining` with `pc.dim()` styling. Counter prefixes: `[X/Y]` for files (`completed + failed` of `totalFiles`), `[dir X/Y]` for directories (`dirCompleted` of `totalDirectories`), `[root]` for root docs (no counter).

**[plan-tracker.ts](./plan-tracker.ts)** — PlanTracker serializes concurrent checkbox updates to GENERATION-PLAN.md via promise-chain queueing preventing file corruption during parallel Phase 1 worker completion. Constructor stores `initialMarkdown` in `content` property, computes `planPath` as `{projectRoot}/.agents-reverse-engineer/GENERATION-PLAN.md`. `initialize()` creates parent directory recursively, writes initial plan (catches errors silently). `markDone(itemPath: string)` replaces `- [ ] \`${itemPath}\`` with `- [x] \`${itemPath}\`` in memory, chains `writeFile()` onto `writeQueue` promise (`this.writeQueue = this.writeQueue.then(() => writeFile(...))`), returns early if regex produces no change (no match), swallows write errors (non-critical tracking). Caller provides exact markdown path format: file tasks use `src/cli/init.ts`, directory tasks append `/AGENTS.md`, root tasks use `CLAUDE.md`. `flush()` awaits `writeQueue` to complete pending writes before exit. Shares serialization pattern with TraceWriter for NDJSON emission.

### Telemetry

**[trace.ts](./trace.ts)** — Implements append-only NDJSON trace event emission for debugging concurrent task/subprocess lifecycles. ITraceWriter interface exposes `emit(event: TraceEventPayload)` appending events with auto-populated base fields (`seq` monotonic, `ts` ISO 8601, `pid` Node.js process, `elapsedMs` high-resolution delta from `startHr`), `finalize()` flushing pending writes and closing FileHandle, readonly `filePath` exposing absolute trace path (empty string for NullTraceWriter). TraceEvent discriminated union of 14 types sharing TraceEventBase: PhaseStartEvent/PhaseEndEvent (`phase`, `taskCount`, `concurrency`, `durationMs`, `tasksCompleted`, `tasksFailed`), WorkerStartEvent/WorkerEndEvent (`workerId`, `phase`, `tasksExecuted`), TaskPickupEvent/TaskDoneEvent (`workerId`, `taskIndex`, `taskLabel`, `activeTasks`, `durationMs`, `success`, `error?`), TaskStartEvent (`taskLabel`, `phase`), SubprocessSpawnEvent/SubprocessExitEvent (`childPid`, `command`, `taskLabel`, `exitCode`, `signal`, `durationMs`, `timedOut`), RetryEvent (`attempt`, `taskLabel`, `errorCode`), DiscoveryStartEvent/DiscoveryEndEvent (`targetPath`, `filesIncluded`, `filesExcluded`, `durationMs`), FilterAppliedEvent (`filterName`, `filesMatched`, `filesRejected`), PlanCreatedEvent (`planType`, `fileCount`, `taskCount`), ConfigLoadedEvent (`configPath`, `model`, `concurrency`). TraceEventPayload alias `DistributiveOmit<TraceEvent, BaseKeys>` strips auto-populated fields for emit() call sites (uses helper type `T extends unknown ? Omit<T, K> : never` to correctly distribute across union members, avoiding standard `Omit<Union, Keys>` failure documented in MEMORY.md). Factory `createTraceWriter(projectRoot, enabled)` returns NullTraceWriter (no-op with empty methods, zero overhead) when enabled=false, otherwise TraceWriter writing to `.agents-reverse-engineer/traces/trace-{ISO-timestamp}.ndjson` with sanitized filename (colons/dots → hyphens). TraceWriter maintains `seq` counter starting 0, `nodePid` from process.pid, `startHr` from `process.hrtime.bigint()` for elapsedMs calculation, `writeQueue` promise chain for serialized appends, `fd` FileHandle opened lazily on first emit. emit() enriches payload with base fields (`seq++`, `new Date().toISOString()`, `nodePid`, `elapsedMs` via `Number(process.hrtime.bigint() - startHr) / 1_000_000`), serializes to `JSON.stringify(event) + '\n'`, chains write via `this.writeQueue = this.writeQueue.then(async () => {...})` pattern, opens fd via `open(filePath, 'a')` on first write after creating parent directory, swallows write errors (trace loss acceptable). finalize() awaits writeQueue then closes fd if open. Promise-chain serialization identical to PlanTracker ensures NDJSON line order matches emission order despite concurrent worker pool calls. Cleanup function `cleanupOldTraces(projectRoot, keepCount=500)` deletes old traces keeping keepCount most recent sorted by lexicographic ISO timestamp, returns deleted file count, tolerates missing directory via ENOENT check. Threaded through CommandRunOptions.tracer, consumed by pool workers (`src/orchestration/pool.ts`), AIService subprocess spawning (`src/ai/service.ts`), phase runners (`src/generation/orchestrator.ts`, `src/update/orchestrator.ts`).

### Type Definitions

**[types.ts](./types.ts)** — Shared orchestration interfaces. FileTaskResult carries per-file AI call metrics: `path`, `success`, `tokensIn`, `tokensOut`, `cacheReadTokens`, `cacheCreationTokens`, `durationMs`, `model`, `error?`. RunSummary aggregates command statistics: `version` (ARE version string), `filesProcessed`, `filesFailed`, `filesSkipped`, `totalCalls`, `totalInputTokens`, `totalOutputTokens`, `totalCacheReadTokens`, `totalCacheCreationTokens`, `totalDurationMs`, `errorCount`, `retryCount`, `totalFilesRead`, `uniqueFilesRead`, `inconsistenciesCodeVsDoc?`, `inconsistenciesCodeVsCode?`, `phantomPaths?`, `inconsistencyReport?` (from quality checks). ProgressEvent discriminated by `type` field ('start'|'done'|'error'|'dir-done'|'root-done'): `filePath`, `index`, `total`, `durationMs?`, `tokensIn?`, `tokensOut?`, `model?`, `error?` for real-time task updates. CommandRunOptions combines config defaults and CLI overrides: `concurrency`, `failFast?`, `debug?`, `dryRun?`, `tracer?` (ITraceWriter for NDJSON logging, NullTraceWriter when disabled), `progressLog?` (ProgressLog for file-backed monitoring). Imports InconsistencyReport from `../quality/index.js`, ProgressLog from `./progress.js`, ITraceWriter from `./trace.js`. Used by pool.ts (consumes CommandRunOptions, emits FileTaskResult), runner.ts (emits ProgressEvent, produces RunSummary), progress.ts (consumes ProgressEvent). Contract interfaces following dependency inversion principle enabling testability via mock implementations.

**[index.ts](./index.ts)** — Barrel export aggregating orchestration subsystems: `runPool()` iterator-based concurrency pool, `cleanupOldTraces()` retention management, CommandRunner high-level orchestrator with `executeGenerate()`, `executeUpdate()`, `executeDiscover()` methods, ProgressReporter streaming progress with ETA calculation, ProgressLog file-backed monitoring, PlanTracker serialized GENERATION-PLAN.md writer, `createTraceWriter()` factory returning ITraceWriter (NullTraceWriter or TraceWriter based on flag). Exports types: FileTaskResult, RunSummary, ProgressEvent, CommandRunOptions (includes `tracer?: ITraceWriter`), PoolOptions (concurrency, fail-fast, callbacks), TaskResult generic result type. Exports interfaces: ITraceWriter (contract with `emit()`, `end()`, `createChild()`), TraceEvent discriminated union (phase:start/end, worker:start/end, task:pickup/done, subprocess:spawn/exit, retry), TraceEventPayload raw event data before auto-populated fields (seq, ts, pid, elapsedMs). Provides single import point for three-phase pipeline orchestration with NDJSON trace emission, serialized file writes, ETA-aware progress streaming.

## Architecture

### Iterator-Based Pool Pattern

`runPool()` uses shared-iterator concurrency limiting where N workers pull from single `tasks.entries()` iterator (not chunked into per-worker arrays), ensuring each task executes exactly once with immediate next-task pickup upon completion avoiding batch-chunking idle time. Spawns `Math.min(options.concurrency, tasks.length)` workers via `Promise.allSettled()`. Uses `aborted` flag checked before each iterator pull to stop work on first error when `options.failFast` is true. Returns `TaskResult<T>[]` indexed by original task position (may be sparse if `failFast` aborts early).

### Promise-Chain Serialization

PlanTracker and TraceWriter use identical serialization pattern: `this.writeQueue = this.writeQueue.then(() => writeFile(...))` where each write call appends to promise chain, ensuring sequential execution despite concurrent pool worker completions. Prevents race conditions where multiple workers completing simultaneously corrupt file via interleaved write operations. ProgressLog applies same pattern for `.agents-reverse-engineer/progress.log` appends.

### Three-Phase Pipeline Coordination

CommandRunner.executeGenerate() orchestrates full pipeline:
1. **Pre-Phase 1**: Read existing `.sum` files via throttled pool (concurrency=20), populate `oldSumCache` for stale doc detection
2. **Phase 1**: File analysis via `runPool()` with `phaseLabel: 'phase-1-files'`, cache `sourceContent` in `sourceContentCache`, compute `contentHash` from memory (avoid double read), write SumFileContent with stripped preamble via `stripPreamble()`, extracted purpose via `extractPurpose()`, update PlanTracker via `markDone()`, integrate ProgressReporter callbacks
3. **Post-Phase 1**: Directory-scoped quality checks via throttled pool (concurrency=10): stale doc check comparing `oldSumCache` against `sourceContent`, fresh doc check reading newly-written `.sum` files, duplicate export check via `checkCodeVsCode()`, build `InconsistencyReport` (non-throwing), clear `sourceContentCache` to free memory
4. **Phase 2**: Directory AGENTS.md generation via post-order traversal (depth-level batching: group by `metadata.depth`, process descending deepest-first), per depth level run pool with `phaseLabel: 'phase-2-dirs-depth-${depth}'`, inject child `.sum` content, subdirectory AGENTS.md, import maps filtered via `knownDirs`, update ProgressReporter via directory callbacks, mark done in PlanTracker
5. **Post-Phase 2**: Phantom path validation via `checkPhantomPaths()` extracting path-like strings via regex, resolving against project root, build `InconsistencyReport` (non-throwing), update `phantomPathCount`
6. **Phase 3**: Root document synthesis via sequential iteration (concurrency=1), inject all AGENTS.md via `collectAgentsDocs()`, strip conversational preamble (find first `'# '` header), write to `rootTask.outputPath`, update ProgressReporter via `onRootDone()`, mark done in PlanTracker
7. **Summary**: Aggregate `aiService.getSummary()` into RunSummary with version, file counts, token totals, duration, error/retry counts, telemetry fields, quality metrics, flush PlanTracker, print summary

CommandRunner.executeUpdate() runs Phase 1 only (no directory/root regeneration), reads optional `GENERATION-PLAN.md` for project structure context, caches source in `updateSourceCache`, runs post-analysis quality checks (fresh doc validation + directory-scoped duplicate detection), clears cache, constructs RunSummary with update-specific inconsistency counters.

### ETA Calculation Strategy

ProgressReporter maintains sliding window (`completionTimes` array, size=10) of last N task durations, computes ETA via moving average: `average(completionTimes) * (totalFiles - completed - failed)` remaining tasks, requires minimum 2 samples, formats as `~12s remaining` or `~2m 30s remaining` with `pc.dim()` styling. Separate windows for file tasks (`completionTimes`) and directory tasks (`dirCompletionTimes`). Updated on each `onFileDone()`/`onDirectoryDone()` callback receiving `durationMs` from pool worker.

### Trace Event Lifecycle

14 trace event types emitted across subsystems:
- **Phase events** (runner.ts): `phase:start/end` at phase boundaries with `taskCount`, `concurrency`, `durationMs`, `tasksCompleted`, `tasksFailed`
- **Worker events** (pool.ts): `worker:start/end` with `workerId`, `phase`, `tasksExecuted` counter
- **Task events** (pool.ts + runner.ts): `task:pickup` with `activeTasks` snapshot before execution, `task:done` with `durationMs`, `success`, `error?`, `activeTasks` after settle, `task:start` for Phase 3 sequential root tasks
- **Subprocess events** (AIService in `src/ai/service.ts`): `subprocess:spawn/exit` with `childPid`, `command`, `exitCode`, `signal`, `durationMs`, `timedOut`
- **Retry events** (AIService): `retry` with `attempt`, `errorCode`
- **Discovery events** (discovery runner): `discovery:start/end`, `filter:applied`
- **Planning events** (execution planner): `plan:created`
- **Config events** (config loader): `config:loaded`

All events enriched with base fields: `seq` (monotonic), `ts` (ISO 8601), `pid` (Node.js process), `elapsedMs` (high-resolution delta from `startHr` via `process.hrtime.bigint()`). Written to `.agents-reverse-engineer/traces/trace-{ISO-timestamp}.ndjson` via promise-chain serialization. Cleanup retains 500 most recent traces via `cleanupOldTraces()`.

### Quality Validation Integration

CommandRunner runs two post-analysis validation phases:
1. **Post-Phase 1**: Directory-scoped checks throttled via pool (concurrency=10): stale doc comparison (cached `oldSumCache` vs. `sourceContent` via `checkCodeVsDoc()`), fresh doc validation (read newly-written `.sum` files, run `checkCodeVsDoc()`), duplicate export detection (aggregate files per directory, run `checkCodeVsCode()`). Builds `InconsistencyReport` with `inconsistenciesCodeVsDoc`, `inconsistenciesCodeVsCode` counters (non-throwing design prevents pipeline breakage).
2. **Post-Phase 2**: Phantom path validation (read generated AGENTS.md files, run `checkPhantomPaths()` extracting path-like strings via regex, resolve against project root). Updates `phantomPathCount` in RunSummary.

Both phases output formatted report via `formatReportForCli()`, log errors via `console.error()` with `[quality]` prefix, swallow exceptions to prevent pipeline failure.