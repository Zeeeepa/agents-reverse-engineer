<!-- Generated by agents-reverse-engineer -->

# src/orchestration

Coordinates parallel file analysis, post-order directory aggregation, and sequential root synthesis through iterator-based worker pools with ETA-aware progress streaming, serialized file writes, and optional NDJSON trace emission for debugging concurrent task lifecycles.

## Contents

### Core Orchestration

**[index.ts](./index.ts)** — Barrel export aggregating orchestration subsystems: `runPool()` iterator-shared concurrency limiter, `CommandRunner` three-phase pipeline executor, `ProgressReporter`/`ProgressLog` streaming updates with moving-average ETA, `PlanTracker` serialized GENERATION-PLAN.md checkbox updates, `createTraceWriter()`/`cleanupOldTraces()` trace file management, shared types (`FileTaskResult`, `RunSummary`, `ProgressEvent`, `CommandRunOptions`, `PoolOptions`, `TaskResult`, `ITraceWriter`, `TraceEvent`, `TraceEventPayload`).

**[runner.ts](./runner.ts)** — `CommandRunner` class wiring `AIService`, `ExecutionPlan`, worker pool, progress reporter, plan tracker, quality validators into cohesive pipeline. The `executeGenerate()` method runs five phases: pre-phase-1 sum cache load (20 workers), phase-1 concurrent file analysis (N workers calling `buildFilePrompt()` → `AIService.call()` → `writeSumFile()`), post-phase-1 quality validation (10 workers running `checkCodeVsDoc()`/`checkCodeVsCode()` per directory group), phase-2 post-order directory aggregation (depth-grouped pools calling `buildDirectoryPrompt()` → `writeAgentsMd()`), post-phase-2 phantom path validation (`checkPhantomPaths()` on all AGENTS.md), phase-3 sequential root synthesis (`buildRootPrompt()` → strip preamble via `stripPreamble()` → write CLAUDE.md/GEMINI.md/OPENCODE.md). The `executeUpdate()` method runs phase-1 file analysis only for `filesToAnalyze` array with quality validation but no directory/root regeneration. Emits trace events for phase boundaries and root tasks. Aggregates token counts via `AIService.getSummary()` into `RunSummary` with quality metrics. Instantiates `ProgressReporter` with optional file logger, calls `onFileStart()`/`onFileDone()`/`onFileError()` from pool callbacks, calls `printSummary()` at finalization.

**[pool.ts](./pool.ts)** — `runPool<T>()` executes task factory array through concurrency-limited worker pool using shared `tasks.entries()` iterator ensuring each task executes exactly once. Spawns `Math.min(concurrency, tasks.length)` workers via `Promise.allSettled()`. Workers increment `activeTasks` counter before execution, decrement after settle. Sets `aborted` flag on first error when `failFast=true` to stop iterator pulls. Returns sparse `TaskResult<T>[]` indexed by original task position. Emits trace events: `worker:start`/`worker:end` with `tasksExecuted` count, `task:pickup` with `activeTasks` snapshot, `task:done` with `durationMs` and `success` boolean. Wraps non-Error rejections via `err instanceof Error ? err : new Error(String(err))`. Invokes `onComplete(result: TaskResult<T>)` callback after each task settle for progress reporting.

### Progress Monitoring

**[progress.ts](./progress.ts)** — `ProgressReporter` streams real-time file/directory processing progress to console with picocolors formatting and ETA calculation via moving-average (window size 10) of completion times. The `onFileDone()`/`onDirectoryDone()` methods push `durationMs` to sliding windows, format ETA as `~Xs` or `~Xm Ys remaining`. Tracks six counters: `started`/`completed`/`failed` (files), `dirStarted`/`dirCompleted` (directories), plus `startTime` for elapsed calculation. Calls `printSummary(summary: RunSummary)` at pipeline end with token counts, file read stats, error/retry counts. The `ProgressLog` class mirrors console output to `.agents-reverse-engineer/progress.log` as ANSI-stripped plain text using promise-chain serialization (`writeQueue`) for concurrent-safe writes from pool workers. Opens file in `'w'` mode on first `write()` call, queues appends, closes `FileHandle` in `finalize()`.

### State Tracking

**[plan-tracker.ts](./plan-tracker.ts)** — `PlanTracker` serializes concurrent checkbox updates to GENERATION-PLAN.md via promise-chain pattern `this.writeQueue = this.writeQueue.then(() => writeFile(...))` preventing file corruption during parallel Phase 1 worker completion. The `markDone(itemPath: string)` method replaces `- [ ] \`${itemPath}\`` with `- [x] \`${itemPath}\`` in memory, then queues serialized write. Computes `planPath` as `{projectRoot}/.agents-reverse-engineer/GENERATION-PLAN.md` via `CONFIG_DIR` constant. The `flush()` method awaits `writeQueue` completion before command exit. Catches write errors silently (non-critical tracking). Instantiated in `CommandRunner.executeGenerate()` with initial markdown from plan generation, receives `markDone()` calls from pool worker callbacks.

### Telemetry Tracing

**[trace.ts](./trace.ts)** — `ITraceWriter` interface contract with `emit(event: TraceEventPayload)` appending NDJSON line with auto-populated base fields (`seq` monotonic, `ts` ISO 8601, `pid` Node.js parent, `elapsedMs` high-resolution delta from `process.hrtime.bigint()`), `finalize()` flushing writes and closing file handle, `filePath` exposing trace file path. The `TraceWriter` class implements append-only NDJSON emission to `.agents-reverse-engineer/traces/trace-{ISO-timestamp}.ndjson` via promise-chain serialization `this.writeQueue = this.writeQueue.then(async () => {...}).catch(() => {})` ensuring line order matches emission order despite concurrent pool workers. Lazily opens `fd` via `open(filePath, 'a')` on first emit after creating parent directory. The `NullTraceWriter` class provides no-op implementation for zero overhead when `--trace` flag absent. The `createTraceWriter(projectRoot, enabled)` factory returns appropriate implementation. The `cleanupOldTraces(projectRoot, keepCount=500)` function deletes old trace files keeping 500 most recent sorted lexicographically. The `TraceEvent` discriminated union defines 14 event types: `phase:start`/`phase:end`, `worker:start`/`worker:end`, `task:pickup`/`task:done`, `task:start`, `subprocess:spawn`/`subprocess:exit`, `retry`, `discovery:start`/`discovery:end`, `filter:applied`, `plan:created`, `config:loaded`. Uses `DistributiveOmit<TraceEvent, BaseKeys>` helper type to strip auto-populated fields for `TraceEventPayload`.

**[types.ts](./types.ts)** — Type contracts shared across orchestration module: `FileTaskResult` (outcome of single file AI analysis with `path`, `success`, token counts, `durationMs`, `model`, optional `error`), `RunSummary` (aggregated metrics with `filesProcessed`/`filesFailed`/`filesSkipped`, total token counts, `errorCount`, `retryCount`, file read stats, optional quality metrics `inconsistenciesCodeVsDoc`/`inconsistenciesCodeVsCode`/`phantomPaths`/`inconsistencyReport`), `ProgressEvent` (streaming event with `type` discriminator `'start'|'done'|'error'|'dir-done'|'root-done'`, `filePath`, `index`, `total`, optional `durationMs`/token counts/`model`/`error`), `CommandRunOptions` (execution config with `concurrency`, `failFast`, `debug`, `dryRun`, optional `tracer: ITraceWriter`, optional `progressLog: ProgressLog`). Used by `CommandRunner`, pool executor, progress reporter for consistent configuration threading.

## Architecture

### Concurrency Model

Iterator-based worker pool (`runPool()`) shares single `tasks.entries()` iterator across N workers so each task executes exactly once without batch-chunking idle time. Workers pull next task immediately upon completion via `for await (const [index, taskFn] of iterator)` pattern. `activeTasks` counter tracks concurrent execution depth for trace events. `aborted` flag checked before each iterator pull stops work on first error when `failFast=true`. Returns sparse `TaskResult<T>[]` array indexed by original task position (may contain undefined entries if `failFast` aborted early).

### Progress Streaming

`ProgressReporter` maintains two sliding windows (`completionTimes[]`, `dirCompletionTimes[]`) with window size 10 for moving-average ETA calculation. The `formatETA()` method computes file task ETA as `avg(completionTimes) * (totalFiles - completed - failed)`, returns empty string if fewer than 2 completions. The `formatDirectoryETA()` method applies same algorithm to directory tasks. Progress logged to console via picocolors (cyan=start, green=done, red=error, blue=directories) and mirrored to `.agents-reverse-engineer/progress.log` via `stripAnsi()` ANSI escape removal.

### Serialized Writes

`PlanTracker` and `TraceWriter` prevent concurrent write corruption via promise-chain pattern `this.writeQueue = this.writeQueue.then(async () => {...}).catch(() => {})`. Each write operation chains onto shared `writeQueue` promise ensuring sequential execution despite concurrent pool worker callbacks. Initialized as `Promise.resolve()`, chains append operations, swallows errors in catch (tracking loss acceptable). Caller must await `flush()`/`finalize()` before exit to ensure all writes persist.

### Quality Validation

Phase integration points in `CommandRunner.executeGenerate()`: post-phase-1 spawns 10-worker pool grouping processed files by directory via `path.dirname()`, runs `checkCodeVsDoc()` twice per file (once against `oldSumCache` detecting stale docs with `' (stale documentation)'` suffix, once against fresh `.sum` detecting LLM omissions), runs `checkCodeVsCode()` per directory group aggregating exports into `Map<symbol, string[]>` detecting duplicates. Post-phase-2 reads all `AGENTS.md` files, runs `checkPhantomPaths()` per directory resolving path-like strings via three regex patterns. Aggregates issues into `InconsistencyReport`, prints via `formatReportForCli()`, populates `RunSummary.inconsistenciesCodeVsDoc`/`inconsistenciesCodeVsCode`/`phantomPaths` counters. All validation wrapped in try/catch with error logging, non-throwing (failures don't abort pipeline).

### Trace Event Flow

Threaded via `CommandRunOptions.tracer` field consumed by: `CommandRunner` emitting `phase:start`/`phase:end`/`task:start`/`task:done` for phase boundaries and root task execution, `runPool()` emitting `worker:start`/`worker:end`/`task:pickup`/`task:done` for pool lifecycle, `AIService` emitting `subprocess:spawn`/`subprocess:exit`/`retry` for subprocess management (see `src/ai/service.ts`). Events share base fields (`seq`, `ts`, `pid`, `elapsedMs`) auto-populated by `TraceWriter.emit()`. Retention managed via `cleanupOldTraces(keepCount=500)` deleting old files sorted lexicographically by ISO timestamp in filename.

## Pipeline Execution Phases

### Pre-Phase 1: Sum Cache Load
- Spawns 20-worker pool reading existing `.sum` files via `readSumFile()` into `oldSumCache` Map
- Enables stale documentation detection by comparing old summary against fresh LLM output
- Phase label: `'pre-phase-1-cache'`

### Phase 1: File Analysis (Concurrent)
- Spawns N-worker pool (concurrency from `CommandRunOptions.concurrency`)
- Per file: reads source, calls `buildFilePrompt()` with import maps/manifest detection, invokes `AIService.call()`, computes SHA-256 `contentHash` via `computeContentHashFromString()`, writes `.sum` with YAML frontmatter via `writeSumFile()`, caches source in `sourceContentCache` Map
- Pool callback updates `ProgressReporter` via `onFileDone()`/`onFileError()`, marks `PlanTracker` via `markDone()`, increments `filesProcessed`/`filesFailed`
- Phase label: `'phase-1-files'` (generate), `'update-phase-1-files'` (update)

### Post-Phase 1: Quality Validation (Non-Throwing)
- Groups processed files by directory via `path.dirname()` into `dirGroups` Map
- Spawns 10-worker pool processing directory groups
- Per group: runs `checkCodeVsDoc()` on cached source against both `oldSumCache` (stale detection) and fresh `.sum` (LLM omission detection), runs `checkCodeVsCode()` aggregating exports per directory
- Clears `sourceContentCache` to free memory
- Builds `InconsistencyReport`, prints via `formatReportForCli()`, populates quality counters
- Phase label: `'post-phase-1-quality'` (generate), `'update-post-phase-1-quality'` (update)

### Phase 2: Directory Aggregation (Post-Order)
- Groups `plan.directoryTasks` by `metadata.depth`, processes depth levels descending (deepest first)
- Per depth level: spawns worker pool with concurrency capped to directory count at depth
- Each task: builds `knownDirs` Set from all directory task paths, calls `buildDirectoryPrompt()` with knownDirs and `plan.projectStructure`, invokes `AIService.call()`, writes `AGENTS.md` via `writeAgentsMd()`, updates `ProgressReporter` via `onDirectoryDone()`
- Phase labels: `'phase-2-dirs-depth-N'` where N is depth integer

### Post-Phase 2: Phantom Path Validation (Non-Throwing)
- Reads all `AGENTS.md` files from `plan.directoryTasks`
- Runs `checkPhantomPaths()` per directory extracting path-like strings via regex, resolving against directory and project root with `.ts`/`.js` fallback
- Aggregates issues into `InconsistencyReport`, prints, populates `RunSummary.phantomPaths`

### Phase 3: Root Document Synthesis (Sequential)
- Processes `plan.rootTasks` sequentially (concurrency=1)
- Per task: emits `task:start` trace event, calls `buildRootPrompt()` with `plan.projectRoot`, invokes `AIService.call()` with `maxTurns: 1`, strips conversational preamble via `stripPreamble()` (pattern 1: content after `\n---\n`, pattern 2: content starting with `**[A-Z]`), writes to `rootTask.outputPath`, updates `ProgressReporter` via `onRootDone()`, emits `task:done` trace event
- Phase label: `'phase-3-root'`

### Finalization
- Calls `planTracker.flush()` awaiting serialized write completion
- Retrieves `AIService.getSummary()` aggregating token counts, durations, errors, retries, file reads
- Builds `RunSummary` with quality metrics (`inconsistenciesCodeVsDoc`, `inconsistenciesCodeVsCode`, `phantomPaths`, `inconsistencyReport`)
- Calls `reporter.printSummary(summary)` printing multi-line summary with token counts, file read stats, elapsed time, error/retry counts

## Usage Patterns

**Instantiate CommandRunner:**
```typescript
const runner = new CommandRunner(aiService, {
  concurrency: 5,
  failFast: false,
  tracer: createTraceWriter(projectRoot, traceEnabled),
  progressLog: ProgressLog.create(projectRoot)
})
```

**Execute full generation:**
```typescript
const summary = await runner.executeGenerate(plan)
console.log(`Processed ${summary.filesProcessed} files, ${summary.filesFailed} failed`)
```

**Execute incremental update:**
```typescript
const summary = await runner.executeUpdate(filesToAnalyze, projectRoot, config)
console.log(`Updated ${summary.filesProcessed} files`)
```

**Monitor progress in real-time:**
```bash
tail -f .agents-reverse-engineer/progress.log
```

**Analyze trace events:**
```bash
cat .agents-reverse-engineer/traces/trace-*.ndjson | jq -r 'select(.type == "subprocess:spawn") | "\(.ts) \(.taskLabel) \(.durationMs)ms"'
```