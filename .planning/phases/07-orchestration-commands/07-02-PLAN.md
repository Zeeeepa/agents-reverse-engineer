---
phase: 07-orchestration-commands
plan: 02
type: execute
wave: 2
depends_on: ["07-01"]
files_modified:
  - src/cli/generate.ts
  - src/cli/index.ts
autonomous: true

must_haves:
  truths:
    - "Running `are generate .` spawns AI CLI subprocesses and produces .sum files without host LLM execution"
    - "Running `are generate --dry-run` lists files and estimated call count without making AI calls"
    - "Running `are generate --quiet` suppresses per-file progress, only shows final summary"
    - "Running `are generate --concurrency 3` processes at most 3 files simultaneously"
    - "Running `are generate --fail-fast` stops on first file failure"
    - "When no AI CLI is installed, `are generate` fails with an actionable error message"
  artifacts:
    - path: "src/cli/generate.ts"
      provides: "Generate command rewritten to use AIService + CommandRunner"
      exports: ["generateCommand", "GenerateOptions"]
    - path: "src/cli/index.ts"
      provides: "CLI entry point with new flags parsed"
      contains: "concurrency"
  key_links:
    - from: "src/cli/generate.ts"
      to: "src/ai/index.ts"
      via: "AIService instantiation and backend resolution"
      pattern: "resolveBackend|new AIService"
    - from: "src/cli/generate.ts"
      to: "src/orchestration/index.ts"
      via: "CommandRunner for concurrent execution"
      pattern: "CommandRunner"
    - from: "src/cli/generate.ts"
      to: "src/generation/executor.ts"
      via: "buildExecutionPlan to create task graph"
      pattern: "buildExecutionPlan"
    - from: "src/cli/index.ts"
      to: "src/cli/generate.ts"
      via: "Passing new options (concurrency, failFast, debug)"
      pattern: "concurrency|failFast|debug"
---

<objective>
Rewrite the generate command to directly execute AI analysis via the orchestration engine, and add all new CLI flags.

Purpose: This is the primary command transformation -- `are generate` goes from outputting plans for a host LLM to directly spawning AI CLI subprocesses and producing documentation. Users see streaming progress and a final summary.
Output: Rewritten `src/cli/generate.ts` using AIService + CommandRunner. Updated `src/cli/index.ts` with --concurrency, --fail-fast, --debug flags.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/07-orchestration-commands/07-CONTEXT.md
@.planning/phases/07-orchestration-commands/07-RESEARCH.md
@.planning/phases/07-orchestration-commands/07-01-SUMMARY.md

@src/cli/generate.ts
@src/cli/index.ts
@src/ai/index.ts
@src/ai/service.ts
@src/ai/types.ts
@src/orchestration/index.ts
@src/generation/executor.ts
@src/generation/orchestrator.ts
@src/config/schema.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Rewrite generate command to use AIService + CommandRunner</name>
  <files>src/cli/generate.ts</files>
  <action>
Rewrite `src/cli/generate.ts` to directly execute AI analysis instead of outputting plans.

**Update GenerateOptions interface:**
- Keep existing: `quiet`, `verbose`, `dryRun`, `budget`
- Add: `concurrency?: number`, `failFast?: boolean`, `debug?: boolean`
- Keep `execute` and `stream` as deprecated (still functional for backward compatibility)

**Rewrite generateCommand function:**

1. **Setup phase** (same as current): resolve path, load config, create logger, discover files, apply filters, create orchestrator, create generation plan

2. **Backend resolution:** Import `createBackendRegistry`, `resolveBackend` from `../ai/index.js`. Call `resolveBackend(createBackendRegistry(), config.ai.backend)`. If it throws `AIServiceError` with code `CLI_NOT_FOUND`, print the install instructions from `getInstallInstructions()` and exit with code 2.

3. **Dry-run handling:** If `options.dryRun`, build the execution plan with `buildExecutionPlan(plan, absolutePath)`, then display a dry-run summary showing: number of files to analyze, number of directories, number of root documents, estimated AI call count, and a list of file paths. Use `picocolors` for formatting. Return without making any AI calls.

4. **Backward compatibility:** If `options.execute` or `options.stream`, keep the existing JSON output behavior (the current code). Print a deprecation notice to stderr: `"Note: --execute and --stream are deprecated. The default behavior now executes analysis directly."` Then run the existing code path.

5. **Direct execution (new default):**
   - Create `AIService` instance with the resolved backend and config options (`{ timeoutMs: config.ai.timeoutMs, maxRetries: config.ai.maxRetries, telemetry: { keepRuns: config.ai.telemetry.keepRuns } }`)
   - Build execution plan: `buildExecutionPlan(plan, absolutePath)`
   - Determine concurrency: `options.concurrency ?? config.ai.concurrency`
   - Create `CommandRunner` from `../orchestration/index.js` with the AIService and `{ concurrency, failFast: options.failFast, quiet: options.quiet, debug: options.debug }`
   - Call `runner.executeGenerate(executionPlan)`
   - Call `aiService.finalize(absolutePath)` to write the telemetry run log
   - Determine exit code from RunSummary:
     - `0` if all files succeeded
     - `1` if some files failed (partial failure)
     - `2` if no files succeeded (total failure)
   - If non-zero exit code, call `process.exit(exitCode)`

6. **Debug mode:** If `options.debug` is true, before each AI call (in the runner, via a callback or by checking the option), log the exact prompt being sent. The CommandRunner already has access to the debug flag via options. In the generate command, if debug is true, also log the backend name and model being used at the start.

Remove the old "Ready for Analysis" text output and the "host LLM will process each file" messaging. The default behavior is now direct execution.

Keep the `formatPlan()` and `formatTypeDistribution()` helper functions -- they are still used for the initial plan display before execution starts.
  </action>
  <verify>
Run `npx tsc --noEmit`. The generate command should compile without errors. Verify that:
1. `GenerateOptions` has concurrency, failFast, debug fields
2. The function imports from `../ai/index.js` and `../orchestration/index.js`
3. No references to "host LLM" remain in the default code path
4. The `--execute` and `--stream` backward compatibility paths still exist
  </verify>
  <done>
`src/cli/generate.ts` rewrites the default behavior to use AIService + CommandRunner for direct execution. Backend resolution happens before execution. Dry-run shows file count and estimated calls. Exit codes follow the 0/1/2 scheme. Backward compatibility preserved for --execute and --stream.
  </done>
</task>

<task type="auto">
  <name>Task 2: Add new CLI flags to cli/index.ts</name>
  <files>src/cli/index.ts</files>
  <action>
Update `src/cli/index.ts` to parse and pass new flags:

**Update USAGE string:**
- Add under "General Options":
  - `--concurrency <n>  Number of concurrent AI calls (default: 5)`
  - `--fail-fast       Stop on first file analysis failure`
  - `--debug           Show AI prompts and backend details`
- Update the `--execute` and `--stream` entries to note they are deprecated:
  - `--execute         [deprecated] Output JSON execution plan`
  - `--stream          [deprecated] Output tasks as streaming JSON`

**Update generate command case:**
- Parse `concurrency` from `values` map (same pattern as `budget`): `concurrency: values.has('concurrency') ? parseInt(values.get('concurrency')!, 10) : undefined`
- Parse `failFast` from `flags`: `failFast: flags.has('fail-fast')`
- Parse `debug` from `flags`: `debug: flags.has('debug')`
- Pass all three to `GenerateOptions`

**Update update command case:**
- Parse the same flags: `concurrency`, `failFast`, `debug`
- Add these to the `UpdateCommandOptions` call (they will be used in Plan 03)
- For now, just parse them -- the update command will use them after Plan 03 rewrites it

**Important:** Keep all existing flag parsing unchanged. Only add the new flags.
  </action>
  <verify>
Run `npx tsc --noEmit`. Verify:
1. USAGE string includes --concurrency, --fail-fast, --debug
2. Generate case passes concurrency, failFast, debug to generateCommand
3. The parseArgs function correctly handles `--concurrency 3` (value flag) and `--fail-fast` (boolean flag)
4. No existing tests or commands are broken
  </verify>
  <done>
`src/cli/index.ts` parses --concurrency (value), --fail-fast (boolean), --debug (boolean) flags and passes them to the generate command. USAGE string documents all new flags. Deprecated flags are marked. All existing flag parsing unchanged.
  </done>
</task>

</tasks>

<verification>
1. `npx tsc --noEmit` passes with zero errors
2. The generate command imports AIService, CommandRunner, and resolveBackend
3. CLI index parses --concurrency, --fail-fast, --debug correctly
4. Backward compatibility: --execute and --stream flags still produce JSON output
5. The dry-run path works without requiring an AI CLI to be installed
</verification>

<success_criteria>
- `are generate` defaults to direct AI execution (not plan output)
- New CLI flags (--concurrency, --fail-fast, --debug, --quiet) are parsed and passed through
- Exit codes: 0 for success, 1 for partial failure, 2 for total failure
- Backward compatible: --execute and --stream still work with deprecation notice
- Clean compilation with no type errors
</success_criteria>

<output>
After completion, create `.planning/phases/07-orchestration-commands/07-02-SUMMARY.md`
</output>
