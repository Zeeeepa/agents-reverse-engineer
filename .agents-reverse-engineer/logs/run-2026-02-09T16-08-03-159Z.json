{
  "runId": "2026-02-09T16:08:03.158Z",
  "startTime": "2026-02-09T16:08:03.159Z",
  "endTime": "2026-02-09T16:23:09.649Z",
  "entries": [
    {
      "timestamp": "2026-02-09T16:08:03.243Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/discovery/filters/custom.ts\n\n```typescript\n/**\n * Custom pattern filter for file discovery.\n *\n * Allows users to specify additional exclusion patterns in their configuration.\n * Uses the `ignore` library for gitignore-style pattern matching.\n */\n\nimport ignore, { type Ignore } from 'ignore';\nimport path from 'node:path';\nimport type { FileFilter } from '../types.js';\n\n/**\n * Creates a custom filter that excludes files matching user-provided patterns.\n *\n * Patterns use gitignore syntax and are checked against relative paths from\n * the root directory. If no patterns are provided, the filter passes all files.\n *\n * @param patterns - Array of gitignore-style patterns to exclude\n * @param root - Root directory for converting absolute paths to relative\n * @returns A FileFilter that checks paths against custom patterns\n *\n * @example\n * ```typescript\n * const filter = createCustomFilter(['*.log', 'tmp/**', 'secret.txt'], '/project');\n * filter.shouldExclude('/project/debug.log'); // true\n * filter.shouldExclude('/project/src/app.ts'); // false\n * ```\n */\nexport function createCustomFilter(patterns: string[], root: string): FileFilter {\n  const ig: Ignore = ignore();\n  const normalizedRoot = path.resolve(root);\n\n  // Add all patterns to the ignore instance\n  if (patterns.length > 0) {\n    ig.add(patterns);\n  }\n\n  return {\n    name: 'custom',\n\n    shouldExclude(absolutePath: string): boolean {\n      // If no patterns, nothing to exclude\n      if (patterns.length === 0) {\n        return false;\n      }\n\n      // Convert to relative path (ignore library requires relative paths)\n      const relativePath = path.relative(normalizedRoot, absolutePath);\n\n      // If path is outside root (starts with ..) or is empty, don't exclude\n      if (!relativePath || relativePath.startsWith('..')) {\n        return false;\n      }\n\n      return ig.ignores(relativePath);\n    },\n  };\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**createCustomFilter constructs a FileFilter that excludes files matching user-provided gitignore-style patterns during discovery.**\n\n## Exports\n\n- `createCustomFilter(patterns: string[], root: string): FileFilter` — constructs filter instance with `ignore` library matcher, returns FileFilter with `name: 'custom'` and `shouldExclude` method\n\n## Implementation Details\n\ncreateCustomFilter instantiates `Ignore` object via `ignore()` factory, normalizes root via `path.resolve(root)`, adds all patterns via `ig.add(patterns)` if array non-empty. Returns FileFilter with shouldExclude implementation that converts absolute paths to relative via `path.relative(normalizedRoot, absolutePath)`, returns false for empty patterns array or paths outside root (relative path starts with `..` or is empty string), otherwise delegates to `ig.ignores(relativePath)`.\n\n## Integration Points\n\nConsumed by file discovery pipeline in `src/discovery/walker.ts` as part of composable filter chain. Patterns sourced from `config.exclude.patterns` array in `.agents-reverse-engineer/config.yaml` loaded via `src/config/loader.ts`. Works alongside gitignore filter (`src/discovery/filters/gitignore.ts`), binary filter (`src/discovery/filters/binary.ts`), and vendor filter (`src/discovery/filters/vendor.ts`) exported from `src/discovery/filters/index.ts`.\n\n## Path Handling\n\nRequires relative paths for `ignore` library compatibility. Absolute input paths resolved via normalizedRoot baseline. Edge case: paths outside root (e.g., symlinks resolving to parent directories) bypass filtering by returning false when relativePath starts with `..`. Empty relative paths also bypass exclusion.\n\n## Dependencies\n\n- `ignore` library (Ignore type) for gitignore-style pattern matching with `ig.ignores(relativePath)` predicate\n- `node:path` for `path.relative()` and `path.resolve()` conversions\n- FileFilter interface from `../types.js` defining `name: string` and `shouldExclude(absolutePath: string): boolean` contract",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 486,
      "cacheReadTokens": 16334,
      "cacheCreationTokens": 10380,
      "latencyMs": 35263,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/discovery/filters/custom.ts",
          "sizeBytes": 1847
        }
      ]
    },
    {
      "timestamp": "2026-02-09T16:08:03.269Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/discovery/filters/index.ts\n\n```typescript\n/**\n * Filter chain orchestration for file discovery.\n *\n * This module exports all filter creators and provides the applyFilters\n * function that runs files through the filter chain, recording which\n * filter excluded each file.\n */\n\nimport pc from 'picocolors';\nimport type { FileFilter, FilterResult, ExcludedFile } from '../types.js';\nimport type { ITraceWriter } from '../../orchestration/trace.js';\n\n// Re-export all filter creators\nexport { createGitignoreFilter } from './gitignore.js';\nexport { createVendorFilter, DEFAULT_VENDOR_DIRS } from './vendor.js';\nexport { createBinaryFilter, BINARY_EXTENSIONS, type BinaryFilterOptions } from './binary.js';\nexport { createCustomFilter } from './custom.js';\n\n/**\n * Applies a chain of filters to a list of files.\n *\n * Each file is run through filters in order until one excludes it\n * (short-circuit evaluation). Files are processed with bounded concurrency\n * to avoid opening too many file handles simultaneously (important for\n * binary content detection which performs file I/O).\n *\n * @param files - Array of absolute file paths to filter\n * @param filters - Array of filters to apply in order\n * @param options - Optional tracing and debug options\n * @returns Promise resolving to FilterResult with included and excluded lists\n *\n * @example\n * ```typescript\n * const filters = [\n *   createVendorFilter(['node_modules']),\n *   createBinaryFilter({}),\n * ];\n * const result = await applyFilters(['/a/b.js', '/a/node_modules/c.js'], filters);\n * // result.included: ['/a/b.js']\n * // result.excluded: [{ path: '/a/node_modules/c.js', filter: 'vendor', reason: '...' }]\n * ```\n */\nexport async function applyFilters(\n  files: string[],\n  filters: FileFilter[],\n  options?: { tracer?: ITraceWriter; debug?: boolean }\n): Promise<FilterResult> {\n  const included: string[] = [];\n  const excluded: ExcludedFile[] = [];\n\n  // Track exclusions per filter for trace events\n  const filterStats = new Map<string, { matched: number; rejected: number }>();\n  for (const filter of filters) {\n    filterStats.set(filter.name, { matched: 0, rejected: 0 });\n  }\n\n  // Process files with bounded concurrency to avoid exhausting file descriptors.\n  // Binary filter calls isBinaryFile() which does file I/O.\n  const CONCURRENCY = 30;\n  const iterator = files.entries();\n\n  async function worker(\n    iter: IterableIterator<[number, string]>,\n  ): Promise<Array<{ index: number; file: string; excluded?: ExcludedFile }>> {\n    const results: Array<{ index: number; file: string; excluded?: ExcludedFile }> = [];\n    for (const [index, file] of iter) {\n      let wasExcluded = false;\n\n      // Run through filters in order, stop at first exclusion\n      for (const filter of filters) {\n        const shouldExclude = await filter.shouldExclude(file);\n\n        if (shouldExclude) {\n          results.push({\n            index,\n            file,\n            excluded: {\n              path: file,\n              reason: `Excluded by ${filter.name} filter`,\n              filter: filter.name,\n            },\n          });\n          wasExcluded = true;\n          break; // Short-circuit: stop checking other filters\n        }\n      }\n\n      if (!wasExcluded) {\n        results.push({ index, file });\n      }\n    }\n    return results;\n  }\n\n  const effectiveConcurrency = Math.min(CONCURRENCY, files.length);\n  const workers = Array.from({ length: effectiveConcurrency }, () =>\n    worker(iterator),\n  );\n\n  const allResults = (await Promise.all(workers)).flat();\n\n  // Sort by original index to preserve order\n  allResults.sort((a, b) => a.index - b.index);\n\n  // Collect results and update filter stats\n  for (const result of allResults) {\n    if (result.excluded) {\n      excluded.push(result.excluded);\n      const stats = filterStats.get(result.excluded.filter);\n      if (stats) {\n        stats.rejected++;\n      }\n    } else {\n      included.push(result.file);\n      // All filters \"matched\" (passed through) this file\n      for (const filter of filters) {\n        const stats = filterStats.get(filter.name);\n        if (stats) {\n          stats.matched++;\n        }\n      }\n    }\n  }\n\n  // Emit trace events for each filter\n  for (const filter of filters) {\n    const stats = filterStats.get(filter.name);\n    if (stats) {\n      options?.tracer?.emit({\n        type: 'filter:applied',\n        filterName: filter.name,\n        filesMatched: stats.matched,\n        filesRejected: stats.rejected,\n      });\n\n      // Debug output\n      if (options?.debug && stats.rejected > 0) {\n        console.error(pc.dim(`[debug] Filter [${filter.name}]: ${stats.rejected} files rejected`));\n      }\n    }\n  }\n\n  return { included, excluded };\n}\n\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**Exports filter chain orchestration via `applyFilters()` function that runs files through sequential `FileFilter` predicates with bounded concurrency, records exclusion metadata per filter, and re-exports all filter creators.**\n\n## Exported Functions\n\n**`applyFilters(files: string[], filters: FileFilter[], options?: { tracer?: ITraceWriter; debug?: boolean }): Promise<FilterResult>`**\n\nApplies filter chain to file list with short-circuit evaluation (stops at first exclusion). Spawns 30 concurrent workers sharing single iterator to bound file descriptor usage (critical for binary detection I/O via `isBinaryFile()`). Each worker processes files serially through filter array, breaks on first `shouldExclude()` true return. Collects results as `Array<{ index: number; file: string; excluded?: ExcludedFile }>`, sorts by original index to preserve input order. Populates `filterStats` map tracking per-filter `matched`/`rejected` counts, emits `filter:applied` trace events with `filterName`, `filesMatched`, `filesRejected` fields. Debug mode logs rejected counts via `console.error(pc.dim(...))` pattern. Returns `FilterResult` with `included: string[]` and `excluded: ExcludedFile[]` where `ExcludedFile` contains `path`, `filter`, `reason: 'Excluded by {filter.name} filter'`.\n\n## Re-Exported Filter Creators\n\n- `createGitignoreFilter` from `./gitignore.js`\n- `createVendorFilter`, `DEFAULT_VENDOR_DIRS` from `./vendor.js`\n- `createBinaryFilter`, `BINARY_EXTENSIONS`, `BinaryFilterOptions` from `./binary.js`\n- `createCustomFilter` from `./custom.js`\n\n## Key Dependencies\n\n- `picocolors` (`pc`) for debug logging color formatting\n- `FileFilter`, `FilterResult`, `ExcludedFile` types from `../types.js`\n- `ITraceWriter` interface from `../../orchestration/trace.js`\n\n## Concurrency Strategy\n\nFixed concurrency limit of 30 workers sharing iterator via `files.entries()` iterator pattern. Worker count capped at `Math.min(CONCURRENCY, files.length)` to avoid spawning unnecessary workers for small file sets. Workers drain shared iterator via `for...of` loop, no work-stealing or queue coordination required beyond iterator protocol.\n\n## Filter Statistics\n\nTracks per-filter statistics in `Map<string, { matched: number; rejected: number }>` where:\n- `rejected` increments when filter excludes file\n- `matched` increments for all filters when file passes entire chain (not when individual filter passes)\n\nStatistics emitted as `filter:applied` trace events after all workers complete, enabling downstream analysis of filter effectiveness.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 655,
      "cacheReadTokens": 16334,
      "cacheCreationTokens": 11223,
      "latencyMs": 36572,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/discovery/filters/index.ts",
          "sizeBytes": 4669
        }
      ]
    },
    {
      "timestamp": "2026-02-09T16:08:03.233Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/ai/telemetry/run-log.ts\n\n```typescript\n/**\n * Run log writer for AI telemetry.\n *\n * Writes a completed {@link RunLog} to disk as pretty-printed JSON.\n * Each run produces a single file in `.agents-reverse-engineer/logs/`\n * with a filename derived from the run's start timestamp.\n *\n * @module\n */\n\nimport * as fs from 'node:fs/promises';\nimport * as path from 'node:path';\nimport type { RunLog } from '../types.js';\n\n/** Directory name for telemetry log files (relative to project root) */\nconst LOGS_DIR = '.agents-reverse-engineer/logs';\n\n/**\n * Write a completed run log to disk as pretty-printed JSON.\n *\n * Creates the logs directory if it does not exist. The filename is derived\n * from the run's `startTime` field with `:` and `.` replaced by `-` so\n * that it forms a valid filename on all platforms.\n *\n * @param projectRoot - Absolute path to the project root directory\n * @param runLog - The completed run log to write\n * @returns Absolute path to the written file\n *\n * @example\n * ```typescript\n * const logPath = await writeRunLog('/home/user/project', runLog);\n * // logPath: /home/user/project/.agents-reverse-engineer/logs/run-2026-02-07T12-00-00-000Z.json\n * ```\n */\nexport async function writeRunLog(projectRoot: string, runLog: RunLog): Promise<string> {\n  const logsDir = path.join(projectRoot, LOGS_DIR);\n\n  // Create the logs directory if it does not exist\n  await fs.mkdir(logsDir, { recursive: true });\n\n  // Build filename: replace : and . with - so ISO timestamps become valid filenames\n  const safeTimestamp = runLog.startTime.replace(/[:.]/g, '-');\n  const filename = `run-${safeTimestamp}.json`;\n  const filePath = path.join(logsDir, filename);\n\n  // Write pretty-printed JSON\n  await fs.writeFile(filePath, JSON.stringify(runLog, null, 2), 'utf-8');\n\n  return filePath;\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**Writes completed `RunLog` objects to disk as timestamped JSON files in `.agents-reverse-engineer/logs/` with directory creation and filename sanitization.**\n\n## Exported Functions\n\n### writeRunLog\n\n```typescript\nasync function writeRunLog(projectRoot: string, runLog: RunLog): Promise<string>\n```\n\nWrites a completed `RunLog` to disk as pretty-printed JSON (2-space indentation). Creates `<projectRoot>/.agents-reverse-engineer/logs/` directory via `fs.mkdir()` with `recursive: true` if absent. Derives filename from `runLog.startTime` by replacing `:` and `.` characters with `-` to produce cross-platform-safe filenames, yielding pattern `run-<ISO8601-sanitized>.json` (e.g., `run-2026-02-07T12-00-00-000Z.json`). Returns absolute path to written file.\n\n## Constants\n\n### LOGS_DIR\n\n```typescript\nconst LOGS_DIR = '.agents-reverse-engineer/logs'\n```\n\nRelative directory path for telemetry log storage. Combined with `projectRoot` parameter to form absolute log destination via `path.join()`.\n\n## Dependencies\n\n- `node:fs/promises`: `mkdir()`, `writeFile()` for async filesystem operations\n- `node:path`: `join()` for cross-platform path construction\n- `RunLog` type from `../types.js`: Structured run metadata with `startTime` (ISO 8601 string), token counts, costs, durations, errors, file read metadata\n\n## Integration Points\n\nCalled by `TelemetryLogger.finalize()` in `src/ai/telemetry/logger.ts` after aggregating per-call metrics into cumulative `RunLog` structure. Works in tandem with `cleanupOldLogs()` from `src/ai/telemetry/cleanup.ts` for retention management (keeps N most recent logs based on `config.ai.telemetry.keepRuns`).\n\n## Filename Sanitization Pattern\n\nApplies `runLog.startTime.replace(/[:.]/g, '-')` to transform ISO 8601 timestamps like `2026-02-07T12:00:00.000Z` into filesystem-safe `2026-02-07T12-00-00-000Z`, avoiding Windows path restrictions and shell escaping issues.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 548,
      "cacheReadTokens": 17114,
      "cacheCreationTokens": 9645,
      "latencyMs": 37321,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/ai/telemetry/run-log.ts",
          "sizeBytes": 1766
        }
      ]
    },
    {
      "timestamp": "2026-02-09T16:08:03.256Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/discovery/filters/vendor.ts\n\n```typescript\n/**\n * Vendor directory filter for file discovery.\n *\n * Excludes files within common vendor/dependency directories that typically\n * contain third-party code not relevant for documentation purposes.\n */\n\nimport path from 'node:path';\nimport type { FileFilter } from '../types.js';\n\n/**\n * Default vendor directories to exclude.\n * These are common directories containing third-party code, build output,\n * or generated files that should not be analyzed.\n */\nexport const DEFAULT_VENDOR_DIRS = [\n  'node_modules',\n  'vendor',\n  '.git',\n  'dist',\n  'build',\n  '__pycache__',\n  '.next',\n  'venv',\n  '.venv',\n  'target',\n] as const;\n\n/**\n * Creates a vendor filter that excludes files within specified directories.\n *\n * Supports two patterns:\n * - Single directory names (e.g., 'node_modules') - matches anywhere in path\n * - Path patterns (e.g., 'apps/vendor' or '.agents/skills') - matches path containing this sequence\n *\n * @param vendorDirs - Array of directory names or path patterns to exclude.\n * @returns A FileFilter that checks if a path is within a vendor directory\n *\n * @example\n * ```typescript\n * const filter = createVendorFilter(['node_modules', '.agents/skills']);\n * filter.shouldExclude('/project/node_modules/lodash/index.js'); // true\n * filter.shouldExclude('/project/apps/foo/.agents/skills/bar.md'); // true\n * filter.shouldExclude('/project/src/utils.js'); // false\n * ```\n */\nexport function createVendorFilter(vendorDirs: string[]): FileFilter {\n  // Separate single segments from path patterns\n  const singleSegments = new Set<string>();\n  const pathPatterns: string[] = [];\n\n  for (const dir of vendorDirs) {\n    // Normalize path separators to current OS\n    const normalized = dir.replace(/[\\\\/]/g, path.sep);\n    if (normalized.includes(path.sep)) {\n      pathPatterns.push(normalized);\n    } else {\n      singleSegments.add(dir);\n    }\n  }\n\n  return {\n    name: 'vendor',\n\n    shouldExclude(absolutePath: string): boolean {\n      // Check single segment matches\n      const segments = absolutePath.split(path.sep);\n      for (const segment of segments) {\n        if (singleSegments.has(segment)) {\n          return true;\n        }\n      }\n\n      // Check path pattern matches\n      for (const pattern of pathPatterns) {\n        if (absolutePath.includes(pattern)) {\n          return true;\n        }\n      }\n\n      return false;\n    },\n  };\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**createVendorFilter() constructs a FileFilter that excludes files within third-party dependency directories and build output paths using path segment matching and path pattern substring matching.**\n\n## Exported Symbols\n\n### DEFAULT_VENDOR_DIRS\n```typescript\nconst DEFAULT_VENDOR_DIRS: readonly [\n  'node_modules', 'vendor', '.git', 'dist', 'build',\n  '__pycache__', '.next', 'venv', '.venv', 'target'\n]\n```\nArray of common vendor directory names: `node_modules`, `vendor`, `.git`, `dist`, `build`, `__pycache__`, `.next`, `venv`, `.venv`, `target`. Used as default exclusion list for third-party code, build artifacts, Python cache, Next.js build output, Python virtual environments, and Rust/Java build directories.\n\n### createVendorFilter\n```typescript\nfunction createVendorFilter(vendorDirs: string[]): FileFilter\n```\nReturns a `FileFilter` instance that excludes paths containing any directory from `vendorDirs`. Implements two matching strategies: (1) single-segment matching where directory names like `'node_modules'` match any path segment (e.g., `/project/node_modules/lodash/index.js`), (2) path-pattern matching where patterns like `'.agents/skills'` or `'apps/vendor'` match via substring search after normalizing separators to `path.sep`. The returned filter has `name: 'vendor'` and `shouldExclude(absolutePath: string): boolean` method.\n\n## Pattern Matching Algorithm\n\nThe `shouldExclude()` method splits `absolutePath` by `path.sep`, iterates segments checking membership in `singleSegments` Set (O(1) lookup), then iterates `pathPatterns` array checking `absolutePath.includes(pattern)` for substring match. Returns `true` on first match, `false` if no matches found.\n\nPath patterns undergo normalization via `dir.replace(/[\\\\/]/g, path.sep)` to convert both forward and backslashes to OS-specific separator before classification. Classification uses `normalized.includes(path.sep)` to distinguish path patterns (containing separators) from single segments (no separators).\n\n## Integration with Discovery Pipeline\n\nImplements `FileFilter` interface from `../types.js` with `name: string` and `shouldExclude(absolutePath: string): boolean` signature. Composed with other filters (gitignore, binary, custom) in discovery filter chain orchestrated by `src/discovery/walker.ts`.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 595,
      "cacheReadTokens": 17114,
      "cacheCreationTokens": 9793,
      "latencyMs": 37677,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/discovery/filters/vendor.ts",
          "sizeBytes": 2375
        }
      ]
    },
    {
      "timestamp": "2026-02-09T16:08:03.222Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/ai/telemetry/cleanup.ts\n\n```typescript\n/**\n * Telemetry log cleanup utility.\n *\n * Removes old run log files from `.agents-reverse-engineer/logs/`,\n * keeping only the N most recent. Files are sorted lexicographically\n * by name, which works correctly because filenames contain ISO timestamps.\n *\n * @module\n */\n\nimport * as fs from 'node:fs/promises';\nimport * as path from 'node:path';\n\n/** Directory name for telemetry log files (relative to project root) */\nconst LOGS_DIR = '.agents-reverse-engineer/logs';\n\n/**\n * Remove old telemetry log files, keeping only the most recent ones.\n *\n * Reads the logs directory, filters for files matching `run-*.json`,\n * sorts newest-first (lexicographic sort on ISO timestamp filenames),\n * and deletes everything beyond `keepCount`.\n *\n * If the logs directory does not exist, returns 0 without error.\n *\n * @param projectRoot - Absolute path to the project root directory\n * @param keepCount - Number of most recent log files to retain\n * @returns Number of files deleted\n *\n * @example\n * ```typescript\n * const deleted = await cleanupOldLogs('/home/user/project', 10);\n * console.log(`Cleaned up ${deleted} old log files`);\n * ```\n */\nexport async function cleanupOldLogs(projectRoot: string, keepCount: number): Promise<number> {\n  const logsDir = path.join(projectRoot, LOGS_DIR);\n\n  let entries: string[];\n  try {\n    const allEntries = await fs.readdir(logsDir);\n    entries = allEntries.filter(\n      (name) => name.startsWith('run-') && name.endsWith('.json'),\n    );\n  } catch (error) {\n    // Directory doesn't exist -- nothing to clean up\n    if ((error as NodeJS.ErrnoException).code === 'ENOENT') {\n      return 0;\n    }\n    throw error;\n  }\n\n  // Sort lexicographically (newest first) and find files to delete\n  entries.sort();\n  entries.reverse();\n\n  const toDelete = entries.slice(keepCount);\n\n  for (const filename of toDelete) {\n    await fs.unlink(path.join(logsDir, filename));\n  }\n\n  return toDelete.length;\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**cleanupOldLogs() deletes old run log JSON files from `.agents-reverse-engineer/logs/`, retaining only the N most recent files sorted by ISO timestamp filename.**\n\n## Exported Functions\n\n### cleanupOldLogs\n\n```typescript\nasync function cleanupOldLogs(projectRoot: string, keepCount: number): Promise<number>\n```\n\nRemoves old telemetry log files exceeding `keepCount` retention limit. Reads `path.join(projectRoot, '.agents-reverse-engineer/logs')`, filters entries matching pattern `run-*.json` via `name.startsWith('run-') && name.endsWith('.json')`, sorts lexicographically via `entries.sort()` then reverses to newest-first ordering, slices beyond `keepCount` into `toDelete[]`, and unlinks each via `fs.unlink()`. Returns count of deleted files. Returns `0` without error if logs directory does not exist (catches `ENOENT` errno).\n\n## Constants\n\n- `LOGS_DIR`: String literal `'.agents-reverse-engineer/logs'` defining telemetry log directory relative to project root.\n\n## Error Handling\n\nENOENT errors from `fs.readdir()` are caught and return `0` (graceful handling when logs directory missing). All other errors propagate to caller.\n\n## Integration Points\n\nCalled by `TelemetryLogger.finalize()` in `src/ai/telemetry/logger.ts` after writing run log JSON, using `config.ai.telemetry.keepRuns` as retention limit (default 50). Complements trace cleanup in `src/orchestration/trace.ts` which retains 500 NDJSON trace files.\n\n## Sorting Strategy\n\nLexicographic sort on ISO 8601 timestamp filenames (format `run-<timestamp>.json`) produces correct chronological ordering without parsing timestamps. Reversal via `entries.reverse()` yields newest-first sequence, enabling slice-based retention via `entries.slice(keepCount)`.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 463,
      "cacheReadTokens": 17114,
      "cacheCreationTokens": 9687,
      "latencyMs": 42082,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/ai/telemetry/cleanup.ts",
          "sizeBytes": 1937
        }
      ]
    },
    {
      "timestamp": "2026-02-09T16:08:03.213Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/ai/backends/gemini.ts\n\n```typescript\n/**\n * Gemini CLI backend stub.\n *\n * Implements the {@link AIBackend} interface for the Gemini CLI (`gemini`).\n * This is a stub that demonstrates the extension pattern -- `parseResponse`\n * throws \"not implemented\". Full implementation deferred to a future phase\n * once the Gemini CLI JSON output is stable (see RESEARCH.md Open Question 2).\n *\n * @module\n */\n\nimport type { AIBackend, AICallOptions, AIResponse } from '../types.js';\nimport { AIServiceError } from '../types.js';\nimport { isCommandOnPath } from './claude.js';\n\n/**\n * Gemini CLI backend stub.\n *\n * Detects CLI availability and builds argument arrays, but throws when\n * `parseResponse` is called since the Gemini adapter is not yet implemented.\n *\n * @example\n * ```typescript\n * const backend = new GeminiBackend();\n * console.log(await backend.isAvailable()); // true if `gemini` is on PATH\n * backend.parseResponse('{}', 0, 0);        // throws AIServiceError\n * ```\n */\nexport class GeminiBackend implements AIBackend {\n  readonly name = 'gemini';\n  readonly cliCommand = 'gemini';\n\n  /**\n   * Check if the `gemini` CLI is available on PATH.\n   */\n  async isAvailable(): Promise<boolean> {\n    return isCommandOnPath(this.cliCommand);\n  }\n\n  /**\n   * Build CLI arguments for a Gemini invocation.\n   *\n   * Based on documented Gemini CLI flags from RESEARCH.md.\n   * The prompt goes to stdin via the subprocess wrapper.\n   *\n   * @param _options - Call options (unused in stub)\n   * @returns Argument array for the Gemini CLI\n   */\n  buildArgs(_options: AICallOptions): string[] {\n    return ['-p', '--output-format', 'json'];\n  }\n\n  /**\n   * Parse Gemini CLI output into a normalized {@link AIResponse}.\n   *\n   * @throws {AIServiceError} Always -- Gemini backend is not yet implemented\n   */\n  parseResponse(_stdout: string, _durationMs: number, _exitCode: number): AIResponse {\n    throw new AIServiceError(\n      'SUBPROCESS_ERROR',\n      'Gemini backend is not yet implemented. Use Claude backend.',\n    );\n  }\n\n  /**\n   * Get user-facing install instructions for the Gemini CLI.\n   */\n  getInstallInstructions(): string {\n    return [\n      'Gemini CLI (experimental):',\n      '  npm install -g @anthropic-ai/gemini-cli',\n      '  https://github.com/google-gemini/gemini-cli',\n    ].join('\\n');\n  }\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**GeminiBackend implements AIBackend interface as a stub demonstrating backend extension pattern, returning build configuration for Gemini CLI but throwing SUBPROCESS_ERROR in parseResponse until Gemini JSON output format stabilizes.**\n\n## Exported Interface\n\n**GeminiBackend** — Stub class implementing AIBackend interface with:\n- `name: 'gemini'` — Backend identifier constant\n- `cliCommand: 'gemini'` — CLI executable name\n- `isAvailable(): Promise<boolean>` — Delegates to `isCommandOnPath(this.cliCommand)` to check PATH availability\n- `buildArgs(_options: AICallOptions): string[]` — Returns `['-p', '--output-format', 'json']` argument array for CLI invocation\n- `parseResponse(_stdout: string, _durationMs: number, _exitCode: number): AIResponse` — Throws `AIServiceError` with code `'SUBPROCESS_ERROR'` and message `'Gemini backend is not yet implemented. Use Claude backend.'`\n- `getInstallInstructions(): string` — Returns multiline instructions referencing `npm install -g @anthropic-ai/gemini-cli` and GitHub URL `https://github.com/google-gemini/gemini-cli`\n\n## Dependencies\n\nImports `AIBackend`, `AICallOptions`, `AIResponse`, `AIServiceError` from `'../types.js'`. Imports `isCommandOnPath` utility from `'./claude.js'` for PATH detection.\n\n## Implementation Status\n\nBackend deferred to future phase pending Gemini CLI JSON stability (references `RESEARCH.md Open Question 2` in module docstring). All methods except `parseResponse` provide operational implementations. Prompt delivery occurs via stdin through subprocess wrapper (noted in `buildArgs` docstring).\n\n## Integration Pattern\n\nFollows same AIBackend contract as ClaudeBackend and OpenCodeBackend, enabling registry-based detection via `AIBackendRegistry.detectAvailableBackend()` in `src/ai/registry.ts`. Backend registered in registry's `backends` array for automatic `--backend auto` resolution.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 487,
      "cacheReadTokens": 17114,
      "cacheCreationTokens": 9795,
      "latencyMs": 42186,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/ai/backends/gemini.ts",
          "sizeBytes": 2281
        }
      ]
    },
    {
      "timestamp": "2026-02-09T16:08:03.227Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/ai/telemetry/logger.ts\n\n```typescript\n/**\n * In-memory telemetry logger for AI service calls.\n *\n * Accumulates {@link TelemetryEntry} instances during a run and computes\n * aggregate summaries. The logger is created once per CLI invocation and\n * finalized when the run completes.\n *\n * @module\n */\n\nimport type { TelemetryEntry, RunLog, FileRead } from '../types.js';\n\n/**\n * Accumulates per-call telemetry entries in memory and produces a\n * complete {@link RunLog} when the run finishes.\n *\n * @example\n * ```typescript\n * const logger = new TelemetryLogger('2026-02-07T12:00:00.000Z');\n * logger.addEntry(entry);\n * const summary = logger.getSummary();\n * const runLog = logger.toRunLog();\n * ```\n */\nexport class TelemetryLogger {\n  /** Unique identifier for this run (ISO timestamp-based) */\n  readonly runId: string;\n\n  /** ISO 8601 timestamp when the run started */\n  readonly startTime: string;\n\n  /** Accumulated telemetry entries */\n  private readonly entries: TelemetryEntry[] = [];\n\n  /**\n   * Create a new telemetry logger for a run.\n   *\n   * @param runId - Unique run identifier (typically an ISO timestamp)\n   */\n  constructor(runId: string) {\n    this.runId = runId;\n    this.startTime = new Date().toISOString();\n  }\n\n  /**\n   * Record a telemetry entry for a completed AI call.\n   *\n   * @param entry - The telemetry entry to record\n   */\n  addEntry(entry: TelemetryEntry): void {\n    this.entries.push(entry);\n  }\n\n  /**\n   * Get all recorded entries as a read-only array.\n   *\n   * @returns Immutable view of the accumulated entries\n   */\n  getEntries(): readonly TelemetryEntry[] {\n    return this.entries;\n  }\n\n  /**\n   * Update the most recent entry's filesRead array.\n   *\n   * Called by the AI service after the command runner attaches file\n   * metadata to the last call.\n   *\n   * @param filesRead - Array of file-read records to attach\n   */\n  setFilesReadOnLastEntry(filesRead: FileRead[]): void {\n    if (this.entries.length === 0) return;\n    this.entries[this.entries.length - 1]!.filesRead = filesRead;\n  }\n\n  /**\n   * Compute aggregate summary statistics from all recorded entries.\n   *\n   * Totals are computed on every call (not cached) so the summary\n   * always reflects the current state of the entries array.\n   *\n   * @returns Summary with totals for calls, tokens, duration, and errors\n   */\n  getSummary(): RunLog['summary'] {\n    let totalInputTokens = 0;\n    let totalOutputTokens = 0;\n    let totalCacheReadTokens = 0;\n    let totalCacheCreationTokens = 0;\n    let totalDurationMs = 0;\n    let errorCount = 0;\n    let totalFilesRead = 0;\n    const uniqueFilePaths = new Set<string>();\n\n    for (const entry of this.entries) {\n      totalInputTokens += entry.inputTokens;\n      totalOutputTokens += entry.outputTokens;\n      totalCacheReadTokens += entry.cacheReadTokens;\n      totalCacheCreationTokens += entry.cacheCreationTokens;\n      totalDurationMs += entry.latencyMs;\n      if (entry.error !== undefined) {\n        errorCount++;\n      }\n      totalFilesRead += entry.filesRead.length;\n      for (const file of entry.filesRead) {\n        uniqueFilePaths.add(file.path);\n      }\n    }\n\n    return {\n      totalCalls: this.entries.length,\n      totalInputTokens,\n      totalOutputTokens,\n      totalCacheReadTokens,\n      totalCacheCreationTokens,\n      totalDurationMs,\n      errorCount,\n      totalFilesRead,\n      uniqueFilesRead: uniqueFilePaths.size,\n    };\n  }\n\n  /**\n   * Assemble the complete {@link RunLog} for this run.\n   *\n   * Sets `endTime` to the current time, includes all entries, and\n   * computes the summary. Call this once when the run is finished.\n   *\n   * @returns Complete run log ready for serialization\n   */\n  toRunLog(): RunLog {\n    return {\n      runId: this.runId,\n      startTime: this.startTime,\n      endTime: new Date().toISOString(),\n      entries: [...this.entries],\n      summary: this.getSummary(),\n    };\n  }\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**TelemetryLogger accumulates per-call AI service telemetry entries in memory during CLI run execution and computes aggregate summary statistics for serialization to RunLog.**\n\n## Exported Class\n\n`TelemetryLogger` — in-memory telemetry accumulator created once per CLI invocation.\n\n**Constructor:**\n```typescript\nconstructor(runId: string)\n```\nAccepts `runId` (typically ISO timestamp string), captures `startTime` as `new Date().toISOString()`, initializes empty `entries: TelemetryEntry[]` array.\n\n**Read-only Properties:**\n- `runId: string` — unique identifier for this run\n- `startTime: string` — ISO 8601 timestamp when run started\n\n## Core Methods\n\n`addEntry(entry: TelemetryEntry): void` — appends telemetry entry to in-memory `entries` array. Called by AI service after each subprocess completion.\n\n`getEntries(): readonly TelemetryEntry[]` — returns immutable view of accumulated entries.\n\n`setFilesReadOnLastEntry(filesRead: FileRead[]): void` — mutates `filesRead` property of most recent entry. Called by AI service after command runner attaches file metadata. No-op if `entries.length === 0`.\n\n## Summary Computation\n\n`getSummary(): RunLog['summary']` — computes aggregate statistics from all entries on every invocation (not cached). Iterates `entries` array to sum:\n- `totalInputTokens`, `totalOutputTokens`, `totalCacheReadTokens`, `totalCacheCreationTokens` (token counts from `TelemetryEntry` fields)\n- `totalDurationMs` (sum of `latencyMs` values)\n- `errorCount` (count of entries where `error !== undefined`)\n- `totalFilesRead` (sum of `filesRead.length` across entries)\n- `uniqueFilesRead` (size of `Set<string>` populated with `file.path` from all `filesRead` arrays)\n\nReturns object with `totalCalls: this.entries.length` plus computed totals.\n\n## Run Finalization\n\n`toRunLog(): RunLog` — assembles complete `RunLog` structure for serialization. Sets `endTime` to `new Date().toISOString()`, spreads `entries` array, invokes `getSummary()`. Called once when CLI run completes before writing to `.agents-reverse-engineer/logs/run-<timestamp>.json`.\n\n## Type Dependencies\n\nImports `TelemetryEntry`, `RunLog`, `FileRead` from `../types.js`. `TelemetryEntry` contains per-call metrics: `inputTokens`, `outputTokens`, `cacheReadTokens`, `cacheCreationTokens`, `latencyMs`, `error?: string`, `filesRead: FileRead[]`. `RunLog` structure includes `runId`, `startTime`, `endTime`, `entries`, `summary`. `FileRead` contains `path`, `sizeBytes`, `linesRead` metadata.\n\n## Integration Point\n\nTelemetryLogger instance passed to AIService via constructor, threaded through `CommandRunOptions.telemetryLogger`. AIService calls `addEntry()` after each subprocess completion, `setFilesReadOnLastEntry()` after file metadata attachment, `toRunLog()` before serialization via `src/ai/telemetry/run-log.ts` writer.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 785,
      "cacheReadTokens": 17114,
      "cacheCreationTokens": 10299,
      "latencyMs": 42488,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/ai/telemetry/logger.ts",
          "sizeBytes": 3866
        }
      ]
    },
    {
      "timestamp": "2026-02-09T16:08:03.250Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/discovery/filters/gitignore.ts\n\n```typescript\n/**\n * Gitignore pattern filter for file discovery.\n *\n * Uses the `ignore` library to parse and match .gitignore patterns.\n * This filter loads the root .gitignore file and checks paths against\n * the patterns to determine exclusion.\n */\n\nimport ignore, { type Ignore } from 'ignore';\nimport fs from 'node:fs/promises';\nimport path from 'node:path';\nimport type { FileFilter } from '../types.js';\n\n/**\n * Creates a gitignore filter that excludes files matching .gitignore patterns.\n *\n * @param root - The root directory containing the .gitignore file\n * @returns A FileFilter that checks paths against gitignore patterns\n *\n * @example\n * ```typescript\n * const filter = await createGitignoreFilter('/path/to/project');\n * if (filter.shouldExclude('/path/to/project/dist/bundle.js')) {\n *   console.log('File is gitignored');\n * }\n * ```\n */\nexport async function createGitignoreFilter(root: string): Promise<FileFilter> {\n  const ig: Ignore = ignore();\n  const normalizedRoot = path.resolve(root);\n\n  // Load .gitignore from root if it exists\n  const gitignorePath = path.join(normalizedRoot, '.gitignore');\n  try {\n    const content = await fs.readFile(gitignorePath, 'utf-8');\n    ig.add(content);\n  } catch {\n    // No .gitignore file, filter will pass everything through\n  }\n\n  return {\n    name: 'gitignore',\n\n    shouldExclude(absolutePath: string): boolean {\n      // Convert to relative path (ignore library requires relative paths)\n      const relativePath = path.relative(normalizedRoot, absolutePath);\n\n      // If path is outside root (starts with ..) or is empty, don't exclude\n      if (!relativePath || relativePath.startsWith('..')) {\n        return false;\n      }\n\n      // CRITICAL: The ignore library treats paths differently based on trailing slash\n      // We check the path as-is (for files) - directory handling would need trailing slash\n      // Since our walker returns files only, we don't append slash here\n      return ig.ignores(relativePath);\n    },\n  };\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**createGitignoreFilter produces FileFilter instances that exclude paths matching .gitignore patterns via the `ignore` library.**\n\n## Exported Function\n\n### createGitignoreFilter\n```typescript\nasync function createGitignoreFilter(root: string): Promise<FileFilter>\n```\nReads `.gitignore` from `root` directory, parses patterns via `ignore()` library, returns `FileFilter` with `shouldExclude()` method. Missing `.gitignore` file caught silently (filter passes all paths). Normalizes `root` to absolute path via `path.resolve()`.\n\n## FileFilter Implementation\n\nReturned object implements `FileFilter` interface with:\n- `name: 'gitignore'` — Filter identifier for debugging/logging\n- `shouldExclude(absolutePath: string): boolean` — Path exclusion predicate\n\n## Path Handling\n\n`shouldExclude()` converts absolute paths to relative via `path.relative(normalizedRoot, absolutePath)`:\n- Returns `false` for paths outside root (starting with `..`)\n- Returns `false` for empty relative paths\n- Delegates to `ig.ignores(relativePath)` for pattern matching\n\n**Critical constraint**: `ignore` library requires relative paths without leading slash. Trailing slash semantics apply to directories; this implementation omits trailing slash since file walker returns files only.\n\n## Dependencies\n\n- `ignore` library (`Ignore` type) — Parses and matches .gitignore patterns\n- `node:fs/promises` — Async `.gitignore` file reading via `readFile()`\n- `node:path` — Path normalization and relative path computation\n- `../types.js` — `FileFilter` interface definition\n\n## Integration Points\n\nUsed by discovery pipeline (`src/discovery/walker.ts`) as composable filter in filter chain alongside binary filter, vendor filter, custom pattern filter. Consumed during Phase 1 file analysis to respect repository gitignore rules.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 443,
      "cacheReadTokens": 0,
      "cacheCreationTokens": 26778,
      "latencyMs": 42951,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/discovery/filters/gitignore.ts",
          "sizeBytes": 1990
        }
      ]
    },
    {
      "timestamp": "2026-02-09T16:08:03.217Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/ai/backends/opencode.ts\n\n```typescript\n/**\n * OpenCode CLI backend stub.\n *\n * Implements the {@link AIBackend} interface for the OpenCode CLI (`opencode`).\n * This is a stub that demonstrates the extension pattern -- `parseResponse`\n * throws \"not implemented\". Full implementation deferred to a future phase\n * once the OpenCode JSONL output parsing is built (see RESEARCH.md Open Question 3).\n *\n * @module\n */\n\nimport type { AIBackend, AICallOptions, AIResponse } from '../types.js';\nimport { AIServiceError } from '../types.js';\nimport { isCommandOnPath } from './claude.js';\n\n/**\n * OpenCode CLI backend stub.\n *\n * Detects CLI availability and builds argument arrays, but throws when\n * `parseResponse` is called since the OpenCode adapter is not yet implemented.\n *\n * @example\n * ```typescript\n * const backend = new OpenCodeBackend();\n * console.log(await backend.isAvailable()); // true if `opencode` is on PATH\n * backend.parseResponse('{}', 0, 0);        // throws AIServiceError\n * ```\n */\nexport class OpenCodeBackend implements AIBackend {\n  readonly name = 'opencode';\n  readonly cliCommand = 'opencode';\n\n  /**\n   * Check if the `opencode` CLI is available on PATH.\n   */\n  async isAvailable(): Promise<boolean> {\n    return isCommandOnPath(this.cliCommand);\n  }\n\n  /**\n   * Build CLI arguments for an OpenCode invocation.\n   *\n   * Based on documented OpenCode CLI flags from RESEARCH.md.\n   * The prompt goes to stdin via the subprocess wrapper.\n   *\n   * @param _options - Call options (unused in stub)\n   * @returns Argument array for the OpenCode CLI\n   */\n  buildArgs(_options: AICallOptions): string[] {\n    return ['run', '--format', 'json'];\n  }\n\n  /**\n   * Parse OpenCode CLI output into a normalized {@link AIResponse}.\n   *\n   * @throws {AIServiceError} Always -- OpenCode backend is not yet implemented\n   */\n  parseResponse(_stdout: string, _durationMs: number, _exitCode: number): AIResponse {\n    throw new AIServiceError(\n      'SUBPROCESS_ERROR',\n      'OpenCode backend is not yet implemented. Use Claude backend.',\n    );\n  }\n\n  /**\n   * Get user-facing install instructions for the OpenCode CLI.\n   */\n  getInstallInstructions(): string {\n    return [\n      'OpenCode (experimental):',\n      '  curl -fsSL https://opencode.ai/install | bash',\n      '  https://opencode.ai',\n    ].join('\\n');\n  }\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**OpenCodeBackend implements AIBackend interface as stub demonstrating extension pattern for future OpenCode CLI integration, currently throwing SUBPROCESS_ERROR in parseResponse() until JSONL output parsing implementation completes.**\n\n## Exported Symbols\n\n**OpenCodeBackend** class implements AIBackend interface with readonly properties `name: 'opencode'` and `cliCommand: 'opencode'`.\n\n## Public Interface\n\n**isAvailable(): Promise<boolean>** checks if `opencode` CLI exists on PATH by delegating to `isCommandOnPath(this.cliCommand)` imported from `claude.js`.\n\n**buildArgs(_options: AICallOptions): string[]** constructs CLI argument array `['run', '--format', 'json']` based on documented OpenCode CLI flags from RESEARCH.md. Prompt content passes via stdin through subprocess wrapper, not as CLI argument.\n\n**parseResponse(_stdout: string, _durationMs: number, _exitCode: number): AIResponse** always throws `AIServiceError('SUBPROCESS_ERROR', 'OpenCode backend is not yet implemented. Use Claude backend.')` since JSONL parsing remains unimplemented per RESEARCH.md Open Question 3.\n\n**getInstallInstructions(): string** returns multi-line string with curl-based install command `curl -fsSL https://opencode.ai/install | bash` and URL reference `https://opencode.ai`.\n\n## Integration Pattern\n\nDemonstrates backend extension architecture where AIBackend interface enables pluggable AI CLI adapters. OpenCodeBackend mirrors ClaudeBackend and GeminiBackend structure (sibling files `claude.ts`, `gemini.ts`) but defers output parsing implementation to future development phase.\n\n## Dependencies\n\nImports `AIBackend`, `AICallOptions`, `AIResponse` interfaces and `AIServiceError` class from `../types.js`. Imports `isCommandOnPath()` utility from `./claude.js` for PATH existence checking.\n\n## Implementation Status\n\nStub implementation allows registry detection and argument construction but blocks execution at response parsing stage. Full implementation requires JSONL format specification and parser matching OpenCode CLI output structure.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 482,
      "cacheReadTokens": 17114,
      "cacheCreationTokens": 9770,
      "latencyMs": 43486,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/ai/backends/opencode.ts",
          "sizeBytes": 2298
        }
      ]
    },
    {
      "timestamp": "2026-02-09T16:08:03.286Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/generation/prompts/index.ts\n\n```typescript\nexport type {\n  PromptContext,\n} from './types.js';\nexport { SUMMARY_GUIDELINES } from './types.js';\nexport {\n  buildFilePrompt,\n  buildDirectoryPrompt,\n  buildRootPrompt,\n  detectLanguage,\n} from './builder.js';\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**index.ts re-exports prompt construction functions, type definitions, and validation guidelines from the generation/prompts module, serving as the public interface for three-phase documentation generation prompt engineering.**\n\n## Exported Types\n\n- `PromptContext` — Type re-exported from `./types.js`, contains contextual data passed to prompt builders (file metadata, import maps, directory structure, aggregated child content)\n- `SUMMARY_GUIDELINES` — Constant re-exported from `./types.js`, contains the behavioral contract text embedded in all AI prompts defining documentation density rules, identifier preservation requirements, and forbidden filler phrases\n\n## Exported Functions\n\n- `buildFilePrompt(context: PromptContext): string` — Re-exported from `./builder.js`, constructs Phase 1 prompts for individual file analysis including import maps and language-specific guidance\n- `buildDirectoryPrompt(context: PromptContext): string` — Re-exported from `./builder.js`, constructs Phase 2 prompts for directory-level `AGENTS.md` aggregation consuming child `.sum` files and subdirectory `AGENTS.md` content\n- `buildRootPrompt(context: PromptContext): string` — Re-exported from `./builder.js`, constructs Phase 3 prompts for root integration document synthesis (`CLAUDE.md`, `GEMINI.md`, `OPENCODE.md`) consuming all `AGENTS.md` files\n- `detectLanguage(filePath: string): string` — Re-exported from `./builder.js`, maps file extensions to language names for prompt customization (e.g., `.ts` → `\"TypeScript\"`, `.py` → `\"Python\"`)\n\n## Module Architecture\n\nThis barrel export consolidates the prompt engineering subsystem, separating public API (`index.ts`) from implementation details across three internal modules:\n- `types.ts` — Type definitions and embedded guideline text\n- `builder.ts` — Prompt construction logic with phase-specific templating\n- `templates.ts` — Raw prompt text fragments (not re-exported, internal only)\n\n## Integration Points\n\nConsumed by `src/generation/executor.ts` which invokes `buildFilePrompt()`, `buildDirectoryPrompt()`, and `buildRootPrompt()` during three-phase pipeline execution, passing `PromptContext` instances populated by `src/generation/collector.ts` with aggregated content, import maps from `src/imports/extractor.ts`, and manifest detection results.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 566,
      "cacheReadTokens": 17114,
      "cacheCreationTokens": 9170,
      "latencyMs": 43779,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/generation/prompts/index.ts",
          "sizeBytes": 213
        }
      ]
    },
    {
      "timestamp": "2026-02-09T16:08:03.237Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/discovery/filters/binary.ts\n\n```typescript\n/**\n * Binary file filter for file discovery.\n *\n * Uses extension-based detection as a fast path, falling back to content\n * analysis via `isbinaryfile` for unknown extensions. Also handles large\n * files by size threshold.\n */\n\nimport { isBinaryFile } from 'isbinaryfile';\nimport fs from 'node:fs/promises';\nimport path from 'node:path';\nimport type { FileFilter } from '../types.js';\n\n/**\n * Set of file extensions known to be binary.\n * These are excluded without content analysis for performance.\n */\nexport const BINARY_EXTENSIONS = new Set([\n  // Images\n  '.png',\n  '.jpg',\n  '.jpeg',\n  '.gif',\n  '.bmp',\n  '.ico',\n  '.webp',\n  '.svg',\n  '.tiff',\n  '.tif',\n  '.psd',\n  '.raw',\n  '.heif',\n  '.heic',\n  // Archives\n  '.zip',\n  '.tar',\n  '.gz',\n  '.rar',\n  '.7z',\n  '.bz2',\n  '.xz',\n  '.tgz',\n  // Executables\n  '.exe',\n  '.dll',\n  '.so',\n  '.dylib',\n  '.bin',\n  '.msi',\n  '.app',\n  '.dmg',\n  // Media\n  '.mp3',\n  '.mp4',\n  '.wav',\n  '.avi',\n  '.mov',\n  '.mkv',\n  '.flac',\n  '.ogg',\n  '.webm',\n  '.m4a',\n  '.aac',\n  '.wma',\n  '.wmv',\n  '.flv',\n  // Documents (binary formats)\n  '.pdf',\n  '.doc',\n  '.docx',\n  '.xls',\n  '.xlsx',\n  '.ppt',\n  '.pptx',\n  '.odt',\n  '.ods',\n  '.odp',\n  // Fonts\n  '.woff',\n  '.woff2',\n  '.ttf',\n  '.eot',\n  '.otf',\n  // Compiled/bytecode\n  '.class',\n  '.pyc',\n  '.pyo',\n  '.o',\n  '.obj',\n  '.a',\n  '.lib',\n  '.wasm',\n  // Database\n  '.db',\n  '.sqlite',\n  '.sqlite3',\n  '.mdb',\n  // Other\n  '.ico',\n  '.icns',\n  '.cur',\n  '.deb',\n  '.rpm',\n  '.jar',\n  '.war',\n  '.ear',\n]);\n\n/**\n * Options for the binary filter.\n */\nexport interface BinaryFilterOptions {\n  /**\n   * Maximum file size in bytes. Files larger than this are excluded.\n   * Default: 1MB (1048576 bytes)\n   */\n  maxFileSize?: number;\n\n  /**\n   * Additional binary extensions to recognize beyond the defaults.\n   */\n  additionalExtensions?: string[];\n}\n\n/** Default maximum file size: 1MB */\nconst DEFAULT_MAX_FILE_SIZE = 1024 * 1024;\n\n/**\n * Creates a binary filter that excludes binary files and files exceeding size limit.\n *\n * The filter uses a two-phase detection approach:\n * 1. Fast path: Check extension against known binary extensions\n * 2. Slow path: For unknown extensions, analyze file content with isbinaryfile\n *\n * @param options - Filter configuration options\n * @returns A FileFilter that identifies binary files\n *\n * @example\n * ```typescript\n * const filter = createBinaryFilter({ maxFileSize: 500000 });\n * await filter.shouldExclude('/path/to/image.png'); // true (extension)\n * await filter.shouldExclude('/path/to/unknown.xyz'); // checks content\n * ```\n */\nexport function createBinaryFilter(options: BinaryFilterOptions = {}): FileFilter {\n  const { maxFileSize = DEFAULT_MAX_FILE_SIZE, additionalExtensions = [] } = options;\n\n  // Create a combined set of all binary extensions\n  const binaryExtensions = new Set(BINARY_EXTENSIONS);\n  for (const ext of additionalExtensions) {\n    // Ensure extension has leading dot\n    binaryExtensions.add(ext.startsWith('.') ? ext : `.${ext}`);\n  }\n\n  return {\n    name: 'binary',\n\n    async shouldExclude(absolutePath: string): Promise<boolean> {\n      // Fast path: check extension\n      const ext = path.extname(absolutePath).toLowerCase();\n      if (binaryExtensions.has(ext)) {\n        return true;\n      }\n\n      // Check file size and content\n      try {\n        const stats = await fs.stat(absolutePath);\n\n        // Exclude files exceeding size limit\n        if (stats.size > maxFileSize) {\n          return true;\n        }\n\n        // Slow path: content analysis for unknown extensions\n        return await isBinaryFile(absolutePath);\n      } catch {\n        // If we can't read it, skip it\n        return true;\n      }\n    },\n  };\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**Binary file filter implementing two-phase detection (extension fast-path, content analysis fallback) with size-based exclusion for file discovery.**\n\n## Exported Interface\n\n`createBinaryFilter(options?: BinaryFilterOptions): FileFilter` — Factory returning a `FileFilter` with `name: 'binary'` and async `shouldExclude(absolutePath: string): Promise<boolean>` method. Combines extension-based detection with content analysis via `isbinaryfile.isBinaryFile()` and size threshold checks via `fs.stat()`.\n\n`BinaryFilterOptions` — Configuration interface with optional `maxFileSize?: number` (bytes, default 1048576) and `additionalExtensions?: string[]` for supplementing default binary extension set.\n\n`BINARY_EXTENSIONS: Set<string>` — Exported set of 82 known binary extensions across 9 categories: images (`.png`, `.jpg`, `.jpeg`, `.gif`, `.bmp`, `.ico`, `.webp`, `.svg`, `.tiff`, `.tif`, `.psd`, `.raw`, `.heif`, `.heic`), archives (`.zip`, `.tar`, `.gz`, `.rar`, `.7z`, `.bz2`, `.xz`, `.tgz`), executables (`.exe`, `.dll`, `.so`, `.dylib`, `.bin`, `.msi`, `.app`, `.dmg`), media (`.mp3`, `.mp4`, `.wav`, `.avi`, `.mov`, `.mkv`, `.flac`, `.ogg`, `.webm`, `.m4a`, `.aac`, `.wma`, `.wmv`, `.flv`), binary documents (`.pdf`, `.doc`, `.docx`, `.xls`, `.xlsx`, `.ppt`, `.pptx`, `.odt`, `.ods`, `.odp`), fonts (`.woff`, `.woff2`, `.ttf`, `.eot`, `.otf`), compiled bytecode (`.class`, `.pyc`, `.pyo`, `.o`, `.obj`, `.a`, `.lib`, `.wasm`), database (`.db`, `.sqlite`, `.sqlite3`, `.mdb`), and miscellaneous (`.ico`, `.icns`, `.cur`, `.deb`, `.rpm`, `.jar`, `.war`, `.ear`).\n\n`DEFAULT_MAX_FILE_SIZE: number` — Constant set to `1024 * 1024` (1MB).\n\n## Detection Algorithm\n\n`shouldExclude()` executes three-phase check:\n\n1. **Fast path (extension)**: Extracts `path.extname(absolutePath).toLowerCase()`, checks membership in `binaryExtensions` set (merge of `BINARY_EXTENSIONS` and `additionalExtensions` normalized with leading dot via `ext.startsWith('.') ? ext : '.' + ext`), returns `true` on match.\n\n2. **Size threshold**: Invokes `fs.stat(absolutePath)`, compares `stats.size > maxFileSize`, returns `true` if exceeded.\n\n3. **Slow path (content)**: Delegates to `isBinaryFile(absolutePath)` from `isbinaryfile` package for unknown extensions.\n\nReturns `true` on `fs.stat()` failure (file unreadable/missing). All operations are async via `async/await` pattern.\n\n## Integration with Discovery Pipeline\n\nImplements `FileFilter` interface from `../types.js` with `name` property for identification in filter chains. Composed with other filters (gitignore, vendor, custom) via `discovery/filters/index.ts` exports. Configured via `config.yaml` schema fields: `options.maxFileSize` and `exclude.binaryExtensions`.\n\n## Dependencies\n\n- `isbinaryfile` — Provides `isBinaryFile(path)` for content-based binary detection\n- `node:fs/promises` — Supplies `fs.stat()` for size retrieval\n- `node:path` — Supplies `path.extname()` for extension extraction",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 914,
      "cacheReadTokens": 17114,
      "cacheCreationTokens": 10364,
      "latencyMs": 47230,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/discovery/filters/binary.ts",
          "sizeBytes": 3701
        }
      ]
    },
    {
      "timestamp": "2026-02-09T16:08:03.328Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/generation/prompts/types.ts\n\n```typescript\n/**\n * Context provided when building a prompt.\n */\nexport interface PromptContext {\n  /** Absolute path to the file */\n  filePath: string;\n  /** File content to analyze */\n  content: string;\n  /** Related files for additional context */\n  contextFiles?: Array<{\n    path: string;\n    content: string;\n  }>;\n  /** Project structure listing for bird's-eye context */\n  projectPlan?: string;\n  /** Existing .sum summary text for incremental updates */\n  existingSum?: string;\n}\n\n/**\n * Guidelines for summary generation (from CONTEXT.md).\n */\nexport const SUMMARY_GUIDELINES = {\n  /** Target word count range */\n  targetLength: { min: 300, max: 500 },\n  /** What to include */\n  include: [\n    'Purpose and responsibility',\n    'Public interface (exports, key functions)',\n    'Key patterns and notable algorithms',\n    'Dependencies with usage context',\n    'Key function signatures as code snippets',\n    'Tightly coupled sibling files',\n    'Behavioral contracts: verbatim regex patterns, format strings, magic constants, sentinel values, output templates, environment variables',\n  ],\n  /** What to exclude */\n  exclude: [\n    'Control flow minutiae (loop structures, variable naming, temporary state)',\n    'Generic TODOs/FIXMEs (keep only security/breaking)',\n    'Broad architectural relationships (handled by AGENTS.md)',\n  ],\n} as const;\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**Defines prompt context structure and summary generation guidelines for AI-driven file analysis.**\n\n## Exported Types\n\n### PromptContext\nInterface supplying input data for prompt construction in file/directory/root analysis phases.\n\n```typescript\ninterface PromptContext {\n  filePath: string;              // Absolute path to analyzed file\n  content: string;               // Raw file content for AI processing\n  contextFiles?: Array<{         // Optional related files for cross-reference\n    path: string;\n    content: string;\n  }>;\n  projectPlan?: string;          // GENERATION-PLAN.md content for hierarchical context\n  existingSum?: string;          // Prior .sum summary text for incremental updates\n}\n```\n\nConsumed by `buildFilePrompt()`, `buildDirectoryPrompt()`, `buildRootPrompt()` in `src/generation/prompts/builder.ts` to construct AI subprocess prompts with file content, import maps, child AGENTS.md aggregations, and project metadata.\n\n### SUMMARY_GUIDELINES\nConst object encoding documentation density rules and inclusion/exclusion constraints for `.sum` generation.\n\n```typescript\nconst SUMMARY_GUIDELINES = {\n  targetLength: { min: 300, max: 500 },  // Word count range for summaries\n  include: [\n    'Purpose and responsibility',\n    'Public interface (exports, key functions)',\n    'Key patterns and notable algorithms',\n    'Dependencies with usage context',\n    'Key function signatures as code snippets',\n    'Tightly coupled sibling files',\n    'Behavioral contracts: verbatim regex patterns, format strings, magic constants, sentinel values, output templates, environment variables',\n  ],\n  exclude: [\n    'Control flow minutiae (loop structures, variable naming, temporary state)',\n    'Generic TODOs/FIXMEs (keep only security/breaking)',\n    'Broad architectural relationships (handled by AGENTS.md)',\n  ],\n}\n```\n\nEmbedded in `buildFilePrompt()` prompt template (`FILE_SUMMARY_TEMPLATE` in `src/generation/prompts/templates.ts`) to enforce consistency across AI backends (Claude Code, Gemini CLI, OpenCode). The `include` array explicitly mandates capturing behavioral contracts (regex patterns, format strings, magic constants, sentinel values, environment variable names) that define reproducible behavior.\n\n## Integration Points\n\n- **Prompt Builder**: `buildFilePrompt()` destructures `PromptContext` fields and injects `SUMMARY_GUIDELINES.include`/`exclude` arrays into template string literals.\n- **Update Workflow**: `existingSum` field populated by `readSumFile()` in `src/generation/writers/sum.ts` for incremental regeneration when `content_hash` mismatches.\n- **Template System**: `SUMMARY_GUIDELINES` constants referenced in `FILE_SUMMARY_TEMPLATE`, `DIRECTORY_AGGREGATION_TEMPLATE`, `ROOT_SYNTHESIS_TEMPLATE` via string interpolation.\n- **Quality Validation**: `targetLength` range not enforced programmatically but used as heuristic during `validateFindability()` (disabled in v0.6.5 after `publicInterface` schema field removal).\n\n## Design Rationale\n\nSeparates data contracts (`PromptContext`) from prompt engineering constraints (`SUMMARY_GUIDELINES`) to enable independent evolution. The `as const` assertion on `SUMMARY_GUIDELINES` provides TypeScript literal type inference for readonly arrays, preventing accidental mutations during template construction. The explicit enumeration of \"behavioral contracts\" in `include` array addresses phantom path and code-vs-doc inconsistencies by mandating verbatim capture of regex patterns, format strings, and sentinel values that define observable system behavior.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 853,
      "cacheReadTokens": 0,
      "cacheCreationTokens": 26568,
      "latencyMs": 48781,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/generation/prompts/types.ts",
          "sizeBytes": 1345
        }
      ]
    },
    {
      "timestamp": "2026-02-09T16:08:03.204Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/ai/backends/claude.ts\n\n```typescript\n/**\n * Claude CLI backend adapter.\n *\n * Full implementation of the {@link AIBackend} interface for the Claude Code\n * CLI (`claude`). Builds CLI arguments, parses structured JSON responses\n * with Zod validation, detects CLI availability on PATH, and provides\n * install instructions.\n *\n * The prompt is NOT included in the args array -- it goes to stdin via\n * the subprocess wrapper ({@link runSubprocess}).\n *\n * @module\n */\n\nimport * as fs from 'node:fs/promises';\nimport * as path from 'node:path';\nimport { z } from 'zod';\nimport type { AIBackend, AICallOptions, AIResponse } from '../types.js';\nimport { AIServiceError } from '../types.js';\n\n// ---------------------------------------------------------------------------\n// Zod schema for Claude CLI JSON output\n// ---------------------------------------------------------------------------\n\n/**\n * Schema validated against Claude CLI v2.1.31 JSON output.\n *\n * When invoked with `claude -p --output-format json`, the CLI produces a\n * single JSON object on stdout matching this shape. See RESEARCH.md for\n * the live verification details.\n */\nconst ClaudeResponseSchema = z.object({\n  type: z.literal('result'),\n  subtype: z.enum(['success', 'error']),\n  is_error: z.boolean(),\n  duration_ms: z.number(),\n  duration_api_ms: z.number(),\n  num_turns: z.number(),\n  result: z.string(),\n  session_id: z.string(),\n  total_cost_usd: z.number(),\n  usage: z.object({\n    input_tokens: z.number(),\n    cache_creation_input_tokens: z.number(),\n    cache_read_input_tokens: z.number(),\n    output_tokens: z.number(),\n  }),\n  modelUsage: z.record(z.object({\n    inputTokens: z.number(),\n    outputTokens: z.number(),\n    cacheReadInputTokens: z.number(),\n    cacheCreationInputTokens: z.number(),\n    costUSD: z.number(),\n  })),\n});\n\n// ---------------------------------------------------------------------------\n// PATH detection utility\n// ---------------------------------------------------------------------------\n\n/**\n * Check whether a command is available on the system PATH.\n *\n * Splits `process.env.PATH` by the platform delimiter and checks each\n * directory for a file matching the command name. On Windows, also checks\n * each extension from `process.env.PATHEXT` (e.g., `.exe`, `.cmd`, `.bat`).\n *\n * Uses `fs.stat` (not `fs.access` with execute bit) for cross-platform\n * compatibility -- Windows does not have Unix execute permissions.\n *\n * @param command - The bare command name to look for (e.g., \"claude\")\n * @returns `true` if the command exists as a file in any PATH directory\n *\n * @example\n * ```typescript\n * if (await isCommandOnPath('claude')) {\n *   console.log('Claude CLI is available');\n * }\n * ```\n */\nexport async function isCommandOnPath(command: string): Promise<boolean> {\n  const envPath = process.env.PATH ?? '';\n  const pathDirs = envPath\n    .replace(/[\"]+/g, '')\n    .split(path.delimiter)\n    .filter(Boolean);\n\n  // On Windows, PATHEXT lists executable extensions (e.g., \".EXE;.CMD;.BAT\").\n  // On other platforms, PATHEXT is unset; check the bare command name only.\n  const envExt = process.env.PATHEXT ?? '';\n  const extensions = envExt ? envExt.split(';') : [''];\n\n  for (const dir of pathDirs) {\n    for (const ext of extensions) {\n      try {\n        const candidate = path.join(dir, command + ext);\n        const stat = await fs.stat(candidate);\n        if (stat.isFile()) {\n          return true;\n        }\n      } catch {\n        // Not found in this dir/ext combination, continue\n      }\n    }\n  }\n\n  return false;\n}\n\n// ---------------------------------------------------------------------------\n// Claude backend\n// ---------------------------------------------------------------------------\n\n/**\n * Claude Code CLI backend adapter.\n *\n * Implements the {@link AIBackend} interface for the `claude` CLI.\n * This is the primary (and currently only fully implemented) backend.\n *\n * @example\n * ```typescript\n * const backend = new ClaudeBackend();\n * if (await backend.isAvailable()) {\n *   const args = backend.buildArgs({ prompt: 'Summarize this file' });\n *   const result = await runSubprocess('claude', args, {\n *     timeoutMs: 120_000,\n *     input: 'Summarize this file',\n *   });\n *   const response = backend.parseResponse(result.stdout, result.durationMs, result.exitCode);\n * }\n * ```\n */\nexport class ClaudeBackend implements AIBackend {\n  readonly name = 'claude';\n  readonly cliCommand = 'claude';\n\n  /**\n   * Check if the `claude` CLI is available on PATH.\n   */\n  async isAvailable(): Promise<boolean> {\n    return isCommandOnPath(this.cliCommand);\n  }\n\n  /**\n   * Build CLI arguments for a Claude invocation.\n   *\n   * Returns the argument array for `claude -p --output-format json`. The\n   * prompt itself is NOT included -- it goes to stdin via the subprocess\n   * wrapper.\n   *\n   * @param options - Call options (model, systemPrompt, maxTurns)\n   * @returns Argument array suitable for {@link runSubprocess}\n   */\n  buildArgs(options: AICallOptions): string[] {\n    const args: string[] = [\n      '-p',                        // Non-interactive print mode\n      '--output-format', 'json',   // Structured JSON output\n      '--no-session-persistence',  // Don't save session to disk\n      '--permission-mode', 'bypassPermissions',  // Non-interactive: skip permission prompts (PITFALLS.md §8)\n    ];\n\n    if (options.model) {\n      args.push('--model', options.model);\n    }\n\n    if (options.systemPrompt) {\n      args.push('--system-prompt', options.systemPrompt);\n    }\n\n    if (options.maxTurns !== undefined) {\n      args.push('--max-turns', String(options.maxTurns));\n    }\n\n    return args;\n  }\n\n  /**\n   * Parse Claude CLI JSON output into a normalized {@link AIResponse}.\n   *\n   * Handles non-JSON prefix text by finding the first `{` character\n   * (defensive parsing per RESEARCH.md Pitfall 4). Validates the response\n   * against the Zod schema and extracts the model name from `modelUsage`.\n   *\n   * @param stdout - Raw stdout from the Claude CLI process\n   * @param durationMs - Wall-clock duration of the subprocess\n   * @param exitCode - Process exit code\n   * @returns Normalized AI response\n   * @throws {AIServiceError} With code `PARSE_ERROR` if JSON is missing or schema validation fails\n   */\n  parseResponse(stdout: string, durationMs: number, exitCode: number): AIResponse {\n    // Find JSON object in stdout (handle any prefix text like upgrade notices)\n    const jsonStart = stdout.indexOf('{');\n    if (jsonStart === -1) {\n      throw new AIServiceError(\n        'PARSE_ERROR',\n        `No JSON object found in Claude CLI output. Raw output (first 200 chars): ${stdout.slice(0, 200)}`,\n      );\n    }\n\n    let parsed: z.infer<typeof ClaudeResponseSchema>;\n    try {\n      parsed = ClaudeResponseSchema.parse(JSON.parse(stdout.slice(jsonStart)));\n    } catch (error) {\n      const message = error instanceof Error ? error.message : String(error);\n      throw new AIServiceError(\n        'PARSE_ERROR',\n        `Failed to parse Claude CLI JSON response: ${message}`,\n      );\n    }\n\n    // Extract model name from modelUsage keys (first key is the model used)\n    const modelName = Object.keys(parsed.modelUsage)[0] ?? 'unknown';\n\n    return {\n      text: parsed.result,\n      model: modelName,\n      inputTokens: parsed.usage.input_tokens,\n      outputTokens: parsed.usage.output_tokens,\n      cacheReadTokens: parsed.usage.cache_read_input_tokens,\n      cacheCreationTokens: parsed.usage.cache_creation_input_tokens,\n      durationMs,\n      exitCode,\n      raw: parsed,\n    };\n  }\n\n  /**\n   * Get user-facing install instructions for the Claude CLI.\n   */\n  getInstallInstructions(): string {\n    return [\n      'Claude Code (recommended):',\n      '  npm install -g @anthropic-ai/claude-code',\n      '  https://code.claude.com',\n    ].join('\\n');\n  }\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**ClaudeBackend implements AIBackend interface for Claude Code CLI (`claude`), handling CLI argument construction, JSON response parsing with Zod validation, PATH availability detection, and install instructions.**\n\n## Exported Symbols\n\n**ClaudeBackend** class implements `AIBackend` interface with properties:\n- `name: 'claude'` — Backend identifier\n- `cliCommand: 'claude'` — CLI executable name\n\nMethods:\n- `async isAvailable(): Promise<boolean>` — Delegates to `isCommandOnPath('claude')` for PATH detection\n- `buildArgs(options: AICallOptions): string[]` — Constructs CLI arguments array for `claude` command\n- `parseResponse(stdout: string, durationMs: number, exitCode: number): AIResponse` — Parses Claude CLI JSON output into normalized `AIResponse`\n- `getInstallInstructions(): string` — Returns multiline string with npm install command and URL\n\n**isCommandOnPath** function `async (command: string): Promise<boolean>` — Cross-platform PATH lookup checking if command exists as file in any PATH directory. Splits `process.env.PATH` by `path.delimiter`, iterates directories, checks `process.env.PATHEXT` extensions on Windows (`.exe`, `.cmd`, `.bat`), uses `fs.stat().isFile()` for detection (not `fs.access()` to avoid Unix permission assumptions).\n\n**ClaudeResponseSchema** Zod schema validates Claude CLI v2.1.31 JSON output structure:\n- Root: `{ type: 'result', subtype: 'success'|'error', is_error: boolean, duration_ms: number, duration_api_ms: number, num_turns: number, result: string, session_id: string, total_cost_usd: number, usage: {...}, modelUsage: {...} }`\n- `usage` object: `{ input_tokens, cache_creation_input_tokens, cache_read_input_tokens, output_tokens }` (all numbers)\n- `modelUsage` record: `Map<modelName, { inputTokens, outputTokens, cacheReadInputTokens, cacheCreationInputTokens, costUSD }>`\n\n## CLI Argument Construction\n\n`buildArgs()` returns fixed base arguments:\n- `-p` — Non-interactive print mode\n- `--output-format json` — Structured JSON output\n- `--no-session-persistence` — Prevents disk session saves\n- `--permission-mode bypassPermissions` — Skips interactive permission prompts (references PITFALLS.md §8)\n\nConditionally appends:\n- `--model <value>` if `options.model` provided\n- `--system-prompt <value>` if `options.systemPrompt` provided\n- `--max-turns <value>` if `options.maxTurns !== undefined`\n\nPrompt text excluded from args array — passed via stdin by `runSubprocess()` caller.\n\n## JSON Response Parsing\n\n`parseResponse()` implements defensive parsing:\n1. Finds first `{` character via `stdout.indexOf('{')` to skip non-JSON prefix text (upgrade notices, warnings)\n2. Throws `AIServiceError` with code `PARSE_ERROR` if no JSON object found\n3. Parses substring `stdout.slice(jsonStart)` and validates against `ClaudeResponseSchema`\n4. Extracts model name from `Object.keys(parsed.modelUsage)[0]` (first key is active model)\n5. Maps to `AIResponse` structure:\n   - `text` ← `parsed.result`\n   - `model` ← first modelUsage key or `'unknown'`\n   - `inputTokens` ← `parsed.usage.input_tokens`\n   - `outputTokens` ← `parsed.usage.output_tokens`\n   - `cacheReadTokens` ← `parsed.usage.cache_read_input_tokens`\n   - `cacheCreationTokens` ← `parsed.usage.cache_creation_input_tokens`\n   - `durationMs` ← function parameter (wall-clock time)\n   - `exitCode` ← function parameter\n   - `raw` ← entire `parsed` object\n\n## Error Handling\n\nThrows `AIServiceError` with discriminated codes:\n- `PARSE_ERROR` when `stdout.indexOf('{')` returns `-1` (includes first 200 chars of raw output in message)\n- `PARSE_ERROR` when `ClaudeResponseSchema.parse()` throws (includes Zod validation error message)\n\n## Dependencies\n\n- `node:fs/promises` — `fs.stat()` for PATH detection\n- `node:path` — `path.join()`, `path.delimiter` for cross-platform path manipulation\n- `zod` — `ClaudeResponseSchema` validation, `z.infer<>` type extraction\n- `../types.js` — `AIBackend`, `AICallOptions`, `AIResponse`, `AIServiceError` imports\n\n## Integration Pattern\n\nIntended usage flow (documented in class example):\n1. Instantiate `new ClaudeBackend()`\n2. Call `await backend.isAvailable()` for availability check\n3. Call `backend.buildArgs({ prompt, model?, systemPrompt?, maxTurns? })` to construct CLI args\n4. Pass args to `runSubprocess('claude', args, { input: prompt, timeoutMs })` (from `../subprocess.ts`)\n5. Call `backend.parseResponse(result.stdout, result.durationMs, result.exitCode)` to normalize response\n\n## Platform-Specific Behavior\n\nWindows PATH detection differences:\n- Reads `process.env.PATHEXT` (typically `\".COM;.EXE;.BAT;.CMD;.VBS;.VBE;.JS;.JSE;.WSF;.WSH;.MSC\"`)\n- Checks each extension appended to command name: `claude.exe`, `claude.cmd`, `claude.bat`, etc.\n- Uses `fs.stat()` rather than Unix execute bit checking for cross-platform compatibility\n\nUnix/Linux/macOS:\n- `process.env.PATHEXT` unset → single iteration with bare command name\n- Checks `claude` (no extension)\n\nPATH string cleaning:\n- Strips double quotes via `.replace(/[\"]+/g, '')` before splitting (handles Windows quoted paths)\n- Splits by `path.delimiter` (`:` on Unix, `;` on Windows)\n- Filters empty strings via `.filter(Boolean)`",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 1534,
      "cacheReadTokens": 16334,
      "cacheCreationTokens": 12124,
      "latencyMs": 52002,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/ai/backends/claude.ts",
          "sizeBytes": 7804
        }
      ]
    },
    {
      "timestamp": "2026-02-09T16:08:39.845Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/generation/writers/index.ts\n\n```typescript\nexport {\n  writeSumFile,\n  readSumFile,\n  getSumPath,\n  sumFileExists,\n  type SumFileContent,\n} from './sum.js';\n\nexport {\n  writeAgentsMd,\n} from './agents-md.js';\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**index.ts re-exports writer utilities for .sum files and AGENTS.md generation.**\n\n## Exported Symbols\n\n**From `./sum.js`:**\n- `writeSumFile` — Writes .sum files with YAML frontmatter containing `generated_at`, `content_hash`, `purpose`, `critical_todos`, `related_files`\n- `readSumFile` — Parses existing .sum files, extracts YAML frontmatter and markdown body\n- `getSumPath` — Computes .sum file path from source file path\n- `sumFileExists` — Checks if .sum file exists for given source file\n- `SumFileContent` — Type interface for .sum file structure with frontmatter fields and markdown content\n\n**From `./agents-md.js`:**\n- `writeAgentsMd` — Writes directory-level AGENTS.md files from aggregated child .sum content and subdirectory AGENTS.md files during Phase 2 post-order traversal\n\n## Module Role\n\nindex.ts serves as public API boundary for `src/generation/writers/` directory. Consumers import writer functions via `import { writeSumFile, readSumFile, writeAgentsMd } from './generation/writers/index.js'` without accessing implementation modules directly. Central to Phase 1 file analysis (writeSumFile) and Phase 2 directory aggregation (writeAgentsMd) in three-phase pipeline. SumFileContent type defines contract for frontmatter schema including SHA-256 content_hash used by incremental update system.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 358,
      "cacheReadTokens": 17114,
      "cacheCreationTokens": 9161,
      "latencyMs": 18940,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/generation/writers/index.ts",
          "sizeBytes": 165
        }
      ]
    },
    {
      "timestamp": "2026-02-09T16:08:03.318Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/generation/prompts/templates.ts\n\n```typescript\n/**\n * Prompt constants for file and directory analysis.\n */\n\nexport const FILE_SYSTEM_PROMPT = `You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation`;\n\nexport const FILE_USER_PROMPT = `Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: {{FILE_PATH}}\n\n\\`\\`\\`typescript\n{{CONTENT}}\n\\`\\`\\`\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.`;\n\n/**\n * System prompt for directory-level AGENTS.md generation.\n * Used by buildDirectoryPrompt() in builder.ts.\n */\nexport const DIRECTORY_SYSTEM_PROMPT = `You are generating an AGENTS.md file — a directory-level overview for AI coding assistants.\n\nCRITICAL: Output ONLY the raw markdown content. No code fences, no preamble, no explanations, no conversational text. Your entire response IS the AGENTS.md file content.\n\nOUTPUT FORMAT:\n- First line MUST be exactly: <!-- Generated by agents-reverse-engineer -->\n- Use a # heading with the directory name\n- Write a one-paragraph purpose statement for the directory\nusage\nADAPTIVE SECTIONS:\nAnalyze the directory contents and choose the most relevant sections. Do NOT use a fixed template. Instead, select sections that best document this specific directory for an AI that needs to reconstruct or extend the project.\n\nConsider these section types (choose what applies):\n- **Contents**: Group files by purpose/category under ## headings. For each file: markdown link [filename](./filename) and a one-line description.\n- **Subdirectories**: If subdirectories exist, list them with links [dirname/](./dirname/) and brief summaries.\n- **Architecture / Data Flow**: If files form a pipeline, request/response chain, or layered architecture, document it.\n- **Stack**: If this is a package root (has package.json, Cargo.toml, go.mod, etc.), document the technology stack, key scripts, and entry points.\n- **Structure**: If the directory layout follows a convention (feature-sliced, domain-driven, MVC, etc.), document it.\n- **Patterns**: If files share recurring design patterns (factory, strategy, middleware, barrel re-export), name and document them.\n- **Configuration**: If the directory contains config files, schemas, or environment definitions, document the config surface area.\n- **API Surface**: If the directory exports a public API (barrel index, route definitions, SDK), document the interface contract.\n- **File Relationships**: How files collaborate, depend on each other, or share state.\n- **Behavioral Contracts**: If files contain regex patterns, format specifications, magic constants, or template strings that define observable behavior, collect them in a dedicated section. Preserve verbatim patterns from file summaries — do NOT paraphrase regex into prose. This section is MANDATORY when file summaries contain behavioral artifacts.\n\nChoose any relevant sections or create your own based on the directory contents. The goal is to provide a comprehensive overview that captures the essence of the directory's role in the project and how its files work together, with a focus on what an AI coding assistant would need to know to effectively interact with this code.\n\nSCOPE:\n- AGENTS.md is a NAVIGATIONAL INDEX — help an AI find the right file quickly\n- Focus on: what each file does, how files relate, directory-level patterns\n- Do NOT reproduce full architecture sections — those belong in the root CLAUDE.md\n\nPATH ACCURACY (MANDATORY):\n- When referencing files or modules outside this directory, use ONLY paths from the \"Import Map\" section\n- Do NOT invent, rename, or guess module paths — if a path isn't in the Import Map, don't reference it\n- Use the exact directory names from \"Project Directory Structure\" — do NOT rename directories\n  (e.g., if the directory is called \"cli\", write \"src/cli/\", NOT \"src/commands/\")\n- Cross-module references must use the specifier format from actual import statements\n  (e.g., \"../generation/writers/sum.js\", NOT \"../fs/sum-file.js\")\n- If you are unsure about a path, omit the cross-reference rather than guessing\n\nCONSISTENCY (MANDATORY):\n- Do not contradict yourself within the same document\n- If you describe a technique (e.g., \"regex-based\"), do not call it something else later (e.g., \"AST-based\")\n- When stating version numbers, engines, or config fields, use ONLY values present in the file summaries\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this directory\", \"this module\", \"provides\", \"responsible for\", \"is used to\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Per-file descriptions in Contents sections: 1-2 sentences maximum. Reference key symbols but do not reproduce full summaries.\n- Behavioral contracts (regex patterns, format specs, constants) belong in a separate Behavioral Contracts section, not in per-file descriptions.\n- Subdirectory descriptions: 1-2 sentences maximum. Capture the directory's role, not its full contents.\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- Key exported symbols from file summaries MUST appear in the directory overview\n- Preserve exact casing of identifiers\n\nUSER NOTES:\n- If \"User Notes\" are provided in the prompt, they contain user-defined instructions that will be automatically prepended to your output\n- Do NOT repeat or paraphrase user notes in your generated content — they are included separately\n- You may reference information from user notes for context`;\n\n/**\n * System prompt for incremental file summary updates.\n * Used by buildFilePrompt() when existingSum is provided.\n */\nexport const FILE_UPDATE_SYSTEM_PROMPT = `You are updating an existing file summary for an AI coding assistant. The source code has changed and the summary needs to reflect those changes.\n\nCRITICAL — INCREMENTAL UPDATE RULES:\n- You are given the EXISTING summary and the UPDATED source code\n- Preserve the structure, section headings, and phrasing of the existing summary wherever the underlying code is unchanged\n- Only modify content that is directly affected by the code changes\n- If a section describes code that has not changed, keep it VERBATIM — do not rephrase, reorganize, or \"improve\" stable text\n- Add new sections only if the code changes introduce entirely new concepts\n- Remove sections only if the code they described has been deleted\n- Update signatures, type names, and identifiers to match the current source exactly\n\nBEHAVIORAL CONTRACT PRESERVATION (MANDATORY):\n- Regex patterns, format strings, magic constants, and template content from the existing summary MUST be preserved verbatim unless the source code changed them\n- If source code changes a regex pattern or constant, update the summary to show the NEW value verbatim\n- Never summarize or paraphrase regex patterns — always show the exact pattern in backticks\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers\n- Missing any exported identifier is a failure\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation`;\n\n/**\n * System prompt for incremental directory AGENTS.md updates.\n * Used by buildDirectoryPrompt() when existingAgentsMd is provided.\n */\nexport const DIRECTORY_UPDATE_SYSTEM_PROMPT = `You are updating an existing AGENTS.md file — a directory-level overview for AI coding assistants. Some file summaries or subdirectory documents have changed, and AGENTS.md needs to reflect those changes.\n\nCRITICAL — INCREMENTAL UPDATE RULES:\n- You are given the EXISTING AGENTS.md and the CURRENT file summaries and subdirectory documents\n- Preserve the structure, section headings, and descriptions that are still accurate\n- Only modify entries for files or subdirectories whose summaries have changed\n- Add entries for new files, remove entries for deleted files\n- Do NOT reorganize, rephrase, or restructure sections that are unaffected by changes\n- Keep the same section ordering unless files were added/removed in a way that requires regrouping\n- Behavioral Contracts section: preserve verbatim regex patterns and constants unless source file summaries show they changed\n\nCRITICAL: Output ONLY the raw markdown content. No code fences, no preamble, no explanations, no conversational text. Your entire response IS the AGENTS.md file content.\n\nOUTPUT FORMAT:\n- First line MUST be exactly: <!-- Generated by agents-reverse-engineer -->\n- Use a # heading with the directory name\n- Preserve the existing purpose statement unless the directory's role has fundamentally changed\n\nSCOPE:\n- AGENTS.md is a NAVIGATIONAL INDEX — help an AI find the right file quickly\n- Focus on: what each file does, how files relate, directory-level patterns\n- Do NOT reproduce full architecture sections — those belong in the root CLAUDE.md\n\nPATH ACCURACY (MANDATORY):\n- When referencing files or modules outside this directory, use ONLY paths from the \"Import Map\" section\n- Do NOT invent, rename, or guess module paths — if a path isn't in the Import Map, don't reference it\n- Use the exact directory names from \"Project Directory Structure\" — do NOT rename directories\n- Cross-module references must use the specifier format from actual import statements\n- If you are unsure about a path, omit the cross-reference rather than guessing\n\nCONSISTENCY (MANDATORY):\n- Do not contradict yourself within the same document\n- If you describe a technique (e.g., \"regex-based\"), do not call it something else later (e.g., \"AST-based\")\n- When stating version numbers, engines, or config fields, use ONLY values present in the file summaries\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this directory\", \"this module\", \"provides\", \"responsible for\", \"is used to\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Per-file descriptions: 1-2 sentences maximum. Reference key symbols but do not reproduce full summaries.\n- Subdirectory descriptions: 1-2 sentences maximum. Capture the directory's role, not its full contents.\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- Key exported symbols from file summaries MUST appear in the directory overview\n- Preserve exact casing of identifiers\n\nUSER NOTES:\n- If \"User Notes\" are provided in the prompt, they contain user-defined instructions that will be automatically prepended to your output\n- Do NOT repeat or paraphrase user notes in your generated content — they are included separately\n- You may reference information from user notes for context`;\n\n/**\n * System prompt for root CLAUDE.md generation.\n * Used by buildRootPrompt() in builder.ts.\n */\nexport const ROOT_SYSTEM_PROMPT = `You generate markdown documentation files. Output ONLY the raw markdown content.\nDo NOT include any conversational text, preamble, or meta-commentary.\nDo NOT say \"Here is...\" or \"I've generated...\" — just output the document itself.\nThe output will be written directly to a file.\n\nCRITICAL CONSTRAINT:\n- Synthesize ONLY from the AGENTS.md content provided in the user prompt\n- Do NOT invent, extrapolate, or hallucinate features, hooks, APIs, patterns, or dependencies not explicitly mentioned\n- If information is missing, omit that section rather than guessing\n- Every claim must be traceable to a specific AGENTS.md file provided`;\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**templates.ts defines system and user prompt templates for AI-driven documentation generation across three pipeline phases: file analysis (FILE_SYSTEM_PROMPT, FILE_USER_PROMPT), directory aggregation (DIRECTORY_SYSTEM_PROMPT), and root synthesis (ROOT_SYSTEM_PROMPT), plus incremental update variants (FILE_UPDATE_SYSTEM_PROMPT, DIRECTORY_UPDATE_SYSTEM_PROMPT).**\n\n## Exported Constants\n\n- `FILE_SYSTEM_PROMPT: string` — System prompt for per-file `.sum` generation instructing AI to produce identifier-rich summaries with mandatory density rules, anchor term preservation, and behavioral contract extraction\n- `FILE_USER_PROMPT: string` — User prompt template for file analysis containing placeholders `{{FILE_PATH}}` and `{{CONTENT}}`, injected with project structure tree and file source code\n- `DIRECTORY_SYSTEM_PROMPT: string` — System prompt for `AGENTS.md` generation enforcing navigational index structure, adaptive section selection, path accuracy constraints, and behavioral contract aggregation\n- `FILE_UPDATE_SYSTEM_PROMPT: string` — Incremental update variant of FILE_SYSTEM_PROMPT instructing AI to preserve unchanged sections verbatim and modify only code-affected content\n- `DIRECTORY_UPDATE_SYSTEM_PROMPT: string` — Incremental update variant of DIRECTORY_SYSTEM_PROMPT instructing AI to preserve structure and modify only entries for changed files/subdirectories\n- `ROOT_SYSTEM_PROMPT: string` — System prompt for root document synthesis (CLAUDE.md, GEMINI.md, OPENCODE.md) constraining AI to synthesize only from provided AGENTS.md content without invention\n\n## Behavioral Contracts\n\n### File Analysis Density Rules\nFILE_SYSTEM_PROMPT mandates: \"Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\" and bans filler phrases: `\"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"`. Enforces pattern `\"[ExportName] does X\"` instead of `\"The ExportName function is responsible for doing X\"`.\n\n### Anchor Term Preservation\nFILE_SYSTEM_PROMPT requires: \"All exported function/class/type/const names MUST appear in the summary exactly as written in source\", \"Preserve exact casing of identifiers (e.g., buildAgentsMd, not 'build agents md')\", \"Missing any exported identifier is a failure\".\n\n### Output Format Requirements\nFILE_SYSTEM_PROMPT enforces: \"Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\", \"Do NOT include any preamble, thinking, or meta-commentary\", \"Do NOT say 'Here is...', 'Now I'll...', 'Based on my analysis...', 'Let me create...', 'Perfect.'\", \"Your response IS the documentation — not a message about the documentation\".\n\n### Behavioral Contract Extraction\nFILE_SYSTEM_PROMPT section \"BEHAVIORAL CONTRACTS (NEVER EXCLUDE)\" lists verbatim patterns required: \"Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\", \"Format strings, output templates, serialization structures — show exact format\", \"Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\", \"Prompt text or template strings that control AI/LLM behavior\", \"Error message patterns and error code strings used for matching\", \"Environment variable names and their expected values\", \"File format specifications (YAML frontmatter schemas, NDJSON line formats)\". States: \"These define observable behavior that must be reproduced exactly.\"\n\n### Directory Path Accuracy\nDIRECTORY_SYSTEM_PROMPT section \"PATH ACCURACY (MANDATORY)\" enforces: \"When referencing files or modules outside this directory, use ONLY paths from the 'Import Map' section\", \"Do NOT invent, rename, or guess module paths — if a path isn't in the Import Map, don't reference it\", \"Use the exact directory names from 'Project Directory Structure' — do NOT rename directories (e.g., if the directory is called 'cli', write 'src/cli/', NOT 'src/commands/')\", \"Cross-module references must use the specifier format from actual import statements (e.g., '../generation/writers/sum.js', NOT '../fs/sum-file.js')\", \"If you are unsure about a path, omit the cross-reference rather than guessing\".\n\n### Directory Output Format\nDIRECTORY_SYSTEM_PROMPT requires: \"Output ONLY the raw markdown content. No code fences, no preamble, no explanations, no conversational text. Your entire response IS the AGENTS.md file content.\", \"First line MUST be exactly: `<!-- Generated by agents-reverse-engineer -->`\".\n\n### Incremental Update Preservation\nFILE_UPDATE_SYSTEM_PROMPT mandates: \"Preserve the structure, section headings, and phrasing of the existing summary wherever the underlying code is unchanged\", \"Only modify content that is directly affected by the code changes\", \"If a section describes code that has not changed, keep it VERBATIM — do not rephrase, reorganize, or 'improve' stable text\", \"Regex patterns, format strings, magic constants, and template content from the existing summary MUST be preserved verbatim unless the source code changed them\", \"Never summarize or paraphrase regex patterns — always show the exact pattern in backticks\".\n\n### Root Synthesis Constraints\nROOT_SYSTEM_PROMPT enforces: \"Synthesize ONLY from the AGENTS.md content provided in the user prompt\", \"Do NOT invent, extrapolate, or hallucinate features, hooks, APIs, patterns, or dependencies not explicitly mentioned\", \"If information is missing, omit that section rather than guessing\", \"Every claim must be traceable to a specific AGENTS.md file provided\", \"Output ONLY the raw markdown content\", \"Do NOT include any conversational text, preamble, or meta-commentary\", \"Do NOT say 'Here is...' or 'I've generated...' — just output the document itself\".\n\n## Template Structure\n\nFILE_USER_PROMPT contains three interpolation regions: project structure tree embedded in `<project-structure>` tags, file path substitution via `{{FILE_PATH}}` placeholder, and source code injection via `{{CONTENT}}` placeholder within triple-backtick TypeScript code fence. Template concludes with instruction: \"Lead with a single bold purpose statement: **[FileName] does X.** Then use ## headings to organize the remaining content.\"\n\nDIRECTORY_SYSTEM_PROMPT section \"ADAPTIVE SECTIONS\" lists optional documentation sections: \"Contents (group files by purpose with markdown links and one-line descriptions)\", \"Subdirectories (list with links and summaries)\", \"Architecture / Data Flow (pipelines, request/response chains, layered architecture)\", \"Stack (technology stack for package roots with package.json/Cargo.toml/go.mod)\", \"Structure (layout conventions: feature-sliced, domain-driven, MVC)\", \"Patterns (factory, strategy, middleware, barrel re-export)\", \"Configuration (config files, schemas, environment definitions)\", \"API Surface (barrel index, route definitions, SDK interface contracts)\", \"File Relationships (collaboration, dependencies, shared state)\", \"Behavioral Contracts (regex patterns, format specifications, magic constants, template strings — MANDATORY when file summaries contain behavioral artifacts)\".\n\n## Integration Points\n\nTemplates consumed by `buildFilePrompt()` and `buildDirectoryPrompt()` in `src/generation/prompts/builder.ts`, which substitute placeholders with project context, source content, import maps, and directory structures. FILE_USER_PROMPT references project file tree embedded at build time. DIRECTORY_SYSTEM_PROMPT references \"Import Map\" and \"Project Directory Structure\" sections injected by prompt builder. ROOT_SYSTEM_PROMPT consumed by root document generation phase expecting aggregated AGENTS.md content from directory traversal.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 1824,
      "cacheReadTokens": 17114,
      "cacheCreationTokens": 13575,
      "latencyMs": 64547,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/generation/prompts/templates.ts",
          "sizeBytes": 16146
        }
      ]
    },
    {
      "timestamp": "2026-02-09T16:08:40.954Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/quality/density/validator.ts\n\n```typescript\n/**\n * Heuristic findability validation for AGENTS.md content.\n *\n * validateFindability checks that key exported symbols from .sum files\n * appear in the parent AGENTS.md content.\n * No LLM calls -- purely string-based symbol matching.\n *\n * NOTE: Previously relied on metadata.publicInterface which has been removed.\n * This module is retained for future structured extraction support.\n */\n\nimport type { SumFileContent } from '../../generation/writers/sum.js';\n\n/**\n * Result of findability validation for a single .sum file.\n *\n * score ranges from 0 (no symbols found) to 1 (all symbols found).\n */\nexport interface FindabilityResult {\n  /** Path to the .sum file that was checked */\n  filePath: string;\n  /** Symbol names that were tested for presence */\n  symbolsTested: string[];\n  /** Symbol names found in AGENTS.md content */\n  symbolsFound: string[];\n  /** Symbol names missing from AGENTS.md content */\n  symbolsMissing: string[];\n  /** Ratio of found to tested (0-1) */\n  score: number;\n}\n\n/**\n * Check that key symbols from .sum files appear in AGENTS.md content.\n *\n * Currently returns an empty array since structured metadata extraction\n * (publicInterface) has been removed. The function signature is preserved\n * for future re-implementation via post-processing passes.\n *\n * @param _agentsMdContent - Full text content of the AGENTS.md file\n * @param _sumFiles - Map of file path to parsed SumFileContent\n * @returns Empty array (no structured symbols to validate)\n */\nexport function validateFindability(\n  _agentsMdContent: string,\n  _sumFiles: Map<string, SumFileContent>,\n): FindabilityResult[] {\n  return [];\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**validator.ts provides a disabled stub for findability validation that previously checked whether exported symbols from .sum files appeared in parent AGENTS.md content, retained for future structured metadata extraction support.**\n\n## Exported Interface\n\n**FindabilityResult** - Validation result for a single .sum file containing:\n- `filePath: string` - Path to the validated .sum file\n- `symbolsTested: string[]` - Symbol names tested for presence\n- `symbolsFound: string[]` - Symbol names found in AGENTS.md content\n- `symbolsMissing: string[]` - Symbol names absent from AGENTS.md content\n- `score: number` - Ratio of found to tested symbols (0.0 to 1.0 range)\n\n## Exported Function\n\n**validateFindability** - `(agentsMdContent: string, sumFiles: Map<string, SumFileContent>) => FindabilityResult[]`\n\nReturns empty array since structured metadata extraction removed. Previously validated that key symbols from .sum files appeared in AGENTS.md by:\n1. Extracting symbol names from `metadata.publicInterface` field in `SumFileContent`\n2. Searching for symbol presence in `agentsMdContent` via substring matching\n3. Computing per-file `FindabilityResult` with score calculation\n\nParameters ignored (prefixed with underscore): `_agentsMdContent` consumed full AGENTS.md text, `_sumFiles` consumed map of file paths to parsed `SumFileContent` objects.\n\n## Integration Context\n\n**Disabled Feature** - Function signature preserved but implementation gutted after `SumFileContent.metadata.publicInterface` field removed from schema (`src/generation/writers/sum.ts`). No callers in codebase invoke this validator (disabled in `src/quality/index.ts` quality validation pipeline).\n\n**Historical Design** - LLM-free validation using string-based symbol matching. No AI subprocess calls, purely heuristic checking via substring search. Intended to detect when directory-level AGENTS.md aggregation failed to preserve critical symbol names from child .sum file summaries.\n\n**Restoration Path** - Future re-implementation requires:\n1. Adding structured export extraction to .sum file generation (Phase 1 of three-phase pipeline)\n2. Parsing YAML frontmatter in .sum files via `src/generation/writers/sum.ts:readSumFile()`\n3. Implementing symbol presence checks against AGENTS.md content\n4. Re-enabling in `src/quality/index.ts` quality validator orchestrator\n\n## Type Dependencies\n\nImports `SumFileContent` from `src/generation/writers/sum.ts` - Interface for parsed .sum file containing YAML frontmatter (`generated_at`, `content_hash`, `purpose`, `critical_todos[]`, `related_files[]`) plus markdown summary content.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 643,
      "cacheReadTokens": 17114,
      "cacheCreationTokens": 9552,
      "latencyMs": 29147,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/quality/density/validator.ts",
          "sizeBytes": 1637
        }
      ]
    },
    {
      "timestamp": "2026-02-09T16:08:46.214Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/quality/phantom-paths/index.ts\n\n```typescript\nexport { checkPhantomPaths } from './validator.js';\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**Re-exports `checkPhantomPaths` validator for phantom path detection in generated documentation.**\n\n## Exported Interface\n\n- `checkPhantomPaths` — Re-exported from `./validator.js`, function implementing phantom path validation for `AGENTS.md` files\n\n## Module Purpose\n\nBarrel export consolidating phantom path validation functionality. Provides single import point for consumers needing to detect unresolvable file path references embedded in generated documentation.\n\n## Integration Context\n\nPart of quality validation subsystem (`src/quality/`) alongside code-vs-doc consistency checking (`src/quality/inconsistency/code-vs-doc.ts`) and code-vs-code duplicate detection (`src/quality/inconsistency/code-vs-code.ts`). Invoked during post-generation validation phase to identify broken path references that may mislead AI coding assistants.\n\n## Implementation Location\n\nCore validation logic resides in `./validator.ts` which extracts path-like strings from `AGENTS.md` using three regex patterns:\n- Markdown link targets: `/\\[(?:[^\\]]*)\\]\\((\\.[^)]+)\\)/g`\n- Backtick-quoted paths: `` /`((?:src\\/|\\.\\.?\\/)[^`]+\\.[a-z]{1,4})`/g ``\n- Prose-embedded paths: `/(?:from|in|by|via|see)\\s+`?(src\\/[\\w\\-./]+)`?/gi`\n\nResolves paths against AGENTS.md directory and project root with `.ts`/`.js` extension fallback, reporting `PhantomPathInconsistency` for unresolved references.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 378,
      "cacheReadTokens": 17114,
      "cacheCreationTokens": 9113,
      "latencyMs": 26341,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/quality/phantom-paths/index.ts",
          "sizeBytes": 52
        }
      ]
    },
    {
      "timestamp": "2026-02-09T16:08:38.512Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/generation/writers/agents-md.ts\n\n```typescript\nimport { writeFile, readFile, mkdir, rename } from 'node:fs/promises';\nimport * as path from 'node:path';\n\n/** Marker comment to identify generated AGENTS.md files */\nexport const GENERATED_MARKER = '<!-- Generated by agents-reverse-engineer -->';\n\n/**\n * Check if an AGENTS.md file was generated by us (contains marker).\n */\nexport async function isGeneratedAgentsMd(filePath: string): Promise<boolean> {\n  try {\n    const content = await readFile(filePath, 'utf-8');\n    return content.includes(GENERATED_MARKER);\n  } catch {\n    return false;\n  }\n}\n\n/**\n * Write AGENTS.md for a directory with LLM-generated content.\n *\n * If a user-defined AGENTS.md exists (not generated by us), it will be\n * preserved as AGENTS.local.md. The user content is then prepended\n * verbatim above the generated content so AI agents see it first.\n *\n * @param dirPath - Directory to write AGENTS.md for\n * @param projectRoot - Project root for relative paths\n * @param content - LLM-generated AGENTS.md content\n * @returns Path to written AGENTS.md\n */\nexport async function writeAgentsMd(\n  dirPath: string,\n  _projectRoot: string,\n  content: string,\n): Promise<string> {\n  const agentsPath = path.join(dirPath, 'AGENTS.md');\n  const localPath = path.join(dirPath, 'AGENTS.local.md');\n\n  // Step 1: Preserve user-authored AGENTS.md (rename to AGENTS.local.md)\n  let userContent: string | null = null;\n  try {\n    const existingContent = await readFile(agentsPath, 'utf-8');\n    if (!existingContent.includes(GENERATED_MARKER)) {\n      userContent = existingContent;\n      await rename(agentsPath, localPath);\n    }\n  } catch {\n    // No existing AGENTS.md\n  }\n\n  // Step 2: Check for already-renamed AGENTS.local.md (from a previous run)\n  if (!userContent) {\n    try {\n      userContent = await readFile(localPath, 'utf-8');\n    } catch {\n      // No AGENTS.local.md either\n    }\n  }\n\n  // Step 3: Strip marker from LLM content (we add it ourselves)\n  let llmContent = content;\n  if (llmContent.startsWith(GENERATED_MARKER)) {\n    llmContent = llmContent.slice(GENERATED_MARKER.length).replace(/^\\n+/, '');\n  }\n\n  // Step 4: Build final content with marker + optional user content + LLM content\n  const parts: string[] = [GENERATED_MARKER, ''];\n\n  if (userContent?.trim()) {\n    parts.push(\n      '<!-- User-defined AGENTS.md preserved as AGENTS.local.md -->',\n      '',\n      userContent.trim(),\n      '',\n      '---',\n      '',\n    );\n  }\n\n  parts.push(llmContent);\n\n  const finalContent = parts.join('\\n');\n\n  await mkdir(path.dirname(agentsPath), { recursive: true });\n  await writeFile(agentsPath, finalContent, 'utf-8');\n\n  return agentsPath;\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**agents-md.ts manages `AGENTS.md` file lifecycle during directory aggregation, preserving user-authored content by renaming existing files to `AGENTS.local.md` and prepending them above generated documentation with marker-based ownership detection.**\n\n## Exported Functions\n\n**writeAgentsMd(dirPath: string, _projectRoot: string, content: string): Promise<string>**\nWrites `AGENTS.md` for a directory with LLM-generated content, returns path to written file. Executes four-step preservation workflow: (1) reads existing `AGENTS.md` at `dirPath`, checks for `GENERATED_MARKER` absence to detect user authorship, renames to `AGENTS.local.md` via `rename()` if user-authored; (2) if no user content found in step 1, attempts to read already-renamed `AGENTS.local.md` from previous run; (3) strips `GENERATED_MARKER` prefix from incoming `content` parameter via `slice()` + regex `/^\\n+/` to normalize LLM output; (4) constructs final content with structure: `GENERATED_MARKER` → newline → optional user content block with `<!-- User-defined AGENTS.md preserved as AGENTS.local.md -->` header + `---` separator → LLM content, writes to `AGENTS.md` via `writeFile()` with `mkdir()` ensuring parent directory exists. Parameter `_projectRoot` unused (underscore-prefixed per linter convention). Called by Phase 2 orchestrator (`src/generation/orchestrator.ts`) during post-order directory aggregation after child `.sum` files processed.\n\n**isGeneratedAgentsMd(filePath: string): Promise<boolean>**\nDetects whether `AGENTS.md` at `filePath` was generated by ARE via substring search for `GENERATED_MARKER` in file content read via `readFile()`. Returns `false` on read errors (file not found, permission denied) via empty catch block. Used by `writeAgentsMd()` to determine preservation logic and by cleanup commands (`src/cli/clean.ts`) to distinguish generated vs. user-authored files for selective deletion.\n\n## Marker Constant\n\n**GENERATED_MARKER: string**\nHTML comment string `'<!-- Generated by agents-reverse-engineer -->'` injected at top of all generated `AGENTS.md` files. Serves as ownership identifier for idempotent regeneration: presence indicates generated content safe to overwrite, absence indicates user authorship requiring preservation via `AGENTS.local.md` rename. Pattern avoids YAML frontmatter to maintain Markdown readability and compatibility with viewers that don't parse frontmatter.\n\n## User Content Preservation Strategy\n\nUser-authored `AGENTS.md` files (lacking `GENERATED_MARKER`) renamed to `AGENTS.local.md` on first `writeAgentsMd()` invocation. Subsequent runs read `AGENTS.local.md` directly without re-checking original path, preventing repeated file system operations. Preserved content prepended above generated content with visual separator (`---`) ensuring AI assistants encounter project-specific context before auto-generated summaries. If `AGENTS.local.md` exists from previous run and user deletes/modifies `AGENTS.md` in interim, the `AGENTS.local.md` version takes precedence (step 2 fallback logic).\n\n## Output Structure\n\nFinal `AGENTS.md` format:\n```\n<!-- Generated by agents-reverse-engineer -->\n\n<!-- User-defined AGENTS.md preserved as AGENTS.local.md -->\n\n[user content verbatim with leading/trailing whitespace trimmed]\n\n---\n\n[LLM-generated content with GENERATED_MARKER prefix stripped]\n```\n\nIf no user content exists, omits comment header and separator, outputs:\n```\n<!-- Generated by agents-reverse-engineer -->\n\n[LLM-generated content]\n```\n\n## Integration Points\n\n- **src/generation/orchestrator.ts**: Calls `writeAgentsMd()` in Phase 2 (`runDirectoryGeneration`) after aggregating child `.sum` files and subdirectory `AGENTS.md` files via `collectAgentsDocs()`, passes `dirPath` from sorted directory list and `content` from `AIService.call()` response\n- **src/cli/clean.ts**: Calls `isGeneratedAgentsMd()` to filter deletion targets, skips user-authored files, restores `AGENTS.local.md` → `AGENTS.md` after removing generated version\n- **src/update/orphan-cleaner.ts**: Uses `isGeneratedAgentsMd()` in `cleanupEmptyDirectoryDocs()` to determine whether to delete `AGENTS.md` from directories with no remaining source files\n\n## Error Handling\n\nBoth functions use empty catch blocks with fallback behavior: `isGeneratedAgentsMd()` returns `false` on read errors (treats unreadable files as non-generated), `writeAgentsMd()` treats read errors as \"no existing file\" and proceeds with writing marker + LLM content only. No explicit error logging since failures indicate normal conditions (file not found on first run). Parent orchestrator (`src/generation/executor.ts`) captures write errors during task execution and reports via progress logger.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 1206,
      "cacheReadTokens": 17114,
      "cacheCreationTokens": 9933,
      "latencyMs": 34970,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/generation/writers/agents-md.ts",
          "sizeBytes": 2636
        }
      ]
    },
    {
      "timestamp": "2026-02-09T16:08:03.274Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/generation/prompts/builder.ts\n\n```typescript\nimport * as path from 'node:path';\nimport { readdir, readFile } from 'node:fs/promises';\nimport pc from 'picocolors';\nimport type { PromptContext } from './types.js';\nimport { FILE_SYSTEM_PROMPT, FILE_USER_PROMPT, FILE_UPDATE_SYSTEM_PROMPT, DIRECTORY_SYSTEM_PROMPT, DIRECTORY_UPDATE_SYSTEM_PROMPT, ROOT_SYSTEM_PROMPT } from './templates.js';\nimport { readSumFile, getSumPath } from '../writers/sum.js';\nimport { GENERATED_MARKER } from '../writers/agents-md.js';\nimport { extractDirectoryImports, formatImportMap } from '../../imports/index.js';\nimport { collectAgentsDocs } from '../collector.js';\n\nfunction logTemplate(debug: boolean, action: string, filePath: string, extra?: string): void {\n  if (!debug) return;\n  const rel = path.relative(process.cwd(), filePath);\n  const msg = `${pc.dim('[prompt]')} ${pc.cyan(action)} ${pc.dim('→')} ${rel}`;\n  console.error(extra ? `${msg} ${pc.dim(extra)}` : msg);\n}\n\n/**\n * Detect language from file extension for syntax highlighting.\n */\nexport function detectLanguage(filePath: string): string {\n  const ext = path.extname(filePath).toLowerCase();\n  const langMap: Record<string, string> = {\n    '.ts': 'typescript',\n    '.tsx': 'tsx',\n    '.js': 'javascript',\n    '.jsx': 'jsx',\n    '.py': 'python',\n    '.rb': 'ruby',\n    '.go': 'go',\n    '.rs': 'rust',\n    '.java': 'java',\n    '.kt': 'kotlin',\n    '.swift': 'swift',\n    '.cs': 'csharp',\n    '.php': 'php',\n    '.vue': 'vue',\n    '.svelte': 'svelte',\n    '.json': 'json',\n    '.yaml': 'yaml',\n    '.yml': 'yaml',\n    '.md': 'markdown',\n    '.css': 'css',\n    '.scss': 'scss',\n    '.html': 'html',\n  };\n  return langMap[ext] ?? 'text';\n}\n\n/**\n * Build a complete prompt for file analysis.\n */\nexport function buildFilePrompt(context: PromptContext, debug = false): {\n  system: string;\n  user: string;\n} {\n  const lang = detectLanguage(context.filePath);\n  logTemplate(debug, 'buildFilePrompt', context.filePath, `lang=${lang}`);\n\n  const planSection = context.projectPlan\n    ? `\\n\\n## Project Structure\\n\\nFull project file listing for context:\\n\\n<project-structure>\\n${context.projectPlan}\\n</project-structure>`\n    : '';\n\n  let userPrompt = FILE_USER_PROMPT\n    .replace(/\\{\\{FILE_PATH\\}\\}/g, context.filePath)\n    .replace(/\\{\\{CONTENT\\}\\}/g, context.content)\n    .replace(/\\{\\{LANG\\}\\}/g, lang)\n    .replace(/\\{\\{PROJECT_PLAN_SECTION\\}\\}/g, planSection);\n\n  // Add context files if provided\n  if (context.contextFiles && context.contextFiles.length > 0) {\n    const contextSection = context.contextFiles\n      .map(\n        (f) =>\n          `\\n### ${f.path}\\n\\`\\`\\`${detectLanguage(f.path)}\\n${f.content}\\n\\`\\`\\``\n      )\n      .join('\\n');\n    userPrompt += `\\n\\n## Related Files\\n${contextSection}`;\n  }\n\n  // For incremental updates: include existing summary and use update-specific system prompt\n  if (context.existingSum) {\n    userPrompt += `\\n\\n## Existing Summary (update this — preserve stable content, modify only what changed)\\n\\n${context.existingSum}`;\n    return {\n      system: FILE_UPDATE_SYSTEM_PROMPT,\n      user: userPrompt,\n    };\n  }\n\n  return {\n    system: FILE_SYSTEM_PROMPT,\n    user: userPrompt,\n  };\n}\n\n/**\n * Build a prompt for generating a directory-level AGENTS.md.\n *\n * Reads all .sum files in the directory, child AGENTS.md files,\n * and AGENTS.local.md to provide full context to the LLM.\n */\nexport async function buildDirectoryPrompt(\n  dirPath: string,\n  projectRoot: string,\n  debug = false,\n  knownDirs?: Set<string>,\n  projectStructure?: string,\n  existingAgentsMd?: string,\n): Promise<{ system: string; user: string }> {\n  const relativePath = path.relative(projectRoot, dirPath) || '.';\n  const dirName = path.basename(dirPath) || 'root';\n\n  // Collect .sum file summaries and subdirectory sections in parallel\n  const entries = await readdir(dirPath, { withFileTypes: true });\n\n  const fileEntries = entries.filter(\n    (e) => e.isFile() && !e.name.endsWith('.sum') && !e.name.startsWith('.'),\n  );\n  const dirEntries = entries.filter((e) => {\n    if (!e.isDirectory()) return false;\n    if (!knownDirs) return true;\n    const relDir = path.relative(projectRoot, path.join(dirPath, e.name));\n    return knownDirs.has(relDir);\n  });\n\n  // Read all .sum files in parallel\n  const fileResults = await Promise.all(\n    fileEntries.map(async (entry) => {\n      const entryPath = path.join(dirPath, entry.name);\n      const sumPath = getSumPath(entryPath);\n      const sumContent = await readSumFile(sumPath);\n      if (sumContent) {\n        return `### ${entry.name}\\n**Purpose:** ${sumContent.metadata.purpose}\\n\\n${sumContent.summary}`;\n      }\n      return null;\n    }),\n  );\n  const fileSummaries = fileResults.filter((r): r is string => r !== null);\n\n  // Read all child AGENTS.md in parallel\n  const subdirResults = await Promise.all(\n    dirEntries.map(async (entry) => {\n      const childAgentsPath = path.join(dirPath, entry.name, 'AGENTS.md');\n      try {\n        const childContent = await readFile(childAgentsPath, 'utf-8');\n        return `### ${entry.name}/\\n${childContent}`;\n      } catch {\n        if (debug) {\n          console.error(pc.dim(`[prompt] Skipping missing ${childAgentsPath}`));\n        }\n        return null;\n      }\n    }),\n  );\n  const subdirSections = subdirResults.filter((r): r is string => r !== null);\n\n  // Check for user-defined documentation: AGENTS.local.md or non-ARE AGENTS.md\n  let localSection = '';\n  try {\n    const localContent = await readFile(path.join(dirPath, 'AGENTS.local.md'), 'utf-8');\n    localSection = `\\n## User Notes (AGENTS.local.md)\\n\\n${localContent}\\n\\nNote: Reference [AGENTS.local.md](./AGENTS.local.md) for additional documentation.`;\n  } catch {\n    // No AGENTS.local.md — check if current AGENTS.md is user-authored (first run)\n    try {\n      const agentsContent = await readFile(path.join(dirPath, 'AGENTS.md'), 'utf-8');\n      if (!agentsContent.includes(GENERATED_MARKER)) {\n        localSection = `\\n## User Notes (existing AGENTS.md)\\n\\n${agentsContent}\\n\\nNote: This user-defined content will be preserved as [AGENTS.local.md](./AGENTS.local.md).`;\n      }\n    } catch {\n      // No AGENTS.md either\n    }\n  }\n\n  // Detect manifest files to hint at package root\n  const manifestNames = ['package.json', 'Cargo.toml', 'go.mod', 'pyproject.toml', 'pom.xml', 'build.gradle', 'Gemfile', 'composer.json', 'CMakeLists.txt', 'Makefile'];\n  const foundManifests = fileEntries\n    .filter((e) => manifestNames.includes(e.name))\n    .map((e) => e.name);\n\n  // Extract actual import statements for cross-reference accuracy\n  const sourceExtensions = /\\.(ts|tsx|js|jsx|py|go|rs|java|kt)$/;\n  const sourceFileNames = fileEntries\n    .filter((e) => sourceExtensions.test(e.name))\n    .map((e) => e.name);\n\n  const fileImports = await extractDirectoryImports(dirPath, sourceFileNames);\n  const importMapText = formatImportMap(fileImports);\n\n  logTemplate(debug, 'buildDirectoryPrompt', dirPath, `files=${fileSummaries.length} subdirs=${subdirSections.length} imports=${fileImports.length}`);\n\n  const userSections: string[] = [\n    `Generate AGENTS.md for directory: \"${relativePath}\" (${dirName})`,\n    '',\n    `## File Summaries (${fileSummaries.length} files)`,\n    '',\n    ...fileSummaries,\n  ];\n\n  if (importMapText) {\n    userSections.push(\n      '',\n      '## Import Map (verified — use these exact paths)',\n      '',\n      importMapText,\n    );\n  }\n\n  if (projectStructure) {\n    userSections.push(\n      '',\n      '## Project Directory Structure',\n      '',\n      '<project-structure>',\n      projectStructure,\n      '</project-structure>',\n    );\n  }\n\n  if (subdirSections.length > 0) {\n    userSections.push('', '## Subdirectories', '', ...subdirSections);\n  }\n\n  if (foundManifests.length > 0) {\n    userSections.push('', '## Directory Hints', '', `Contains manifest file(s): ${foundManifests.join(', ')} — likely a package or project root.`);\n  }\n\n  if (localSection) {\n    userSections.push(localSection);\n  }\n\n  // For incremental updates: include existing AGENTS.md and use update-specific system prompt\n  if (existingAgentsMd) {\n    userSections.push(\n      '',\n      '## Existing AGENTS.md (update this — preserve stable content, modify only what changed)',\n      '',\n      existingAgentsMd,\n    );\n    return {\n      system: DIRECTORY_UPDATE_SYSTEM_PROMPT,\n      user: userSections.join('\\n'),\n    };\n  }\n\n  return {\n    system: DIRECTORY_SYSTEM_PROMPT,\n    user: userSections.join('\\n'),\n  };\n}\n\n/**\n * Build a prompt for generating the root CLAUDE.md document.\n *\n * Collects all generated AGENTS.md files and optional package.json,\n * embedding them directly in the prompt so the LLM does not need\n * tool access to read files.\n */\nexport async function buildRootPrompt(\n  projectRoot: string,\n  debug = false,\n): Promise<{ system: string; user: string }> {\n  // 1. Collect all AGENTS.md files via shared collector\n  const agentsDocs = await collectAgentsDocs(projectRoot);\n  const agentsSections: string[] = agentsDocs.map(\n    (doc) => `### ${doc.relativePath}\\n\\n${doc.content}`,\n  );\n\n  // 3. Read root package.json for project metadata\n  let packageSection = '';\n  try {\n    const pkgRaw = await readFile(path.join(projectRoot, 'package.json'), 'utf-8');\n    const pkg = JSON.parse(pkgRaw) as Record<string, unknown>;\n    const parts: string[] = [];\n    if (pkg.name) parts.push(`- **Name**: ${pkg.name}`);\n    if (pkg.version) parts.push(`- **Version**: ${pkg.version}`);\n    if (pkg.description) parts.push(`- **Description**: ${pkg.description}`);\n    if (pkg.packageManager) parts.push(`- **Package Manager**: ${pkg.packageManager}`);\n    if (pkg.scripts && typeof pkg.scripts === 'object') {\n      const scripts = Object.entries(pkg.scripts as Record<string, string>)\n        .map(([k, v]) => `  - \\`${k}\\`: \\`${v}\\``)\n        .join('\\n');\n      parts.push(`- **Scripts**:\\n${scripts}`);\n    }\n    if (parts.length > 0) {\n      packageSection = `\\n## Package Metadata (package.json)\\n\\n${parts.join('\\n')}`;\n    }\n  } catch {\n    // No package.json or parse error\n  }\n\n  logTemplate(debug, 'buildRootPrompt', projectRoot, `agents=${agentsSections.length}`);\n\n  const userSections: string[] = [\n    'Generate CLAUDE.md for the project root.',\n    '',\n    'Synthesize the following AGENTS.md files into a single comprehensive project overview document.',\n    'Use ONLY the information provided below. Do NOT invent features, hooks, patterns, or APIs that are not explicitly mentioned in the AGENTS.md content.',\n    '',\n    `## AGENTS.md Files (${agentsSections.length} directories)`,\n    '',\n    ...agentsSections,\n  ];\n\n  if (packageSection) {\n    userSections.push(packageSection);\n  }\n\n  userSections.push(\n    '',\n    '## Output Requirements',\n    '',\n    'The document MUST include:',\n    '- Project purpose and description',\n    '- Architecture overview with directory structure',\n    '- Key directories table',\n    '- Getting started (install, build, run commands)',\n    '- Key technologies and dependencies',\n    '',\n    'This document is the COMPREHENSIVE reference for the entire project.',\n    'It should contain architecture, configuration, build instructions, and project-wide patterns.',\n    'Individual file details belong in directory-level AGENTS.md files — reference them, don\\'t duplicate them.',\n    '',\n    'Output ONLY the markdown content. No preamble.',\n  );\n\n  return {\n    system: ROOT_SYSTEM_PROMPT,\n    user: userSections.join('\\n'),\n  };\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**buildFilePrompt, buildDirectoryPrompt, buildRootPrompt construct system+user prompt pairs for file analysis, directory aggregation, and root synthesis by assembling templates with runtime context (source code, .sum summaries, AGENTS.md, import maps, project structure).**\n\n## Exported Functions\n\n**buildFilePrompt(context: PromptContext, debug = false): { system: string; user: string }**  \nConstructs file analysis prompts by substituting `{{FILE_PATH}}`, `{{CONTENT}}`, `{{LANG}}`, `{{PROJECT_PLAN_SECTION}}` placeholders in `FILE_USER_PROMPT` template. Detects language via `detectLanguage()` for syntax highlighting. Appends contextFiles section if `context.contextFiles` populated. Returns `FILE_UPDATE_SYSTEM_PROMPT` when `context.existingSum` present (incremental update mode), otherwise `FILE_SYSTEM_PROMPT`.\n\n**buildDirectoryPrompt(dirPath: string, projectRoot: string, debug = false, knownDirs?: Set<string>, projectStructure?: string, existingAgentsMd?: string): Promise<{ system: string; user: string }>**  \nAssembles directory aggregation prompts by reading all `.sum` files via `readSumFile(getSumPath(entryPath))`, collecting child `AGENTS.md` files, extracting directory imports via `extractDirectoryImports()` + `formatImportMap()`, detecting manifest files (9 types: `package.json`, `Cargo.toml`, `go.mod`, `pyproject.toml`, `pom.xml`, `build.gradle`, `Gemfile`, `composer.json`, `CMakeLists.txt`, `Makefile`), and checking for `AGENTS.local.md` or non-generated `AGENTS.md` (lacking `GENERATED_MARKER`). Returns `DIRECTORY_UPDATE_SYSTEM_PROMPT` when `existingAgentsMd` supplied (incremental), otherwise `DIRECTORY_SYSTEM_PROMPT`. Filters subdirectories against `knownDirs` set when provided to skip non-source directories. Embeds `projectStructure` in `<project-structure>` tags when available.\n\n**buildRootPrompt(projectRoot: string, debug = false): Promise<{ system: string; user: string }>**  \nConstructs root synthesis prompts by collecting all `AGENTS.md` files via `collectAgentsDocs(projectRoot)`, parsing root `package.json` for metadata (name, version, description, packageManager, scripts), and embedding synthesis constraints: \"Use ONLY the information provided below. Do NOT invent features, hooks, patterns, or APIs that are not explicitly mentioned in the AGENTS.md content.\" Returns `ROOT_SYSTEM_PROMPT` paired with user prompt containing all AGENTS.md content as `### ${relativePath}` sections, package metadata, and output requirements checklist (purpose, architecture, directories table, getting started, technologies).\n\n**detectLanguage(filePath: string): string**  \nMaps file extensions to syntax highlighting identifiers via lookup table. Supports 22 extensions: `.ts` → `'typescript'`, `.tsx` → `'tsx'`, `.js` → `'javascript'`, `.jsx` → `'jsx'`, `.py` → `'python'`, `.rb` → `'ruby'`, `.go` → `'go'`, `.rs` → `'rust'`, `.java` → `'java'`, `.kt` → `'kotlin'`, `.swift` → `'swift'`, `.cs` → `'csharp'`, `.php` → `'php'`, `.vue` → `'vue'`, `.svelte` → `'svelte'`, `.json` → `'json'`, `.yaml`/`.yml` → `'yaml'`, `.md` → `'markdown'`, `.css` → `'css'`, `.scss` → `'scss'`, `.html` → `'html'`. Defaults to `'text'` for unknown extensions.\n\n## Internal Helpers\n\n**logTemplate(debug: boolean, action: string, filePath: string, extra?: string): void**  \nEmits debug logs to stderr with format: `[prompt] ${action} → ${relativePath} ${extra}`. Only executes when `debug=true`. Uses picocolors for formatting: `pc.dim()` for brackets/arrows, `pc.cyan()` for action labels.\n\n## Integration Points\n\nImports template constants from `./templates.js`: `FILE_SYSTEM_PROMPT`, `FILE_USER_PROMPT`, `FILE_UPDATE_SYSTEM_PROMPT`, `DIRECTORY_SYSTEM_PROMPT`, `DIRECTORY_UPDATE_SYSTEM_PROMPT`, `ROOT_SYSTEM_PROMPT`. Calls `readSumFile()` and `getSumPath()` from `../writers/sum.js` to retrieve `.sum` file metadata+content. References `GENERATED_MARKER` from `../writers/agents-md.js` to detect user-authored vs. generated AGENTS.md. Invokes `extractDirectoryImports()` + `formatImportMap()` from `../../imports/index.js` to build verified import maps. Calls `collectAgentsDocs(projectRoot)` from `../collector.js` to aggregate all AGENTS.md files recursively.\n\n## Behavioral Contracts\n\n**Source file extension regex for import extraction:**  \n`/\\.(ts|tsx|js|jsx|py|go|rs|java|kt)$/` — only files matching these extensions processed by `extractDirectoryImports()`.\n\n**Manifest file names for package root detection:**  \n`['package.json', 'Cargo.toml', 'go.mod', 'pyproject.toml', 'pom.xml', 'build.gradle', 'Gemfile', 'composer.json', 'CMakeLists.txt', 'Makefile']` — presence triggers \"likely a package or project root\" hint in directory prompts.\n\n**File summary format in directory prompts:**  \n```\n### ${fileName}\n**Purpose:** ${metadata.purpose}\n\n${summaryContent}\n```\n\n**Subdirectory section format:**  \n```\n### ${dirName}/\n${childAgentsMdContent}\n```\n\n**User notes section header:**  \n`\"## User Notes (AGENTS.local.md)\"` when AGENTS.local.md exists, `\"## User Notes (existing AGENTS.md)\"` when pre-existing non-generated AGENTS.md detected on first run.\n\n**Incremental update instruction appended to prompts:**  \n`\"## Existing Summary (update this — preserve stable content, modify only what changed)\"` for file prompts with `context.existingSum`, `\"## Existing AGENTS.md (update this — preserve stable content, modify only what changed)\"` for directory prompts with `existingAgentsMd`.\n\n**Root prompt synthesis constraint:**  \n`\"Use ONLY the information provided below. Do NOT invent features, hooks, patterns, or APIs that are not explicitly mentioned in the AGENTS.md content.\"` — enforces faithful synthesis without hallucination.\n\n**Root prompt output requirements:**  \n```\nThe document MUST include:\n- Project purpose and description\n- Architecture overview with directory structure\n- Key directories table\n- Getting started (install, build, run commands)\n- Key technologies and dependencies\n```\n\n**Root prompt de-duplication directive:**  \n`\"Individual file details belong in directory-level AGENTS.md files — reference them, don't duplicate them.\"` — prevents redundant file-level detail in root document.\n\n## Data Flow\n\n1. **File analysis:** `buildFilePrompt()` receives `PromptContext` (filePath, content, contextFiles, existingSum), detects language, substitutes template placeholders, appends related files section, returns system+user pair selecting update vs. fresh template based on `existingSum` presence.\n\n2. **Directory aggregation:** `buildDirectoryPrompt()` reads directory via `readdir()`, filters known directories via `knownDirs` set, reads `.sum` files in parallel via `Promise.all()`, reads child `AGENTS.md` files in parallel, extracts imports from source files matching `/\\.(ts|tsx|js|jsx|py|go|rs|java|kt)$/`, detects manifest files, checks for user documentation (AGENTS.local.md or non-generated AGENTS.md), assembles sections in order: file summaries, import map, project structure, subdirectories, directory hints, user notes, existing AGENTS.md (update mode), returns system+user pair with `DIRECTORY_UPDATE_SYSTEM_PROMPT` if `existingAgentsMd` supplied.\n\n3. **Root synthesis:** `buildRootPrompt()` calls `collectAgentsDocs()` to recursively gather all `AGENTS.md` files, reads root `package.json` and extracts metadata fields (name, version, description, packageManager, scripts), embeds all AGENTS.md content as `### ${relativePath}` sections, appends package metadata, includes synthesis constraints and output requirements checklist, returns `ROOT_SYSTEM_PROMPT` + user prompt.\n\n## Error Handling\n\nSilent fallback on missing files: `try/catch` blocks around `readFile()` for child AGENTS.md, AGENTS.local.md, existing AGENTS.md, package.json — logs skipped files to stderr when `debug=true` but proceeds without throwing. `readSumFile()` returns null on missing `.sum` files, filtered out via `.filter((r): r is string => r !== null)` type guard.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 2226,
      "cacheReadTokens": 17114,
      "cacheCreationTokens": 12771,
      "latencyMs": 71248,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/generation/prompts/builder.ts",
          "sizeBytes": 11422
        }
      ]
    },
    {
      "timestamp": "2026-02-09T16:08:40.561Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/generation/writers/sum.ts\n\n```typescript\nimport { writeFile, readFile, mkdir } from 'node:fs/promises';\nimport * as path from 'node:path';\nimport type { SummaryMetadata } from '../types.js';\n\n/**\n * Content structure for a .sum file.\n */\nexport interface SumFileContent {\n  /** Main summary text (detailed description) */\n  summary: string;\n  /** Extracted metadata */\n  metadata: SummaryMetadata;\n  /** Generation timestamp */\n  generatedAt: string;\n  /** SHA-256 hash of source file content (for change detection) */\n  contentHash: string;\n}\n\n/**\n * Parse a .sum file back into structured content.\n * Returns null if file doesn't exist or is invalid.\n */\nexport async function readSumFile(sumPath: string): Promise<SumFileContent | null> {\n  try {\n    const content = await readFile(sumPath, 'utf-8');\n    return parseSumFile(content);\n  } catch {\n    return null;\n  }\n}\n\n/**\n * Parse a YAML-style array from frontmatter.\n * Supports both inline [a, b, c] and multi-line formats.\n */\nfunction parseYamlArray(frontmatter: string, key: string): string[] {\n  // Try inline format first: key: [a, b, c]\n  const inlineMatch = frontmatter.match(new RegExp(`${key}:\\\\s*\\\\[([^\\\\]]*)\\\\]`));\n  if (inlineMatch) {\n    return inlineMatch[1]\n      .split(',')\n      .map(s => s.trim().replace(/^[\"']|[\"']$/g, ''))\n      .filter(s => s.length > 0);\n  }\n\n  // Try multi-line format:\n  // key:\n  //   - item1\n  //   - item2\n  const multiLineMatch = frontmatter.match(new RegExp(`${key}:\\\\s*\\\\n((?:\\\\s+-\\\\s+.+\\\\n?)+)`, 'm'));\n  if (multiLineMatch) {\n    return multiLineMatch[1]\n      .split('\\n')\n      .map(line => line.replace(/^\\s*-\\s*/, '').trim())\n      .filter(s => s.length > 0);\n  }\n\n  return [];\n}\n\n/**\n * Parse .sum file content into structured data.\n */\nfunction parseSumFile(content: string): SumFileContent | null {\n  try {\n    // Extract frontmatter\n    const frontmatterMatch = content.match(/^---\\n([\\s\\S]*?)\\n---\\n/);\n    if (!frontmatterMatch) return null;\n\n    const frontmatter = frontmatterMatch[1];\n    const summary = content.slice(frontmatterMatch[0].length).trim();\n\n    // Parse frontmatter (simple YAML-like parsing)\n    const generatedAt = frontmatter.match(/generated_at:\\s*(.+)/)?.[1]?.trim() ?? '';\n    const contentHash = frontmatter.match(/content_hash:\\s*(.+)/)?.[1]?.trim() ?? '';\n\n    // Parse purpose (single line value)\n    const purpose = frontmatter.match(/purpose:\\s*(.+)/)?.[1]?.trim() ?? '';\n\n    const metadata: SummaryMetadata = {\n      purpose,\n    };\n\n    // Parse optional fields\n    const criticalTodos = parseYamlArray(frontmatter, 'critical_todos');\n    if (criticalTodos.length > 0) {\n      metadata.criticalTodos = criticalTodos;\n    }\n\n    const relatedFiles = parseYamlArray(frontmatter, 'related_files');\n    if (relatedFiles.length > 0) {\n      metadata.relatedFiles = relatedFiles;\n    }\n\n    return {\n      summary,\n      metadata,\n      generatedAt,\n      contentHash,\n    };\n  } catch {\n    return null;\n  }\n}\n\n/**\n * Format a YAML array for frontmatter.\n * Uses inline format for short arrays, multi-line for longer ones.\n */\nfunction formatYamlArray(key: string, values: string[]): string {\n  if (values.length === 0) {\n    return `${key}: []`;\n  }\n  if (values.length <= 3 && values.every(v => v.length < 40)) {\n    // Inline format for short arrays\n    return `${key}: [${values.join(', ')}]`;\n  }\n  // Multi-line format for longer arrays\n  return `${key}:\\n${values.map(v => `  - ${v}`).join('\\n')}`;\n}\n\n/**\n * Format .sum file content for writing.\n */\nfunction formatSumFile(content: SumFileContent): string {\n  const lines = [\n    '---',\n    `generated_at: ${content.generatedAt}`,\n    `content_hash: ${content.contentHash}`,\n    `purpose: ${content.metadata.purpose}`,\n  ];\n\n  // Add optional fields if present\n  if (content.metadata.criticalTodos && content.metadata.criticalTodos.length > 0) {\n    lines.push(formatYamlArray('critical_todos', content.metadata.criticalTodos));\n  }\n  if (content.metadata.relatedFiles && content.metadata.relatedFiles.length > 0) {\n    lines.push(formatYamlArray('related_files', content.metadata.relatedFiles));\n  }\n\n  lines.push('---', '');\n\n  return lines.join('\\n') + content.summary;\n}\n\n/**\n * Write a .sum file alongside a source file.\n * Creates: foo.ts -> foo.ts.sum\n *\n * @param sourcePath - Path to the source file\n * @param content - Summary content to write\n * @returns Path to the written .sum file\n */\nexport async function writeSumFile(\n  sourcePath: string,\n  content: SumFileContent\n): Promise<string> {\n  const sumPath = `${sourcePath}.sum`;\n  const dir = path.dirname(sumPath);\n\n  // Ensure directory exists\n  await mkdir(dir, { recursive: true });\n\n  // Write file\n  const formatted = formatSumFile(content);\n  await writeFile(sumPath, formatted, 'utf-8');\n\n  return sumPath;\n}\n\n/**\n * Get the .sum path for a source file.\n */\nexport function getSumPath(sourcePath: string): string {\n  return `${sourcePath}.sum`;\n}\n\n/**\n * Check if a .sum file exists for a source file.\n */\nexport async function sumFileExists(sourcePath: string): Promise<boolean> {\n  const sumPath = getSumPath(sourcePath);\n  const content = await readSumFile(sumPath);\n  return content !== null;\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**sum.ts manages `.sum` file I/O with YAML frontmatter serialization, parsing, and path resolution for per-file documentation artifacts.**\n\n## Exported Types\n\n**SumFileContent** interface structures `.sum` file data with `summary: string` (main documentation text), `metadata: SummaryMetadata` (purpose/todos/related files), `generatedAt: string` (ISO 8601 timestamp), `contentHash: string` (SHA-256 hex digest for change detection).\n\n## File Format\n\n`.sum` files use YAML frontmatter delimited by `---` markers containing:\n- `generated_at: <ISO8601>` — timestamp\n- `content_hash: <sha256-hex>` — SHA-256 digest\n- `purpose: <single-line-string>` — one-line purpose statement\n- `critical_todos: [...]` or multi-line list — optional security/breaking issues\n- `related_files: [...]` or multi-line list — optional dependency paths\n\nFrontmatter ends with `---\\n` followed by markdown summary body.\n\n## Core Functions\n\n**writeSumFile(sourcePath: string, content: SumFileContent): Promise<string>** creates `.sum` file at `${sourcePath}.sum`, calls `mkdir(dir, {recursive: true})` to ensure parent directory exists, invokes `formatSumFile()` for serialization, returns written path.\n\n**readSumFile(sumPath: string): Promise<SumFileContent | null>** reads file via `readFile(sumPath, 'utf-8')`, delegates to `parseSumFile()`, returns `null` on missing file or parse failure.\n\n**getSumPath(sourcePath: string): string** returns `${sourcePath}.sum` path for given source file.\n\n**sumFileExists(sourcePath: string): Promise<boolean>** calls `readSumFile(getSumPath(sourcePath))` and returns `content !== null`.\n\n## Parsing Logic\n\n**parseSumFile(content: string): SumFileContent | null** extracts frontmatter via `/^---\\n([\\s\\S]*?)\\n---\\n/` regex, parses fields using `String.match()` with patterns:\n- `generated_at:\\s*(.+)` captures ISO timestamp\n- `content_hash:\\s*(.+)` captures SHA-256 hash\n- `purpose:\\s*(.+)` captures single-line purpose\n\nCalls `parseYamlArray(frontmatter, key)` for `critical_todos` and `related_files`, returns `null` on exception or missing frontmatter delimiter.\n\n**parseYamlArray(frontmatter: string, key: string): string[]** supports two formats:\n1. Inline: `key: [a, b, c]` matched via `/key:\\s*\\[([^\\]]*)\\]/`, splits on commas, trims quotes via `.replace(/^[\"']|[\"']$/g, '')`\n2. Multi-line: `key:\\n  - item1\\n  - item2` matched via `/key:\\s*\\n((?:\\s+-\\s+.+\\n?)+)/m`, extracts lines, strips `^\\s*-\\s*` prefix\n\nReturns empty array if neither pattern matches.\n\n## Serialization Logic\n\n**formatSumFile(content: SumFileContent): string** constructs frontmatter lines array with mandatory fields `generated_at`, `content_hash`, `purpose`, conditionally appends `formatYamlArray()` output for `critical_todos` and `related_files` if arrays non-empty, joins with `\\n`, appends delimiter `---\\n`, concatenates with `content.summary`.\n\n**formatYamlArray(key: string, values: string[]): string** chooses format by heuristics:\n- Empty: `key: []`\n- Inline: `key: [${values.join(', ')}]` if `values.length <= 3` and all items `< 40` chars\n- Multi-line: `key:\\n${values.map(v => '  - ' + v).join('\\n')}` otherwise\n\n## Integration Points\n\nUsed by `src/update/orphan-cleaner.ts` via `readSumFile()` for `content_hash` extraction during incremental updates. Called by `src/generation/executor.ts` Phase 1 workers via `writeSumFile()` after AI subprocess returns summary. Consumed by Phase 2 directory aggregation via `readSumFile()` to load child `.sum` files for `AGENTS.md` synthesis prompts.\n\n## Dependencies\n\nImports `writeFile`, `readFile`, `mkdir` from `node:fs/promises`, `path` from `node:path`, `SummaryMetadata` from `../types.js` (defines `purpose`, `criticalTodos?`, `relatedFiles?` fields).",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 1136,
      "cacheReadTokens": 17114,
      "cacheCreationTokens": 10786,
      "latencyMs": 34526,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/generation/writers/sum.ts",
          "sizeBytes": 5136
        }
      ]
    },
    {
      "timestamp": "2026-02-09T16:08:47.079Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/.github/workflows/publish.yml\n\n```yaml\nname: Publish to npm\n\non:\n  release:\n    types: [published]\n  workflow_dispatch:\n\njobs:\n  publish:\n    runs-on: ubuntu-latest\n    permissions:\n      contents: read\n      id-token: write\n    steps:\n      - name: Checkout\n        uses: actions/checkout@v4\n\n      - name: Setup Node.js\n        uses: actions/setup-node@v4\n        with:\n          node-version: '20'\n          registry-url: 'https://registry.npmjs.org'\n\n      - name: Install dependencies\n        run: npm ci\n\n      - name: Build\n        run: npm run build\n\n      - name: Publish to npm\n        run: npm publish --provenance --access public\n        env:\n          NODE_AUTH_TOKEN: ${{ secrets.NPM_TOKEN }}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**GitHub Actions workflow automating npm package publication with Sigstore provenance attestation on release events.**\n\n## Workflow Triggers\n\nExecutes on two event patterns:\n- `release[published]` event when GitHub release published\n- `workflow_dispatch` for manual triggering via Actions UI\n\n## Job Configuration\n\n`publish` job runs on `ubuntu-latest` with permission grants:\n- `contents: read` — checkout repository access\n- `id-token: write` — enables Sigstore-signed provenance generation\n\n## Build Pipeline Steps\n\n**Checkout Step:**\n- `actions/checkout@v4` clones repository at release commit SHA\n\n**Node.js Setup:**\n- `actions/setup-node@v4` installs Node.js version 20\n- Configures npm registry URL as `https://registry.npmjs.org` via `registry-url` parameter\n\n**Dependency Installation:**\n- `npm ci` performs clean install from package-lock.json\n\n**Build Execution:**\n- `npm run build` invokes TypeScript compilation and hook copying (triggers `prepublishOnly` script: `tsc` + `build:hooks` from `scripts/build-hooks.js`)\n\n**Publication:**\n- `npm publish --provenance --access public` publishes package with cryptographic attestation\n- `--provenance` flag generates Sigstore-signed build provenance linking published tarball to source commit SHA via SLSA attestation\n- `--access public` ensures scoped package visibility\n- `NODE_AUTH_TOKEN` environment variable sourced from `secrets.NPM_TOKEN` repository secret for npm registry authentication\n\n## Security Features\n\nCryptographic provenance attestation creates verifiable supply chain metadata:\n- Links published artifact to GitHub Actions workflow execution\n- Records commit SHA, workflow file path, repository coordinates\n- Signed via Sigstore's keyless signing using GitHub's OIDC token\n- Consumers verify authenticity via `npm audit signatures` or `sigstore-js` tooling",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 447,
      "cacheReadTokens": 17114,
      "cacheCreationTokens": 9292,
      "latencyMs": 29329,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": ".github/workflows/publish.yml",
          "sizeBytes": 668
        }
      ]
    },
    {
      "timestamp": "2026-02-09T16:08:45.325Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/quality/inconsistency/code-vs-code.ts\n\n```typescript\n/**\n * Heuristic cross-file inconsistency detection.\n *\n * Detects duplicate exports: the same symbol name exported from multiple\n * files within a group. The caller is responsible for scoping the file\n * list (typically per-directory) to avoid false positives across\n * unrelated modules.\n */\n\nimport { extractExports } from './code-vs-doc.js';\nimport type { CodeCodeInconsistency } from '../types.js';\n\n/**\n * Detect duplicate exports across a group of files.\n *\n * For each file, extracts exported symbol names via {@link extractExports}\n * and flags any symbol that appears in more than one file.\n *\n * This is a heuristic-only check (no AI calls). The caller should scope\n * the input to per-directory file groups to avoid false positives.\n *\n * @param files - Array of source files with path and content\n * @returns Array of detected cross-file inconsistencies\n */\nexport function checkCodeVsCode(\n  files: Array<{ path: string; content: string }>,\n): CodeCodeInconsistency[] {\n  // Map: export name -> list of file paths that export it\n  const exportMap = new Map<string, string[]>();\n\n  for (const file of files) {\n    const exports = extractExports(file.content);\n    for (const name of exports) {\n      const paths = exportMap.get(name);\n      if (paths) {\n        paths.push(file.path);\n      } else {\n        exportMap.set(name, [file.path]);\n      }\n    }\n  }\n\n  const inconsistencies: CodeCodeInconsistency[] = [];\n\n  for (const [name, paths] of exportMap) {\n    if (paths.length > 1) {\n      inconsistencies.push({\n        type: 'code-vs-code',\n        severity: 'warning',\n        files: paths,\n        description: `Symbol \"${name}\" exported from ${paths.length} files`,\n        pattern: 'duplicate-export',\n      });\n    }\n  }\n\n  return inconsistencies;\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**checkCodeVsCode() detects duplicate exports across source files by aggregating symbol names per file and flagging symbols exported from multiple files within a scoped group.**\n\n## Exported Functions\n\n**checkCodeVsCode(files: Array<{ path: string; content: string }>): CodeCodeInconsistency[]**\nDetects duplicate exports across file groups by extracting symbol names via `extractExports()` from `code-vs-doc.ts`, building a `Map<string, string[]>` where keys are export names and values are file paths, and returning inconsistency records for symbols appearing in multiple files. Each inconsistency has `type: 'code-vs-code'`, `severity: 'warning'`, `pattern: 'duplicate-export'`, `files` array of paths, and `description` string with count.\n\n## Dependencies\n\n**extractExports()** imported from `./code-vs-doc.js` performs regex-based symbol extraction using pattern `/^[ \\t]*export\\s+(?:default\\s+)?(?:function|class|const|let|var|type|interface|enum)\\s+(\\w+)/gm`.\n\n**CodeCodeInconsistency** type imported from `../types.js` represents cross-file inconsistencies with discriminator `type: 'code-vs-code'`.\n\n## Algorithm\n\nIterates `files` array, calls `extractExports(file.content)` to get symbol set, populates `exportMap` by appending `file.path` to existing symbol arrays or initializing new entries. Second pass filters `exportMap` entries where `paths.length > 1` and constructs inconsistency objects with formatted description `\"Symbol \\\"${name}\\\" exported from ${paths.length} files\"`.\n\n## Scoping Contract\n\nCaller must scope input to per-directory file groups to avoid false positives across unrelated modules. No AI calls or AST analysis—purely heuristic substring matching. Intentional duplication (e.g., factory patterns, parallel implementations) cannot be distinguished without semantic analysis.\n\n## Integration Points\n\nUsed by `reporter.ts` in quality validation pipeline alongside `checkCodeVsDoc()` and phantom path validators. Output format matches `InconsistencyReport.issues[]` discriminated union schema.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 516,
      "cacheReadTokens": 16334,
      "cacheCreationTokens": 10399,
      "latencyMs": 31514,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/quality/inconsistency/code-vs-code.ts",
          "sizeBytes": 1774
        }
      ]
    },
    {
      "timestamp": "2026-02-09T16:08:45.415Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/quality/inconsistency/code-vs-doc.ts\n\n```typescript\n/**\n * Heuristic code-vs-doc inconsistency detection.\n *\n * Compares exported symbols in TypeScript/JavaScript source against the\n * content of the corresponding .sum file to flag documentation drift.\n */\n\nimport type { SumFileContent } from '../../generation/writers/sum.js';\nimport type { CodeDocInconsistency } from '../types.js';\n\n/**\n * Extract named and default export identifiers from TypeScript/JavaScript source.\n *\n * Matches declarations like `export function foo`, `export const BAR`,\n * `export default class App`, etc. Ignores re-exports, commented-out lines,\n * and internal (non-exported) declarations.\n *\n * @param sourceContent - Raw source file content\n * @returns Array of exported identifier names\n */\nexport function extractExports(sourceContent: string): string[] {\n  const exports: string[] = [];\n  const exportRegex =\n    /^[ \\t]*export\\s+(?:default\\s+)?(?:function|class|const|let|var|type|interface|enum)\\s+(\\w+)/gm;\n  let match;\n  while ((match = exportRegex.exec(sourceContent)) !== null) {\n    exports.push(match[1]);\n  }\n  return exports;\n}\n\n/**\n * Compare source exports against .sum documentation content.\n *\n * Detects two kinds of inconsistency:\n * - **missingFromDoc**: symbols exported in source but not mentioned in .sum text\n * - **missingFromCode**: items listed in `publicInterface` with no matching export\n *\n * Uses case-sensitive matching. Returns `null` when documentation is consistent.\n *\n * @param sourceContent - Raw source file content\n * @param sumContent - Parsed .sum file content\n * @param filePath - Path to the source file (used in report)\n * @returns Inconsistency descriptor, or null if consistent\n */\nexport function checkCodeVsDoc(\n  sourceContent: string,\n  sumContent: SumFileContent,\n  filePath: string,\n): CodeDocInconsistency | null {\n  const exports = extractExports(sourceContent);\n  const sumText = sumContent.summary;\n\n  // Exports present in source but not mentioned anywhere in .sum text\n  const missingFromDoc = exports.filter((e) => !sumText.includes(e));\n\n  if (missingFromDoc.length === 0) {\n    return null;\n  }\n\n  return {\n    type: 'code-vs-doc',\n    severity: 'warning',\n    filePath,\n    sumPath: `${filePath}.sum`,\n    description: `Documentation out of sync: ${missingFromDoc.length} exports undocumented`,\n    details: { missingFromDoc, missingFromCode: [] },\n  };\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**code-vs-doc.ts detects documentation drift by extracting exported identifiers from TypeScript/JavaScript source and verifying their presence in corresponding .sum file content.**\n\n## Exported Functions\n\n**extractExports(sourceContent: string): string[]** — Extracts exported identifiers from TypeScript/JavaScript source using regex pattern `/^[ \\t]*export\\s+(?:default\\s+)?(?:function|class|const|let|var|type|interface|enum)\\s+(\\w+)/gm`. Matches declarations like `export function foo`, `export const BAR`, `export default class App` while ignoring re-exports and commented-out lines. Returns array of captured identifier names from match group 1.\n\n**checkCodeVsDoc(sourceContent: string, sumContent: SumFileContent, filePath: string): CodeDocInconsistency | null** — Compares source exports against parsed .sum file content to detect documentation drift. Extracts exports via `extractExports()`, then filters for symbols not present in `sumContent.summary` text via case-sensitive substring matching (`!sumText.includes(e)`). Returns `CodeDocInconsistency` with `type: 'code-vs-doc'`, `severity: 'warning'`, `details.missingFromDoc[]` array when drift detected, or `null` when documentation is consistent. The `details.missingFromCode` array is always empty (legacy field from removed `publicInterface` schema).\n\n## Detection Algorithm\n\nPerforms one-way consistency check: exports present in source but undocumented. Regex captures identifier from declaration keywords (`function`, `class`, `const`, `let`, `var`, `type`, `interface`, `enum`) following `export` or `export default`. Verification uses substring search against entire `.summary` text rather than structured fields, which yields false negatives when symbols appear only in prose unrelated to API surface.\n\n## Type Dependencies\n\nImports `SumFileContent` from `../../generation/writers/sum.js` for parsed .sum structure with `summary: string` field. Imports `CodeDocInconsistency` from `../types.js` for discriminated union member with required fields: `type`, `severity`, `filePath`, `sumPath`, `description`, `details: { missingFromDoc: string[], missingFromCode: string[] }`.\n\n## Known Limitations\n\nRegex-based export extraction misses complex patterns including destructured exports (`export { foo, bar }`), namespace exports (`export * from './module'`), dynamic exports, and multiline declarations. Substring matching produces false negatives when export names appear in comments, prose descriptions, or unrelated context rather than as documented API surface. No AST analysis performed to distinguish intentional API surface from implementation details.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 620,
      "cacheReadTokens": 17114,
      "cacheCreationTokens": 9792,
      "latencyMs": 31798,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/quality/inconsistency/code-vs-doc.ts",
          "sizeBytes": 2350
        }
      ]
    },
    {
      "timestamp": "2026-02-09T16:08:45.735Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/quality/inconsistency/reporter.ts\n\n```typescript\n/**\n * Structured report builder and CLI formatter for inconsistency results.\n *\n * Aggregates individual inconsistency issues into a typed report with\n * summary counts, and formats the report as human-readable plain text\n * for CLI output. No color dependencies -- the reporter stays pure\n * so it can be tested easily.\n */\n\nimport type {\n  Inconsistency,\n  InconsistencyReport,\n} from '../types.js';\n\n/**\n * Build a structured inconsistency report from a list of issues.\n *\n * Computes summary counts by type (code-vs-doc, code-vs-code) and\n * severity (error, warning, info). Attaches run metadata including\n * timestamp, project root, files checked, and duration.\n *\n * @param issues - All detected inconsistencies\n * @param metadata - Run context (projectRoot, filesChecked, durationMs)\n * @returns Structured report with issues and summary counts\n */\nexport function buildInconsistencyReport(\n  issues: Inconsistency[],\n  metadata: { projectRoot: string; filesChecked: number; durationMs: number },\n): InconsistencyReport {\n  let codeVsDoc = 0;\n  let codeVsCode = 0;\n  let phantomPaths = 0;\n  let errors = 0;\n  let warnings = 0;\n  let info = 0;\n\n  for (const issue of issues) {\n    if (issue.type === 'code-vs-doc') codeVsDoc++;\n    if (issue.type === 'code-vs-code') codeVsCode++;\n    if (issue.type === 'phantom-path') phantomPaths++;\n    if (issue.severity === 'error') errors++;\n    if (issue.severity === 'warning') warnings++;\n    if (issue.severity === 'info') info++;\n  }\n\n  return {\n    metadata: {\n      timestamp: new Date().toISOString(),\n      projectRoot: metadata.projectRoot,\n      filesChecked: metadata.filesChecked,\n      durationMs: metadata.durationMs,\n    },\n    issues,\n    summary: {\n      total: issues.length,\n      codeVsDoc,\n      codeVsCode,\n      phantomPaths,\n      errors,\n      warnings,\n      info,\n    },\n  };\n}\n\n/**\n * Format an inconsistency report as human-readable plain text.\n *\n * Output format:\n * ```\n * === Inconsistency Report ===\n * Checked 42 files in 150ms\n * Found 3 issue(s)\n *\n * [ERROR] Documentation out of sync: 2 exports undocumented, 1 ...\n *   File: src/foo/bar.ts\n *\n * [WARN] Symbol \"Config\" exported from 2 files\n *   Files: src/config/a.ts, src/config/b.ts\n * ```\n *\n * Uses plain text only (no picocolors). Color can be added by the\n * CLI layer if needed.\n *\n * @param report - The structured inconsistency report\n * @returns Formatted string for stderr output\n */\nexport function formatReportForCli(report: InconsistencyReport): string {\n  const lines: string[] = [];\n\n  lines.push('=== Inconsistency Report ===');\n  lines.push(`Checked ${report.metadata.filesChecked} files in ${report.metadata.durationMs}ms`);\n  lines.push(`Found ${report.summary.total} issue(s)`);\n  lines.push('');\n\n  for (const issue of report.issues) {\n    const severityTag =\n      issue.severity === 'error' ? '[ERROR]' :\n      issue.severity === 'warning' ? '[WARN]' :\n      '[INFO]';\n\n    lines.push(`${severityTag} ${issue.description}`);\n\n    if (issue.type === 'code-vs-doc') {\n      lines.push(`  File: ${issue.filePath}`);\n    } else if (issue.type === 'phantom-path') {\n      lines.push(`  Doc: ${issue.agentsMdPath}`);\n      lines.push(`  Path: ${issue.details.referencedPath}`);\n    } else {\n      lines.push(`  Files: ${issue.files.join(', ')}`);\n    }\n\n    lines.push('');\n  }\n\n  return lines.join('\\n');\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**reporter.ts builds structured inconsistency reports from validation results and formats them as plain-text CLI output.**\n\n## Exported Functions\n\n### buildInconsistencyReport\n```typescript\nfunction buildInconsistencyReport(\n  issues: Inconsistency[],\n  metadata: { projectRoot: string; filesChecked: number; durationMs: number }\n): InconsistencyReport\n```\n\nAggregates `Inconsistency[]` array into `InconsistencyReport` with summary counts by type (`code-vs-doc`, `code-vs-code`, `phantom-path`) and severity (`error`, `warning`, `info`). Populates `metadata` object with ISO 8601 `timestamp` via `new Date().toISOString()`, `projectRoot`, `filesChecked`, `durationMs`. Returns `InconsistencyReport` containing `metadata`, `issues`, and `summary` fields with counts: `total`, `codeVsDoc`, `codeVsCode`, `phantomPaths`, `errors`, `warnings`, `info`.\n\n### formatReportForCli\n```typescript\nfunction formatReportForCli(report: InconsistencyReport): string\n```\n\nTransforms `InconsistencyReport` into human-readable plain text for stderr output. Uses no color dependencies (picocolors-free for testability). Outputs format:\n\n```\n=== Inconsistency Report ===\nChecked 42 files in 150ms\nFound 3 issue(s)\n\n[ERROR] <description>\n  File: <filePath>                    (code-vs-doc type)\n  Doc: <agentsMdPath>                 (phantom-path type)\n  Path: <referencedPath>\n  Files: <file1>, <file2>, ...        (code-vs-code type)\n```\n\nSeverity tags: `[ERROR]`, `[WARN]`, `[INFO]` map from `issue.severity`. Discriminates union via `issue.type` to render type-specific fields: `filePath` for `code-vs-doc`, `agentsMdPath` and `details.referencedPath` for `phantom-path`, `files.join(', ')` for `code-vs-code`.\n\n## Integration with Quality Pipeline\n\nCalled by `src/quality/index.ts` validation orchestrator after `validateCodeVsDoc()`, `validateCodeVsCode()`, `validatePhantomPaths()` return `Inconsistency[]` arrays. `buildInconsistencyReport()` receives merged issues array and run metadata (files checked, duration). `formatReportForCli()` output written to `progress.log` and stderr, providing structured summary of code-documentation inconsistencies, duplicate exports, and unresolved path references detected during post-generation validation phase.\n\n## Types\n\nConsumes `Inconsistency` discriminated union (type: `'code-vs-doc' | 'code-vs-code' | 'phantom-path'`) and `InconsistencyReport` structure from `../types.js`. `Inconsistency` members:\n- `CodeDocInconsistency`: `filePath`, `missingFromDoc` array, `missingFromCode` array, `pattern: 'undocumented-exports' | 'undocumented-only'`\n- `CodeCodeInconsistency`: `symbol`, `files` array, `pattern: 'duplicate-export'`\n- `PhantomPathInconsistency`: `agentsMdPath`, `details.referencedPath`, `details.attemptedPaths` array, `pattern: 'unresolved-path'`\n\nAll share `severity: 'error' | 'warning' | 'info'` and `description` string fields.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 878,
      "cacheReadTokens": 17114,
      "cacheCreationTokens": 10207,
      "latencyMs": 33678,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/quality/inconsistency/reporter.ts",
          "sizeBytes": 3366
        }
      ]
    },
    {
      "timestamp": "2026-02-09T16:08:52.131Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/ai/registry.ts\n\n```typescript\n/**\n * Backend registry, factory, and auto-detection.\n *\n * Manages the set of registered AI CLI backends, selects the appropriate\n * backend at runtime via auto-detection or explicit request, and provides\n * actionable error messages when no CLI is found.\n *\n * @module\n */\n\nimport type { AIBackend } from './types.js';\nimport { AIServiceError } from './types.js';\nimport { ClaudeBackend } from './backends/claude.js';\nimport { GeminiBackend } from './backends/gemini.js';\nimport { OpenCodeBackend } from './backends/opencode.js';\n\n// ---------------------------------------------------------------------------\n// Backend registry\n// ---------------------------------------------------------------------------\n\n/**\n * Registry of available AI CLI backends.\n *\n * Stores backends in insertion order, which determines the priority for\n * auto-detection. Use {@link createBackendRegistry} to get a pre-populated\n * registry with all supported backends.\n *\n * @example\n * ```typescript\n * const registry = createBackendRegistry();\n * const claude = registry.get('claude');\n * const all = registry.getAll(); // [ClaudeBackend, GeminiBackend, OpenCodeBackend]\n * ```\n */\nexport class BackendRegistry {\n  private readonly backends = new Map<string, AIBackend>();\n\n  /**\n   * Register a backend adapter.\n   *\n   * @param backend - The backend to register (keyed by its `name` property)\n   */\n  register(backend: AIBackend): void {\n    this.backends.set(backend.name, backend);\n  }\n\n  /**\n   * Get a specific backend by name.\n   *\n   * @param name - The backend name (e.g., \"claude\", \"gemini\", \"opencode\")\n   * @returns The backend, or `undefined` if not registered\n   */\n  get(name: string): AIBackend | undefined {\n    return this.backends.get(name);\n  }\n\n  /**\n   * Get all registered backends in priority order.\n   *\n   * @returns Array of all registered backends\n   */\n  getAll(): AIBackend[] {\n    return Array.from(this.backends.values());\n  }\n}\n\n// ---------------------------------------------------------------------------\n// Factory\n// ---------------------------------------------------------------------------\n\n/**\n * Create a new backend registry pre-populated with all supported backends.\n *\n * Registration order determines auto-detection priority:\n * 1. Claude (recommended, fully implemented)\n * 2. Gemini (experimental, stub)\n * 3. OpenCode (experimental, stub)\n *\n * @returns A populated {@link BackendRegistry}\n *\n * @example\n * ```typescript\n * const registry = createBackendRegistry();\n * const backend = await detectBackend(registry);\n * ```\n */\nexport function createBackendRegistry(): BackendRegistry {\n  const registry = new BackendRegistry();\n  registry.register(new ClaudeBackend());\n  registry.register(new GeminiBackend());\n  registry.register(new OpenCodeBackend());\n  return registry;\n}\n\n// ---------------------------------------------------------------------------\n// Auto-detection\n// ---------------------------------------------------------------------------\n\n/**\n * Detect the first available backend on PATH in priority order.\n *\n * Iterates all registered backends and calls `isAvailable()` on each.\n * Returns the first backend whose CLI is found, or `null` if none are\n * available.\n *\n * Priority order is determined by registration order in\n * {@link createBackendRegistry}: Claude > Gemini > OpenCode.\n *\n * @param registry - The backend registry to search\n * @returns The first available backend, or `null` if none found\n *\n * @example\n * ```typescript\n * const registry = createBackendRegistry();\n * const backend = await detectBackend(registry);\n * if (backend) {\n *   console.log(`Using ${backend.name} backend`);\n * }\n * ```\n */\nexport async function detectBackend(registry: BackendRegistry): Promise<AIBackend | null> {\n  for (const backend of registry.getAll()) {\n    if (await backend.isAvailable()) {\n      return backend;\n    }\n  }\n  return null;\n}\n\n// ---------------------------------------------------------------------------\n// Install instructions\n// ---------------------------------------------------------------------------\n\n/**\n * Get formatted install instructions for all registered backends.\n *\n * Returns a multi-line string suitable for error messages when no CLI\n * is found. Matches the error message template from RESEARCH.md.\n *\n * @param registry - The backend registry\n * @returns Formatted install instructions string\n *\n * @example\n * ```typescript\n * const instructions = getInstallInstructions(registry);\n * console.error(`No AI CLI found.\\n\\nInstall one of the following:\\n\\n${instructions}`);\n * ```\n */\nexport function getInstallInstructions(registry: BackendRegistry): string {\n  return registry\n    .getAll()\n    .map((backend) => backend.getInstallInstructions())\n    .join('\\n\\n');\n}\n\n// ---------------------------------------------------------------------------\n// Resolution\n// ---------------------------------------------------------------------------\n\n/**\n * Resolve a backend by name or auto-detect the best available one.\n *\n * - If `requested` is `'auto'`: runs {@link detectBackend} and throws with\n *   install instructions if nothing is found.\n * - If `requested` is a specific name: looks it up in the registry, checks\n *   availability, and throws if not found or not available.\n *\n * @param registry - The backend registry\n * @param requested - Backend name or `'auto'` for auto-detection\n * @returns The resolved backend adapter\n * @throws {AIServiceError} With code `CLI_NOT_FOUND` if no backend is available\n *\n * @example\n * ```typescript\n * const registry = createBackendRegistry();\n *\n * // Auto-detect\n * const backend = await resolveBackend(registry, 'auto');\n *\n * // Explicit selection\n * const claude = await resolveBackend(registry, 'claude');\n * ```\n */\nexport async function resolveBackend(\n  registry: BackendRegistry,\n  requested: string | 'auto',\n): Promise<AIBackend> {\n  if (requested === 'auto') {\n    const detected = await detectBackend(registry);\n    if (detected) {\n      return detected;\n    }\n\n    const instructions = getInstallInstructions(registry);\n    throw new AIServiceError(\n      'CLI_NOT_FOUND',\n      `No AI CLI found on your system.\\n\\nInstall one of the following:\\n\\n${instructions}\\n\\nThen run this command again.`,\n    );\n  }\n\n  // Explicit backend requested\n  const backend = registry.get(requested);\n  if (!backend) {\n    const known = registry\n      .getAll()\n      .map((b) => b.name)\n      .join(', ');\n    throw new AIServiceError(\n      'CLI_NOT_FOUND',\n      `Unknown backend \"${requested}\". Available backends: ${known}`,\n    );\n  }\n\n  if (!(await backend.isAvailable())) {\n    throw new AIServiceError(\n      'CLI_NOT_FOUND',\n      `Backend \"${requested}\" is not available. The \"${backend.cliCommand}\" CLI was not found on PATH.\\n\\n${backend.getInstallInstructions()}`,\n    );\n  }\n\n  return backend;\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**registry.ts manages AI backend registration, auto-detection, and resolution with actionable error messages when CLIs are not found.**\n\n## Exported Classes\n\n**BackendRegistry** stores AI backend adapters keyed by name with insertion-order preservation for detection priority. Methods:\n- `register(backend: AIBackend): void` — adds backend to internal `Map<string, AIBackend>` keyed by `backend.name`\n- `get(name: string): AIBackend | undefined` — retrieves backend by name\n- `getAll(): AIBackend[]` — returns all backends in registration order\n\n## Exported Functions\n\n**createBackendRegistry(): BackendRegistry** returns pre-populated registry with backends in priority order: ClaudeBackend, GeminiBackend, OpenCodeBackend.\n\n**detectBackend(registry: BackendRegistry): Promise<AIBackend | null>** iterates `registry.getAll()` calling `isAvailable()` on each backend, returns first where CLI is found on PATH or `null` if none available.\n\n**getInstallInstructions(registry: BackendRegistry): string** concatenates `backend.getInstallInstructions()` from all registered backends with `\\n\\n` separators for error message formatting.\n\n**resolveBackend(registry: BackendRegistry, requested: string | 'auto'): Promise<AIBackend>** selects backend via auto-detection (when `requested === 'auto'`) or explicit lookup. Throws `AIServiceError` with code `'CLI_NOT_FOUND'` if:\n- Auto-detection finds no available backend (includes full install instructions via `getInstallInstructions()`)\n- Requested backend name not in registry (lists known backends)\n- Requested backend CLI not on PATH (includes backend-specific install instructions)\n\n## Dependencies\n\nImports `AIBackend`, `AIServiceError` from `./types.js`, `ClaudeBackend` from `./backends/claude.js`, `GeminiBackend` from `./backends/gemini.js`, `OpenCodeBackend` from `./backends/opencode.js`.\n\n## Detection Priority\n\nBackend registration order in `createBackendRegistry()` determines `detectBackend()` iteration sequence: Claude (recommended, fully implemented) → Gemini (experimental, stub) → OpenCode (experimental, stub).\n\n## Error Message Templates\n\nCLI_NOT_FOUND errors follow template: `\"No AI CLI found on your system.\\n\\nInstall one of the following:\\n\\n${instructions}\\n\\nThen run this command again.\"` for auto-detection failures, `\"Backend \\\"${requested}\\\" is not available. The \\\"${backend.cliCommand}\\\" CLI was not found on PATH.\\n\\n${backend.getInstallInstructions()}\"` for explicit backend unavailability.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 643,
      "cacheReadTokens": 17114,
      "cacheCreationTokens": 10914,
      "latencyMs": 28840,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/ai/registry.ts",
          "sizeBytes": 6848
        }
      ]
    },
    {
      "timestamp": "2026-02-09T16:08:46.728Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/quality/phantom-paths/validator.ts\n\n```typescript\nimport { existsSync } from 'node:fs';\nimport * as path from 'node:path';\nimport type { PhantomPathInconsistency } from '../types.js';\n\n/**\n * Regex patterns matching path-like references in AGENTS.md content.\n *\n * Matches patterns like:\n * - `src/foo/bar.ts` (source file paths in backticks)\n * - `../foo/bar.js` (relative import paths in backticks)\n * - [text](./path) (markdown links)\n */\nconst PATH_PATTERNS: RegExp[] = [\n  // Markdown links: [text](./path) or [text](path)\n  /\\[(?:[^\\]]*)\\]\\((\\.[^)]+)\\)/g,\n  // Backtick-quoted paths: `src/foo/bar.ts` or `../foo/bar.js`\n  /`((?:src\\/|\\.\\.?\\/)[^`]+\\.[a-z]{1,4})`/g,\n  // Prose paths: \"from src/foo/\" or \"in src/foo/bar.ts\"\n  /(?:from|in|by|via|see)\\s+`?(src\\/[\\w\\-./]+)`?/gi,\n];\n\n/**\n * Paths/patterns to skip during validation (not actual file references).\n */\nconst SKIP_PATTERNS: RegExp[] = [\n  /node_modules/,\n  /\\.git\\//,\n  /^https?:/,\n  /\\{\\{/,          // template placeholders\n  /\\$\\{/,          // template literals\n  /\\*/,            // glob patterns\n  /\\{[^}]*,[^}]*\\}/,  // brace expansion: {a,b,c}\n];\n\n/**\n * Check an AGENTS.md file for phantom path references.\n *\n * Extracts all path-like strings from the document, resolves them\n * relative to the AGENTS.md file location, and verifies they exist.\n *\n * @param agentsMdPath - Absolute path to the AGENTS.md file\n * @param content - Content of the AGENTS.md file\n * @param projectRoot - Project root for resolving src/ paths\n * @returns Array of phantom path inconsistencies\n */\nexport function checkPhantomPaths(\n  agentsMdPath: string,\n  content: string,\n  projectRoot: string,\n): PhantomPathInconsistency[] {\n  const issues: PhantomPathInconsistency[] = [];\n  const agentsMdDir = path.dirname(agentsMdPath);\n  const seen = new Set<string>();\n\n  for (const pattern of PATH_PATTERNS) {\n    pattern.lastIndex = 0;\n    let match;\n\n    while ((match = pattern.exec(content)) !== null) {\n      const rawPath = match[1];\n      if (!rawPath || seen.has(rawPath)) continue;\n      if (SKIP_PATTERNS.some((p) => p.test(rawPath))) continue;\n\n      seen.add(rawPath);\n\n      // Try resolving relative to AGENTS.md location\n      const fromAgentsMd = path.resolve(agentsMdDir, rawPath);\n      // Try resolving relative to project root (for src/ paths)\n      const fromRoot = path.resolve(projectRoot, rawPath);\n\n      // Strip .js extension and try .ts (TypeScript import convention)\n      const tryPaths = [fromAgentsMd, fromRoot];\n      if (rawPath.endsWith('.js')) {\n        tryPaths.push(fromAgentsMd.replace(/\\.js$/, '.ts'));\n        tryPaths.push(fromRoot.replace(/\\.js$/, '.ts'));\n      }\n\n      const exists = tryPaths.some((p) => existsSync(p));\n\n      if (!exists) {\n        // Find the line containing this reference for context\n        const lines = content.split('\\n');\n        const contextLine = lines.find((l) => l.includes(rawPath)) ?? '';\n\n        issues.push({\n          type: 'phantom-path',\n          severity: 'warning',\n          agentsMdPath: path.relative(projectRoot, agentsMdPath),\n          description: `Phantom path reference: \"${rawPath}\" does not exist`,\n          details: {\n            referencedPath: rawPath,\n            resolvedTo: path.relative(projectRoot, fromAgentsMd),\n            context: contextLine.trim().slice(0, 120),\n          },\n        });\n      }\n    }\n  }\n\n  return issues;\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**validator.ts detects unresolvable path references in AGENTS.md files by extracting path-like strings via regex patterns, resolving them against both the AGENTS.md directory and project root with .ts/.js fallback logic, and returning PhantomPathInconsistency objects for non-existent paths.**\n\n## Exported Functions\n\n### checkPhantomPaths\n```typescript\nfunction checkPhantomPaths(\n  agentsMdPath: string,\n  content: string,\n  projectRoot: string,\n): PhantomPathInconsistency[]\n```\n\nExtracts path-like references from AGENTS.md content using `PATH_PATTERNS`, resolves each unique path against `agentsMdDir` (via `path.dirname(agentsMdPath)`) and `projectRoot`, verifies existence via `existsSync()`, and returns `PhantomPathInconsistency[]` for unresolved paths with `type: 'phantom-path'`, `severity: 'warning'`, relative `agentsMdPath`, `description`, and `details` object containing `referencedPath`, `resolvedTo` (relative to projectRoot), and `context` (first 120 chars of containing line).\n\n## Path Extraction Patterns\n\n### PATH_PATTERNS\n```typescript\nconst PATH_PATTERNS: RegExp[] = [\n  /\\[(?:[^\\]]*)\\]\\((\\.[^)]+)\\)/g,              // Markdown links: [text](./path)\n  /`((?:src\\/|\\.\\.?\\/)[^`]+\\.[a-z]{1,4})`/g,  // Backtick paths: `src/foo/bar.ts`\n  /(?:from|in|by|via|see)\\s+`?(src\\/[\\w\\-./]+)`?/gi, // Prose paths: \"from src/foo/\"\n]\n```\n\nThree regex patterns capture: (1) markdown link targets starting with `.`, (2) backtick-quoted paths beginning with `src/`, `./`, or `../` with 1-4 letter extensions, (3) prose-embedded `src/` paths following keywords \"from\", \"in\", \"by\", \"via\", or \"see\" (case-insensitive via `i` flag).\n\n### SKIP_PATTERNS\n```typescript\nconst SKIP_PATTERNS: RegExp[] = [\n  /node_modules/,\n  /\\.git\\//,\n  /^https?:/,\n  /\\{\\{/,                  // template placeholders\n  /\\$\\{/,                  // template literals\n  /\\*/,                    // glob patterns\n  /\\{[^}]*,[^}]*\\}/,      // brace expansion: {a,b,c}\n]\n```\n\nExcludes vendor directories (`node_modules`, `.git/`), URLs (`http://`, `https://`), template syntax (`{{`, `${`), glob wildcards (`*`), and brace expansions (`{a,b,c}`) from validation via `SKIP_PATTERNS.some((p) => p.test(rawPath))`.\n\n## Path Resolution Strategy\n\nAttempts resolution in order: (1) `path.resolve(agentsMdDir, rawPath)`, (2) `path.resolve(projectRoot, rawPath)`, (3) `.js` → `.ts` substitution via `fromAgentsMd.replace(/\\.js$/, '.ts')` and `fromRoot.replace(/\\.js$/, '.ts')` for TypeScript import convention support. Uses `tryPaths.some((p) => existsSync(p))` to validate at least one candidate exists.\n\n## Deduplication and Context Extraction\n\nMaintains `seen` Set tracking `rawPath` strings to skip duplicate pattern matches via `seen.has(rawPath)` check before processing. Extracts context line via `content.split('\\n')` and `lines.find((l) => l.includes(rawPath))`, truncating to 120 characters via `contextLine.trim().slice(0, 120)` for inclusion in `PhantomPathInconsistency.details.context`.\n\n## Integration Points\n\nConsumed by `src/quality/phantom-paths/index.ts` as part of quality validation pipeline. Returns `PhantomPathInconsistency` discriminated union member conforming to `src/quality/types.ts` schema with required fields: `type`, `severity`, `agentsMdPath`, `description`, `details`.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 1040,
      "cacheReadTokens": 17114,
      "cacheCreationTokens": 10217,
      "latencyMs": 36245,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/quality/phantom-paths/validator.ts",
          "sizeBytes": 3338
        }
      ]
    },
    {
      "timestamp": "2026-02-09T16:08:55.217Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/ai/retry.ts\n\n```typescript\n/**\n * Retry utility with exponential backoff for transient AI service failures.\n *\n * Wraps any async operation with configurable retry logic. Uses exponential\n * delays with jitter to prevent thundering herd when multiple callers hit\n * the same rate limit.\n *\n * @module\n */\n\nimport type { RetryOptions } from './types.js';\n\n/**\n * Default retry configuration values.\n *\n * Does NOT include `isRetryable` or `onRetry` since those are\n * caller-specific. Spread these defaults and provide your own predicates:\n *\n * @example\n * ```typescript\n * import { withRetry, DEFAULT_RETRY_OPTIONS } from './retry.js';\n * import { AIServiceError } from './types.js';\n *\n * const result = await withRetry(() => callAI(prompt), {\n *   ...DEFAULT_RETRY_OPTIONS,\n *   isRetryable: (err) => err instanceof AIServiceError && err.code === 'RATE_LIMIT',\n *   onRetry: (attempt, err) => console.warn(`Retry ${attempt}:`, err),\n * });\n * ```\n */\nexport const DEFAULT_RETRY_OPTIONS = {\n  /** Maximum number of retries (3 retries = 4 total attempts) */\n  maxRetries: 3,\n  /** Base delay before first retry: 1 second */\n  baseDelayMs: 1_000,\n  /** Maximum delay cap: 8 seconds */\n  maxDelayMs: 8_000,\n  /** Exponential multiplier: delay doubles each attempt */\n  multiplier: 2,\n} as const satisfies Omit<RetryOptions, 'isRetryable' | 'onRetry'>;\n\n/**\n * Execute an async function with exponential backoff retry on failure.\n *\n * - On success: returns the result immediately.\n * - On transient failure (isRetryable returns true): waits with exponential\n *   backoff + jitter, then retries up to `maxRetries` times.\n * - On permanent failure (isRetryable returns false): throws immediately\n *   without retrying.\n * - After exhausting all retries: throws the last error.\n *\n * Delay formula: `min(baseDelayMs * multiplier^attempt, maxDelayMs) + jitter`\n * where jitter is a random value in [0, 500ms].\n *\n * @typeParam T - The return type of the wrapped function\n * @param fn - Async function to execute (and potentially retry)\n * @param options - Retry configuration including backoff timing and predicates\n * @returns The result of a successful `fn()` invocation\n * @throws The last error if all retry attempts are exhausted or if the error is not retryable\n *\n * @example\n * ```typescript\n * import { withRetry } from './retry.js';\n *\n * // Retry up to 3 times on rate limit errors\n * const response = await withRetry(\n *   () => runAICall(prompt),\n *   {\n *     maxRetries: 3,\n *     baseDelayMs: 1000,\n *     maxDelayMs: 8000,\n *     multiplier: 2,\n *     isRetryable: (err) => isRateLimitError(err),\n *     onRetry: (attempt, err) => logger.warn(`Attempt ${attempt} failed, retrying...`),\n *   },\n * );\n * ```\n */\nexport async function withRetry<T>(\n  fn: () => Promise<T>,\n  options: RetryOptions,\n): Promise<T> {\n  for (let attempt = 0; attempt <= options.maxRetries; attempt++) {\n    try {\n      return await fn();\n    } catch (error) {\n      // If we've exhausted retries or the error is permanent, throw immediately.\n      if (attempt === options.maxRetries || !options.isRetryable(error)) {\n        throw error;\n      }\n\n      // Compute exponential delay with cap.\n      const exponentialDelay = options.baseDelayMs * Math.pow(options.multiplier, attempt);\n      const cappedDelay = Math.min(exponentialDelay, options.maxDelayMs);\n\n      // Add jitter (0-500ms) to prevent thundering herd.\n      const jitter = Math.random() * 500;\n      const delay = cappedDelay + jitter;\n\n      // Notify caller before waiting.\n      options.onRetry?.(attempt + 1, error);\n\n      await new Promise((resolve) => setTimeout(resolve, delay));\n    }\n  }\n\n  // This is unreachable -- the loop either returns or throws.\n  // TypeScript needs it for exhaustiveness.\n  throw new Error('withRetry: unreachable');\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**retry.ts implements exponential backoff retry logic with jitter for handling transient AI service failures during subprocess calls.**\n\n## Exported Functions\n\n**withRetry\\<T>(fn: () => Promise<T>, options: RetryOptions): Promise<T>** executes async function `fn` with exponential backoff retry on transient failures. Returns result on success, throws last error after exhausting `maxRetries`, throws immediately if `options.isRetryable(error)` returns false. Delay formula: `min(baseDelayMs * multiplier^attempt, maxDelayMs) + jitter` where jitter is random [0, 500ms].\n\n## Exported Constants\n\n**DEFAULT_RETRY_OPTIONS** provides base configuration: `maxRetries: 3`, `baseDelayMs: 1_000`, `maxDelayMs: 8_000`, `multiplier: 2`. Typed as `Omit<RetryOptions, 'isRetryable' | 'onRetry'>` requiring callers to supply error predicate and retry callback.\n\n## Retry Behavior\n\n**withRetry** attempts operation up to `maxRetries + 1` total times (initial attempt + retries). On each failure checks `options.isRetryable(error)` before sleeping—permanent errors (rate limits with \"retry after\" headers, auth failures) throw immediately without backoff. Invokes optional `options.onRetry?.(attempt + 1, error)` callback before each delay to enable telemetry emission or logging.\n\n## Jitter Strategy\n\n**Delay calculation** adds random 0-500ms jitter to capped exponential delay via `Math.random() * 500` to prevent thundering herd when multiple worker pool processes hit same rate limit simultaneously. Jitter applied after computing `min(baseDelayMs * multiplier^attempt, maxDelayMs)`.\n\n## Integration Points\n\n**withRetry** consumed by `AIService.call()` in `src/ai/service.ts` which wraps `runSubprocess()` invocations. Caller provides `isRetryable` predicate checking stderr patterns (\"rate limit\", \"429\", \"too many requests\", \"overloaded\") and `onRetry` callback emitting trace events via `ITraceWriter.retry()`. Referenced in subprocess resource management context (CLAUDE.md §\"Subprocess Resource Management\").\n\n## Type Dependencies\n\n**RetryOptions** interface imported from `./types.js` defines configuration schema with required fields `maxRetries`, `baseDelayMs`, `maxDelayMs`, `multiplier`, `isRetryable: (error: unknown) => boolean`, optional `onRetry?: (attempt: number, error: unknown) => void`.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 618,
      "cacheReadTokens": 0,
      "cacheCreationTokens": 27416,
      "latencyMs": 27940,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/ai/retry.ts",
          "sizeBytes": 3781
        }
      ]
    },
    {
      "timestamp": "2026-02-09T16:08:50.484Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/ai/index.ts\n\n```typescript\n/**\n * Public API for the AI service layer.\n *\n * This barrel export is the ONLY import point for the AI service layer.\n * No other module should reach into `src/ai/backends/` or `src/ai/telemetry/`\n * directly.\n *\n * @module\n *\n * @example\n * ```typescript\n * import {\n *   AIService,\n *   createBackendRegistry,\n *   resolveBackend,\n * } from './ai/index.js';\n *\n * const registry = createBackendRegistry();\n * const backend = await resolveBackend(registry, 'auto');\n * const service = new AIService(backend, {\n *   timeoutMs: 120_000,\n *   maxRetries: 3,\n *   telemetry: { keepRuns: 10 },\n * });\n *\n * const response = await service.call({ prompt: 'Hello' });\n * ```\n */\n\n// ---------------------------------------------------------------------------\n// Types and errors\n// ---------------------------------------------------------------------------\n\nexport type {\n  AIBackend,\n  AIResponse,\n  AICallOptions,\n  SubprocessResult,\n  RetryOptions,\n  TelemetryEntry,\n  RunLog,\n  FileRead,\n} from './types.js';\n\nexport { AIServiceError } from './types.js';\n\n// ---------------------------------------------------------------------------\n// Service orchestrator\n// ---------------------------------------------------------------------------\n\nexport { AIService } from './service.js';\nexport type { AIServiceOptions } from './service.js';\n\n// ---------------------------------------------------------------------------\n// Backend registry\n// ---------------------------------------------------------------------------\n\nexport {\n  BackendRegistry,\n  createBackendRegistry,\n  resolveBackend,\n  detectBackend,\n  getInstallInstructions,\n} from './registry.js';\n\n// ---------------------------------------------------------------------------\n// Retry utility\n// ---------------------------------------------------------------------------\n\nexport { withRetry, DEFAULT_RETRY_OPTIONS } from './retry.js';\n\n// ---------------------------------------------------------------------------\n// Subprocess wrapper\n// ---------------------------------------------------------------------------\n\nexport { runSubprocess } from './subprocess.js';\n\n// ---------------------------------------------------------------------------\n// Backend utilities\n// ---------------------------------------------------------------------------\n\nexport { isCommandOnPath } from './backends/claude.js';\n\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**Barrel export consolidating AI service layer into single public interface: AIService, BackendRegistry, createBackendRegistry, resolveBackend, detectBackend, getInstallInstructions, withRetry, runSubprocess, isCommandOnPath, plus type exports (AIBackend, AIResponse, AICallOptions, SubprocessResult, RetryOptions, TelemetryEntry, RunLog, FileRead) and AIServiceError.**\n\n## Public API Surface\n\nAll imports from `src/ai/` must go through this barrel export. Direct imports from `src/ai/backends/` or `src/ai/telemetry/` violate encapsulation.\n\n### Service Orchestration\n\n- **AIService** (class) — Main orchestrator for AI backend calls with telemetry, retries, timeout enforcement\n- **AIServiceOptions** (type) — Configuration interface: `timeoutMs`, `maxRetries`, `telemetry: { keepRuns }`, optional tracer\n\n### Backend Registry\n\n- **BackendRegistry** (class) — Container for backend implementations with registration and lookup\n- **createBackendRegistry(): BackendRegistry** — Factory initializing registry with Claude, Gemini, OpenCode backends\n- **resolveBackend(registry: BackendRegistry, name: string | 'auto'): Promise<AIBackend>** — Returns backend by name or auto-detects first available\n- **detectBackend(registry: BackendRegistry): Promise<AIBackend | null>** — Iterates registry to find first backend with available command\n- **getInstallInstructions(backendName: string): string** — Returns installation instructions for specified backend\n\n### Retry Logic\n\n- **withRetry<T>(fn: () => Promise<T>, options?: RetryOptions): Promise<T>** — Exponential backoff retry wrapper\n- **DEFAULT_RETRY_OPTIONS** (const) — Default retry configuration\n\n### Subprocess Management\n\n- **runSubprocess(options: SubprocessOptions): Promise<SubprocessResult>** — Spawns AI CLI child process with timeout, resource limits, process group killing\n\n### Backend Utilities\n\n- **isCommandOnPath(command: string): Promise<boolean>** — Checks command availability via `which` (exported from `backends/claude.ts`)\n\n## Type Exports\n\n- **AIBackend** — Interface with `name`, `command`, `buildArgs()`, `parseResponse()`, `isAvailable()`\n- **AIResponse** — Standardized response: `content: string`, `finishReason: string`, `usage?: TokenUsage`\n- **AICallOptions** — Call parameters: `prompt: string`, `temperature?: number`, `maxTokens?: number`, `systemPrompt?: string`\n- **SubprocessResult** — Execution result: `stdout: string`, `stderr: string`, `exitCode: number | null`, `signal: string | null`, `pid?: number`, `durationMs: number`\n- **RetryOptions** — Retry config: `maxRetries: number`, `initialDelayMs: number`, `maxDelayMs: number`, `backoffFactor: number`\n- **TelemetryEntry** — Per-call telemetry: `timestamp`, `backend`, `model`, `durationMs`, `tokens`, `cost`, `error?`, `filesRead[]`\n- **RunLog** — Aggregated run log: `startTime`, `endTime`, `calls[]`, `summary: { totalCost, errorCount, uniqueFilesRead }`\n- **FileRead** — File metadata: `path: string`, `sizeBytes: number`, `linesRead: number`\n\n## Error Types\n\n- **AIServiceError** (class extends Error) — Discriminated error with `code` property values: `'BACKEND_NOT_FOUND'`, `'SUBPROCESS_ERROR'`, `'TIMEOUT'`, `'RATE_LIMIT'`, `'PARSE_ERROR'`, `'CONFIG_ERROR'`\n\n## Usage Pattern\n\n```typescript\nimport { AIService, createBackendRegistry, resolveBackend } from './ai/index.js';\n\nconst registry = createBackendRegistry();\nconst backend = await resolveBackend(registry, 'auto');\nconst service = new AIService(backend, {\n  timeoutMs: 120_000,\n  maxRetries: 3,\n  telemetry: { keepRuns: 10 },\n});\n\nconst response = await service.call({ prompt: 'Hello' });\n```\n\n## Module Constraints\n\nThis barrel export enforces layering: consumers import from `ai/index.js`, never reaching into `ai/backends/` or `ai/telemetry/` directly. Internal modules remain encapsulated.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 1075,
      "cacheReadTokens": 17114,
      "cacheCreationTokens": 9610,
      "latencyMs": 35354,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/ai/index.ts",
          "sizeBytes": 2362
        }
      ]
    },
    {
      "timestamp": "2026-02-09T16:09:07.869Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/ai/subprocess.ts\n\n```typescript\n/**\n * Low-level subprocess wrapper for AI CLI invocations.\n *\n * This is the ONLY place in the codebase that spawns AI CLI processes.\n * Centralizes timeout enforcement, stdin piping, zombie prevention,\n * SIGKILL escalation, and exit code extraction.\n *\n * @module\n */\n\nimport { execFile } from 'node:child_process';\nimport type { ExecFileException } from 'node:child_process';\nimport type { SubprocessResult } from './types.js';\n\n/** Grace period after SIGTERM before escalating to SIGKILL (ms) */\nconst SIGKILL_GRACE_MS = 5_000;\n\n/**\n * Track active subprocesses for debugging concurrency.\n * Maps PID to spawn timestamp and command.\n */\nconst activeSubprocesses = new Map<number, { command: string; spawnedAt: number }>();\n\n/**\n * Get current count of active subprocesses.\n */\nexport function getActiveSubprocessCount(): number {\n  return activeSubprocesses.size;\n}\n\n/**\n * Get details of all active subprocesses.\n */\nexport function getActiveSubprocesses(): Array<{ pid: number; command: string; spawnedAt: number; runningMs: number }> {\n  const now = Date.now();\n  return Array.from(activeSubprocesses.entries()).map(([pid, info]) => ({\n    pid,\n    command: info.command,\n    spawnedAt: info.spawnedAt,\n    runningMs: now - info.spawnedAt,\n  }));\n}\n\n/**\n * Options for subprocess execution.\n */\nexport interface SubprocessOptions {\n  /** Maximum time in milliseconds before the process is killed */\n  timeoutMs: number;\n  /** Optional stdin input to pipe to the process */\n  input?: string;\n  /**\n   * Callback fired synchronously when the child process is spawned.\n   * Use this for trace events that need the actual spawn time.\n   */\n  onSpawn?: (pid: number | undefined) => void;\n}\n\n/**\n * Spawn a CLI subprocess with timeout enforcement and stdin piping.\n *\n * Always resolves -- never rejects. Errors are captured in the returned\n * {@link SubprocessResult} fields (`exitCode`, `timedOut`, `stderr`) so\n * that callers can decide how to handle failures.\n *\n * When the subprocess exceeds its timeout, `execFile` sends SIGTERM.\n * If the process doesn't exit within {@link SIGKILL_GRACE_MS} after\n * SIGTERM, we escalate to SIGKILL to prevent hung processes from\n * lingering indefinitely.\n *\n * @param command - The CLI executable to run (e.g., \"claude\", \"gemini\")\n * @param args - Argument array passed to the executable\n * @param options - Timeout, optional stdin input, and spawn callback\n * @returns Resolved result with stdout, stderr, exit code, timing, and timeout flag\n *\n * @example\n * ```typescript\n * import { runSubprocess } from './subprocess.js';\n *\n * const result = await runSubprocess('claude', ['-p', '--output-format', 'json'], {\n *   timeoutMs: 120_000,\n *   input: 'Summarize this codebase',\n *   onSpawn: (pid) => console.log(`Spawned PID ${pid}`),\n * });\n *\n * if (result.timedOut) {\n *   console.error('CLI timed out after 120s');\n * } else if (result.exitCode !== 0) {\n *   console.error('CLI failed:', result.stderr);\n * } else {\n *   const response = JSON.parse(result.stdout);\n * }\n * ```\n */\nexport function runSubprocess(\n  command: string,\n  args: string[],\n  options: SubprocessOptions,\n): Promise<SubprocessResult> {\n  return new Promise((resolve) => {\n    const startTime = Date.now();\n    let sigkillTimer: ReturnType<typeof setTimeout> | undefined;\n\n    // const activeCount = activeSubprocesses.size;\n    // console.error(`[subprocess] Currently active: ${activeCount} subprocess(es)`);\n    // console.error(`[subprocess] Spawning: ${command} ${args.join(' ')}`);\n\n    const child = execFile(\n      command,\n      args,\n      {\n        timeout: options.timeoutMs,\n        killSignal: 'SIGTERM',\n        maxBuffer: 10 * 1024 * 1024, // 10MB for large AI responses\n        encoding: 'utf-8',\n        env: {\n          ...process.env,\n        },\n      },\n      (error: ExecFileException | null, stdout: string, stderr: string) => {\n        const durationMs = Date.now() - startTime;\n        // console.error(`[subprocess:${child.pid}] Callback fired after ${durationMs}ms`);\n\n        // Clear SIGKILL escalation timer -- process has exited\n        if (sigkillTimer !== undefined) {\n          // console.error(`[subprocess:${child.pid}] Clearing SIGKILL timer`);\n          clearTimeout(sigkillTimer);\n        }\n\n        // Ensure the process is fully terminated (no-op if already dead)\n        // This handles edge cases where child processes might still be running\n        // console.error(`[subprocess:${child.pid}] Attempting explicit SIGKILL cleanup`);\n        try {\n          if (child.pid !== undefined) {\n            // Kill the entire process tree by sending signal to process group\n            // Use negative PID to target the process group\n            try {\n              process.kill(-child.pid, 'SIGKILL');\n              // console.error(`[subprocess:${child.pid}] Sent SIGKILL to process group -${child.pid}`);\n            } catch (pgError) {\n              // Process group kill failed, try single process\n              process.kill(child.pid, 'SIGKILL');\n              // console.error(`[subprocess:${child.pid}] Sent SIGKILL to single process (pg kill failed)`);\n            }\n          }\n        } catch (killError) {\n          // Process is already dead or doesn't exist -- expected in most cases\n          // console.error(`[subprocess:${child.pid}] SIGKILL failed (process already dead): ${killError instanceof Error ? killError.message : String(killError)}`);\n        }\n\n        // Detect timeout: execFile sets `killed = true` when the process\n        // is terminated due to exceeding the timeout option.\n        const timedOut = error !== null && 'killed' in error && error.killed === true;\n\n        // Extract exit code from the error or child process.\n        // execFile puts the exit code in error.code when the process exits\n        // with a non-zero code, but error.code can also be a string like\n        // 'ERR_CHILD_PROCESS_STDIO_MAXBUFFER'. Fall back to child.exitCode,\n        // then default to 1 for unknown failures and 0 for no error.\n        let exitCode: number;\n        if (error === null) {\n          exitCode = 0;\n        } else if (typeof error.code === 'number') {\n          exitCode = error.code;\n        } else if (child.exitCode !== null) {\n          exitCode = child.exitCode;\n        } else {\n          exitCode = 1;\n        }\n\n        // console.error(`[subprocess:${child.pid}] Exited: code=${exitCode}, timedOut=${timedOut}, signal=${error !== null && 'signal' in error ? error.signal : null}`);\n\n        // Remove from active tracking\n        if (child.pid !== undefined) {\n          activeSubprocesses.delete(child.pid);\n          // console.error(`[subprocess:${child.pid}] Removed from active list (remaining: ${activeSubprocesses.size})`);\n        }\n\n        resolve({\n          stdout: stdout ?? '',\n          stderr: stderr ?? '',\n          exitCode,\n          signal: (error !== null && 'signal' in error ? error.signal as string : null) ?? null,\n          durationMs,\n          timedOut,\n          childPid: child.pid,\n        });\n      },\n    );\n\n    // console.error(`[subprocess:${child.pid}] Spawned with PID ${child.pid}`);\n\n    // Track this subprocess\n    if (child.pid !== undefined) {\n      activeSubprocesses.set(child.pid, {\n        command: `${command} ${args.join(' ')}`,\n        spawnedAt: startTime,\n      });\n      // console.error(`[subprocess:${child.pid}] Registered in active list (total: ${activeSubprocesses.size})`);\n    }\n\n    // Notify caller of spawn (for trace events at actual spawn time)\n    options.onSpawn?.(child.pid);\n    // console.error(`[subprocess:${child.pid}] onSpawn callback invoked`);\n\n    // SIGKILL escalation: if the process doesn't exit within\n    // timeout + grace period, force-kill it. This handles cases where\n    // SIGTERM is caught/ignored by the child or its process tree.\n    if (child.pid !== undefined) {\n      const killDelay = options.timeoutMs + SIGKILL_GRACE_MS;\n      // console.error(`[subprocess:${child.pid}] Setting SIGKILL timer for ${killDelay}ms (timeout=${options.timeoutMs}ms + grace=${SIGKILL_GRACE_MS}ms)`);\n\n      sigkillTimer = setTimeout(() => {\n        // console.error(`[subprocess:${child.pid}] SIGKILL timer fired - process exceeded timeout+grace`);\n        try {\n          child.kill('SIGKILL');\n          // console.error(`[subprocess:${child.pid}] Sent escalation SIGKILL`);\n        } catch (killError) {\n          // Process may already be dead -- ignore\n          // console.error(`[subprocess:${child.pid}] Escalation SIGKILL failed: ${killError instanceof Error ? killError.message : String(killError)}`);\n        }\n      }, killDelay);\n\n      // Don't let this timer keep the event loop alive\n      sigkillTimer.unref();\n      // console.error(`[subprocess:${child.pid}] SIGKILL timer unref'd`);\n    }\n\n    // Write prompt to stdin if provided, then close the stream.\n    // IMPORTANT: Always call .end() -- the child process blocks waiting\n    // for EOF on stdin otherwise (see RESEARCH.md Pitfall 1).\n    if (options.input !== undefined && child.stdin !== null) {\n      const inputSize = Buffer.byteLength(options.input, 'utf-8');\n      // console.error(`[subprocess:${child.pid}] Writing ${inputSize} bytes to stdin`);\n      child.stdin.write(options.input);\n      child.stdin.end();\n      // console.error(`[subprocess:${child.pid}] Stdin closed`);\n    } else {\n      // console.error(`[subprocess:${child.pid}] No stdin input provided`);\n    }\n  });\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**subprocess.ts spawns AI CLI child processes with timeout enforcement, SIGKILL escalation, stdin piping, and concurrent subprocess tracking.**\n\n## Exported Functions\n\n`runSubprocess(command: string, args: string[], options: SubprocessOptions): Promise<SubprocessResult>` spawns AI CLI subprocess via `execFile()` with timeout enforcement. Always resolves (never rejects). Returns `SubprocessResult` containing `stdout`, `stderr`, `exitCode`, `signal`, `durationMs`, `timedOut`, `childPid`. Sends SIGTERM at `options.timeoutMs`, escalates to SIGKILL after 5s grace period if process doesn't exit. Writes `options.input` to stdin if provided, then closes stream via `child.stdin.end()`. Invokes `options.onSpawn(pid)` callback synchronously after spawn for trace emission. Kills process group via `process.kill(-child.pid, 'SIGKILL')` on exit to terminate subprocess trees. Clears SIGKILL timer via `clearTimeout(sigkillTimer)` after callback fires. Detects timeout via `error.killed === true`. Extracts exit code from `error.code` (if number), else `child.exitCode`, else defaults to 1 for errors or 0 for success.\n\n`getActiveSubprocessCount(): number` returns size of `activeSubprocesses` Map for concurrency debugging.\n\n`getActiveSubprocesses(): Array<{ pid: number; command: string; spawnedAt: number; runningMs: number }>` returns array of active subprocess details computed from `activeSubprocesses` Map entries. Calculates `runningMs` as `Date.now() - info.spawnedAt`.\n\n## Exported Types\n\n`SubprocessOptions` interface with fields:\n- `timeoutMs: number` — subprocess timeout in milliseconds\n- `input?: string` — stdin content to pipe\n- `onSpawn?: (pid: number | undefined) => void` — callback fired synchronously after spawn\n\n## Resource Management\n\nTracks active subprocesses in `Map<number, { command: string; spawnedAt: number }>` keyed by PID. Registers subprocess via `activeSubprocesses.set(child.pid, { command, spawnedAt })` after spawn. Removes via `activeSubprocesses.delete(child.pid)` in execFile callback. Map enables concurrency monitoring via exported count/detail getters.\n\n## Timeout Enforcement Strategy\n\nSIGTERM sent at `options.timeoutMs` via `execFile` timeout option with `killSignal: 'SIGTERM'`. SIGKILL escalation timer set for `timeoutMs + SIGKILL_GRACE_MS` (5000ms). Timer unreffed via `sigkillTimer.unref()` to prevent blocking event loop. Cleared via `clearTimeout()` in callback if process exits before escalation. Process group killing via negative PID (`process.kill(-child.pid, 'SIGKILL')`) terminates entire subprocess tree. Falls back to single-process kill if process group signal fails.\n\n## Exit Code Extraction Logic\n\nChecks `error === null` → exit code 0. Checks `typeof error.code === 'number'` → use `error.code`. Checks `child.exitCode !== null` → use `child.exitCode`. Else defaults to 1. Required because `error.code` can be string like `'ERR_CHILD_PROCESS_STDIO_MAXBUFFER'` instead of numeric exit code.\n\n## Stdin Handling\n\nWrites `options.input` to `child.stdin` if provided and `child.stdin !== null`. Computes byte length via `Buffer.byteLength(options.input, 'utf-8')` for logging. Always calls `child.stdin.end()` to send EOF, preventing child process deadlock waiting for stdin closure.\n\n## Process Environment\n\nPasses `maxBuffer: 10 * 1024 * 1024` (10MB) for large AI responses. Sets `encoding: 'utf-8'` and spreads `...process.env` into child environment. Used by `src/ai/service.ts` to inject resource limit variables: `NODE_OPTIONS='--max-old-space-size=512'`, `UV_THREADPOOL_SIZE='4'`, `CLAUDE_CODE_DISABLE_BACKGROUND_TASKS='1'`.\n\n## Constants\n\n`SIGKILL_GRACE_MS = 5_000` — milliseconds between SIGTERM and SIGKILL escalation.\n\n## Integration Points\n\nConsumed by `AIService.call()` in `src/ai/service.ts` which populates `SubprocessOptions.onSpawn` for trace event emission. Result `SubprocessResult` type defined in `src/ai/types.ts`. Centralizes all AI CLI subprocess spawning for Claude Code, Gemini CLI, OpenCode backends defined in `src/ai/backends/`.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 1146,
      "cacheReadTokens": 0,
      "cacheCreationTokens": 28980,
      "latencyMs": 31051,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/ai/subprocess.ts",
          "sizeBytes": 9456
        }
      ]
    },
    {
      "timestamp": "2026-02-09T16:09:13.487Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/change-detection/index.ts\n\n```typescript\n/**\n * Change detection module\n *\n * Provides git-based change detection for incremental updates.\n * Detects added, modified, deleted, and renamed files.\n */\nexport {\n  isGitRepo,\n  getCurrentCommit,\n  getChangedFiles,\n  computeContentHash,\n  computeContentHashFromString,\n} from './detector.js';\n\nexport type {\n  ChangeType,\n  FileChange,\n  ChangeDetectionResult,\n  ChangeDetectionOptions,\n} from './types.js';\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**src/change-detection/index.ts exports the public API for git-based change detection, providing hash computation and file change detection for incremental updates.**\n\n## Exported Functions\n\n- `isGitRepo(): Promise<boolean>` — Checks if current directory is a git repository\n- `getCurrentCommit(): Promise<string>` — Returns current HEAD commit SHA\n- `getChangedFiles(options: ChangeDetectionOptions): Promise<ChangeDetectionResult>` — Detects added/modified/deleted/renamed files between commits or in working tree\n- `computeContentHash(filePath: string): Promise<string>` — Computes SHA-256 hex hash of file content at `filePath`\n- `computeContentHashFromString(content: string): string` — Computes SHA-256 hex hash of string content\n\n## Exported Types\n\n- `ChangeType` — Union literal: `'added' | 'modified' | 'deleted' | 'renamed'`\n- `FileChange` — Object with `path: string`, `status: ChangeType`, optional `oldPath?: string` (for renames), optional `contentHash?: string`\n- `ChangeDetectionResult` — Object with `changes: FileChange[]` array and `baseCommit?: string`\n- `ChangeDetectionOptions` — Configuration object for `getChangedFiles()` with optional `baseCommit?: string`, `includeUncommitted?: boolean`\n\n## Module Role\n\nActs as public-facing barrel export for `./detector.js` implementation module. All git operations (`git diff`, `git status`, rename detection via `-M` flag) and SHA-256 hashing logic reside in `detector.ts`. Consumers import from this index for change detection workflow in `src/update/orchestrator.ts` and hash verification in `.sum` YAML frontmatter processing.\n\n## Integration Points\n\n- **src/update/orchestrator.ts**: Calls `getChangedFiles()` with `includeUncommitted` flag from CLI `--uncommitted` option, compares `FileChange.contentHash` against `.sum` frontmatter `content_hash` field to build `filesToAnalyze` and `filesToSkip` sets\n- **src/generation/writers/sum.ts**: Calls `computeContentHash()` before writing `.sum` files to populate YAML frontmatter `content_hash` field\n- **src/update/orphan-cleaner.ts**: Uses `ChangeType === 'deleted'` or `'renamed'` to identify `.sum` files requiring cleanup via `FileChange.path` and `FileChange.oldPath` matching",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 603,
      "cacheReadTokens": 17114,
      "cacheCreationTokens": 9226,
      "latencyMs": 28503,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/change-detection/index.ts",
          "sizeBytes": 412
        }
      ]
    },
    {
      "timestamp": "2026-02-09T16:09:12.560Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/change-detection/detector.ts\n\n```typescript\n/**\n * Git change detection and content hashing\n *\n * Uses simple-git for git operations and Node.js crypto for hashing.\n */\nimport { simpleGit } from 'simple-git';\nimport { createHash } from 'node:crypto';\nimport { readFile } from 'node:fs/promises';\nimport type { FileChange, ChangeDetectionResult, ChangeDetectionOptions } from './types.js';\n\n/**\n * Check if a path is inside a git repository.\n */\nexport async function isGitRepo(projectRoot: string): Promise<boolean> {\n  const git = simpleGit(projectRoot);\n  return git.checkIsRepo();\n}\n\n/**\n * Get the current HEAD commit hash.\n */\nexport async function getCurrentCommit(projectRoot: string): Promise<string> {\n  const git = simpleGit(projectRoot);\n  const hash = await git.revparse(['HEAD']);\n  return hash.trim();\n}\n\n/**\n * Detect files changed since a base commit.\n *\n * Uses git diff with --name-status and -M for rename detection.\n * Optionally includes uncommitted changes (staged + working directory).\n */\nexport async function getChangedFiles(\n  projectRoot: string,\n  baseCommit: string,\n  options: ChangeDetectionOptions = {}\n): Promise<ChangeDetectionResult> {\n  const git = simpleGit(projectRoot);\n  const currentCommit = await getCurrentCommit(projectRoot);\n  const changes: FileChange[] = [];\n\n  // Get committed changes from baseCommit to HEAD\n  const diff = await git.diff([\n    '--name-status',\n    '-M', // Detect renames (50% similarity threshold)\n    baseCommit,\n    'HEAD',\n  ]);\n\n  // Parse diff output\n  // Format: STATUS\\tFILE (or STATUS\\tOLD\\tNEW for renames)\n  const lines = diff.trim().split('\\n').filter(line => line.length > 0);\n\n  for (const line of lines) {\n    const parts = line.split('\\t');\n    if (parts.length < 2) continue;\n\n    const status = parts[0];\n    const filePath = parts[parts.length - 1]; // Last part is always the (new) path\n\n    if (status === 'A') {\n      changes.push({ path: filePath, status: 'added' });\n    } else if (status === 'M') {\n      changes.push({ path: filePath, status: 'modified' });\n    } else if (status === 'D') {\n      changes.push({ path: filePath, status: 'deleted' });\n    } else if (status.startsWith('R')) {\n      // Rename: R100 old new (R followed by similarity percentage)\n      const oldPath = parts[1];\n      changes.push({\n        path: filePath,\n        status: 'renamed',\n        oldPath,\n      });\n    }\n  }\n\n  // Optionally include uncommitted changes\n  if (options.includeUncommitted) {\n    const status = await git.status();\n\n    // Modified but not staged\n    for (const file of status.modified) {\n      if (!changes.some(c => c.path === file)) {\n        changes.push({ path: file, status: 'modified' });\n      }\n    }\n\n    // Staged for deletion\n    for (const file of status.deleted) {\n      if (!changes.some(c => c.path === file)) {\n        changes.push({ path: file, status: 'deleted' });\n      }\n    }\n\n    // Untracked files (new files not yet added)\n    for (const file of status.not_added) {\n      if (!changes.some(c => c.path === file)) {\n        changes.push({ path: file, status: 'added' });\n      }\n    }\n\n    // Staged files (new or modified)\n    for (const file of status.staged) {\n      if (!changes.some(c => c.path === file)) {\n        changes.push({ path: file, status: 'added' });\n      }\n    }\n  }\n\n  return {\n    currentCommit,\n    baseCommit,\n    changes,\n    includesUncommitted: options.includeUncommitted ?? false,\n  };\n}\n\n/**\n * Compute SHA-256 hash of a file's content.\n *\n * @param filePath - Absolute path to the file\n * @returns Hex-encoded SHA-256 hash\n */\nexport async function computeContentHash(filePath: string): Promise<string> {\n  const content = await readFile(filePath);\n  return createHash('sha256').update(content).digest('hex');\n}\n\n/**\n * Compute SHA-256 hash from an already-loaded string.\n *\n * Use this when the file content is already in memory to avoid\n * a redundant disk read.\n *\n * @param content - The file content as a string\n * @returns Hex-encoded SHA-256 hash\n */\nexport function computeContentHashFromString(content: string): string {\n  return createHash('sha256').update(content).digest('hex');\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**detector.ts implements Git-based change detection via simple-git and SHA-256 content hashing via Node.js crypto for incremental documentation updates.**\n\n## Exported Functions\n\n### isGitRepo\n```typescript\nasync function isGitRepo(projectRoot: string): Promise<boolean>\n```\nChecks if `projectRoot` contains a Git repository by calling `simpleGit(projectRoot).checkIsRepo()`.\n\n### getCurrentCommit\n```typescript\nasync function getCurrentCommit(projectRoot: string): Promise<string>\n```\nReturns HEAD commit hash via `git.revparse(['HEAD'])`, trimmed to remove trailing whitespace.\n\n### getChangedFiles\n```typescript\nasync function getChangedFiles(\n  projectRoot: string,\n  baseCommit: string,\n  options: ChangeDetectionOptions = {}\n): Promise<ChangeDetectionResult>\n```\nDetects files changed between `baseCommit` and HEAD using `git diff --name-status -M` (rename detection with 50% similarity threshold). Parses diff output line-by-line with tab-separated status codes: `'A'` (added), `'M'` (modified), `'D'` (deleted), `'R'` prefix (renamed). For renames, extracts `oldPath` from `parts[1]` and new path from `parts[parts.length - 1]`. When `options.includeUncommitted` is true, merges uncommitted changes from `git.status()` arrays: `modified`, `deleted`, `not_added` (untracked), `staged`. Returns `ChangeDetectionResult` with `currentCommit`, `baseCommit`, `changes[]` array of `FileChange`, and `includesUncommitted` boolean.\n\n### computeContentHash\n```typescript\nasync function computeContentHash(filePath: string): Promise<string>\n```\nReads file content via `readFile(filePath)`, computes SHA-256 hash via `createHash('sha256').update(content).digest('hex')`, returns hex-encoded digest. Used for comparing `.sum` frontmatter `content_hash` against current file state.\n\n### computeContentHashFromString\n```typescript\nfunction computeContentHashFromString(content: string): string\n```\nSynchronous variant accepting in-memory string content, returns hex-encoded SHA-256 digest. Avoids redundant disk read when file content already loaded.\n\n## Git Diff Parsing\n\nParses `git diff --name-status -M` output with format `STATUS\\tFILE` or `STATUS\\tOLD\\tNEW` for renames. Status codes:\n- `'A'` → `status: 'added'`\n- `'M'` → `status: 'modified'`\n- `'D'` → `status: 'deleted'`\n- `'R'` prefix (e.g., `'R100'`) → `status: 'renamed'` with `oldPath` extracted from second tab-delimited part\n\n## Uncommitted Changes Merge\n\nWhen `options.includeUncommitted` is true, calls `git.status()` and merges:\n- `status.modified[]` → `status: 'modified'`\n- `status.deleted[]` → `status: 'deleted'`\n- `status.not_added[]` → `status: 'added'` (untracked files)\n- `status.staged[]` → `status: 'added'` (staged additions)\n\nDeduplicates via `changes.some(c => c.path === file)` check before pushing to `changes[]` array.\n\n## Integration with Update Workflow\n\nUsed by `src/update/orchestrator.ts` to compute `ChangeDetectionResult`, which drives incremental update decisions: hash mismatches trigger `.sum` regeneration, renamed `oldPath` values enable orphan cleanup, uncommitted changes support `--uncommitted` flag behavior.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 911,
      "cacheReadTokens": 17114,
      "cacheCreationTokens": 10365,
      "latencyMs": 30145,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/change-detection/detector.ts",
          "sizeBytes": 4102
        }
      ]
    },
    {
      "timestamp": "2026-02-09T16:09:10.108Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/ai/types.ts\n\n```typescript\n/**\n * Shared types for the AI service layer.\n *\n * Defines the contract for backends, responses, subprocess results,\n * retry configuration, and telemetry logging. Every AI service module\n * imports from this file.\n */\n\n// ---------------------------------------------------------------------------\n// Subprocess\n// ---------------------------------------------------------------------------\n\n/**\n * Result returned by the subprocess wrapper after a CLI process completes.\n *\n * Always populated -- even on error or timeout, the fields are filled\n * with whatever information was available.\n */\nexport interface SubprocessResult {\n  /** Standard output captured from the child process */\n  stdout: string;\n  /** Standard error captured from the child process */\n  stderr: string;\n  /** Numeric exit code (0 = success, non-zero = failure) */\n  exitCode: number;\n  /** Signal that terminated the process, or `null` if it exited normally */\n  signal: string | null;\n  /** Wall-clock duration in milliseconds */\n  durationMs: number;\n  /** Whether the process was killed because it exceeded its timeout */\n  timedOut: boolean;\n  /** OS PID of the child process (undefined if spawn failed) */\n  childPid?: number;\n}\n\n// ---------------------------------------------------------------------------\n// AI Call\n// ---------------------------------------------------------------------------\n\n/**\n * Input options for an AI call.\n *\n * Only `prompt` is required; all other fields have backend-specific defaults.\n */\nexport interface AICallOptions {\n  /** The prompt to send to the AI model */\n  prompt: string;\n  /** Optional system prompt to set context/behavior */\n  systemPrompt?: string;\n  /** Model identifier (e.g., \"sonnet\", \"opus\") -- backend interprets this */\n  model?: string;\n  /** Subprocess timeout in milliseconds (overrides config default) */\n  timeoutMs?: number;\n  /** Maximum number of agentic turns (backend-specific) */\n  maxTurns?: number;\n  /** Label for tracing (e.g., file path being processed) */\n  taskLabel?: string;\n}\n\n/**\n * Normalized response from any AI CLI backend.\n *\n * Every backend adapter must parse its CLI's raw output into this shape\n * so that callers never need to know which backend was used.\n */\nexport interface AIResponse {\n  /** The AI model's text response */\n  text: string;\n  /** Model identifier as reported by the backend */\n  model: string;\n  /** Number of input tokens consumed */\n  inputTokens: number;\n  /** Number of output tokens generated */\n  outputTokens: number;\n  /** Number of tokens served from cache reads */\n  cacheReadTokens: number;\n  /** Number of tokens written to cache */\n  cacheCreationTokens: number;\n  /** Wall-clock duration in milliseconds */\n  durationMs: number;\n  /** Process exit code from the CLI */\n  exitCode: number;\n  /** Original CLI JSON output for debugging */\n  raw: unknown;\n}\n\n// ---------------------------------------------------------------------------\n// Backend\n// ---------------------------------------------------------------------------\n\n/**\n * Contract for an AI CLI backend adapter.\n *\n * Each supported CLI (Claude, Gemini, OpenCode) implements this interface.\n * The registry selects the appropriate backend at runtime.\n *\n * @example\n * ```typescript\n * const backend: AIBackend = new ClaudeBackend();\n * if (await backend.isAvailable()) {\n *   const args = backend.buildArgs({ prompt: 'Hello' });\n *   // spawn the process with backend.cliCommand and args\n * }\n * ```\n */\nexport interface AIBackend {\n  /** Human-readable backend name (e.g., \"Claude\", \"Gemini\") */\n  readonly name: string;\n  /** CLI executable name on PATH (e.g., \"claude\", \"gemini\") */\n  readonly cliCommand: string;\n\n  /** Check whether this backend's CLI is available on PATH */\n  isAvailable(): Promise<boolean>;\n\n  /** Build the CLI argument array for a given call */\n  buildArgs(options: AICallOptions): string[];\n\n  /** Parse the CLI's stdout into a normalized {@link AIResponse} */\n  parseResponse(stdout: string, durationMs: number, exitCode: number): AIResponse;\n\n  /** Get user-facing install instructions when the CLI is not found */\n  getInstallInstructions(): string;\n}\n\n// ---------------------------------------------------------------------------\n// Retry\n// ---------------------------------------------------------------------------\n\n/**\n * Configuration for the retry utility.\n *\n * Controls exponential backoff timing and which errors are retryable.\n */\nexport interface RetryOptions {\n  /** Maximum number of retries (e.g., 3 means up to 4 total attempts) */\n  maxRetries: number;\n  /** Base delay in milliseconds before first retry */\n  baseDelayMs: number;\n  /** Maximum delay cap in milliseconds */\n  maxDelayMs: number;\n  /** Exponential multiplier applied to the base delay */\n  multiplier: number;\n  /** Predicate that returns `true` if the error is transient and retryable */\n  isRetryable: (error: unknown) => boolean;\n  /** Optional callback invoked before each retry attempt */\n  onRetry?: (attempt: number, error: unknown) => void;\n}\n\n// ---------------------------------------------------------------------------\n// Telemetry\n// ---------------------------------------------------------------------------\n\n/**\n * Record of a single file read that was sent as context to an AI call.\n */\nexport interface FileRead {\n  /** File path relative to project root */\n  path: string;\n  /** File size in bytes at time of read */\n  sizeBytes: number;\n}\n\n/**\n * Per-call telemetry log entry.\n *\n * Captures everything needed to replay or debug a single AI call\n * without re-running it.\n */\nexport interface TelemetryEntry {\n  /** ISO 8601 timestamp when the call was initiated */\n  timestamp: string;\n  /** The prompt that was sent */\n  prompt: string;\n  /** The system prompt, if one was used */\n  systemPrompt?: string;\n  /** The AI model's text response */\n  response: string;\n  /** Model identifier */\n  model: string;\n  /** Number of input tokens consumed */\n  inputTokens: number;\n  /** Number of output tokens generated */\n  outputTokens: number;\n  /** Number of tokens served from cache reads */\n  cacheReadTokens: number;\n  /** Number of tokens written to cache */\n  cacheCreationTokens: number;\n  /** Wall-clock latency in milliseconds */\n  latencyMs: number;\n  /** Process exit code */\n  exitCode: number;\n  /** Error message, if the call failed */\n  error?: string;\n  /** Number of retries that occurred before this result */\n  retryCount: number;\n  /** AI thinking/reasoning content. \"not supported\" when backend doesn't provide it */\n  thinking: string;\n  /** Files sent as context for this call */\n  filesRead: FileRead[];\n}\n\n/**\n * Per-run log file structure.\n *\n * Aggregates all {@link TelemetryEntry} instances for a single CLI run,\n * plus a computed summary for quick performance review.\n */\nexport interface RunLog {\n  /** Unique run identifier (ISO timestamp-based) */\n  runId: string;\n  /** ISO 8601 timestamp when the run started */\n  startTime: string;\n  /** ISO 8601 timestamp when the run finished */\n  endTime: string;\n  /** All individual call entries */\n  entries: TelemetryEntry[];\n  /** Aggregated summary across all entries */\n  summary: {\n    /** Total number of AI calls made */\n    totalCalls: number;\n    /** Sum of input tokens across all calls */\n    totalInputTokens: number;\n    /** Sum of output tokens across all calls */\n    totalOutputTokens: number;\n    /** Total wall-clock duration in milliseconds */\n    totalDurationMs: number;\n    /** Number of calls that resulted in an error */\n    errorCount: number;\n    /** Sum of cache read tokens across all calls */\n    totalCacheReadTokens: number;\n    /** Sum of cache creation tokens across all calls */\n    totalCacheCreationTokens: number;\n    /** Total file reads across all calls (including duplicates) */\n    totalFilesRead: number;\n    /** Unique files read (deduped by path) */\n    uniqueFilesRead: number;\n  };\n}\n\n// ---------------------------------------------------------------------------\n// Errors\n// ---------------------------------------------------------------------------\n\n/** Error codes for typed error handling in the AI service layer */\nexport type AIServiceErrorCode =\n  | 'CLI_NOT_FOUND'\n  | 'TIMEOUT'\n  | 'PARSE_ERROR'\n  | 'SUBPROCESS_ERROR'\n  | 'RATE_LIMIT';\n\n/**\n * Typed error for AI service failures.\n *\n * Carries a machine-readable {@link AIServiceErrorCode} so callers can\n * branch on the error type without parsing message strings.\n *\n * @example\n * ```typescript\n * try {\n *   await callAI(options);\n * } catch (error) {\n *   if (error instanceof AIServiceError && error.code === 'RATE_LIMIT') {\n *     // handle rate limiting\n *   }\n * }\n * ```\n */\nexport class AIServiceError extends Error {\n  /** Machine-readable error code */\n  readonly code: AIServiceErrorCode;\n\n  constructor(code: AIServiceErrorCode, message: string) {\n    super(message);\n    this.name = 'AIServiceError';\n    this.code = code;\n  }\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**Defines TypeScript interfaces, type aliases, and the AIServiceError class for the AI service layer, establishing contracts for backend adapters, subprocess results, API responses, retry configuration, and telemetry logging structures.**\n\n## Exported Types\n\n### Subprocess Result\n\n**SubprocessResult** captures the outcome of a CLI subprocess execution with fields:\n- `stdout: string` — standard output captured from child process\n- `stderr: string` — standard error captured from child process  \n- `exitCode: number` — numeric exit code (0 = success, non-zero = failure)\n- `signal: string | null` — signal that terminated the process, or null for normal exit\n- `durationMs: number` — wall-clock duration in milliseconds\n- `timedOut: boolean` — whether process exceeded its timeout\n- `childPid?: number` — OS PID of child process (undefined if spawn failed)\n\n### AI Call Interfaces\n\n**AICallOptions** defines input parameters for AI service calls:\n- `prompt: string` — required prompt text sent to AI model\n- `systemPrompt?: string` — optional system prompt for context/behavior\n- `model?: string` — model identifier (e.g., \"sonnet\", \"opus\") interpreted by backend\n- `timeoutMs?: number` — subprocess timeout override in milliseconds\n- `maxTurns?: number` — maximum agentic turns (backend-specific)\n- `taskLabel?: string` — label for tracing (e.g., file path being processed)\n\n**AIResponse** normalizes backend CLI outputs into a unified structure:\n- `text: string` — AI model's text response\n- `model: string` — model identifier reported by backend\n- `inputTokens: number` — input tokens consumed\n- `outputTokens: number` — output tokens generated\n- `cacheReadTokens: number` — tokens served from cache reads\n- `cacheCreationTokens: number` — tokens written to cache\n- `durationMs: number` — wall-clock duration in milliseconds\n- `exitCode: number` — process exit code from CLI\n- `raw: unknown` — original CLI JSON output for debugging\n\n### Backend Contract\n\n**AIBackend** interface defines the contract for AI CLI backend adapters (Claude, Gemini, OpenCode):\n- `readonly name: string` — human-readable backend name (e.g., \"Claude\", \"Gemini\")\n- `readonly cliCommand: string` — CLI executable name on PATH (e.g., \"claude\", \"gemini\")\n- `isAvailable(): Promise<boolean>` — checks whether CLI is available on PATH\n- `buildArgs(options: AICallOptions): string[]` — builds CLI argument array for a call\n- `parseResponse(stdout: string, durationMs: number, exitCode: number): AIResponse` — parses CLI stdout into normalized AIResponse\n- `getInstallInstructions(): string` — returns user-facing install instructions when CLI not found\n\n### Retry Configuration\n\n**RetryOptions** controls exponential backoff retry behavior:\n- `maxRetries: number` — maximum retry attempts (e.g., 3 means up to 4 total attempts)\n- `baseDelayMs: number` — base delay in milliseconds before first retry\n- `maxDelayMs: number` — maximum delay cap in milliseconds\n- `multiplier: number` — exponential multiplier applied to base delay\n- `isRetryable: (error: unknown) => boolean` — predicate returning true if error is transient and retryable\n- `onRetry?: (attempt: number, error: unknown) => void` — optional callback invoked before each retry\n\n### Telemetry Structures\n\n**FileRead** records a single file read sent as context to an AI call:\n- `path: string` — file path relative to project root\n- `sizeBytes: number` — file size in bytes at time of read\n\n**TelemetryEntry** captures per-call telemetry for a single AI invocation:\n- `timestamp: string` — ISO 8601 timestamp when call initiated\n- `prompt: string` — prompt text sent\n- `systemPrompt?: string` — system prompt if used\n- `response: string` — AI model's text response\n- `model: string` — model identifier\n- `inputTokens: number` — input tokens consumed\n- `outputTokens: number` — output tokens generated\n- `cacheReadTokens: number` — tokens served from cache reads\n- `cacheCreationTokens: number` — tokens written to cache\n- `latencyMs: number` — wall-clock latency in milliseconds\n- `exitCode: number` — process exit code\n- `error?: string` — error message if call failed\n- `retryCount: number` — number of retries before this result\n- `thinking: string` — AI thinking/reasoning content, \"not supported\" when backend doesn't provide it\n- `filesRead: FileRead[]` — files sent as context for this call\n\n**RunLog** aggregates all TelemetryEntry instances for a single CLI run:\n- `runId: string` — unique run identifier (ISO timestamp-based)\n- `startTime: string` — ISO 8601 timestamp when run started\n- `endTime: string` — ISO 8601 timestamp when run finished\n- `entries: TelemetryEntry[]` — all individual call entries\n- `summary` — aggregated summary object with fields:\n  - `totalCalls: number` — total AI calls made\n  - `totalInputTokens: number` — sum of input tokens across all calls\n  - `totalOutputTokens: number` — sum of output tokens across all calls\n  - `totalDurationMs: number` — total wall-clock duration in milliseconds\n  - `errorCount: number` — calls resulting in errors\n  - `totalCacheReadTokens: number` — sum of cache read tokens\n  - `totalCacheCreationTokens: number` — sum of cache creation tokens\n  - `totalFilesRead: number` — total file reads including duplicates\n  - `uniqueFilesRead: number` — unique files read, deduped by path\n\n### Error Types\n\n**AIServiceErrorCode** is a discriminated union type alias for typed error handling:\n```typescript\ntype AIServiceErrorCode = 'CLI_NOT_FOUND' | 'TIMEOUT' | 'PARSE_ERROR' | 'SUBPROCESS_ERROR' | 'RATE_LIMIT'\n```\n\n**AIServiceError** extends Error with a machine-readable code field:\n- `readonly code: AIServiceErrorCode` — machine-readable error code\n- `constructor(code: AIServiceErrorCode, message: string)` — constructs typed error with code and message\n- Sets `name` property to `'AIServiceError'`\n\nEnables branching on error.code without parsing message strings (e.g., `if (error.code === 'RATE_LIMIT')`).\n\n## Integration Points\n\nThis types module is imported by all AI service modules:\n- `src/ai/service.ts` — main AIService class consuming AIBackend and AICallOptions\n- `src/ai/subprocess.ts` — runSubprocess() returning SubprocessResult\n- `src/ai/registry.ts` — backend registry managing AIBackend implementations\n- `src/ai/retry.ts` — retry utility consuming RetryOptions\n- `src/ai/telemetry/logger.ts` — telemetry logger writing TelemetryEntry and RunLog structures\n- `src/ai/backends/*.ts` — Claude/Gemini/OpenCode adapters implementing AIBackend interface",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 1744,
      "cacheReadTokens": 17114,
      "cacheCreationTokens": 11411,
      "latencyMs": 33420,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/ai/types.ts",
          "sizeBytes": 8930
        }
      ]
    },
    {
      "timestamp": "2026-02-09T16:09:14.540Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/change-detection/types.ts\n\n```typescript\n/**\n * Types for git change detection\n */\n\n/**\n * Type of change detected for a file\n */\nexport type ChangeType = 'added' | 'modified' | 'deleted' | 'renamed';\n\n/**\n * A file change detected from git diff\n */\nexport interface FileChange {\n  /** Relative path to the file (new path for renames) */\n  path: string;\n  /** Type of change */\n  status: ChangeType;\n  /** Original path for renamed files */\n  oldPath?: string;\n}\n\n/**\n * Result of change detection\n */\nexport interface ChangeDetectionResult {\n  /** Current commit hash */\n  currentCommit: string;\n  /** Commit hash we're comparing from */\n  baseCommit: string;\n  /** List of changed files */\n  changes: FileChange[];\n  /** Whether uncommitted changes were included */\n  includesUncommitted: boolean;\n}\n\n/**\n * Options for change detection\n */\nexport interface ChangeDetectionOptions {\n  /** Include uncommitted (staged and working directory) changes */\n  includeUncommitted?: boolean;\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**Defines TypeScript types and interfaces for git-based change detection, representing file modifications tracked via `ChangeType` discriminants ('added' | 'modified' | 'deleted' | 'renamed') and structured as `FileChange` objects with path information and optional rename tracking.**\n\n## Exported Types\n\n**`ChangeType`** — String literal union type representing the four possible file change states:\n```typescript\ntype ChangeType = 'added' | 'modified' | 'deleted' | 'renamed'\n```\n\n## Exported Interfaces\n\n**`FileChange`** — Represents a single file modification detected from git diff output:\n- `path: string` — Relative path to the file (for renamed files, contains the new path)\n- `status: ChangeType` — Discriminant field indicating change type\n- `oldPath?: string` — Optional field present only when `status === 'renamed'`, stores original pre-rename path\n\n**`ChangeDetectionResult`** — Aggregated output from git diff analysis:\n- `currentCommit: string` — SHA-1 hash of HEAD or current working tree state\n- `baseCommit: string` — SHA-1 hash of comparison baseline (e.g., previous commit, tag, or branch)\n- `changes: FileChange[]` — Array of detected modifications\n- `includesUncommitted: boolean` — Flag indicating whether working tree and staged changes were merged into results (corresponds to `git status --porcelain` inclusion)\n\n**`ChangeDetectionOptions`** — Configuration object for change detection behavior:\n- `includeUncommitted?: boolean` — When `true`, merges uncommitted working tree and staged changes with committed diff results via `git status --porcelain` parsing\n\n## Usage Context\n\nThese types support the incremental update workflow (`src/update/orchestrator.ts`) where `FileChange` objects drive selective `.sum` regeneration. The `status` field enables conditional logic:\n- `'added'` or `'modified'` → queue for Phase 1 analysis\n- `'deleted'` → trigger orphan cleanup via `cleanupOrphans()`\n- `'renamed'` → use `oldPath` to locate stale `.sum` file and `path` for new generation target\n\nThe `includesUncommitted` flag in `ChangeDetectionResult` determines whether `getAffectedDirectories()` must account for working tree modifications not yet committed, ensuring `AGENTS.md` regeneration covers all dirty state.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 566,
      "cacheReadTokens": 17114,
      "cacheCreationTokens": 9365,
      "latencyMs": 30675,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/change-detection/types.ts",
          "sizeBytes": 946
        }
      ]
    },
    {
      "timestamp": "2026-02-09T16:09:19.418Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/cli/init.ts\n\n```typescript\n/**\n * `are init` command - Create default configuration\n *\n * Creates the `.agents-reverse/config.yaml` file with documented defaults.\n * Warns if configuration already exists.\n */\n\nimport path from 'node:path';\nimport { configExists, writeDefaultConfig, CONFIG_DIR, CONFIG_FILE } from '../config/loader.js';\nimport { createLogger } from '../output/logger.js';\n\n/**\n * Execute the `are init` command.\n *\n * Creates a default configuration file at `.agents-reverse/config.yaml`.\n * If the file already exists, logs a warning and returns without modification.\n *\n * @param root - Root directory where config will be created\n *\n * @example\n * ```typescript\n * await initCommand('.');\n * // Creates .agents-reverse/config.yaml in current directory\n * ```\n */\nexport async function initCommand(root: string, options?: { force?: boolean }): Promise<void> {\n  const resolvedRoot = path.resolve(root);\n  const configPath = path.join(resolvedRoot, CONFIG_DIR, CONFIG_FILE);\n  const force = options?.force ?? false;\n\n  const logger = createLogger({ colors: true });\n\n  try {\n    // Check if config already exists\n    if (!force && await configExists(resolvedRoot)) {\n      logger.warn(`Config already exists at ${configPath}`);\n      logger.info('Edit the file to customize exclusions and options.');\n    } else {\n      // Create default config\n      await writeDefaultConfig(resolvedRoot);\n\n      logger.info(`Created configuration at ${configPath}`);\n      logger.info('');\n      logger.info('Edit the file to customize:');\n      logger.info('  - exclude.patterns: Custom glob patterns to exclude');\n      logger.info('  - ai.concurrency: Parallel AI calls (1-20, default: auto)');\n      logger.info('  - ai.timeoutMs: Subprocess timeout (default: 300,000ms = 5 min)');\n      logger.info('  - ai.backend: AI backend (claude/gemini/opencode/auto)');\n      logger.info('');\n      logger.info('See README.md for full configuration reference.');\n    }\n  } catch (err) {\n    const error = err as NodeJS.ErrnoException;\n\n    // Permission error\n    if (error.code === 'EACCES' || error.code === 'EPERM') {\n      logger.error(`Permission denied: Cannot create ${configPath}`);\n      logger.info('Check that you have write permissions to this directory.');\n      process.exit(1);\n    }\n\n    // Other error\n    logger.error(`Failed to create configuration: ${error.message}`);\n    process.exit(1);\n  }\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**initCommand() creates `.agents-reverse-engineer/config.yaml` with defaults via writeDefaultConfig(), warns if configExists() returns true, and exits on EACCES/EPERM errors.**\n\n## Exported Interface\n\n```typescript\nasync function initCommand(root: string, options?: { force?: boolean }): Promise<void>\n```\n\nAccepts `root` (resolved via `path.resolve()`) and optional `options.force` flag (default: `false`). When `force` is false and `configExists(resolvedRoot)` returns true, logs warning and skips creation. Otherwise calls `writeDefaultConfig(resolvedRoot)` to generate configuration file.\n\n## Dependencies\n\n- `configExists()` — Checks for existing configuration at `${root}/${CONFIG_DIR}/${CONFIG_FILE}`\n- `writeDefaultConfig()` — Writes YAML config with documented defaults from `src/config/loader.ts`\n- `CONFIG_DIR` — Constant `'.agents-reverse-engineer'` (config directory name)\n- `CONFIG_FILE` — Constant `'config.yaml'` (config file name)\n- `createLogger()` — Factory from `src/output/logger.ts` instantiated with `{ colors: true }`\n\n## Error Handling\n\nCatches `NodeJS.ErrnoException` and discriminates on `error.code`:\n\n- `'EACCES'` or `'EPERM'` → Logs `\"Permission denied: Cannot create ${configPath}\"` with permission guidance, calls `process.exit(1)`\n- Other errors → Logs `\"Failed to create configuration: ${error.message}\"`, calls `process.exit(1)`\n\n## Output Messages\n\nSuccess path logs:\n- `\"Created configuration at ${configPath}\"`\n- Multi-line guidance referencing `exclude.patterns`, `ai.concurrency` (range: 1-20), `ai.timeoutMs` (default: 300,000ms = 5 minutes), `ai.backend` (values: claude/gemini/opencode/auto)\n- `\"See README.md for full configuration reference.\"`\n\nExisting config path logs:\n- `\"Config already exists at ${configPath}\"`\n- `\"Edit the file to customize exclusions and options.\"`\n\n## File System Operations\n\nConstructs config path via `path.join(resolvedRoot, CONFIG_DIR, CONFIG_FILE)` where:\n- `CONFIG_DIR` = `'.agents-reverse-engineer'`\n- `CONFIG_FILE` = `'config.yaml'`\n\nExample resolved path: `${resolvedRoot}/.agents-reverse-engineer/config.yaml`",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 606,
      "cacheReadTokens": 17114,
      "cacheCreationTokens": 9761,
      "latencyMs": 27355,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/cli/init.ts",
          "sizeBytes": 2388
        }
      ]
    },
    {
      "timestamp": "2026-02-09T16:09:15.093Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/cli/clean.ts\n\n```typescript\n/**\n * `are clean` command - Delete all generated documentation artifacts\n *\n * Removes .sum files, AGENTS.md files, root documents (CLAUDE.md),\n * and the GENERATION-PLAN.md file.\n */\n\nimport path from 'node:path';\nimport { access, readFile, rename, unlink } from 'node:fs/promises';\nimport { constants } from 'node:fs';\nimport fg from 'fast-glob';\nimport pc from 'picocolors';\nimport { createLogger } from '../output/logger.js';\nimport { GENERATED_MARKER } from '../generation/writers/agents-md.js';\n\n/**\n * Options for the clean command.\n */\nexport interface CleanOptions {\n  /**\n   * Show files that would be deleted without deleting them.\n   * @default false\n   */\n  dryRun: boolean;\n}\n\n/**\n * Execute the `are clean` command.\n *\n * Finds and deletes all generated documentation artifacts:\n * - `*.sum` files\n * - `AGENTS.md` files\n * - `CLAUDE.md` at project root\n * - `.agents-reverse-engineer/GENERATION-PLAN.md`\n *\n * @param targetPath - Project root directory (defaults to current working directory)\n * @param options - Command options\n */\nexport async function cleanCommand(\n  targetPath: string,\n  options: CleanOptions\n): Promise<void> {\n  const resolvedPath = path.resolve(targetPath || process.cwd());\n\n  const logger = createLogger({ colors: true });\n\n  // Verify target path exists\n  try {\n    await access(resolvedPath, constants.R_OK);\n  } catch (err) {\n    const error = err as NodeJS.ErrnoException;\n    if (error.code === 'ENOENT') {\n      logger.error(`Directory not found: ${resolvedPath}`);\n      process.exit(1);\n    }\n    if (error.code === 'EACCES' || error.code === 'EPERM') {\n      logger.error(`Permission denied: ${resolvedPath}`);\n      process.exit(1);\n    }\n    throw error;\n  }\n\n  // Find all artifacts\n  const [sumFiles, agentsFiles, localAgentsFiles] = await Promise.all([\n    fg.glob('**/*.sum', {\n      cwd: resolvedPath,\n      absolute: true,\n      onlyFiles: true,\n      dot: true,\n      ignore: ['**/node_modules/**', '**/.git/**'],\n    }),\n    fg.glob('**/AGENTS.md', {\n      cwd: resolvedPath,\n      absolute: true,\n      onlyFiles: true,\n      dot: true,\n      ignore: ['**/node_modules/**', '**/.git/**'],\n    }),\n    fg.glob('**/AGENTS.local.md', {\n      cwd: resolvedPath,\n      absolute: true,\n      onlyFiles: true,\n      dot: true,\n      ignore: ['**/node_modules/**', '**/.git/**'],\n    }),\n  ]);\n\n  // Filter AGENTS.md to only those generated by ARE (contain the marker).\n  // User-authored AGENTS.md files (e.g. SDK docs) must not be deleted.\n  const generatedAgentsFiles: string[] = [];\n  const skippedAgentsFiles: string[] = [];\n  for (const file of agentsFiles) {\n    try {\n      const content = await readFile(file, 'utf-8');\n      if (content.includes(GENERATED_MARKER)) {\n        generatedAgentsFiles.push(file);\n      } else {\n        skippedAgentsFiles.push(file);\n      }\n    } catch {\n      // Can't read — skip silently\n    }\n  }\n\n  // Check for root docs and plan file\n  const singleFiles: string[] = [];\n  const claudeMd = path.join(resolvedPath, 'CLAUDE.md');\n  const planFile = path.join(resolvedPath, '.agents-reverse-engineer', 'GENERATION-PLAN.md');\n\n  for (const filePath of [claudeMd, planFile]) {\n    try {\n      await access(filePath, constants.F_OK);\n      singleFiles.push(filePath);\n    } catch {\n      // File doesn't exist - skip\n    }\n  }\n\n  const allFiles = [...sumFiles, ...generatedAgentsFiles, ...singleFiles];\n\n  if (allFiles.length === 0 && localAgentsFiles.length === 0) {\n    logger.info('No generated artifacts found.');\n    return;\n  }\n\n  // Display found files\n  const relativePath = (absPath: string): string =>\n    path.relative(resolvedPath, absPath);\n\n  if (options.dryRun) {\n    logger.info('Files that would be deleted:');\n  }\n\n  for (const file of allFiles) {\n    logger.info(`  ${relativePath(file)}`);\n  }\n\n  if (skippedAgentsFiles.length > 0) {\n    logger.info('');\n    logger.info('Preserving user-authored AGENTS.md:');\n    for (const file of skippedAgentsFiles) {\n      logger.info(`  ${relativePath(file)}`);\n    }\n  }\n\n  if (localAgentsFiles.length > 0) {\n    logger.info('');\n    logger.info(options.dryRun ? 'Files that would be restored:' : 'Restoring user-defined files:');\n    for (const file of localAgentsFiles) {\n      const target = path.join(path.dirname(file), 'AGENTS.md');\n      logger.info(`  ${relativePath(file)} → ${relativePath(target)}`);\n    }\n  }\n\n  logger.info('');\n  logger.info(\n    `${pc.bold(String(sumFiles.length))} .sum file(s), ` +\n    `${pc.bold(String(generatedAgentsFiles.length))} AGENTS.md file(s), ` +\n    `${pc.bold(String(singleFiles.length))} root doc(s), ` +\n    `${pc.bold(String(localAgentsFiles.length))} AGENTS.local.md to restore`\n  );\n\n  if (options.dryRun) {\n    logger.info('');\n    logger.info(pc.yellow('Dry run — no files were changed.'));\n    return;\n  }\n\n  // Delete all generated files\n  let deleted = 0;\n  for (const file of allFiles) {\n    try {\n      await unlink(file);\n      deleted++;\n    } catch (err) {\n      logger.error(`Failed to delete ${relativePath(file)}: ${(err as Error).message}`);\n    }\n  }\n\n  // Restore AGENTS.local.md → AGENTS.md (undo the rename from generation)\n  let restored = 0;\n  for (const localFile of localAgentsFiles) {\n    const agentsPath = path.join(path.dirname(localFile), 'AGENTS.md');\n    try {\n      await rename(localFile, agentsPath);\n      restored++;\n    } catch (err) {\n      logger.error(`Failed to restore ${relativePath(localFile)}: ${(err as Error).message}`);\n    }\n  }\n\n  logger.info('');\n  const parts = [`Deleted ${deleted} file(s)`];\n  if (restored > 0) {\n    parts.push(`restored ${restored} AGENTS.local.md file(s)`);\n  }\n  logger.info(pc.green(`${parts.join(', ')}.`));\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**cleanCommand() deletes generated ARE documentation artifacts (.sum files, AGENTS.md with marker, CLAUDE.md, GENERATION-PLAN.md) while preserving user-authored AGENTS.md and restoring AGENTS.local.md backups.**\n\n## Exported Interface\n\n**CleanOptions** interface with:\n- `dryRun: boolean` — Preview deletions without modifying filesystem (default: false)\n\n**cleanCommand(targetPath: string, options: CleanOptions): Promise<void>** — Entry point for `are clean` command, resolves targetPath to absolute directory, discovers artifacts via fast-glob, filters AGENTS.md by `GENERATED_MARKER` presence, deletes generated files, restores AGENTS.local.md → AGENTS.md.\n\n## Discovery Strategy\n\nUses `fast-glob` with three parallel glob calls:\n- `**/*.sum` — All summary files\n- `**/AGENTS.md` — Directory documentation (requires marker filtering)\n- `**/AGENTS.local.md` — User-authored backups to restore\n\nAll globs use `{ cwd: resolvedPath, absolute: true, onlyFiles: true, dot: true, ignore: ['**/node_modules/**', '**/.git/**'] }` options.\n\nSingle-file checks via `access(filePath, constants.F_OK)` for:\n- `CLAUDE.md` at project root\n- `.agents-reverse-engineer/GENERATION-PLAN.md`\n\n## User Content Preservation\n\n**Generated vs. User-Authored Distinction:**\n- Reads each `AGENTS.md` file with `readFile(file, 'utf-8')`\n- Checks for substring `GENERATED_MARKER` (imported from `src/generation/writers/agents-md.js`)\n- Marker present → add to `generatedAgentsFiles[]` for deletion\n- Marker absent → add to `skippedAgentsFiles[]`, log preservation message\n- Pattern prevents deletion of SDK documentation or manually written AGENTS.md files\n\n**AGENTS.local.md Restoration:**\n- `rename(localFile, agentsPath)` moves AGENTS.local.md → AGENTS.md\n- Reverses the backup operation performed during generation phase\n- Tracked separately in `restored` counter\n\n## Error Handling\n\n**Directory access validation:**\n- `access(resolvedPath, constants.R_OK)` before operation\n- `ENOENT` → log error, `process.exit(1)`\n- `EACCES`/`EPERM` → log error, `process.exit(1)`\n- Other errors → re-throw\n\n**Per-file deletion:**\n- Wraps `unlink()` and `rename()` in try-catch\n- Logs failure message with `relativePath()` and error message\n- Continues processing remaining files (no fail-fast)\n\n## Output Format\n\n**Discovery summary:**\n```\nFiles that would be deleted: (if dryRun)\n  path/to/file.sum\n  path/to/AGENTS.md\n\nPreserving user-authored AGENTS.md:\n  docs/AGENTS.md\n\nFiles that would be restored: (if dryRun)\n  path/AGENTS.local.md → path/AGENTS.md\n\nN .sum file(s), M AGENTS.md file(s), P root doc(s), Q AGENTS.local.md to restore\n```\n\n**Completion message:**\n- Dry run: `Dry run — no files were changed.` (yellow)\n- Success: `Deleted N file(s), restored M AGENTS.local.md file(s).` (green)\n- Failure messages logged individually per file\n\n## Integration Points\n\n**Imports:**\n- `GENERATED_MARKER` from `src/generation/writers/agents-md.js` — HTML comment string for filtering\n- `createLogger({ colors: true })` from `src/output/logger.js` — Terminal output with picocolors\n- `fast-glob` — Artifact discovery with ignore patterns\n- Node.js `fs/promises`: `access`, `readFile`, `rename`, `unlink`\n\n**Invoked by:** `src/cli/index.ts` command dispatcher for `/are-clean` skill and `are clean` CLI command.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 988,
      "cacheReadTokens": 17114,
      "cacheCreationTokens": 10982,
      "latencyMs": 36712,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/cli/clean.ts",
          "sizeBytes": 5700
        }
      ]
    },
    {
      "timestamp": "2026-02-09T16:09:16.423Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/cli/discover.ts\n\n```typescript\n/**\n * `are discover` command - Discover files to analyze\n *\n * Walks a directory tree and applies filters (gitignore, vendor, binary, custom)\n * to identify files suitable for analysis.\n */\n\nimport path from 'node:path';\nimport { access, mkdir, writeFile } from 'node:fs/promises';\nimport { constants } from 'node:fs';\nimport pc from 'picocolors';\nimport { loadConfig } from '../config/loader.js';\nimport { discoverFiles } from '../discovery/run.js';\nimport { createLogger } from '../output/logger.js';\nimport { createOrchestrator } from '../generation/orchestrator.js';\nimport { buildExecutionPlan, formatExecutionPlanAsMarkdown } from '../generation/executor.js';\nimport { ProgressLog } from '../orchestration/index.js';\nimport type { DiscoveryResult } from '../types/index.js';\nimport type { ITraceWriter } from '../orchestration/trace.js';\n\n/**\n * Options for the discover command.\n */\nexport interface DiscoverOptions {\n  /**\n   * Optional trace writer for emitting discovery events.\n   */\n  tracer?: ITraceWriter;\n\n  /**\n   * Enable debug output.\n   * @default false\n   */\n  debug?: boolean;\n}\n\n/**\n * Execute the `are discover` command.\n *\n * Discovers files in the target directory, applying all configured filters\n * (gitignore, vendor, binary, custom patterns).\n *\n * @param targetPath - Directory to scan (defaults to current working directory)\n * @param options - Command options\n *\n * @example\n * ```typescript\n * await discoverCommand('.', {});\n * ```\n */\nexport async function discoverCommand(\n  targetPath: string,\n  options: DiscoverOptions\n): Promise<void> {\n  // Resolve to absolute path (default to cwd)\n  const resolvedPath = path.resolve(targetPath || process.cwd());\n\n  // Load configuration (uses defaults if no config file)\n  const config = await loadConfig(resolvedPath);\n\n  const logger = createLogger({ colors: config.output.colors });\n\n  // Verify target path exists\n  try {\n    await access(resolvedPath, constants.R_OK);\n  } catch (err) {\n    const error = err as NodeJS.ErrnoException;\n    if (error.code === 'ENOENT') {\n      logger.error(`Directory not found: ${resolvedPath}`);\n      process.exit(1);\n    }\n    if (error.code === 'EACCES' || error.code === 'EPERM') {\n      logger.error(`Permission denied: ${resolvedPath}`);\n      process.exit(1);\n    }\n    throw error;\n  }\n\n  // Create progress log for tail -f monitoring\n  const progressLog = ProgressLog.create(resolvedPath);\n  progressLog.write(`=== ARE Discover (${new Date().toISOString()}) ===`);\n  progressLog.write(`Project: ${resolvedPath}`);\n  progressLog.write('');\n\n  logger.info(`Discovering files in ${resolvedPath}...`);\n  logger.info('');\n  progressLog.write(`Discovering files in ${resolvedPath}...`);\n\n  // Emit discovery start trace event\n  const discoveryStartTime = process.hrtime.bigint();\n  options.tracer?.emit({\n    type: 'discovery:start',\n    targetPath: resolvedPath,\n  });\n\n  if (options.debug) {\n    console.error(pc.dim(`[debug] Discovering files in: ${resolvedPath}`));\n  }\n\n  // Run shared discovery pipeline (walk + filter)\n  const result = await discoverFiles(resolvedPath, config, {\n    tracer: options.tracer,\n    debug: options.debug,\n  });\n\n  // Emit discovery end trace event\n  const discoveryEndTime = process.hrtime.bigint();\n  const discoveryDurationMs = Number(discoveryEndTime - discoveryStartTime) / 1_000_000;\n  options.tracer?.emit({\n    type: 'discovery:end',\n    filesIncluded: result.included.length,\n    filesExcluded: result.excluded.length,\n    durationMs: discoveryDurationMs,\n  });\n\n  if (options.debug) {\n    console.error(\n      pc.dim(\n        `[debug] Discovery complete: ${result.included.length} files included, ${result.excluded.length} excluded`\n      )\n    );\n  }\n\n  // Log results\n  // Make paths relative for cleaner output\n  const relativePath = (absPath: string): string =>\n    path.relative(resolvedPath, absPath);\n\n  // Show each included file\n  for (const file of result.included) {\n    const rel = relativePath(file);\n    logger.file(rel);\n    progressLog.write(`  + ${rel}`);\n  }\n\n  // Show each excluded file\n  for (const excluded of result.excluded) {\n    const rel = relativePath(excluded.path);\n    logger.excluded(rel, excluded.reason, excluded.filter);\n    progressLog.write(`  - ${rel} (${excluded.reason}: ${excluded.filter})`);\n  }\n\n  // Summary\n  logger.summary(result.included.length, result.excluded.length);\n  progressLog.write(`\\nDiscovered ${result.included.length} files (${result.excluded.length} excluded)`);\n\n  // Generate GENERATION-PLAN.md\n  {\n    logger.info('');\n    logger.info('Generating execution plan...');\n    progressLog.write('');\n    progressLog.write('Generating execution plan...');\n\n    // Create discovery result for orchestrator\n    const discoveryResult: DiscoveryResult = {\n      files: result.included,\n      excluded: result.excluded.map(e => ({ path: e.path, reason: e.reason })),\n    };\n\n    // Create orchestrator and build generation plan\n    const orchestrator = createOrchestrator(config, resolvedPath);\n    const generationPlan = await orchestrator.createPlan(discoveryResult);\n\n    // Build execution plan with post-order traversal\n    const executionPlan = buildExecutionPlan(generationPlan, resolvedPath);\n\n    // Format as markdown\n    const markdown = formatExecutionPlanAsMarkdown(executionPlan);\n\n    // Write to .agents-reverse-engineer/GENERATION-PLAN.md\n    const configDir = path.join(resolvedPath, '.agents-reverse-engineer');\n    const planPath = path.join(configDir, 'GENERATION-PLAN.md');\n\n    try {\n      await mkdir(configDir, { recursive: true });\n      await writeFile(planPath, markdown, 'utf8');\n      const planRelPath = path.relative(resolvedPath, planPath);\n      logger.info(`Created ${planRelPath}`);\n      progressLog.write(`Created ${planRelPath}`);\n    } catch (err) {\n      const msg = (err as Error).message;\n      logger.error(`Failed to write plan: ${msg}`);\n      progressLog.write(`Error: Failed to write plan: ${msg}`);\n      await progressLog.finalize();\n      process.exit(1);\n    }\n  }\n\n  await progressLog.finalize();\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**discoverCommand() orchestrates file discovery with filter application, GENERATION-PLAN.md creation via post-order traversal, and dual-stream progress reporting (console + progress.log).**\n\n## Exported Interface\n\n**discoverCommand(targetPath: string, options: DiscoverOptions): Promise<void>**\n- Resolves `targetPath` to absolute path (defaults to `process.cwd()`)\n- Loads config via `loadConfig(resolvedPath)` with Zod validation defaults\n- Verifies target path exists via `access(resolvedPath, constants.R_OK)`, exits on `ENOENT`/`EACCES`/`EPERM`\n- Invokes `discoverFiles(resolvedPath, config, { tracer, debug })` for gitignore/vendor/binary/custom filtering\n- Emits `discovery:start` and `discovery:end` trace events with `filesIncluded`/`filesExcluded`/`durationMs`\n- Logs included files via `logger.file(rel)`, excluded files via `logger.excluded(rel, excluded.reason, excluded.filter)`\n- Creates `DiscoveryResult` from `result.included` and `result.excluded` arrays\n- Calls `createOrchestrator(config, resolvedPath).createPlan(discoveryResult)` to build `GenerationPlan`\n- Transforms plan via `buildExecutionPlan(generationPlan, resolvedPath)` applying post-order directory traversal\n- Formats plan via `formatExecutionPlanAsMarkdown(executionPlan)` with phase breakdown (file analysis, directory docs, root synthesis)\n- Writes markdown to `.agents-reverse-engineer/GENERATION-PLAN.md` via `mkdir(configDir, { recursive: true })` + `writeFile(planPath, markdown, 'utf8')`\n- Writes parallel progress entries to `ProgressLog` for `tail -f` monitoring\n\n**DiscoverOptions**\n- `tracer?: ITraceWriter` — Optional NDJSON trace emitter for `discovery:start/end` events\n- `debug?: boolean` — Enables `console.error(pc.dim('[debug] ...'))` output with discovery counts\n\n## Progress Reporting Strategy\n\nDual-stream logging writes identical output to console (via `createLogger()`) and `.agents-reverse-engineer/progress.log` (via `ProgressLog.create(resolvedPath)`). Progress log includes ISO 8601 timestamp header `=== ARE Discover (${new Date().toISOString()}) ===`, project path, file inclusion/exclusion details with relative paths, summary counts, and plan creation status. Finalizes via `progressLog.finalize()` before exit.\n\n## Error Handling\n\nThree filesystem error codes handled explicitly:\n- `ENOENT` → `logger.error('Directory not found')` + `process.exit(1)`\n- `EACCES`/`EPERM` → `logger.error('Permission denied')` + `process.exit(1)`\n- Plan write failure → `logger.error('Failed to write plan')` + `progressLog.finalize()` + `process.exit(1)`\n\nAll other errors rethrown via `throw error`.\n\n## Trace Event Emission\n\nEmits two trace event types via `options.tracer?.emit()`:\n- `discovery:start` with `targetPath: resolvedPath`\n- `discovery:end` with `filesIncluded: result.included.length`, `filesExcluded: result.excluded.length`, `durationMs: Number(discoveryEndTime - discoveryStartTime) / 1_000_000` computed from `process.hrtime.bigint()` delta\n\n## Path Relativization\n\nDefines inline helper `relativePath(absPath: string): string => path.relative(resolvedPath, absPath)` to convert discovered file absolute paths to project-relative paths for cleaner console/log output. Applied to all included files (`logger.file(rel)`), excluded files (`logger.excluded(rel, ...)`), and plan path (`logger.info('Created ' + planRelPath)`).\n\n## Dependencies\n\n- `discoverFiles()` from `../discovery/run.js` — Executes filter chain (gitignore/vendor/binary/custom)\n- `createOrchestrator()` from `../generation/orchestrator.js` — Factory for generation plan builder\n- `buildExecutionPlan()` from `../generation/executor.js` — Post-order directory traversal with phase separation\n- `formatExecutionPlanAsMarkdown()` from `../generation/executor.js` — Markdown formatter with file count/phase summaries\n- `loadConfig()` from `../config/loader.js` — YAML config loader with Zod schema defaults\n- `createLogger()` from `../output/logger.js` — Console logger with picocolors formatting\n- `ProgressLog` from `../orchestration/index.js` — Append-only log file writer for `tail -f` monitoring\n- `ITraceWriter` from `../orchestration/trace.js` — NDJSON trace event emitter interface",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 1198,
      "cacheReadTokens": 17114,
      "cacheCreationTokens": 10888,
      "latencyMs": 37434,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/cli/discover.ts",
          "sizeBytes": 6071
        }
      ]
    },
    {
      "timestamp": "2026-02-09T16:09:17.220Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/cli/index.ts\n\n```typescript\n#!/usr/bin/env node\n/**\n * CLI entry point for agents-reverse\n *\n * Commands:\n *   init              Create default configuration\n *   discover [path]   Discover files to analyze\n *   generate [path]   Generate documentation plan\n *   update [path]     Update docs incrementally\n *   specify [path]    Generate project specification from AGENTS.md docs\n *   clean [path]      Delete all generated artifacts\n */\n\nimport { initCommand } from './init.js';\nimport { discoverCommand } from './discover.js';\nimport { generateCommand, type GenerateOptions } from './generate.js';\nimport { updateCommand, type UpdateCommandOptions } from './update.js';\nimport { cleanCommand, type CleanOptions } from './clean.js';\nimport { specifyCommand, type SpecifyOptions } from './specify.js';\n\nimport { runInstaller, parseInstallerArgs } from '../installer/index.js';\nimport { getVersion } from '../version.js';\n\nconst VERSION = getVersion();\n\nconst USAGE = `\nagents-reverse-engineer - AI-friendly codebase documentation\n\nCommands:\n  install           Install commands and hooks to AI assistant\n  uninstall         Remove installed commands and hooks\n  init              Create default configuration\n  discover [path]   Discover files to analyze (default: current directory)\n  generate [path]   Generate documentation plan (default: current directory)\n  update [path]     Update docs incrementally (default: current directory)\n  specify [path]    Generate project specification from AGENTS.md docs\n  clean [path]      Delete all generated artifacts (.sum, AGENTS.md, etc.)\n\nInstall/Uninstall Options:\n  --runtime <name>  Runtime to target (claude, opencode, gemini, all)\n  -g, --global      Target global config directory\n  -l, --local       Target current project directory\n  --force           Overwrite existing files (init, install, specify)\n\nGeneral Options:\n  --debug           Show AI prompts and backend details\n  --trace           Enable concurrency tracing (.agents-reverse-engineer/traces/)\n  --dry-run         Show plan without writing files (generate, update, specify)\n  --output <path>   Output path for specification (specify only)\n  --multi-file      Split specification into multiple files (specify only)\n  --concurrency <n> Number of concurrent AI calls (default: auto)\n  --fail-fast       Stop on first file analysis failure\n  --uncommitted     Include uncommitted changes (update only)\n  --help, -h        Show this help\n  --version, -V     Show version number\n\nExamples:\n  are install\n  are install --runtime claude -g\n  are uninstall\n  are uninstall --runtime claude -g\n  are init\n  are discover\n  are generate --dry-run\n  are generate --concurrency 3\n  are generate ./my-project --concurrency 3\n  are update\n  are update --uncommitted\n  are specify --dry-run\n  are specify --output ./docs/spec.md --force\n`;\n\n/**\n * Parse command-line arguments.\n *\n * Extracts the command, positional arguments, and flags.\n * Handles global flags (--help, -h) that may appear before the command.\n */\nfunction parseArgs(args: string[]): {\n  command: string | undefined;\n  positional: string[];\n  flags: Set<string>;\n  values: Map<string, string>;\n} {\n  let command: string | undefined;\n  const positional: string[] = [];\n  const flags = new Set<string>();\n  const values = new Map<string, string>();\n\n  for (let i = 0; i < args.length; i++) {\n    const arg = args[i];\n    if (arg.startsWith('--')) {\n      const flagName = arg.slice(2);\n      // Check if next arg is a value (not starting with -)\n      if (i + 1 < args.length && !args[i + 1].startsWith('-')) {\n        values.set(flagName, args[i + 1]);\n        i++; // Skip the value\n      } else {\n        flags.add(flagName);\n      }\n    } else if (arg.startsWith('-')) {\n      // Handle short flags (e.g., -h, -g, -l)\n      for (const char of arg.slice(1)) {\n        switch (char) {\n          case 'h':\n            flags.add('help');\n            break;\n          case 'g':\n            flags.add('global');\n            break;\n          case 'l':\n            flags.add('local');\n            break;\n          case 'V':\n            flags.add('version');\n            break;\n          default:\n            // Unknown short flag - ignore\n            break;\n        }\n      }\n    } else if (!command) {\n      // First non-flag argument is the command\n      command = arg;\n    } else {\n      // Subsequent non-flag arguments are positional\n      positional.push(arg);\n    }\n  }\n\n  return { command, positional, flags, values };\n}\n\n/**\n * Show version and exit.\n */\nfunction showVersion(): void {\n  console.log(`agents-reverse-engineer v${VERSION}`);\n  process.exit(0);\n}\n\n/**\n * Display version banner.\n */\nfunction showVersionBanner(): void {\n  console.log(`agents-reverse-engineer v${VERSION}\\n`);\n}\n\n/**\n * Show usage information and exit.\n */\nfunction showHelp(): void {\n  console.log(USAGE);\n  process.exit(0);\n}\n\n/**\n * Show error for unknown command and exit.\n */\nfunction showUnknownCommand(command: string): void {\n  console.error(`Unknown command: ${command}`);\n  console.error(`Run 'are --help' for usage information.`);\n  process.exit(1);\n}\n\n/**\n * Check if command-line has installer-related flags.\n *\n * Used to detect direct installer invocation without 'install' command.\n */\nfunction hasInstallerFlags(flags: Set<string>, values: Map<string, string>): boolean {\n  return (\n    flags.has('global') ||\n    flags.has('local') ||\n    flags.has('force') ||\n    values.has('runtime')\n  );\n}\n\n/**\n * Main CLI entry point.\n */\nasync function main(): Promise<void> {\n  const args = process.argv.slice(2);\n  const { command, positional, flags, values } = parseArgs(args);\n\n  // Handle version flag\n  if (flags.has('version')) {\n    showVersion();\n  }\n\n  // Handle help flag anywhere (but not if --help is for install command)\n  if (flags.has('help') && !command && !hasInstallerFlags(flags, values)) {\n    showHelp();\n  }\n\n  // No command and no args - launch interactive installer\n  if (args.length === 0) {\n    await runInstaller({\n      global: false,\n      local: false,\n      uninstall: false,\n      force: false,\n      help: false,\n      quiet: false,\n    });\n    return;\n  }\n\n  // Direct installer invocation without 'install' command\n  // Supports: npx agents-reverse-engineer --runtime claude -g\n  if (!command && hasInstallerFlags(flags, values)) {\n    const installerArgs = parseInstallerArgs(args);\n    await runInstaller(installerArgs);\n    return;\n  }\n\n  // Show version banner\n  showVersionBanner();\n\n  // Route to command handlers\n  switch (command) {\n    case 'install': {\n      // Re-parse args for installer-specific flags\n      const installerArgs = parseInstallerArgs(args);\n      await runInstaller(installerArgs);\n      break;\n    }\n\n    case 'uninstall': {\n      // Re-parse args and force uninstall mode\n      const installerArgs = parseInstallerArgs(args);\n      installerArgs.uninstall = true;\n      await runInstaller(installerArgs);\n      break;\n    }\n\n    case 'init': {\n      await initCommand(positional[0] || '.', { force: flags.has('force') });\n      break;\n    }\n\n    case 'clean': {\n      const cleanOpts: CleanOptions = {\n        dryRun: flags.has('dry-run'),\n      };\n      await cleanCommand(positional[0] || '.', cleanOpts);\n      break;\n    }\n\n    case 'discover': {\n      await discoverCommand(positional[0] || '.', {});\n      break;\n    }\n\n    case 'generate': {\n      const options: GenerateOptions = {\n        dryRun: flags.has('dry-run'),\n        concurrency: values.has('concurrency') ? parseInt(values.get('concurrency')!, 10) : undefined,\n        failFast: flags.has('fail-fast'),\n        debug: flags.has('debug'),\n        trace: flags.has('trace'),\n      };\n      await generateCommand(positional[0] || '.', options);\n      break;\n    }\n\n    case 'update': {\n      const options: UpdateCommandOptions = {\n        uncommitted: flags.has('uncommitted'),\n        dryRun: flags.has('dry-run'),\n        concurrency: values.has('concurrency') ? parseInt(values.get('concurrency')!, 10) : undefined,\n        failFast: flags.has('fail-fast'),\n        debug: flags.has('debug'),\n        trace: flags.has('trace'),\n      };\n      await updateCommand(positional[0] || '.', options);\n      break;\n    }\n\n    case 'specify': {\n      const specifyOpts: SpecifyOptions = {\n        output: values.get('output'),\n        force: flags.has('force'),\n        dryRun: flags.has('dry-run'),\n        multiFile: flags.has('multi-file'),\n        debug: flags.has('debug'),\n        trace: flags.has('trace'),\n      };\n      await specifyCommand(positional[0] || '.', specifyOpts);\n      break;\n    }\n\n    default:\n      if (command) {\n        showUnknownCommand(command);\n      }\n      showHelp();\n  }\n}\n\n// Run main and handle any uncaught errors\nmain().catch((err: Error) => {\n  console.error(`Error: ${err.message}`);\n  process.exit(1);\n});\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**index.ts orchestrates CLI command routing, argument parsing, and interactive installer invocation for the agents-reverse-engineer tool.**\n\n## Exported Symbols\n\nNone (entry point script with shebang `#!/usr/bin/env node`).\n\n## Command Routing\n\n`main()` async function parses `process.argv.slice(2)` via `parseArgs()`, routes to command handlers:\n\n- `install` → `runInstaller()` with `parseInstallerArgs()`\n- `uninstall` → `runInstaller()` with `uninstall: true` override\n- `init` → `initCommand(positional[0] || '.', { force })`\n- `clean` → `cleanCommand(positional[0] || '.', { dryRun })`\n- `discover` → `discoverCommand(positional[0] || '.', {})`\n- `generate` → `generateCommand(positional[0] || '.', GenerateOptions)`\n- `update` → `updateCommand(positional[0] || '.', UpdateCommandOptions)`\n- `specify` → `specifyCommand(positional[0] || '.', SpecifyOptions)`\n\n## Argument Parsing\n\n`parseArgs(args: string[])` returns `{ command: string | undefined, positional: string[], flags: Set<string>, values: Map<string, string> }`. Splits args into:\n\n- First non-flag arg → `command`\n- Subsequent non-flag args → `positional[]`\n- `--flagName value` → `values.set('flagName', 'value')`\n- `--flagName` (no value) → `flags.add('flagName')`\n- Short flags `-h`, `-g`, `-l`, `-V` → mapped to long forms (`help`, `global`, `local`, `version`)\n\n## Installer Invocation Modes\n\nThree entry paths to installer:\n\n1. **Interactive mode:** `args.length === 0` → `runInstaller({ global: false, local: false, uninstall: false, force: false, help: false, quiet: false })`\n2. **Direct installer flags:** No command + `hasInstallerFlags()` returns `true` → `runInstaller(parseInstallerArgs(args))` (supports `npx agents-reverse-engineer --runtime claude -g`)\n3. **Explicit install/uninstall command:** `command === 'install'` or `'uninstall'` → `runInstaller(parseInstallerArgs(args))`\n\n`hasInstallerFlags(flags, values)` checks for `global`, `local`, `force`, or `runtime` presence.\n\n## Flag Mappings\n\n`GenerateOptions`:\n- `dryRun: flags.has('dry-run')`\n- `concurrency: values.has('concurrency') ? parseInt(values.get('concurrency')!, 10) : undefined`\n- `failFast: flags.has('fail-fast')`\n- `debug: flags.has('debug')`\n- `trace: flags.has('trace')`\n\n`UpdateCommandOptions` extends `GenerateOptions` with:\n- `uncommitted: flags.has('uncommitted')`\n\n`SpecifyOptions`:\n- `output: values.get('output')`\n- `force: flags.has('force')`\n- `dryRun: flags.has('dry-run')`\n- `multiFile: flags.has('multi-file')`\n- `debug: flags.has('debug')`\n- `trace: flags.has('trace')`\n\n`CleanOptions`:\n- `dryRun: flags.has('dry-run')`\n\n## Version and Help Handling\n\n`showVersion()` prints `agents-reverse-engineer v${VERSION}` via `getVersion()` and exits with code 0.\n\n`showHelp()` prints `USAGE` constant and exits with code 0.\n\n`showVersionBanner()` prints `agents-reverse-engineer v${VERSION}\\n` without exiting (shown before command execution).\n\n`showUnknownCommand(command)` prints error message and exits with code 1.\n\n## Usage Documentation\n\n`USAGE` constant (multiline string) documents:\n\n- Commands: `install`, `uninstall`, `init`, `discover`, `generate`, `update`, `specify`, `clean`\n- Install/Uninstall options: `--runtime <name>`, `-g, --global`, `-l, --local`, `--force`\n- General options: `--debug`, `--trace`, `--dry-run`, `--output <path>`, `--multi-file`, `--concurrency <n>`, `--fail-fast`, `--uncommitted`, `--help, -h`, `--version, -V`\n- Examples: `are install`, `are install --runtime claude -g`, `are uninstall`, `are init`, `are discover`, `are generate --dry-run`, `are generate ./my-project --concurrency 3`, `are update --uncommitted`, `are specify --output ./docs/spec.md --force`\n\n## Error Handling\n\n`main().catch((err: Error) => { console.error(\\`Error: ${err.message}\\`); process.exit(1); })` catches uncaught errors and exits with code 1.\n\n## Dependencies\n\nImports command handlers from:\n- `./init.js` → `initCommand`\n- `./discover.js` → `discoverCommand`\n- `./generate.js` → `generateCommand, GenerateOptions`\n- `./update.js` → `updateCommand, UpdateCommandOptions`\n- `./clean.js` → `cleanCommand, CleanOptions`\n- `./specify.js` → `specifyCommand, SpecifyOptions`\n\nImports installer from `../installer/index.js` → `runInstaller, parseInstallerArgs`.\n\nImports version from `../version.js` → `getVersion()`.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 1380,
      "cacheReadTokens": 17114,
      "cacheCreationTokens": 11653,
      "latencyMs": 37538,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/cli/index.ts",
          "sizeBytes": 8809
        }
      ]
    },
    {
      "timestamp": "2026-02-09T16:09:23.184Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/config/defaults.ts\n\n```typescript\n/**\n * Default configuration values for agents-reverse\n */\n\nimport os from 'node:os';\n\n/** Multiplier applied to CPU core count for default concurrency */\nconst CONCURRENCY_MULTIPLIER = 5;\n\n/** Minimum default concurrency */\nconst MIN_CONCURRENCY = 2;\n\n/** Maximum default concurrency (matches schema .max(20)) */\nconst MAX_CONCURRENCY = 20;\n\n/** Heap budget per subprocess in GB (matches NODE_OPTIONS --max-old-space-size=512) */\nconst SUBPROCESS_HEAP_GB = 0.512;\n\n/** Fraction of total system memory to allocate to subprocesses */\nconst MEMORY_FRACTION = 0.5;\n\n/**\n * Compute the default concurrency based on available CPU cores and system memory.\n *\n * Formula: clamp(cores * 5, MIN, min(memCap, MAX))\n * - cores: os.availableParallelism() or os.cpus().length\n * - memCap: floor(totalMemGB * 0.5 / 0.512) — use at most 50% of RAM for subprocesses\n *\n * @returns Default concurrency value (integer between MIN_CONCURRENCY and MAX_CONCURRENCY)\n */\nexport function getDefaultConcurrency(): number {\n  const cores = typeof os.availableParallelism === 'function'\n    ? os.availableParallelism()\n    : (os.cpus().length || MIN_CONCURRENCY);\n\n  const totalMemGB = os.totalmem() / (1024 ** 3);\n  const memCap = totalMemGB > 1\n    ? Math.floor((totalMemGB * MEMORY_FRACTION) / SUBPROCESS_HEAP_GB)\n    : Infinity;\n\n  const computed = cores * CONCURRENCY_MULTIPLIER;\n  return Math.max(MIN_CONCURRENCY, Math.min(computed, memCap, MAX_CONCURRENCY));\n}\n\n/**\n * Default vendor directories to exclude from analysis.\n * These are typically package managers, build outputs, or version control directories.\n */\nexport const DEFAULT_VENDOR_DIRS = [\n  'node_modules',\n  'vendor',\n  '.git',\n  'dist',\n  'build',\n  '__pycache__',\n  '.next',\n  'venv',\n  '.venv',\n  'target',\n  '.cargo',\n  '.gradle',\n  // AI assistant tooling directories\n  '.agents-reverse-engineer',\n  '.agents',\n  '.planning',\n  '.claude',\n  '.opencode',\n  '.gemini',\n] as const;\n\n/**\n * Default file patterns to exclude from analysis.\n * These patterns use gitignore syntax and are matched by the custom filter.\n */\nexport const DEFAULT_EXCLUDE_PATTERNS = [\n  // AI assistant documentation files\n  'AGENTS.md',\n  'CLAUDE.md',\n  'OPENCODE.md',\n  'GEMINI.md',\n  '**/AGENTS.md',\n  '**/CLAUDE.md',\n  '**/OPENCODE.md',\n  '**/GEMINI.md',\n  // Lock files (not useful for documentation, can be very large)\n  '*.lock',\n  'package-lock.json',\n  'yarn.lock',\n  'pnpm-lock.yaml',\n  'bun.lock',\n  'bun.lockb',\n  'Gemfile.lock',\n  'Cargo.lock',\n  'poetry.lock',\n  'composer.lock',\n  'go.sum',\n  // Dotfiles and generated artifacts (path.extname returns '' for dotfiles,\n  // so these must be matched as glob patterns, not binary extensions)\n  '.gitignore',\n  '.gitattributes',\n  '.gitkeep',\n  '.env',\n  '**/.env',\n  '**/.env.*',\n  '*.log',\n  '*.sum',\n  '**/*.sum',\n  '**/SKILL.md',\n] as const;\n\n/**\n * Default binary file extensions to exclude from analysis.\n * These files cannot be meaningfully analyzed as text.\n */\nexport const DEFAULT_BINARY_EXTENSIONS = [\n  // Images\n  '.png',\n  '.jpg',\n  '.jpeg',\n  '.gif',\n  '.bmp',\n  '.ico',\n  '.webp',\n  // Archives\n  '.zip',\n  '.tar',\n  '.gz',\n  '.rar',\n  '.7z',\n  // Executables\n  '.exe',\n  '.dll',\n  '.so',\n  '.dylib',\n  // Media\n  '.mp3',\n  '.mp4',\n  '.wav',\n  // Documents\n  '.pdf',\n  // Fonts\n  '.woff',\n  '.woff2',\n  '.ttf',\n  '.eot',\n  // Compiled\n  '.class',\n  '.pyc',\n] as const;\n\n/**\n * Default maximum file size in bytes (1MB).\n * Files larger than this will be skipped with a warning.\n */\nexport const DEFAULT_MAX_FILE_SIZE = 1024 * 1024;\n\n/**\n * Default configuration object matching the schema structure.\n * This is used when no config file is present or for missing fields.\n */\nexport const DEFAULT_CONFIG = {\n  exclude: {\n    patterns: [...DEFAULT_EXCLUDE_PATTERNS],\n    vendorDirs: [...DEFAULT_VENDOR_DIRS],\n    binaryExtensions: [...DEFAULT_BINARY_EXTENSIONS],\n  },\n  options: {\n    followSymlinks: false,\n    maxFileSize: DEFAULT_MAX_FILE_SIZE,\n  },\n  output: {\n    colors: true,\n  },\n} as const;\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**defaults.ts computes adaptive concurrency limits based on CPU cores and system memory, exports default exclusion lists for vendor directories, file patterns, and binary extensions, and provides the complete default configuration object structure.**\n\n## Exported Functions\n\n**getDefaultConcurrency(): number** computes default worker pool size using formula `clamp(cores * 5, MIN, min(memCap, MAX))` where cores comes from `os.availableParallelism()` or `os.cpus().length` fallback, memCap equals `Math.floor((totalMemGB * 0.5) / 0.512)` capping memory usage at 50% of total RAM divided by 512MB subprocess heap budget, MIN_CONCURRENCY is 2, MAX_CONCURRENCY is 20, and CONCURRENCY_MULTIPLIER is 5.\n\n## Exported Constants\n\n**DEFAULT_VENDOR_DIRS** is readonly array of 18 directory names: `['node_modules', 'vendor', '.git', 'dist', 'build', '__pycache__', '.next', 'venv', '.venv', 'target', '.cargo', '.gradle', '.agents-reverse-engineer', '.agents', '.planning', '.claude', '.opencode', '.gemini']`.\n\n**DEFAULT_EXCLUDE_PATTERNS** is readonly array of 26 gitignore-style glob patterns excluding AI assistant docs (`AGENTS.md`, `CLAUDE.md`, `OPENCODE.md`, `GEMINI.md`, `**/AGENTS.md`, `**/CLAUDE.md`, `**/OPENCODE.md`, `**/GEMINI.md`), lock files (`*.lock`, `package-lock.json`, `yarn.lock`, `pnpm-lock.yaml`, `bun.lock`, `bun.lockb`, `Gemfile.lock`, `Cargo.lock`, `poetry.lock`, `composer.lock`, `go.sum`), and dotfiles/artifacts (`.gitignore`, `.gitattributes`, `.gitkeep`, `.env`, `**/.env`, `**/.env.*`, `*.log`, `*.sum`, `**/*.sum`, `**/SKILL.md`).\n\n**DEFAULT_BINARY_EXTENSIONS** is readonly array of 26 extensions: images (`.png`, `.jpg`, `.jpeg`, `.gif`, `.bmp`, `.ico`, `.webp`), archives (`.zip`, `.tar`, `.gz`, `.rar`, `.7z`), executables (`.exe`, `.dll`, `.so`, `.dylib`), media (`.mp3`, `.mp4`, `.wav`), documents (`.pdf`), fonts (`.woff`, `.woff2`, `.ttf`, `.eot`), compiled (`.class`, `.pyc`).\n\n**DEFAULT_MAX_FILE_SIZE** equals `1024 * 1024` (1MB byte threshold for binary detection).\n\n**DEFAULT_CONFIG** is readonly object with structure `{ exclude: { patterns, vendorDirs, binaryExtensions }, options: { followSymlinks: false, maxFileSize }, output: { colors: true } }` spreading all default arrays and using DEFAULT_MAX_FILE_SIZE value.\n\n## Internal Constants\n\n**CONCURRENCY_MULTIPLIER** is 5 (factor applied to CPU core count).\n\n**MIN_CONCURRENCY** is 2 (floor enforced by getDefaultConcurrency).\n\n**MAX_CONCURRENCY** is 20 (ceiling matching Zod schema `.max(20)` constraint).\n\n**SUBPROCESS_HEAP_GB** is 0.512 (512MB heap budget per subprocess matching `NODE_OPTIONS='--max-old-space-size=512'` in subprocess.ts).\n\n**MEMORY_FRACTION** is 0.5 (allocates 50% of total system memory to subprocess pool).\n\n## Memory-Aware Concurrency Logic\n\ngetDefaultConcurrency checks if `totalMemGB > 1` before computing memCap to avoid division-by-zero or invalid floor operations on systems with less than 1GB RAM (returns Infinity bypassing memory constraint). Returns `Math.max(MIN_CONCURRENCY, Math.min(computed, memCap, MAX_CONCURRENCY))` ensuring result falls within [2, 20] interval while respecting memory capacity ceiling.\n\n## Integration Points\n\nReferenced by `src/config/schema.ts` (merges with user config via `config.ai?.concurrency ?? getDefaultConcurrency()`), `src/config/loader.ts` (spreads DEFAULT_CONFIG as base object before YAML overlay), and indirectly by `src/orchestration/pool.ts` (consumes final concurrency value for worker pool sizing).",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 1078,
      "cacheReadTokens": 17114,
      "cacheCreationTokens": 10488,
      "latencyMs": 32458,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/config/defaults.ts",
          "sizeBytes": 3998
        }
      ]
    },
    {
      "timestamp": "2026-02-09T16:08:58.812Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/ai/service.ts\n\n```typescript\n/**\n * AI service orchestrator.\n *\n * The {@link AIService} class is the main entry point for making AI calls.\n * It ties together the subprocess wrapper, retry logic, backend selection,\n * and telemetry logging into a clean `call()` method.\n *\n * @module\n */\n\nimport { writeFile, mkdir } from 'node:fs/promises';\nimport path from 'node:path';\nimport type { AIBackend, AICallOptions, AIResponse, SubprocessResult, TelemetryEntry, RunLog, FileRead } from './types.js';\nimport { AIServiceError } from './types.js';\nimport { runSubprocess } from './subprocess.js';\nimport { withRetry, DEFAULT_RETRY_OPTIONS } from './retry.js';\nimport { TelemetryLogger } from './telemetry/logger.js';\nimport { writeRunLog } from './telemetry/run-log.js';\nimport { cleanupOldLogs } from './telemetry/cleanup.js';\nimport type { ITraceWriter } from '../orchestration/trace.js';\n\n// ---------------------------------------------------------------------------\n// Rate-limit detection patterns\n// ---------------------------------------------------------------------------\n\n/** Patterns in stderr that indicate a transient rate-limit error */\nconst RATE_LIMIT_PATTERNS = [\n  'rate limit',\n  '429',\n  'too many requests',\n  'overloaded',\n];\n\n/**\n * Check whether stderr text contains rate-limit indicators.\n *\n * @param stderr - Standard error output from the subprocess\n * @returns `true` if any rate-limit pattern matches (case-insensitive)\n */\nfunction isRateLimitStderr(stderr: string): boolean {\n  const lower = stderr.toLowerCase();\n  return RATE_LIMIT_PATTERNS.some((pattern) => lower.includes(pattern));\n}\n\n/**\n * Format bytes as a human-readable string.\n */\nfunction formatBytes(bytes: number): string {\n  if (bytes < 1024) return `${bytes}B`;\n  if (bytes < 1024 * 1024) return `${(bytes / 1024).toFixed(1)}KB`;\n  return `${(bytes / (1024 * 1024)).toFixed(1)}MB`;\n}\n\n// ---------------------------------------------------------------------------\n// AIService options\n// ---------------------------------------------------------------------------\n\n/**\n * Configuration options for the {@link AIService}.\n *\n * These are typically sourced from the config schema's `ai` section.\n */\nexport interface AIServiceOptions {\n  /** Default subprocess timeout in milliseconds */\n  timeoutMs: number;\n  /** Maximum number of retries for transient errors */\n  maxRetries: number;\n  /** Default model identifier (e.g., \"sonnet\", \"opus\") applied to all calls unless overridden per-call */\n  model?: string;\n  /** Telemetry settings */\n  telemetry: {\n    /** Number of most recent run logs to keep on disk */\n    keepRuns: number;\n  };\n}\n\n// ---------------------------------------------------------------------------\n// AIService\n// ---------------------------------------------------------------------------\n\n/**\n * Orchestrates AI CLI calls with retry, timeout, and telemetry.\n *\n * Create one instance per CLI run. Call {@link call} for each AI invocation.\n * Call {@link finalize} at the end to write the run log and clean up old files.\n *\n * @example\n * ```typescript\n * import { AIService } from './service.js';\n * import { resolveBackend, createBackendRegistry } from './registry.js';\n *\n * const registry = createBackendRegistry();\n * const backend = await resolveBackend(registry, 'auto');\n * const service = new AIService(backend, {\n *   timeoutMs: 120_000,\n *   maxRetries: 3,\n *   telemetry: { keepRuns: 10 },\n * });\n *\n * const response = await service.call({ prompt: 'Summarize this codebase' });\n * console.log(response.text);\n *\n * const { logPath, summary } = await service.finalize('/path/to/project');\n * console.log(`Log written to ${logPath}`);\n * ```\n */\nexport class AIService {\n  /** The backend adapter used for CLI invocations */\n  private readonly backend: AIBackend;\n\n  /** Service configuration */\n  private readonly options: AIServiceOptions;\n\n  /** In-memory telemetry logger for this run */\n  private readonly logger: TelemetryLogger;\n\n  /** Running count of calls made (used for entry tracking) */\n  private callCount: number = 0;\n\n  /** Trace writer for concurrency debugging (may be no-op) */\n  private tracer: ITraceWriter | null = null;\n\n  /** Whether debug mode is enabled */\n  private debug: boolean = false;\n\n  /** Number of currently active subprocesses */\n  private activeSubprocesses: number = 0;\n\n  /** Directory for subprocess output logs (null = disabled) */\n  private subprocessLogDir: string | null = null;\n\n  /** Serializes log writes so concurrent workers don't interleave mkdirs */\n  private logWriteQueue: Promise<void> = Promise.resolve();\n\n  /**\n   * Create a new AI service instance.\n   *\n   * @param backend - The resolved backend adapter\n   * @param options - Service configuration (timeout, retries, telemetry)\n   */\n  constructor(backend: AIBackend, options: AIServiceOptions) {\n    this.backend = backend;\n    this.options = options;\n    this.logger = new TelemetryLogger(new Date().toISOString());\n  }\n\n  /**\n   * Set the trace writer for subprocess and retry event tracing.\n   *\n   * @param tracer - The trace writer instance\n   */\n  setTracer(tracer: ITraceWriter): void {\n    this.tracer = tracer;\n  }\n\n  /**\n   * Enable debug mode for verbose subprocess logging to stderr.\n   */\n  setDebug(enabled: boolean): void {\n    this.debug = enabled;\n  }\n\n  /**\n   * Set a directory for writing subprocess stdout/stderr log files.\n   *\n   * When set, each subprocess invocation writes a `.log` file containing\n   * the metadata header, stdout, and stderr. Useful for diagnosing\n   * timed-out or failed subprocesses whose output would otherwise be lost.\n   *\n   * @param dir - Absolute path to the log directory (created on first write)\n   */\n  setSubprocessLogDir(dir: string): void {\n    this.subprocessLogDir = dir;\n  }\n\n  /**\n   * Make an AI call with retry logic and telemetry recording.\n   *\n   * The call flow:\n   * 1. Build CLI args via the backend adapter\n   * 2. Wrap the subprocess invocation in retry logic\n   * 3. On success: parse response via backend, record telemetry entry\n   * 4. On failure: record error telemetry entry, throw the error\n   *\n   * Retries are attempted for `RATE_LIMIT` and `TIMEOUT` errors only.\n   * All other errors are treated as permanent failures.\n   *\n   * @param options - The call options (prompt, model, timeout, etc.)\n   * @returns The normalized AI response\n   * @throws {AIServiceError} On timeout, rate limit exhaustion, parse error, or subprocess failure\n   */\n  async call(options: AICallOptions): Promise<AIResponse> {\n    this.callCount++;\n    const callStart = Date.now();\n    const timestamp = new Date().toISOString();\n    const taskLabel = options.taskLabel ?? 'unknown';\n\n    // Merge service-level model as default (per-call options.model wins)\n    const effectiveOptions: AICallOptions = {\n      ...options,\n      model: options.model ?? this.options.model,\n    };\n\n    const args = this.backend.buildArgs(effectiveOptions);\n    const timeoutMs = options.timeoutMs ?? this.options.timeoutMs;\n\n    let retryCount = 0;\n\n    try {\n      const response = await withRetry(\n        async () => {\n          if (this.debug) {\n            const mem = process.memoryUsage();\n            console.error(\n              `[debug] Spawning subprocess for \"${taskLabel}\" ` +\n              `(active: ${this.activeSubprocesses}, ` +\n              `heapUsed: ${formatBytes(mem.heapUsed)}, ` +\n              `rss: ${formatBytes(mem.rss)}, ` +\n              `timeout: ${(timeoutMs / 1000).toFixed(0)}s)`,\n            );\n          }\n\n          this.activeSubprocesses++;\n\n          const result = await runSubprocess(this.backend.cliCommand, args, {\n            timeoutMs,\n            input: options.prompt,\n            onSpawn: (pid) => {\n              // Emit subprocess:spawn at actual spawn time (not after completion)\n              this.tracer?.emit({\n                type: 'subprocess:spawn',\n                childPid: pid ?? -1,\n                command: this.backend.cliCommand,\n                taskLabel,\n              });\n            },\n          });\n\n          this.activeSubprocesses--;\n\n          // Emit subprocess:exit after completion\n          if (this.tracer && result.childPid !== undefined) {\n            this.tracer.emit({\n              type: 'subprocess:exit',\n              childPid: result.childPid,\n              command: this.backend.cliCommand,\n              taskLabel,\n              exitCode: result.exitCode,\n              signal: result.signal,\n              durationMs: result.durationMs,\n              timedOut: result.timedOut,\n            });\n          }\n\n          // Write subprocess output log (fire-and-forget, non-critical)\n          this.enqueueSubprocessLog(result, taskLabel);\n\n          if (result.timedOut) {\n            console.error(\n              `[warn] Subprocess timed out after ${(result.durationMs / 1000).toFixed(1)}s ` +\n              `for \"${taskLabel}\" (PID ${result.childPid ?? 'unknown'}, ` +\n              `timeout was ${(timeoutMs / 1000).toFixed(0)}s)`,\n            );\n            throw new AIServiceError('TIMEOUT', 'Subprocess timed out');\n          }\n\n          if (this.debug) {\n            console.error(\n              `[debug] Subprocess exited for \"${taskLabel}\" ` +\n              `(PID ${result.childPid ?? 'unknown'}, ` +\n              `exitCode: ${result.exitCode}, ` +\n              `duration: ${(result.durationMs / 1000).toFixed(1)}s, ` +\n              `active: ${this.activeSubprocesses})`,\n            );\n          }\n\n          if (result.exitCode !== 0) {\n            if (isRateLimitStderr(result.stderr)) {\n              throw new AIServiceError(\n                'RATE_LIMIT',\n                `Rate limited by ${this.backend.name}: ${result.stderr.slice(0, 200)}`,\n              );\n            }\n            throw new AIServiceError(\n              'SUBPROCESS_ERROR',\n              `${this.backend.name} CLI exited with code ${result.exitCode}: ${result.stderr.slice(0, 500)}`,\n            );\n          }\n\n          // Parse the response -- wrap in try/catch for parse errors\n          try {\n            return this.backend.parseResponse(result.stdout, result.durationMs, result.exitCode);\n          } catch (error) {\n            if (error instanceof AIServiceError) {\n              throw error;\n            }\n            const message = error instanceof Error ? error.message : String(error);\n            throw new AIServiceError('PARSE_ERROR', `Failed to parse response: ${message}`);\n          }\n        },\n        {\n          ...DEFAULT_RETRY_OPTIONS,\n          maxRetries: this.options.maxRetries,\n          isRetryable: (error: unknown): boolean => {\n            // Only retry rate limits. Timeouts are NOT retried because\n            // spawning another heavyweight subprocess on a system that's\n            // already struggling (or against an unresponsive API) makes\n            // things worse and can exhaust system resources.\n            return (\n              error instanceof AIServiceError &&\n              error.code === 'RATE_LIMIT'\n            );\n          },\n          onRetry: (attempt: number, error: unknown) => {\n            retryCount++;\n\n            const errorCode = error instanceof AIServiceError ? error.code : 'UNKNOWN';\n\n            // Always warn on retry (not just debug) -- retries are noteworthy\n            console.error(\n              `[warn] Retrying \"${taskLabel}\" (attempt ${attempt}/${this.options.maxRetries}, reason: ${errorCode})`,\n            );\n\n            // Emit retry trace event\n            if (this.tracer) {\n              this.tracer.emit({\n                type: 'retry',\n                attempt,\n                taskLabel,\n                errorCode,\n              });\n            }\n          },\n        },\n      );\n\n      // Record successful call\n      this.logger.addEntry({\n        timestamp,\n        prompt: options.prompt,\n        systemPrompt: options.systemPrompt,\n        response: response.text,\n        model: response.model,\n        inputTokens: response.inputTokens,\n        outputTokens: response.outputTokens,\n        cacheReadTokens: response.cacheReadTokens,\n        cacheCreationTokens: response.cacheCreationTokens,\n        latencyMs: response.durationMs,\n        exitCode: response.exitCode,\n        retryCount,\n        thinking: 'not supported',\n        filesRead: [],\n      });\n\n      return response;\n    } catch (error) {\n      // Record failed call\n      const latencyMs = Date.now() - callStart;\n      const errorMessage = error instanceof Error ? error.message : String(error);\n\n      this.logger.addEntry({\n        timestamp,\n        prompt: options.prompt,\n        systemPrompt: options.systemPrompt,\n        response: '',\n        model: options.model ?? 'unknown',\n        inputTokens: 0,\n        outputTokens: 0,\n        cacheReadTokens: 0,\n        cacheCreationTokens: 0,\n        latencyMs,\n        exitCode: 1,\n        error: errorMessage,\n        retryCount,\n        thinking: 'not supported',\n        filesRead: [],\n      });\n\n      throw error;\n    }\n  }\n\n  /**\n   * Finalize the run: write the run log to disk and clean up old files.\n   *\n   * Call this once at the end of a CLI invocation, after all `call()`\n   * invocations have completed (or failed).\n   *\n   * @param projectRoot - Absolute path to the project root directory\n   * @returns The log file path and the run summary\n   */\n  async finalize(projectRoot: string): Promise<{ logPath: string; summary: RunLog['summary'] }> {\n    const runLog = this.logger.toRunLog();\n    const logPath = await writeRunLog(projectRoot, runLog);\n    await cleanupOldLogs(projectRoot, this.options.telemetry.keepRuns);\n    return { logPath, summary: runLog.summary };\n  }\n\n  /**\n   * Attach file-read metadata to the most recent telemetry entry.\n   *\n   * Called by the command runner after an AI call completes, to record\n   * which source files were sent as context for that call.\n   *\n   * @param filesRead - Array of file-read records (path + size)\n   */\n  addFilesReadToLastEntry(filesRead: FileRead[]): void {\n    this.logger.setFilesReadOnLastEntry(filesRead);\n  }\n\n  /**\n   * Get the current run summary without finalizing.\n   *\n   * Useful for displaying progress during a run.\n   *\n   * @returns Current summary statistics\n   */\n  getSummary(): RunLog['summary'] {\n    return this.logger.getSummary();\n  }\n\n  /**\n   * Enqueue a subprocess output log write.\n   *\n   * Serializes writes via a promise chain to avoid concurrent mkdir races.\n   * Failures are silently swallowed -- log writing is non-critical.\n   */\n  private enqueueSubprocessLog(result: SubprocessResult, taskLabel: string): void {\n    if (this.subprocessLogDir === null) return;\n\n    const dir = this.subprocessLogDir;\n    const sanitized = taskLabel.replace(/\\//g, '--').replace(/[^a-zA-Z0-9._-]/g, '_');\n    const filename = `${sanitized}_pid${result.childPid ?? 0}.log`;\n    const filePath = path.join(dir, filename);\n\n    const content =\n      `task:      ${taskLabel}\\n` +\n      `pid:       ${result.childPid ?? 'unknown'}\\n` +\n      `command:   ${this.backend.cliCommand}\\n` +\n      `exit:      ${result.exitCode}\\n` +\n      `signal:    ${result.signal ?? 'none'}\\n` +\n      `duration:  ${result.durationMs}ms\\n` +\n      `timed_out: ${result.timedOut}\\n` +\n      `\\n--- stdout ---\\n` +\n      result.stdout +\n      `\\n--- stderr ---\\n` +\n      result.stderr;\n\n    this.logWriteQueue = this.logWriteQueue\n      .then(async () => {\n        await mkdir(dir, { recursive: true });\n        await writeFile(filePath, content, 'utf-8');\n      })\n      .catch(() => { /* non-critical -- log loss is acceptable */ });\n  }\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**AIService orchestrates AI CLI subprocess invocations with retry logic, timeout enforcement, telemetry recording, and trace emission for concurrent file analysis workflows.**\n\n## Exported API\n\n**AIService** class coordinates AI backend interactions through:\n- `constructor(backend: AIBackend, options: AIServiceOptions)` — Initializes service with resolved backend adapter and config (timeoutMs, maxRetries, telemetry.keepRuns)\n- `call(options: AICallOptions): Promise<AIResponse>` — Executes AI invocation with retry wrapper, records TelemetryEntry, returns normalized AIResponse\n- `finalize(projectRoot: string): Promise<{ logPath: string; summary: RunLog['summary'] }>` — Writes RunLog to `.agents-reverse-engineer/logs/run-<timestamp>.json`, prunes old logs via cleanupOldLogs()\n- `setTracer(tracer: ITraceWriter): void` — Attaches trace writer for subprocess:spawn/exit/retry events\n- `setDebug(enabled: boolean): void` — Enables verbose subprocess logging with heap/RSS metrics to stderr\n- `setSubprocessLogDir(dir: string): void` — Configures directory for writing per-subprocess `.log` files (stdout+stderr+metadata)\n- `addFilesReadToLastEntry(filesRead: FileRead[]): void` — Attaches file context metadata to most recent telemetry entry\n- `getSummary(): RunLog['summary']` — Returns current run statistics (totalInputTokens, totalCacheReadTokens, errorCount, etc.) without finalizing\n\n**AIServiceOptions** interface specifies:\n- `timeoutMs: number` — Default subprocess timeout (from config.ai.timeoutMs, typically 120000)\n- `maxRetries: number` — Retry attempt limit for transient errors (from config.ai.maxRetries, typically 3)\n- `model?: string` — Default model identifier applied to all calls unless overridden per AICallOptions.model\n- `telemetry.keepRuns: number` — Retention count for historical run logs (from config.ai.telemetry.keepRuns, typically 50)\n\n**AIServiceError** type (re-exported from `./types.js`) with discriminated codes: `'TIMEOUT' | 'RATE_LIMIT' | 'SUBPROCESS_ERROR' | 'PARSE_ERROR'`\n\n## Retry Strategy\n\n**withRetry()** invoked from `./retry.js` with custom configuration:\n- `maxRetries: options.maxRetries` (typically 3)\n- `isRetryable: (error) => error.code === 'RATE_LIMIT'` — ONLY retries rate limits; timeouts are permanent failures to avoid resource exhaustion\n- `onRetry: (attempt, error) => ...` — Emits `retry` trace event and stderr warning with `[warn] Retrying \"${taskLabel}\" (attempt ${attempt}/${maxRetries}, reason: ${errorCode})`\n\n**isRateLimitStderr(stderr: string): boolean** detects transient errors via pattern matching:\n- Patterns: `['rate limit', '429', 'too many requests', 'overloaded']` (case-insensitive substring search)\n- Maps subprocess stderr to `AIServiceError('RATE_LIMIT', ...)` for retry eligibility\n\n## Subprocess Lifecycle\n\n**call()** execution flow:\n1. Merges `options.model ?? this.options.model` into effectiveOptions\n2. Calls `backend.buildArgs(effectiveOptions)` to construct CLI argv\n3. Increments `activeSubprocesses` counter before spawn\n4. Invokes `runSubprocess(backend.cliCommand, args, { timeoutMs, input: prompt, onSpawn: (pid) => tracer.emit('subprocess:spawn', ...) })`\n5. Decrements `activeSubprocesses` after completion\n6. Emits `subprocess:exit` trace event with childPid, exitCode, signal, durationMs, timedOut\n7. Enqueues subprocess log write via `enqueueSubprocessLog()` (fire-and-forget, non-critical)\n8. Checks `result.timedOut` → throws `AIServiceError('TIMEOUT', ...)` with stderr warning showing PID and duration\n9. Checks `exitCode !== 0` → applies `isRateLimitStderr()` detection → throws RATE_LIMIT or SUBPROCESS_ERROR\n10. Calls `backend.parseResponse(stdout, durationMs, exitCode)` with try/catch wrapping parse errors as `AIServiceError('PARSE_ERROR', ...)`\n11. Records TelemetryEntry via `logger.addEntry()` with model, tokens, latencyMs, retryCount, filesRead (empty array initially)\n\n## Telemetry Recording\n\n**TelemetryLogger** (from `./telemetry/logger.js`) accumulates entries in-memory:\n- Each `call()` invocation adds entry with timestamp, prompt, systemPrompt, response, model, inputTokens, outputTokens, cacheReadTokens, cacheCreationTokens, latencyMs, exitCode, retryCount, thinking='not supported', filesRead=[]\n- Failed calls record empty response, 0 tokens, error message string, exitCode=1\n- `addFilesReadToLastEntry(filesRead)` mutates most recent entry to attach FileRead[] metadata (path + sizeBytes)\n- `toRunLog()` converts entries to RunLog with aggregated summary: totalInputTokens, totalOutputTokens, totalCacheReadTokens, totalCacheCreationTokens, totalLatencyMs, callCount, errorCount, uniqueFilesRead\n\n**writeRunLog(projectRoot, runLog)** (from `./telemetry/run-log.js`) serializes to `.agents-reverse-engineer/logs/run-<timestamp>.json`\n\n**cleanupOldLogs(projectRoot, keepRuns)** (from `./telemetry/cleanup.js`) enforces retention policy by deleting oldest logs beyond keepRuns limit\n\n## Debug Logging\n\n**setDebug(true)** enables stderr output:\n- Before spawn: `[debug] Spawning subprocess for \"${taskLabel}\" (active: ${activeSubprocesses}, heapUsed: ${formatBytes(mem.heapUsed)}, rss: ${formatBytes(mem.rss)}, timeout: ${timeoutMs/1000}s)`\n- After exit: `[debug] Subprocess exited for \"${taskLabel}\" (PID ${childPid}, exitCode: ${exitCode}, duration: ${durationMs/1000}s, active: ${activeSubprocesses})`\n- Uses `formatBytes(bytes): string` helper producing `\"123B\"` / `\"4.5KB\"` / `\"67.8MB\"` strings\n\n**setSubprocessLogDir(dir)** writes `.log` files asynchronously:\n- Filename pattern: `${taskLabel.replace(/\\//g, '--').replace(/[^a-zA-Z0-9._-]/g, '_')}_pid${childPid}.log`\n- Content format: metadata header (`task:`, `pid:`, `command:`, `exit:`, `signal:`, `duration:`, `timed_out:`) followed by `--- stdout ---` and `--- stderr ---` sections\n- Serialized via promise chain (`logWriteQueue`) to prevent concurrent mkdir races from worker pool\n- Failures silently swallowed (non-critical operation)\n\n## Trace Event Emission\n\n**setTracer(tracer)** enables ITraceWriter event emission:\n- `subprocess:spawn` — Emitted synchronously in `onSpawn` callback with childPid, command, taskLabel\n- `subprocess:exit` — Emitted after subprocess completion with childPid, exitCode, signal, durationMs, timedOut\n- `retry` — Emitted in retry handler with attempt number, taskLabel, errorCode\n\n## Backend Integration\n\n**AIBackend** interface (from `./types.js`) requires:\n- `name: string` — Backend identifier (e.g., \"Claude\", \"Gemini\", \"OpenCode\")\n- `cliCommand: string` — Executable name (e.g., \"claude\", \"gemini\", \"opencode\")\n- `buildArgs(options: AICallOptions): string[]` — Constructs argv from options (model, prompt, systemPrompt)\n- `parseResponse(stdout: string, durationMs: number, exitCode: number): AIResponse` — Extracts text, model, tokens, cacheReadTokens, cacheCreationTokens\n\n**Backend adapter examples:**\n- `./backends/claude.ts` — Parses JSON with `usage.input_tokens`, `usage.output_tokens`, `usage.cache_read_input_tokens`, `usage.cache_creation_input_tokens`\n- `./backends/gemini.ts` — Stub implementation throwing SUBPROCESS_ERROR\n- `./backends/opencode.ts` — Stub implementation throwing SUBPROCESS_ERROR\n\n## Dependencies\n\n- `runSubprocess()` from `./subprocess.js` — Spawns child processes with timeout, process group killing, SIGTERM/SIGKILL escalation\n- `withRetry()` from `./retry.js` — Exponential backoff wrapper with isRetryable predicate and onRetry callback\n- `TelemetryLogger` from `./telemetry/logger.js` — In-memory entry accumulation with summary aggregation\n- `writeRunLog()` from `./telemetry/run-log.js` — Persists RunLog JSON to disk\n- `cleanupOldLogs()` from `./telemetry/cleanup.js` — Prunes old logs via readdir + stat + unlink\n- `ITraceWriter` from `../orchestration/trace.js` — Trace event emission interface (NullTraceWriter or TraceWriter with NDJSON output)\n\n## Design Patterns\n\n**Template Method pattern** — `call()` defines subprocess invocation skeleton, delegates backend-specific logic to `backend.buildArgs()` and `backend.parseResponse()`\n\n**Strategy pattern** — Backend adapters (Claude, Gemini, OpenCode) implement AIBackend interface for pluggable CLI invocation\n\n**Promise chaining for serialization** — `logWriteQueue` ensures sequential execution of async mkdir+writeFile to avoid race conditions from concurrent workers\n\n**Resource tracking** — `activeSubprocesses` counter enables debug logging of subprocess concurrency for diagnosing resource exhaustion (see CLAUDE.md subprocess management section)",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 2474,
      "cacheReadTokens": 17114,
      "cacheCreationTokens": 13456,
      "latencyMs": 57768,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/ai/service.ts",
          "sizeBytes": 15566
        }
      ]
    },
    {
      "timestamp": "2026-02-09T16:09:43.533Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/discovery/walker.ts\n\n```typescript\n/**\n * Directory walker for the agents-reverse file discovery system.\n * \n * Uses fast-glob to traverse directories and return all candidate files.\n * Filters are applied separately via the filter chain (not in this module).\n */\n\nimport fg from 'fast-glob';\nimport type { WalkerOptions } from './types.js';\n\n/**\n * Walk a directory tree and return all files.\n * \n * This walker returns ALL files in the directory tree. Filtering happens\n * separately via the filter chain (gitignore, binary, vendor, custom patterns).\n * \n * @param options - Walker configuration\n * @returns Array of absolute file paths\n * \n * @example\n * ```typescript\n * const files = await walkDirectory({ cwd: '/path/to/repo' });\n * console.log(`Found ${files.length} files`);\n * ```\n */\nexport async function walkDirectory(options: WalkerOptions): Promise<string[]> {\n  return fg.glob('**/*', {\n    cwd: options.cwd,\n    absolute: true,\n    onlyFiles: true,\n    dot: options.dot ?? true,\n    followSymbolicLinks: options.followSymlinks ?? false,\n    suppressErrors: true, // Don't throw on permission errors (per RESEARCH.md)\n    // Always exclude .git internals for performance\n    ignore: ['**/.git/**'],\n  });\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**walker.ts implements directory traversal via fast-glob for ARE's file discovery pipeline.**\n\n## Exported Function\n\n`walkDirectory(options: WalkerOptions): Promise<string[]>` returns absolute paths to all files in directory tree. Uses `fg.glob('**/*', ...)` with cwd-relative traversal. Returns unfiltered file list—filtering applied separately by filter chain in `src/discovery/filters/`.\n\n## Configuration Options\n\nConsumes `WalkerOptions` interface with:\n- `cwd: string` — Base directory for glob traversal\n- `dot?: boolean` — Include dotfiles (default `true`)\n- `followSymlinks?: boolean` — Follow symbolic links (default `false`)\n\n## fast-glob Parameters\n\nPasses these options to `fg.glob()`:\n- `absolute: true` — Return absolute paths (not relative)\n- `onlyFiles: true` — Exclude directories from results\n- `suppressErrors: true` — Ignore permission errors per RESEARCH.md requirement\n- `ignore: ['**/.git/**']` — Hard-coded exclusion for `.git` internals (performance optimization)\n\n## Integration Points\n\nCalled by `src/discovery/run.ts` which applies post-traversal filters: gitignore patterns via `src/discovery/filters/gitignore.ts`, binary detection via `src/discovery/filters/binary.ts`, vendor directories via `src/discovery/filters/vendor.ts`, custom glob patterns via `src/discovery/filters/custom.ts`. Walker does not inspect file contents or apply business logic—purely delegates to fast-glob's optimized filesystem traversal.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 367,
      "cacheReadTokens": 17114,
      "cacheCreationTokens": 9438,
      "latencyMs": 17653,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/discovery/walker.ts",
          "sizeBytes": 1187
        }
      ]
    },
    {
      "timestamp": "2026-02-09T16:09:16.872Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/cli/generate.ts\n\n```typescript\n/**\n * CLI generate command\n *\n * Creates and executes a documentation generation plan by:\n * 1. Discovering files to analyze\n * 2. Detecting file types and creating analysis tasks\n * 3. Resolving an AI CLI backend\n * 4. Running concurrent AI analysis via CommandRunner\n * 5. Producing .sum files, AGENTS.md, and root documents\n *\n * With --dry-run, shows the plan without making any AI calls.\n */\n\nimport * as path from 'node:path';\nimport pc from 'picocolors';\nimport { loadConfig } from '../config/loader.js';\nimport { createLogger } from '../output/logger.js';\nimport { discoverFiles } from '../discovery/run.js';\nimport { createOrchestrator, type GenerationPlan } from '../generation/orchestrator.js';\nimport { buildExecutionPlan } from '../generation/executor.js';\nimport {\n  AIService,\n  AIServiceError,\n  createBackendRegistry,\n  resolveBackend,\n  getInstallInstructions,\n} from '../ai/index.js';\nimport { CommandRunner, ProgressLog, createTraceWriter, cleanupOldTraces } from '../orchestration/index.js';\n\n/**\n * Options for the generate command.\n */\nexport interface GenerateOptions {\n  /** Dry run - show plan without generating */\n  dryRun?: boolean;\n  /** Number of concurrent AI calls */\n  concurrency?: number;\n  /** Stop on first file analysis failure */\n  failFast?: boolean;\n  /** Show AI prompts and backend details */\n  debug?: boolean;\n  /** Enable concurrency tracing to .agents-reverse-engineer/traces/ */\n  trace?: boolean;\n}\n\n/**\n * Format the generation plan for display.\n */\nfunction formatPlan(plan: GenerationPlan): string {\n  const lines: string[] = [];\n\n  lines.push(`\\n=== Generation Plan ===\\n`);\n\n  // File summary\n  lines.push(`Files to analyze: ${plan.files.length}`);\n  lines.push(`Tasks to execute: ${plan.tasks.length}`);\n  lines.push('');\n\n  // Complexity\n  lines.push('Complexity:');\n  lines.push(`  Files: ${plan.complexity.fileCount}`);\n  lines.push(`  Directory depth: ${plan.complexity.directoryDepth}`);\n  lines.push('');\n\n  return lines.join('\\n');\n}\n\n/**\n * Generate command - discovers files, plans analysis, and executes AI-driven\n * documentation generation.\n *\n * Default behavior: resolves an AI CLI backend, builds an execution plan,\n * and runs concurrent AI analysis via the CommandRunner. Produces .sum files,\n * AGENTS.md per directory, and root documents (CLAUDE.md, ARCHITECTURE.md, etc.).\n *\n * @param targetPath - Directory to generate documentation for\n * @param options - Command options (concurrency, failFast, debug, etc.)\n */\nexport async function generateCommand(\n  targetPath: string,\n  options: GenerateOptions\n): Promise<void> {\n  const absolutePath = path.resolve(targetPath);\n  const logger = createLogger({ colors: true });\n\n  logger.info(`Generating documentation plan for: ${absolutePath}`);\n\n  // Create trace writer (moved earlier to use in config loading and discovery)\n  const tracer = createTraceWriter(absolutePath, options.trace ?? false);\n  if (options.trace && tracer.filePath) {\n    console.error(pc.dim(`[trace] Writing to ${tracer.filePath}`));\n  }\n\n  // Load configuration\n  const config = await loadConfig(absolutePath, {\n    tracer,\n    debug: options.debug,\n  });\n\n  // Discover files\n  logger.info('Discovering files...');\n\n  const filterResult = await discoverFiles(absolutePath, config, {\n    tracer,\n    debug: options.debug,\n  });\n\n  // Create discovery result for orchestrator\n  const discoveryResult = {\n    files: filterResult.included,\n    excluded: filterResult.excluded.map(e => ({ path: e.path, reason: e.reason })),\n  };\n\n  logger.info(`Found ${discoveryResult.files.length} files to analyze`);\n\n  // Create generation plan\n  logger.info('Creating generation plan...');\n  const orchestrator = createOrchestrator(\n    config,\n    absolutePath,\n    { tracer, debug: options.debug }\n  );\n  const plan = await orchestrator.createPlan(discoveryResult);\n\n  // Display plan\n  console.log(formatPlan(plan));\n\n  // ---------------------------------------------------------------------------\n  // Dry-run: show execution plan summary without making AI calls\n  // ---------------------------------------------------------------------------\n\n  if (options.dryRun) {\n    const executionPlan = buildExecutionPlan(plan, absolutePath);\n    const dirCount = Object.keys(executionPlan.directoryFileMap).length;\n\n    console.log(pc.bold('\\n--- Dry Run Summary ---\\n'));\n    console.log(`  Files to analyze:     ${pc.cyan(String(executionPlan.fileTasks.length))}`);\n    console.log(`  Directories:          ${pc.cyan(String(dirCount))}`);\n    console.log(`  Root documents:       ${pc.cyan(String(executionPlan.rootTasks.length))}`);\n    console.log(`  Estimated AI calls:   ${pc.cyan(String(executionPlan.tasks.length))}`);\n    console.log('');\n    console.log(pc.dim('Files:'));\n    for (const task of executionPlan.fileTasks) {\n      console.log(pc.dim(`  ${task.path}`));\n    }\n    console.log('');\n    console.log(pc.dim('No AI calls made (dry run).'));\n    return;\n  }\n\n  // ---------------------------------------------------------------------------\n  // Resolve backend and run AI analysis\n  // ---------------------------------------------------------------------------\n\n  // Resolve AI CLI backend\n  const registry = createBackendRegistry();\n  let backend;\n  try {\n    backend = await resolveBackend(registry, config.ai.backend);\n  } catch (error) {\n    if (error instanceof AIServiceError && error.code === 'CLI_NOT_FOUND') {\n      console.error(pc.red('Error: No AI CLI found.\\n'));\n      console.error(getInstallInstructions(registry));\n      process.exit(2);\n    }\n    throw error;\n  }\n\n  // Debug: log backend info\n  if (options.debug) {\n    console.log(pc.dim(`[debug] Backend: ${backend.name}`));\n    console.log(pc.dim(`[debug] CLI command: ${backend.cliCommand}`));\n    console.log(pc.dim(`[debug] Model: ${config.ai.model}`));\n  }\n\n  // Create AI service\n  const aiService = new AIService(backend, {\n    timeoutMs: config.ai.timeoutMs,\n    maxRetries: config.ai.maxRetries,\n    model: config.ai.model,\n    telemetry: { keepRuns: config.ai.telemetry.keepRuns },\n  });\n\n  if (options.debug) {\n    aiService.setDebug(true);\n  }\n\n  // Build execution plan\n  const executionPlan = buildExecutionPlan(plan, absolutePath);\n\n  // Determine concurrency\n  const concurrency = options.concurrency ?? config.ai.concurrency;\n\n  // Enable subprocess output logging alongside tracing\n  if (options.trace) {\n    const logDir = path.join(\n      absolutePath, '.agents-reverse-engineer', 'subprocess-logs',\n      new Date().toISOString().replace(/[:.]/g, '-'),\n    );\n    aiService.setSubprocessLogDir(logDir);\n    console.error(pc.dim(`[trace] Subprocess logs → ${logDir}`));\n  }\n\n  // Create progress log for tail -f monitoring\n  const progressLog = ProgressLog.create(absolutePath);\n  progressLog.write(`=== ARE Generate (${new Date().toISOString()}) ===`);\n  progressLog.write(`Project: ${absolutePath}`);\n  progressLog.write(`Files: ${executionPlan.fileTasks.length} | Directories: ${executionPlan.directoryTasks.length}`);\n  progressLog.write('');\n\n  // Create command runner\n  const runner = new CommandRunner(aiService, {\n    concurrency,\n    failFast: options.failFast,\n    debug: options.debug,\n    tracer,\n    progressLog,\n  });\n\n  // Execute the three-phase pipeline\n  const summary = await runner.executeGenerate(executionPlan);\n\n  // Write telemetry run log\n  await aiService.finalize(absolutePath);\n\n  // Finalize trace, progress log, and clean up old trace files\n  await progressLog.finalize();\n  await tracer.finalize();\n  if (options.trace) {\n    await cleanupOldTraces(absolutePath);\n  }\n\n  // Determine exit code from RunSummary\n  //   0: all files succeeded\n  //   1: some files failed (partial failure)\n  //   2: no files succeeded (total failure)\n  if (summary.filesProcessed === 0 && summary.filesFailed > 0) {\n    process.exit(2);\n  } else if (summary.filesFailed > 0) {\n    process.exit(1);\n  }\n  // Exit code 0 -- all files succeeded (or no files to process)\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**generateCommand() orchestrates full three-phase documentation pipeline: discovers files via discoverFiles(), builds GenerationPlan via createOrchestrator().createPlan(), resolves AI backend via resolveBackend(), creates AIService and CommandRunner, executes concurrent file analysis → directory aggregation → root synthesis via runner.executeGenerate(), writes telemetry/traces, exits with status codes (0=success, 1=partial failure, 2=total failure).**\n\n## Exported Interface\n\n```typescript\ninterface GenerateOptions {\n  dryRun?: boolean;        // Show plan without AI calls\n  concurrency?: number;    // Worker pool size override\n  failFast?: boolean;      // Abort on first failure\n  debug?: boolean;         // Log AI prompts/backend details\n  trace?: boolean;         // Emit NDJSON trace events\n}\n\nasync function generateCommand(\n  targetPath: string,\n  options: GenerateOptions\n): Promise<void>\n```\n\n## Execution Flow\n\ngenerateCommand() executes seven sequential phases:\n\n1. **Path resolution**: `path.resolve(targetPath)` converts relative to absolute path\n2. **Trace initialization**: `createTraceWriter(absolutePath, options.trace ?? false)` creates NDJSON tracer before config/discovery\n3. **Config loading**: `loadConfig(absolutePath, { tracer, debug })` reads `.agents-reverse-engineer/config.yaml` with Zod validation\n4. **File discovery**: `discoverFiles(absolutePath, config, { tracer, debug })` applies filter chain (gitignore/binary/vendor/custom) producing `{ included, excluded }`\n5. **Plan creation**: `createOrchestrator(config, absolutePath, { tracer, debug }).createPlan(discoveryResult)` returns GenerationPlan with `files[]`, `tasks[]`, `complexity`\n6. **Backend resolution**: `resolveBackend(createBackendRegistry(), config.ai.backend)` auto-detects Claude/Gemini/OpenCode CLI availability, throws `AIServiceError` with `code: 'CLI_NOT_FOUND'` if none found, logs `getInstallInstructions(registry)` on stderr\n7. **Pipeline execution**: `buildExecutionPlan(plan, absolutePath)` splits tasks into `fileTasks`/`directoryTasks`/`rootTasks`, `new CommandRunner(aiService, { concurrency, failFast, debug, tracer, progressLog }).executeGenerate(executionPlan)` runs three-phase pool concurrency\n\n## Dry-Run Mode\n\nWhen `options.dryRun === true`, formatPlan() displays plan summary and buildExecutionPlan() shows task breakdown without calling AIService:\n\n```\n--- Dry Run Summary ---\nFiles to analyze:     <fileTasks.length>\nDirectories:          <directoryFileMap keys>\nRoot documents:       <rootTasks.length>\nEstimated AI calls:   <total tasks.length>\n```\n\nExits without backend resolution or AIService instantiation.\n\n## AI Service Configuration\n\nAIService instantiated with backend-specific options merged from config and CLI:\n\n```typescript\nconst aiService = new AIService(backend, {\n  timeoutMs: config.ai.timeoutMs,           // SIGTERM timeout (default 120000ms)\n  maxRetries: config.ai.maxRetries,         // Exponential backoff retries (default 3)\n  model: config.ai.model,                   // Override backend default\n  telemetry: { keepRuns: config.ai.telemetry.keepRuns }  // Retention limit (default 50)\n});\n```\n\nIf `options.debug === true`, calls `aiService.setDebug(true)` enabling subprocess stdout/stderr logging.\n\nIf `options.trace === true`, calls `aiService.setSubprocessLogDir(logDir)` where `logDir = path.join(absolutePath, '.agents-reverse-engineer', 'subprocess-logs', <ISO timestamp>)` to capture per-call output files.\n\n## Progress Logging\n\nProgressLog.create() writes human-readable streaming log to `.agents-reverse-engineer/progress.log`:\n\n```typescript\nconst progressLog = ProgressLog.create(absolutePath);\nprogressLog.write(`=== ARE Generate (${new Date().toISOString()}) ===`);\nprogressLog.write(`Project: ${absolutePath}`);\nprogressLog.write(`Files: ${fileTasks.length} | Directories: ${directoryTasks.length}`);\n```\n\nPassed to CommandRunner constructor, receives task pickup/completion events with ETA calculations. Finalized via `progressLog.finalize()` before exit.\n\n## Exit Codes\n\nProcess exit status determined by RunSummary from runner.executeGenerate():\n\n- **0**: `summary.filesFailed === 0` (all files succeeded or no files to process)\n- **1**: `summary.filesFailed > 0 && summary.filesProcessed > 0` (partial failure)\n- **2**: `summary.filesProcessed === 0 && summary.filesFailed > 0` (total failure, no progress)\n- **2** (CLI not found): `AIServiceError.code === 'CLI_NOT_FOUND'` triggers stderr instructions via `getInstallInstructions(registry)`, calls `process.exit(2)` before pipeline execution\n\n## Trace Cleanup\n\nWhen `options.trace === true`, calls `cleanupOldTraces(absolutePath)` after `tracer.finalize()` to enforce retention limit (default 500 trace files) via mtime-based pruning.\n\n## Dependencies\n\n- **loadConfig**: Zod-validated config loading from `.agents-reverse-engineer/config.yaml`\n- **discoverFiles**: Filter chain execution returning `{ included, excluded }` discovery result\n- **createOrchestrator**: Factory producing orchestrator with `createPlan()` method\n- **buildExecutionPlan**: Splits GenerationPlan into phase-specific task groups (`fileTasks`/`directoryTasks`/`rootTasks`)\n- **resolveBackend**: Auto-detects first available AI CLI (Claude/Gemini/OpenCode) via sequential `isAvailable()` checks\n- **AIService**: Subprocess manager with exponential backoff retry, telemetry logging, resource limits (`NODE_OPTIONS`, `UV_THREADPOOL_SIZE`)\n- **CommandRunner**: Worker pool executor with three-phase dispatch (`executeGenerate()`), progress reporting, trace emission\n- **ProgressLog**: Streaming append-only log writer for `tail -f` monitoring\n- **createTraceWriter**: NDJSON trace event emitter with promise-chain serialization\n- **cleanupOldTraces**: Mtime-based retention enforcement for `.agents-reverse-engineer/traces/` directory",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 1584,
      "cacheReadTokens": 17114,
      "cacheCreationTokens": 11433,
      "latencyMs": 56059,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/cli/generate.ts",
          "sizeBytes": 7977
        }
      ]
    },
    {
      "timestamp": "2026-02-09T16:09:42.709Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/discovery/types.ts\n\n```typescript\n/**\n * Discovery types for the agents-reverse file discovery system.\n * \n * This module defines the core interfaces and types used by the directory\n * walker and filter chain for file discovery operations.\n */\n\nimport type { Stats } from 'node:fs';\n\n/**\n * Interface for file filters in the discovery pipeline.\n * \n * Filters are applied to each file discovered by the walker to determine\n * whether it should be included or excluded from analysis. Filters can be\n * synchronous or asynchronous.\n * \n * Examples: GitignoreFilter, BinaryFilter, VendorFilter, CustomPatternFilter\n */\nexport interface FileFilter {\n  /** Name of the filter for logging which filter excluded a file */\n  readonly name: string;\n  \n  /**\n   * Determine whether a file should be excluded from discovery.\n   * \n   * @param path - Absolute path to the file\n   * @param stats - Optional file stats (for size-based filtering, etc.)\n   * @returns true if the file should be excluded, false to include\n   */\n  shouldExclude(path: string, stats?: Stats): Promise<boolean> | boolean;\n}\n\n/**\n * Record of an excluded file with reason and responsible filter.\n */\nexport interface ExcludedFile {\n  /** Absolute path to the excluded file */\n  path: string;\n  /** Human-readable reason for exclusion */\n  reason: string;\n  /** Name of the filter that excluded this file */\n  filter: string;\n}\n\n/**\n * Result of running the discovery and filter chain.\n */\nexport interface FilterResult {\n  /** Files that passed all filters and should be analyzed */\n  included: string[];\n  /** Files that were excluded with reasons */\n  excluded: ExcludedFile[];\n}\n\n/**\n * Options for the directory walker.\n */\nexport interface WalkerOptions {\n  /** Root directory to walk (absolute path) */\n  cwd: string;\n  \n  /** \n   * Whether to follow symbolic links.\n   * Default: false (per CONTEXT.md - skip symlinks by default)\n   */\n  followSymlinks?: boolean;\n  \n  /**\n   * Whether to include dotfiles (files starting with .).\n   * Default: true (include dotfiles for analysis)\n   */\n  dot?: boolean;\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**types.ts defines TypeScript interfaces for the file discovery system's filter chain architecture, including filter contracts, exclusion metadata, and walker configuration.**\n\n## Exported Interfaces\n\n**FileFilter** — Filter contract with two properties:\n- `name: string` (readonly) identifies which filter excluded a file for logging\n- `shouldExclude(path: string, stats?: Stats): Promise<boolean> | boolean` determines file inclusion (returns true to exclude)\n\nTakes absolute file path and optional `Stats` from `node:fs` for size-based or metadata filtering. Supports both sync and async implementations. Known implementations: GitignoreFilter, BinaryFilter, VendorFilter, CustomPatternFilter (per inline documentation).\n\n**ExcludedFile** — Exclusion record with three properties:\n- `path: string` absolute path to excluded file\n- `reason: string` human-readable exclusion explanation\n- `filter: string` name of the filter that excluded the file\n\nUsed to track why files were rejected during discovery pipeline execution.\n\n**FilterResult** — Discovery pipeline output with two properties:\n- `included: string[]` files passing all filters (ready for analysis)\n- `excluded: ExcludedFile[]` rejected files with metadata\n\nAggregates results from filter chain execution across all discovered files.\n\n**WalkerOptions** — Directory traversal configuration with three properties:\n- `cwd: string` root directory for walking (absolute path, required)\n- `followSymlinks?: boolean` whether to traverse symlinks (default false per CONTEXT.md)\n- `dot?: boolean` whether to include dotfiles starting with `.` (default true for analysis)\n\nPassed to directory walker to control filesystem traversal behavior.\n\n## Integration Points\n\nAll interfaces consumed by `src/discovery/walker.ts` (directory walker) and `src/discovery/filters/` implementations (gitignore.ts, binary.ts, vendor.ts, custom.ts). The `FileFilter` interface enables composable filter chains where each filter can independently reject files. The `Stats` parameter in `shouldExclude()` allows filters to inspect file metadata without additional filesystem calls.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 475,
      "cacheReadTokens": 0,
      "cacheCreationTokens": 26762,
      "latencyMs": 19318,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/discovery/types.ts",
          "sizeBytes": 2043
        }
      ]
    },
    {
      "timestamp": "2026-02-09T16:09:20.994Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/cli/specify.ts\n\n```typescript\n/**\n * CLI specify command\n *\n * Generates a project specification from AGENTS.md documentation by:\n * 1. Loading configuration\n * 2. Collecting all AGENTS.md files (auto-generating if none exist)\n * 3. Building a synthesis prompt from the collected docs\n * 4. Resolving an AI CLI backend and calling the AI service\n * 5. Writing the specification to disk (single or multi-file)\n *\n * With --dry-run, shows input statistics without making any AI calls.\n */\n\nimport * as path from 'node:path';\nimport pc from 'picocolors';\nimport { loadConfig } from '../config/loader.js';\nimport { collectAgentsDocs } from '../generation/collector.js';\nimport { buildSpecPrompt, writeSpec, SpecExistsError } from '../specify/index.js';\nimport {\n  AIService,\n  AIServiceError,\n  createBackendRegistry,\n  resolveBackend,\n  getInstallInstructions,\n} from '../ai/index.js';\nimport { ProgressLog } from '../orchestration/index.js';\nimport { generateCommand } from './generate.js';\n\n/**\n * Options for the specify command.\n */\nexport interface SpecifyOptions {\n  /** Custom output path (default: specs/SPEC.md) */\n  output?: string;\n  /** Overwrite existing specs */\n  force?: boolean;\n  /** Show plan without calling AI */\n  dryRun?: boolean;\n  /** Split output into multiple files */\n  multiFile?: boolean;\n  /** Show verbose debug info */\n  debug?: boolean;\n  /** Enable tracing */\n  trace?: boolean;\n}\n\n/**\n * Specify command - collects AGENTS.md documentation, synthesizes it via AI,\n * and writes a comprehensive project specification.\n *\n * @param targetPath - Directory to generate specification for\n * @param options - Command options (output, force, dryRun, multiFile, debug, trace)\n */\nexport async function specifyCommand(\n  targetPath: string,\n  options: SpecifyOptions,\n): Promise<void> {\n  const absolutePath = path.resolve(targetPath);\n  const outputPath = options.output\n    ? path.resolve(options.output)\n    : path.join(absolutePath, 'specs', 'SPEC.md');\n\n  // Load configuration\n  const config = await loadConfig(absolutePath, { debug: options.debug });\n\n  // Collect AGENTS.md files\n  let docs = await collectAgentsDocs(absolutePath);\n\n  // ---------------------------------------------------------------------------\n  // Dry-run mode: show summary without calling AI or generating\n  // ---------------------------------------------------------------------------\n\n  if (options.dryRun) {\n    const totalChars = docs.reduce((sum, d) => sum + d.content.length, 0);\n    const estimatedTokensK = Math.ceil(totalChars / 4) / 1000;\n\n    console.log(pc.bold('\\n--- Dry Run Summary ---\\n'));\n    console.log(`  AGENTS.md files:   ${pc.cyan(String(docs.length))}`);\n    console.log(`  Total input:       ${pc.cyan(`~${estimatedTokensK}K tokens`)}`);\n    console.log(`  Output:            ${pc.cyan(outputPath)}`);\n    console.log(`  Mode:              ${pc.cyan(options.multiFile ? 'multi-file' : 'single-file')}`);\n    console.log('');\n    console.log(pc.dim('No AI calls made (dry run).'));\n\n    if (docs.length === 0) {\n      console.log('');\n      console.log(pc.yellow('Warning: No AGENTS.md files found. Run `are generate` first or omit --dry-run to auto-generate.'));\n    } else if (estimatedTokensK > 150) {\n      console.log('');\n      console.log(pc.yellow('Warning: Input exceeds 150K tokens. Consider using a model with extended context.'));\n    }\n\n    return;\n  }\n\n  // Auto-generate if no AGENTS.md files exist\n  if (docs.length === 0) {\n    console.log(pc.yellow('No AGENTS.md files found. Running generate first...'));\n    await generateCommand(targetPath, {\n      debug: options.debug,\n      trace: options.trace,\n    });\n    docs = await collectAgentsDocs(absolutePath);\n    if (docs.length === 0) {\n      console.error(pc.red('Error: No AGENTS.md files found after generation. Cannot proceed.'));\n      process.exit(1);\n    }\n  }\n\n  // ---------------------------------------------------------------------------\n  // Resolve backend and run AI synthesis\n  // ---------------------------------------------------------------------------\n\n  const registry = createBackendRegistry();\n  let backend;\n  try {\n    backend = await resolveBackend(registry, config.ai.backend);\n  } catch (error) {\n    if (error instanceof AIServiceError && error.code === 'CLI_NOT_FOUND') {\n      console.error(pc.red('Error: No AI CLI found.\\n'));\n      console.error(getInstallInstructions(registry));\n      process.exit(2);\n    }\n    throw error;\n  }\n\n  // Debug: log backend info\n  if (options.debug) {\n    console.error(pc.dim(`[debug] Backend: ${backend.name}`));\n    console.error(pc.dim(`[debug] CLI command: ${backend.cliCommand}`));\n    console.error(pc.dim(`[debug] Model: ${config.ai.model}`));\n  }\n\n  // Create AI service with extended timeout (spec generation takes longer)\n  const aiService = new AIService(backend, {\n    timeoutMs: Math.max(config.ai.timeoutMs, 600_000),\n    maxRetries: config.ai.maxRetries,\n    model: config.ai.model,\n    telemetry: { keepRuns: config.ai.telemetry.keepRuns },\n  });\n\n  if (options.debug) {\n    aiService.setDebug(true);\n  }\n\n  // Build prompt from collected docs\n  const prompt = buildSpecPrompt(docs);\n\n  if (options.debug) {\n    console.error(pc.dim(`[debug] System prompt: ${prompt.system.length} chars`));\n    console.error(pc.dim(`[debug] User prompt: ${prompt.user.length} chars`));\n  }\n\n  // Create progress log for tail -f monitoring\n  const progressLog = ProgressLog.create(absolutePath);\n  progressLog.write(`=== ARE Specify (${new Date().toISOString()}) ===`);\n  progressLog.write(`Project: ${absolutePath}`);\n  progressLog.write(`AGENTS.md files: ${docs.length}`);\n  progressLog.write('');\n\n  console.log(pc.bold('Generating specification...'));\n  console.log(pc.dim('This may take several minutes depending on project size.'));\n  progressLog.write('Generating specification...');\n\n  const response = await aiService.call({\n    prompt: prompt.user,\n    systemPrompt: prompt.system,\n    taskLabel: 'specify',\n  });\n\n  // ---------------------------------------------------------------------------\n  // Write output\n  // ---------------------------------------------------------------------------\n\n  try {\n    const writtenFiles = await writeSpec(response.text, {\n      outputPath,\n      force: options.force ?? false,\n      multiFile: options.multiFile ?? false,\n    });\n\n    console.log('');\n    console.log(pc.green(pc.bold('Specification written successfully:')));\n    for (const file of writtenFiles) {\n      console.log(pc.green(`  ${file}`));\n      progressLog.write(`Written: ${file}`);\n    }\n  } catch (error) {\n    if (error instanceof SpecExistsError) {\n      progressLog.write(`Error: ${error.message}`);\n      await progressLog.finalize();\n      console.error(pc.red(error.message));\n      process.exit(1);\n    }\n    throw error;\n  }\n\n  // ---------------------------------------------------------------------------\n  // Finalize telemetry\n  // ---------------------------------------------------------------------------\n\n  const { summary } = await aiService.finalize(absolutePath);\n\n  const summaryLine = `Tokens: ${summary.totalInputTokens} in / ${summary.totalOutputTokens} out` +\n    ` | Duration: ${(summary.totalDurationMs / 1000).toFixed(1)}s` +\n    ` | Output: ${outputPath}`;\n  console.log('');\n  console.log(pc.dim(summaryLine));\n  progressLog.write(summaryLine);\n  await progressLog.finalize();\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**CLI entry point for synthesizing project specifications from AGENTS.md documentation via AI backend, with single/multi-file output modes, dry-run preview, and auto-generation fallback.**\n\n## Exported Functions\n\n**`specifyCommand(targetPath: string, options: SpecifyOptions): Promise<void>`** — orchestrates specification generation: loads config via `loadConfig()`, collects AGENTS.md files via `collectAgentsDocs()`, auto-invokes `generateCommand()` if no docs exist, resolves AI backend via `resolveBackend()`, constructs prompt via `buildSpecPrompt()`, calls `AIService.call()` with extended timeout (minimum 600,000ms), writes output via `writeSpec()`, logs telemetry summary with token counts and duration.\n\n**`SpecifyOptions`** — interface with fields: `output?: string` (custom path, defaults to `specs/SPEC.md`), `force?: boolean` (overwrite existing specs), `dryRun?: boolean` (preview without AI calls), `multiFile?: boolean` (split output by directory), `debug?: boolean` (verbose logging), `trace?: boolean` (enable NDJSON trace events).\n\n## Workflow Phases\n\n**Phase 1: Collection and Validation** — resolves absolute path via `path.resolve(targetPath)`, derives `outputPath` from `options.output` or defaults to `path.join(absolutePath, 'specs', 'SPEC.md')`, calls `loadConfig(absolutePath, { debug })`, invokes `collectAgentsDocs(absolutePath)` returning array of `{ path, content }` tuples, auto-generates via `generateCommand(targetPath, { debug, trace })` if `docs.length === 0` and not in dry-run mode, exits with code 1 if no docs found after generation.\n\n**Phase 2: Dry-Run Preview** — computes `totalChars` via `docs.reduce((sum, d) => sum + d.content.length, 0)`, estimates tokens as `Math.ceil(totalChars / 4) / 1000`, prints summary table showing `docs.length`, estimated tokens, `outputPath`, and mode (`single-file` or `multi-file`), warns if no AGENTS.md files exist or if `estimatedTokensK > 150`, returns early without AI calls.\n\n**Phase 3: Backend Resolution and AI Call** — creates registry via `createBackendRegistry()`, resolves backend via `resolveBackend(registry, config.ai.backend)`, catches `AIServiceError` with code `'CLI_NOT_FOUND'` and prints installation instructions via `getInstallInstructions(registry)` before exiting with code 2, instantiates `AIService` with backend and options `{ timeoutMs: Math.max(config.ai.timeoutMs, 600_000), maxRetries, model, telemetry }`, enables debug mode via `aiService.setDebug(true)` if `options.debug` is true, constructs prompt via `buildSpecPrompt(docs)` returning `{ system, user }`, creates `ProgressLog` via `ProgressLog.create(absolutePath)` for tail monitoring, invokes `aiService.call({ prompt: prompt.user, systemPrompt: prompt.system, taskLabel: 'specify' })`.\n\n**Phase 4: Output Writing** — calls `writeSpec(response.text, { outputPath, force, multiFile })` returning `writtenFiles[]` array, catches `SpecExistsError` and logs to progress before exiting with code 1, prints green success message listing each written file, finalizes telemetry via `aiService.finalize(absolutePath)` returning `{ summary }`, logs summary line with format `Tokens: ${totalInputTokens} in / ${totalOutputTokens} out | Duration: ${totalDurationMs/1000}s | Output: ${outputPath}`, writes summary to progress log and finalizes via `progressLog.finalize()`.\n\n## Dependencies\n\n**Core imports:** `path.resolve()`, `path.join()` for path resolution; `picocolors` as `pc` for terminal color formatting (`pc.bold()`, `pc.green()`, `pc.red()`, `pc.yellow()`, `pc.cyan()`, `pc.dim()`).\n\n**Internal modules:** `loadConfig` from `config/loader.js` for YAML config loading; `collectAgentsDocs` from `generation/collector.js` for recursive AGENTS.md traversal; `buildSpecPrompt`, `writeSpec`, `SpecExistsError` from `specify/index.js` for prompt construction and file writing; `AIService`, `AIServiceError`, `createBackendRegistry`, `resolveBackend`, `getInstallInstructions` from `ai/index.js` for backend orchestration; `ProgressLog` from `orchestration/index.js` for streaming log output; `generateCommand` from `./generate.js` for auto-generation fallback.\n\n## Error Handling\n\n**CLI not found:** catches `AIServiceError` with `code === 'CLI_NOT_FOUND'` during backend resolution, prints error message and installation instructions via `getInstallInstructions(registry)`, exits with code 2.\n\n**Spec file exists:** catches `SpecExistsError` during `writeSpec()`, logs to progress via `progressLog.write()`, finalizes log, prints red error message, exits with code 1.\n\n**No docs after generation:** validates `docs.length === 0` after auto-generation attempt, prints red error message `'No AGENTS.md files found after generation. Cannot proceed.'`, exits with code 1.\n\n## Telemetry Integration\n\n**Progress log format:** writes header line `=== ARE Specify (${ISO timestamp}) ===`, project path, AGENTS.md count, generation status, written file paths, token/duration summary line, finalizes via `progressLog.finalize()` for flush.\n\n**Token estimation:** computes `estimatedTokensK = Math.ceil(totalChars / 4) / 1000` using 1 token ≈ 4 characters heuristic, warns if exceeds 150K tokens.\n\n**Telemetry finalization:** invokes `aiService.finalize(absolutePath)` returning `summary` with fields `totalInputTokens`, `totalOutputTokens`, `totalDurationMs`, formats summary line with duration in seconds (`(totalDurationMs / 1000).toFixed(1)`).\n\n## Timeout Configuration\n\n**Extended timeout:** overrides config timeout with minimum 600,000ms (10 minutes) via `Math.max(config.ai.timeoutMs, 600_000)` to accommodate large specification synthesis tasks.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 1555,
      "cacheReadTokens": 17114,
      "cacheCreationTokens": 11210,
      "latencyMs": 42743,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/cli/specify.ts",
          "sizeBytes": 7367
        }
      ]
    },
    {
      "timestamp": "2026-02-09T16:09:38.922Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/config/schema.ts\n\n```typescript\n/**\n * Zod schema for configuration validation\n *\n * This schema defines the structure of `.agents-reverse/config.yaml`\n * and provides sensible defaults for all fields.\n */\n\nimport { z } from 'zod';\nimport {\n  DEFAULT_VENDOR_DIRS,\n  DEFAULT_BINARY_EXTENSIONS,\n  DEFAULT_MAX_FILE_SIZE,\n  DEFAULT_EXCLUDE_PATTERNS,\n  getDefaultConcurrency,\n} from './defaults.js';\n\n/**\n * Schema for exclusion configuration\n */\nconst ExcludeSchema = z.object({\n  /** Custom glob patterns to exclude (e.g., [\"*.log\", \"temp/**\"]) */\n  patterns: z.array(z.string()).default([...DEFAULT_EXCLUDE_PATTERNS]),\n  /** Vendor directories to exclude */\n  vendorDirs: z.array(z.string()).default([...DEFAULT_VENDOR_DIRS]),\n  /** Binary file extensions to exclude */\n  binaryExtensions: z.array(z.string()).default([...DEFAULT_BINARY_EXTENSIONS]),\n}).default({});\n\n/**\n * Schema for options configuration\n */\nconst OptionsSchema = z.object({\n  /** Whether to follow symbolic links during traversal */\n  followSymlinks: z.boolean().default(false),\n  /** Maximum file size in bytes (files larger than this are skipped) */\n  maxFileSize: z.number().positive().default(DEFAULT_MAX_FILE_SIZE),\n}).default({});\n\n/**\n * Schema for output configuration\n */\nconst OutputSchema = z.object({\n  /** Whether to use colors in terminal output */\n  colors: z.boolean().default(true),\n}).default({});\n\n/**\n * Schema for AI service configuration.\n *\n * Controls backend selection, model, timeout, retry behavior, and\n * telemetry log retention. All fields have sensible defaults.\n */\nconst AISchema = z.object({\n  /** AI CLI backend to use ('auto' detects from PATH) */\n  backend: z.enum(['claude', 'gemini', 'opencode', 'auto']).default('auto'),\n  /** Model identifier (backend-specific, e.g., \"sonnet\", \"opus\") */\n  model: z.string().default('sonnet'),\n  /** Default subprocess timeout in milliseconds */\n  timeoutMs: z.number().positive().default(300_000),\n  /** Maximum number of retries for transient errors */\n  maxRetries: z.number().min(0).default(3),\n  /** Default parallelism for concurrent AI calls (1-20). Auto-detected from CPU cores and available memory. */\n  concurrency: z.number().min(1).max(20).default(getDefaultConcurrency),\n  /** Telemetry settings */\n  telemetry: z.object({\n    /** Number of most recent run logs to keep on disk */\n    keepRuns: z.number().min(0).default(50),\n  }).default({}),\n}).default({});\n\n/**\n * Main configuration schema for agents-reverse.\n *\n * All fields have sensible defaults, so an empty object `{}` is valid\n * and will result in a fully populated configuration.\n *\n * @example\n * ```typescript\n * // Parse with defaults\n * const config = ConfigSchema.parse({});\n *\n * // Parse with partial overrides\n * const config = ConfigSchema.parse({\n *   exclude: { patterns: ['*.log'] },\n *   ai: { backend: 'claude', model: 'opus' },\n * });\n * ```\n */\nexport const ConfigSchema = z.object({\n  /** Exclusion rules for files and directories */\n  exclude: ExcludeSchema,\n  /** Discovery options */\n  options: OptionsSchema,\n  /** Output formatting options */\n  output: OutputSchema,\n  /** AI service configuration */\n  ai: AISchema,\n}).default({});\n\n/**\n * Inferred TypeScript type from the schema.\n * Use this type for function parameters and return types.\n */\nexport type Config = z.infer<typeof ConfigSchema>;\n\n/**\n * Type for the exclude section of config\n */\nexport type ExcludeConfig = z.infer<typeof ExcludeSchema>;\n\n/**\n * Type for the options section of config\n */\nexport type OptionsConfig = z.infer<typeof OptionsSchema>;\n\n/**\n * Type for the output section of config\n */\nexport type OutputConfig = z.infer<typeof OutputSchema>;\n\n/**\n * Type for the AI service section of config\n */\nexport type AIConfig = z.infer<typeof AISchema>;\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**ConfigSchema defines Zod validation schema for `.agents-reverse-engineer/config.yaml` with nested object schemas for exclusion rules, discovery options, output formatting, and AI service configuration, providing type-safe defaults via z.default() chaining.**\n\n## Exported Schemas\n\n**ConfigSchema** (z.ZodDefault<z.ZodObject>) — Root configuration schema with four nested sections: `exclude`, `options`, `output`, `ai`. Accepts empty object `{}` which populates all fields with defaults via `.default({})` at schema level.\n\n**ExcludeSchema** (z.ZodDefault<z.ZodObject>) — Defines `patterns` (array of gitignore-style globs from `DEFAULT_EXCLUDE_PATTERNS`), `vendorDirs` (array from `DEFAULT_VENDOR_DIRS`), `binaryExtensions` (array from `DEFAULT_BINARY_EXTENSIONS`). All arrays cloned via spread operator `[...DEFAULT_*]` to prevent mutation.\n\n**OptionsSchema** (z.ZodDefault<z.ZodObject>) — Defines `followSymlinks` (boolean, default false), `maxFileSize` (positive number from `DEFAULT_MAX_FILE_SIZE`). Controls file discovery traversal behavior.\n\n**OutputSchema** (z.ZodDefault<z.ZodObject>) — Defines `colors` (boolean, default true) for ANSI terminal color code enablement.\n\n**AISchema** (z.ZodDefault<z.ZodObject>) — Defines `backend` (enum: 'claude' | 'gemini' | 'opencode' | 'auto', default 'auto'), `model` (string, default 'sonnet'), `timeoutMs` (positive number, default 300000 = 5 minutes), `maxRetries` (min 0, default 3), `concurrency` (min 1, max 20, default from `getDefaultConcurrency()` function), `telemetry` nested object with `keepRuns` (min 0, default 50). Concurrency default is computed dynamically via function invocation during schema construction.\n\n## Exported TypeScript Types\n\n**Config** — Inferred via `z.infer<typeof ConfigSchema>` representing fully parsed configuration object with all defaults applied.\n\n**ExcludeConfig** — Inferred from ExcludeSchema: `{ patterns: string[], vendorDirs: string[], binaryExtensions: string[] }`.\n\n**OptionsConfig** — Inferred from OptionsSchema: `{ followSymlinks: boolean, maxFileSize: number }`.\n\n**OutputConfig** — Inferred from OutputSchema: `{ colors: boolean }`.\n\n**AIConfig** — Inferred from AISchema: `{ backend: 'claude' | 'gemini' | 'opencode' | 'auto', model: string, timeoutMs: number, maxRetries: number, concurrency: number, telemetry: { keepRuns: number } }`.\n\n## Import Dependencies\n\nImports `z` from 'zod' for schema construction. Imports constants `DEFAULT_VENDOR_DIRS`, `DEFAULT_BINARY_EXTENSIONS`, `DEFAULT_MAX_FILE_SIZE`, `DEFAULT_EXCLUDE_PATTERNS` and function `getDefaultConcurrency` from `./defaults.js`. These provide platform-specific defaults (e.g., concurrency adjusted for WSL vs. macOS/Linux based on CPU cores and available memory).\n\n## Schema Validation Pattern\n\nAll schemas use `.default()` chaining at both field level (individual fields) and object level (entire section). This enables partial configuration merging: user provides `{ ai: { backend: 'claude' } }`, Zod merges with defaults to produce complete AIConfig with timeoutMs, maxRetries, concurrency, telemetry.keepRuns populated from schema defaults.\n\nArray defaults use spread operator `[...DEFAULT_PATTERNS]` to clone imported arrays, preventing accidental mutation of shared constant references during schema validation.\n\n## Dynamic Default Resolution\n\nAISchema.concurrency uses function invocation `.default(getDefaultConcurrency)` instead of static value. Zod evaluates function during parse to compute platform-specific concurrency (2 for WSL, 5 for others, adjusted for CPU cores/memory). Function must return number matching `.min(1).max(20)` constraint.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 1002,
      "cacheReadTokens": 17114,
      "cacheCreationTokens": 10211,
      "latencyMs": 25583,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/config/schema.ts",
          "sizeBytes": 3737
        }
      ]
    },
    {
      "timestamp": "2026-02-09T16:09:41.994Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/discovery/run.ts\n\n```typescript\n/**\n * High-level file discovery pipeline.\n *\n * Combines directory walking and filter application into a single function\n * that all commands can share.\n */\n\nimport { walkDirectory } from './walker.js';\nimport {\n  applyFilters,\n  createGitignoreFilter,\n  createVendorFilter,\n  createBinaryFilter,\n  createCustomFilter,\n} from './filters/index.js';\nimport type { FilterResult } from './types.js';\nimport type { ITraceWriter } from '../orchestration/trace.js';\n\n/**\n * Configuration subset needed for file discovery.\n * Structurally compatible with the full Config type from config/schema.ts.\n */\nexport interface DiscoveryConfig {\n  exclude: {\n    vendorDirs: string[];\n    binaryExtensions: string[];\n    patterns: string[];\n  };\n  options: {\n    maxFileSize: number;\n    followSymlinks: boolean;\n  };\n}\n\n/**\n * Options for the discovery pipeline.\n */\nexport interface DiscoverFilesOptions {\n  tracer?: ITraceWriter;\n  debug?: boolean;\n}\n\n/**\n * Discover files in a directory by walking and applying the standard filter chain.\n *\n * Creates the four standard filters (gitignore, vendor, binary, custom),\n * walks the directory tree, and applies the filters. Returns the full\n * FilterResult so callers can access both included and excluded files\n * with filter attribution.\n *\n * @param root - Absolute path to the directory to scan\n * @param config - Discovery-related configuration (exclude rules and options)\n * @param options - Optional tracing and debug settings\n * @returns FilterResult with included and excluded file lists\n */\nexport async function discoverFiles(\n  root: string,\n  config: DiscoveryConfig,\n  options?: DiscoverFilesOptions,\n): Promise<FilterResult> {\n  // Create filters in standard order\n  const gitignoreFilter = await createGitignoreFilter(root);\n  const vendorFilter = createVendorFilter(config.exclude.vendorDirs);\n  const binaryFilter = createBinaryFilter({\n    maxFileSize: config.options.maxFileSize,\n    additionalExtensions: config.exclude.binaryExtensions,\n  });\n  const customFilter = createCustomFilter(config.exclude.patterns, root);\n  const filters = [gitignoreFilter, vendorFilter, binaryFilter, customFilter];\n\n  // Walk directory\n  const files = await walkDirectory({\n    cwd: root,\n    followSymlinks: config.options.followSymlinks,\n  });\n\n  // Apply filters and return\n  return applyFilters(files, filters, {\n    tracer: options?.tracer,\n    debug: options?.debug,\n  });\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**`run.ts` exports `discoverFiles()`, the high-level entry point orchestrating four-filter pipeline execution (gitignore → vendor → binary → custom) after directory walking, returning `FilterResult` with included/excluded file lists and filter attribution.**\n\n## Exported Interface\n\n**`DiscoveryConfig`** — Structural subset of full `Config` schema from `src/config/schema.ts` containing:\n- `exclude.vendorDirs: string[]` — Third-party directories to skip\n- `exclude.binaryExtensions: string[]` — Non-text file extensions\n- `exclude.patterns: string[]` — Gitignore-style globs\n- `options.maxFileSize: number` — Binary detection threshold in bytes\n- `options.followSymlinks: boolean` — Symlink traversal flag\n\n**`DiscoverFilesOptions`** — Optional parameters for pipeline execution:\n- `tracer?: ITraceWriter` — Trace event emitter for telemetry\n- `debug?: boolean` — Verbose logging flag\n\n**`discoverFiles(root: string, config: DiscoveryConfig, options?: DiscoverFilesOptions): Promise<FilterResult>`** — Async pipeline function creating filters in deterministic order (gitignore, vendor, binary, custom), invoking `walkDirectory()` with `cwd` and `followSymlinks` options, then applying filter chain via `applyFilters()` with tracer/debug propagation. Returns `FilterResult` containing `included: FileInfo[]`, `excluded: FileInfo[]` arrays with per-file filter attribution.\n\n## Filter Chain Composition\n\nConstructs four filters via factory functions:\n1. **`createGitignoreFilter(root)`** — Parses `.gitignore` files using `ignore` library\n2. **`createVendorFilter(config.exclude.vendorDirs)`** — Blocks third-party directories (node_modules, .git, dist)\n3. **`createBinaryFilter({ maxFileSize, additionalExtensions })`** — Detects binary files via extension + content analysis\n4. **`createCustomFilter(config.exclude.patterns, root)`** — Applies user-defined glob patterns\n\nFilters applied sequentially in declared order via `applyFilters()` from `./filters/index.js`.\n\n## Integration Points\n\n**Callers:** All CLI commands requiring file discovery (`src/cli/discover.ts`, `src/cli/generate.ts`, `src/cli/update.ts`) invoke `discoverFiles()` with config loaded from `loadConfig()`.\n\n**Dependencies:**\n- `walkDirectory()` from `./walker.js` — Filesystem traversal with symlink control\n- `applyFilters()` from `./filters/index.js` — Sequential filter application with attribution\n- `ITraceWriter` from `../orchestration/trace.js` — Telemetry event emission\n\n**Output consumption:** Returned `FilterResult.included` feeds into Phase 1 file analysis worker pool, `FilterResult.excluded` populates `GENERATION-PLAN.md` statistics.\n\n## Design Pattern\n\nFacade pattern consolidating filter creation, directory walking, and filter application into single async function with config-driven parameterization. Promotes structural compatibility via `DiscoveryConfig` interface instead of tight coupling to full `Config` type, enabling unit testing with minimal mock objects.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 773,
      "cacheReadTokens": 17114,
      "cacheCreationTokens": 9772,
      "latencyMs": 24690,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/discovery/run.ts",
          "sizeBytes": 2419
        }
      ]
    },
    {
      "timestamp": "2026-02-09T16:09:45.220Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/generation/collector.ts\n\n```typescript\nimport * as path from 'node:path';\nimport { readdir, readFile } from 'node:fs/promises';\n\n/**\n * A collected AGENTS.md document with its project-relative path and content.\n */\nexport type AgentsDocs = Array<{ relativePath: string; content: string }>;\n\n/** Directories to skip during recursive AGENTS.md collection. */\nconst SKIP_DIRS = new Set([\n  'node_modules', '.git', '.agents-reverse-engineer',\n  'vendor', 'dist', 'build', '__pycache__', '.next',\n  'venv', '.venv', 'target', '.cargo', '.gradle',\n]);\n\n/**\n * Recursively collect all AGENTS.md files under `projectRoot`,\n * returning their relative paths and content sorted alphabetically.\n *\n * Skips vendor/build/meta directories and gracefully handles\n * unreadable directories or files.\n */\nexport async function collectAgentsDocs(projectRoot: string): Promise<AgentsDocs> {\n  const results: AgentsDocs = [];\n\n  async function walk(currentDir: string): Promise<void> {\n    let entries;\n    try {\n      entries = await readdir(currentDir, { withFileTypes: true });\n    } catch {\n      return; // Permission denied or inaccessible\n    }\n    for (const entry of entries) {\n      if (entry.isDirectory() && !SKIP_DIRS.has(entry.name)) {\n        await walk(path.join(currentDir, entry.name));\n      } else if (entry.name === 'AGENTS.md') {\n        try {\n          const filePath = path.join(currentDir, entry.name);\n          const content = await readFile(filePath, 'utf-8');\n          results.push({\n            relativePath: path.relative(projectRoot, filePath),\n            content,\n          });\n        } catch {\n          // Skip unreadable files\n        }\n      }\n    }\n  }\n\n  await walk(projectRoot);\n  results.sort((a, b) => a.relativePath.localeCompare(b.relativePath));\n  return results;\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**collector.ts recursively discovers and aggregates all `AGENTS.md` files from a project tree into a sorted array of relative paths and content, skipping vendor/build/meta directories.**\n\n## Exported Types\n\n**`AgentsDocs`**: Type alias for `Array<{ relativePath: string; content: string }>` representing collected documentation files with project-relative paths.\n\n## Exported Functions\n\n**`collectAgentsDocs(projectRoot: string): Promise<AgentsDocs>`**: Recursively walks directory tree starting at `projectRoot`, collects all `AGENTS.md` files (ignoring unreadable directories/files), returns alphabetically sorted array by `relativePath`. Uses `readdir()` with `withFileTypes: true` for efficient type checking, `readFile()` for UTF-8 content loading, `path.relative()` for path normalization, and `Array.sort()` with `localeCompare()` for lexicographic ordering.\n\n## Directory Exclusion\n\n**`SKIP_DIRS`**: `Set<string>` containing 13 directory names to exclude during traversal: `'node_modules'`, `'.git'`, `'.agents-reverse-engineer'`, `'vendor'`, `'dist'`, `'build'`, `'__pycache__'`, `'.next'`, `'venv'`, `'.venv'`, `'target'`, `'.cargo'`, `'.gradle'`. Prevents recursion into build artifacts, version control, Python/Rust/Gradle caches, and ARE's own metadata directory.\n\n## Implementation Pattern\n\n**Recursive walker**: Inner `async walk(currentDir: string)` function implements depth-first traversal with try-catch around `readdir()` and `readFile()` to gracefully handle permission errors or inaccessible paths (continues execution without throwing). Checks `entry.isDirectory()` and `!SKIP_DIRS.has(entry.name)` before recursion. Matches filename via exact string equality `entry.name === 'AGENTS.md'`.\n\n## Integration Context\n\nUsed by Phase 3 root document synthesis in `src/generation/orchestrator.ts` to aggregate all directory-level documentation before generating platform-specific integration files (`CLAUDE.md`, `GEMINI.md`, `OPENCODE.md`). Consumed by `src/specify/index.ts` for project specification synthesis across entire documentation corpus.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 550,
      "cacheReadTokens": 17114,
      "cacheCreationTokens": 9618,
      "latencyMs": 22115,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/generation/collector.ts",
          "sizeBytes": 1756
        }
      ]
    },
    {
      "timestamp": "2026-02-09T16:09:25.861Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/config/loader.ts\n\n```typescript\n/**\n * Configuration loader for agents-reverse\n *\n * Loads and validates configuration from `.agents-reverse/config.yaml`.\n * Returns sensible defaults when no config file exists.\n */\n\nimport { readFile, writeFile, mkdir, access } from 'node:fs/promises';\nimport { constants } from 'node:fs';\nimport path from 'node:path';\nimport { parse, stringify } from 'yaml';\nimport { ZodError } from 'zod';\nimport pc from 'picocolors';\nimport { ConfigSchema, Config } from './schema.js';\nimport { DEFAULT_VENDOR_DIRS, DEFAULT_BINARY_EXTENSIONS, DEFAULT_MAX_FILE_SIZE, DEFAULT_EXCLUDE_PATTERNS, getDefaultConcurrency } from './defaults.js';\nimport type { ITraceWriter } from '../orchestration/trace.js';\n\n/**\n * Quote a string value for YAML output if it contains characters that\n * would be misinterpreted (e.g. `*` is the YAML alias indicator).\n */\nfunction yamlScalar(value: string): string {\n  // Characters that require quoting: *, {, }, [, ], ?, :, #, &, !, |, >, etc.\n  if (/[*{}\\[\\]?,:#&!|>'\"%@`]/.test(value)) {\n    return `\"${value.replace(/\\\\/g, '\\\\\\\\').replace(/\"/g, '\\\\\"')}\"`;\n  }\n  return value;\n}\n\n/** Directory name for agents-reverse-engineer configuration */\nexport const CONFIG_DIR = '.agents-reverse-engineer';\n\n/** Configuration file name */\nexport const CONFIG_FILE = 'config.yaml';\n\n/**\n * Error thrown when configuration parsing or validation fails\n */\nexport class ConfigError extends Error {\n  constructor(\n    message: string,\n    public readonly filePath: string,\n    public readonly cause?: Error\n  ) {\n    super(message);\n    this.name = 'ConfigError';\n  }\n}\n\n/**\n * Load configuration from `.agents-reverse/config.yaml`.\n *\n * If the file doesn't exist, returns default configuration.\n * If the file exists but is invalid, throws a ConfigError with details.\n *\n * @param root - Root directory containing `.agents-reverse/` folder\n * @param options - Optional configuration loading options\n * @param options.tracer - Trace writer for emitting config:loaded events\n * @param options.debug - Enable debug output for configuration loading\n * @returns Validated configuration object with all defaults applied\n * @throws ConfigError if the config file exists but is invalid\n *\n * @example\n * ```typescript\n * const config = await loadConfig('/path/to/project');\n * console.log(config.exclude.vendorDirs);\n * ```\n */\nexport async function loadConfig(\n  root: string,\n  options?: { tracer?: ITraceWriter; debug?: boolean }\n): Promise<Config> {\n  const configPath = path.join(root, CONFIG_DIR, CONFIG_FILE);\n\n  try {\n    const content = await readFile(configPath, 'utf-8');\n    const raw = parse(content);\n\n    try {\n      const config = ConfigSchema.parse(raw);\n\n      // Emit trace event\n      options?.tracer?.emit({\n        type: 'config:loaded',\n        configPath: path.relative(root, configPath),\n        model: config.ai.model,\n        concurrency: config.ai.concurrency,\n      });\n\n      // Debug output\n      if (options?.debug) {\n        console.error(pc.dim(`[debug] Config loaded from: ${path.relative(root, configPath)}`));\n        console.error(\n          pc.dim(`[debug] Model: ${config.ai.model}, Concurrency: ${config.ai.concurrency}`)\n        );\n      }\n\n      return config;\n    } catch (err) {\n      if (err instanceof ZodError) {\n        const issues = err.issues\n          .map((issue) => `  - ${issue.path.join('.')}: ${issue.message}`)\n          .join('\\n');\n        throw new ConfigError(\n          `Invalid configuration in ${configPath}:\\n${issues}`,\n          configPath,\n          err\n        );\n      }\n      throw err;\n    }\n  } catch (err) {\n    // File not found - return defaults\n    if ((err as NodeJS.ErrnoException).code === 'ENOENT') {\n      const config = ConfigSchema.parse({});\n\n      // Emit trace event for defaults\n      options?.tracer?.emit({\n        type: 'config:loaded',\n        configPath: '(defaults)',\n        model: config.ai.model,\n        concurrency: config.ai.concurrency,\n      });\n\n      // Debug output\n      if (options?.debug) {\n        console.error(pc.dim(`[debug] Config file not found, using defaults`));\n        console.error(\n          pc.dim(`[debug] Model: ${config.ai.model}, Concurrency: ${config.ai.concurrency}`)\n        );\n      }\n\n      return config;\n    }\n\n    // Re-throw ConfigError as-is\n    if (err instanceof ConfigError) {\n      throw err;\n    }\n\n    // YAML parse error\n    throw new ConfigError(\n      `Failed to parse ${configPath}: ${(err as Error).message}`,\n      configPath,\n      err as Error\n    );\n  }\n}\n\n/**\n * Check if a configuration file exists.\n *\n * @param root - Root directory to check\n * @returns true if `.agents-reverse/config.yaml` exists\n *\n * @example\n * ```typescript\n * if (!await configExists('.')) {\n *   console.log('Run `are init` to create configuration');\n * }\n * ```\n */\nexport async function configExists(root: string): Promise<boolean> {\n  const configPath = path.join(root, CONFIG_DIR, CONFIG_FILE);\n  try {\n    await access(configPath, constants.F_OK);\n    return true;\n  } catch {\n    return false;\n  }\n}\n\n/**\n * Write a default configuration file with helpful comments.\n *\n * Creates the `.agents-reverse/` directory if it doesn't exist.\n * The generated file includes comments explaining each option.\n *\n * @param root - Root directory where `.agents-reverse/` will be created\n *\n * @example\n * ```typescript\n * await writeDefaultConfig('/path/to/project');\n * // Creates /path/to/project/.agents-reverse/config.yaml\n * ```\n */\nexport async function writeDefaultConfig(root: string): Promise<void> {\n  const configDir = path.join(root, CONFIG_DIR);\n  const configPath = path.join(configDir, CONFIG_FILE);\n\n  // Create directory if needed\n  await mkdir(configDir, { recursive: true });\n\n  // Generate config content with comments\n  const configContent = `# agents-reverse-engineer configuration\n# See: https://github.com/your-org/agents-reverse-engineer\n\n# ============================================================================\n# FILE & DIRECTORY EXCLUSIONS\n# ============================================================================\nexclude:\n  # Custom glob patterns to exclude (e.g., [\"*.log\", \"temp/**\"])\n  # Default patterns exclude AI-generated documentation files\n  patterns:\n${DEFAULT_EXCLUDE_PATTERNS.map((pattern) => `    - ${yamlScalar(pattern)}`).join('\\n')}\n\n  # Vendor directories to exclude from analysis\n  # These are typically package managers, build outputs, or version control\n  vendorDirs:\n${DEFAULT_VENDOR_DIRS.map((dir) => `    - ${dir}`).join('\\n')}\n\n  # Binary file extensions to exclude from analysis\n  # These files cannot be meaningfully analyzed as text\n  binaryExtensions:\n${DEFAULT_BINARY_EXTENSIONS.map((ext) => `    - ${ext}`).join('\\n')}\n\n# ============================================================================\n# DISCOVERY OPTIONS\n# ============================================================================\noptions:\n  # Whether to follow symbolic links during traversal\n  followSymlinks: false\n\n  # Maximum file size in bytes (files larger than this are skipped)\n  # Default: ${DEFAULT_MAX_FILE_SIZE} (1MB)\n  maxFileSize: ${DEFAULT_MAX_FILE_SIZE}\n\n# ============================================================================\n# OUTPUT FORMATTING\n# ============================================================================\noutput:\n  # Whether to use colors in terminal output\n  colors: true\n\n# ============================================================================\n# AI SERVICE CONFIGURATION\n# ============================================================================\nai:\n  # AI CLI backend to use\n  # Options: 'claude', 'gemini', 'opencode', 'auto' (auto-detect from PATH)\n  backend: auto\n\n  # Model identifier (backend-specific)\n  # Examples: \"sonnet\", \"opus\", \"haiku\" (for Claude)\n  model: sonnet\n\n  # Subprocess timeout in milliseconds\n  # Default: 300,000ms (5 minutes)\n  # Increase for very large files or slow connections\n  timeoutMs: 300000\n\n  # Maximum number of retries for transient errors\n  # Default: 3\n  maxRetries: 3\n\n  # Number of concurrent AI calls (parallelism)\n  # Range: 1-20, Default: auto-detected from CPU cores and available memory\n  # Current machine default: ${getDefaultConcurrency()}\n  # Uncomment to override:\n  # concurrency: ${getDefaultConcurrency()}\n\n  # Telemetry settings\n  telemetry:\n    # Number of most recent run logs to keep on disk\n    # Logs stored in .agents-reverse-engineer/logs/\n    keepRuns: 50\n`;\n\n  await writeFile(configPath, configContent, 'utf-8');\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**loadConfig() loads `.agents-reverse-engineer/config.yaml` via YAML parsing, validates with ConfigSchema Zod schema, returns Config with defaults when file absent, and throws ConfigError on parse/validation failures; writeDefaultConfig() generates commented YAML with DEFAULT_VENDOR_DIRS/DEFAULT_BINARY_EXTENSIONS/DEFAULT_EXCLUDE_PATTERNS arrays; configExists() checks file presence via fs.access().**\n\n## Exported Functions\n\n**loadConfig(root: string, options?: { tracer?: ITraceWriter; debug?: boolean }): Promise<Config>**\nReads `${root}/${CONFIG_DIR}/${CONFIG_FILE}` (resolves to `.agents-reverse-engineer/config.yaml`), parses YAML via `parse()`, validates with `ConfigSchema.parse(raw)`, returns validated Config object. When ENOENT caught, returns `ConfigSchema.parse({})` triggering default population. Emits `config:loaded` trace event via `options.tracer?.emit()` with fields `{ type, configPath, model, concurrency }`. Writes debug output via `console.error(pc.dim())` when `options.debug` enabled. Throws ConfigError wrapping ZodError with formatted `issue.path.join('.')` and `issue.message` for validation failures. Throws ConfigError for YAML parse errors with underlying error as `cause`.\n\n**configExists(root: string): Promise<boolean>**\nChecks presence of `${root}/${CONFIG_DIR}/${CONFIG_FILE}` via `access(configPath, constants.F_OK)`, returns true on success, false on catch.\n\n**writeDefaultConfig(root: string): Promise<void>**\nCreates `${root}/${CONFIG_DIR}` via `mkdir(configDir, { recursive: true })`, writes YAML template to `${root}/${CONFIG_DIR}/${CONFIG_FILE}` via `writeFile()`. Template includes comment headers (`# FILE & DIRECTORY EXCLUSIONS`, `# DISCOVERY OPTIONS`, `# OUTPUT FORMATTING`, `# AI SERVICE CONFIGURATION`) with inline defaults: `exclude.patterns` array from DEFAULT_EXCLUDE_PATTERNS (mapped with `yamlScalar(pattern)` quoting), `exclude.vendorDirs` from DEFAULT_VENDOR_DIRS, `exclude.binaryExtensions` from DEFAULT_BINARY_EXTENSIONS, `options.maxFileSize: ${DEFAULT_MAX_FILE_SIZE}` interpolation, `ai.timeoutMs: 300000`, `ai.maxRetries: 3`, `ai.concurrency` commented with `${getDefaultConcurrency()}` current machine value, `ai.telemetry.keepRuns: 50`.\n\n## Exported Constants\n\n**CONFIG_DIR = '.agents-reverse-engineer'**\nDirectory name for configuration root.\n\n**CONFIG_FILE = 'config.yaml'**\nConfiguration filename within CONFIG_DIR.\n\n## Exported Classes\n\n**ConfigError extends Error**\nConstructor signature: `(message: string, filePath: string, cause?: Error)`. Public readonly property `filePath: string` stores config file path. Public readonly property `cause?: Error` stores underlying ZodError or YAML parse error. Sets `name` property to `'ConfigError'` string literal.\n\n## Dependencies\n\nImports `readFile`, `writeFile`, `mkdir`, `access` from `node:fs/promises`, `constants` from `node:fs`, `path` from `node:path`, `parse`/`stringify` from `yaml` package, `ZodError` from `zod`, `pc` (picocolors) for `pc.dim()` formatting. Imports `ConfigSchema` and `Config` type from `./schema.js`. Imports DEFAULT_VENDOR_DIRS, DEFAULT_BINARY_EXTENSIONS, DEFAULT_MAX_FILE_SIZE, DEFAULT_EXCLUDE_PATTERNS, getDefaultConcurrency from `./defaults.js`. Imports `ITraceWriter` type from `../orchestration/trace.js`.\n\n## Helper Functions\n\n**yamlScalar(value: string): string**\nInternal function wrapping strings containing YAML metacharacters in double quotes. Regex pattern `/[*{}\\[\\]?,:#&!|>'\"%@`]/` detects characters requiring quoting (asterisk for aliases, braces for flow collections, square brackets for sequences, question mark/colon for mappings, hash for comments, ampersand for anchors, pipe/greater for block scalars, quotes, percent for directives, at for reserved indicators, backtick). Escapes backslashes via `.replace(/\\\\/g, '\\\\\\\\')` before escaping quotes via `.replace(/\"/g, '\\\\\"')`. Returns unquoted value when regex test fails.\n\n## Error Handling Patterns\n\nConfigError wraps two error types: ZodError from schema validation (extracts `err.issues[]` array, maps to formatted strings `${issue.path.join('.')}: ${issue.message}`, joins with newlines in error message), and generic Error from YAML parse failures (includes `(err as Error).message` in error message). ENOENT catch block for missing config files returns defaults without throwing. ConfigError instances rethrown as-is during loadConfig() error handling. All other errors cast to Error and wrapped in ConfigError with `configPath` and `cause` properties.\n\n## Trace Event Schema\n\n`config:loaded` event emitted via `tracer.emit()` with fields:\n- `type: 'config:loaded'` (string literal discriminator)\n- `configPath: string` (relative path via `path.relative(root, configPath)` or literal `'(defaults)'` when file absent)\n- `model: string` (from `config.ai.model`)\n- `concurrency: number` (from `config.ai.concurrency`)\n\n## YAML Template Structure\n\nGenerated config includes four section headers with inline documentation:\n1. `# FILE & DIRECTORY EXCLUSIONS` — maps `exclude.patterns`, `exclude.vendorDirs`, `exclude.binaryExtensions` arrays\n2. `# DISCOVERY OPTIONS` — sets `options.followSymlinks: false`, `options.maxFileSize: ${DEFAULT_MAX_FILE_SIZE}`\n3. `# OUTPUT FORMATTING` — sets `output.colors: true`\n4. `# AI SERVICE CONFIGURATION` — maps `ai.backend: auto`, `ai.model: sonnet`, `ai.timeoutMs: 300000`, `ai.maxRetries: 3`, commented `ai.concurrency` line with current default, nested `ai.telemetry.keepRuns: 50`",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 1508,
      "cacheReadTokens": 17114,
      "cacheCreationTokens": 11461,
      "latencyMs": 43459,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/config/loader.ts",
          "sizeBytes": 8501
        }
      ]
    },
    {
      "timestamp": "2026-02-09T16:09:46.780Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/generation/complexity.ts\n\n```typescript\nimport * as path from 'node:path';\n\n/**\n * Metrics about codebase complexity.\n */\nexport interface ComplexityMetrics {\n  /** Total number of source files */\n  fileCount: number;\n  /** Maximum directory depth */\n  directoryDepth: number;\n  /** List of source file paths */\n  files: string[];\n  /** Unique directory paths */\n  directories: Set<string>;\n}\n\n/**\n * Calculate maximum directory depth from file paths.\n */\nfunction calculateDirectoryDepth(files: string[], projectRoot: string): number {\n  let maxDepth = 0;\n\n  for (const file of files) {\n    const relativePath = path.relative(projectRoot, file);\n    const depth = relativePath.split(path.sep).length - 1; // -1 for the file itself\n    maxDepth = Math.max(maxDepth, depth);\n  }\n\n  return maxDepth;\n}\n\n/**\n * Extract unique directories from file paths.\n */\nfunction extractDirectories(files: string[]): Set<string> {\n  const directories = new Set<string>();\n\n  for (const file of files) {\n    let dir = path.dirname(file);\n    while (dir && dir !== '.') {\n      directories.add(dir);\n      const parent = path.dirname(dir);\n      if (parent === dir) break; // Reached root\n      dir = parent;\n    }\n  }\n\n  return directories;\n}\n\n/**\n * Analyze codebase complexity from discovered files.\n *\n * @param files - List of source file paths\n * @param projectRoot - Project root directory\n * @returns Complexity metrics\n */\nexport function analyzeComplexity(\n  files: string[],\n  projectRoot: string\n): ComplexityMetrics {\n  return {\n    fileCount: files.length,\n    directoryDepth: calculateDirectoryDepth(files, projectRoot),\n    files,\n    directories: extractDirectories(files),\n  };\n}\n\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**complexity.ts computes codebase complexity metrics (file count, directory depth, unique directories) from discovered file paths for documentation generation planning.**\n\n## Exported Interface\n\n**ComplexityMetrics** - Aggregated complexity measurements\n- `fileCount: number` - Total source files discovered\n- `directoryDepth: number` - Maximum nested directory depth\n- `files: string[]` - Complete list of source file absolute paths\n- `directories: Set<string>` - Unique directory paths extracted from files\n\n## Exported Function\n\n**analyzeComplexity(files: string[], projectRoot: string): ComplexityMetrics** - Orchestrates complexity computation by invoking `calculateDirectoryDepth()` and `extractDirectories()` on input file array, returns populated ComplexityMetrics object.\n\n## Internal Functions\n\n**calculateDirectoryDepth(files: string[], projectRoot: string): number** - Computes maximum depth by calling `path.relative(projectRoot, file)` for each file, splitting on `path.sep`, subtracting 1 (excludes filename from depth count), tracking `Math.max()` across all files.\n\n**extractDirectories(files: string[]): Set<string>** - Walks parent chain via `path.dirname()` loop for each file, terminating when `dir === '.'` or `parent === dir` (root reached), accumulates unique directories in Set to deduplicate shared ancestor paths.\n\n## Integration Points\n\nUsed by `src/generation/orchestrator.ts` or `src/cli/discover.ts` to compute metrics for GENERATION-PLAN.md phase breakdown and concurrency decisions. The `directoryDepth` metric influences post-order directory aggregation sorting (Phase 2 processes deepest directories first). The `directories` Set drives `AGENTS.md` generation target list.\n\n## Algorithm Details\n\n**Depth calculation**: Relative path splitting produces array like `['src', 'generation', 'complexity.ts']`, length 3 minus 1 equals depth 2 for the `src/generation/` directory.\n\n**Directory extraction**: For path `/home/pascal/wks/are/src/generation/complexity.ts`, extracts `/home/pascal/wks/are/src/generation`, `/home/pascal/wks/are/src`, `/home/pascal/wks/are`, stops before root `/home` when `parent === dir`.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 542,
      "cacheReadTokens": 17114,
      "cacheCreationTokens": 9577,
      "latencyMs": 22913,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/generation/complexity.ts",
          "sizeBytes": 1642
        }
      ]
    },
    {
      "timestamp": "2026-02-09T16:09:54.763Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/generation/types.ts\n\n```typescript\n/**\n * Types for the documentation generation pipeline\n */\n\n/**\n * Result of file analysis (populated by LLM via host)\n */\nexport interface AnalysisResult {\n  /** Generated summary text */\n  summary: string;\n  /** Extracted metadata */\n  metadata: SummaryMetadata;\n}\n\n/**\n * Metadata extracted during analysis\n */\nexport interface SummaryMetadata {\n  /** Primary purpose of the file */\n  purpose: string;\n  /** Only security/breaking issues */\n  criticalTodos?: string[];\n  /** Tightly coupled siblings */\n  relatedFiles?: string[];\n}\n\n/**\n * Options for summary generation\n */\nexport interface SummaryOptions {\n  /** Target length for generated summaries */\n  targetLength: 'short' | 'standard' | 'detailed';\n  /** Whether to include code snippets in output */\n  includeCodeSnippets: boolean;\n}\n\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**types.ts defines the core TypeScript interfaces for the documentation generation pipeline: file analysis results (`AnalysisResult`), extracted metadata schemas (`SummaryMetadata`), and generation options (`SummaryOptions`).**\n\n## Exported Interfaces\n\n**`AnalysisResult`** — Result structure returned after LLM-based file analysis, containing:\n- `summary: string` — Generated markdown summary text\n- `metadata: SummaryMetadata` — Extracted structured metadata\n\n**`SummaryMetadata`** — Metadata schema extracted during file analysis, with fields:\n- `purpose: string` — Primary purpose statement (mandatory)\n- `criticalTodos?: string[]` — Optional array of security/breaking issues only\n- `relatedFiles?: string[]` — Optional array of tightly-coupled sibling file paths\n\n**`SummaryOptions`** — Configuration object for summary generation behavior:\n- `targetLength: 'short' | 'standard' | 'detailed'` — Controls output verbosity\n- `includeCodeSnippets: boolean` — Toggles code snippet inclusion in generated summaries\n\n## Integration Points\n\nThese types flow through the three-phase pipeline defined in `src/generation/orchestrator.ts`:\n- Phase 1 (file analysis): AI backends in `src/ai/backends/` return `AnalysisResult` via `AIService.call()`\n- Phase 2 (directory aggregation): `SummaryMetadata` fields (`relatedFiles`, `criticalTodos`) aggregated into `AGENTS.md` via `src/generation/writers/agents-md.ts`\n- Phase 3 (root synthesis): `purpose` field extracted from `.sum` YAML frontmatter via `src/generation/collector.ts`\n\n`SummaryOptions` consumed by prompt builders in `src/generation/prompts/builder.ts` to adjust template verbosity and code snippet inclusion rules.\n\n## Design Constraints\n\n- `SummaryMetadata.criticalTodos` intentionally limited to security/breaking issues (not general TODOs) per density rules in project instructions\n- `SummaryMetadata.relatedFiles` paths must resolve to actual source files to avoid phantom path validation errors from `src/quality/phantom-paths/validator.ts`\n- `AnalysisResult` populated by LLM via subprocess output parsing in `src/ai/subprocess.ts` (expects JSON-serializable structure)",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 555,
      "cacheReadTokens": 17114,
      "cacheCreationTokens": 9317,
      "latencyMs": 21767,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/generation/types.ts",
          "sizeBytes": 797
        }
      ]
    },
    {
      "timestamp": "2026-02-09T16:09:22.984Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/cli/update.ts\n\n```typescript\n/**\n * CLI update command\n *\n * Updates documentation incrementally based on git changes since last run.\n * Uses the AI service for real file analysis (not placeholders) and processes\n * changed files concurrently via the CommandRunner orchestration engine.\n * Regenerates AGENTS.md for affected directories after analysis.\n */\nimport * as path from 'node:path';\nimport { readFile } from 'node:fs/promises';\nimport pc from 'picocolors';\nimport { loadConfig } from '../config/loader.js';\nimport { createLogger } from '../output/logger.js';\nimport {\n  createUpdateOrchestrator,\n  type UpdatePlan,\n} from '../update/index.js';\nimport { writeAgentsMd, GENERATED_MARKER } from '../generation/writers/agents-md.js';\nimport { buildDirectoryPrompt } from '../generation/prompts/index.js';\nimport {\n  AIService,\n  AIServiceError,\n  createBackendRegistry,\n  resolveBackend,\n  getInstallInstructions,\n} from '../ai/index.js';\nimport { CommandRunner, ProgressReporter, ProgressLog, createTraceWriter, cleanupOldTraces } from '../orchestration/index.js';\n\n/**\n * Options for the update command.\n */\nexport interface UpdateCommandOptions {\n  /** Include uncommitted changes (staged + working directory) */\n  uncommitted?: boolean;\n  /** Dry run - show plan without making changes */\n  dryRun?: boolean;\n  /** Number of concurrent AI calls */\n  concurrency?: number;\n  /** Stop on first file analysis failure */\n  failFast?: boolean;\n  /** Show AI prompts and backend details */\n  debug?: boolean;\n  /** Enable concurrency tracing to .agents-reverse-engineer/traces/ */\n  trace?: boolean;\n}\n\n/**\n * Format cleanup results for display.\n */\nfunction formatCleanup(plan: UpdatePlan): string[] {\n  const lines: string[] = [];\n\n  if (plan.cleanup.deletedSumFiles.length > 0) {\n    lines.push(pc.yellow('Cleanup (deleted .sum files):'));\n    for (const file of plan.cleanup.deletedSumFiles) {\n      lines.push(`  ${pc.red('-')} ${file}`);\n    }\n  }\n\n  if (plan.cleanup.deletedAgentsMd.length > 0) {\n    lines.push(pc.yellow('Cleanup (deleted AGENTS.md from empty dirs):'));\n    for (const file of plan.cleanup.deletedAgentsMd) {\n      lines.push(`  ${pc.red('-')} ${file}`);\n    }\n  }\n\n  return lines;\n}\n\n/**\n * Format the update plan for display.\n */\nfunction formatPlan(plan: UpdatePlan): string {\n  const lines: string[] = [];\n\n  // Header\n  lines.push('');\n  lines.push(pc.bold('=== Update Plan ==='));\n  lines.push('');\n\n  // Baseline info\n  if (plan.isFirstRun) {\n    lines.push(pc.yellow('First run detected. Use \"are generate\" for initial documentation.'));\n    lines.push('');\n  } else {\n    lines.push(`Current commit: ${pc.dim(plan.currentCommit.slice(0, 7))}`);\n    lines.push('');\n  }\n\n  // Summary\n  const analyzeCount = plan.filesToAnalyze.length;\n  const skipCount = plan.filesToSkip.length;\n  const cleanupCount = plan.cleanup.deletedSumFiles.length + plan.cleanup.deletedAgentsMd.length;\n\n  if (analyzeCount === 0 && skipCount === 0 && cleanupCount === 0) {\n    lines.push(pc.green('No changes detected since last run.'));\n    lines.push('');\n    return lines.join('\\n');\n  }\n\n  lines.push(`Files to analyze: ${pc.cyan(String(analyzeCount))}`);\n  lines.push(`Files unchanged: ${pc.dim(String(skipCount))}`);\n  if (cleanupCount > 0) {\n    lines.push(`Cleanup actions: ${pc.yellow(String(cleanupCount))}`);\n  }\n  lines.push('');\n\n  // File list with status markers\n  if (plan.filesToAnalyze.length > 0) {\n    lines.push(pc.cyan('Files to analyze:'));\n    for (const file of plan.filesToAnalyze) {\n      const status = file.status === 'added' ? pc.green('+') :\n                    file.status === 'renamed' ? pc.blue('R') :\n                    pc.yellow('M');\n      lines.push(`  ${status} ${file.path}`);\n      if (file.status === 'renamed' && file.oldPath) {\n        lines.push(`    ${pc.dim(`(was: ${file.oldPath})`)}`);\n      }\n    }\n    lines.push('');\n  }\n\n  if (plan.filesToSkip.length > 0) {\n    lines.push(pc.dim('Files unchanged (skipped):'));\n    for (const file of plan.filesToSkip) {\n      lines.push(`  ${pc.dim('=')} ${pc.dim(file)}`);\n    }\n    lines.push('');\n  }\n\n  // Cleanup\n  lines.push(...formatCleanup(plan));\n\n  // Affected directories\n  if (plan.affectedDirs.length > 0) {\n    lines.push('');\n    lines.push(pc.cyan('Directories for AGENTS.md regeneration:'));\n    for (const dir of plan.affectedDirs) {\n      lines.push(`  ${dir}`);\n    }\n  }\n\n  lines.push('');\n  return lines.join('\\n');\n}\n\n/**\n * Update command - incrementally updates documentation based on git changes.\n *\n * This command:\n * 1. Checks git repository status\n * 2. Detects files changed since last run (via content-hash comparison)\n * 3. Cleans up orphaned .sum files\n * 4. Resolves an AI CLI backend and creates the AI service\n * 5. Analyzes changed files concurrently via CommandRunner\n * 6. Regenerates AGENTS.md for affected directories\n * 7. Writes telemetry run log and prints run summary\n *\n * Exit codes: 0 = all success, 1 = partial failure, 2 = total failure / no CLI\n */\nexport async function updateCommand(\n  targetPath: string,\n  options: UpdateCommandOptions\n): Promise<void> {\n  const absolutePath = path.resolve(targetPath);\n  const logger = createLogger({ colors: true });\n\n  logger.info(`Checking for updates in: ${absolutePath}`);\n\n  // Create trace writer (moved earlier to use in config loading and orchestrator)\n  const tracer = createTraceWriter(absolutePath, options.trace ?? false);\n  if (options.trace && tracer.filePath) {\n    console.error(pc.dim(`[trace] Writing to ${tracer.filePath}`));\n  }\n\n  // Load configuration\n  const config = await loadConfig(absolutePath, {\n    tracer,\n    debug: options.debug,\n  });\n\n  // Create orchestrator\n  const orchestrator = createUpdateOrchestrator(config, absolutePath, {\n    tracer,\n    debug: options.debug,\n  });\n\n  try {\n    // Prepare update plan\n    const plan = await orchestrator.preparePlan({\n      includeUncommitted: options.uncommitted,\n      dryRun: options.dryRun,\n    });\n\n    // Display plan\n    console.log(formatPlan(plan));\n\n    // Handle first run\n    if (plan.isFirstRun) {\n      console.log(pc.yellow('Hint: Run \"are generate\" first to create initial documentation.'));\n      console.log(pc.yellow('Then run \"are update\" after making changes.'));\n      return;\n    }\n\n    // Handle no changes\n    if (plan.filesToAnalyze.length === 0 &&\n        plan.cleanup.deletedSumFiles.length === 0 &&\n        plan.cleanup.deletedAgentsMd.length === 0) {\n      console.log(pc.green('All files are up to date.'));\n      return;\n    }\n\n    if (options.dryRun) {\n      logger.info('Dry run complete. No files written.');\n      return;\n    }\n\n    // -------------------------------------------------------------------------\n    // Backend resolution (same pattern as generate command)\n    // -------------------------------------------------------------------------\n\n    const registry = createBackendRegistry();\n    let backend;\n    try {\n      backend = await resolveBackend(registry, config.ai.backend);\n    } catch (error) {\n      if (error instanceof AIServiceError && error.code === 'CLI_NOT_FOUND') {\n        console.error(pc.red('Error: No AI CLI found.\\n'));\n        console.error(getInstallInstructions(registry));\n        process.exit(2);\n      }\n      throw error;\n    }\n\n    // Debug: log backend info\n    if (options.debug) {\n      console.log(pc.dim(`[debug] Backend: ${backend.name}`));\n      console.log(pc.dim(`[debug] CLI command: ${backend.cliCommand}`));\n      console.log(pc.dim(`[debug] Model: ${config.ai.model}`));\n    }\n\n    // -------------------------------------------------------------------------\n    // AI service setup\n    // -------------------------------------------------------------------------\n\n    const aiService = new AIService(backend, {\n      timeoutMs: config.ai.timeoutMs,\n      maxRetries: config.ai.maxRetries,\n      model: config.ai.model,\n      telemetry: { keepRuns: config.ai.telemetry.keepRuns },\n    });\n\n    if (options.debug) {\n      aiService.setDebug(true);\n    }\n\n    // Determine concurrency\n    const concurrency = options.concurrency ?? config.ai.concurrency;\n\n    // Enable subprocess output logging alongside tracing\n    if (options.trace) {\n      const logDir = path.join(\n        absolutePath, '.agents-reverse-engineer', 'subprocess-logs',\n        new Date().toISOString().replace(/[:.]/g, '-'),\n      );\n      aiService.setSubprocessLogDir(logDir);\n      console.error(pc.dim(`[trace] Subprocess logs → ${logDir}`));\n    }\n\n    // Create progress log for tail -f monitoring\n    const progressLog = ProgressLog.create(absolutePath);\n    progressLog.write(`=== ARE Update (${new Date().toISOString()}) ===`);\n    progressLog.write(`Project: ${absolutePath}`);\n    progressLog.write(`Files to analyze: ${plan.filesToAnalyze.length} | Directories: ${plan.affectedDirs.length}`);\n    progressLog.write('');\n\n    // Create command runner\n    const runner = new CommandRunner(aiService, {\n      concurrency,\n      failFast: options.failFast,\n      debug: options.debug,\n      tracer,\n      progressLog,\n    });\n\n    // -------------------------------------------------------------------------\n    // Phase 1: File analysis via CommandRunner (concurrent AI calls)\n    // -------------------------------------------------------------------------\n\n    const summary = await runner.executeUpdate(\n      plan.filesToAnalyze,\n      absolutePath,\n      config,\n    );\n\n    // -------------------------------------------------------------------------\n    // Phase 2: AGENTS.md regeneration for affected directories\n    // -------------------------------------------------------------------------\n\n    if (plan.affectedDirs.length > 0) {\n      const knownDirs = new Set(plan.affectedDirs);\n      const phase2Start = Date.now();\n      let dirsCompleted = 0;\n      let dirsFailed = 0;\n\n      // Emit phase start\n      tracer.emit({\n        type: 'phase:start',\n        phase: 'update-phase-dir-regen',\n        taskCount: plan.affectedDirs.length,\n        concurrency: 1,\n      });\n\n      const dirReporter = new ProgressReporter(0, plan.affectedDirs.length, progressLog);\n      for (const dir of plan.affectedDirs) {\n        const taskStart = Date.now();\n        const taskLabel = dir || '.';\n\n        // Emit task:start\n        tracer.emit({\n          type: 'task:start',\n          taskLabel,\n          phase: 'update-phase-dir-regen',\n        });\n\n        const dirPath = dir === '.' ? absolutePath : path.join(absolutePath, dir);\n        dirReporter.onDirectoryStart(dir || '.');\n        try {\n          // Read existing generated AGENTS.md for incremental update context\n          let existingAgentsMd: string | undefined;\n          try {\n            const agentsContent = await readFile(path.join(dirPath, 'AGENTS.md'), 'utf-8');\n            if (agentsContent.includes(GENERATED_MARKER)) {\n              existingAgentsMd = agentsContent;\n            }\n          } catch {\n            // No existing AGENTS.md — will generate from scratch\n          }\n\n          const prompt = await buildDirectoryPrompt(dirPath, absolutePath, options.debug, knownDirs, undefined, existingAgentsMd);\n          const response = await aiService.call({\n            prompt: prompt.user,\n            systemPrompt: prompt.system,\n          });\n          await writeAgentsMd(dirPath, absolutePath, response.text);\n          const dirDurationMs = Date.now() - taskStart;\n          dirReporter.onDirectoryDone(\n            dir || '.',\n            dirDurationMs,\n            response.inputTokens,\n            response.outputTokens,\n            response.model,\n            response.cacheReadTokens,\n            response.cacheCreationTokens,\n          );\n          dirsCompleted++;\n\n          // Emit task:done (success)\n          tracer.emit({\n            type: 'task:done',\n            workerId: 0,\n            taskIndex: dirsCompleted - 1,\n            taskLabel,\n            durationMs: Date.now() - taskStart,\n            success: true,\n            activeTasks: 0,\n          });\n        } catch (error) {\n          dirsFailed++;\n          const errorMsg = error instanceof Error ? error.message : String(error);\n          console.log(`${pc.dim('[dir]')} ${pc.yellow('WARN')} ${dir || '.'}: ${errorMsg}`);\n\n          // Emit task:done (failure)\n          tracer.emit({\n            type: 'task:done',\n            workerId: 0,\n            taskIndex: dirsCompleted + dirsFailed - 1,\n            taskLabel,\n            durationMs: Date.now() - taskStart,\n            success: false,\n            error: errorMsg,\n            activeTasks: 0,\n          });\n        }\n      }\n\n      // Emit phase end\n      tracer.emit({\n        type: 'phase:end',\n        phase: 'update-phase-dir-regen',\n        durationMs: Date.now() - phase2Start,\n        tasksCompleted: dirsCompleted,\n        tasksFailed: dirsFailed,\n      });\n    }\n\n    // -------------------------------------------------------------------------\n    // Telemetry finalization\n    // -------------------------------------------------------------------------\n\n    await aiService.finalize(absolutePath);\n\n    // Finalize progress log, trace, and clean up old trace files\n    await progressLog.finalize();\n    await tracer.finalize();\n    if (options.trace) {\n      await cleanupOldTraces(absolutePath);\n    }\n\n    // -------------------------------------------------------------------------\n    // Record run state (no-op in frontmatter mode, kept for API compatibility)\n    // -------------------------------------------------------------------------\n\n    const filesSkipped = plan.filesToSkip.length;\n    await orchestrator.recordRun(\n      plan.currentCommit,\n      summary.filesProcessed,\n      filesSkipped\n    );\n\n    // -------------------------------------------------------------------------\n    // Exit code: 0 = all success, 1 = partial failure, 2 = total failure\n    // -------------------------------------------------------------------------\n\n    if (summary.filesProcessed === 0 && summary.filesFailed > 0) {\n      process.exit(2);\n    } else if (summary.filesFailed > 0) {\n      process.exit(1);\n    }\n    // Exit code 0 -- all files succeeded (or no files to process)\n\n  } finally {\n    orchestrator.close();\n  }\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**update.ts implements the incremental documentation update CLI command, detecting changed files via SHA-256 hash comparison, spawning concurrent AI analysis via CommandRunner, regenerating AGENTS.md for affected directories, and writing telemetry/trace logs with exit code 0/1/2 for success/partial/total failure.**\n\n## Exported Function\n\n`updateCommand(targetPath: string, options: UpdateCommandOptions): Promise<void>` — Main entry point for the `are update` CLI command. Orchestrates seven-step workflow: (1) loads config via `loadConfig()`, (2) creates `UpdateOrchestrator` via `createUpdateOrchestrator()`, (3) generates `UpdatePlan` via `orchestrator.preparePlan()` with `includeUncommitted`/`dryRun` flags, (4) resolves AI backend via `resolveBackend()` and instantiates `AIService`, (5) executes Phase 1 file analysis via `runner.executeUpdate()` with configured concurrency, (6) executes Phase 2 AGENTS.md regeneration for `plan.affectedDirs` sequentially (concurrency=1), (7) finalizes telemetry via `aiService.finalize()` and records run state via `orchestrator.recordRun()`. Exits with code 0 (all success), 1 (partial failure with `summary.filesFailed > 0`), or 2 (total failure or CLI not found).\n\n## Exported Interface\n\n`UpdateCommandOptions` — Configuration for `updateCommand()` with fields:\n- `uncommitted?: boolean` — Include staged + working directory changes in change detection\n- `dryRun?: boolean` — Display plan without writing files\n- `concurrency?: number` — Override config worker pool size\n- `failFast?: boolean` — Abort on first file analysis error\n- `debug?: boolean` — Enable verbose subprocess logging with heap/RSS metrics\n- `trace?: boolean` — Emit NDJSON trace events to `.agents-reverse-engineer/traces/`\n\n## AI Service Integration\n\nBackend resolution follows pattern: `createBackendRegistry()` → `resolveBackend(registry, config.ai.backend)` → catch `AIServiceError` with `code === 'CLI_NOT_FOUND'` → print `getInstallInstructions(registry)` → `process.exit(2)`. Creates `AIService` instance with timeout/retry/model from config, enables debug mode via `aiService.setDebug(true)` if `options.debug`, configures subprocess log directory via `aiService.setSubprocessLogDir(logDir)` if `options.trace` (path pattern: `.agents-reverse-engineer/subprocess-logs/<ISO-timestamp>/`).\n\n## Update Plan Display\n\n`formatPlan(plan: UpdatePlan): string` — Renders plan summary with sections: baseline commit hash (7-char abbreviated), file counts (`analyzeCount`, `skipCount`, `cleanupCount`), file list with status markers (`+` for added via `pc.green()`, `R` for renamed via `pc.blue()`, `M` for modified via `pc.yellow()`, `=` for unchanged via `pc.dim()`), cleanup actions via `formatCleanup()`, affected directories for AGENTS.md regeneration. Returns early with message \"No changes detected since last run.\" if all counts are zero.\n\n`formatCleanup(plan: UpdatePlan): string[]` — Formats `plan.cleanup.deletedSumFiles` and `plan.cleanup.deletedAgentsMd` arrays with `pc.red('-')` markers under `pc.yellow()` headers \"Cleanup (deleted .sum files):\" and \"Cleanup (deleted AGENTS.md from empty dirs):\".\n\n## Phase Execution\n\n**Phase 1 (File Analysis):** Invokes `runner.executeUpdate(plan.filesToAnalyze, absolutePath, config)` with `CommandRunner` configured with `concurrency` from options/config. Returns `summary` object with `filesProcessed`/`filesFailed` counts. Writes progress to `ProgressLog` instance created via `ProgressLog.create(absolutePath)` with initial metadata: `=== ARE Update (<ISO-timestamp>) ===`, `Project: <absolutePath>`, `Files to analyze: <count> | Directories: <count>`.\n\n**Phase 2 (Directory Regeneration):** Iterates `plan.affectedDirs` sequentially (no worker pool). For each directory: (1) reads existing `AGENTS.md` via `readFile()` and checks for `GENERATED_MARKER`, stores as `existingAgentsMd` context, (2) builds prompt via `buildDirectoryPrompt(dirPath, absolutePath, debug, knownDirs, undefined, existingAgentsMd)` where `knownDirs = new Set(plan.affectedDirs)`, (3) calls `aiService.call()` with prompt, (4) writes output via `writeAgentsMd()`, (5) emits trace events `task:start`/`task:done` with `phase: 'update-phase-dir-regen'`. Emits phase-level events `phase:start`/`phase:end` with `phase: 'update-phase-dir-regen'`, `taskCount`, `tasksCompleted`, `tasksFailed`. Updates `ProgressReporter` via `onDirectoryStart()`/`onDirectoryDone()` with token counts and duration. Catches errors and logs via `pc.yellow('WARN')` without aborting loop (continues to next directory).\n\n## Trace Integration\n\nCreates `TraceWriter` via `createTraceWriter(absolutePath, options.trace ?? false)` before config loading (used in `loadConfig()` and `createUpdateOrchestrator()` options). Passes `tracer` to `CommandRunner` constructor for worker/task event emission. Phase 2 manually emits events with `type: 'phase:start'/'phase:end'/'task:start'/'task:done'`, `phase: 'update-phase-dir-regen'`. Finalizes trace via `tracer.finalize()` and cleans old traces via `cleanupOldTraces(absolutePath)` if `options.trace`. Logs trace file path to stderr via `pc.dim('[trace] Writing to <path>')`.\n\n## Progress Logging\n\nCreates `ProgressLog` instance with header lines: `=== ARE Update (<ISO-timestamp>) ===`, `Project: <absolutePath>`, `Files to analyze: <count> | Directories: <count>`, empty line. Passes `progressLog` to `CommandRunner` and `ProgressReporter` for real-time streaming output. Finalizes via `progressLog.finalize()` after Phase 2 completion. Designed for tail -f monitoring pattern.\n\n## First Run Handling\n\nChecks `plan.isFirstRun` flag after `orchestrator.preparePlan()`. Prints `pc.yellow()` message: \"First run detected. Use 'are generate' for initial documentation.\" followed by \"Hint: Run 'are generate' first to create initial documentation. Then run 'are update' after making changes.\" Returns early without backend resolution or execution phases.\n\n## No-Change Handling\n\nAfter first run check, verifies `plan.filesToAnalyze.length === 0 && plan.cleanup.deletedSumFiles.length === 0 && plan.cleanup.deletedAgentsMd.length === 0`. Prints `pc.green('All files are up to date.')` and returns early.\n\n## Exit Code Strategy\n\nAfter telemetry finalization and run state recording:\n- `process.exit(2)` if `summary.filesProcessed === 0 && summary.filesFailed > 0` (total failure)\n- `process.exit(1)` if `summary.filesFailed > 0` (partial failure)\n- Implicit exit code 0 if all files succeeded or no files to process\n\nAlso uses `process.exit(2)` during backend resolution if `AIServiceError` with `code === 'CLI_NOT_FOUND'` caught.\n\n## Dry Run Mode\n\nIf `options.dryRun` is true, displays plan via `console.log(formatPlan(plan))` then logs \"Dry run complete. No files written.\" and returns early before backend resolution phase. Does not create AI service or execute Phase 1/Phase 2.\n\n## State Recording\n\nCalls `orchestrator.recordRun(plan.currentCommit, summary.filesProcessed, filesSkipped)` after telemetry finalization where `filesSkipped = plan.filesToSkip.length`. Comment indicates this is \"no-op in frontmatter mode, kept for API compatibility\" (legacy from state file approach before SHA-256 hash migration).\n\n## Dependencies\n\nImports: `path` (Node.js), `readFile` from `node:fs/promises`, `pc` (picocolors), `loadConfig` from `../config/loader.js`, `createLogger` from `../output/logger.js`, `createUpdateOrchestrator`/`UpdatePlan` from `../update/index.js`, `writeAgentsMd`/`GENERATED_MARKER` from `../generation/writers/agents-md.js`, `buildDirectoryPrompt` from `../generation/prompts/index.js`, `AIService`/`AIServiceError`/`createBackendRegistry`/`resolveBackend`/`getInstallInstructions` from `../ai/index.js`, `CommandRunner`/`ProgressReporter`/`ProgressLog`/`createTraceWriter`/`cleanupOldTraces` from `../orchestration/index.js`.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 2255,
      "cacheReadTokens": 17114,
      "cacheCreationTokens": 13159,
      "latencyMs": 57919,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/cli/update.ts",
          "sizeBytes": 14176
        }
      ]
    },
    {
      "timestamp": "2026-02-09T16:10:01.192Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/imports/types.ts\n\n```typescript\n/** A single import statement extracted from source. */\nexport interface ImportEntry {\n  /** The raw import specifier as written in source (e.g., '../ai/index.js') */\n  specifier: string;\n  /** Imported symbols (e.g., ['AIService', 'AIResponse']) */\n  symbols: string[];\n  /** Whether this is a type-only import */\n  typeOnly: boolean;\n}\n\n/** All imports from a single source file. */\nexport interface FileImports {\n  /** Relative file path (e.g., 'runner.ts') */\n  fileName: string;\n  /** External imports (from other modules, not same directory) */\n  externalImports: ImportEntry[];\n  /** Internal imports (from same directory) */\n  internalImports: ImportEntry[];\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**ImportEntry and FileImports type definitions model static import statement extraction results for dependency graph construction and import map generation in AGENTS.md prompts.**\n\n## Exported Types\n\n**ImportEntry** represents a single parsed import statement with three fields:\n- `specifier: string` — Raw import path as written in source (e.g., `'../ai/index.js'`, `'lodash'`)\n- `symbols: string[]` — Named/default imports extracted from statement (e.g., `['AIService', 'AIResponse']`)\n- `typeOnly: boolean` — Discriminates `import type` statements from runtime imports\n\n**FileImports** aggregates all imports discovered in a single source file with three fields:\n- `fileName: string` — Relative file path from discovery root (e.g., `'runner.ts'`, `'ai/service.ts'`)\n- `externalImports: ImportEntry[]` — Cross-directory or external module imports (used for inter-module dependency tracking)\n- `internalImports: ImportEntry[]` — Same-directory imports (used for intra-module coupling analysis)\n\n## Integration Points\n\nConsumed by `src/imports/extractor.ts` which implements regex-based import parsing producing `FileImports[]` arrays. `src/generation/prompts/builder.ts` calls `extractDirectoryImports()` to construct import maps embedded in directory aggregation prompts for `AGENTS.md` generation. The `externalImports` vs `internalImports` partitioning enables prompts to emphasize cross-module dependencies while suppressing local coupling noise.\n\n## Design Rationale\n\nThe `typeOnly` field supports TypeScript-specific workflows where type imports don't affect runtime dependency graphs. The `symbols: string[]` array enables precise tracking of which exports are actually consumed, supporting future dead code detection or import optimization features. The internal/external split allows `AGENTS.md` prompts to prioritize architectural boundaries over implementation details.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 444,
      "cacheReadTokens": 17114,
      "cacheCreationTokens": 9281,
      "latencyMs": 22090,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/imports/types.ts",
          "sizeBytes": 669
        }
      ]
    },
    {
      "timestamp": "2026-02-09T16:09:56.586Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/imports/index.ts\n\n```typescript\nexport { extractImports, extractDirectoryImports, formatImportMap } from './extractor.js';\nexport type { ImportEntry, FileImports } from './types.js';\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**Exports type definitions and functions for static import analysis via `extractor.js`, exposing `extractImports()`, `extractDirectoryImports()`, `formatImportMap()`, plus `ImportEntry` and `FileImports` types used throughout the generation pipeline for dependency graph construction and import map rendering in AI prompts.**\n\n## Public Interface\n\n```typescript\nfunction extractImports(filePath: string, content: string): FileImports\n```\nExtracts import statements from a single source file, returning `FileImports` object containing the file path and an array of `ImportEntry` objects.\n\n```typescript\nfunction extractDirectoryImports(directoryPath: string, discoveredFiles: string[]): Map<string, FileImports>\n```\nAggregates imports across all files within a directory by filtering `discoveredFiles` to those starting with `directoryPath + path.sep`, invoking `extractImports()` on each file, returning `Map<filePath, FileImports>` for directory-level dependency analysis.\n\n```typescript\nfunction formatImportMap(importMap: Map<string, FileImports>): string\n```\nSerializes `Map<string, FileImports>` into human-readable Markdown format for injection into Phase 2 directory aggregation prompts, enabling AI context on intra-directory dependencies.\n\n```typescript\ninterface ImportEntry {\n  source: string;          // Import specifier (relative path or package name)\n  isRelative: boolean;     // true if source starts with './' or '../'\n  importedNames: string[]; // Named imports/namespaces\n  lineNumber: number;      // Source line for debugging\n}\n```\nDiscriminates relative file imports from package imports via `isRelative` boolean, stores extracted identifiers in `importedNames[]` for API surface tracking.\n\n```typescript\ninterface FileImports {\n  file: string;            // Absolute or project-relative file path\n  imports: ImportEntry[];  // Extracted import statements\n}\n```\nContainer type mapping file paths to their import dependency lists, consumed by `extractDirectoryImports()` aggregation and `formatImportMap()` serialization.\n\n## Integration Points\n\n- **Generation Phase 2**: `src/generation/prompts/builder.ts` calls `extractDirectoryImports()` during `buildDirectoryPrompt()` to construct import context for `AGENTS.md` synthesis\n- **Orchestration**: `src/generation/orchestrator.ts` passes `discoveredFiles` list to enable directory-level filtering without redundant filesystem scans\n- **Prompt Templates**: `src/generation/prompts/templates.ts` consumes formatted import maps via `formatImportMap()` output injected into `directoryPromptTemplate`\n- **Quality Validation**: `src/quality/inconsistency/code-vs-doc.ts` may leverage `ImportEntry.importedNames[]` for cross-file symbol resolution (currently unused)\n\n## Module Organization\n\nBarrel export pattern delegates implementation to `extractor.ts` and type definitions to `types.ts`, maintaining single public interface at `src/imports/index.ts` for external consumption. All three functions and both types exported without namespace wrapping for direct destructured imports.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 729,
      "cacheReadTokens": 17114,
      "cacheCreationTokens": 9135,
      "latencyMs": 27869,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/imports/index.ts",
          "sizeBytes": 151
        }
      ]
    },
    {
      "timestamp": "2026-02-09T16:10:12.939Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/installer/banner.ts\n\n```typescript\n/**\n * ASCII banner and styled output for the installer\n *\n * Provides colored banner display, help text, and styled message helpers.\n * Uses picocolors for terminal coloring.\n */\n\nimport pc from 'picocolors';\nimport { getVersion } from '../version.js';\n\n/** Package version read from package.json */\nexport const VERSION = getVersion();\n\n/**\n * Display the ASCII banner at installer launch\n *\n * Shows big ASCII art \"ARE\" letters in green with version and tagline.\n */\nexport function displayBanner(): void {\n  const art = pc.green;\n  const dim = pc.dim;\n\n  console.log();\n  console.log(art('  █████╗ ██████╗ ███████╗'));\n  console.log(art(' ██╔══██╗██╔══██╗██╔════╝'));\n  console.log(art(' ███████║██████╔╝█████╗  '));\n  console.log(art(' ██╔══██║██╔══██╗██╔══╝  '));\n  console.log(art(' ██║  ██║██║  ██║███████╗'));\n  console.log(art(' ╚═╝  ╚═╝╚═╝  ╚═╝╚══════╝'));\n  console.log();\n  console.log(dim(` agents-reverse-engineer v${VERSION}`));\n  console.log(dim(' AI-friendly codebase documentation'));\n  console.log();\n}\n\n/**\n * Display help text showing usage, flags, and examples\n */\nexport function showHelp(): void {\n  console.log(pc.bold('Usage:') + ' npx agents-reverse-engineer [options]');\n  console.log();\n  console.log(pc.bold('Options:'));\n  console.log('  --runtime <runtime>  Select runtime: claude, opencode, gemini, or all');\n  console.log('  -g, --global         Install to global config (~/.claude, etc.)');\n  console.log('  -l, --local          Install to local project (./.claude, etc.)');\n  console.log('  -u, --uninstall      Remove installed files');\n  console.log('  --force              Overwrite existing files');\n  console.log('  -q, --quiet          Suppress banner and info messages');\n  console.log('  -h, --help           Show this help');\n  console.log();\n  console.log(pc.bold('Examples:'));\n  console.log('  npx agents-reverse-engineer');\n  console.log('    Interactive mode - prompts for runtime and location');\n  console.log();\n  console.log('  npx agents-reverse-engineer --runtime claude -g');\n  console.log('    Install Claude Code commands globally');\n  console.log();\n  console.log('  npx agents-reverse-engineer --runtime all -l');\n  console.log('    Install commands for all runtimes to local project');\n  console.log();\n  console.log('  npx agents-reverse-engineer --runtime claude -g -u');\n  console.log('    Uninstall global Claude Code commands');\n}\n\n/**\n * Display a success message with green checkmark prefix\n *\n * @param msg - Message to display\n */\nexport function showSuccess(msg: string): void {\n  console.log(pc.green('✓') + ' ' + msg);\n}\n\n/**\n * Display an error message with red X prefix\n *\n * @param msg - Message to display\n */\nexport function showError(msg: string): void {\n  console.log(pc.red('✗') + ' ' + msg);\n}\n\n/**\n * Display a warning message with yellow exclamation prefix\n *\n * @param msg - Message to display\n */\nexport function showWarning(msg: string): void {\n  console.log(pc.yellow('!') + ' ' + msg);\n}\n\n/**\n * Display an info message with cyan arrow prefix\n *\n * @param msg - Message to display\n */\nexport function showInfo(msg: string): void {\n  console.log(pc.cyan('>') + ' ' + msg);\n}\n\n/**\n * Display post-install next steps\n *\n * Shows what to do after installation with helpful links.\n *\n * @param runtime - Which runtime was installed\n * @param filesCreated - Number of files created\n */\nexport function showNextSteps(runtime: string, filesCreated: number): void {\n  console.log();\n  console.log(pc.bold('Installation complete!'));\n  console.log(pc.dim(`${filesCreated} files installed for ${runtime}`));\n  console.log();\n  console.log(pc.bold('Next steps:'));\n  console.log('  1. Run ' + pc.cyan('/are-help') + ' in your AI assistant to verify');\n  console.log('  2. Run ' + pc.cyan('/are-init') + ' to initialize a project');\n  console.log('  3. Run ' + pc.cyan('/are-discover') + ' to create the generation plan');\n  console.log('  4. Run ' + pc.cyan('/are-generate') + ' to generate documentation');\n  console.log('  5. Run ' + pc.cyan('/are-update') + ' to update documentation after changes');\n  console.log('  6. Run ' + pc.cyan('/are-specify') + ' to generate a specification document');\n  console.log('  7. Run ' + pc.cyan('/are-clean') + ' to remove all generated artifacts');\n  console.log();\n  console.log(pc.dim('Docs: https://github.com/GeoloeG-IsT/agents-reverse-engineer'));\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**banner.ts provides styled terminal output functions for the ARE installer with ASCII art display, help text formatting, and colorized message helpers using picocolors.**\n\n## Exported Constants\n\n- `VERSION: string` — Package version string obtained via `getVersion()` from `../version.js`\n\n## Exported Functions\n\n### Display Functions\n\n- `displayBanner(): void` — Renders ASCII art \"ARE\" logo in green via `pc.green()` with version number and tagline \"AI-friendly codebase documentation\" in dimmed text via `pc.dim()`\n\n- `showHelp(): void` — Prints usage instructions with `pc.bold()` section headers covering CLI options (`--runtime`, `-g`/`--global`, `-l`/`--local`, `-u`/`--uninstall`, `--force`, `-q`/`--quiet`, `-h`/`--help`), example invocations with runtime selectors, and installation mode combinations\n\n- `showNextSteps(runtime: string, filesCreated: number): void` — Displays post-installation instructions with bold \"Installation complete!\" header, file count summary via `pc.dim()`, numbered steps referencing ARE commands (`/are-help`, `/are-init`, `/are-discover`, `/are-generate`, `/are-update`, `/are-specify`, `/are-clean`) in cyan via `pc.cyan()`, and GitHub documentation URL\n\n### Message Helpers\n\n- `showSuccess(msg: string): void` — Prints message prefixed with green checkmark `pc.green('✓')`\n\n- `showError(msg: string): void` — Prints message prefixed with red X `pc.red('✗')`\n\n- `showWarning(msg: string): void` — Prints message prefixed with yellow exclamation `pc.yellow('!')`\n\n- `showInfo(msg: string): void` — Prints message prefixed with cyan arrow `pc.cyan('>')`\n\n## Dependencies\n\n- `picocolors` (imported as `pc`) — Terminal color library for ANSI escape sequences\n- `../version.js` — Provides `getVersion()` function returning package version from package.json\n\n## ASCII Art Format\n\nThe ASCII banner uses Unicode box-drawing characters (U+2550–U+255D range) to render block letters \"ARE\" across 6 lines with 25–27 characters per line, followed by version/tagline footer lines.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 550,
      "cacheReadTokens": 17114,
      "cacheCreationTokens": 10501,
      "latencyMs": 12467,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/installer/banner.ts",
          "sizeBytes": 4637
        }
      ]
    },
    {
      "timestamp": "2026-02-09T16:09:55.645Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/imports/extractor.ts\n\n```typescript\nimport { readFile } from 'node:fs/promises';\nimport * as path from 'node:path';\nimport type { ImportEntry, FileImports } from './types.js';\n\n/**\n * Regex matching TypeScript/JavaScript import statements.\n *\n * Captures:\n * - Group 1: 'type' keyword if present (type-only import)\n * - Group 2: named symbols (between braces)\n * - Group 3: namespace import (* as name)\n * - Group 4: default import (bare identifier)\n * - Group 5: module specifier (the string after 'from')\n */\nconst IMPORT_REGEX =\n  /^import\\s+(type\\s+)?(?:\\{([^}]*)\\}|(\\*\\s+as\\s+\\w+)|(\\w+))\\s+from\\s+['\"]([^'\"]+)['\"]/gm;\n\n/**\n * Extract import statements from source content.\n *\n * Only processes lines starting with 'import' to avoid matching\n * dynamic imports or imports inside comments/strings.\n */\nexport function extractImports(sourceContent: string): ImportEntry[] {\n  const entries: ImportEntry[] = [];\n  let match;\n  IMPORT_REGEX.lastIndex = 0;\n\n  while ((match = IMPORT_REGEX.exec(sourceContent)) !== null) {\n    const typeOnly = !!match[1];\n    const namedSymbols = match[2]; // { Foo, Bar }\n    const namespaceImport = match[3]; // * as name\n    const defaultImport = match[4]; // name\n    const specifier = match[5];\n\n    let symbols: string[] = [];\n    if (namedSymbols) {\n      symbols = namedSymbols\n        .split(',')\n        .map((s) => s.trim().replace(/\\s+as\\s+\\w+/, ''))\n        .filter(Boolean);\n    } else if (namespaceImport) {\n      symbols = [namespaceImport.replace('* as ', '').trim()];\n    } else if (defaultImport) {\n      symbols = [defaultImport];\n    }\n\n    entries.push({ specifier, symbols, typeOnly });\n  }\n\n  return entries;\n}\n\n/**\n * Extract and classify imports for all source files in a directory.\n *\n * Reads only the first 100 lines of each file (imports are at the top)\n * for performance. Classifies imports as internal (same directory via './')\n * or external (everything else).\n *\n * Skips node: built-ins and bare package specifiers (npm packages).\n */\nexport async function extractDirectoryImports(\n  dirPath: string,\n  fileNames: string[],\n): Promise<FileImports[]> {\n  const results: FileImports[] = [];\n\n  for (const fileName of fileNames) {\n    const filePath = path.join(dirPath, fileName);\n    try {\n      const content = await readFile(filePath, 'utf-8');\n      // Only read import region (first 100 lines) for performance\n      const importRegion = content.split('\\n').slice(0, 100).join('\\n');\n      const imports = extractImports(importRegion);\n\n      // Filter out bare specifiers (npm packages) and node: builtins\n      const relativeImports = imports.filter(\n        (i) => i.specifier.startsWith('.') || i.specifier.startsWith('..'),\n      );\n\n      const internal = relativeImports.filter((i) =>\n        i.specifier.startsWith('./'),\n      );\n      const external = relativeImports.filter((i) =>\n        i.specifier.startsWith('../'),\n      );\n\n      if (external.length > 0 || internal.length > 0) {\n        results.push({\n          fileName,\n          externalImports: external,\n          internalImports: internal,\n        });\n      }\n    } catch {\n      // Skip unreadable files\n    }\n  }\n\n  return results;\n}\n\n/**\n * Format import data as a structured text block for LLM prompts.\n *\n * Example output:\n * ```\n * runner.ts:\n *   ../ai/index.js → AIService\n *   ../generation/executor.js → ExecutionPlan, ExecutionTask\n *\n * pool.ts:\n *   ./trace.js → ITraceWriter (type)\n * ```\n */\nexport function formatImportMap(fileImports: FileImports[]): string {\n  const sections: string[] = [];\n\n  for (const fi of fileImports) {\n    const lines: string[] = [`${fi.fileName}:`];\n    for (const imp of fi.externalImports) {\n      const typeTag = imp.typeOnly ? ' (type)' : '';\n      lines.push(`  ${imp.specifier} → ${imp.symbols.join(', ')}${typeTag}`);\n    }\n    if (lines.length > 1) {\n      sections.push(lines.join('\\n'));\n    }\n  }\n\n  return sections.join('\\n\\n');\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**extractor.ts extracts and classifies TypeScript/JavaScript import statements from source files using regex parsing, producing structured import maps for LLM prompt inclusion.**\n\n## Exported Functions\n\n**extractImports(sourceContent: string): ImportEntry[]**\nParses source content using `IMPORT_REGEX` pattern `/^import\\s+(type\\s+)?(?:\\{([^}]*)\\}|(\\*\\s+as\\s+\\w+)|(\\w+))\\s+from\\s+['\"]([^'\"]+)['\"]/gm` to extract import statements. Captures five groups: (1) `type` keyword for type-only imports, (2) named symbols between braces, (3) namespace imports (`* as name`), (4) default imports, (5) module specifier. Returns `ImportEntry[]` with `specifier`, `symbols[]`, and `typeOnly` boolean. Handles aliased named imports via regex replacement `/\\s+as\\s+\\w+/` removal.\n\n**extractDirectoryImports(dirPath: string, fileNames: string[]): Promise<FileImports[]>**\nReads first 100 lines of each file in `fileNames` array (performance optimization for import-region scanning), calls `extractImports()` on content, filters out bare specifiers (npm packages like `'react'`) and `node:` built-ins. Classifies relative imports into `internal` (starts with `./`) and `external` (starts with `../`). Returns `FileImports[]` containing `fileName`, `externalImports[]`, and `internalImports[]`. Silently catches read errors to skip unreadable files.\n\n**formatImportMap(fileImports: FileImports[]): string**\nConverts `FileImports[]` into human-readable text block for LLM prompt injection. Format template:\n```\nrunner.ts:\n  ../ai/index.js → AIService\n  ../generation/executor.js → ExecutionPlan, ExecutionTask\n\npool.ts:\n  ./trace.js → ITraceWriter (type)\n```\nAppends `(type)` suffix for type-only imports. Filters out files with no external imports (excludes internal-only dependencies from output).\n\n## Import Regex Pattern\n\n`IMPORT_REGEX` constant matches ES module import syntax with five capture groups:\n- Group 1: `type` keyword (`import type { Foo }` patterns)\n- Group 2: Named symbols within braces (`{ Foo, Bar as Baz }`)\n- Group 3: Namespace imports (`* as name`)\n- Group 4: Default imports (bare identifier after `import`)\n- Group 5: Module specifier (quoted string after `from`)\n\nPattern anchored with `^` to match line start, uses `gm` flags for global multiline matching. Does not capture dynamic imports (`import('...')`) or side-effect imports (`import './styles.css'`).\n\n## Integration Points\n\nDepends on `node:fs/promises.readFile` for file I/O and `node:path` for path joining. Consumed by `src/generation/prompts/builder.ts` which calls `extractDirectoryImports()` during Phase 2 directory aggregation to populate import maps in `AGENTS.md` generation prompts. Verifies external import paths exist via constraints defined in prompt templates to prevent phantom path references.\n\n## Data Structures\n\nUses `ImportEntry` type from `./types.js` with shape `{ specifier: string, symbols: string[], typeOnly: boolean }`. `FileImports` type contains `{ fileName: string, externalImports: ImportEntry[], internalImports: ImportEntry[] }`. No internal state beyond regex lastIndex cursor during extraction loop.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 859,
      "cacheReadTokens": 17114,
      "cacheCreationTokens": 10356,
      "latencyMs": 30691,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/imports/extractor.ts",
          "sizeBytes": 3904
        }
      ]
    },
    {
      "timestamp": "2026-02-09T16:10:04.509Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/installer/paths.ts\n\n```typescript\n/**\n * Path resolution for installer module\n *\n * Provides cross-platform path resolution for AI coding assistant runtimes.\n * Uses os.homedir() for global paths and path.join() for cross-platform compatibility.\n */\n\nimport * as os from 'node:os';\nimport * as path from 'node:path';\nimport { stat } from 'node:fs/promises';\nimport type { Runtime, Location, RuntimePaths } from './types.js';\n\n/**\n * Get all supported runtime identifiers (excludes 'all' meta-runtime)\n *\n * @returns Array of concrete runtime identifiers\n */\nexport function getAllRuntimes(): Array<Exclude<Runtime, 'all'>> {\n  return ['claude', 'opencode', 'gemini'];\n}\n\n/**\n * Get path configuration for a specific runtime\n *\n * Returns global and local installation paths plus settings file location.\n * All paths are cross-platform using os.homedir() and path.join().\n *\n * Environment variable overrides (in priority order):\n * - Claude: CLAUDE_CONFIG_DIR\n * - OpenCode: OPENCODE_CONFIG_DIR > XDG_CONFIG_HOME/opencode\n * - Gemini: GEMINI_CONFIG_DIR\n *\n * @param runtime - Target runtime (claude, opencode, or gemini)\n * @returns Path configuration object with global, local, and settingsFile paths\n */\nexport function getRuntimePaths(runtime: Exclude<Runtime, 'all'>): RuntimePaths {\n  const home = os.homedir();\n\n  switch (runtime) {\n    case 'claude': {\n      // CLAUDE_CONFIG_DIR overrides default ~/.claude\n      const globalPath = process.env.CLAUDE_CONFIG_DIR || path.join(home, '.claude');\n      return {\n        global: globalPath,\n        local: '.claude',\n        settingsFile: path.join(globalPath, 'settings.json'),\n      };\n    }\n\n    case 'opencode': {\n      // OPENCODE_CONFIG_DIR > XDG_CONFIG_HOME/opencode > ~/.config/opencode\n      const xdgConfig = process.env.XDG_CONFIG_HOME || path.join(home, '.config');\n      const globalPath = process.env.OPENCODE_CONFIG_DIR || path.join(xdgConfig, 'opencode');\n      return {\n        global: globalPath,\n        local: '.opencode',\n        settingsFile: path.join(globalPath, 'settings.json'),\n      };\n    }\n\n    case 'gemini': {\n      // GEMINI_CONFIG_DIR overrides default ~/.gemini\n      const globalPath = process.env.GEMINI_CONFIG_DIR || path.join(home, '.gemini');\n      return {\n        global: globalPath,\n        local: '.gemini',\n        settingsFile: path.join(globalPath, 'settings.json'),\n      };\n    }\n  }\n}\n\n/**\n * Resolve full installation path for a runtime and location\n *\n * For global location, returns the absolute global path.\n * For local location, returns the local path joined with project root.\n *\n * @param runtime - Target runtime (claude, opencode, or gemini)\n * @param location - Installation location (global or local)\n * @param projectRoot - Project root directory for local installs (defaults to cwd)\n * @returns Resolved absolute path for installation\n */\nexport function resolveInstallPath(\n  runtime: Exclude<Runtime, 'all'>,\n  location: Location,\n  projectRoot?: string,\n): string {\n  const paths = getRuntimePaths(runtime);\n\n  if (location === 'global') {\n    return paths.global;\n  }\n\n  // Local installation - join with project root or cwd\n  const root = projectRoot || process.cwd();\n  return path.join(root, paths.local);\n}\n\n/**\n * Get the settings file path for a runtime\n *\n * Settings files are used for hook registration (Claude Code uses settings.json).\n *\n * @param runtime - Target runtime (claude, opencode, or gemini)\n * @returns Absolute path to the settings file\n */\nexport function getSettingsPath(runtime: Exclude<Runtime, 'all'>): string {\n  return getRuntimePaths(runtime).settingsFile;\n}\n\n/**\n * Check if a runtime is installed locally in a project.\n *\n * Checks for the presence of the local config directory (e.g., .claude, .opencode, .gemini).\n *\n * @param runtime - Target runtime (claude, opencode, or gemini)\n * @param projectRoot - Project root directory to check\n * @returns True if the runtime's local config directory exists\n */\nexport async function isRuntimeInstalledLocally(\n  runtime: Exclude<Runtime, 'all'>,\n  projectRoot: string\n): Promise<boolean> {\n  const paths = getRuntimePaths(runtime);\n  const localPath = path.join(projectRoot, paths.local);\n\n  try {\n    const stats = await stat(localPath);\n    return stats.isDirectory();\n  } catch {\n    return false;\n  }\n}\n\n/**\n * Check if a runtime is installed globally.\n *\n * Checks for the presence of the global config directory.\n *\n * @param runtime - Target runtime (claude, opencode, or gemini)\n * @returns True if the runtime's global config directory exists\n */\nexport async function isRuntimeInstalledGlobally(\n  runtime: Exclude<Runtime, 'all'>\n): Promise<boolean> {\n  const paths = getRuntimePaths(runtime);\n\n  try {\n    const stats = await stat(paths.global);\n    return stats.isDirectory();\n  } catch {\n    return false;\n  }\n}\n\n/**\n * Get list of runtimes installed locally in a project.\n *\n * @param projectRoot - Project root directory to check\n * @returns Array of runtime identifiers that are installed locally\n */\nexport async function getInstalledRuntimes(\n  projectRoot: string\n): Promise<Array<Exclude<Runtime, 'all'>>> {\n  const runtimes = getAllRuntimes();\n  const installed: Array<Exclude<Runtime, 'all'>> = [];\n\n  for (const runtime of runtimes) {\n    if (await isRuntimeInstalledLocally(runtime, projectRoot)) {\n      installed.push(runtime);\n    }\n  }\n\n  return installed;\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**paths.ts provides cross-platform path resolution for AI coding assistant runtime installations (Claude, OpenCode, Gemini) with environment variable overrides and installation detection.**\n\n## Exported Functions\n\n**getAllRuntimes(): Array<Exclude<Runtime, 'all'>>**\nReturns `['claude', 'opencode', 'gemini']` array of concrete runtime identifiers, excluding the `'all'` meta-runtime.\n\n**getRuntimePaths(runtime: Exclude<Runtime, 'all'>): RuntimePaths**\nReturns `RuntimePaths` object containing `global` (absolute path), `local` (relative path), and `settingsFile` (absolute path to settings.json) for specified runtime. Uses `os.homedir()` and `path.join()` for cross-platform compatibility.\n\n**resolveInstallPath(runtime: Exclude<Runtime, 'all'>, location: Location, projectRoot?: string): string**\nReturns absolute installation path. For `location === 'global'`, returns `getRuntimePaths(runtime).global`. For local installations, joins `paths.local` with `projectRoot || process.cwd()`.\n\n**getSettingsPath(runtime: Exclude<Runtime, 'all'>): string**\nReturns absolute path to runtime's settings.json file via `getRuntimePaths(runtime).settingsFile`.\n\n**isRuntimeInstalledLocally(runtime: Exclude<Runtime, 'all'>, projectRoot: string): Promise<boolean>**\nChecks if local config directory exists (e.g., `.claude`, `.opencode`, `.gemini`) via `stat()` and `stats.isDirectory()`. Returns false if stat throws.\n\n**isRuntimeInstalledGlobally(runtime: Exclude<Runtime, 'all'>): Promise<boolean>**\nChecks if global config directory exists via `stat()` on `paths.global`. Returns false if stat throws.\n\n**getInstalledRuntimes(projectRoot: string): Promise<Array<Exclude<Runtime, 'all'>>>**\nIterates through `getAllRuntimes()`, filters via `isRuntimeInstalledLocally()`, returns array of installed runtime identifiers.\n\n## Environment Variable Overrides\n\n**Claude:**\n- `CLAUDE_CONFIG_DIR` overrides default `~/.claude`\n\n**OpenCode:**\n- `OPENCODE_CONFIG_DIR` highest priority\n- Falls back to `XDG_CONFIG_HOME/opencode`\n- Falls back to `~/.config/opencode`\n\n**Gemini:**\n- `GEMINI_CONFIG_DIR` overrides default `~/.gemini`\n\n## Runtime Path Mappings\n\n| Runtime | Global Default | Local Path | Settings File |\n|---------|---------------|------------|---------------|\n| claude | `~/.claude` | `.claude` | `~/.claude/settings.json` |\n| opencode | `~/.config/opencode` | `.opencode` | `~/.config/opencode/settings.json` |\n| gemini | `~/.gemini` | `.gemini` | `~/.gemini/settings.json` |\n\n## Dependencies\n\nUses `node:os` for `homedir()`, `node:path` for `join()`, `node:fs/promises` for `stat()`. Imports `Runtime`, `Location`, `RuntimePaths` types from `./types.js`.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 775,
      "cacheReadTokens": 17114,
      "cacheCreationTokens": 10689,
      "latencyMs": 26730,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/installer/paths.ts",
          "sizeBytes": 5356
        }
      ]
    },
    {
      "timestamp": "2026-02-09T16:09:53.861Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/generation/orchestrator.ts\n\n```typescript\n/**\n * Generation orchestrator\n *\n * Coordinates the documentation generation workflow:\n * - Discovers and prepares files for analysis\n * - Creates file analysis tasks with prompts\n * - Creates directory tasks for LLM-generated descriptions\n */\n\nimport { readFile } from 'node:fs/promises';\nimport * as path from 'node:path';\nimport pc from 'picocolors';\nimport type { Config } from '../config/schema.js';\nimport type { DiscoveryResult } from '../types/index.js';\nimport { buildFilePrompt } from './prompts/index.js';\nimport { analyzeComplexity } from './complexity.js';\nimport type { ComplexityMetrics } from './complexity.js';\nimport type { ITraceWriter } from '../orchestration/trace.js';\n\n/**\n * A file prepared for analysis.\n */\nexport interface PreparedFile {\n  /** Absolute path to the file */\n  filePath: string;\n  /** Relative path from project root */\n  relativePath: string;\n  /** File content */\n  content: string;\n}\n\n/**\n * Analysis task for a file.\n */\nexport interface AnalysisTask {\n  /** Type of task */\n  type: 'file' | 'directory';\n  /** File or directory path */\n  filePath: string;\n  /** System prompt (set for file tasks; directory prompts built at execution time) */\n  systemPrompt?: string;\n  /** User prompt (set for file tasks; directory prompts built at execution time) */\n  userPrompt?: string;\n  /** Directory info for directory tasks */\n  directoryInfo?: {\n    /** Paths of .sum files in this directory */\n    sumFiles: string[];\n    /** Number of files analyzed */\n    fileCount: number;\n  };\n}\n\n/**\n * Result of the generation planning process.\n */\nexport interface GenerationPlan {\n  /** Files to be analyzed */\n  files: PreparedFile[];\n  /** Analysis tasks to execute */\n  tasks: AnalysisTask[];\n  /** Complexity metrics */\n  complexity: ComplexityMetrics;\n  /** Compact project directory listing for bird's-eye context */\n  projectStructure?: string;\n}\n\n/**\n * Orchestrates the documentation generation workflow.\n */\nexport class GenerationOrchestrator {\n  private config: Config;\n  private projectRoot: string;\n  private tracer?: ITraceWriter;\n  private debug: boolean;\n\n  constructor(\n    config: Config,\n    projectRoot: string,\n    options?: { tracer?: ITraceWriter; debug?: boolean }\n  ) {\n    this.config = config;\n    this.projectRoot = projectRoot;\n    this.tracer = options?.tracer;\n    this.debug = options?.debug ?? false;\n  }\n\n  /**\n   * Prepare files for analysis by reading content and detecting types.\n   */\n  async prepareFiles(discoveryResult: DiscoveryResult): Promise<PreparedFile[]> {\n    const prepared: PreparedFile[] = [];\n\n    for (let i = 0; i < discoveryResult.files.length; i++) {\n      const filePath = discoveryResult.files[i];\n      try {\n        const content = await readFile(filePath, 'utf-8');\n        const relativePath = path.relative(this.projectRoot, filePath);\n\n        prepared.push({\n          filePath,\n          relativePath,\n          content,\n        });\n      } catch {\n        // Skip files that can't be read (permission errors, etc.)\n        // Silently ignore - these files won't appear in the plan\n      }\n    }\n\n    return prepared;\n  }\n\n  /**\n   * Build a compact project structure listing from prepared files.\n   * Groups files by directory to give the AI bird's-eye context.\n   */\n  private buildProjectStructure(files: PreparedFile[]): string {\n    const byDir = new Map<string, string[]>();\n    for (const file of files) {\n      const dir = path.dirname(file.relativePath) || '.';\n      const group = byDir.get(dir) ?? [];\n      group.push(path.basename(file.relativePath));\n      byDir.set(dir, group);\n    }\n\n    const lines: string[] = [];\n    for (const [dir, dirFiles] of [...byDir.entries()].sort(([a], [b]) => a.localeCompare(b))) {\n      lines.push(`${dir}/`);\n      for (const f of dirFiles.sort()) {\n        lines.push(`  ${f}`);\n      }\n    }\n    return lines.join('\\n');\n  }\n\n  /**\n   * Create analysis tasks for all files.\n   */\n  createFileTasks(files: PreparedFile[], projectStructure?: string): AnalysisTask[] {\n    const tasks: AnalysisTask[] = [];\n\n    for (const file of files) {\n      const prompt = buildFilePrompt({\n        filePath: file.filePath,\n        content: file.content,\n        projectPlan: projectStructure,\n      }, this.debug);\n\n      tasks.push({\n        type: 'file',\n        filePath: file.relativePath,\n        systemPrompt: prompt.system,\n        userPrompt: prompt.user,\n      });\n    }\n\n    return tasks;\n  }\n\n  /**\n   * Create directory tasks for LLM-generated directory descriptions.\n   * These tasks run after all files in a directory are analyzed, allowing\n   * the LLM to synthesize a richer directory overview from the .sum files.\n   * Prompts are built at execution time by buildDirectoryPrompt().\n   */\n  createDirectoryTasks(files: PreparedFile[]): AnalysisTask[] {\n    const tasks: AnalysisTask[] = [];\n\n    // Group files by directory\n    const filesByDir = new Map<string, PreparedFile[]>();\n    for (const file of files) {\n      const dir = path.dirname(file.relativePath);\n      const dirFiles = filesByDir.get(dir) ?? [];\n      dirFiles.push(file);\n      filesByDir.set(dir, dirFiles);\n    }\n\n    // Create a directory task for each directory with analyzed files\n    for (const [dir, dirFiles] of Array.from(filesByDir.entries())) {\n      const sumFilePaths = dirFiles.map(f => `${f.relativePath}.sum`);\n\n      tasks.push({\n        type: 'directory',\n        filePath: dir || '.',\n        directoryInfo: {\n          sumFiles: sumFilePaths,\n          fileCount: dirFiles.length,\n        },\n      });\n    }\n\n    return tasks;\n  }\n\n  /**\n   * Create a complete generation plan.\n   */\n  async createPlan(discoveryResult: DiscoveryResult): Promise<GenerationPlan> {\n    const planStartTime = process.hrtime.bigint();\n\n    // Emit phase start\n    this.tracer?.emit({\n      type: 'phase:start',\n      phase: 'plan-creation',\n      taskCount: discoveryResult.files.length,\n      concurrency: 1,\n    });\n\n    if (this.debug) {\n      console.error(pc.dim('[debug] Preparing files: reading and detecting types...'));\n    }\n\n    const files = await this.prepareFiles(discoveryResult);\n\n    if (this.debug) {\n      console.error(pc.dim(`[debug] Analyzing complexity...`));\n    }\n\n    const complexity = analyzeComplexity(\n      files.map(f => f.filePath),\n      this.projectRoot\n    );\n\n    if (this.debug) {\n      console.error(pc.dim(`[debug] Complexity analysis: depth=${complexity.directoryDepth}`));\n    }\n\n    const projectStructure = this.buildProjectStructure(files);\n    const fileTasks = this.createFileTasks(files, projectStructure);\n\n    // Add directory tasks for LLM-generated directory descriptions\n    // These run after file analysis to synthesize richer directory overviews\n    const dirTasks = this.createDirectoryTasks(files);\n    const tasks = [...fileTasks, ...dirTasks];\n\n    if (this.debug) {\n      console.error(\n        pc.dim(\n          `[debug] Generation plan: ${files.length} files, ${tasks.length} tasks (${dirTasks.length} directories)`\n        )\n      );\n    }\n\n    // Release file content from PreparedFile objects to free memory.\n    // Content has already been embedded into task prompts by createFileTasks()\n    // and is no longer needed. The runner re-reads files from disk.\n    for (const file of files) {\n      (file as { content: string }).content = '';\n    }\n\n    const plan: GenerationPlan = {\n      files,\n      tasks,\n      complexity,\n      projectStructure,\n    };\n\n    // Emit plan created event\n    // +1 for root CLAUDE.md task added later by buildExecutionPlan()\n    this.tracer?.emit({\n      type: 'plan:created',\n      planType: 'generate',\n      fileCount: files.length,\n      taskCount: tasks.length + 1,\n    });\n\n    // Emit phase end\n    const planEndTime = process.hrtime.bigint();\n    const planDurationMs = Number(planEndTime - planStartTime) / 1_000_000;\n    this.tracer?.emit({\n      type: 'phase:end',\n      phase: 'plan-creation',\n      durationMs: planDurationMs,\n      tasksCompleted: 1,\n      tasksFailed: 0,\n    });\n\n    return plan;\n  }\n}\n\n/**\n * Create a generation orchestrator with default config.\n */\nexport function createOrchestrator(\n  config: Config,\n  projectRoot: string,\n  options?: { tracer?: ITraceWriter; debug?: boolean }\n): GenerationOrchestrator {\n  return new GenerationOrchestrator(config, projectRoot, options);\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**GenerationOrchestrator coordinates three-phase documentation generation: prepares files from DiscoveryResult, creates AnalysisTask lists for concurrent file analysis and post-file directory aggregation, computes ComplexityMetrics, and emits trace events for plan lifecycle.**\n\n## Exported Types\n\n**PreparedFile** interface contains `filePath: string` (absolute), `relativePath: string` (from project root), `content: string` (file text).\n\n**AnalysisTask** discriminated union via `type: 'file' | 'directory'`:\n- `filePath: string` (relative path for files, directory path for directories)\n- `systemPrompt?: string` and `userPrompt?: string` (populated for file tasks via buildFilePrompt, built at execution time for directory tasks via buildDirectoryPrompt)\n- `directoryInfo?: { sumFiles: string[], fileCount: number }` (populated only for directory tasks)\n\n**GenerationPlan** interface aggregates:\n- `files: PreparedFile[]` (all prepared files with content initially populated, then cleared)\n- `tasks: AnalysisTask[]` (file tasks followed by directory tasks)\n- `complexity: ComplexityMetrics` (computed via analyzeComplexity)\n- `projectStructure?: string` (compact directory tree listing)\n\n## Core Class\n\n**GenerationOrchestrator** constructor accepts `config: Config`, `projectRoot: string`, `options?: { tracer?: ITraceWriter, debug?: boolean }`. Stores as private fields `this.config`, `this.projectRoot`, `this.tracer`, `this.debug`.\n\n**prepareFiles(discoveryResult: DiscoveryResult): Promise<PreparedFile[]>** reads content via `readFile(filePath, 'utf-8')`, computes `relativePath` via `path.relative(this.projectRoot, filePath)`, silently skips unreadable files (permission errors), returns array of PreparedFile objects with populated content.\n\n**createFileTasks(files: PreparedFile[], projectStructure?: string): AnalysisTask[]** invokes `buildFilePrompt({ filePath, content, projectPlan: projectStructure }, this.debug)` for each file, returns AnalysisTask array with `type: 'file'`, `filePath: relativePath`, `systemPrompt: prompt.system`, `userPrompt: prompt.user`.\n\n**createDirectoryTasks(files: PreparedFile[]): AnalysisTask[]** groups files via `Map<string, PreparedFile[]>` keyed by `path.dirname(relativePath)`, generates one AnalysisTask per directory with `type: 'directory'`, `filePath: dir || '.'`, `directoryInfo: { sumFiles: string[], fileCount: number }` where sumFiles contains `${relativePath}.sum` paths. Prompts intentionally omitted (built at execution time).\n\n**createPlan(discoveryResult: DiscoveryResult): Promise<GenerationPlan>** orchestrates full planning:\n1. Emits `phase:start` trace event with `phase: 'plan-creation'`, `taskCount: discoveryResult.files.length`, `concurrency: 1`\n2. Calls `prepareFiles(discoveryResult)` to read file content\n3. Calls `analyzeComplexity(files.map(f => f.filePath), projectRoot)` from `./complexity.js`\n4. Calls `buildProjectStructure(files)` to generate compact directory tree string\n5. Calls `createFileTasks(files, projectStructure)` to generate file analysis tasks\n6. Calls `createDirectoryTasks(files)` to generate directory aggregation tasks\n7. Concatenates `[...fileTasks, ...dirTasks]` into single tasks array\n8. Clears `content` field from PreparedFile objects via `(file as { content: string }).content = ''` to free memory (content already embedded in prompts)\n9. Emits `plan:created` trace event with `planType: 'generate'`, `fileCount`, `taskCount: tasks.length + 1` (+1 for root CLAUDE.md task added by buildExecutionPlan)\n10. Emits `phase:end` trace event with `phase: 'plan-creation'`, `durationMs`, `tasksCompleted: 1`, `tasksFailed: 0`\n11. Returns GenerationPlan object\n\n## Factory Function\n\n**createOrchestrator(config: Config, projectRoot: string, options?: { tracer?: ITraceWriter, debug?: boolean }): GenerationOrchestrator** constructs and returns new GenerationOrchestrator instance with provided parameters.\n\n## Private Methods\n\n**buildProjectStructure(files: PreparedFile[]): string** groups files via `Map<string, string[]>` keyed by `path.dirname(relativePath) || '.'`, sorts directories alphabetically, formats output as:\n```\ndir1/\n  file1.ts\n  file2.ts\ndir2/\n  file3.ts\n```\nReturns multi-line string for LLM bird's-eye context.\n\n## Integration Points\n\nImports `buildFilePrompt` from `./prompts/index.js` (called per-file to generate system/user prompts with content and project structure).\n\nImports `analyzeComplexity` from `./complexity.js` (returns ComplexityMetrics with directoryDepth and other metrics).\n\nImports `Config` from `../config/schema.js` (Zod-validated configuration object).\n\nImports `DiscoveryResult` from `../types/index.js` (contains `files: string[]` array from file discovery phase).\n\nImports `ITraceWriter` from `../orchestration/trace.js` (optional tracer for NDJSON event emission with `.emit()` method).\n\n## Memory Management\n\nAfter createFileTasks embeds file content into task prompts, createPlan clears PreparedFile.content fields via cast `(file as { content: string }).content = ''` to release memory. Comment notes \"The runner re-reads files from disk\" during execution phase.\n\n## Task Ordering\n\nTasks array contains file analysis tasks first (concurrent execution), followed by directory aggregation tasks (post-file execution). Directory tasks intentionally omit systemPrompt/userPrompt fields — buildDirectoryPrompt constructs these at execution time after .sum files exist.\n\n## Trace Events\n\nEmits three trace event types via `this.tracer?.emit()`:\n- `phase:start` with `phase: 'plan-creation'`, `taskCount`, `concurrency: 1`\n- `plan:created` with `planType: 'generate'`, `fileCount`, `taskCount`\n- `phase:end` with `phase: 'plan-creation'`, `durationMs`, `tasksCompleted: 1`, `tasksFailed: 0`\n\nDuration computed via `process.hrtime.bigint()` delta converted to milliseconds via `Number(delta) / 1_000_000`.\n\n## Debug Logging\n\nWhen `this.debug === true`, emits stderr messages via `console.error(pc.dim())`:\n- `\"[debug] Preparing files: reading and detecting types...\"`\n- `\"[debug] Analyzing complexity...\"`\n- `\"[debug] Complexity analysis: depth=${complexity.directoryDepth}\"`\n- `\"[debug] Generation plan: ${files.length} files, ${tasks.length} tasks (${dirTasks.length} directories)\"`\n\nUses `picocolors` (imported as `pc`) for dimmed output.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 1775,
      "cacheReadTokens": 17114,
      "cacheCreationTokens": 11537,
      "latencyMs": 38344,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/generation/orchestrator.ts",
          "sizeBytes": 8340
        }
      ]
    },
    {
      "timestamp": "2026-02-09T16:10:09.712Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/integration/detect.ts\n\n```typescript\n/**\n * Environment detection for AI coding assistants\n *\n * Detects which AI coding assistant environments are present in a project\n * by checking for their configuration directories and files.\n */\n\nimport { existsSync } from 'node:fs';\nimport * as path from 'node:path';\nimport type { DetectedEnvironment, EnvironmentType } from './types.js';\n\n/**\n * Detect all AI coding assistant environments present in a project\n *\n * Checks for:\n * - Claude Code: .claude/ directory OR CLAUDE.md file\n * - OpenCode: .opencode/ directory\n * - Aider: .aider.conf.yml file OR .aider/ directory\n *\n * @param projectRoot - Root directory to check for environments\n * @returns Array of detected environments (may include multiple)\n */\nexport function detectEnvironments(projectRoot: string): DetectedEnvironment[] {\n  const environments: DetectedEnvironment[] = [];\n\n  // Check for Claude Code\n  const claudeDir = path.join(projectRoot, '.claude');\n  const claudeMd = path.join(projectRoot, 'CLAUDE.md');\n  if (existsSync(claudeDir) || existsSync(claudeMd)) {\n    environments.push({\n      type: 'claude',\n      configDir: '.claude',\n      detected: true,\n    });\n  }\n\n  // Check for OpenCode\n  const openCodeDir = path.join(projectRoot, '.opencode');\n  if (existsSync(openCodeDir)) {\n    environments.push({\n      type: 'opencode',\n      configDir: '.opencode',\n      detected: true,\n    });\n  }\n\n  // Check for Aider\n  const aiderConfig = path.join(projectRoot, '.aider.conf.yml');\n  const aiderDir = path.join(projectRoot, '.aider');\n  if (existsSync(aiderConfig) || existsSync(aiderDir)) {\n    environments.push({\n      type: 'aider',\n      configDir: '.aider',\n      detected: true,\n    });\n  }\n\n  return environments;\n}\n\n/**\n * Check if a specific AI coding assistant environment is present\n *\n * @param projectRoot - Root directory to check\n * @param type - Environment type to check for\n * @returns true if the environment is detected\n */\nexport function hasEnvironment(\n  projectRoot: string,\n  type: EnvironmentType\n): boolean {\n  const environments = detectEnvironments(projectRoot);\n  return environments.some((env) => env.type === type);\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**detectEnvironments() and hasEnvironment() identify AI coding assistant runtime environments (Claude Code, OpenCode, Aider) via filesystem marker detection.**\n\n## Exported Functions\n\n**detectEnvironments(projectRoot: string): DetectedEnvironment[]**\nScans `projectRoot` for AI assistant configuration markers and returns array of detected environments. Checks three patterns:\n- Claude Code: `.claude/` directory OR `CLAUDE.md` file existence\n- OpenCode: `.opencode/` directory existence\n- Aider: `.aider.conf.yml` file OR `.aider/` directory existence\n\nReturns `DetectedEnvironment[]` objects with fields: `type: EnvironmentType`, `configDir: string`, `detected: true`.\n\n**hasEnvironment(projectRoot: string, type: EnvironmentType): boolean**\nPredicate wrapper around `detectEnvironments()` that returns `true` if specified `EnvironmentType` exists in project. Uses `Array.some()` to match `env.type === type`.\n\n## Detection Logic\n\nUses `existsSync()` from `node:fs` and `path.join()` for absolute path construction. No async I/O—all checks synchronous.\n\n**Claude Code detection:**\n- Primary marker: `.claude/` directory\n- Fallback marker: `CLAUDE.md` root file\n- Returns `configDir: '.claude'` regardless of detection path\n\n**OpenCode detection:**\n- Single marker: `.opencode/` directory\n- Returns `configDir: '.opencode'`\n\n**Aider detection:**\n- Primary marker: `.aider.conf.yml` file\n- Fallback marker: `.aider/` directory\n- Returns `configDir: '.aider'` regardless of detection path\n\n## Type Dependencies\n\nImports `DetectedEnvironment` and `EnvironmentType` from `./types.js`. `DetectedEnvironment` contains discriminated union with `type` field matching `'claude' | 'opencode' | 'aider'`.\n\n## Integration Points\n\nCalled by `src/integration/generate.ts` to determine which platform-specific templates to generate (`CLAUDE.md`, `OPENCODE.md`, etc.). Used by installer modules (`src/installer/`) to validate target environment presence before hook installation.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 518,
      "cacheReadTokens": 17114,
      "cacheCreationTokens": 9726,
      "latencyMs": 23436,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/integration/detect.ts",
          "sizeBytes": 2136
        }
      ]
    },
    {
      "timestamp": "2026-02-09T16:09:51.824Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/generation/executor.ts\n\n```typescript\n/**\n * Plan executor for documentation generation\n *\n * Builds execution plans from generation plans:\n * - File tasks as individual analysis jobs\n * - Directory completion tracking\n * - Markdown plan output for dry-run display\n */\n\nimport * as path from 'node:path';\nimport type { GenerationPlan } from './orchestrator.js';\nimport { sumFileExists } from './writers/sum.js';\n\n/**\n * Execution task ready for AI processing.\n */\nexport interface ExecutionTask {\n  /** Unique task ID */\n  id: string;\n  /** Task type */\n  type: 'file' | 'directory' | 'root-doc';\n  /** File or directory path (relative) */\n  path: string;\n  /** Absolute path */\n  absolutePath: string;\n  /** System prompt for AI */\n  systemPrompt: string;\n  /** User prompt for AI */\n  userPrompt: string;\n  /** Dependencies (task IDs that must complete first) */\n  dependencies: string[];\n  /** Output path for generated content */\n  outputPath: string;\n  /** Metadata for tracking */\n  metadata: {\n    directoryFiles?: string[];\n    /** Directory depth (for post-order traversal) */\n    depth?: number;\n    /** Package root path (for supplementary docs) */\n    packageRoot?: string;\n  };\n}\n\n/**\n * Execution plan with dependency graph.\n */\nexport interface ExecutionPlan {\n  /** Project root */\n  projectRoot: string;\n  /** All tasks in execution order */\n  tasks: ExecutionTask[];\n  /** File tasks (can run in parallel) */\n  fileTasks: ExecutionTask[];\n  /** Directory tasks (depend on file tasks) */\n  directoryTasks: ExecutionTask[];\n  /** Root document tasks (depend on directories) */\n  rootTasks: ExecutionTask[];\n  /** Directory to file mapping */\n  directoryFileMap: Record<string, string[]>;\n  /** Compact project directory listing for directory prompt context */\n  projectStructure?: string;\n}\n\n/**\n * Calculate directory depth (number of path segments).\n * Root \".\" has depth 0, \"src\" has depth 1, \"src/cli\" has depth 2, etc.\n */\nfunction getDirectoryDepth(dir: string): number {\n  if (dir === '.') return 0;\n  return dir.split(path.sep).length;\n}\n\n/**\n * Build execution plan from generation plan.\n *\n * Directory tasks are sorted using post-order traversal (deepest directories first)\n * so child AGENTS.md files are generated before their parents.\n */\nexport function buildExecutionPlan(\n  plan: GenerationPlan,\n  projectRoot: string\n): ExecutionPlan {\n  const fileTasks: ExecutionTask[] = [];\n  const directoryTasks: ExecutionTask[] = [];\n  const rootTasks: ExecutionTask[] = [];\n  const directoryFileMap: Record<string, string[]> = {};\n\n  // Track files by directory\n  for (const file of plan.files) {\n    const dir = path.dirname(file.relativePath);\n    if (!directoryFileMap[dir]) {\n      directoryFileMap[dir] = [];\n    }\n    directoryFileMap[dir].push(file.relativePath);\n  }\n\n  // Create file tasks\n  for (const task of plan.tasks) {\n    if (task.type === 'file') {\n      const absolutePath = path.join(projectRoot, task.filePath);\n      fileTasks.push({\n        id: `file:${task.filePath}`,\n        type: 'file',\n        path: task.filePath,\n        absolutePath,\n        systemPrompt: task.systemPrompt!,\n        userPrompt: task.userPrompt!,\n        dependencies: [],\n        outputPath: `${absolutePath}.sum`,\n        metadata: {},\n      });\n    }\n  }\n\n  // Sort file tasks by directory depth (deepest first) for post-order traversal\n  fileTasks.sort((a, b) => {\n    const depthA = getDirectoryDepth(path.dirname(a.path));\n    const depthB = getDirectoryDepth(path.dirname(b.path));\n    return depthB - depthA;\n  });\n\n  // Create directory tasks in post-order (deepest first)\n  // Sort directories by depth descending so children are processed before parents\n  const sortedDirs = Object.entries(directoryFileMap).sort(\n    ([dirA], [dirB]) => getDirectoryDepth(dirB) - getDirectoryDepth(dirA)\n  );\n\n  for (const [dir, files] of sortedDirs) {\n    const dirAbsPath = path.join(projectRoot, dir);\n    const fileTaskIds = files.map(f => `file:${f}`);\n\n    directoryTasks.push({\n      id: `dir:${dir}`,\n      type: 'directory',\n      path: dir,\n      absolutePath: dirAbsPath,\n      systemPrompt: 'Built at execution time by buildDirectoryPrompt()',\n      userPrompt: `Directory \"${dir}\" — ${files.length} files. Prompt populated from .sum files at runtime.`,\n      dependencies: fileTaskIds,\n      outputPath: path.join(dirAbsPath, 'AGENTS.md'),\n      metadata: {\n        directoryFiles: files,\n        depth: getDirectoryDepth(dir),\n      },\n    });\n  }\n\n  // Create root document tasks\n  const allDirTaskIds = Object.keys(directoryFileMap).map(d => `dir:${d}`);\n\n  rootTasks.push({\n    id: 'root:CLAUDE.md',\n    type: 'root-doc',\n    path: 'CLAUDE.md',\n    absolutePath: path.join(projectRoot, 'CLAUDE.md'),\n    // Prompts are built at runtime by buildRootPrompt() in runner.ts Phase 3.\n    // These placeholders exist only for plan display and dependency tracking.\n    systemPrompt: 'Built at runtime by buildRootPrompt()',\n    userPrompt: 'Root document — prompt populated from AGENTS.md files at runtime.',\n    dependencies: allDirTaskIds,\n    outputPath: path.join(projectRoot, 'CLAUDE.md'),\n    metadata: {},\n  });\n\n  return {\n    projectRoot,\n    tasks: [...fileTasks, ...directoryTasks, ...rootTasks],\n    fileTasks,\n    directoryTasks,\n    rootTasks,\n    directoryFileMap,\n    projectStructure: plan.projectStructure,\n  };\n}\n\n/**\n * Check if all files in a directory have been analyzed (.sum files exist).\n */\nexport async function isDirectoryComplete(\n  dirPath: string,\n  expectedFiles: string[],\n  projectRoot: string\n): Promise<{ complete: boolean; missing: string[] }> {\n  const missing: string[] = [];\n\n  for (const relativePath of expectedFiles) {\n    const absolutePath = path.join(projectRoot, relativePath);\n    const exists = await sumFileExists(absolutePath);\n    if (!exists) {\n      missing.push(relativePath);\n    }\n  }\n\n  return {\n    complete: missing.length === 0,\n    missing,\n  };\n}\n\n/**\n * Get all directories that are ready for AGENTS.md generation.\n * A directory is ready when all its files have .sum files.\n */\nexport async function getReadyDirectories(\n  executionPlan: ExecutionPlan\n): Promise<string[]> {\n  const ready: string[] = [];\n\n  for (const [dir, files] of Object.entries(executionPlan.directoryFileMap)) {\n    const { complete } = await isDirectoryComplete(\n      dir,\n      files,\n      executionPlan.projectRoot\n    );\n    if (complete) {\n      ready.push(dir);\n    }\n  }\n\n  return ready;\n}\n\n/**\n * Format execution plan as markdown for GENERATION-PLAN.md.\n * Uses post-order traversal (deepest directories first).\n */\nexport function formatExecutionPlanAsMarkdown(plan: ExecutionPlan): string {\n  const lines: string[] = [];\n  const today = new Date().toISOString().split('T')[0];\n\n  // Header\n  lines.push('# Documentation Generation Plan');\n  lines.push('');\n  lines.push(`Generated: ${today}`);\n  lines.push(`Project: ${plan.projectRoot}`);\n  lines.push('');\n\n  // Summary\n  lines.push('## Summary');\n  lines.push('');\n  lines.push(`- **Total Tasks**: ${plan.tasks.length}`);\n  lines.push(`- **File Tasks**: ${plan.fileTasks.length}`);\n  lines.push(`- **Directory Tasks**: ${plan.directoryTasks.length}`);\n  lines.push(`- **Root Tasks**: ${plan.rootTasks.length}`);\n  lines.push('- **Traversal**: Post-order (children before parents)');\n  lines.push('');\n  lines.push('---');\n  lines.push('');\n\n  // Phase 1: File Analysis\n  lines.push('## Phase 1: File Analysis (Post-Order Traversal)');\n  lines.push('');\n\n  // Group files by directory, use directory task order (already post-order)\n  // Deduplicate paths\n  const filesByDir: Record<string, Set<string>> = {};\n  for (const task of plan.fileTasks) {\n    const dir = task.path.includes('/')\n      ? task.path.substring(0, task.path.lastIndexOf('/'))\n      : '.';\n    if (!filesByDir[dir]) filesByDir[dir] = new Set();\n    filesByDir[dir].add(task.path);\n  }\n\n  // Output files grouped by directory in post-order (using directoryTasks order)\n  for (const dirTask of plan.directoryTasks) {\n    const dir = dirTask.path;\n    const filesSet = filesByDir[dir];\n    if (filesSet && filesSet.size > 0) {\n      const files = Array.from(filesSet);\n      const depth = dirTask.metadata.depth ?? 0;\n      lines.push(`### Depth ${depth}: ${dir}/ (${files.length} files)`);\n      for (const file of files) {\n        lines.push(`- [ ] \\`${file}\\``);\n      }\n      lines.push('');\n    }\n  }\n\n  lines.push('---');\n  lines.push('');\n\n  // Phase 2: Directory AGENTS.md\n  lines.push(`## Phase 2: Directory AGENTS.md (Post-Order Traversal, ${plan.directoryTasks.length} directories)`);\n  lines.push('');\n\n  // Group by depth\n  const dirsByDepth: Record<number, string[]> = {};\n  for (const task of plan.directoryTasks) {\n    const depth = task.metadata.depth ?? 0;\n    if (!dirsByDepth[depth]) dirsByDepth[depth] = [];\n    dirsByDepth[depth].push(task.path);\n  }\n\n  // Output in depth order (descending)\n  const depths = Object.keys(dirsByDepth).map(Number).sort((a, b) => b - a);\n  for (const depth of depths) {\n    lines.push(`### Depth ${depth}`);\n    for (const dir of dirsByDepth[depth]) {\n      const suffix = dir === '.' ? ' (root)' : '';\n      lines.push(`- [ ] \\`${dir}/AGENTS.md\\`${suffix}`);\n    }\n    lines.push('');\n  }\n\n  lines.push('---');\n  lines.push('');\n\n  // Phase 3: Root Documents\n  lines.push('## Phase 3: Root Documents');\n  lines.push('');\n  lines.push('- [ ] `CLAUDE.md`');\n  lines.push('');\n\n  return lines.join('\\n');\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**Transforms GenerationPlan into ExecutionPlan with dependency-ordered tasks for the three-phase documentation pipeline, enforcing post-order directory traversal via depth-based sorting.**\n\n## Exported Types\n\n**ExecutionTask** — Individual work unit for AI processing with dependency tracking\n- `id: string` — Unique identifier (format: `file:<path>` | `dir:<path>` | `root:<filename>`)\n- `type: 'file' | 'directory' | 'root-doc'` — Task category determining pipeline phase\n- `path: string` — Relative path from project root\n- `absolutePath: string` — Resolved filesystem path\n- `systemPrompt: string` — AI system context (placeholder for dir/root, actual for file)\n- `userPrompt: string` — AI generation instructions (placeholder for dir/root, actual for file)\n- `dependencies: string[]` — Task IDs that must complete before this task can execute\n- `outputPath: string` — Target file for generated content (`.sum`, `AGENTS.md`, `CLAUDE.md`)\n- `metadata: { directoryFiles?: string[]; depth?: number; packageRoot?: string }` — Task-specific context\n\n**ExecutionPlan** — Complete dependency graph for pipeline execution\n- `projectRoot: string` — Absolute project base path\n- `tasks: ExecutionTask[]` — All tasks in dependency-satisfying order\n- `fileTasks: ExecutionTask[]` — Phase 1 tasks (parallel-eligible)\n- `directoryTasks: ExecutionTask[]` — Phase 2 tasks (post-order sorted by depth descending)\n- `rootTasks: ExecutionTask[]` — Phase 3 tasks (sequential, depends on all directories)\n- `directoryFileMap: Record<string, string[]>` — Directory to file paths mapping for completion tracking\n- `projectStructure?: string` — Compact file tree for directory prompt context\n\n## Core Functions\n\n**buildExecutionPlan(plan: GenerationPlan, projectRoot: string): ExecutionPlan** — Constructs execution plan with post-order directory traversal\n\nCreates three task categories:\n1. **fileTasks**: Maps `plan.tasks` with `type === 'file'` to ExecutionTask, generates IDs via `file:${task.filePath}`, outputs to `${absolutePath}.sum`, zero dependencies, sorts by directory depth descending via `getDirectoryDepth(path.dirname(a.path))` comparison\n2. **directoryTasks**: Builds from `directoryFileMap` entries sorted by `getDirectoryDepth(dirB) - getDirectoryDepth(dirA)`, assigns `dependencies` as `files.map(f => 'file:${f}')`, placeholder prompts (\"Built at execution time by buildDirectoryPrompt()\"), outputs to `${dirAbsPath}/AGENTS.md`, stores `metadata.depth` and `metadata.directoryFiles`\n3. **rootTasks**: Single task `root:CLAUDE.md` depending on all directory task IDs, placeholder prompts (\"Built at runtime by buildRootPrompt()\"), outputs to `${projectRoot}/CLAUDE.md`\n\n**isDirectoryComplete(dirPath: string, expectedFiles: string[], projectRoot: string): Promise<{ complete: boolean; missing: string[] }>** — Verifies all files in directory have `.sum` artifacts via `sumFileExists()`\n\nIterates `expectedFiles`, resolves each via `path.join(projectRoot, relativePath)`, checks `sumFileExists(absolutePath)`, accumulates missing paths, returns completion status and missing array.\n\n**getReadyDirectories(executionPlan: ExecutionPlan): Promise<string[]>** — Filters directories with all `.sum` files present\n\nIterates `executionPlan.directoryFileMap` entries, calls `isDirectoryComplete(dir, files, executionPlan.projectRoot)`, collects directories where `complete === true`.\n\n**formatExecutionPlanAsMarkdown(plan: ExecutionPlan): string** — Generates GENERATION-PLAN.md with post-order task breakdown\n\nProduces markdown with:\n- Header: `# Documentation Generation Plan` with ISO date from `new Date().toISOString().split('T')[0]` and `plan.projectRoot`\n- Summary: Task counts (`plan.tasks.length`, `plan.fileTasks.length`, `plan.directoryTasks.length`, `plan.rootTasks.length`) and traversal strategy (\"Post-order (children before parents)\")\n- Phase 1: Groups `plan.fileTasks` by directory via `task.path.lastIndexOf('/')`, outputs sections `### Depth ${depth}: ${dir}/ (${files.length} files)` using `plan.directoryTasks` order, checkbox items `- [ ] \\`${file}\\``\n- Phase 2: Groups `plan.directoryTasks` by `metadata.depth`, sorts depths descending, outputs `### Depth ${depth}` sections with `- [ ] \\`${dir}/AGENTS.md\\`` items\n- Phase 3: Hardcoded `- [ ] \\`CLAUDE.md\\`` entry\n\n## Dependency Relationships\n\n**Input:** GenerationPlan from `src/generation/orchestrator.ts` containing `tasks[]` with `type/filePath/systemPrompt/userPrompt`, `files[]` with `relativePath`\n\n**Output:** ExecutionPlan consumed by `src/orchestration/runner.ts` for phase execution\n\n**External dependencies:**\n- `sumFileExists()` from `src/generation/writers/sum.ts` — Checks `.sum` file existence for completion tracking\n- `path.join()`, `path.dirname()`, `path.sep` — Path manipulation for task ID generation and depth calculation\n\n## Post-Order Traversal Strategy\n\n**getDirectoryDepth(dir: string): number** — Calculates directory nesting level\n\nReturns 0 for root `\".\"`, otherwise counts path segments via `dir.split(path.sep).length`. Examples: `\"src\"` → 1, `\"src/cli\"` → 2, `\"src/generation/prompts\"` → 3.\n\nSorting pattern: `(a, b) => getDirectoryDepth(dirB) - getDirectoryDepth(dirA)` ensures deepest directories process first, guaranteeing child `AGENTS.md` exist before parent synthesis (Phase 2 requirement documented in `src/generation/orchestrator.ts`).\n\n## Task ID Format\n\n**File tasks:** `file:<relativePath>` (e.g., `file:src/cli/index.ts`)\n**Directory tasks:** `dir:<relativePath>` (e.g., `dir:src/cli`)\n**Root tasks:** `root:<filename>` (e.g., `root:CLAUDE.md`)\n\nDependency arrays reference these IDs exactly: `directoryTask.dependencies = files.map(f => 'file:${f}')`, `rootTask.dependencies = allDirTaskIds`.\n\n## Placeholder Prompts\n\nDirectory and root tasks store placeholder strings for `systemPrompt`/`userPrompt`:\n- Directory: `\"Built at execution time by buildDirectoryPrompt()\"` / `\"Directory \"${dir}\" — ${files.length} files. Prompt populated from .sum files at runtime.\"`\n- Root: `\"Built at runtime by buildRootPrompt()\"` / `\"Root document — prompt populated from AGENTS.md files at runtime.\"`\n\nActual prompts constructed in `src/orchestration/runner.ts` Phase 2/3 via `buildDirectoryPrompt()` and `buildRootPrompt()` consuming `.sum` and `AGENTS.md` files. Placeholders exist only for plan display via `formatExecutionPlanAsMarkdown()` and dependency graph structure.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 1817,
      "cacheReadTokens": 0,
      "cacheCreationTokens": 29152,
      "latencyMs": 41433,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/generation/executor.ts",
          "sizeBytes": 9421
        }
      ]
    },
    {
      "timestamp": "2026-02-09T16:10:07.353Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/installer/types.ts\n\n```typescript\n/**\n * Installer types for npx installation workflow\n *\n * Defines types for the interactive installer that copies command files and hooks\n * to runtime-specific directories (Claude Code, OpenCode, Gemini).\n */\n\n/**\n * Supported AI coding assistant runtimes for installation\n *\n * - 'claude': Claude Code (~/.claude or .claude)\n * - 'opencode': OpenCode (~/.config/opencode or .opencode)\n * - 'gemini': Gemini (~/.gemini or .gemini)\n * - 'all': Install to all supported runtimes\n */\nexport type Runtime = 'claude' | 'opencode' | 'gemini' | 'all';\n\n/**\n * Installation location target\n *\n * - 'global': User-level installation (~/.claude, ~/.config/opencode, etc.)\n * - 'local': Project-level installation (.claude, .opencode, etc.)\n */\nexport type Location = 'global' | 'local';\n\n/**\n * Arguments parsed from installer command line\n *\n * Supports both interactive mode (prompts) and non-interactive mode (flags).\n */\nexport interface InstallerArgs {\n  /** Target runtime (claude, opencode, gemini, or all) */\n  runtime?: Runtime;\n  /** Install to global/user location */\n  global: boolean;\n  /** Install to local/project location */\n  local: boolean;\n  /** Uninstall instead of install */\n  uninstall: boolean;\n  /** Force overwrite existing files */\n  force: boolean;\n  /** Show help and exit */\n  help: boolean;\n  /** Suppress banner and info messages */\n  quiet: boolean;\n}\n\n/**\n * Result of an installation operation for a single runtime/location\n */\nexport interface InstallerResult {\n  /** Whether the installation succeeded */\n  success: boolean;\n  /** Runtime that was installed */\n  runtime: Exclude<Runtime, 'all'>;\n  /** Location that was installed to */\n  location: Location;\n  /** Files that were successfully created */\n  filesCreated: string[];\n  /** Files that were skipped (already exist, no --force) */\n  filesSkipped: string[];\n  /** Error messages if any */\n  errors: string[];\n  /** Whether hook was registered in settings.json (Claude only) */\n  hookRegistered?: boolean;\n  /** Whether VERSION file was written */\n  versionWritten?: boolean;\n}\n\n/**\n * Path configuration for a specific runtime\n *\n * Contains resolved paths for global and local installation locations.\n */\nexport interface RuntimePaths {\n  /** Global installation path (e.g., ~/.claude) */\n  global: string;\n  /** Local installation path (e.g., .claude) */\n  local: string;\n  /** Path to settings.json for hook registration */\n  settingsFile: string;\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**types.ts defines TypeScript interfaces and union types for the npx installer workflow supporting multi-runtime (Claude/OpenCode/Gemini) installation with global/local targeting, operation result tracking, and path resolution.**\n\n## Exported Types\n\n**Runtime** — Union type: `'claude' | 'opencode' | 'gemini' | 'all'`\n- Maps to platform-specific directories: `~/.claude`, `~/.config/opencode`, `~/.gemini`\n- Special value `'all'` triggers installation to all three runtimes\n\n**Location** — Union type: `'global' | 'local'`\n- `'global'` targets user-level directories (home directory)\n- `'local'` targets project-level directories (working directory)\n\n**InstallerArgs** — Interface for CLI argument parsing\n```typescript\ninterface InstallerArgs {\n  runtime?: Runtime;      // Target platform (optional, prompts if missing)\n  global: boolean;        // Install to ~/.claude, ~/.config/opencode, ~/.gemini\n  local: boolean;         // Install to .claude, .opencode, .gemini\n  uninstall: boolean;     // Remove instead of install\n  force: boolean;         // Overwrite existing files\n  help: boolean;          // Show usage and exit\n  quiet: boolean;         // Suppress banner/info output\n}\n```\nSupports both interactive mode (prompts when `runtime` undefined) and non-interactive mode (all flags set).\n\n**InstallerResult** — Interface for per-runtime operation outcomes\n```typescript\ninterface InstallerResult {\n  success: boolean;                  // Overall operation status\n  runtime: Exclude<Runtime, 'all'>; // Resolved runtime (never 'all')\n  location: Location;                // Target location\n  filesCreated: string[];            // Successfully written file paths\n  filesSkipped: string[];            // Existing files (no --force)\n  errors: string[];                  // Failure messages\n  hookRegistered?: boolean;          // Claude-specific: SessionEnd hook in settings.json\n  versionWritten?: boolean;          // ~/.claude/ARE-VERSION existence\n}\n```\nUsed by installer operations (`src/installer/operations.ts`) for aggregated reporting.\n\n**RuntimePaths** — Interface for resolved directory paths\n```typescript\ninterface RuntimePaths {\n  global: string;        // ~/.claude, ~/.config/opencode, ~/.gemini\n  local: string;         // .claude, .opencode, .gemini\n  settingsFile: string;  // settings.json path for hook registration\n}\n```\nEnvironment overrides respected via `CLAUDE_CONFIG_DIR`, `OPENCODE_CONFIG_DIR`, `GEMINI_CONFIG_DIR` (see `src/installer/paths.ts`).\n\n## Integration Points\n\n- `src/installer/prompts.ts` constructs `InstallerArgs` from interactive prompts\n- `src/installer/paths.ts` returns `RuntimePaths` for each runtime with environment variable overrides\n- `src/installer/operations.ts` produces `InstallerResult[]` arrays from installation operations\n- `src/installer/index.ts` consumes these types for CLI entry point orchestration\n\n## Usage Patterns\n\n**Parallel installation to all runtimes:**\n```typescript\nconst results: InstallerResult[] = installToRuntime('all', 'global', args);\n// Expands to three operations: claude, opencode, gemini\n```\n\n**Location precedence validation:**\n```typescript\nif (args.global && args.local) {\n  throw new Error('Cannot specify both --global and --local');\n}\n```\n\n**Path resolution with environment override:**\n```typescript\nconst paths: RuntimePaths = getRuntimePaths('claude');\n// Uses CLAUDE_CONFIG_DIR if set, else ~/.claude\n```",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 886,
      "cacheReadTokens": 17114,
      "cacheCreationTokens": 9788,
      "latencyMs": 28561,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/installer/types.ts",
          "sizeBytes": 2442
        }
      ]
    },
    {
      "timestamp": "2026-02-09T16:10:02.035Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/installer/index.ts\n\n```typescript\n/**\n * Main installer entry point for agents-reverse-engineer\n *\n * Provides the runInstaller function for npx installation workflow.\n * Supports interactive prompts and non-interactive flags for CI/scripted installs.\n */\n\nimport type { InstallerArgs, InstallerResult, Runtime, Location } from './types.js';\nimport { getAllRuntimes, resolveInstallPath } from './paths.js';\nimport {\n  displayBanner,\n  showHelp,\n  showSuccess,\n  showError,\n  showWarning,\n  showInfo,\n  showNextSteps,\n} from './banner.js';\nimport { selectRuntime, selectLocation, confirmAction, isInteractive } from './prompts.js';\nimport { installFiles, verifyInstallation, formatInstallResult } from './operations.js';\nimport { uninstallFiles, deleteConfigFolder } from './uninstall.js';\n\n// Re-export types for external consumers\nexport type { InstallerArgs, InstallerResult, Runtime, Location, RuntimePaths } from './types.js';\nexport { getRuntimePaths, getAllRuntimes, resolveInstallPath, getSettingsPath } from './paths.js';\nexport { displayBanner, showHelp, showSuccess, showError, showWarning, showInfo, showNextSteps, VERSION } from './banner.js';\nexport { selectRuntime, selectLocation, confirmAction, isInteractive } from './prompts.js';\n\n/**\n * Parse command-line arguments for the installer\n *\n * Handles both short (-g, -l, -h) and long (--global, --local, --help) flags.\n * Uses pattern from cli/index.ts for consistency.\n *\n * @param args - Command line arguments (process.argv.slice(2))\n * @returns Parsed installer arguments\n */\nexport function parseInstallerArgs(args: string[]): InstallerArgs {\n  const flags = new Set<string>();\n  const values = new Map<string, string>();\n\n  for (let i = 0; i < args.length; i++) {\n    const arg = args[i];\n\n    if (arg === '--runtime' && i + 1 < args.length) {\n      // --runtime requires a value\n      values.set('runtime', args[++i]);\n    } else if (arg === '-g' || arg === '--global') {\n      flags.add('global');\n    } else if (arg === '-l' || arg === '--local') {\n      flags.add('local');\n    } else if (arg === '--force') {\n      flags.add('force');\n    } else if (arg === '-q' || arg === '--quiet') {\n      flags.add('quiet');\n    } else if (arg === '-h' || arg === '--help') {\n      flags.add('help');\n    }\n  }\n\n  // Validate runtime value if provided\n  const runtimeValue = values.get('runtime');\n  const validRuntimes: Runtime[] = ['claude', 'opencode', 'gemini', 'all'];\n  const runtime = runtimeValue && validRuntimes.includes(runtimeValue as Runtime)\n    ? (runtimeValue as Runtime)\n    : undefined;\n\n  return {\n    runtime,\n    global: flags.has('global'),\n    local: flags.has('local'),\n    uninstall: false, // Set by 'uninstall' command, not flags\n    force: flags.has('force'),\n    help: flags.has('help'),\n    quiet: flags.has('quiet'),\n  };\n}\n\n/**\n * Determine installation location from args or return undefined for prompt\n *\n * @param args - Parsed installer arguments\n * @returns Location if specified, undefined if needs prompt\n */\nfunction determineLocation(args: InstallerArgs): Location | undefined {\n  if (args.global && !args.local) {\n    return 'global';\n  }\n  if (args.local && !args.global) {\n    return 'local';\n  }\n  // Both or neither - needs interactive prompt\n  return undefined;\n}\n\n/**\n * Determine target runtimes from args\n *\n * @param runtime - Runtime from args (may be 'all' or specific runtime)\n * @returns Array of specific runtimes to install to\n */\nfunction determineRuntimes(runtime: Runtime | undefined): Array<Exclude<Runtime, 'all'>> {\n  if (!runtime) {\n    // No runtime specified - will need interactive prompt\n    return [];\n  }\n  if (runtime === 'all') {\n    return getAllRuntimes();\n  }\n  return [runtime];\n}\n\n/**\n * Run the installer workflow\n *\n * This is the main entry point for the installation process.\n * Supports both interactive mode (prompts) and non-interactive mode (flags).\n *\n * @param args - Parsed installer arguments\n * @returns Array of installation results (one per runtime/location combination)\n */\nexport async function runInstaller(args: InstallerArgs): Promise<InstallerResult[]> {\n  // Handle help flag\n  if (args.help) {\n    showHelp();\n    return [];\n  }\n\n  // Display banner unless quiet mode\n  if (!args.quiet) {\n    displayBanner();\n  }\n\n  // Determine location and runtimes from flags\n  let location = determineLocation(args);\n  const runtimeArg = args.runtime;\n\n  // Non-interactive mode: require all flags\n  if (!isInteractive()) {\n    if (!runtimeArg) {\n      showError('Missing --runtime flag (required in non-interactive mode)');\n      process.exit(1);\n    }\n    if (!location) {\n      showError('Missing -g/--global or -l/--local flag (required in non-interactive mode)');\n      process.exit(1);\n    }\n  }\n\n  // Interactive mode: prompt for missing values\n  const mode = args.uninstall ? 'uninstall' : 'install';\n  let selectedRuntime: Runtime | undefined = runtimeArg;\n  if (!selectedRuntime && isInteractive()) {\n    selectedRuntime = await selectRuntime(mode);\n  }\n\n  if (!location && isInteractive()) {\n    location = await selectLocation(mode);\n  }\n\n  // Safety check - should not reach here without values\n  if (!selectedRuntime || !location) {\n    showError('Unable to determine runtime and location');\n    process.exit(1);\n  }\n\n  // UNINSTALL MODE\n  if (args.uninstall) {\n    return runUninstall(selectedRuntime, location, args.quiet);\n  }\n\n  // INSTALL MODE\n  return runInstall(selectedRuntime, location, args.force, args.quiet);\n}\n\n/**\n * Run the installation workflow\n *\n * @param runtime - Target runtime or 'all'\n * @param location - Installation location\n * @param force - Overwrite existing files\n * @param quiet - Suppress output\n * @returns Array of installation results\n */\nasync function runInstall(\n  runtime: Runtime,\n  location: Location,\n  force: boolean,\n  quiet: boolean,\n): Promise<InstallerResult[]> {\n  // Install files\n  const results = installFiles(runtime, location, { force, dryRun: false });\n\n  // Verify installation\n  const allCreatedFiles = results.flatMap((r) => r.filesCreated);\n  const verification = verifyInstallation(allCreatedFiles);\n\n  if (!verification.success) {\n    showError('Installation verification failed - some files missing:');\n    for (const missing of verification.missing) {\n      showWarning(`  Missing: ${missing}`);\n    }\n  }\n\n  // Display results\n  if (!quiet) {\n    displayInstallResults(results);\n  }\n\n  return results;\n}\n\n/**\n * Run the uninstallation workflow\n *\n * @param runtime - Target runtime or 'all'\n * @param location - Installation location\n * @param quiet - Suppress output\n * @returns Array of uninstallation results\n */\nfunction runUninstall(\n  runtime: Runtime,\n  location: Location,\n  quiet: boolean,\n): InstallerResult[] {\n  const results = uninstallFiles(runtime, location, false);\n\n  // Delete .agents-reverse-engineer config folder (local only)\n  const configDeleted = deleteConfigFolder(location, false);\n\n  // Display results\n  if (!quiet) {\n    displayUninstallResults(results, configDeleted);\n  }\n\n  return results;\n}\n\n/**\n * Display installation results with styled output\n *\n * Shows checkmarks for successful actions, warnings for skipped files,\n * and next steps for using the installed commands.\n *\n * @param results - Array of installation results\n */\nfunction displayInstallResults(results: InstallerResult[]): void {\n  console.log();\n\n  let totalCreated = 0;\n  let totalSkipped = 0;\n  let hooksRegistered = 0;\n\n  for (const result of results) {\n    if (result.success) {\n      showSuccess(`Installed ${result.runtime} (${result.location})`);\n    } else {\n      showError(`Failed to install ${result.runtime} (${result.location})`);\n      for (const err of result.errors) {\n        showWarning(`  ${err}`);\n      }\n    }\n\n    totalCreated += result.filesCreated.length;\n    totalSkipped += result.filesSkipped.length;\n\n    if (result.hookRegistered) {\n      hooksRegistered++;\n    }\n  }\n\n  // Summary\n  console.log();\n  if (totalCreated > 0) {\n    showSuccess(`Created ${totalCreated} command files`);\n  }\n  if (hooksRegistered > 0) {\n    showSuccess(`Registered ${hooksRegistered} session hook(s)`);\n  }\n  if (totalSkipped > 0) {\n    showWarning(`Skipped ${totalSkipped} existing files (use --force to overwrite)`);\n  }\n\n  // Next steps\n  const primaryRuntime = results[0]?.runtime || 'claude';\n  showNextSteps(primaryRuntime, totalCreated);\n\n  // GitHub link\n  console.log();\n  showInfo('Docs: https://github.com/GeoloeG-IsT/agents-reverse-engineer');\n}\n\n/**\n * Display uninstallation results with styled output\n *\n * @param results - Array of uninstallation results\n * @param configDeleted - Whether the .agents-reverse-engineer folder was deleted\n */\nfunction displayUninstallResults(results: InstallerResult[], configDeleted: boolean = false): void {\n  console.log();\n\n  let totalDeleted = 0;\n  let hooksUnregistered = 0;\n\n  for (const result of results) {\n    // In uninstall context, filesCreated tracks deleted files\n    const deletedCount = result.filesCreated.length;\n    const notFoundCount = result.filesSkipped.length;\n\n    if (result.success) {\n      if (deletedCount > 0) {\n        showSuccess(`Uninstalled ${result.runtime} (${result.location}) - ${deletedCount} files removed`);\n      } else {\n        showInfo(`No ${result.runtime} files found in ${result.location}`);\n      }\n    } else {\n      showError(`Failed to uninstall ${result.runtime} (${result.location})`);\n      for (const err of result.errors) {\n        showWarning(`  ${err}`);\n      }\n    }\n\n    totalDeleted += deletedCount;\n\n    // hookRegistered is repurposed for uninstall to mean \"hook was unregistered\"\n    if (result.hookRegistered) {\n      hooksUnregistered++;\n    }\n  }\n\n  // Summary\n  console.log();\n  if (totalDeleted > 0) {\n    showSuccess(`Removed ${totalDeleted} files`);\n  }\n  if (hooksUnregistered > 0) {\n    showSuccess(`Unregistered ${hooksUnregistered} session hook(s)`);\n  }\n  if (configDeleted) {\n    showSuccess(`Removed .agents-reverse-engineer folder`);\n  }\n  if (totalDeleted === 0 && !configDeleted) {\n    showInfo('No files were removed');\n  }\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**Orchestrates npx installation workflow for IDE command/hook registration with runtime-specific path resolution, interactive prompts for missing parameters, and parallel support for Claude Code, OpenCode, and Gemini backends.**\n\n## Exported Functions\n\n**`runInstaller(args: InstallerArgs): Promise<InstallerResult[]>`** — Main entry point executing install/uninstall workflow with location determination, runtime selection, file operations via `installFiles()`/`uninstallFiles()`, verification via `verifyInstallation()`, and styled output formatting. Returns empty array if `args.help` is true. Exits with code 1 in non-interactive mode when `--runtime` or location flags missing.\n\n**`parseInstallerArgs(args: string[]): InstallerArgs`** — Parses CLI arguments supporting short flags (`-g`, `-l`, `-h`, `-q`) and long flags (`--global`, `--local`, `--force`, `--quiet`, `--help`), extracts `--runtime` value with validation against `validRuntimes: Runtime[] = ['claude', 'opencode', 'gemini', 'all']`, returns `InstallerArgs` with boolean flags and optional `runtime` field.\n\n## Re-exported Symbols\n\nExports types from `./types.js`: `InstallerArgs`, `InstallerResult`, `Runtime`, `Location`, `RuntimePaths`\n\nExports functions from module dependencies:\n- `./paths.js`: `getRuntimePaths`, `getAllRuntimes`, `resolveInstallPath`, `getSettingsPath`\n- `./banner.js`: `displayBanner`, `showHelp`, `showSuccess`, `showError`, `showWarning`, `showInfo`, `showNextSteps`, `VERSION`\n- `./prompts.js`: `selectRuntime`, `selectLocation`, `confirmAction`, `isInteractive`\n\n## Internal Workflow Functions\n\n**`determineLocation(args: InstallerArgs): Location | undefined`** — Returns `'global'` if `args.global && !args.local`, returns `'local'` if `args.local && !args.global`, returns `undefined` (triggers interactive prompt) if both or neither flags set.\n\n**`determineRuntimes(runtime: Runtime | undefined): Array<Exclude<Runtime, 'all'>>`** — Returns empty array if `runtime` is undefined (triggers prompt), calls `getAllRuntimes()` if `runtime === 'all'`, otherwise returns single-element array with specific runtime.\n\n**`runInstall(runtime: Runtime, location: Location, force: boolean, quiet: boolean): Promise<InstallerResult[]>`** — Executes installation by calling `installFiles(runtime, location, { force, dryRun: false })`, flattens `results.flatMap((r) => r.filesCreated)` for verification input, calls `verifyInstallation(allCreatedFiles)`, logs errors for missing files via `verification.missing`, invokes `displayInstallResults(results)` unless `quiet` is true.\n\n**`runUninstall(runtime: Runtime, location: Location, quiet: boolean): InstallerResult[]`** — Executes uninstallation by calling `uninstallFiles(runtime, location, false)`, calls `deleteConfigFolder(location, false)` returning `configDeleted` boolean, invokes `displayUninstallResults(results, configDeleted)` unless `quiet` is true.\n\n## Display Functions\n\n**`displayInstallResults(results: InstallerResult[]): void`** — Iterates over results accumulating `totalCreated`, `totalSkipped`, `hooksRegistered` counters, calls `showSuccess()`/`showError()` per result, displays summary with `showSuccess()` for created files and registered hooks, calls `showWarning()` for skipped files with `--force` hint, invokes `showNextSteps(primaryRuntime, totalCreated)` where `primaryRuntime = results[0]?.runtime || 'claude'`, outputs GitHub docs link via `showInfo('Docs: https://github.com/GeoloeG-IsT/agents-reverse-engineer')`.\n\n**`displayUninstallResults(results: InstallerResult[], configDeleted: boolean = false): void`** — Iterates results tracking `totalDeleted` (from `result.filesCreated.length` repurposed for deleted files), `hooksUnregistered` (from `result.hookRegistered` repurposed for unregister count), calls `showSuccess()` for non-zero deletions, `showInfo()` when no files found, displays summary with removal counts and config folder deletion status.\n\n## Non-Interactive Mode Constraints\n\nRequires `--runtime` flag with validated value (`'claude'` | `'opencode'` | `'gemini'` | `'all'`), requires `-g`/`--global` or `-l`/`--local` flag, calls `showError()` and `process.exit(1)` if either missing when `!isInteractive()`, skips prompts entirely in non-interactive context.\n\n## Integration Points\n\nDepends on `./operations.js` for `installFiles()`, `verifyInstallation()`, `formatInstallResult()`, depends on `./uninstall.js` for `uninstallFiles()`, `deleteConfigFolder()`, depends on `./prompts.js` for `selectRuntime()`, `selectLocation()`, `isInteractive()` interactive mode detection, depends on `./banner.js` for all styled terminal output functions, depends on `./paths.js` for `getAllRuntimes()` and `resolveInstallPath()`.\n\n## Runtime Type Definition\n\nType `Runtime = 'claude' | 'opencode' | 'gemini' | 'all'` validated in `parseInstallerArgs()` via `validRuntimes` array, expanded to specific runtime array via `determineRuntimes()` when value is `'all'`, passed to `installFiles()`/`uninstallFiles()` which handle multi-runtime fan-out internally.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 1403,
      "cacheReadTokens": 17114,
      "cacheCreationTokens": 12123,
      "latencyMs": 37051,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/installer/index.ts",
          "sizeBytes": 10066
        }
      ]
    },
    {
      "timestamp": "2026-02-09T16:10:06.690Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/installer/prompts.ts\n\n```typescript\n/**\n * Interactive prompts module for the installer\n *\n * Provides arrow key selection in TTY mode with numbered fallback for CI/non-interactive.\n * Uses Node.js readline module with raw mode for keypress handling.\n *\n * CRITICAL: Raw mode is always cleaned up via try/finally and process exit handlers.\n */\n\nimport * as readline from 'node:readline';\nimport pc from 'picocolors';\nimport type { Runtime, Location } from './types.js';\n\n/**\n * Check if stdin is a TTY (interactive terminal)\n *\n * @returns true if running in interactive terminal, false for CI/piped input\n */\nexport function isInteractive(): boolean {\n  return process.stdin.isTTY === true;\n}\n\n/**\n * Option type for selection prompts\n */\ninterface SelectOption<T> {\n  label: string;\n  value: T;\n}\n\n/**\n * Raw mode state tracker for cleanup\n */\nlet rawModeActive = false;\n\n/**\n * Cleanup function to restore terminal state\n */\nfunction cleanupRawMode(): void {\n  if (rawModeActive && process.stdin.isTTY) {\n    try {\n      process.stdin.setRawMode(false);\n      process.stdin.pause();\n    } catch {\n      // Ignore errors during cleanup\n    }\n    rawModeActive = false;\n  }\n}\n\n// Register global cleanup handlers\nprocess.on('exit', cleanupRawMode);\nprocess.on('SIGINT', () => {\n  cleanupRawMode();\n  process.exit(0);\n});\n\n/**\n * Generic option selector that uses arrow keys in TTY, numbered in non-TTY\n *\n * @param prompt - Question to display\n * @param options - Array of options with labels and values\n * @returns Selected value\n */\nexport async function selectOption<T>(\n  prompt: string,\n  options: SelectOption<T>[],\n): Promise<T> {\n  if (isInteractive()) {\n    return arrowKeySelect(prompt, options);\n  }\n  return numberedSelect(prompt, options);\n}\n\n/**\n * Arrow key selection for interactive terminals\n *\n * Uses raw mode to capture keypresses for up/down/enter navigation.\n * Always cleans up raw mode even on error or Ctrl+C.\n *\n * @param prompt - Question to display\n * @param options - Array of options with labels and values\n * @returns Selected value\n */\nasync function arrowKeySelect<T>(\n  prompt: string,\n  options: SelectOption<T>[],\n): Promise<T> {\n  return new Promise((resolve) => {\n    let selectedIndex = 0;\n\n    // Render the current selection state\n    const render = (clear: boolean = false): void => {\n      // Move cursor up and clear lines if re-rendering\n      if (clear) {\n        process.stdout.write(`\\x1b[${options.length + 1}A`);\n        for (let i = 0; i <= options.length; i++) {\n          process.stdout.write('\\x1b[2K\\x1b[1B');\n        }\n        process.stdout.write(`\\x1b[${options.length + 1}A`);\n      }\n\n      console.log(pc.bold(prompt));\n      options.forEach((opt, idx) => {\n        const prefix = idx === selectedIndex ? pc.cyan('> ') : '  ';\n        const label = idx === selectedIndex ? pc.cyan(opt.label) : opt.label;\n        console.log(prefix + label);\n      });\n    };\n\n    // Handle keypress events\n    const handleKeypress = (\n      _str: string | undefined,\n      key: { name?: string; ctrl?: boolean },\n    ): void => {\n      if (key.ctrl && key.name === 'c') {\n        cleanupRawMode();\n        process.exit(0);\n      }\n\n      switch (key.name) {\n        case 'up':\n          selectedIndex = Math.max(0, selectedIndex - 1);\n          render(true);\n          break;\n        case 'down':\n          selectedIndex = Math.min(options.length - 1, selectedIndex + 1);\n          render(true);\n          break;\n        case 'return':\n          // Cleanup and resolve\n          process.stdin.off('keypress', handleKeypress);\n          cleanupRawMode();\n          console.log();\n          resolve(options[selectedIndex].value);\n          break;\n      }\n    };\n\n    try {\n      // Setup raw mode for keypress handling\n      readline.emitKeypressEvents(process.stdin);\n      if (process.stdin.isTTY) {\n        process.stdin.setRawMode(true);\n        rawModeActive = true;\n      }\n      process.stdin.resume();\n      process.stdin.on('keypress', handleKeypress);\n\n      // Initial render\n      render(false);\n    } catch (err) {\n      cleanupRawMode();\n      throw err;\n    }\n  });\n}\n\n/**\n * Numbered selection for non-interactive environments\n *\n * Prints numbered options and reads a number from stdin.\n * Used in CI environments or when stdin is piped.\n *\n * @param prompt - Question to display\n * @param options - Array of options with labels and values\n * @returns Selected value\n */\nasync function numberedSelect<T>(\n  prompt: string,\n  options: SelectOption<T>[],\n): Promise<T> {\n  return new Promise((resolve, reject) => {\n    console.log(pc.bold(prompt));\n    options.forEach((opt, idx) => {\n      console.log(`  ${idx + 1}. ${opt.label}`);\n    });\n\n    const rl = readline.createInterface({\n      input: process.stdin,\n      output: process.stdout,\n    });\n\n    rl.question('Enter number: ', (answer) => {\n      rl.close();\n\n      const num = parseInt(answer, 10);\n      if (isNaN(num) || num < 1 || num > options.length) {\n        reject(new Error(`Invalid selection: ${answer}. Expected 1-${options.length}`));\n        return;\n      }\n\n      resolve(options[num - 1].value);\n    });\n  });\n}\n\n/**\n * Prompt user to select a runtime\n *\n * @param mode - 'install' or 'uninstall' to customize prompt text\n * @returns Selected runtime value\n */\nexport async function selectRuntime(mode: 'install' | 'uninstall' = 'install'): Promise<Runtime> {\n  const prompt = mode === 'uninstall' ? 'Select runtime to uninstall:' : 'Select runtime to install:';\n  return selectOption<Runtime>(prompt, [\n    { label: 'Claude Code', value: 'claude' },\n    { label: 'OpenCode', value: 'opencode' },\n    { label: 'Gemini CLI', value: 'gemini' },\n    { label: 'All runtimes', value: 'all' },\n  ]);\n}\n\n/**\n * Prompt user to select installation location\n *\n * @param mode - 'install' or 'uninstall' to customize prompt text\n * @returns Selected location value\n */\nexport async function selectLocation(mode: 'install' | 'uninstall' = 'install'): Promise<Location> {\n  const prompt = mode === 'uninstall' ? 'Select uninstallation location:' : 'Select installation location:';\n  return selectOption<Location>(prompt, [\n    { label: 'Global (~/.claude, ~/.config/opencode, etc.)', value: 'global' },\n    { label: 'Local (./.claude, ./.opencode, etc.)', value: 'local' },\n  ]);\n}\n\n/**\n * Prompt user to confirm an action\n *\n * @param message - Confirmation message to display\n * @returns true if confirmed, false if declined\n */\nexport async function confirmAction(message: string): Promise<boolean> {\n  return selectOption<boolean>(message, [\n    { label: 'Yes', value: true },\n    { label: 'No', value: false },\n  ]);\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**Interactive terminal prompt system providing arrow-key selection in TTY mode with numbered fallback for non-interactive environments, enforcing raw mode cleanup via try/finally blocks and process exit handlers to prevent terminal state corruption.**\n\n## Exported Functions\n\n### isInteractive\n\n```typescript\nfunction isInteractive(): boolean\n```\n\nReturns `true` if `process.stdin.isTTY === true`, indicating an interactive terminal capable of arrow-key navigation. Returns `false` for CI environments or piped stdin.\n\n### selectOption\n\n```typescript\nasync function selectOption<T>(\n  prompt: string,\n  options: SelectOption<T>[],\n): Promise<T>\n```\n\nGeneric option selector dispatching to `arrowKeySelect()` in TTY mode or `numberedSelect()` in non-TTY mode. Accepts array of `SelectOption<T>` with `{label: string, value: T}` structure. Returns selected value of type `T`.\n\n### selectRuntime\n\n```typescript\nasync function selectRuntime(mode: 'install' | 'uninstall' = 'install'): Promise<Runtime>\n```\n\nPrompts user to select from `'claude'`, `'opencode'`, `'gemini'`, or `'all'` runtime values. Customizes prompt text based on `mode` parameter: `\"Select runtime to install:\"` or `\"Select runtime to uninstall:\"`. Returns `Runtime` type from `./types.js`.\n\n### selectLocation\n\n```typescript\nasync function selectLocation(mode: 'install' | 'uninstall' = 'install'): Promise<Location>\n```\n\nPrompts user to select between `'global'` (user config directories like `~/.claude`) or `'local'` (project-level directories like `./.claude`). Returns `Location` type from `./types.js`.\n\n### confirmAction\n\n```typescript\nasync function confirmAction(message: string): Promise<boolean>\n```\n\nPrompts user with custom message and Yes/No options. Returns `true` for confirmed, `false` for declined.\n\n## Types\n\n### SelectOption\n\n```typescript\ninterface SelectOption<T> {\n  label: string;\n  value: T;\n}\n```\n\nGeneric option structure for `selectOption()` mapping display labels to typed values.\n\n## Terminal Raw Mode Management\n\n### cleanupRawMode\n\n```typescript\nfunction cleanupRawMode(): void\n```\n\nRestores terminal state by calling `process.stdin.setRawMode(false)` and `process.stdin.pause()` when `rawModeActive` flag is true. Ignores errors during cleanup. Sets `rawModeActive = false`.\n\n### Process Exit Handlers\n\nRegisters `cleanupRawMode()` on `process.on('exit')` and `process.on('SIGINT')` to guarantee terminal state restoration even on Ctrl+C interruption or unexpected termination.\n\n### rawModeActive\n\nModule-level `boolean` state tracker preventing double-cleanup attempts. Set to `true` when `process.stdin.setRawMode(true)` succeeds, reset to `false` by `cleanupRawMode()`.\n\n## Arrow Key Selection (TTY Mode)\n\n### arrowKeySelect\n\n```typescript\nasync function arrowKeySelect<T>(\n  prompt: string,\n  options: SelectOption<T>[],\n): Promise<T>\n```\n\nCaptures keypress events via `readline.emitKeypressEvents(process.stdin)` after enabling raw mode with `process.stdin.setRawMode(true)`. Maintains `selectedIndex` state responding to:\n\n- `key.name === 'up'` → `selectedIndex = Math.max(0, selectedIndex - 1)`\n- `key.name === 'down'` → `selectedIndex = Math.min(options.length - 1, selectedIndex + 1)`\n- `key.name === 'return'` → cleanup and resolve `options[selectedIndex].value`\n- `key.ctrl && key.name === 'c'` → cleanup and `process.exit(0)`\n\nRenders selected option with cyan color via `pc.cyan('> ')` prefix and `pc.cyan(opt.label)`. Re-renders on arrow key by clearing lines with ANSI escape sequences: `\\x1b[${options.length + 1}A` (move cursor up), `\\x1b[2K` (clear line), `\\x1b[1B` (move down).\n\nAlways executes `cleanupRawMode()` via `process.stdin.off('keypress', handleKeypress)` before resolving promise. Wraps setup in try/catch calling `cleanupRawMode()` on error.\n\n## Numbered Selection (Non-TTY Mode)\n\n### numberedSelect\n\n```typescript\nasync function numberedSelect<T>(\n  prompt: string,\n  options: SelectOption<T>[],\n): Promise<T>\n```\n\nPrints numbered list `1. ${opt.label}` and reads integer input via `readline.createInterface()` with `rl.question('Enter number: ')`. Validates input with `parseInt(answer, 10)` rejecting if `isNaN(num) || num < 1 || num > options.length`. Returns `options[num - 1].value`.\n\n## Dependencies\n\n- **node:readline**: Provides `emitKeypressEvents()`, `createInterface()` for stdin handling\n- **picocolors** (`pc`): Terminal color formatting with `pc.bold()`, `pc.cyan()`\n- **./types.js**: Imports `Runtime` and `Location` types for installer-specific prompts\n\n## Critical Design Constraints\n\nRaw mode cleanup enforced via:\n1. Try/finally blocks in `arrowKeySelect()` setup\n2. Global process event handlers for `'exit'` and `'SIGINT'`\n3. Explicit cleanup before promise resolution in `arrowKeySelect()`\n4. Module-level `rawModeActive` flag preventing double-cleanup races\n\nPrevents terminal state corruption where raw mode persists after process termination, breaking user's shell input echoing.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 1407,
      "cacheReadTokens": 17114,
      "cacheCreationTokens": 11134,
      "latencyMs": 37081,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/installer/prompts.ts",
          "sizeBytes": 6609
        }
      ]
    },
    {
      "timestamp": "2026-02-09T16:10:23.316Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/integration/types.ts\n\n```typescript\n/**\n * Integration types for AI coding assistant environments\n *\n * Defines types for detecting AI assistant environments (Claude Code, OpenCode, etc.)\n * and generating appropriate integration templates (command files, hooks).\n */\n\n/**\n * Supported AI coding assistant environment types\n */\nexport type EnvironmentType = 'claude' | 'opencode' | 'aider' | 'gemini';\n\n/**\n * Result of detecting an AI coding assistant environment\n */\nexport interface DetectedEnvironment {\n  /** Type of AI assistant detected */\n  type: EnvironmentType;\n  /** Configuration directory for this environment (e.g., '.claude', '.opencode') */\n  configDir: string;\n  /** Whether this environment was detected in the project */\n  detected: boolean;\n}\n\n/**\n * Template for integration files (command files, hooks, etc.)\n */\nexport interface IntegrationTemplate {\n  /** File name (e.g., 'generate.md') */\n  filename: string;\n  /** Relative path from project root (e.g., '.claude/commands/ar/generate.md') */\n  path: string;\n  /** Template content to write to the file */\n  content: string;\n}\n\n/**\n * Result of generating integration files for an environment\n */\nexport interface IntegrationResult {\n  /** Environment type that was configured */\n  environment: EnvironmentType;\n  /** Files that were successfully created */\n  filesCreated: string[];\n  /** Files that were skipped (already exist) */\n  filesSkipped: string[];\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**Defines TypeScript types for AI coding assistant environment detection and integration template generation across Claude Code, OpenCode, Gemini, and Aider platforms.**\n\n## Exported Types\n\n### EnvironmentType\n```typescript\ntype EnvironmentType = 'claude' | 'opencode' | 'aider' | 'gemini'\n```\nString literal union representing supported AI assistant platforms.\n\n### DetectedEnvironment\n```typescript\ninterface DetectedEnvironment {\n  type: EnvironmentType;\n  configDir: string;\n  detected: boolean;\n}\n```\nRepresents discovery result from environment detection: `type` identifies the AI platform, `configDir` stores the configuration directory path (e.g., `.claude`, `.opencode`), `detected` boolean indicates presence in project.\n\n### IntegrationTemplate\n```typescript\ninterface IntegrationTemplate {\n  filename: string;\n  path: string;\n  content: string;\n}\n```\nDescribes a single integration file to be written: `filename` is the base name (e.g., `'generate.md'`), `path` is the relative path from project root (e.g., `.claude/commands/ar/generate.md`), `content` holds the template text to write.\n\n### IntegrationResult\n```typescript\ninterface IntegrationResult {\n  environment: EnvironmentType;\n  filesCreated: string[];\n  filesSkipped: string[];\n}\n```\nAggregates outcome of integration file generation: `environment` identifies the target platform, `filesCreated` lists successfully written file paths, `filesSkipped` tracks paths that existed and were not overwritten.\n\n## Integration Points\n\nUsed by `src/integration/detect.ts` for environment detection return values and by `src/integration/generate.ts` for template construction and file writing results. Consumed by `src/installer/operations.ts` during npx-based command/hook installation workflow.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 439,
      "cacheReadTokens": 17114,
      "cacheCreationTokens": 9464,
      "latencyMs": 21100,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/integration/types.ts",
          "sizeBytes": 1400
        }
      ]
    },
    {
      "timestamp": "2026-02-09T16:10:16.536Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/integration/generate.ts\n\n```typescript\n/**\n * Integration file generation for AI coding assistants\n *\n * Generates command files and hooks for detected AI assistant environments.\n * Handles file creation with directory creation and skip-if-exists behavior.\n */\n\nimport { existsSync, mkdirSync, writeFileSync, readFileSync } from 'node:fs';\nimport * as path from 'node:path';\nimport { fileURLToPath } from 'node:url';\nimport type { IntegrationResult, EnvironmentType } from './types.js';\nimport { detectEnvironments } from './detect.js';\nimport {\n  getClaudeTemplates,\n  getOpenCodeTemplates,\n  getGeminiTemplates,\n} from './templates.js';\n\n/**\n * Get the path to a bundled hook file\n *\n * @param hookName - Name of the hook file (e.g., 'are-session-end.js')\n * @returns Absolute path to the bundled hook file\n */\nfunction getBundledHookPath(hookName: string): string {\n  const __filename = fileURLToPath(import.meta.url);\n  const __dirname = path.dirname(__filename);\n  // From dist/integration/ go up two levels to project root, then to hooks/dist/\n  return path.join(__dirname, '..', '..', 'hooks', 'dist', hookName);\n}\n\n/**\n * Read bundled hook content\n *\n * @param hookName - Name of the hook file\n * @returns Hook file content as string\n * @throws Error if hook file not found\n */\nfunction readBundledHook(hookName: string): string {\n  const hookPath = getBundledHookPath(hookName);\n  if (!existsSync(hookPath)) {\n    throw new Error(`Bundled hook not found: ${hookPath}`);\n  }\n  return readFileSync(hookPath, 'utf-8');\n}\n\n/**\n * Options for generating integration files\n */\nexport interface GenerateOptions {\n  /** If true, don't actually write files - just report what would be done */\n  dryRun?: boolean;\n  /** If true, overwrite existing files instead of skipping them */\n  force?: boolean;\n  /** Specific environment to generate for (bypasses auto-detection) */\n  environment?: EnvironmentType;\n}\n\n/**\n * Ensure parent directories exist for a file path\n *\n * @param filePath - Full path to the file\n */\nfunction ensureDir(filePath: string): void {\n  const dir = path.dirname(filePath);\n  if (!existsSync(dir)) {\n    mkdirSync(dir, { recursive: true });\n  }\n}\n\n/**\n * Generate integration files for all detected AI assistant environments\n *\n * For each detected environment:\n * - Gets appropriate templates (command files)\n * - Creates files if they don't exist (or if force=true)\n * - For Claude: also creates the session-end hook\n *\n * @param projectRoot - Root directory of the project\n * @param options - Generation options\n * @returns Array of results, one per environment\n *\n * @example\n * ```typescript\n * const results = await generateIntegrationFiles('/path/to/project');\n * // [{ environment: 'claude', filesCreated: ['...'], filesSkipped: [] }]\n * ```\n */\nexport async function generateIntegrationFiles(\n  projectRoot: string,\n  options: GenerateOptions = {}\n): Promise<IntegrationResult[]> {\n  const { dryRun = false, force = false, environment: specificEnv } = options;\n  const results: IntegrationResult[] = [];\n\n  // Use specific environment if provided, otherwise auto-detect\n  let environments: { type: EnvironmentType; configDir: string }[];\n  if (specificEnv) {\n    // Map environment type to config directory\n    const configDirMap: Record<EnvironmentType, string> = {\n      claude: '.claude',\n      opencode: '.opencode',\n      aider: '.aider',\n      gemini: '.gemini',\n    };\n    environments = [{ type: specificEnv, configDir: configDirMap[specificEnv] }];\n  } else {\n    // Detect which environments are present\n    environments = detectEnvironments(projectRoot);\n  }\n\n  for (const env of environments) {\n    const result: IntegrationResult = {\n      environment: env.type,\n      filesCreated: [],\n      filesSkipped: [],\n    };\n\n    // Get templates for this environment\n    const templates = getTemplatesForEnvironment(env.type);\n\n    // Process each template\n    for (const template of templates) {\n      const fullPath = path.join(projectRoot, template.path);\n\n      if (existsSync(fullPath) && !force) {\n        // File exists and force is not set - skip it\n        result.filesSkipped.push(template.path);\n      } else {\n        // Create the file\n        if (!dryRun) {\n          ensureDir(fullPath);\n          writeFileSync(fullPath, template.content, 'utf-8');\n        }\n        result.filesCreated.push(template.path);\n      }\n    }\n\n    // For Claude, also generate the hook file\n    if (env.type === 'claude') {\n      const hookPath = '.claude/hooks/are-session-end.js';\n      const fullHookPath = path.join(projectRoot, hookPath);\n\n      if (existsSync(fullHookPath) && !force) {\n        result.filesSkipped.push(hookPath);\n      } else {\n        if (!dryRun) {\n          ensureDir(fullHookPath);\n          const hookContent = readBundledHook('are-session-end.js');\n          writeFileSync(fullHookPath, hookContent, 'utf-8');\n        }\n        result.filesCreated.push(hookPath);\n      }\n    }\n\n    results.push(result);\n  }\n\n  return results;\n}\n\n/**\n * Get templates for a specific environment type\n *\n * @param type - Environment type\n * @returns Array of templates for that environment\n */\nfunction getTemplatesForEnvironment(\n  type: EnvironmentType\n): ReturnType<typeof getClaudeTemplates> {\n  switch (type) {\n    case 'claude':\n      return getClaudeTemplates();\n    case 'opencode':\n      return getOpenCodeTemplates();\n    case 'gemini':\n      return getGeminiTemplates();\n    case 'aider':\n      // Aider doesn't have command files yet - return empty\n      return [];\n    default:\n      return [];\n  }\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**generateIntegrationFiles() orchestrates AI assistant integration setup by detecting environments, retrieving platform-specific templates, writing command files, and copying bundled hook files from `hooks/dist/` to project directories.**\n\n## Exported Functions\n\n**generateIntegrationFiles(projectRoot: string, options?: GenerateOptions): Promise<IntegrationResult[]>**\n- Detects AI environments via `detectEnvironments()` unless `options.environment` overrides detection\n- Maps `EnvironmentType` to config directories: `claude` → `.claude`, `opencode` → `.opencode`, `aider` → `.aider`, `gemini` → `.gemini`\n- Iterates detected environments, retrieves templates via `getTemplatesForEnvironment()`, writes files via `writeFileSync()` with `ensureDir()` parent directory creation\n- Skips existing files unless `options.force` is true, populates `IntegrationResult.filesSkipped[]`\n- For `claude` environment specifically, copies bundled hook `are-session-end.js` from `hooks/dist/` to `.claude/hooks/are-session-end.js` via `readBundledHook()` → `writeFileSync()`\n- Returns `IntegrationResult[]` with `filesCreated[]` and `filesSkipped[]` per environment\n- Respects `options.dryRun` to skip actual writes\n\n## Internal Functions\n\n**getBundledHookPath(hookName: string): string**\n- Resolves path from `dist/integration/` up two levels to project root, then into `hooks/dist/{hookName}`\n- Uses `fileURLToPath(import.meta.url)` for ES module `__dirname` equivalent\n\n**readBundledHook(hookName: string): string**\n- Reads hook file content from path returned by `getBundledHookPath()`\n- Throws `Error` if `existsSync()` check fails with message pattern `\"Bundled hook not found: {hookPath}\"`\n\n**ensureDir(filePath: string): void**\n- Extracts directory via `path.dirname()`, creates recursively via `mkdirSync({ recursive: true })` if `existsSync()` returns false\n\n**getTemplatesForEnvironment(type: EnvironmentType): ReturnType<typeof getClaudeTemplates>**\n- Switch statement dispatching on `EnvironmentType`: `claude` → `getClaudeTemplates()`, `opencode` → `getOpenCodeTemplates()`, `gemini` → `getGeminiTemplates()`, `aider` → empty array `[]`\n- Returns array of template objects with `path` and `content` properties\n\n## Types\n\n**GenerateOptions interface**\n- `dryRun?: boolean` — Preview mode, no file writes\n- `force?: boolean` — Overwrite existing files instead of skipping\n- `environment?: EnvironmentType` — Bypass auto-detection, force specific environment\n\n## Integration Points\n\n- Imports `detectEnvironments()` from `./detect.js` for environment auto-detection\n- Imports `getClaudeTemplates()`, `getOpenCodeTemplates()`, `getGeminiTemplates()` from `./templates.js` for platform-specific command file content\n- Imports `IntegrationResult` and `EnvironmentType` from `./types.js`\n- File operations via Node.js `fs` module: `existsSync()`, `mkdirSync()`, `writeFileSync()`, `readFileSync()`\n- Path manipulation via `node:path` module and `fileURLToPath()` from `node:url`\n\n## File Path Patterns\n\n**Hook bundling convention:**\n- Hooks copied from `hooks/dist/{hookName}` (post-build artifacts from `scripts/build-hooks.js`)\n- Destination: `.claude/hooks/are-session-end.js` (only for `claude` environment)\n\n**Command file destinations:**\n- Platform-specific via templates: `.claude/skills/`, `.opencode/commands/`, `.gemini/commands/`",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 948,
      "cacheReadTokens": 17114,
      "cacheCreationTokens": 10734,
      "latencyMs": 29485,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/integration/generate.ts",
          "sizeBytes": 5521
        }
      ]
    },
    {
      "timestamp": "2026-02-09T16:10:25.418Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/orchestration/plan-tracker.ts\n\n```typescript\n/**\n * Progress tracker that updates GENERATION-PLAN.md checkboxes during generation.\n *\n * Maintains the markdown content in memory for fast updates and serializes\n * disk writes via a promise chain to handle concurrent Phase 1 completions.\n *\n * @module\n */\n\nimport { writeFile, mkdir } from 'node:fs/promises';\nimport * as path from 'node:path';\nimport { CONFIG_DIR } from '../config/loader.js';\n\n/**\n * Tracks generation progress by ticking checkboxes in GENERATION-PLAN.md.\n *\n * Create one instance at the start of `executeGenerate()`, call `markDone()`\n * as tasks complete, and `flush()` before returning.\n */\nexport class PlanTracker {\n  private content: string;\n  private readonly planPath: string;\n  private writeQueue: Promise<void> = Promise.resolve();\n\n  constructor(projectRoot: string, initialMarkdown: string) {\n    this.planPath = path.join(projectRoot, CONFIG_DIR, 'GENERATION-PLAN.md');\n    this.content = initialMarkdown;\n  }\n\n  /** Write the initial plan file to disk. */\n  async initialize(): Promise<void> {\n    try {\n      await mkdir(path.dirname(this.planPath), { recursive: true });\n      await writeFile(this.planPath, this.content, 'utf8');\n    } catch {\n      // Non-critical — generation continues without tracking\n    }\n  }\n\n  /**\n   * Mark a task as done by replacing its checkbox.\n   *\n   * The caller must pass the exact path as it appears in the markdown:\n   * - File: `src/cli/init.ts`\n   * - Directory: `src/cli/AGENTS.md`  (caller appends `/AGENTS.md`)\n   * - Root doc: `CLAUDE.md`\n   */\n  markDone(itemPath: string): void {\n    const before = this.content;\n    this.content = this.content.replace(\n      `- [ ] \\`${itemPath}\\``,\n      `- [x] \\`${itemPath}\\``,\n    );\n\n    if (this.content === before) return; // no match — skip write\n\n    // Queue a serialized write so concurrent markDone() calls don't corrupt the file\n    this.writeQueue = this.writeQueue\n      .then(() => writeFile(this.planPath, this.content, 'utf8'))\n      .catch(() => {/* non-critical */});\n  }\n\n  /** Wait for all queued writes to finish. */\n  async flush(): Promise<void> {\n    await this.writeQueue;\n  }\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**PlanTracker maintains in-memory markdown state for GENERATION-PLAN.md and serializes concurrent checkbox updates via promise-chain writes during three-phase generation.**\n\n## Exported Interface\n\n**PlanTracker** class with constructor signature `constructor(projectRoot: string, initialMarkdown: string)` stores plan path as `path.join(projectRoot, CONFIG_DIR, 'GENERATION-PLAN.md')` and initializes `content: string` field with provided markdown. Imports `CONFIG_DIR` from `../config/loader.js` (constant value `.agents-reverse-engineer`).\n\n**initialize()** async method creates parent directory via `mkdir(path.dirname(this.planPath), { recursive: true })` and writes `this.content` to disk with `writeFile(this.planPath, this.content, 'utf8')`. Swallows exceptions (non-critical operation—generation continues without tracking).\n\n**markDone(itemPath: string)** synchronous method replaces checkbox via exact string match: `- [ ] \\`${itemPath}\\`` → `- [x] \\`${itemPath}\\``. Returns early if no replacement occurred (no match). Chains `writeFile(this.planPath, this.content, 'utf8')` onto `this.writeQueue` promise field, catching errors silently (non-critical).\n\n**flush()** async method awaits `this.writeQueue` to ensure all pending writes complete before exiting.\n\n## Serialization Pattern\n\n**writeQueue** field (type `Promise<void>`, initialized as `Promise.resolve()`) forms promise chain preventing concurrent `markDone()` calls from corrupting disk file. Each `markDone()` invocation updates in-memory `content`, then chains: `this.writeQueue = this.writeQueue.then(() => writeFile(...)).catch(() => {})`. Subsequent calls wait for previous write to finish before starting next write.\n\n## Path Format Requirements\n\nCallers must pass `itemPath` matching exact markdown format used in GENERATION-PLAN.md:\n- File task: `src/cli/init.ts` (relative source path)\n- Directory task: `src/cli/AGENTS.md` (caller appends `/AGENTS.md` suffix)\n- Root document task: `CLAUDE.md`, `GEMINI.md`, `OPENCODE.md` (root-level filenames)\n\nCheckbox pattern uses backtick-wrapped paths: `` `${itemPath}` ``.\n\n## Integration Context\n\nCreated once per `executeGenerate()` invocation in `src/generation/executor.ts`. Workers call `markDone()` from concurrent Phase 1 file analysis pool. Sequential Phase 2 directory aggregation and Phase 3 root synthesis also call `markDone()` but without concurrency concerns. Final `flush()` call before `executeGenerate()` returns ensures all queued writes complete.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 661,
      "cacheReadTokens": 17114,
      "cacheCreationTokens": 9746,
      "latencyMs": 28644,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/orchestration/plan-tracker.ts",
          "sizeBytes": 2130
        }
      ]
    },
    {
      "timestamp": "2026-02-09T16:10:03.747Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/installer/operations.ts\n\n```typescript\n/**\n * File operations for installer module\n *\n * Handles copying command/hook files to runtime directories,\n * verifying installations, and registering hooks in settings.json.\n */\n\nimport { existsSync, mkdirSync, writeFileSync, readFileSync } from 'node:fs';\nimport * as path from 'node:path';\nimport { fileURLToPath } from 'node:url';\nimport type { Runtime, Location, InstallerResult } from './types.js';\nimport { resolveInstallPath, getAllRuntimes } from './paths.js';\nimport {\n  getClaudeTemplates,\n  getOpenCodeTemplates,\n  getGeminiTemplates,\n} from '../integration/templates.js';\n\n/**\n * Options for install operations\n */\nexport interface InstallOptions {\n  /** Overwrite existing files */\n  force: boolean;\n  /** Preview mode - don't write files */\n  dryRun: boolean;\n}\n\n/**\n * Ensure directory exists for a file path\n *\n * @param filePath - Full path to the file\n */\nfunction ensureDir(filePath: string): void {\n  const dir = path.dirname(filePath);\n  if (!existsSync(dir)) {\n    mkdirSync(dir, { recursive: true });\n  }\n}\n\n/**\n * Get the path to a bundled hook file\n *\n * Hooks are bundled in hooks/dist/ during npm prepublishOnly.\n *\n * @param hookName - Name of the hook file (e.g., 'are-session-end.js')\n * @returns Absolute path to the bundled hook file\n */\nfunction getBundledHookPath(hookName: string): string {\n  // Navigate from dist/installer/operations.js to hooks/dist/\n  const __filename = fileURLToPath(import.meta.url);\n  const __dirname = path.dirname(__filename);\n  // From dist/installer/ go up two levels to project root, then to hooks/dist/\n  return path.join(__dirname, '..', '..', 'hooks', 'dist', hookName);\n}\n\n/**\n * Read bundled hook content\n *\n * @param hookName - Name of the hook file\n * @returns Hook file content as string\n * @throws Error if hook file not found\n */\nfunction readBundledHook(hookName: string): string {\n  const hookPath = getBundledHookPath(hookName);\n  if (!existsSync(hookPath)) {\n    throw new Error(`Bundled hook not found: ${hookPath}`);\n  }\n  return readFileSync(hookPath, 'utf-8');\n}\n\n/**\n * Get templates for a specific runtime\n *\n * @param runtime - Target runtime (claude, opencode, or gemini)\n * @returns Array of template objects for the runtime\n */\nfunction getTemplatesForRuntime(runtime: Exclude<Runtime, 'all'>) {\n  switch (runtime) {\n    case 'claude':\n      return getClaudeTemplates();\n    case 'opencode':\n      return getOpenCodeTemplates();\n    case 'gemini':\n      return getGeminiTemplates();\n  }\n}\n\n/**\n * Install files for one or all runtimes\n *\n * If runtime is 'all', installs to all supported runtimes.\n * Otherwise, installs to the specified runtime only.\n *\n * @param runtime - Target runtime or 'all'\n * @param location - Installation location (global or local)\n * @param options - Install options (force, dryRun)\n * @returns Array of installation results (one per runtime)\n */\nexport function installFiles(\n  runtime: Runtime,\n  location: Location,\n  options: InstallOptions,\n): InstallerResult[] {\n  if (runtime === 'all') {\n    return getAllRuntimes().map((r) => installFilesForRuntime(r, location, options));\n  }\n  return [installFilesForRuntime(runtime, location, options)];\n}\n\n/**\n * Install files for a specific runtime\n *\n * Copies command templates and hook files to the installation directory.\n * Skips existing files unless force=true.\n *\n * @param runtime - Target runtime (claude, opencode, or gemini)\n * @param location - Installation location (global or local)\n * @param options - Install options (force, dryRun)\n * @returns Installation result with files created/skipped\n */\nfunction installFilesForRuntime(\n  runtime: Exclude<Runtime, 'all'>,\n  location: Location,\n  options: InstallOptions,\n): InstallerResult {\n  const basePath = resolveInstallPath(runtime, location);\n  const templates = getTemplatesForRuntime(runtime);\n  const filesCreated: string[] = [];\n  const filesSkipped: string[] = [];\n  const errors: string[] = [];\n\n  // Install command templates\n  for (const template of templates) {\n    // Template path is relative (e.g., .claude/commands/are/generate.md)\n    // Extract the part after the runtime directory (e.g., commands/are/generate.md)\n    const relativePath = template.path.split('/').slice(1).join('/');\n    const fullPath = path.join(basePath, relativePath);\n\n    if (existsSync(fullPath) && !options.force) {\n      filesSkipped.push(fullPath);\n    } else {\n      if (!options.dryRun) {\n        try {\n          ensureDir(fullPath);\n          writeFileSync(fullPath, template.content, 'utf-8');\n        } catch (err) {\n          errors.push(`Failed to write ${fullPath}: ${err}`);\n          continue;\n        }\n      }\n      filesCreated.push(fullPath);\n    }\n  }\n\n  // Install hooks/plugins based on runtime\n  let hookRegistered = false;\n  if (runtime === 'claude' || runtime === 'gemini') {\n    // Claude and Gemini: install session hooks\n    for (const hookDef of ARE_HOOKS) {\n      const hookPath = path.join(basePath, 'hooks', hookDef.filename);\n      if (existsSync(hookPath) && !options.force) {\n        filesSkipped.push(hookPath);\n      } else {\n        if (!options.dryRun) {\n          try {\n            ensureDir(hookPath);\n            const hookContent = readBundledHook(hookDef.filename);\n            writeFileSync(hookPath, hookContent, 'utf-8');\n          } catch (err) {\n            errors.push(`Failed to write hook ${hookPath}: ${err}`);\n          }\n        }\n        if (!errors.some((e) => e.includes(hookPath))) {\n          filesCreated.push(hookPath);\n        }\n      }\n    }\n\n    // Register hooks in settings.json\n    hookRegistered = registerHooks(basePath, runtime, options.dryRun);\n\n    // Register permissions for Claude (reduces friction for users)\n    if (runtime === 'claude') {\n      const settingsPath = path.join(basePath, 'settings.json');\n      registerPermissions(settingsPath, options.dryRun);\n    }\n  } else if (runtime === 'opencode') {\n    // OpenCode: install plugins (auto-loaded from plugins/ directory)\n    for (const pluginDef of ARE_PLUGINS) {\n      const pluginPath = path.join(basePath, 'plugins', pluginDef.destFilename);\n      if (existsSync(pluginPath) && !options.force) {\n        filesSkipped.push(pluginPath);\n      } else {\n        if (!options.dryRun) {\n          try {\n            ensureDir(pluginPath);\n            const pluginContent = readBundledHook(pluginDef.srcFilename);\n            writeFileSync(pluginPath, pluginContent, 'utf-8');\n          } catch (err) {\n            errors.push(`Failed to write plugin ${pluginPath}: ${err}`);\n          }\n        }\n        if (!errors.some((e) => e.includes(pluginPath))) {\n          filesCreated.push(pluginPath);\n          hookRegistered = true;\n        }\n      }\n    }\n  }\n\n  // Write VERSION file if files were created and not dry run\n  let versionWritten = false;\n  if (filesCreated.length > 0 && !options.dryRun) {\n    try {\n      writeVersionFile(basePath, options.dryRun);\n      versionWritten = true;\n    } catch {\n      // Non-fatal, don't add to errors\n    }\n  }\n\n  return {\n    success: errors.length === 0,\n    runtime,\n    location,\n    filesCreated,\n    filesSkipped,\n    errors,\n    hookRegistered,\n    versionWritten,\n  };\n}\n\n/**\n * Verify that installed files exist\n *\n * @param files - Array of file paths to verify\n * @returns Object with success status and list of missing files\n */\nexport function verifyInstallation(files: string[]): { success: boolean; missing: string[] } {\n  const missing = files.filter((f) => !existsSync(f));\n  return {\n    success: missing.length === 0,\n    missing,\n  };\n}\n\n/**\n * Session hook configuration for settings.json\n */\ninterface SessionHook {\n  type: 'command';\n  command: string;\n}\n\ninterface HookEvent {\n  hooks: SessionHook[];\n}\n\ninterface SettingsJson {\n  hooks?: {\n    SessionStart?: HookEvent[];\n    SessionEnd?: HookEvent[];\n  };\n  permissions?: {\n    allow?: string[];\n    deny?: string[];\n  };\n  [key: string]: unknown;\n}\n\n/**\n * Gemini hook configuration (simpler format)\n */\ninterface GeminiHook {\n  name: string;\n  type: 'command';\n  command: string;\n}\n\ninterface GeminiSettingsJson {\n  hooks?: {\n    SessionStart?: GeminiHook[];\n    SessionEnd?: GeminiHook[];\n  };\n  [key: string]: unknown;\n}\n\n/**\n * Hook definitions for ARE (Claude and Gemini)\n */\ninterface HookDefinition {\n  event: 'SessionStart' | 'SessionEnd';\n  filename: string;\n  name: string; // For Gemini format\n}\n\nconst ARE_HOOKS: HookDefinition[] = [\n  // { event: 'SessionStart', filename: 'are-check-update.js', name: 'are-check-update' },\n  // { event: 'SessionEnd', filename: 'are-session-end.js', name: 'are-session-end' }, // Disabled - causing issues\n];\n\n/**\n * Plugin definitions for ARE (OpenCode)\n *\n * OpenCode uses a plugin system (.opencode/plugins/) instead of hooks.\n * Plugins are JS/TS modules that export async functions returning event handlers.\n */\ninterface PluginDefinition {\n  /** Source filename in hooks/dist/ (prefixed with opencode-) */\n  srcFilename: string;\n  /** Destination filename in .opencode/plugins/ */\n  destFilename: string;\n}\n\nconst ARE_PLUGINS: PluginDefinition[] = [\n  { srcFilename: 'opencode-are-check-update.js', destFilename: 'are-check-update.js' },\n  // { srcFilename: 'opencode-are-session-end.js', destFilename: 'are-session-end.js' }, // Disabled - causing issues\n];\n\n/**\n * Register ARE hooks in settings.json\n *\n * Registers both SessionStart (update check) and SessionEnd (auto-update) hooks.\n * Supports both Claude Code and Gemini CLI formats.\n * Merges with existing hooks, doesn't overwrite.\n *\n * @param basePath - Base installation path (e.g., ~/.claude or ~/.gemini)\n * @param runtime - Target runtime (claude or gemini)\n * @param dryRun - If true, don't write changes\n * @returns true if any hook was added, false if all already existed\n */\nexport function registerHooks(\n  basePath: string,\n  runtime: Exclude<Runtime, 'all'>,\n  dryRun: boolean,\n): boolean {\n  // Only for Claude and Gemini installations\n  if (runtime !== 'claude' && runtime !== 'gemini') {\n    return false;\n  }\n\n  const settingsPath = path.join(basePath, 'settings.json');\n  const runtimeDir = runtime === 'claude' ? '.claude' : '.gemini';\n\n  if (runtime === 'gemini') {\n    return registerGeminiHooks(settingsPath, runtimeDir, dryRun);\n  }\n\n  return registerClaudeHooks(settingsPath, runtimeDir, dryRun);\n}\n\n/**\n * Register ARE hooks in Claude Code settings.json format\n */\nfunction registerClaudeHooks(settingsPath: string, runtimeDir: string, dryRun: boolean): boolean {\n  // Load or create settings\n  let settings: SettingsJson = {};\n  if (existsSync(settingsPath)) {\n    try {\n      const content = readFileSync(settingsPath, 'utf-8');\n      settings = JSON.parse(content) as SettingsJson;\n    } catch {\n      // If can't parse, start fresh\n      settings = {};\n    }\n  }\n\n  // Ensure hooks structure exists\n  if (!settings.hooks) {\n    settings.hooks = {};\n  }\n\n  let addedAny = false;\n\n  for (const hookDef of ARE_HOOKS) {\n    const hookCommand = `node ${runtimeDir}/hooks/${hookDef.filename}`;\n\n    // Ensure event array exists\n    if (!settings.hooks[hookDef.event]) {\n      settings.hooks[hookDef.event] = [];\n    }\n\n    // Check if hook already exists (by command string match)\n    const hookExists = settings.hooks[hookDef.event]!.some((event) =>\n      event.hooks?.some((h) => h.command === hookCommand),\n    );\n\n    if (!hookExists) {\n      // Define our hook (Claude format: nested hooks array)\n      const newHook: HookEvent = {\n        hooks: [\n          {\n            type: 'command',\n            command: hookCommand,\n          },\n        ],\n      };\n      settings.hooks[hookDef.event]!.push(newHook);\n      addedAny = true;\n    }\n  }\n\n  if (!addedAny) {\n    return false;\n  }\n\n  // Write settings if not dry run\n  if (!dryRun) {\n    ensureDir(settingsPath);\n    writeFileSync(settingsPath, JSON.stringify(settings, null, 2), 'utf-8');\n  }\n\n  return true;\n}\n\n/**\n * Permissions to auto-allow for ARE commands\n */\nconst ARE_PERMISSIONS = [\n  'Bash(npx agents-reverse-engineer@latest init*)',\n  'Bash(npx agents-reverse-engineer@latest discover*)',\n  'Bash(npx agents-reverse-engineer@latest generate*)',\n  'Bash(npx agents-reverse-engineer@latest update*)',\n  'Bash(npx agents-reverse-engineer@latest clean*)',\n  'Bash(rm -f .agents-reverse-engineer/progress.log*)',\n  'Bash(sleep *)',\n];\n\n/**\n * Register ARE permissions in Claude Code settings.json\n *\n * Adds bash command permissions for ARE commands to reduce friction.\n *\n * @param settingsPath - Path to settings.json\n * @param dryRun - If true, don't write changes\n * @returns true if permissions were added, false if already existed\n */\nexport function registerPermissions(settingsPath: string, dryRun: boolean): boolean {\n  // Load or create settings\n  let settings: SettingsJson = {};\n  if (existsSync(settingsPath)) {\n    try {\n      const content = readFileSync(settingsPath, 'utf-8');\n      settings = JSON.parse(content) as SettingsJson;\n    } catch {\n      // If can't parse, start fresh\n      settings = {};\n    }\n  }\n\n  // Ensure permissions structure exists\n  if (!settings.permissions) {\n    settings.permissions = {};\n  }\n  if (!settings.permissions.allow) {\n    settings.permissions.allow = [];\n  }\n\n  // Add any missing ARE permissions\n  let addedAny = false;\n  for (const perm of ARE_PERMISSIONS) {\n    if (!settings.permissions.allow.includes(perm)) {\n      settings.permissions.allow.push(perm);\n      addedAny = true;\n    }\n  }\n\n  if (!addedAny) {\n    return false;\n  }\n\n  // Write settings if not dry run\n  if (!dryRun) {\n    ensureDir(settingsPath);\n    writeFileSync(settingsPath, JSON.stringify(settings, null, 2), 'utf-8');\n  }\n\n  return true;\n}\n\n/**\n * Register ARE hooks in Gemini CLI settings.json format\n */\nfunction registerGeminiHooks(settingsPath: string, runtimeDir: string, dryRun: boolean): boolean {\n  // Load or create settings\n  let settings: GeminiSettingsJson = {};\n  if (existsSync(settingsPath)) {\n    try {\n      const content = readFileSync(settingsPath, 'utf-8');\n      settings = JSON.parse(content) as GeminiSettingsJson;\n    } catch {\n      // If can't parse, start fresh\n      settings = {};\n    }\n  }\n\n  // Ensure hooks structure exists\n  if (!settings.hooks) {\n    settings.hooks = {};\n  }\n\n  let addedAny = false;\n\n  for (const hookDef of ARE_HOOKS) {\n    const hookCommand = `node ${runtimeDir}/hooks/${hookDef.filename}`;\n\n    // Ensure event array exists\n    if (!settings.hooks[hookDef.event]) {\n      settings.hooks[hookDef.event] = [];\n    }\n\n    // Check if hook already exists (by command string match)\n    const hookExists = settings.hooks[hookDef.event]!.some((h) => h.command === hookCommand);\n\n    if (!hookExists) {\n      // Define our hook (Gemini format: flat object with name)\n      const newHook: GeminiHook = {\n        name: hookDef.name,\n        type: 'command',\n        command: hookCommand,\n      };\n      settings.hooks[hookDef.event]!.push(newHook);\n      addedAny = true;\n    }\n  }\n\n  if (!addedAny) {\n    return false;\n  }\n\n  // Write settings if not dry run\n  if (!dryRun) {\n    ensureDir(settingsPath);\n    writeFileSync(settingsPath, JSON.stringify(settings, null, 2), 'utf-8');\n  }\n\n  return true;\n}\n\n/**\n * Get package version from package.json\n *\n * @returns Version string or 'unknown' if can't read\n */\nexport function getPackageVersion(): string {\n  try {\n    // Navigate from dist/installer/operations.js to package.json\n    // In ESM, we need to use import.meta.url\n    const __filename = fileURLToPath(import.meta.url);\n    const __dirname = path.dirname(__filename);\n    // From src/installer/ go up two levels to project root\n    const packagePath = path.join(__dirname, '..', '..', 'package.json');\n    const content = readFileSync(packagePath, 'utf-8');\n    const pkg = JSON.parse(content) as { version?: string };\n    return pkg.version || 'unknown';\n  } catch {\n    return 'unknown';\n  }\n}\n\n/**\n * Write ARE-VERSION file to track installed version\n *\n * @param basePath - Base installation path\n * @param dryRun - If true, don't write the file\n */\nexport function writeVersionFile(basePath: string, dryRun: boolean): void {\n  if (dryRun) {\n    return;\n  }\n\n  const versionPath = path.join(basePath, 'ARE-VERSION');\n  const version = getPackageVersion();\n\n  ensureDir(versionPath);\n  writeFileSync(versionPath, version, 'utf-8');\n}\n\n/**\n * Format installation result for display\n *\n * Generates human-readable lines showing created/skipped files.\n *\n * @param result - Installation result to format\n * @returns Array of formatted lines for display\n */\nexport function formatInstallResult(result: InstallerResult): string[] {\n  const lines: string[] = [];\n\n  // Header with runtime and location\n  lines.push(`  ${result.runtime} (${result.location}):`);\n\n  // Created files\n  for (const file of result.filesCreated) {\n    lines.push(`    Created: ${file}`);\n  }\n\n  // Skipped files\n  for (const file of result.filesSkipped) {\n    lines.push(`    Skipped: ${file} (already exists)`);\n  }\n\n  // Hook registration status (Claude only)\n  if (result.hookRegistered) {\n    lines.push(`    Registered: SessionEnd hook in settings.json`);\n  }\n\n  // Summary line\n  const created = result.filesCreated.length;\n  const skipped = result.filesSkipped.length;\n  lines.push(`    ${created} files installed, ${skipped} skipped`);\n\n  return lines;\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**`operations.ts` installs ARE command templates and session hooks to IDE runtime directories (~/.claude, ~/.opencode, ~/.gemini), registers hooks in settings.json, writes ARE-VERSION files, and verifies installations.**\n\n## Exported Functions\n\n**`installFiles(runtime: Runtime, location: Location, options: InstallOptions): InstallerResult[]`**\nInstalls command templates and hooks for one or all runtimes. When `runtime === 'all'`, calls `getAllRuntimes()` and maps `installFilesForRuntime()` across all supported runtimes. Returns array of `InstallerResult` objects with `filesCreated`, `filesSkipped`, `errors`, `hookRegistered`, `versionWritten` fields.\n\n**`verifyInstallation(files: string[]): { success: boolean; missing: string[] }`**\nChecks existence of file paths via `existsSync()`. Returns object with `success: true` if all files exist, otherwise `missing` array of absent paths.\n\n**`registerHooks(basePath: string, runtime: Exclude<Runtime, 'all'>, dryRun: boolean): boolean`**\nRegisters ARE session hooks in `settings.json` for Claude and Gemini runtimes. Dispatches to `registerClaudeHooks()` or `registerGeminiHooks()` based on runtime. Returns `true` if any hooks were added, `false` if all already existed. Skips for OpenCode (uses plugin system).\n\n**`registerPermissions(settingsPath: string, dryRun: boolean): boolean`**\nAdds `ARE_PERMISSIONS` bash command patterns to Claude Code `settings.json` permissions.allow array. Returns `true` if any permissions added. Used to reduce friction for ARE commands during execution.\n\n**`getPackageVersion(): string`**\nReads version from `package.json` via `fileURLToPath(import.meta.url)` → `path.join(__dirname, '..', '..', 'package.json')`. Returns version string or `'unknown'` on parse failure.\n\n**`writeVersionFile(basePath: string, dryRun: boolean): void`**\nWrites `ARE-VERSION` file containing result of `getPackageVersion()` to `basePath`. Used by session hooks to detect installed version. Skips if `dryRun === true`.\n\n**`formatInstallResult(result: InstallerResult): string[]`**\nGenerates human-readable display lines showing runtime/location header, created/skipped file paths, hook registration status, and summary counts.\n\n## Exported Interfaces\n\n**`InstallOptions`**\n```typescript\n{\n  force: boolean;    // Overwrite existing files\n  dryRun: boolean;   // Preview without writing\n}\n```\n\n## Internal Functions\n\n**`installFilesForRuntime(runtime: Exclude<Runtime, 'all'>, location: Location, options: InstallOptions): InstallerResult`**\nCore installation logic. Calls `resolveInstallPath()` to get base directory, `getTemplatesForRuntime()` for command templates, writes template files via `writeFileSync()` after `ensureDir()`, copies hooks/plugins via `readBundledHook()` → `writeFileSync()`, registers hooks in settings.json, calls `writeVersionFile()`. Returns `InstallerResult` with success status and file lists.\n\n**`getTemplatesForRuntime(runtime: Exclude<Runtime, 'all'>): Template[]`**\nDispatches to `getClaudeTemplates()`, `getOpenCodeTemplates()`, or `getGeminiTemplates()` from `src/integration/templates.ts` based on runtime.\n\n**`ensureDir(filePath: string): void`**\nCreates parent directory for `filePath` via `mkdirSync(path.dirname(filePath), { recursive: true })` if it doesn't exist.\n\n**`getBundledHookPath(hookName: string): string`**\nResolves path to bundled hook file in `hooks/dist/` relative to compiled module location. Uses `fileURLToPath(import.meta.url)` → `path.join(__dirname, '..', '..', 'hooks', 'dist', hookName)`.\n\n**`readBundledHook(hookName: string): string`**\nReads hook file content via `getBundledHookPath()` and `readFileSync()`. Throws error if hook not found.\n\n**`registerClaudeHooks(settingsPath: string, runtimeDir: string, dryRun: boolean): boolean`**\nRegisters hooks in Claude Code format (nested `hooks` array). For each `ARE_HOOKS` entry, constructs `hookCommand: \"node ${runtimeDir}/hooks/${filename}\"`, checks if hook exists via command string match, appends `HookEvent` object with `hooks: [{ type: 'command', command }]` structure. Writes merged settings to `settingsPath`.\n\n**`registerGeminiHooks(settingsPath: string, runtimeDir: string, dryRun: boolean): boolean`**\nRegisters hooks in Gemini CLI format (flat object with `name` field). Constructs `GeminiHook: { name, type: 'command', command }`, checks existence, appends to event array. Writes merged settings.\n\n## Hook/Plugin Definitions\n\n**`ARE_HOOKS: HookDefinition[]`**\nArray defining session hooks for Claude and Gemini. Currently empty (hooks disabled due to issues). Format:\n```typescript\n{ event: 'SessionStart' | 'SessionEnd', filename: string, name: string }\n```\nPreviously included:\n- `'are-check-update.js'` for SessionStart\n- `'are-session-end.js'` for SessionEnd (commented out)\n\n**`ARE_PLUGINS: PluginDefinition[]`**\nArray defining plugins for OpenCode runtime. Format:\n```typescript\n{ srcFilename: string, destFilename: string }\n```\nIncludes:\n- `{ srcFilename: 'opencode-are-check-update.js', destFilename: 'are-check-update.js' }`\n- SessionEnd plugin commented out\n\n**`ARE_PERMISSIONS: string[]`**\nBash command permission patterns for Claude Code auto-allow. Glob patterns:\n- `'Bash(npx agents-reverse-engineer@latest init*)'`\n- `'Bash(npx agents-reverse-engineer@latest discover*)'`\n- `'Bash(npx agents-reverse-engineer@latest generate*)'`\n- `'Bash(npx agents-reverse-engineer@latest update*)'`\n- `'Bash(npx agents-reverse-engineer@latest clean*)'`\n- `'Bash(rm -f .agents-reverse-engineer/progress.log*)'`\n- `'Bash(sleep *)'`\n\n## Settings.json Schema\n\n**`SettingsJson` (Claude Code format)**\n```typescript\n{\n  hooks?: {\n    SessionStart?: HookEvent[];\n    SessionEnd?: HookEvent[];\n  };\n  permissions?: {\n    allow?: string[];\n    deny?: string[];\n  };\n}\n```\nWhere `HookEvent = { hooks: SessionHook[] }` and `SessionHook = { type: 'command', command: string }`.\n\n**`GeminiSettingsJson` (Gemini CLI format)**\n```typescript\n{\n  hooks?: {\n    SessionStart?: GeminiHook[];\n    SessionEnd?: GeminiHook[];\n  };\n}\n```\nWhere `GeminiHook = { name: string, type: 'command', command: string }`.\n\n## Integration Points\n\nImports `resolveInstallPath()` and `getAllRuntimes()` from `./paths.js` for runtime directory resolution. Imports `getClaudeTemplates()`, `getOpenCodeTemplates()`, `getGeminiTemplates()` from `../integration/templates.js` for command template content. Uses `Runtime`, `Location`, `InstallerResult` types from `./types.js`.\n\n## File Writing Pattern\n\nAll writes follow pattern: `ensureDir(fullPath)` → `writeFileSync(fullPath, content, 'utf-8')`. Errors caught individually and pushed to `errors[]` array. Template files written from in-memory template objects, hook files copied from bundled `hooks/dist/` directory. Settings.json merges with existing content, preserving user hooks/permissions.\n\n## Installation Workflow\n\nFor each runtime: resolve base path → get templates → iterate templates (skip if exists and !force, else write) → iterate hooks/plugins (skip if exists and !force, else copy from bundled) → register in settings.json → write VERSION file if files created. OpenCode installs to `plugins/` directory (auto-loaded), Claude/Gemini install to `hooks/` directory (requires settings.json registration).",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 2091,
      "cacheReadTokens": 17114,
      "cacheCreationTokens": 14522,
      "latencyMs": 50484,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/installer/operations.ts",
          "sizeBytes": 17303
        }
      ]
    },
    {
      "timestamp": "2026-02-09T16:10:24.459Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/orchestration/index.ts\n\n```typescript\n/**\n * Public API for the orchestration module.\n *\n * This barrel export is the single import point for the orchestration\n * engine. It re-exports the concurrency pool, progress reporter,\n * command runner, and all shared types.\n *\n * @module\n *\n * @example\n * ```typescript\n * import {\n *   CommandRunner,\n *   ProgressReporter,\n *   runPool,\n * } from './orchestration/index.js';\n *\n * const runner = new CommandRunner(aiService, { concurrency: 5 });\n * const summary = await runner.executeGenerate(plan);\n * ```\n */\n\n// ---------------------------------------------------------------------------\n// Types\n// ---------------------------------------------------------------------------\n\nexport type {\n  FileTaskResult,\n  RunSummary,\n  ProgressEvent,\n  CommandRunOptions,\n} from './types.js';\n\n// ---------------------------------------------------------------------------\n// Concurrency pool\n// ---------------------------------------------------------------------------\n\nexport { runPool } from './pool.js';\nexport type { PoolOptions, TaskResult } from './pool.js';\n\n// ---------------------------------------------------------------------------\n// Progress reporter\n// ---------------------------------------------------------------------------\n\nexport { ProgressReporter, ProgressLog } from './progress.js';\n\n// ---------------------------------------------------------------------------\n// Plan tracker\n// ---------------------------------------------------------------------------\n\nexport { PlanTracker } from './plan-tracker.js';\n\n// ---------------------------------------------------------------------------\n// Tracing\n// ---------------------------------------------------------------------------\n\nexport type { ITraceWriter, TraceEvent, TraceEventPayload } from './trace.js';\nexport { createTraceWriter, cleanupOldTraces } from './trace.js';\n\n// ---------------------------------------------------------------------------\n// Command runner\n// ---------------------------------------------------------------------------\n\nexport { CommandRunner } from './runner.js';\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**Barrel export module exposing the complete orchestration API: runPool, ProgressReporter, ProgressLog, PlanTracker, CommandRunner, ITraceWriter, TraceEvent, TraceEventPayload, createTraceWriter, cleanupOldTraces, and shared types (FileTaskResult, RunSummary, ProgressEvent, CommandRunOptions, PoolOptions, TaskResult).**\n\n## Exported Types\n\n**FileTaskResult** — Outcome of a single file analysis task (success/failure with error details).\n\n**RunSummary** — Aggregated metrics from CommandRunner execution (total files, errors, duration, token counts).\n\n**ProgressEvent** — Event payload emitted during pool execution for progress tracking.\n\n**CommandRunOptions** — Configuration object passed to CommandRunner methods (includes concurrency, timeout, tracer, failFast flags).\n\n**PoolOptions** — Configuration for runPool including concurrency limit and optional tracer.\n\n**TaskResult** — Generic result type from runPool execution (success/failure discriminated union).\n\n**ITraceWriter** — Interface defining emit() method for NDJSON trace event emission.\n\n**TraceEvent** — Union type of all trace event payloads (phase:start/end, worker:start/end, task:pickup/done, subprocess:spawn/exit, retry).\n\n**TraceEventPayload** — Base payload structure for all trace events (seq, ts, pid, elapsedMs).\n\n## Exported Functions\n\n**runPool** — Iterator-based worker pool executing async tasks with shared iterator across N workers, returns aggregated results with error handling.\n\n**createTraceWriter** — Factory function returning TraceWriter instance (writes NDJSON to `.agents-reverse-engineer/traces/`) or NullTraceWriter if tracing disabled.\n\n**cleanupOldTraces** — Deletes old trace files from `.agents-reverse-engineer/traces/` directory, retaining only the most recent keepCount files (default 500).\n\n## Exported Classes\n\n**ProgressReporter** — Streaming progress reporter with ETA calculation via moving average of last 10 task durations, emits ProgressEvent objects, supports spinner UI via ora integration.\n\n**ProgressLog** — File-based progress logger writing human-readable output to `.agents-reverse-engineer/progress.log`, mirrors console output for real-time monitoring.\n\n**PlanTracker** — Serialized writer for `GENERATION-PLAN.md` using promise-chain pattern to prevent corruption from concurrent workers, tracks phase breakdown with file counts.\n\n**CommandRunner** — High-level orchestrator executing three-phase pipeline (file analysis, directory aggregation, root synthesis) with configurable concurrency, timeout, and tracing.\n\n## Module Organization\n\nRe-exports from five submodules:\n- `./types.js` — Shared interfaces for task results and run configuration\n- `./pool.js` — Concurrency pool implementation with shared iterator pattern\n- `./progress.js` — Progress reporting with console and file logging\n- `./plan-tracker.js` — Serialized plan file writer\n- `./trace.js` — NDJSON trace event emission with cleanup utilities\n- `./runner.js` — Three-phase pipeline orchestration\n\n## Usage Pattern\n\nPrimary entry point for orchestration layer. Import CommandRunner for high-level three-phase execution, or runPool for custom concurrent task processing. ProgressReporter provides real-time feedback with ETA calculation. TraceWriter enables detailed execution telemetry when `--trace` flag passed via CommandRunOptions.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 799,
      "cacheReadTokens": 17114,
      "cacheCreationTokens": 9493,
      "latencyMs": 30965,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/orchestration/index.ts",
          "sizeBytes": 2076
        }
      ]
    },
    {
      "timestamp": "2026-02-09T16:10:26.366Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/orchestration/pool.ts\n\n```typescript\n/**\n * Iterator-based concurrency pool.\n *\n * Provides a zero-dependency concurrency limiter using the shared-iterator\n * worker pattern. Workers pull from a shared iterator so exactly N tasks\n * execute concurrently, with new tasks starting as previous ones complete.\n *\n * This avoids the \"batch\" anti-pattern where `Promise.all` on chunks of N\n * tasks idles workers while waiting for the slowest task in each batch.\n *\n * @module\n */\n\nimport type { ITraceWriter } from './trace.js';\n\n// ---------------------------------------------------------------------------\n// Types\n// ---------------------------------------------------------------------------\n\n/**\n * Options for the concurrency pool.\n */\nexport interface PoolOptions {\n  /** Maximum number of concurrent workers */\n  concurrency: number;\n  /** Stop pulling new tasks on first error */\n  failFast?: boolean;\n  /** Trace writer for concurrency debugging (no-op when tracing is off) */\n  tracer?: ITraceWriter;\n  /** Phase label for trace events (e.g., 'phase-1-files') */\n  phaseLabel?: string;\n  /** Labels for each task by index (e.g., file paths). Used in trace events. */\n  taskLabels?: string[];\n}\n\n/**\n * Result of a single task execution within the pool.\n *\n * Indexed by the task's position in the original array so callers\n * can correlate results back to their inputs.\n */\nexport interface TaskResult<T> {\n  /** Zero-based index of the task in the original array */\n  index: number;\n  /** Whether the task completed successfully */\n  success: boolean;\n  /** The resolved value (present when success is true) */\n  value?: T;\n  /** The error (present when success is false) */\n  error?: Error;\n}\n\n// ---------------------------------------------------------------------------\n// Pool implementation\n// ---------------------------------------------------------------------------\n\n/**\n * Run an array of async task factories through a concurrency-limited pool.\n *\n * Uses the shared-iterator pattern: all workers iterate over the same\n * `entries()` iterator, so each task is picked up by exactly one worker.\n * When a worker finishes a task, it immediately pulls the next one from\n * the iterator, keeping all worker slots busy.\n *\n * @typeParam T - The resolved type of each task\n * @param tasks - Array of zero-argument async functions to execute\n * @param options - Pool configuration (concurrency, failFast, tracing)\n * @param onComplete - Optional callback invoked after each task settles\n * @returns Array of results indexed by original task position (may be sparse if aborted)\n *\n * @example\n * ```typescript\n * const results = await runPool(\n *   urls.map(url => () => fetch(url).then(r => r.json())),\n *   { concurrency: 5, failFast: false },\n *   (result) => console.log(`Task ${result.index}: ${result.success}`),\n * );\n * ```\n */\nexport async function runPool<T>(\n  tasks: Array<() => Promise<T>>,\n  options: PoolOptions,\n  onComplete?: (result: TaskResult<T>) => void,\n): Promise<TaskResult<T>[]> {\n  const results: TaskResult<T>[] = [];\n\n  if (tasks.length === 0) {\n    return results;\n  }\n\n  const tracer = options.tracer;\n  const phase = options.phaseLabel ?? 'unknown';\n  const taskLabels = options.taskLabels;\n\n  // Shared abort flag -- workers check before pulling next task\n  let aborted = false;\n\n  // Active task counter for trace snapshots\n  let activeTasks = 0;\n\n  async function worker(\n    iterator: IterableIterator<[number, () => Promise<T>]>,\n    workerId: number,\n  ): Promise<void> {\n    let tasksExecuted = 0;\n\n    tracer?.emit({ type: 'worker:start', workerId, phase });\n\n    for (const [index, task] of iterator) {\n      if (aborted) break;\n\n      activeTasks++;\n      const taskStart = Date.now();\n      const label = taskLabels?.[index] ?? `task-${index}`;\n\n      tracer?.emit({\n        type: 'task:pickup',\n        workerId,\n        taskIndex: index,\n        taskLabel: label,\n        activeTasks,\n      });\n\n      try {\n        const value = await task();\n        activeTasks--;\n        tasksExecuted++;\n\n        tracer?.emit({\n          type: 'task:done',\n          workerId,\n          taskIndex: index,\n          taskLabel: label,\n          durationMs: Date.now() - taskStart,\n          success: true,\n          activeTasks,\n        });\n\n        const result: TaskResult<T> = { index, success: true, value };\n        results[index] = result;\n        onComplete?.(result);\n      } catch (err) {\n        activeTasks--;\n        tasksExecuted++;\n\n        const error = err instanceof Error ? err : new Error(String(err));\n\n        tracer?.emit({\n          type: 'task:done',\n          workerId,\n          taskIndex: index,\n          taskLabel: label,\n          durationMs: Date.now() - taskStart,\n          success: false,\n          error: error.message,\n          activeTasks,\n        });\n\n        const result: TaskResult<T> = { index, success: false, error };\n        results[index] = result;\n        onComplete?.(result);\n\n        if (options.failFast) {\n          aborted = true;\n          break;\n        }\n      }\n    }\n\n    tracer?.emit({ type: 'worker:end', workerId, phase, tasksExecuted });\n  }\n\n  // Spawn workers, capped at the number of available tasks\n  const effectiveConcurrency = Math.min(options.concurrency, tasks.length);\n  const iterator = tasks.entries();\n  const workers = Array.from({ length: effectiveConcurrency }, (_, workerId) =>\n    worker(iterator, workerId),\n  );\n\n  await Promise.allSettled(workers);\n\n  return results;\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**Iterator-based concurrency pool implementing the shared-iterator worker pattern to execute N async tasks concurrently without idling workers between batches.**\n\n## Exported Functions\n\n`runPool<T>(tasks: Array<() => Promise<T>>, options: PoolOptions, onComplete?: (result: TaskResult<T>) => void): Promise<TaskResult<T>[]>` executes an array of async task factories through a concurrency-limited pool where all workers share a single `entries()` iterator. Each worker pulls the next task immediately after completing the previous one, ensuring all N worker slots stay busy until the iterator exhausts. Returns an array of `TaskResult<T>` indexed by original task position (may be sparse if `failFast` aborts early).\n\n## Core Types\n\n`PoolOptions` configures pool behavior with required `concurrency: number` (maximum concurrent workers), optional `failFast?: boolean` (stop pulling new tasks on first error), optional `tracer?: ITraceWriter` (emit trace events for debugging), optional `phaseLabel?: string` (phase identifier for trace events like `'phase-1-files'`), and optional `taskLabels?: string[]` (human-readable labels for each task by index, used in trace events).\n\n`TaskResult<T>` represents a single task execution outcome with `index: number` (zero-based position in original array), `success: boolean` (completion status), optional `value?: T` (resolved value when `success: true`), and optional `error?: Error` (error when `success: false`).\n\n## Worker Pattern Implementation\n\n`runPool()` creates a shared `iterator` via `tasks.entries()` which yields `[index, taskFn]` tuples. Spawns `effectiveConcurrency = Math.min(options.concurrency, tasks.length)` workers, each invoking the internal `worker(iterator, workerId)` async function. Each worker iterates over the shared iterator via `for (const [index, task] of iterator)`, executing `await task()`, storing results in `results[index]`, invoking `onComplete?.(result)` callback, and checking `aborted` flag before pulling next task. Uses `Promise.allSettled(workers)` to wait for all workers to finish.\n\n## Abort Mechanism\n\nThe shared mutable `aborted` boolean flag stops task pickup when `options.failFast` is true and any worker encounters an error. Workers check `if (aborted) break` before pulling the next task from the iterator, but do not interrupt already-running tasks.\n\n## Trace Event Emission\n\nEmits `worker:start` with `workerId`, `phase` when worker begins. Emits `task:pickup` with `workerId`, `taskIndex`, `taskLabel`, `activeTasks` when worker pulls a task. Emits `task:done` with `workerId`, `taskIndex`, `taskLabel`, `durationMs`, `success`, optional `error`, `activeTasks` when task settles. Emits `worker:end` with `workerId`, `phase`, `tasksExecuted` when worker exhausts iterator or aborts. The `activeTasks` counter increments before task execution and decrements after task settles, providing snapshot of concurrent task count.\n\n## Error Handling\n\nCatches task execution errors via try-catch, converts non-Error exceptions to Error via `err instanceof Error ? err : new Error(String(err))`, stores `TaskResult<T>` with `success: false` and `error` field. When `failFast: false` (default), worker continues pulling tasks after error. When `failFast: true`, worker sets `aborted = true` and breaks from iterator loop.\n\n## Dependency on External Modules\n\nImports `ITraceWriter` from `'./trace.js'` (module-relative path with `.js` extension for ES module resolution). No runtime dependencies on external npm packages (zero-dependency concurrency limiter).\n\n## Anti-Pattern Mitigation\n\nAvoids batch-based concurrency where `Promise.all()` on N-sized chunks idles workers while waiting for the slowest task in each batch. The shared iterator ensures new tasks start immediately as workers become available, maximizing throughput.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 955,
      "cacheReadTokens": 17114,
      "cacheCreationTokens": 10604,
      "latencyMs": 32075,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/orchestration/pool.ts",
          "sizeBytes": 5458
        }
      ]
    },
    {
      "timestamp": "2026-02-09T16:10:09.343Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/installer/uninstall.ts\n\n```typescript\n/**\n * Uninstall module for agents-reverse-engineer installer\n *\n * Handles removing installed command files, hooks, and hook registrations.\n * Mirrors the installation logic in operations.ts for clean reversal.\n */\n\nimport { existsSync, unlinkSync, readFileSync, writeFileSync, readdirSync, rmdirSync, rmSync } from 'node:fs';\nimport * as path from 'node:path';\nimport type { Runtime, Location, InstallerResult } from './types.js';\nimport { resolveInstallPath, getAllRuntimes, getRuntimePaths } from './paths.js';\nimport {\n  getClaudeTemplates,\n  getOpenCodeTemplates,\n  getGeminiTemplates,\n} from '../integration/templates.js';\n\n/**\n * Session hook configuration for settings.json (matches operations.ts)\n */\ninterface SessionHook {\n  type: 'command';\n  command: string;\n}\n\ninterface HookEvent {\n  hooks: SessionHook[];\n}\n\ninterface SettingsJson {\n  hooks?: {\n    SessionStart?: HookEvent[];\n    SessionEnd?: HookEvent[];\n  };\n  permissions?: {\n    allow?: string[];\n    deny?: string[];\n  };\n  [key: string]: unknown;\n}\n\n/**\n * Hook definitions for ARE (must match operations.ts)\n */\ninterface HookDefinition {\n  event: 'SessionStart' | 'SessionEnd';\n  filename: string;\n}\n\nconst ARE_HOOKS: HookDefinition[] = [\n  { event: 'SessionStart', filename: 'are-check-update.js' },\n  { event: 'SessionEnd', filename: 'are-session-end.js' },\n];\n\n/**\n * Plugin definitions for ARE OpenCode (must match operations.ts)\n */\nconst ARE_PLUGIN_FILENAMES = [\n  'are-check-update.js',\n  'are-session-end.js',\n];\n\n/**\n * Permissions to remove during uninstall (must match operations.ts)\n */\nconst ARE_PERMISSIONS = [\n  'Bash(npx agents-reverse-engineer@latest init*)',\n  'Bash(npx agents-reverse-engineer@latest discover*)',\n  'Bash(npx agents-reverse-engineer@latest generate*)',\n  'Bash(npx agents-reverse-engineer@latest update*)',\n  'Bash(npx agents-reverse-engineer@latest clean*)',\n];\n\n/**\n * Get templates for a specific runtime\n *\n * @param runtime - Target runtime (claude, opencode, or gemini)\n * @returns Array of template objects for the runtime\n */\nfunction getTemplatesForRuntime(runtime: Exclude<Runtime, 'all'>) {\n  switch (runtime) {\n    case 'claude':\n      return getClaudeTemplates();\n    case 'opencode':\n      return getOpenCodeTemplates();\n    case 'gemini':\n      return getGeminiTemplates();\n  }\n}\n\n/**\n * Uninstall files for one or all runtimes\n *\n * If runtime is 'all', uninstalls from all supported runtimes.\n * Otherwise, uninstalls from the specified runtime only.\n *\n * @param runtime - Target runtime or 'all'\n * @param location - Installation location (global or local)\n * @param dryRun - If true, don't actually delete files\n * @returns Array of uninstallation results (one per runtime)\n */\nexport function uninstallFiles(\n  runtime: Runtime,\n  location: Location,\n  dryRun: boolean = false,\n): InstallerResult[] {\n  if (runtime === 'all') {\n    return getAllRuntimes().map((r) => uninstallFilesForRuntime(r, location, dryRun));\n  }\n  return [uninstallFilesForRuntime(runtime, location, dryRun)];\n}\n\n/**\n * Uninstall files for a specific runtime\n *\n * Removes command templates, hook files, and VERSION file from the installation directory.\n * Also unregisters hooks from settings.json for Claude global installs.\n *\n * @param runtime - Target runtime (claude, opencode, or gemini)\n * @param location - Installation location (global or local)\n * @param dryRun - If true, don't actually delete files\n * @returns Uninstallation result with files deleted\n */\nfunction uninstallFilesForRuntime(\n  runtime: Exclude<Runtime, 'all'>,\n  location: Location,\n  dryRun: boolean,\n): InstallerResult {\n  const basePath = resolveInstallPath(runtime, location);\n  const templates = getTemplatesForRuntime(runtime);\n  const filesCreated: string[] = []; // In uninstall context, this tracks files deleted\n  const filesSkipped: string[] = []; // Files that didn't exist\n  const errors: string[] = [];\n\n  // Remove command templates\n  for (const template of templates) {\n    // Template path is relative (e.g., .claude/commands/are/generate.md)\n    // Extract the part after the runtime directory (e.g., commands/are/generate.md)\n    const relativePath = template.path.split('/').slice(1).join('/');\n    const fullPath = path.join(basePath, relativePath);\n\n    if (existsSync(fullPath)) {\n      if (!dryRun) {\n        try {\n          unlinkSync(fullPath);\n        } catch (err) {\n          errors.push(`Failed to delete ${fullPath}: ${err}`);\n          continue;\n        }\n      }\n      filesCreated.push(fullPath); // Track as \"deleted\"\n    } else {\n      filesSkipped.push(fullPath); // File didn't exist\n    }\n  }\n\n  // Remove hooks/plugins based on runtime\n  let hookUnregistered = false;\n  if (runtime === 'claude' || runtime === 'gemini') {\n    // Remove all ARE hook files\n    for (const hookDef of ARE_HOOKS) {\n      const hookPath = path.join(basePath, 'hooks', hookDef.filename);\n      if (existsSync(hookPath)) {\n        if (!dryRun) {\n          try {\n            unlinkSync(hookPath);\n          } catch (err) {\n            errors.push(`Failed to delete hook ${hookPath}: ${err}`);\n          }\n        }\n        if (!errors.some((e) => e.includes(hookPath))) {\n          filesCreated.push(hookPath);\n        }\n      }\n    }\n\n    // Unregister hooks from settings.json\n    hookUnregistered = unregisterHooks(basePath, runtime, dryRun);\n\n    // Unregister permissions from settings.json (Claude only)\n    if (runtime === 'claude') {\n      unregisterPermissions(basePath, dryRun);\n    }\n  } else if (runtime === 'opencode') {\n    // Remove all ARE plugin files\n    for (const pluginFilename of ARE_PLUGIN_FILENAMES) {\n      const pluginPath = path.join(basePath, 'plugins', pluginFilename);\n      if (existsSync(pluginPath)) {\n        if (!dryRun) {\n          try {\n            unlinkSync(pluginPath);\n          } catch (err) {\n            errors.push(`Failed to delete plugin ${pluginPath}: ${err}`);\n          }\n        }\n        if (!errors.some((e) => e.includes(pluginPath))) {\n          filesCreated.push(pluginPath);\n          hookUnregistered = true;\n        }\n      }\n    }\n  }\n\n  // Remove ARE-VERSION file if exists\n  const versionPath = path.join(basePath, 'ARE-VERSION');\n  if (existsSync(versionPath)) {\n    if (!dryRun) {\n      try {\n        unlinkSync(versionPath);\n      } catch (err) {\n        errors.push(`Failed to delete ARE-VERSION: ${err}`);\n      }\n    }\n    if (!errors.some((e) => e.includes('ARE-VERSION'))) {\n      filesCreated.push(versionPath);\n    }\n  }\n\n  // Try to clean up empty directories\n  if (!dryRun) {\n    if (runtime === 'claude') {\n      // Claude uses skills format: clean up skills/are-* directories\n      const skillsDir = path.join(basePath, 'skills');\n      cleanupAreSkillDirs(skillsDir);\n      cleanupEmptyDirs(skillsDir);\n    } else if (runtime === 'gemini') {\n      // Gemini uses flat commands/ directory for TOML files\n      const commandsDir = path.join(basePath, 'commands');\n      cleanupEmptyDirs(commandsDir);\n      // Clean up legacy files from old installations\n      cleanupLegacyGeminiFiles(commandsDir);\n    } else {\n      // OpenCode uses commands format with flat .md files\n      const commandsDir = path.join(basePath, 'commands');\n      cleanupEmptyDirs(commandsDir);\n    }\n\n    // Clean up hooks/plugins directory if empty\n    if (runtime === 'claude' || runtime === 'gemini') {\n      const hooksDir = path.join(basePath, 'hooks');\n      cleanupEmptyDirs(hooksDir);\n    } else if (runtime === 'opencode') {\n      const pluginsDir = path.join(basePath, 'plugins');\n      cleanupEmptyDirs(pluginsDir);\n    }\n  }\n\n  return {\n    success: errors.length === 0,\n    runtime,\n    location,\n    filesCreated, // Actually files deleted in uninstall context\n    filesSkipped, // Files that didn't exist\n    errors,\n    hookRegistered: hookUnregistered, // Repurpose: true if hook was unregistered\n  };\n}\n\n/**\n * Gemini hook configuration (simpler format)\n */\ninterface GeminiHook {\n  name: string;\n  type: 'command';\n  command: string;\n}\n\ninterface GeminiSettingsJson {\n  hooks?: {\n    SessionStart?: GeminiHook[];\n    SessionEnd?: GeminiHook[];\n  };\n  [key: string]: unknown;\n}\n\n/**\n * Unregister ARE hooks from settings.json\n *\n * Removes all ARE hook entries from SessionStart and SessionEnd arrays.\n * Cleans up empty hooks structures. Handles both old and new hook paths.\n *\n * @param basePath - Base installation path (e.g., ~/.claude or ~/.gemini)\n * @param runtime - Target runtime (claude or gemini)\n * @param dryRun - If true, don't write changes\n * @returns true if any hook was removed, false if none found\n */\nexport function unregisterHooks(\n  basePath: string,\n  runtime: Exclude<Runtime, 'all'>,\n  dryRun: boolean,\n): boolean {\n  if (runtime === 'gemini') {\n    return unregisterGeminiHooks(basePath, dryRun);\n  }\n  return unregisterClaudeHooks(basePath, dryRun);\n}\n\n/**\n * Build hook command patterns for matching (includes legacy paths)\n */\nfunction getHookPatterns(runtimeDir: string): string[] {\n  const patterns: string[] = [];\n  for (const hookDef of ARE_HOOKS) {\n    // Current path format\n    patterns.push(`node ${runtimeDir}/hooks/${hookDef.filename}`);\n    // Legacy path format\n    patterns.push(`node hooks/${hookDef.filename}`);\n  }\n  return patterns;\n}\n\n/**\n * Unregister ARE hooks from Claude Code settings.json\n */\nfunction unregisterClaudeHooks(basePath: string, dryRun: boolean): boolean {\n  const settingsPath = path.join(basePath, 'settings.json');\n\n  // Settings file must exist\n  if (!existsSync(settingsPath)) {\n    return false;\n  }\n\n  // Load settings\n  let settings: SettingsJson;\n  try {\n    const content = readFileSync(settingsPath, 'utf-8');\n    settings = JSON.parse(content) as SettingsJson;\n  } catch {\n    return false;\n  }\n\n  if (!settings.hooks) {\n    return false;\n  }\n\n  const hookPatterns = getHookPatterns('.claude');\n  let removedAny = false;\n\n  // Process both SessionStart and SessionEnd\n  for (const eventType of ['SessionStart', 'SessionEnd'] as const) {\n    if (!settings.hooks[eventType]) {\n      continue;\n    }\n\n    const originalLength = settings.hooks[eventType]!.length;\n\n    settings.hooks[eventType] = settings.hooks[eventType]!.filter(\n      (event) => !event.hooks?.some((h) => hookPatterns.includes(h.command)),\n    );\n\n    if (settings.hooks[eventType]!.length < originalLength) {\n      removedAny = true;\n    }\n\n    // Clean up empty array\n    if (settings.hooks[eventType]!.length === 0) {\n      delete settings.hooks[eventType];\n    }\n  }\n\n  if (!removedAny) {\n    return false;\n  }\n\n  // Clean up empty hooks object\n  if (Object.keys(settings.hooks).length === 0) {\n    delete settings.hooks;\n  }\n\n  // Write updated settings\n  if (!dryRun) {\n    writeFileSync(settingsPath, JSON.stringify(settings, null, 2), 'utf-8');\n  }\n\n  return true;\n}\n\n/**\n * Unregister ARE permissions from Claude Code settings.json\n *\n * Removes all ARE-related bash command permissions from the allow list.\n *\n * @param basePath - Base installation path (e.g., ~/.claude)\n * @param dryRun - If true, don't write changes\n * @returns true if any permissions were removed, false if none found\n */\nexport function unregisterPermissions(basePath: string, dryRun: boolean): boolean {\n  const settingsPath = path.join(basePath, 'settings.json');\n\n  // Settings file must exist\n  if (!existsSync(settingsPath)) {\n    return false;\n  }\n\n  // Load settings\n  let settings: SettingsJson;\n  try {\n    const content = readFileSync(settingsPath, 'utf-8');\n    settings = JSON.parse(content) as SettingsJson;\n  } catch {\n    return false;\n  }\n\n  // Check if permissions.allow exists\n  if (!settings.permissions?.allow) {\n    return false;\n  }\n\n  const originalLength = settings.permissions.allow.length;\n\n  // Remove all ARE permissions\n  settings.permissions.allow = settings.permissions.allow.filter(\n    (perm) => !ARE_PERMISSIONS.includes(perm),\n  );\n\n  // Check if we actually removed something\n  if (settings.permissions.allow.length === originalLength) {\n    return false;\n  }\n\n  // Clean up empty structures\n  if (settings.permissions.allow.length === 0) {\n    delete settings.permissions.allow;\n  }\n\n  if (settings.permissions && Object.keys(settings.permissions).length === 0) {\n    delete settings.permissions;\n  }\n\n  // Write updated settings\n  if (!dryRun) {\n    writeFileSync(settingsPath, JSON.stringify(settings, null, 2), 'utf-8');\n  }\n\n  return true;\n}\n\n/**\n * Unregister ARE hooks from Gemini CLI settings.json\n */\nfunction unregisterGeminiHooks(basePath: string, dryRun: boolean): boolean {\n  const settingsPath = path.join(basePath, 'settings.json');\n\n  // Settings file must exist\n  if (!existsSync(settingsPath)) {\n    return false;\n  }\n\n  // Load settings\n  let settings: GeminiSettingsJson;\n  try {\n    const content = readFileSync(settingsPath, 'utf-8');\n    settings = JSON.parse(content) as GeminiSettingsJson;\n  } catch {\n    return false;\n  }\n\n  if (!settings.hooks) {\n    return false;\n  }\n\n  const hookPatterns = getHookPatterns('.gemini');\n  let removedAny = false;\n\n  // Process both SessionStart and SessionEnd\n  for (const eventType of ['SessionStart', 'SessionEnd'] as const) {\n    if (!settings.hooks[eventType]) {\n      continue;\n    }\n\n    const originalLength = settings.hooks[eventType]!.length;\n\n    settings.hooks[eventType] = settings.hooks[eventType]!.filter(\n      (h) => !hookPatterns.includes(h.command),\n    );\n\n    if (settings.hooks[eventType]!.length < originalLength) {\n      removedAny = true;\n    }\n\n    // Clean up empty array\n    if (settings.hooks[eventType]!.length === 0) {\n      delete settings.hooks[eventType];\n    }\n  }\n\n  if (!removedAny) {\n    return false;\n  }\n\n  // Clean up empty hooks object\n  if (Object.keys(settings.hooks).length === 0) {\n    delete settings.hooks;\n  }\n\n  // Write updated settings\n  if (!dryRun) {\n    writeFileSync(settingsPath, JSON.stringify(settings, null, 2), 'utf-8');\n  }\n\n  return true;\n}\n\n/**\n * Clean up ARE skill directories\n *\n * Removes all empty are-* skill directories from the skills folder.\n *\n * @param skillsDir - Path to the skills directory\n */\nfunction cleanupAreSkillDirs(skillsDir: string): void {\n  try {\n    if (!existsSync(skillsDir)) {\n      return;\n    }\n\n    const entries = readdirSync(skillsDir);\n    for (const entry of entries) {\n      // Only clean up are-* directories\n      if (entry.startsWith('are-')) {\n        const skillDir = path.join(skillsDir, entry);\n        cleanupEmptyDirs(skillDir);\n      }\n    }\n  } catch {\n    // Ignore errors\n  }\n}\n\n/**\n * Clean up empty directories recursively\n *\n * Removes a directory if it's empty, then tries parent directories.\n * Stops when hitting a non-empty directory or the runtime root.\n *\n * @param dirPath - Directory path to check and potentially remove\n */\nfunction cleanupEmptyDirs(dirPath: string): void {\n  try {\n    if (!existsSync(dirPath)) {\n      return;\n    }\n\n    const entries = readdirSync(dirPath);\n    if (entries.length === 0) {\n      rmdirSync(dirPath);\n\n      // Try parent directory (but don't go above runtime root)\n      const parent = path.dirname(dirPath);\n      // Stop at common runtime roots (.claude, .opencode, .gemini)\n      const baseName = path.basename(parent);\n      if (\n        baseName !== '.claude' &&\n        baseName !== '.opencode' &&\n        baseName !== '.gemini' &&\n        baseName !== '.config'\n      ) {\n        cleanupEmptyDirs(parent);\n      }\n    }\n  } catch {\n    // Ignore errors - directory might be in use or we don't have permissions\n  }\n}\n\n/**\n * Clean up legacy Gemini files from old installations\n *\n * Removes:\n * - Old are-*.md files from .gemini/commands/ (pre-TOML format)\n * - Old .toml files from .gemini/commands/are/ (pre-flat structure)\n *\n * @param commandsDir - Path to the commands directory\n */\nfunction cleanupLegacyGeminiFiles(commandsDir: string): void {\n  try {\n    if (!existsSync(commandsDir)) {\n      return;\n    }\n\n    const entries = readdirSync(commandsDir);\n    for (const entry of entries) {\n      // Remove legacy are-*.md files\n      if (entry.startsWith('are-') && entry.endsWith('.md')) {\n        const filePath = path.join(commandsDir, entry);\n        try {\n          unlinkSync(filePath);\n        } catch {\n          // Ignore errors\n        }\n      }\n    }\n\n    // Remove legacy nested are/ directory (old /are:* namespace structure)\n    const areDir = path.join(commandsDir, 'are');\n    if (existsSync(areDir)) {\n      const areEntries = readdirSync(areDir);\n      for (const entry of areEntries) {\n        if (entry.endsWith('.toml')) {\n          try {\n            unlinkSync(path.join(areDir, entry));\n          } catch {\n            // Ignore errors\n          }\n        }\n      }\n      cleanupEmptyDirs(areDir);\n    }\n  } catch {\n    // Ignore errors\n  }\n}\n\n/**\n * Configuration directory name (matches config/loader.ts)\n */\nconst CONFIG_DIR = '.agents-reverse-engineer';\n\n/**\n * Delete the .agents-reverse-engineer configuration folder\n *\n * Only applicable for local installations. Removes the entire folder\n * including configuration files and generation plans.\n *\n * @param location - Installation location (only 'local' triggers deletion)\n * @param dryRun - If true, don't actually delete\n * @returns true if folder was deleted, false if not found or not local\n */\nexport function deleteConfigFolder(location: Location, dryRun: boolean): boolean {\n  // Only delete for local installations\n  if (location !== 'local') {\n    return false;\n  }\n\n  const configPath = path.join(process.cwd(), CONFIG_DIR);\n\n  if (!existsSync(configPath)) {\n    return false;\n  }\n\n  if (!dryRun) {\n    try {\n      rmSync(configPath, { recursive: true, force: true });\n    } catch {\n      return false;\n    }\n  }\n\n  return true;\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**Uninstall module removes installed ARE command files, hooks, hook registrations from settings.json, and permissions for Claude Code/Gemini CLI/OpenCode runtimes with dry-run support and empty directory cleanup.**\n\n## Exported Functions\n\n**`uninstallFiles(runtime: Runtime, location: Location, dryRun: boolean = false): InstallerResult[]`**\nUninstalls files for one or all runtimes, returning array of `InstallerResult` objects (one per runtime processed). When `runtime === 'all'`, calls `getAllRuntimes()` and maps over each runtime with `uninstallFilesForRuntime()`. Otherwise returns single-element array with result for specified runtime.\n\n**`unregisterHooks(basePath: string, runtime: Exclude<Runtime, 'all'>, dryRun: boolean): boolean`**\nRemoves ARE hook entries from `settings.json` for Claude/Gemini runtimes, dispatching to `unregisterClaudeHooks()` or `unregisterGeminiHooks()`. Returns `true` if any hook was removed. Handles both current hook paths (`node ${runtimeDir}/hooks/${filename}`) and legacy paths (`node hooks/${filename}`).\n\n**`unregisterPermissions(basePath: string, dryRun: boolean): boolean`**\nRemoves ARE Bash command permissions from Claude Code `settings.json` allow list. Filters out all entries matching `ARE_PERMISSIONS` array, cleans up empty `permissions.allow`/`permissions` structures. Returns `true` if any permissions were removed.\n\n**`deleteConfigFolder(location: Location, dryRun: boolean): boolean`**\nDeletes `.agents-reverse-engineer` directory from `process.cwd()` when `location === 'local'`. Uses `rmSync(configPath, { recursive: true, force: true })`. Returns `false` for global installations or missing folders.\n\n## Internal Functions\n\n**`uninstallFilesForRuntime(runtime: Exclude<Runtime, 'all'>, location: Location, dryRun: boolean): InstallerResult`**\nCore uninstallation logic for single runtime. Resolves `basePath` via `resolveInstallPath()`, retrieves templates via `getTemplatesForRuntime()`. Deletes command template files by extracting relative path (`template.path.split('/').slice(1).join('/')`) and joining with `basePath`. For Claude/Gemini, deletes hook files matching `ARE_HOOKS` definitions, calls `unregisterHooks()` and (Claude only) `unregisterPermissions()`. For OpenCode, deletes plugin files matching `ARE_PLUGIN_FILENAMES`. Removes `ARE-VERSION` file. Calls cleanup functions: `cleanupAreSkillDirs()` (Claude), `cleanupLegacyGeminiFiles()` (Gemini), `cleanupEmptyDirs()` (all runtimes). Returns `InstallerResult` with `filesCreated` tracking deleted files, `hookRegistered` repurposed to indicate hook unregistration success.\n\n**`getTemplatesForRuntime(runtime: Exclude<Runtime, 'all'>)`**\nDispatches to `getClaudeTemplates()`, `getOpenCodeTemplates()`, or `getGeminiTemplates()` based on runtime parameter.\n\n**`getHookPatterns(runtimeDir: string): string[]`**\nBuilds array of hook command patterns for matching in `settings.json`. Iterates `ARE_HOOKS`, generates current format (`node ${runtimeDir}/hooks/${filename}`) and legacy format (`node hooks/${filename}`) for each hook.\n\n**`unregisterClaudeHooks(basePath: string, dryRun: boolean): boolean`**\nLoads `settings.json` as `SettingsJson` type. Filters `settings.hooks.SessionStart` and `settings.hooks.SessionEnd` arrays, removing events where `event.hooks` contains any command matching `getHookPatterns('.claude')`. Deletes empty event arrays and empty `hooks` object. Writes updated JSON with 2-space indentation unless `dryRun`. Returns `true` if any hook removed.\n\n**`unregisterGeminiHooks(basePath: string, dryRun: boolean): boolean`**\nSimilar to Claude variant but operates on `GeminiSettingsJson` with simplified hook structure (direct `GeminiHook` objects vs nested `HookEvent` wrappers). Filters by `h.command` matching `getHookPatterns('.gemini')`.\n\n**`cleanupAreSkillDirs(skillsDir: string): void`**\nIterates entries in Claude skills directory, calling `cleanupEmptyDirs()` on subdirectories whose names start with `'are-'`.\n\n**`cleanupEmptyDirs(dirPath: string): void`**\nRecursively removes empty directories via `rmdirSync()`, then attempts parent directory cleanup. Stops recursion at runtime root directories (basenames: `.claude`, `.opencode`, `.gemini`, `.config`).\n\n**`cleanupLegacyGeminiFiles(commandsDir: string): void`**\nRemoves legacy files from old Gemini installations: `are-*.md` files (pre-TOML format), `.toml` files from nested `are/` subdirectory (pre-flat structure). Calls `cleanupEmptyDirs()` on `are/` subdirectory after deletion.\n\n## Hook and Permission Definitions\n\n**`ARE_HOOKS: HookDefinition[]`**\nArray defining two hooks: `{ event: 'SessionStart', filename: 'are-check-update.js' }`, `{ event: 'SessionEnd', filename: 'are-session-end.js' }`. Must match `operations.ts` definitions.\n\n**`ARE_PLUGIN_FILENAMES: string[]`**\nArray of OpenCode plugin filenames: `['are-check-update.js', 'are-session-end.js']`. Must match `operations.ts`.\n\n**`ARE_PERMISSIONS: string[]`**\nArray of five Bash permission strings for Claude Code: patterns like `'Bash(npx agents-reverse-engineer@latest init*)'` covering init/discover/generate/update/clean commands. Must match `operations.ts`.\n\n**`CONFIG_DIR: string`**\nConstant `'.agents-reverse-engineer'` for local configuration directory name (matches `config/loader.ts`).\n\n## Type Interfaces\n\n**`SessionHook`**\nClaude hook entry: `{ type: 'command', command: string }`.\n\n**`HookEvent`**\nClaude hook event wrapper: `{ hooks: SessionHook[] }`.\n\n**`SettingsJson`**\nClaude settings schema: `hooks?: { SessionStart?: HookEvent[], SessionEnd?: HookEvent[] }`, `permissions?: { allow?: string[], deny?: string[] }`.\n\n**`GeminiHook`**\nGemini hook entry: `{ name: string, type: 'command', command: string }`.\n\n**`GeminiSettingsJson`**\nGemini settings schema: `hooks?: { SessionStart?: GeminiHook[], SessionEnd?: GeminiHook[] }`.\n\n**`HookDefinition`**\nHook metadata: `{ event: 'SessionStart' | 'SessionEnd', filename: string }`.\n\n## Integration Points\n\nImports `Runtime`, `Location`, `InstallerResult` from `./types.js`, `resolveInstallPath()`, `getAllRuntimes()`, `getRuntimePaths()` from `./paths.js`, template getters from `../integration/templates.js`. Mirrors installation logic from `operations.ts` for clean reversal. Uses Node.js `fs` module (`existsSync`, `unlinkSync`, `readFileSync`, `writeFileSync`, `readdirSync`, `rmdirSync`, `rmSync`) and `path` module for filesystem operations.\n\n## Behavioral Contracts\n\nHook command pattern format: `node ${runtimeDir}/hooks/${filename}` (current), `node hooks/${filename}` (legacy). Permission pattern format: `Bash(npx agents-reverse-engineer@latest <command>*)`. Settings JSON written with `JSON.stringify(settings, null, 2)` for 2-space indentation. Empty array/object cleanup order: delete empty event arrays first, then empty `hooks`/`permissions` parent objects. Runtime root basenames for recursion stopping: `.claude`, `.opencode`, `.gemini`, `.config`. Legacy Gemini file patterns: `are-*.md` (commands directory), `*.toml` (nested `are/` subdirectory).",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 2029,
      "cacheReadTokens": 17114,
      "cacheCreationTokens": 14625,
      "latencyMs": 50681,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/installer/uninstall.ts",
          "sizeBytes": 17689
        }
      ]
    },
    {
      "timestamp": "2026-02-09T16:10:20.911Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/integration/templates.ts\n\n```typescript\n/**\n * Template generators for AI coding assistant integration files\n *\n * Generates command file templates for Claude Code, OpenCode, Gemini CLI, and session hooks.\n */\n\nimport type { IntegrationTemplate } from './types.js';\n\n// =============================================================================\n// Shared Command Content\n// =============================================================================\n\nconst COMMANDS = {\n  generate: {\n    description: 'Generate AI-friendly documentation for the entire codebase',\n    argumentHint: '[path] [--dry-run] [--concurrency N] [--fail-fast] [--debug] [--trace]',\n    content: `Generate comprehensive documentation for this codebase using agents-reverse-engineer.\n\n<execution>\nRun the generate command in the background and monitor progress in real time.\n\n## Steps\n\n1. **Display version**: Read \\`VERSION_FILE_PATH\\` and show the user: \\`agents-reverse-engineer vX.Y.Z\\`\n\n2. **Run the generate command in the background** using \\`run_in_background: true\\`:\n   \\`\\`\\`bash\n   npx agents-reverse-engineer@latest generate $ARGUMENTS\n   \\`\\`\\`\n\n3. **Monitor progress by polling** \\`.agents-reverse-engineer/progress.log\\`:\n   - Wait ~15 seconds (use \\`sleep 15\\` in Bash), then use the **Read** tool to read \\`.agents-reverse-engineer/progress.log\\` (use the \\`offset\\` parameter to read only the last ~20 lines for long files)\n   - Show the user a brief progress update (e.g. \"32/96 files analyzed, ~12m remaining\")\n   - Check whether the background task has completed using \\`TaskOutput\\` with \\`block: false\\`\n   - Repeat until the background task finishes\n   - **Important**: Keep polling even if progress.log doesn't exist yet (the command takes a few seconds to start writing)\n\n4. **On completion**, read the full background task output and summarize:\n   - Number of files analyzed and any failures\n   - Number of directories documented\n   - Root and per-package documents generated\n   - Any inconsistency warnings from the quality report\n\nThis executes a three-phase pipeline:\n\n1. **Discovery & Planning**: Walks the directory tree, applies filters (gitignore, vendor, binary, custom), detects file types, and creates a generation plan.\n\n2. **File Analysis** (concurrent): Analyzes each source file via AI and writes \\`.sum\\` summary files with YAML frontmatter (\\`content_hash\\`, \\`file_type\\`, \\`purpose\\`, \\`public_interface\\`, \\`dependencies\\`, \\`patterns\\`).\n\n3. **Directory & Root Documents** (sequential):\n   - Generates \\`AGENTS.md\\` per directory in post-order traversal (deepest first, so child summaries feed into parents)\n   - Creates root document: \\`CLAUDE.md\\`\n\n**Options:**\n- \\`--dry-run\\`: Preview the plan without making AI calls\n- \\`--concurrency N\\`: Control number of parallel AI calls (default: auto)\n- \\`--fail-fast\\`: Stop on first file analysis failure\n- \\`--debug\\`: Show AI prompts and backend details\n- \\`--trace\\`: Enable concurrency tracing to \\`.agents-reverse-engineer/traces/\\`\n</execution>`,\n  },\n\n  update: {\n    description: 'Incrementally update documentation for changed files',\n    argumentHint: '[path] [--uncommitted] [--dry-run] [--concurrency N] [--fail-fast] [--debug] [--trace]',\n    content: `Update documentation for files that changed since last run.\n\n<execution>\nRun the update command in the background and monitor progress in real time.\n\n## Steps\n\n1. **Display version**: Read \\`VERSION_FILE_PATH\\` and show the user: \\`agents-reverse-engineer vX.Y.Z\\`\n\n2. **Run the update command in the background** using \\`run_in_background: true\\`:\n   \\`\\`\\`bash\n   npx agents-reverse-engineer@latest update $ARGUMENTS\n   \\`\\`\\`\n\n3. **Monitor progress by polling** \\`.agents-reverse-engineer/progress.log\\`:\n   - Wait ~15 seconds (use \\`sleep 15\\` in Bash), then use the **Read** tool to read \\`.agents-reverse-engineer/progress.log\\` (use the \\`offset\\` parameter to read only the last ~20 lines for long files)\n   - Show the user a brief progress update (e.g. \"12/30 files updated, ~5m remaining\")\n   - Check whether the background task has completed using \\`TaskOutput\\` with \\`block: false\\`\n   - Repeat until the background task finishes\n   - **Important**: Keep polling even if progress.log doesn't exist yet (the command takes a few seconds to start writing)\n\n4. **On completion**, read the full background task output and summarize:\n   - Files updated\n   - Files unchanged\n   - Any orphaned docs cleaned up\n\n**Options:**\n- \\`--uncommitted\\`: Include staged but uncommitted changes\n- \\`--dry-run\\`: Show what would be updated without writing\n- \\`--concurrency N\\`: Control number of parallel AI calls (default: auto)\n- \\`--fail-fast\\`: Stop on first file analysis failure\n- \\`--debug\\`: Show AI prompts and backend details\n- \\`--trace\\`: Enable concurrency tracing to \\`.agents-reverse-engineer/traces/\\`\n</execution>`,\n  },\n\n  init: {\n    description: 'Initialize agents-reverse-engineer configuration',\n    argumentHint: '',\n    content: `Initialize agents-reverse-engineer configuration in this project.\n\n<execution>\n1. **Display version**: Read \\`VERSION_FILE_PATH\\` and show the user: \\`agents-reverse-engineer vX.Y.Z\\`\n\n2. Run the agents-reverse-engineer init command:\n\n\\`\\`\\`bash\nnpx agents-reverse-engineer@latest init\n\\`\\`\\`\n\nThis creates \\`.agents-reverse-engineer/config.yaml\\` configuration file.\n</execution>`,\n  },\n\n  discover: {\n    description: 'Discover files in codebase',\n    argumentHint: '[path] [--debug] [--trace]',\n    content: `List files that would be analyzed for documentation.\n\n<execution>\n## STRICT RULES - VIOLATION IS FORBIDDEN\n\n1. Run ONLY this exact command: \\`npx agents-reverse-engineer@latest discover $ARGUMENTS\\`\n2. DO NOT add ANY flags the user did not explicitly type\n3. If user typed nothing after \\`COMMAND_PREFIXdiscover\\`, run with ZERO flags\n\n## Steps\n\n1. **Display version**: Read \\`VERSION_FILE_PATH\\` and show the user: \\`agents-reverse-engineer vX.Y.Z\\`\n\n2. **Run the discover command in the background** using \\`run_in_background: true\\`:\n   \\`\\`\\`bash\n   npx agents-reverse-engineer@latest discover $ARGUMENTS\n   \\`\\`\\`\n\n3. **Monitor progress by polling** \\`.agents-reverse-engineer/progress.log\\`:\n   - Wait ~10 seconds (use \\`sleep 10\\` in Bash), then use the **Read** tool to read \\`.agents-reverse-engineer/progress.log\\` (use the \\`offset\\` parameter to read only the last ~20 lines for long files)\n   - Show the user a brief progress update\n   - Check whether the background task has completed using \\`TaskOutput\\` with \\`block: false\\`\n   - Repeat until the background task finishes\n   - **Important**: Keep polling even if progress.log doesn't exist yet (the command takes a few seconds to start writing)\n\n4. **On completion**, read the full background task output and report number of files found.\n</execution>`,\n  },\n\n  clean: {\n    description: 'Delete all generated documentation artifacts (.sum, AGENTS.md, plan)',\n    argumentHint: '[path] [--dry-run]',\n    content: `Remove all documentation artifacts generated by agents-reverse-engineer.\n\n<execution>\n## STRICT RULES - VIOLATION IS FORBIDDEN\n\n1. Run ONLY this exact command: \\`npx agents-reverse-engineer@latest clean $ARGUMENTS\\`\n2. DO NOT add ANY flags the user did not explicitly type\n3. If user typed nothing after \\`COMMAND_PREFIXclean\\`, run with ZERO flags\n\n**Display version**: Read \\`VERSION_FILE_PATH\\` and show the user: \\`agents-reverse-engineer vX.Y.Z\\`\n\n\\`\\`\\`bash\nnpx agents-reverse-engineer@latest clean $ARGUMENTS\n\\`\\`\\`\n\nReport number of files deleted.\n</execution>`,\n  },\n\n  specify: {\n    description: 'Generate project specification from AGENTS.md docs',\n    argumentHint: '[path] [--dry-run] [--output <path>] [--multi-file] [--force] [--debug] [--trace]',\n    content: `Generate a project specification from existing AGENTS.md documentation.\n\n<execution>\nRun the specify command in the background and monitor progress in real time.\n\n## Steps\n\n1. **Display version**: Read \\`VERSION_FILE_PATH\\` and show the user: \\`agents-reverse-engineer vX.Y.Z\\`\n\n2. **Run the specify command in the background** using \\`run_in_background: true\\`:\n   \\`\\`\\`bash\n   npx agents-reverse-engineer@latest specify $ARGUMENTS\n   \\`\\`\\`\n\n3. **Monitor progress by polling** \\`.agents-reverse-engineer/progress.log\\`:\n   - Wait ~15 seconds (use \\`sleep 15\\` in Bash), then use the **Read** tool to read \\`.agents-reverse-engineer/progress.log\\` (use the \\`offset\\` parameter to read only the last ~20 lines for long files)\n   - Show the user a brief progress update\n   - Check whether the background task has completed using \\`TaskOutput\\` with \\`block: false\\`\n   - Repeat until the background task finishes\n   - **Important**: Keep polling even if progress.log doesn't exist yet (the command takes a few seconds to start writing)\n\n4. **On completion**, read the full background task output and summarize:\n   - Number of AGENTS.md files collected\n   - Output file(s) written\n\nThis collects all AGENTS.md files, synthesizes them via AI, and writes a comprehensive project specification.\n\nIf no AGENTS.md files exist, it will auto-run \\`generate\\` first.\n\n**Options:**\n- \\`--dry-run\\`: Show input statistics without making AI calls\n- \\`--output <path>\\`: Custom output path (default: specs/SPEC.md)\n- \\`--multi-file\\`: Split specification into multiple files\n- \\`--force\\`: Overwrite existing specification\n- \\`--debug\\`: Show AI prompts and backend details\n- \\`--trace\\`: Enable concurrency tracing to \\`.agents-reverse-engineer/traces/\\`\n</execution>`,\n  },\n\n  help: {\n    description: 'Show available ARE commands and usage guide',\n    argumentHint: '',\n    // Content uses COMMAND_PREFIX placeholder, replaced per platform\n    content: `<objective>\nDisplay the complete ARE command reference.\n\n**First**: Read \\`VERSION_FILE_PATH\\` and show the user the version: \\`agents-reverse-engineer vX.Y.Z\\`\n\n**Then**: Output ONLY the reference content below. Do NOT add:\n- Project-specific analysis\n- Git status or file context\n- Next-step suggestions\n- Any commentary beyond the reference\n</objective>\n\n<reference>\n# agents-reverse-engineer (ARE) Command Reference\n\n**ARE** generates AI-friendly documentation for codebases, creating structured summaries optimized for AI assistants.\n\n## Quick Start\n\n1. \\`COMMAND_PREFIXinit\\` — Create configuration file\n2. \\`COMMAND_PREFIXgenerate\\` — Generate documentation for the codebase\n3. \\`COMMAND_PREFIXupdate\\` — Keep docs in sync after code changes\n\n## Commands Reference\n\n### \\`COMMAND_PREFIXinit\\`\nInitialize configuration in this project.\n\nCreates \\`.agents-reverse-engineer/config.yaml\\` with customizable settings.\n\n**Usage:** \\`COMMAND_PREFIXinit\\`\n**CLI:** \\`npx are init\\`\n\n---\n\n### \\`COMMAND_PREFIXdiscover\\`\nDiscover files that would be analyzed for documentation.\n\nShows included files, excluded files with reasons, and generates a \\`GENERATION-PLAN.md\\` execution plan.\n\n**Options:**\n| Flag | Description |\n|------|-------------|\n| \\`[path]\\` | Target directory (default: current directory) |\n| \\`--debug\\` | Show verbose debug output |\n| \\`--trace\\` | Enable concurrency tracing to \\`.agents-reverse-engineer/traces/\\` |\n**Usage:**\n- \\`COMMAND_PREFIXdiscover\\` — Discover files and generate execution plan\n\n**CLI:**\n\\`\\`\\`bash\nnpx are discover\nnpx are discover ./src\n\\`\\`\\`\n\n---\n\n### \\`COMMAND_PREFIXgenerate\\`\nGenerate comprehensive documentation for the codebase.\n\n**Options:**\n| Flag | Description |\n|------|-------------|\n| \\`[path]\\` | Target directory (default: current directory) |\n| \\`--concurrency N\\` | Number of concurrent AI calls (default: auto) |\n| \\`--dry-run\\` | Show what would be generated without writing |\n| \\`--fail-fast\\` | Stop on first file analysis failure |\n| \\`--debug\\` | Show AI prompts and backend details |\n| \\`--trace\\` | Enable concurrency tracing to \\`.agents-reverse-engineer/traces/\\` |\n**Usage:**\n- \\`COMMAND_PREFIXgenerate\\` — Generate docs\n- \\`COMMAND_PREFIXgenerate --dry-run\\` — Preview without writing\n- \\`COMMAND_PREFIXgenerate --concurrency 3\\` — Limit parallel AI calls\n\n**CLI:**\n\\`\\`\\`bash\nnpx are generate\nnpx are generate --dry-run\nnpx are generate ./my-project --concurrency 3\nnpx are generate --debug --trace\n\\`\\`\\`\n\n**How it works:**\n1. Discovers files, applies filters, detects file types, and creates a generation plan\n2. Analyzes each file via concurrent AI calls, writes \\`.sum\\` summary files\n3. Generates \\`AGENTS.md\\` for each directory (post-order traversal)\n4. Creates root document: \\`CLAUDE.md\\`\n\n---\n\n### \\`COMMAND_PREFIXupdate\\`\nIncrementally update documentation for changed files.\n\n**Options:**\n| Flag | Description |\n|------|-------------|\n| \\`[path]\\` | Target directory (default: current directory) |\n| \\`--uncommitted\\` | Include staged but uncommitted changes |\n| \\`--dry-run\\` | Show what would be updated without writing |\n| \\`--concurrency N\\` | Number of concurrent AI calls (default: auto) |\n| \\`--fail-fast\\` | Stop on first file analysis failure |\n| \\`--debug\\` | Show AI prompts and backend details |\n| \\`--trace\\` | Enable concurrency tracing to \\`.agents-reverse-engineer/traces/\\` |\n**Usage:**\n- \\`COMMAND_PREFIXupdate\\` — Update docs for committed changes\n- \\`COMMAND_PREFIXupdate --uncommitted\\` — Include uncommitted changes\n\n**CLI:**\n\\`\\`\\`bash\nnpx are update\nnpx are update --uncommitted\nnpx are update --dry-run\nnpx are update ./my-project --concurrency 3\n\\`\\`\\`\n\n---\n\n### \\`COMMAND_PREFIXspecify\\`\nGenerate a project specification from AGENTS.md documentation.\n\nCollects all AGENTS.md files, synthesizes them via AI, and writes a comprehensive project specification. Auto-runs \\`generate\\` if no AGENTS.md files exist.\n\n**Options:**\n| Flag | Description |\n|------|-------------|\n| \\`[path]\\` | Target directory (default: current directory) |\n| \\`--output <path>\\` | Custom output path (default: specs/SPEC.md) |\n| \\`--multi-file\\` | Split specification into multiple files |\n| \\`--force\\` | Overwrite existing specification |\n| \\`--dry-run\\` | Show input statistics without making AI calls |\n| \\`--debug\\` | Show AI prompts and backend details |\n| \\`--trace\\` | Enable concurrency tracing to \\`.agents-reverse-engineer/traces/\\` |\n**Usage:**\n- \\`COMMAND_PREFIXspecify\\` — Generate specification\n- \\`COMMAND_PREFIXspecify --dry-run\\` — Preview without calling AI\n- \\`COMMAND_PREFIXspecify --output ./docs/spec.md --force\\` — Custom output path\n\n**CLI:**\n\\`\\`\\`bash\nnpx are specify\nnpx are specify --dry-run\nnpx are specify --output ./docs/spec.md --force\nnpx are specify --multi-file\n\\`\\`\\`\n\n---\n\n### \\`COMMAND_PREFIXclean\\`\nRemove all generated documentation artifacts.\n\n**Options:**\n| Flag | Description |\n|------|-------------|\n| \\`--dry-run\\` | Show what would be deleted without deleting |\n\n**What gets deleted:**\n- \\`.agents-reverse-engineer/GENERATION-PLAN.md\\`\n- All \\`*.sum\\` files\n- All \\`AGENTS.md\\` files\n- Root docs: \\`CLAUDE.md\\`\n\n**Usage:**\n- \\`COMMAND_PREFIXclean --dry-run\\` — Preview deletions\n- \\`COMMAND_PREFIXclean\\` — Delete all artifacts\n\n**CLI:**\n\\`\\`\\`bash\nnpx are clean --dry-run\nnpx are clean\n\\`\\`\\`\n\n---\n\n### \\`COMMAND_PREFIXhelp\\`\nShow this command reference.\n\n## CLI Installation\n\nInstall ARE commands to your AI assistant:\n\n\\`\\`\\`bash\nnpx agents-reverse-engineer install              # Interactive mode\nnpx agents-reverse-engineer install --runtime claude -g  # Global Claude\nnpx agents-reverse-engineer install --runtime claude -l  # Local project\nnpx agents-reverse-engineer install --runtime all -g     # All runtimes\n\\`\\`\\`\n\n**Install/Uninstall Options:**\n| Flag | Description |\n|------|-------------|\n| \\`--runtime <name>\\` | Target: \\`claude\\`, \\`opencode\\`, \\`gemini\\`, \\`all\\` |\n| \\`-g, --global\\` | Install to global config directory |\n| \\`-l, --local\\` | Install to current project directory |\n| \\`--force\\` | Overwrite existing files (install only) |\n\n## Configuration\n\n**File:** \\`.agents-reverse-engineer/config.yaml\\`\n\n\\`\\`\\`yaml\n# Exclusion patterns\nexclude:\n  patterns:\n    - \"**/*.test.ts\"\n    - \"**/__mocks__/**\"\n  vendorDirs:\n    - node_modules\n    - dist\n    - .git\n  binaryExtensions:\n    - .png\n    - .jpg\n    - .pdf\n\n# Options\noptions:\n  followSymlinks: false\n  maxFileSize: 100000\n\n# Output settings\noutput:\n  colors: true\n\\`\\`\\`\n\n## Generated Files\n\n### Per Source File\n\n**\\`*.sum\\`** — File summaries with YAML frontmatter + detailed prose.\n\n\\`\\`\\`yaml\n---\nfile_type: service\ngenerated_at: 2025-01-15T10:30:00Z\ncontent_hash: abc123...\npurpose: Handles user authentication and session management\npublic_interface: [login(), logout(), refreshToken(), AuthService]\ndependencies: [express, jsonwebtoken, ./user-model]\npatterns: [singleton, factory, observer]\nrelated_files: [./types.ts, ./middleware.ts]\n---\n\n<300-500 word summary covering implementation, patterns, edge cases>\n\\`\\`\\`\n\n### Per Directory\n\n**\\`AGENTS.md\\`** — Directory overview synthesized from \\`.sum\\` files. Groups files by purpose and links to subdirectories.\n\n### Root Documents\n\n| File | Purpose |\n|------|---------|\n| \\`CLAUDE.md\\` | Project entry point — synthesizes all AGENTS.md |\n\n## Common Workflows\n\n**Initial documentation:**\n\\`\\`\\`\nCOMMAND_PREFIXinit\nCOMMAND_PREFIXgenerate\n\\`\\`\\`\n\n**After code changes:**\n\\`\\`\\`\nCOMMAND_PREFIXupdate\n\\`\\`\\`\n\n**Full regeneration:**\n\\`\\`\\`\nCOMMAND_PREFIXclean\nCOMMAND_PREFIXgenerate\n\\`\\`\\`\n\n**Preview before generating:**\n\\`\\`\\`\nCOMMAND_PREFIXdiscover                   # Check files and exclusions\nCOMMAND_PREFIXgenerate --dry-run         # Preview generation\n\\`\\`\\`\n\n## Tips\n\n- **Custom exclusions**: Edit \\`.agents-reverse-engineer/config.yaml\\` to skip files\n- **Hook auto-update**: Install creates a session-end hook that auto-runs update\n\n## Resources\n\n- **Repository:** https://github.com/GeoloeG-IsT/agents-reverse-engineer\n- **Update:** \\`npx agents-reverse-engineer@latest install --force\\`\n</reference>`,\n  },\n} as const;\n\n// =============================================================================\n// Platform-specific template generators\n// =============================================================================\n\ntype Platform = 'claude' | 'opencode' | 'gemini';\n\ninterface PlatformConfig {\n  commandPrefix: string; // /are- (claude, opencode) or /are: (gemini)\n  pathPrefix: string; // .claude/commands/are/ or .opencode/commands/ etc\n  filenameSeparator: string; // . or -\n  extraFrontmatter?: string; // e.g., \"agent: build\" for OpenCode\n  usesName: boolean; // Claude uses \"name:\" in frontmatter\n  versionFilePath: string; // .claude/ARE-VERSION, .opencode/ARE-VERSION, etc.\n}\n\nconst PLATFORM_CONFIGS: Record<Platform, PlatformConfig> = {\n  claude: {\n    commandPrefix: '/are-',\n    pathPrefix: '.claude/skills/',\n    filenameSeparator: '.',\n    usesName: true,\n    versionFilePath: '.claude/ARE-VERSION',\n  },\n  opencode: {\n    commandPrefix: '/are-',\n    pathPrefix: '.opencode/commands/',\n    filenameSeparator: '-',\n    extraFrontmatter: 'agent: build',\n    usesName: false,\n    versionFilePath: '.opencode/ARE-VERSION',\n  },\n  gemini: {\n    commandPrefix: '/are-',\n    pathPrefix: '.gemini/commands/',\n    filenameSeparator: '-',\n    usesName: false,\n    versionFilePath: '.gemini/ARE-VERSION',\n  },\n};\n\nfunction buildFrontmatter(\n  platform: Platform,\n  commandName: string,\n  description: string\n): string {\n  const config = PLATFORM_CONFIGS[platform];\n  const lines = ['---'];\n\n  if (config.usesName) {\n    lines.push(`name: are-${commandName}`);\n  }\n\n  lines.push(`description: ${description}`);\n\n  if (config.extraFrontmatter) {\n    lines.push(config.extraFrontmatter);\n  }\n\n  lines.push('---');\n  return lines.join('\\n');\n}\n\n/**\n * Build TOML content for Gemini CLI commands\n *\n * Gemini uses TOML format with description and prompt fields.\n * See: https://geminicli.com/docs/cli/custom-commands/\n */\nfunction buildGeminiToml(\n  commandName: string,\n  command: (typeof COMMANDS)[keyof typeof COMMANDS]\n): string {\n  const config = PLATFORM_CONFIGS.gemini;\n  // Replace placeholders in content\n  const promptContent = command.content\n    .replace(/COMMAND_PREFIX/g, config.commandPrefix)\n    .replace(/VERSION_FILE_PATH/g, config.versionFilePath);\n\n  // Build TOML content\n  // Use triple quotes for multi-line prompt\n  const lines = [`description = \"${command.description}\"`];\n\n  if (command.argumentHint) {\n    lines.push(`# Arguments: ${command.argumentHint}`);\n  }\n\n  lines.push(`prompt = \"\"\"`);\n  lines.push(promptContent);\n  lines.push(`\"\"\"`);\n\n  return lines.join('\\n');\n}\n\nfunction buildTemplate(\n  platform: Platform,\n  commandName: string,\n  command: (typeof COMMANDS)[keyof typeof COMMANDS]\n): IntegrationTemplate {\n  const config = PLATFORM_CONFIGS[platform];\n\n  // Platform-specific file naming:\n  // - Claude: .claude/skills/are-{command}/SKILL.md\n  // - OpenCode: .opencode/commands/are-{command}.md\n  // - Gemini: .gemini/commands/are-{command}.toml (TOML format)\n  if (platform === 'gemini') {\n    const filename = `are-${commandName}.toml`;\n    const path = `${config.pathPrefix}${filename}`;\n    const content = buildGeminiToml(commandName, command);\n\n    return {\n      filename,\n      path,\n      content: `${content}\\n`,\n    };\n  }\n\n  const filename = platform === 'claude' ? 'SKILL.md' : `are-${commandName}.md`;\n  const path =\n    platform === 'claude'\n      ? `${config.pathPrefix}are-${commandName}/${filename}`\n      : `${config.pathPrefix}${filename}`;\n\n  const frontmatter = buildFrontmatter(\n    platform,\n    commandName,\n    command.description\n  );\n\n  // Replace placeholders in content\n  const content = command.content\n    .replace(/COMMAND_PREFIX/g, config.commandPrefix)\n    .replace(/VERSION_FILE_PATH/g, config.versionFilePath);\n\n  return {\n    filename,\n    path,\n    content: `${frontmatter}\\n\\n${content}\\n`,\n  };\n}\n\nfunction getTemplatesForPlatform(platform: Platform): IntegrationTemplate[] {\n  return Object.entries(COMMANDS).map(([name, command]) =>\n    buildTemplate(platform, name, command)\n  );\n}\n\n// =============================================================================\n// Public API\n// =============================================================================\n\n/**\n * Get Claude Code command file templates\n */\nexport function getClaudeTemplates(): IntegrationTemplate[] {\n  return getTemplatesForPlatform('claude');\n}\n\n/**\n * Get OpenCode command file templates\n */\nexport function getOpenCodeTemplates(): IntegrationTemplate[] {\n  return getTemplatesForPlatform('opencode');\n}\n\n/**\n * Get Gemini CLI command file templates\n */\nexport function getGeminiTemplates(): IntegrationTemplate[] {\n  return getTemplatesForPlatform('gemini');\n}\n\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**templates.ts generates platform-specific command file content (SKILL.md, .md, .toml) for Claude Code, OpenCode, and Gemini CLI by substituting command prefix and version file path placeholders into shared command templates, then exporting via getClaudeTemplates(), getOpenCodeTemplates(), and getGeminiTemplates().**\n\n## Exported Functions\n\n- `getClaudeTemplates(): IntegrationTemplate[]` — returns Claude Code SKILL.md templates for all commands\n- `getOpenCodeTemplates(): IntegrationTemplate[]` — returns OpenCode .md command templates\n- `getGeminiTemplates(): IntegrationTemplate[]` — returns Gemini .toml command templates\n\n## Command Definitions\n\n`COMMANDS` object defines six command templates: `generate`, `update`, `init`, `discover`, `clean`, `specify`, `help`. Each has `description`, `argumentHint`, and `content` properties. Content uses placeholder strings `COMMAND_PREFIX` (replaced with `/are-` for all platforms), `VERSION_FILE_PATH` (replaced with `.claude/ARE-VERSION`, `.opencode/ARE-VERSION`, or `.gemini/ARE-VERSION`), and `$ARGUMENTS` (preserves user-supplied CLI flags).\n\n## Platform Configuration\n\n`PLATFORM_CONFIGS` maps `Platform` type (`'claude' | 'opencode' | 'gemini'`) to `PlatformConfig` objects:\n\n- **Claude**: `commandPrefix: '/are-'`, `pathPrefix: '.claude/skills/'`, `filenameSeparator: '.'`, `usesName: true`, `versionFilePath: '.claude/ARE-VERSION'`\n- **OpenCode**: `commandPrefix: '/are-'`, `pathPrefix: '.opencode/commands/'`, `filenameSeparator: '-'`, `extraFrontmatter: 'agent: build'`, `usesName: false`, `versionFilePath: '.opencode/ARE-VERSION'`\n- **Gemini**: `commandPrefix: '/are-'`, `pathPrefix: '.gemini/commands/'`, `filenameSeparator: '-'`, `usesName: false`, `versionFilePath: '.gemini/ARE-VERSION'`\n\n## Template Generation Strategy\n\n`buildTemplate()` constructs `IntegrationTemplate` with `filename`, `path`, and `content` properties. Claude produces `.claude/skills/are-{command}/SKILL.md`, OpenCode produces `.opencode/commands/are-{command}.md`, Gemini produces `.gemini/commands/are-{command}.toml`. `buildFrontmatter()` inserts YAML frontmatter with `name:` field only when `PlatformConfig.usesName` is true (Claude only), adds `description:` field for all platforms, appends `extraFrontmatter` if present (OpenCode's `agent: build`). `buildGeminiToml()` generates TOML format with `description = \"...\"` and triple-quoted `prompt = \"\"\"...\"\"\"` fields wrapping command content.\n\n## Command Content Patterns\n\nAll command templates except `help` and `clean` include background execution instructions using `run_in_background: true` with `TaskOutput` polling. Monitoring pattern appears verbatim in `generate`, `update`, `discover`, `specify` templates:\n\n```\n3. **Monitor progress by polling** `.agents-reverse-engineer/progress.log`:\n   - Wait ~15 seconds (use `sleep 15` in Bash), then use the **Read** tool to read `.agents-reverse-engineer/progress.log` (use the `offset` parameter to read only the last ~20 lines for long files)\n   - Show the user a brief progress update\n   - Check whether the background task has completed using `TaskOutput` with `block: false`\n   - Repeat until the background task finishes\n   - **Important**: Keep polling even if progress.log doesn't exist yet (the command takes a few seconds to start writing)\n```\n\n`discover` and `clean` commands include strict rule blocks enforcing zero flag addition beyond user input:\n\n```\n## STRICT RULES - VIOLATION IS FORBIDDEN\n\n1. Run ONLY this exact command: `npx agents-reverse-engineer@latest COMMAND $ARGUMENTS`\n2. DO NOT add ANY flags the user did not explicitly type\n3. If user typed nothing after `COMMAND_PREFIXcommand`, run with ZERO flags\n```\n\n`help` command template contains full reference documentation with command tables, configuration schema, generated file examples, and common workflows. Uses `<objective>` and `<reference>` XML tags. Instructs AI to read `VERSION_FILE_PATH` first and output reference without project-specific analysis.\n\n## Path Construction\n\nClaude commands use nested directory structure: `are-{command}/SKILL.md` under `.claude/skills/`. OpenCode and Gemini use flat structure: `are-{command}.md` or `are-{command}.toml` directly under `.opencode/commands/` or `.gemini/commands/`. `getTemplatesForPlatform()` iterates `COMMANDS` object entries, calls `buildTemplate()` for each, returns array of `IntegrationTemplate` objects consumed by installer module for file writes.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 1208,
      "cacheReadTokens": 17114,
      "cacheCreationTokens": 15855,
      "latencyMs": 39839,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/integration/templates.ts",
          "sizeBytes": 22502
        }
      ]
    },
    {
      "timestamp": "2026-02-09T16:10:35.937Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/output/logger.ts\n\n```typescript\n/**\n * Terminal logger for agents-reverse\n *\n * Provides colored output.\n * Output format follows CONTEXT.md human-readable specification.\n */\n\nimport pc from 'picocolors';\n\n/**\n * Logger interface for CLI output.\n */\nexport interface Logger {\n  /** Log an informational message */\n  info(message: string): void;\n\n  /** Log a discovered file */\n  file(path: string): void;\n\n  /** Log an excluded file with reason */\n  excluded(path: string, reason: string, filter: string): void;\n\n  /** Log discovery summary */\n  summary(included: number, excluded: number): void;\n\n  /** Log a warning message */\n  warn(message: string): void;\n\n  /** Log an error message */\n  error(message: string): void;\n}\n\n/**\n * Options for creating a logger instance.\n */\nexport interface LoggerOptions {\n  /**\n   * Use colors in terminal output.\n   * @default true\n   */\n  colors: boolean;\n}\n\n/**\n * Color functions type - either picocolors or identity functions\n */\ninterface ColorFunctions {\n  green: (s: string) => string;\n  dim: (s: string) => string;\n  red: (s: string) => string;\n  bold: (s: string) => string;\n  yellow: (s: string) => string;\n}\n\n/**\n * Identity function for no-color mode\n */\nconst identity = (s: string): string => s;\n\n/**\n * No-color formatter that returns strings unchanged\n */\nconst noColor: ColorFunctions = {\n  green: identity,\n  dim: identity,\n  red: identity,\n  bold: identity,\n  yellow: identity,\n};\n\n/**\n * Create a logger instance with the given options.\n *\n * Output format per CONTEXT.md (human-readable):\n * - file: green \"  +\" prefix + relative path\n * - excluded: dim \"  -\" prefix + path + reason (when shown)\n * - summary: bold count + dim excluded count\n * - warn: yellow \"Warning:\" prefix\n * - error: red \"Error:\" prefix\n *\n * @param options - Logger configuration\n * @returns Logger instance\n *\n * @example\n * ```typescript\n * const log = createLogger({ colors: true });\n *\n * log.file('src/index.ts');\n * log.summary(42, 10);\n * ```\n */\nexport function createLogger(options: LoggerOptions): Logger {\n  const c: ColorFunctions = options.colors ? pc : noColor;\n\n  return {\n    info(message: string): void {\n      console.log(message);\n    },\n\n    file(path: string): void {\n      console.log(c.green('  +') + ' ' + path);\n    },\n\n    excluded(path: string, reason: string, filter: string): void {\n      console.log(c.dim('  -') + ' ' + path + c.dim(` (${reason}: ${filter})`));\n    },\n\n    summary(included: number, excluded: number): void {\n      console.log(\n        c.bold(`\\nDiscovered ${included} files`) +\n          c.dim(` (${excluded} excluded)`)\n      );\n    },\n\n    warn(message: string): void {\n      console.warn(c.yellow('Warning: ') + message);\n    },\n\n    error(message: string): void {\n      console.error(c.red('Error: ') + message);\n    },\n  };\n}\n\n/**\n * Create a silent logger that produces no output.\n *\n * Useful for testing or programmatic usage.\n *\n * @returns Logger instance with all no-op methods\n */\nexport function createSilentLogger(): Logger {\n  const noop = (): void => {};\n  return {\n    info: noop,\n    file: noop,\n    excluded: noop,\n    summary: noop,\n    warn: noop,\n    error: noop,\n  };\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**Terminal logging facade with picocolors-based output formatting for CLI discovery, exclusion, summary, warning, and error messages.**\n\n## Exported Interface\n\n`Logger` interface defines six methods:\n- `info(message: string): void` — Log informational message\n- `file(path: string): void` — Log discovered file\n- `excluded(path: string, reason: string, filter: string): void` — Log excluded file with reason and filter name\n- `summary(included: number, excluded: number): void` — Log discovery summary with counts\n- `warn(message: string): void` — Log warning message\n- `error(message: string): void` — Log error message\n\n`LoggerOptions` interface:\n- `colors: boolean` — Enable ANSI color codes (default true)\n\n## Factory Functions\n\n`createLogger(options: LoggerOptions): Logger` constructs logger with conditional color support. When `options.colors` is true, uses `picocolors` (`pc`) for formatting; otherwise uses `noColor` identity functions. Output format per CONTEXT.md specification:\n- `file()`: green `\"  +\"` prefix + path\n- `excluded()`: dim `\"  -\"` prefix + path + dim `\" (${reason}: ${filter})\"`\n- `summary()`: bold `\"\\nDiscovered ${included} files\"` + dim `\" (${excluded} excluded)\"`\n- `warn()`: yellow `\"Warning: \"` prefix + message via `console.warn()`\n- `error()`: red `\"Error: \"` prefix + message via `console.error()`\n- `info()`: plain `console.log()` without formatting\n\n`createSilentLogger(): Logger` returns no-op logger with all methods as empty functions. Used for testing or programmatic usage where output suppression is required.\n\n## Internal Types\n\n`ColorFunctions` interface defines five formatting functions:\n- `green(s: string): string`\n- `dim(s: string): string`\n- `red(s: string): string`\n- `bold(s: string): string`\n- `yellow(s: string): string`\n\n`identity(s: string): string` returns input unchanged.\n\n`noColor: ColorFunctions` constant binds all five color functions to `identity` for no-color mode.\n\n## Dependencies\n\nImports `picocolors` as `pc` for ANSI color code generation. Falls back to `noColor` identity functions when `options.colors` is false.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 566,
      "cacheReadTokens": 17114,
      "cacheCreationTokens": 10076,
      "latencyMs": 25066,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/output/logger.ts",
          "sizeBytes": 3144
        }
      ]
    },
    {
      "timestamp": "2026-02-09T16:10:44.422Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/specify/index.ts\n\n```typescript\nexport { buildSpecPrompt } from './prompts.js';\nexport type { SpecPrompt } from './prompts.js';\n\nexport { writeSpec, SpecExistsError } from './writer.js';\nexport type { WriteSpecOptions } from './writer.js';\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**Barrel module exporting project specification generation capabilities: buildSpecPrompt for prompt construction, writeSpec for file system output with collision detection.**\n\n## Exported Functions\n\n- `buildSpecPrompt` — constructs AI prompt from AGENTS.md corpus (from `./prompts.js`)\n- `writeSpec` — writes synthesized specification to disk with overwrite protection (from `./writer.js`)\n\n## Exported Types\n\n- `SpecPrompt` — prompt structure returned by buildSpecPrompt (from `./prompts.js`)\n- `WriteSpecOptions` — configuration for writeSpec operation (from `./writer.js`)\n\n## Exported Errors\n\n- `SpecExistsError` — thrown by writeSpec when target file exists and `force: false` (from `./writer.js`)\n\n## Module Role\n\nEntry point for `/are-specify` command implementation (invoked from `src/cli/specify.ts`). Aggregates AGENTS.md files via buildSpecPrompt, generates unified project specification via AI synthesis, writes output to `specs/` directory via writeSpec. Supports single-file (`specs/SPEC.md`) and multi-file (`specs/<dirname>.md`) output modes.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 271,
      "cacheReadTokens": 17114,
      "cacheCreationTokens": 9157,
      "latencyMs": 19580,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/specify/index.ts",
          "sizeBytes": 208
        }
      ]
    },
    {
      "timestamp": "2026-02-09T16:10:33.269Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/orchestration/types.ts\n\n```typescript\n/**\n * Shared types for the orchestration module.\n *\n * These types are used across the concurrency pool, progress reporter,\n * and command runner to represent task results, run summaries, progress\n * events, and command options.\n *\n * @module\n */\n\nimport type { InconsistencyReport } from '../quality/index.js';\nimport type { ProgressLog } from './progress.js';\nimport type { ITraceWriter } from './trace.js';\n\n// ---------------------------------------------------------------------------\n// File task result\n// ---------------------------------------------------------------------------\n\n/**\n * Result of processing a single file through AI analysis.\n *\n * Produced by the command runner for each file task, carrying token\n * counts and timing data needed for the run summary.\n */\nexport interface FileTaskResult {\n  /** Relative path to the source file */\n  path: string;\n  /** Whether the AI call succeeded */\n  success: boolean;\n  /** Number of input tokens consumed (non-cached) */\n  tokensIn: number;\n  /** Number of output tokens generated */\n  tokensOut: number;\n  /** Number of cache read input tokens */\n  cacheReadTokens: number;\n  /** Number of cache creation input tokens */\n  cacheCreationTokens: number;\n  /** Wall-clock duration in milliseconds */\n  durationMs: number;\n  /** Model identifier used for this call */\n  model: string;\n  /** Error message if the call failed */\n  error?: string;\n}\n\n// ---------------------------------------------------------------------------\n// Run summary\n// ---------------------------------------------------------------------------\n\n/**\n * Aggregated summary of a command run.\n *\n * Produced at the end of a generate or update command execution,\n * combining per-file results into totals for display and telemetry.\n */\nexport interface RunSummary {\n  /** agents-reverse-engineer version that produced this run */\n  version: string;\n  /** Number of files that were successfully processed */\n  filesProcessed: number;\n  /** Number of files that failed processing */\n  filesFailed: number;\n  /** Number of files that were skipped (e.g., dry-run) */\n  filesSkipped: number;\n  /** Total number of AI calls made */\n  totalCalls: number;\n  /** Sum of input tokens across all calls */\n  totalInputTokens: number;\n  /** Sum of output tokens across all calls */\n  totalOutputTokens: number;\n  /** Sum of cache read tokens across all calls */\n  totalCacheReadTokens: number;\n  /** Sum of cache creation tokens across all calls */\n  totalCacheCreationTokens: number;\n  /** Total wall-clock duration in milliseconds */\n  totalDurationMs: number;\n  /** Number of errors encountered */\n  errorCount: number;\n  /** Number of retries that occurred */\n  retryCount: number;\n  /** Total file reads across all calls */\n  totalFilesRead: number;\n  /** Unique files read (deduped by path) */\n  uniqueFilesRead: number;\n  /** Number of code-vs-doc inconsistencies detected */\n  inconsistenciesCodeVsDoc?: number;\n  /** Number of code-vs-code inconsistencies detected */\n  inconsistenciesCodeVsCode?: number;\n  /** Number of phantom path references detected in AGENTS.md files */\n  phantomPaths?: number;\n  /** Full inconsistency report (undefined if no checks ran) */\n  inconsistencyReport?: InconsistencyReport;\n}\n\n// ---------------------------------------------------------------------------\n// Progress events\n// ---------------------------------------------------------------------------\n\n/**\n * Event emitted by the command runner to the progress reporter.\n *\n * Each event type carries different optional fields:\n * - `start`: filePath, index, total\n * - `done`: filePath, index, total, durationMs, tokensIn, tokensOut, model\n * - `error`: filePath, index, total, error\n * - `dir-done`: filePath (directory path)\n * - `root-done`: filePath (root document path)\n */\nexport interface ProgressEvent {\n  /** Event type */\n  type: 'start' | 'done' | 'error' | 'dir-done' | 'root-done';\n  /** File or directory path */\n  filePath: string;\n  /** Zero-based index of this task in the current phase */\n  index: number;\n  /** Total number of tasks in the current phase */\n  total: number;\n  /** Wall-clock duration in milliseconds (for 'done' events) */\n  durationMs?: number;\n  /** Input tokens consumed (for 'done' events) */\n  tokensIn?: number;\n  /** Output tokens generated (for 'done' events) */\n  tokensOut?: number;\n  /** Model identifier (for 'done' events) */\n  model?: string;\n  /** Error message (for 'error' events) */\n  error?: string;\n}\n\n// ---------------------------------------------------------------------------\n// Command run options\n// ---------------------------------------------------------------------------\n\n/**\n * Options that control how commands execute.\n *\n * These are populated from a combination of config file defaults\n * and CLI flag overrides.\n */\nexport interface CommandRunOptions {\n  /** Maximum number of concurrent AI calls */\n  concurrency: number;\n  /** Stop pulling new tasks on first error */\n  failFast?: boolean;\n  /** Show debug information (exact prompts sent) */\n  debug?: boolean;\n  /** List files that would be processed without executing */\n  dryRun?: boolean;\n  /** Trace writer for concurrency debugging (no-op when tracing is off) */\n  tracer?: ITraceWriter;\n  /** Progress log for file-based output mirroring (tail -f monitoring) */\n  progressLog?: ProgressLog;\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**Shared type definitions for orchestration module: FileTaskResult carries per-file AI call metrics (tokensIn/tokensOut/cacheReadTokens/cacheCreationTokens/durationMs/model), RunSummary aggregates cross-command totals with quality metrics (inconsistenciesCodeVsDoc/inconsistenciesCodeVsCode/phantomPaths), ProgressEvent discriminates five event types (start/done/error/dir-done/root-done) with type-specific optional fields, CommandRunOptions threads concurrency/failFast/debug/dryRun/tracer/progressLog through pool/runner/AIService.**\n\n## Exported Types\n\n### FileTaskResult\nPer-file AI analysis outcome returned by command runner workers:\n```typescript\ninterface FileTaskResult {\n  path: string;              // Relative source file path\n  success: boolean;          // Whether AI call succeeded\n  tokensIn: number;          // Input tokens (non-cached)\n  tokensOut: number;         // Output tokens generated\n  cacheReadTokens: number;   // Cached input tokens\n  cacheCreationTokens: number; // Cache creation tokens\n  durationMs: number;        // Wall-clock duration\n  model: string;             // Model identifier used\n  error?: string;            // Error message on failure\n}\n```\n\n### RunSummary\nAggregated metrics spanning entire generate/update command execution:\n```typescript\ninterface RunSummary {\n  version: string;                     // ARE version\n  filesProcessed: number;              // Successful file count\n  filesFailed: number;                 // Failed file count\n  filesSkipped: number;                // Skipped file count (dry-run)\n  totalCalls: number;                  // AI call count\n  totalInputTokens: number;            // Sum of tokensIn\n  totalOutputTokens: number;           // Sum of tokensOut\n  totalCacheReadTokens: number;        // Sum of cacheReadTokens\n  totalCacheCreationTokens: number;    // Sum of cacheCreationTokens\n  totalDurationMs: number;             // Total wall-clock time\n  errorCount: number;                  // Error count\n  retryCount: number;                  // Retry attempt count\n  totalFilesRead: number;              // Total file reads across calls\n  uniqueFilesRead: number;             // Deduplicated file reads\n  inconsistenciesCodeVsDoc?: number;   // Code-vs-doc issue count\n  inconsistenciesCodeVsCode?: number;  // Code-vs-code issue count\n  phantomPaths?: number;               // Phantom path reference count\n  inconsistencyReport?: InconsistencyReport; // Full quality report\n}\n```\n\n### ProgressEvent\nDiscriminated union emitted by runner to progress reporter with five event types:\n```typescript\ninterface ProgressEvent {\n  type: 'start' | 'done' | 'error' | 'dir-done' | 'root-done';\n  filePath: string;   // File or directory path\n  index: number;      // Zero-based task index in phase\n  total: number;      // Total phase task count\n  durationMs?: number; // Wall-clock duration ('done' events)\n  tokensIn?: number;   // Input tokens ('done' events)\n  tokensOut?: number;  // Output tokens ('done' events)\n  model?: string;      // Model identifier ('done' events)\n  error?: string;      // Error message ('error' events)\n}\n```\n\n**Event type field mappings:**\n- `start`: filePath, index, total\n- `done`: filePath, index, total, durationMs, tokensIn, tokensOut, model\n- `error`: filePath, index, total, error\n- `dir-done`: filePath (directory path)\n- `root-done`: filePath (root document path)\n\n### CommandRunOptions\nConfiguration threading object passed from CLI through runner to pool and AIService:\n```typescript\ninterface CommandRunOptions {\n  concurrency: number;       // Max concurrent AI calls\n  failFast?: boolean;        // Stop on first error\n  debug?: boolean;           // Show exact prompts sent\n  dryRun?: boolean;          // Preview without execution\n  tracer?: ITraceWriter;     // NDJSON trace emitter (NullTraceWriter when --trace off)\n  progressLog?: ProgressLog; // File-based progress mirror (tail -f monitoring)\n}\n```\n\n## Cross-Module Dependencies\n\nImports `InconsistencyReport` from `src/quality/index.ts` (embedded in RunSummary.inconsistencyReport), `ProgressLog` from `src/orchestration/progress.ts` (threaded via CommandRunOptions.progressLog), `ITraceWriter` from `src/orchestration/trace.ts` (threaded via CommandRunOptions.tracer).\n\n## Integration with Orchestration Flow\n\nFileTaskResult flows from `src/orchestration/runner.ts` worker callbacks → aggregated into RunSummary by runner.run() → consumed by telemetry logger (`src/ai/telemetry/logger.ts`). ProgressEvent emitted by runner via `progressReporter.emit()` → rendered by `src/orchestration/progress.ts` with ETA calculation. CommandRunOptions populated in CLI entry points (`src/cli/generate.ts`, `src/cli/update.ts`) from config defaults + CLI flag overrides → threaded through executor → pool → AIService.call().",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 1291,
      "cacheReadTokens": 17114,
      "cacheCreationTokens": 10465,
      "latencyMs": 32663,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/orchestration/types.ts",
          "sizeBytes": 5350
        }
      ]
    },
    {
      "timestamp": "2026-02-09T16:10:54.238Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/types/index.ts\n\n```typescript\n/**\n * Shared types used across modules\n */\n\n/**\n * Information about an excluded file\n */\nexport interface ExcludedFile {\n  /** Absolute or relative path to the excluded file */\n  path: string;\n  /** Reason for exclusion (e.g., \"gitignore pattern\", \"binary file\", \"vendor directory\") */\n  reason: string;\n}\n\n/**\n * Result of the file discovery process\n */\nexport interface DiscoveryResult {\n  /** List of file paths that should be analyzed */\n  files: string[];\n  /** List of files that were excluded with reasons */\n  excluded: ExcludedFile[];\n}\n\n/**\n * Statistics about the discovery process\n */\nexport interface DiscoveryStats {\n  /** Total number of files found */\n  totalFiles: number;\n  /** Number of files included for analysis */\n  includedFiles: number;\n  /** Number of files excluded */\n  excludedFiles: number;\n  /** Breakdown of exclusion reasons */\n  exclusionReasons: Record<string, number>;\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**index.ts defines shared TypeScript interfaces for file discovery results, exclusion tracking, and statistics aggregation used across ARE modules.**\n\n## Exported Interfaces\n\n### ExcludedFile\nRepresents a file excluded during discovery with metadata:\n- `path: string` — Absolute or relative path to excluded file\n- `reason: string` — Exclusion rationale (e.g., \"gitignore pattern\", \"binary file\", \"vendor directory\")\n\n### DiscoveryResult\nOutput contract for file discovery process:\n- `files: string[]` — Paths of files selected for analysis\n- `excluded: ExcludedFile[]` — Files rejected with explanatory reasons\n\n### DiscoveryStats\nAggregated metrics from discovery execution:\n- `totalFiles: number` — Count of all files encountered\n- `includedFiles: number` — Count selected for analysis\n- `excludedFiles: number` — Count rejected by filters\n- `exclusionReasons: Record<string, number>` — Histogram mapping reason strings to occurrence counts\n\n## Integration Points\n\nConsumed by `src/discovery/walker.ts` as return types for directory traversal functions. Used by `src/cli/discover.ts` to format console output and write `GENERATION-PLAN.md`. Referenced in `src/orchestration/runner.ts` for telemetry logging of discovery phase metrics.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 308,
      "cacheReadTokens": 17114,
      "cacheCreationTokens": 9346,
      "latencyMs": 16729,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/types/index.ts",
          "sizeBytes": 909
        }
      ]
    },
    {
      "timestamp": "2026-02-09T16:10:31.244Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/orchestration/progress.ts\n\n```typescript\n/**\n * Streaming build-log progress reporter with ETA calculation.\n *\n * Outputs one line per event (start, done, fail, dir-done, root-done)\n * using `console.log` for atomic, non-corrupting concurrent output.\n * Each line shows progress counter, status, file path, timing, and\n * token counts using colored output via `picocolors`.\n *\n * ETA is computed via a moving average of the last 10 completion times,\n * displayed after 2 or more files have completed.\n *\n * Optionally mirrors all output to a plain-text progress log file\n * (`.agents-reverse-engineer/progress.log`) via {@link ProgressLog},\n * enabling `tail -f` monitoring when running inside buffered environments\n * (e.g. Claude Code's Bash tool).\n *\n * @module\n */\n\nimport { open, mkdir } from 'node:fs/promises';\nimport type { FileHandle } from 'node:fs/promises';\nimport * as path from 'node:path';\nimport pc from 'picocolors';\nimport type { RunSummary } from './types.js';\n\n// ---------------------------------------------------------------------------\n// ANSI stripping\n// ---------------------------------------------------------------------------\n\n/** Strip ANSI escape sequences from a string for plain-text output. */\nfunction stripAnsi(str: string): string {\n  // Matches all common ANSI escape codes (SGR, cursor, erase, etc.)\n  // eslint-disable-next-line no-control-regex\n  return str.replace(/\\x1b\\[[0-9;]*m/g, '');\n}\n\n// ---------------------------------------------------------------------------\n// ProgressLog\n// ---------------------------------------------------------------------------\n\n/** Relative path for the progress log file */\nconst PROGRESS_LOG_FILENAME = 'progress.log';\n\n/**\n * Plain-text progress log file writer.\n *\n * Mirrors console progress output to `.agents-reverse-engineer/progress.log`\n * without ANSI escape codes, enabling real-time monitoring via `tail -f`\n * when the CLI runs inside buffered environments (e.g. Claude Code).\n *\n * Uses promise-chain serialization (same pattern as {@link TraceWriter})\n * to handle concurrent writes from multiple pool workers safely.\n *\n * @example\n * ```typescript\n * const log = new ProgressLog('/project/.agents-reverse-engineer/progress.log');\n * log.write('=== ARE Generate (2026-02-09) ===');\n * log.write('[1/10] ANALYZING src/index.ts');\n * await log.finalize();\n * ```\n */\nexport class ProgressLog {\n  private writeQueue: Promise<void> = Promise.resolve();\n  private fd: FileHandle | null = null;\n\n  constructor(private readonly filePath: string) {}\n\n  /**\n   * Create a ProgressLog for a project root.\n   *\n   * @param projectRoot - Absolute path to the project root directory\n   * @returns A new ProgressLog instance\n   */\n  static create(projectRoot: string): ProgressLog {\n    return new ProgressLog(\n      path.join(projectRoot, '.agents-reverse-engineer', PROGRESS_LOG_FILENAME),\n    );\n  }\n\n  /**\n   * Append a line to the progress log file.\n   *\n   * On first call, creates the parent directory and opens the file\n   * in truncate mode ('w'). Subsequent writes append to the open handle.\n   * Write failures are silently swallowed (non-critical telemetry).\n   */\n  write(line: string): void {\n    this.writeQueue = this.writeQueue\n      .then(async () => {\n        if (!this.fd) {\n          await mkdir(path.dirname(this.filePath), { recursive: true });\n          this.fd = await open(this.filePath, 'w');\n        }\n        await this.fd.write(line + '\\n');\n      })\n      .catch(() => { /* non-critical -- progress log loss is acceptable */ });\n  }\n\n  /** Flush all pending writes and close the file handle. */\n  async finalize(): Promise<void> {\n    await this.writeQueue;\n    if (this.fd) {\n      await this.fd.close();\n      this.fd = null;\n    }\n  }\n}\n\n// ---------------------------------------------------------------------------\n// ProgressReporter\n// ---------------------------------------------------------------------------\n\n/**\n * Streaming build-log progress reporter.\n *\n * Create one instance per command run. Call the event methods as files\n * are processed. Call {@link printSummary} at the end of the run.\n *\n * @example\n * ```typescript\n * const reporter = new ProgressReporter(fileCount);\n * reporter.onFileStart('src/index.ts');\n * reporter.onFileDone('src/index.ts', 1200, 500, 300, 'sonnet');\n * reporter.printSummary(summary);\n * ```\n */\nexport class ProgressReporter {\n  /** Total number of file tasks in this run */\n  private readonly totalFiles: number;\n\n  /** Number of files that have started processing */\n  private started: number = 0;\n\n  /** Number of files completed successfully */\n  private completed: number = 0;\n\n  /** Number of files that failed */\n  private failed: number = 0;\n\n  /** Sliding window of recent completion durations for ETA */\n  private readonly completionTimes: number[] = [];\n\n  /** Maximum window size for ETA moving average */\n  private readonly windowSize: number = 10;\n\n  /** Timestamp when the reporter was created */\n  private readonly startTime: number = Date.now();\n\n  /** Total number of directory tasks in this run */\n  private totalDirectories: number = 0;\n\n  /** Number of directory tasks that have started */\n  private dirStarted: number = 0;\n\n  /** Number of directory tasks completed */\n  private dirCompleted: number = 0;\n\n  /** Sliding window of recent directory completion durations for ETA */\n  private readonly dirCompletionTimes: number[] = [];\n\n  /** Optional file-based progress log for tail -f monitoring */\n  private readonly progressLog: ProgressLog | null;\n\n  /**\n   * Create a new progress reporter.\n   *\n   * @param totalFiles - Total number of file tasks to process\n   * @param totalDirectories - Total number of directory tasks to process\n   * @param progressLog - Optional progress log for file-based output mirroring\n   */\n  constructor(totalFiles: number, totalDirectories: number = 0, progressLog?: ProgressLog) {\n    this.totalFiles = totalFiles;\n    this.totalDirectories = totalDirectories;\n    this.progressLog = progressLog ?? null;\n  }\n\n  /**\n   * Log the start of file analysis.\n   *\n   * Output format: `[X/Y] ANALYZING path`\n   *\n   * @param filePath - Relative path to the file being analyzed\n   */\n  onFileStart(filePath: string): void {\n    this.started++;\n    const line = `${pc.dim(`[${this.started}/${this.totalFiles}]`)} ${pc.cyan('ANALYZING')} ${filePath}`;\n    console.log(line);\n    this.progressLog?.write(stripAnsi(line));\n  }\n\n  /**\n   * Log the successful completion of file analysis.\n   *\n   * Output format: `[X/Y] DONE path Xs in/out tok ~Ns remaining`\n   *\n   * Records the completion time for ETA calculation.\n   *\n   * @param filePath - Relative path to the completed file\n   * @param durationMs - Wall-clock duration of the AI call\n   * @param tokensIn - Number of input tokens consumed (non-cached)\n   * @param tokensOut - Number of output tokens generated\n   * @param model - Model identifier used for this call\n   * @param cacheReadTokens - Number of cache read input tokens\n   */\n  onFileDone(\n    filePath: string,\n    durationMs: number,\n    tokensIn: number,\n    tokensOut: number,\n    model: string,\n    cacheReadTokens = 0,\n    cacheCreationTokens = 0,\n  ): void {\n    this.completed++;\n\n    // Record completion time for ETA\n    this.completionTimes.push(durationMs);\n    if (this.completionTimes.length > this.windowSize) {\n      this.completionTimes.shift();\n    }\n\n    const counter = pc.dim(`[${this.completed + this.failed}/${this.totalFiles}]`);\n    const time = pc.dim(`${(durationMs / 1000).toFixed(1)}s`);\n    // Total prompt size = non-cached + cache read + cache creation tokens\n    const totalIn = tokensIn + cacheReadTokens + cacheCreationTokens;\n    const tokens = pc.dim(`${totalIn}/${tokensOut} tok`);\n    const modelLabel = pc.dim(model);\n    const eta = this.formatETA();\n\n    const line = `${counter} ${pc.green('DONE')} ${filePath} ${time} ${tokens} ${modelLabel}${eta}`;\n    console.log(line);\n    this.progressLog?.write(stripAnsi(line));\n  }\n\n  /**\n   * Log a file analysis failure.\n   *\n   * Output format: `[X/Y] FAIL path error`\n   *\n   * @param filePath - Relative path to the failed file\n   * @param error - Error message describing the failure\n   */\n  onFileError(filePath: string, error: string): void {\n    this.failed++;\n\n    const line = `${pc.dim(`[${this.completed + this.failed}/${this.totalFiles}]`)} ${pc.red('FAIL')} ${filePath} ${pc.dim(error)}`;\n    console.log(line);\n    this.progressLog?.write(stripAnsi(line));\n  }\n\n  /**\n   * Log the start of directory AGENTS.md generation.\n   *\n   * Output format: `[dir X/Y] ANALYZING dirPath/AGENTS.md`\n   *\n   * @param dirPath - Path to the directory\n   */\n  onDirectoryStart(dirPath: string): void {\n    this.dirStarted++;\n    const line = `${pc.dim(`[dir ${this.dirStarted}/${this.totalDirectories}]`)} ${pc.cyan('ANALYZING')} ${dirPath}/AGENTS.md`;\n    console.log(line);\n    this.progressLog?.write(stripAnsi(line));\n  }\n\n  /**\n   * Log the completion of directory AGENTS.md generation.\n   *\n   * Output format: `[dir X/Y] DONE dirPath/AGENTS.md Xs in/out tok model ~ETA`\n   *\n   * @param dirPath - Path to the directory\n   * @param durationMs - Wall-clock duration of the AI call\n   * @param tokensIn - Number of input tokens consumed (non-cached)\n   * @param tokensOut - Number of output tokens generated\n   * @param model - Model identifier used for this call\n   * @param cacheReadTokens - Number of cache read input tokens\n   */\n  onDirectoryDone(\n    dirPath: string,\n    durationMs: number,\n    tokensIn: number,\n    tokensOut: number,\n    model: string,\n    cacheReadTokens = 0,\n    cacheCreationTokens = 0,\n  ): void {\n    this.dirCompleted++;\n\n    // Record completion time for directory ETA\n    this.dirCompletionTimes.push(durationMs);\n    if (this.dirCompletionTimes.length > this.windowSize) {\n      this.dirCompletionTimes.shift();\n    }\n\n    const counter = pc.dim(`[dir ${this.dirCompleted}/${this.totalDirectories}]`);\n    const time = pc.dim(`${(durationMs / 1000).toFixed(1)}s`);\n    // Total prompt size = non-cached + cache read + cache creation tokens\n    const totalIn = tokensIn + cacheReadTokens + cacheCreationTokens;\n    const tokens = pc.dim(`${totalIn}/${tokensOut} tok`);\n    const modelLabel = pc.dim(model);\n    const eta = this.formatDirectoryETA();\n\n    const line = `${counter} ${pc.blue('DONE')} ${dirPath}/AGENTS.md ${time} ${tokens} ${modelLabel}${eta}`;\n    console.log(line);\n    this.progressLog?.write(stripAnsi(line));\n  }\n\n  /**\n   * Log the completion of a root document generation.\n   *\n   * Output format: `[root] DONE docPath`\n   *\n   * @param docPath - Path to the root document\n   */\n  onRootDone(docPath: string): void {\n    const line = `${pc.dim('[root]')} ${pc.blue('DONE')} ${docPath}`;\n    console.log(line);\n    this.progressLog?.write(stripAnsi(line));\n  }\n\n  /**\n   * Print the end-of-run summary.\n   *\n   * Shows files processed, token counts, files read with unique dedup,\n   * time elapsed, errors, and retries.\n   *\n   * @param summary - Aggregated run summary\n   */\n  printSummary(summary: RunSummary): void {\n    const elapsed = ((Date.now() - this.startTime) / 1000).toFixed(1);\n\n    const lines: string[] = [];\n    lines.push('');\n    lines.push(pc.bold('=== Run Summary ==='));\n    lines.push(`  ARE version:     ${summary.version}`);\n    lines.push(`  Files processed: ${pc.green(String(summary.filesProcessed))}`);\n    if (summary.filesFailed > 0) {\n      lines.push(`  Files failed:    ${pc.red(String(summary.filesFailed))}`);\n    }\n    if (summary.filesSkipped > 0) {\n      lines.push(`  Files skipped:   ${pc.yellow(String(summary.filesSkipped))}`);\n    }\n    lines.push(`  Total calls:     ${summary.totalCalls}`);\n    const totalIn = summary.totalInputTokens + summary.totalCacheReadTokens + summary.totalCacheCreationTokens;\n    lines.push(`  Tokens:          ${totalIn} in / ${summary.totalOutputTokens} out`);\n    if (summary.totalCacheReadTokens > 0) {\n      lines.push(`  Cache:           ${summary.totalCacheReadTokens} read / ${summary.totalCacheCreationTokens} created`);\n    }\n\n    if (summary.totalFilesRead > 0) {\n      lines.push(`  Files read:      ${summary.totalFilesRead} (${summary.uniqueFilesRead} unique)`);\n    }\n\n    lines.push(`  Total time:      ${elapsed}s`);\n    lines.push(`  Errors:          ${summary.errorCount}`);\n    if (summary.retryCount > 0) {\n      lines.push(`  Retries:         ${summary.retryCount}`);\n    }\n\n    for (const line of lines) {\n      console.log(line);\n      this.progressLog?.write(stripAnsi(line));\n    }\n  }\n\n  // -------------------------------------------------------------------------\n  // ETA calculation\n  // -------------------------------------------------------------------------\n\n  /**\n   * Compute and format the estimated time remaining.\n   *\n   * Uses a moving average of the last 10 completion times.\n   * Returns an empty string if fewer than 2 completions have occurred.\n   *\n   * @returns Formatted ETA string like ` ~12s remaining` or ` ~2m 30s remaining`\n   */\n  private formatETA(): string {\n    if (this.completionTimes.length < 2) return '';\n\n    const avg =\n      this.completionTimes.reduce((a, b) => a + b, 0) /\n      this.completionTimes.length;\n    const remaining = this.totalFiles - this.completed - this.failed;\n\n    if (remaining <= 0) return '';\n\n    const etaMs = avg * remaining;\n    const seconds = Math.round(etaMs / 1000);\n\n    if (seconds < 60) {\n      return pc.dim(` ~${seconds}s remaining`);\n    }\n\n    const minutes = Math.floor(seconds / 60);\n    const secs = seconds % 60;\n    return pc.dim(` ~${minutes}m ${secs}s remaining`);\n  }\n\n  /**\n   * Compute and format the estimated time remaining for directory tasks.\n   *\n   * Uses a moving average of the last 10 directory completion times.\n   * Returns an empty string if fewer than 2 completions have occurred.\n   */\n  private formatDirectoryETA(): string {\n    if (this.dirCompletionTimes.length < 2) return '';\n\n    const avg =\n      this.dirCompletionTimes.reduce((a, b) => a + b, 0) /\n      this.dirCompletionTimes.length;\n    const remaining = this.totalDirectories - this.dirCompleted;\n\n    if (remaining <= 0) return '';\n\n    const etaMs = avg * remaining;\n    const seconds = Math.round(etaMs / 1000);\n\n    if (seconds < 60) {\n      return pc.dim(` ~${seconds}s remaining`);\n    }\n\n    const minutes = Math.floor(seconds / 60);\n    const secs = seconds % 60;\n    return pc.dim(` ~${minutes}m ${secs}s remaining`);\n  }\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**ProgressReporter streams colored build-log progress events with ETA calculation via moving average of completion times, optionally mirroring ANSI-stripped output to `.agents-reverse-engineer/progress.log` through ProgressLog for `tail -f` monitoring in buffered environments.**\n\n## Exported Classes\n\n**ProgressLog**: Promise-chain serialized file writer for plain-text progress logging.\n- `static create(projectRoot: string): ProgressLog` — creates instance with path `<projectRoot>/.agents-reverse-engineer/progress.log`\n- `write(line: string): void` — appends line to log file, creating parent directory and opening handle on first call in truncate mode ('w'), silently swallows write errors\n- `async finalize(): Promise<void>` — flushes pending writes and closes file handle\n- Uses pattern `writeQueue: Promise<void>` with chained `.then()` for serialization (same as TraceWriter)\n- Private fields: `filePath: string`, `fd: FileHandle | null`, `writeQueue: Promise<void>`\n\n**ProgressReporter**: Streaming progress reporter with ETA calculation for file/directory/root tasks.\n- `constructor(totalFiles: number, totalDirectories: number = 0, progressLog?: ProgressLog)` — initializes with task counts and optional log mirror\n- `onFileStart(filePath: string): void` — logs `[X/Y] ANALYZING path` with cyan color\n- `onFileDone(filePath: string, durationMs: number, tokensIn: number, tokensOut: number, model: string, cacheReadTokens = 0, cacheCreationTokens = 0): void` — logs `[X/Y] DONE path Xs in/out tok model ~ETA` with green color, records completion time for ETA\n- `onFileError(filePath: string, error: string): void` — logs `[X/Y] FAIL path error` with red color\n- `onDirectoryStart(dirPath: string): void` — logs `[dir X/Y] ANALYZING dirPath/AGENTS.md` with cyan color\n- `onDirectoryDone(dirPath: string, durationMs: number, tokensIn: number, tokensOut: number, model: string, cacheReadTokens = 0, cacheCreationTokens = 0): void` — logs `[dir X/Y] DONE dirPath/AGENTS.md Xs in/out tok model ~ETA` with blue color, records directory completion time\n- `onRootDone(docPath: string): void` — logs `[root] DONE docPath` with blue color\n- `printSummary(summary: RunSummary): void` — prints final summary with ARE version, files processed/failed/skipped, total calls, tokens (input + cache read + cache creation / output), cache statistics, files read (total and unique), elapsed time, errors, retries\n\n## ETA Calculation Algorithm\n\n- **File ETA**: `formatETA()` computes moving average of last `windowSize` (10) completion times from `completionTimes` array, multiplies by remaining file count (`totalFiles - completed - failed`), formats as `~Ns remaining` or `~Mm Ss remaining`, returns empty string if fewer than 2 completions\n- **Directory ETA**: `formatDirectoryETA()` uses same algorithm with `dirCompletionTimes` array and `totalDirectories - dirCompleted` remaining count\n- Sliding window maintenance: `completionTimes.push(durationMs); if (completionTimes.length > windowSize) completionTimes.shift()`\n\n## State Tracking\n\nProgressReporter maintains counters:\n- `totalFiles: number`, `totalDirectories: number` — immutable task counts\n- `started: number`, `completed: number`, `failed: number` — file task progress\n- `dirStarted: number`, `dirCompleted: number` — directory task progress\n- `completionTimes: number[]`, `dirCompletionTimes: number[]` — sliding windows for ETA (max length 10)\n- `startTime: number` — timestamp from `Date.now()` at construction for total elapsed calculation\n- `progressLog: ProgressLog | null` — optional file mirror\n\n## Output Format\n\nAll console output uses `console.log()` for atomic writes. Colored via `picocolors`:\n- `pc.dim()` — counters, timestamps, token counts, model labels, ETA\n- `pc.cyan('ANALYZING')` — file/directory start events\n- `pc.green('DONE')` — file completion\n- `pc.blue('DONE')` — directory/root completion\n- `pc.red('FAIL')` — file errors\n- `pc.bold()` — summary header\n- `pc.yellow()` — skipped files count\n\n## ANSI Stripping\n\n`stripAnsi(str: string): string` removes ANSI escape codes via regex `/\\x1b\\[[0-9;]*m/g` for plain-text log output.\n\n## Token Accounting\n\nTotal input tokens calculated as `tokensIn + cacheReadTokens + cacheCreationTokens` for display in completion messages and summary. Output tokens tracked separately.\n\n## Integration Points\n\n- Imported by `src/orchestration/runner.ts` and `src/generation/executor.ts` for progress reporting during pool execution\n- Consumes `RunSummary` type from `src/orchestration/types.ts` for summary formatting\n- Mirrors output to `PROGRESS_LOG_FILENAME` constant (`'progress.log'`) within `.agents-reverse-engineer/` directory",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 1319,
      "cacheReadTokens": 17114,
      "cacheCreationTokens": 13448,
      "latencyMs": 39963,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/orchestration/progress.ts",
          "sizeBytes": 14428
        }
      ]
    },
    {
      "timestamp": "2026-02-09T16:10:43.805Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/quality/types.ts\n\n```typescript\n/**\n * Shared types for quality analysis: inconsistency detection and density measurement.\n */\n\n/** Severity level for detected inconsistencies. */\nexport type InconsistencySeverity = 'info' | 'warning' | 'error';\n\n/**\n * Inconsistency between .sum documentation and source code.\n * Detected by comparing exported symbols against .sum content.\n */\nexport interface CodeDocInconsistency {\n  type: 'code-vs-doc';\n  severity: InconsistencySeverity;\n  /** Path to the source file */\n  filePath: string;\n  /** Path to the corresponding .sum file */\n  sumPath: string;\n  description: string;\n  details: {\n    /** Symbols exported in source but not mentioned in .sum */\n    missingFromDoc: string[];\n    /** Symbols mentioned in .sum but not found in source */\n    missingFromCode: string[];\n    /** Purpose statement that contradicts observable behavior */\n    purposeMismatch?: string;\n  };\n}\n\n/**\n * Inconsistency across multiple source files.\n * Detected by comparing patterns and exports across files.\n */\nexport interface CodeCodeInconsistency {\n  type: 'code-vs-code';\n  severity: InconsistencySeverity;\n  /** Paths to the conflicting files */\n  files: string[];\n  description: string;\n  /** Pattern that was detected (e.g. 'duplicate-export') */\n  pattern: string;\n}\n\n/**\n * Path reference in generated documentation that doesn't resolve to a real file/directory.\n */\nexport interface PhantomPathInconsistency {\n  type: 'phantom-path';\n  severity: InconsistencySeverity;\n  /** Path to the AGENTS.md file containing the phantom reference */\n  agentsMdPath: string;\n  description: string;\n  details: {\n    /** The phantom path as written in the document */\n    referencedPath: string;\n    /** What it was resolved against (project root or AGENTS.md location) */\n    resolvedTo: string;\n    /** The line of text containing the phantom reference */\n    context: string;\n  };\n}\n\n/** Union of all inconsistency types. */\nexport type Inconsistency = CodeDocInconsistency | CodeCodeInconsistency | PhantomPathInconsistency;\n\n/** Structured report produced by inconsistency analysis. */\nexport interface InconsistencyReport {\n  /** Run metadata */\n  metadata: {\n    timestamp: string;\n    projectRoot: string;\n    filesChecked: number;\n    durationMs: number;\n  };\n  /** All detected inconsistencies */\n  issues: Inconsistency[];\n  /** Summary counts */\n  summary: {\n    total: number;\n    codeVsDoc: number;\n    codeVsCode: number;\n    phantomPaths: number;\n    errors: number;\n    warnings: number;\n    info: number;\n  };\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**types.ts defines the discriminated union types for quality validation inconsistencies detected across code-vs-doc, code-vs-code, and phantom-path analysis, plus the structured report format for validation output.**\n\n## Exported Types\n\n**InconsistencySeverity**: String literal union `'info' | 'warning' | 'error'` classifying inconsistency impact levels.\n\n**CodeDocInconsistency**: Interface with `type: 'code-vs-doc'` discriminator representing mismatches between exported symbols in source files and their `.sum` documentation. Contains `severity: InconsistencySeverity`, `filePath: string` (source file), `sumPath: string` (corresponding `.sum` file), `description: string`, and `details` object with `missingFromDoc: string[]` (symbols exported but undocumented), `missingFromCode: string[]` (symbols documented but not exported), and optional `purposeMismatch?: string` for contradictory purpose statements.\n\n**CodeCodeInconsistency**: Interface with `type: 'code-vs-code'` discriminator representing conflicts between multiple source files. Contains `severity: InconsistencySeverity`, `files: string[]` (conflicting file paths), `description: string`, and `pattern: string` identifying the detected inconsistency pattern (e.g., `'duplicate-export'` for symbols appearing across multiple files).\n\n**PhantomPathInconsistency**: Interface with `type: 'phantom-path'` discriminator representing unresolvable path references in generated `AGENTS.md` documentation. Contains `severity: InconsistencySeverity`, `agentsMdPath: string` (document containing the reference), `description: string`, and `details` object with `referencedPath: string` (raw phantom path), `resolvedTo: string` (resolution attempt context: project root or AGENTS.md location), and `context: string` (line of text containing the reference).\n\n**Inconsistency**: Discriminated union `CodeDocInconsistency | CodeCodeInconsistency | PhantomPathInconsistency` enabling type-safe pattern matching via the `type` discriminator field.\n\n**InconsistencyReport**: Interface structuring validation output with `metadata` object (contains `timestamp: string`, `projectRoot: string`, `filesChecked: number`, `durationMs: number`), `issues: Inconsistency[]` array of all detected problems, and `summary` object with aggregate counts (`total: number`, `codeVsDoc: number`, `codeVsCode: number`, `phantomPaths: number`, `errors: number`, `warnings: number`, `info: number`).\n\n## Integration Points\n\nConsumed by quality validation modules:\n- `src/quality/inconsistency/code-vs-doc.ts`: Produces `CodeDocInconsistency` instances via regex-based export extraction and substring matching\n- `src/quality/inconsistency/code-vs-code.ts`: Produces `CodeCodeInconsistency` instances via cross-file export aggregation into `Map<symbol, string[]>`\n- `src/quality/phantom-paths/validator.ts`: Produces `PhantomPathInconsistency` instances via regex path extraction and `existsSync()` resolution\n- `src/quality/inconsistency/reporter.ts`: Aggregates all inconsistency types into `InconsistencyReport` with formatted output\n\n## Design Pattern\n\nUses discriminated union pattern with `type` literal field enabling exhaustive type narrowing:\n```typescript\nswitch (inconsistency.type) {\n  case 'code-vs-doc': /* access missingFromDoc, missingFromCode */ break;\n  case 'code-vs-code': /* access files, pattern */ break;\n  case 'phantom-path': /* access referencedPath, resolvedTo */ break;\n}\n```",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 898,
      "cacheReadTokens": 17114,
      "cacheCreationTokens": 9818,
      "latencyMs": 27409,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/quality/types.ts",
          "sizeBytes": 2518
        }
      ]
    },
    {
      "timestamp": "2026-02-09T16:10:33.154Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/orchestration/trace.ts\n\n```typescript\n/**\n * Concurrency tracing system for debugging task/subprocess lifecycle.\n *\n * Produces append-only NDJSON trace files in `.agents-reverse-engineer/traces/`\n * when the `--trace` CLI flag is set. When disabled, the {@link NullTraceWriter}\n * ensures zero overhead -- every call site can unconditionally call `emit()`\n * without branching.\n *\n * Uses promise-chain serialization (same pattern as {@link PlanTracker}) to\n * handle concurrent writes from multiple pool workers safely.\n *\n * @module\n */\n\nimport { open, mkdir, readdir, unlink } from 'node:fs/promises';\nimport type { FileHandle } from 'node:fs/promises';\nimport * as path from 'node:path';\n\n// ---------------------------------------------------------------------------\n// Trace directory\n// ---------------------------------------------------------------------------\n\n/** Directory for trace files (relative to project root) */\nconst TRACES_DIR = '.agents-reverse-engineer/traces';\n\n// ---------------------------------------------------------------------------\n// Trace event types\n// ---------------------------------------------------------------------------\n\n/** Common fields present on every trace event */\ninterface TraceEventBase {\n  /** Monotonically increasing sequence number (per-run) */\n  seq: number;\n  /** ISO 8601 timestamp at event creation time */\n  ts: string;\n  /** process.pid of the Node.js parent process */\n  pid: number;\n  /** High-resolution elapsed time since run start (ms, fractional) */\n  elapsedMs: number;\n}\n\n/** Emitted when a phase begins execution */\ninterface PhaseStartEvent extends TraceEventBase {\n  type: 'phase:start';\n  phase: string;\n  taskCount: number;\n  concurrency: number;\n}\n\n/** Emitted when a phase completes */\ninterface PhaseEndEvent extends TraceEventBase {\n  type: 'phase:end';\n  phase: string;\n  durationMs: number;\n  tasksCompleted: number;\n  tasksFailed: number;\n}\n\n/** Emitted when a pool worker starts pulling from the shared iterator */\ninterface WorkerStartEvent extends TraceEventBase {\n  type: 'worker:start';\n  workerId: number;\n  phase: string;\n}\n\n/** Emitted when a pool worker exhausts the iterator or is aborted */\ninterface WorkerEndEvent extends TraceEventBase {\n  type: 'worker:end';\n  workerId: number;\n  phase: string;\n  tasksExecuted: number;\n}\n\n/** Emitted when a worker picks up a task from the iterator */\ninterface TaskPickupEvent extends TraceEventBase {\n  type: 'task:pickup';\n  workerId: number;\n  taskIndex: number;\n  taskLabel: string;\n  activeTasks: number;\n}\n\n/** Emitted when a task completes (success or failure) */\ninterface TaskDoneEvent extends TraceEventBase {\n  type: 'task:done';\n  workerId: number;\n  taskIndex: number;\n  taskLabel: string;\n  durationMs: number;\n  success: boolean;\n  error?: string;\n  activeTasks: number;\n}\n\n/** Emitted when a child process is spawned */\ninterface SubprocessSpawnEvent extends TraceEventBase {\n  type: 'subprocess:spawn';\n  childPid: number;\n  command: string;\n  taskLabel: string;\n}\n\n/** Emitted when a child process exits */\ninterface SubprocessExitEvent extends TraceEventBase {\n  type: 'subprocess:exit';\n  childPid: number;\n  command: string;\n  taskLabel: string;\n  exitCode: number;\n  signal: string | null;\n  durationMs: number;\n  timedOut: boolean;\n}\n\n/** Emitted before a retry attempt */\ninterface RetryEvent extends TraceEventBase {\n  type: 'retry';\n  attempt: number;\n  taskLabel: string;\n  errorCode: string;\n}\n\n/** Emitted when a non-pool task starts execution */\ninterface TaskStartEvent extends TraceEventBase {\n  type: 'task:start';\n  taskLabel: string;\n  phase: string;\n}\n\n/** Emitted when file discovery begins */\ninterface DiscoveryStartEvent extends TraceEventBase {\n  type: 'discovery:start';\n  targetPath: string;\n}\n\n/** Emitted when file discovery completes */\ninterface DiscoveryEndEvent extends TraceEventBase {\n  type: 'discovery:end';\n  filesIncluded: number;\n  filesExcluded: number;\n  durationMs: number;\n}\n\n/** Emitted when a filter is applied during discovery */\ninterface FilterAppliedEvent extends TraceEventBase {\n  type: 'filter:applied';\n  filterName: string;\n  filesMatched: number;\n  filesRejected: number;\n}\n\n/** Emitted when a generation/update plan is created */\ninterface PlanCreatedEvent extends TraceEventBase {\n  type: 'plan:created';\n  planType: 'generate' | 'update';\n  fileCount: number;\n  taskCount: number;\n}\n\n/** Emitted when configuration is loaded */\ninterface ConfigLoadedEvent extends TraceEventBase {\n  type: 'config:loaded';\n  configPath: string;\n  model: string;\n  concurrency: number;\n}\n\n/** Discriminated union of all trace event types */\nexport type TraceEvent =\n  | PhaseStartEvent\n  | PhaseEndEvent\n  | WorkerStartEvent\n  | WorkerEndEvent\n  | TaskPickupEvent\n  | TaskDoneEvent\n  | TaskStartEvent\n  | SubprocessSpawnEvent\n  | SubprocessExitEvent\n  | RetryEvent\n  | DiscoveryStartEvent\n  | DiscoveryEndEvent\n  | FilterAppliedEvent\n  | PlanCreatedEvent\n  | ConfigLoadedEvent;\n\n/** Keys auto-populated by the trace writer */\ntype BaseKeys = 'seq' | 'ts' | 'pid' | 'elapsedMs';\n\n/** Distributive Omit that works correctly across union members */\ntype DistributiveOmit<T, K extends PropertyKey> = T extends unknown ? Omit<T, K> : never;\n\n/** Event payload without auto-populated base fields */\nexport type TraceEventPayload = DistributiveOmit<TraceEvent, BaseKeys>;\n\n// ---------------------------------------------------------------------------\n// ITraceWriter interface\n// ---------------------------------------------------------------------------\n\n/**\n * Public interface for trace event emission.\n *\n * All consumers depend only on this interface, allowing the no-op\n * implementation to be swapped in when tracing is disabled.\n */\nexport interface ITraceWriter {\n  /** Emit a trace event. Base fields (seq, ts, pid, elapsedMs) are auto-populated. */\n  emit(event: TraceEventPayload): void;\n  /** Flush all pending writes and close the file handle. */\n  finalize(): Promise<void>;\n  /** Absolute path to the trace file (empty string for NullTraceWriter). */\n  readonly filePath: string;\n}\n\n// ---------------------------------------------------------------------------\n// NullTraceWriter (no-op)\n// ---------------------------------------------------------------------------\n\n/**\n * No-op trace writer. Returned when `--trace` is not set.\n * All methods are empty -- zero overhead at call sites.\n */\nclass NullTraceWriter implements ITraceWriter {\n  readonly filePath = '';\n  emit(): void { /* no-op */ }\n  async finalize(): Promise<void> { /* no-op */ }\n}\n\n// ---------------------------------------------------------------------------\n// TraceWriter (real implementation)\n// ---------------------------------------------------------------------------\n\n/**\n * Append-only NDJSON trace writer.\n *\n * Each `emit()` call serializes the event to a single JSON line and\n * enqueues a file append via a promise chain. This guarantees correct\n * ordering even when multiple pool workers emit concurrently.\n */\nclass TraceWriter implements ITraceWriter {\n  readonly filePath: string;\n\n  private seq = 0;\n  private readonly nodePid = process.pid;\n  private readonly startHr = process.hrtime.bigint();\n  private writeQueue: Promise<void> = Promise.resolve();\n  private fd: FileHandle | null = null;\n\n  constructor(filePath: string) {\n    this.filePath = filePath;\n  }\n\n  emit(partial: TraceEventPayload): void {\n    const event = {\n      ...partial,\n      seq: this.seq++,\n      ts: new Date().toISOString(),\n      pid: this.nodePid,\n      elapsedMs: Number(process.hrtime.bigint() - this.startHr) / 1_000_000,\n    };\n    const line = JSON.stringify(event) + '\\n';\n\n    this.writeQueue = this.writeQueue\n      .then(async () => {\n        if (!this.fd) {\n          await mkdir(path.dirname(this.filePath), { recursive: true });\n          this.fd = await open(this.filePath, 'a');\n        }\n        await this.fd.write(line);\n      })\n      .catch(() => { /* non-critical -- trace loss is acceptable */ });\n  }\n\n  async finalize(): Promise<void> {\n    await this.writeQueue;\n    if (this.fd) {\n      await this.fd.close();\n      this.fd = null;\n    }\n  }\n}\n\n// ---------------------------------------------------------------------------\n// Factory\n// ---------------------------------------------------------------------------\n\n/**\n * Create a trace writer.\n *\n * Returns a {@link NullTraceWriter} when `enabled` is false (zero overhead).\n * Otherwise returns a {@link TraceWriter} that appends NDJSON to\n * `.agents-reverse-engineer/traces/trace-{timestamp}.ndjson`.\n *\n * @param projectRoot - Absolute path to the project root directory\n * @param enabled - Whether tracing is enabled (typically from `--trace` flag)\n * @returns A trace writer instance\n */\nexport function createTraceWriter(projectRoot: string, enabled: boolean): ITraceWriter {\n  if (!enabled) return new NullTraceWriter();\n  const safeTimestamp = new Date().toISOString().replace(/[:.]/g, '-');\n  const filePath = path.join(projectRoot, TRACES_DIR, `trace-${safeTimestamp}.ndjson`);\n  return new TraceWriter(filePath);\n}\n\n// ---------------------------------------------------------------------------\n// Cleanup\n// ---------------------------------------------------------------------------\n\n/**\n * Remove old trace files, keeping only the most recent ones.\n *\n * Mirrors the pattern in `src/ai/telemetry/cleanup.ts`.\n *\n * @param projectRoot - Absolute path to the project root directory\n * @param keepCount - Number of most recent trace files to retain (default: 500)\n * @returns Number of files deleted\n */\nexport async function cleanupOldTraces(projectRoot: string, keepCount: number = 500): Promise<number> {\n  const tracesDir = path.join(projectRoot, TRACES_DIR);\n\n  let entries: string[];\n  try {\n    const allEntries = await readdir(tracesDir);\n    entries = allEntries.filter(\n      (name) => name.startsWith('trace-') && name.endsWith('.ndjson'),\n    );\n  } catch (error) {\n    if ((error as NodeJS.ErrnoException).code === 'ENOENT') {\n      return 0;\n    }\n    throw error;\n  }\n\n  // Sort newest first (ISO timestamps sort lexicographically)\n  entries.sort();\n  entries.reverse();\n\n  const toDelete = entries.slice(keepCount);\n\n  for (const filename of toDelete) {\n    await unlink(path.join(tracesDir, filename));\n  }\n\n  return toDelete.length;\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**trace.ts provides append-only NDJSON tracing system for debugging task/subprocess lifecycle with promise-chain serialization to handle concurrent pool worker writes safely.**\n\n## Public Interface\n\n### Factory Function\n- `createTraceWriter(projectRoot: string, enabled: boolean): ITraceWriter` — returns `NullTraceWriter` when `enabled` is false (zero overhead), otherwise returns `TraceWriter` appending to `.agents-reverse-engineer/traces/trace-{timestamp}.ndjson`\n\n### Cleanup Function\n- `cleanupOldTraces(projectRoot: string, keepCount: number = 500): Promise<number>` — removes old trace files keeping only most recent ones, returns deletion count, mirrors pattern in `src/ai/telemetry/cleanup.ts`\n\n### Core Interface\n```typescript\ninterface ITraceWriter {\n  emit(event: TraceEventPayload): void;\n  finalize(): Promise<void>;\n  readonly filePath: string;\n}\n```\n\n### Event Type Union\n`TraceEvent` discriminated union includes 14 event types: `PhaseStartEvent`, `PhaseEndEvent`, `WorkerStartEvent`, `WorkerEndEvent`, `TaskPickupEvent`, `TaskDoneEvent`, `TaskStartEvent`, `SubprocessSpawnEvent`, `SubprocessExitEvent`, `RetryEvent`, `DiscoveryStartEvent`, `DiscoveryEndEvent`, `FilterAppliedEvent`, `PlanCreatedEvent`, `ConfigLoadedEvent`\n\n### Event Payload Type\n`TraceEventPayload = DistributiveOmit<TraceEvent, BaseKeys>` — user-supplied event data without auto-populated fields (`seq`, `ts`, `pid`, `elapsedMs`)\n\n## Event Schema\n\n### Common Base Fields (Auto-Populated)\nEvery `TraceEvent` extends `TraceEventBase`:\n- `seq: number` — monotonically increasing sequence number per-run\n- `ts: string` — ISO 8601 timestamp at event creation\n- `pid: number` — `process.pid` of Node.js parent process\n- `elapsedMs: number` — high-resolution elapsed time since run start (milliseconds, fractional)\n\n### Phase Events\n- `PhaseStartEvent` (`type: 'phase:start'`) — includes `phase: string`, `taskCount: number`, `concurrency: number`\n- `PhaseEndEvent` (`type: 'phase:end'`) — includes `phase: string`, `durationMs: number`, `tasksCompleted: number`, `tasksFailed: number`\n\n### Worker Events\n- `WorkerStartEvent` (`type: 'worker:start'`) — includes `workerId: number`, `phase: string`\n- `WorkerEndEvent` (`type: 'worker:end'`) — includes `workerId: number`, `phase: string`, `tasksExecuted: number`\n\n### Task Events\n- `TaskPickupEvent` (`type: 'task:pickup'`) — includes `workerId: number`, `taskIndex: number`, `taskLabel: string`, `activeTasks: number`\n- `TaskDoneEvent` (`type: 'task:done'`) — includes `workerId: number`, `taskIndex: number`, `taskLabel: string`, `durationMs: number`, `success: boolean`, `error?: string`, `activeTasks: number`\n- `TaskStartEvent` (`type: 'task:start'`) — includes `taskLabel: string`, `phase: string`\n\n### Subprocess Events\n- `SubprocessSpawnEvent` (`type: 'subprocess:spawn'`) — includes `childPid: number`, `command: string`, `taskLabel: string`\n- `SubprocessExitEvent` (`type: 'subprocess:exit'`) — includes `childPid: number`, `command: string`, `taskLabel: string`, `exitCode: number`, `signal: string | null`, `durationMs: number`, `timedOut: boolean`\n\n### Retry Event\n- `RetryEvent` (`type: 'retry'`) — includes `attempt: number`, `taskLabel: string`, `errorCode: string`\n\n### Discovery Events\n- `DiscoveryStartEvent` (`type: 'discovery:start'`) — includes `targetPath: string`\n- `DiscoveryEndEvent` (`type: 'discovery:end'`) — includes `filesIncluded: number`, `filesExcluded: number`, `durationMs: number`\n\n### Filter Event\n- `FilterAppliedEvent` (`type: 'filter:applied'`) — includes `filterName: string`, `filesMatched: number`, `filesRejected: number`\n\n### Plan Event\n- `PlanCreatedEvent` (`type: 'plan:created'`) — includes `planType: 'generate' | 'update'`, `fileCount: number`, `taskCount: number`\n\n### Config Event\n- `ConfigLoadedEvent` (`type: 'config:loaded'`) — includes `configPath: string`, `model: string`, `concurrency: number`\n\n## Implementation Classes\n\n### NullTraceWriter\nNo-op implementation returned when `--trace` flag not set. All methods empty (zero overhead at call sites). `filePath` returns empty string. `emit()` and `finalize()` perform no operations.\n\n### TraceWriter\nReal NDJSON writer with promise-chain serialization. Constructor stores `filePath: string`. Maintains state: `seq: number` (incremented per event), `nodePid = process.pid`, `startHr = process.hrtime.bigint()` (for `elapsedMs` calculation), `writeQueue: Promise<void>` (serialization chain), `fd: FileHandle | null` (lazy-opened file handle).\n\n`emit(partial: TraceEventPayload)` populates base fields (`seq`, `ts`, `pid`, `elapsedMs`), serializes to JSON line, enqueues write via `writeQueue = writeQueue.then(async () => {...})` pattern. Lazy-opens file handle via `open(filePath, 'a')` after ensuring directory exists with `mkdir(path.dirname(filePath), { recursive: true })`. Catches write errors silently (trace loss acceptable).\n\n`finalize()` awaits `writeQueue` completion, closes file handle via `fd.close()`, nulls `fd` reference.\n\n## Serialization Pattern\n\nPromise-chain pattern identical to `PlanTracker` from `src/orchestration/plan-tracker.ts`. Each `emit()` call chains onto previous write: `this.writeQueue = this.writeQueue.then(async () => { await this.fd.write(line); })`. Guarantees NDJSON line order matches emission order despite concurrent pool workers.\n\n## Storage Location\n\nConstant `TRACES_DIR = '.agents-reverse-engineer/traces'` defines trace directory. Filename format: `trace-{timestamp}.ndjson` where timestamp uses ISO 8601 with colons/periods replaced by hyphens: `new Date().toISOString().replace(/[:.]/g, '-')`.\n\n## Type Utilities\n\n`DistributiveOmit<T, K extends PropertyKey> = T extends unknown ? Omit<T, K> : never` — correctly distributes `Omit` across union members (standard `Omit<Union, Keys>` fails for discriminated unions per TypeScript semantics). Used to define `TraceEventPayload` by stripping auto-populated base fields from `TraceEvent` union.\n\n## Integration Points\n\nThreaded via `CommandRunOptions.tracer: ITraceWriter` parameter passed to pool (`src/orchestration/pool.ts`), AIService (`src/ai/service.ts`), and runner (`src/orchestration/runner.ts`). Enabled by `--trace` CLI flag in command entry points (`src/cli/generate.ts`, `src/cli/update.ts`).",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 1841,
      "cacheReadTokens": 0,
      "cacheCreationTokens": 29083,
      "latencyMs": 40281,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/orchestration/trace.ts",
          "sizeBytes": 10277
        }
      ]
    },
    {
      "timestamp": "2026-02-09T16:10:55.430Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/update/index.ts\n\n```typescript\n/**\n * Update module\n *\n * Provides incremental documentation update functionality.\n * Coordinates state management, change detection, and orphan cleanup.\n */\nexport {\n  UpdateOrchestrator,\n  createUpdateOrchestrator,\n  type UpdatePlan,\n} from './orchestrator.js';\n\nexport {\n  cleanupOrphans,\n  cleanupEmptyDirectoryDocs,\n  getAffectedDirectories,\n} from './orphan-cleaner.js';\n\nexport type {\n  UpdateOptions,\n  UpdateResult,\n  UpdateProgress,\n  CleanupResult,\n} from './types.js';\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**Module barrel for incremental update functionality exporting UpdateOrchestrator, orchestrator factory, cleanup utilities, and type definitions.**\n\n## Exported Symbols\n\n### From `./orchestrator.js`\n- **UpdateOrchestrator** (class) — Coordinates incremental update workflow with state management and change detection\n- **createUpdateOrchestrator** (factory function) — Creates UpdateOrchestrator instance\n- **UpdatePlan** (type) — Describes files to analyze/skip and directories to regenerate\n\n### From `./orphan-cleaner.js`\n- **cleanupOrphans** (function) — Deletes `.sum` files for deleted source files or renamed oldPaths\n- **cleanupEmptyDirectoryDocs** (function) — Removes `AGENTS.md` from directories with no remaining sources\n- **getAffectedDirectories** (function) — Computes parent directory paths requiring AGENTS.md regeneration by walking up from changed files\n\n### From `./types.js`\n- **UpdateOptions** (type) — Configuration for update command including uncommitted changes flag\n- **UpdateResult** (type) — Aggregated outcome with counts for analyzed/skipped/orphaned files\n- **UpdateProgress** (type) — Streaming progress events during update execution\n- **CleanupResult** (type) — Counts of orphaned `.sum` files and empty `AGENTS.md` files removed\n\n## Integration Points\n\nRe-exports types and functions consumed by:\n- `src/cli/update.ts` — CLI entry point invoking `createUpdateOrchestrator()` with parsed options\n- `src/update/orchestrator.ts` — Core incremental update logic calling `cleanupOrphans()` and `getAffectedDirectories()`\n- `hooks/are-session-end.js` — Session hook spawning `npx agents-reverse-engineer@latest update --quiet` on uncommitted changes\n\n## Module Role\n\nServes as public API boundary for the `src/update/` directory, exposing only the orchestrator, factory, cleanup utilities, and type contracts while hiding internal implementation details of SHA-256 hash comparison, git diff parsing, and orphan detection logic.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 497,
      "cacheReadTokens": 17114,
      "cacheCreationTokens": 9246,
      "latencyMs": 19784,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/update/index.ts",
          "sizeBytes": 482
        }
      ]
    },
    {
      "timestamp": "2026-02-09T16:10:39.097Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/quality/index.ts\n\n```typescript\n/**\n * Public API for the quality analysis module.\n *\n * Re-exports all quality analysis types and functions from the\n * inconsistency detection and density validation submodules.\n *\n * @module\n */\n\n// ---------------------------------------------------------------------------\n// Shared types\n// ---------------------------------------------------------------------------\n\nexport type {\n  InconsistencySeverity,\n  CodeDocInconsistency,\n  CodeCodeInconsistency,\n  PhantomPathInconsistency,\n  Inconsistency,\n  InconsistencyReport,\n} from './types.js';\n\n// ---------------------------------------------------------------------------\n// Inconsistency detection: code-vs-doc\n// ---------------------------------------------------------------------------\n\nexport { extractExports, checkCodeVsDoc } from './inconsistency/code-vs-doc.js';\n\n// ---------------------------------------------------------------------------\n// Inconsistency detection: code-vs-code\n// ---------------------------------------------------------------------------\n\nexport { checkCodeVsCode } from './inconsistency/code-vs-code.js';\n\n// ---------------------------------------------------------------------------\n// Inconsistency reporting\n// ---------------------------------------------------------------------------\n\nexport { buildInconsistencyReport, formatReportForCli } from './inconsistency/reporter.js';\n\n// ---------------------------------------------------------------------------\n// Phantom path detection\n// ---------------------------------------------------------------------------\n\nexport { checkPhantomPaths } from './phantom-paths/index.js';\n\n// ---------------------------------------------------------------------------\n// Density validation\n// ---------------------------------------------------------------------------\n\nexport { validateFindability } from './density/validator.js';\nexport type { FindabilityResult } from './density/validator.js';\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**Public API barrel module for quality analysis subsystem, re-exporting inconsistency detection functions (`checkCodeVsDoc`, `checkCodeVsCode`, `checkPhantomPaths`), reporting utilities (`buildInconsistencyReport`, `formatReportForCli`), density validation (`validateFindability`), and all discriminated union types for inconsistency classification.**\n\n## Exported Types\n\nRe-exports from `./types.js`:\n- `InconsistencySeverity` — Severity classification type\n- `CodeDocInconsistency` — Discriminated union member for missing exported symbols in `.sum` documentation\n- `CodeCodeInconsistency` — Discriminated union member for duplicate symbol exports across multiple files\n- `PhantomPathInconsistency` — Discriminated union member for unresolved path references in `AGENTS.md`\n- `Inconsistency` — Discriminated union of all inconsistency types\n- `InconsistencyReport` — Container type with `metadata`, `issues[]`, and `summary` aggregation\n\nRe-exports from `./density/validator.js`:\n- `FindabilityResult` — Return type from `validateFindability()` containing density metrics\n\n## Code-vs-Doc Validation\n\nExports from `./inconsistency/code-vs-doc.js`:\n- `extractExports(filePath: string): string[]` — Regex-based extraction of exported symbols via pattern `/^[ \\t]*export\\s+(?:default\\s+)?(?:function|class|const|let|var|type|interface|enum)\\s+(\\w+)/gm`\n- `checkCodeVsDoc(filePath: string, sumContent: string): CodeDocInconsistency | null` — Verifies all extracted exports appear in `.sum` summary text via substring search, returns `null` on pass or `CodeDocInconsistency` with `missingFromDoc` array on failure\n\n## Code-vs-Code Validation\n\nExports from `./inconsistency/code-vs-code.js`:\n- `checkCodeVsCode(files: string[]): CodeCodeInconsistency[]` — Aggregates exports across per-directory file groups into `Map<symbol, string[]>`, returns array of `CodeCodeInconsistency` for symbols appearing in multiple files with pattern `'duplicate-export'`\n\n## Phantom Path Detection\n\nExports from `./phantom-paths/index.js`:\n- `checkPhantomPaths(agentsMdPath: string): PhantomPathInconsistency[]` — Extracts path-like strings from `AGENTS.md` via three regex patterns (markdown links `/\\[(?:[^\\]]*)\\]\\((\\.[^)]+)\\)/g`, backtick-quoted paths `` /`((?:src\\/|\\.\\.?\\/)[^`]+\\.[a-z]{1,4})`/g ``, prose-embedded `/(?:from|in|by|via|see)\\s+`?(src\\/[\\w\\-./]+)`?/gi`), resolves against AGENTS.md directory and project root with `.ts`/`.js` fallback, returns array of unresolved references\n\n## Inconsistency Reporting\n\nExports from `./inconsistency/reporter.js`:\n- `buildInconsistencyReport(issues: Inconsistency[], metadata: object): InconsistencyReport` — Constructs report with timestamp, projectRoot, filesChecked, durationMs metadata and summary counts by type/severity\n- `formatReportForCli(report: InconsistencyReport): string` — Formats `InconsistencyReport` for terminal display with ANSI color codes via picocolors\n\n## Density Validation\n\nExports from `./density/validator.js`:\n- `validateFindability(sumContent: string): FindabilityResult` — Disabled feature after removal of structured `publicInterface` from `SumFileContent` schema, returns density metrics for identifier-rich summary validation\n\n## Module Organization\n\nBarrel pattern aggregates four subsystems:\n1. **Inconsistency detection** — code-vs-doc (exports missing from summaries), code-vs-code (duplicate symbols), phantom-paths (unresolved references in `AGENTS.md`)\n2. **Reporting** — `buildInconsistencyReport()` aggregation, `formatReportForCli()` terminal rendering\n3. **Density validation** — `validateFindability()` checks for identifier density in summaries (currently disabled)\n4. **Shared types** — Discriminated unions for inconsistency classification with `pattern` discriminator field\n\nConsumer modules import via `import { checkCodeVsDoc, buildInconsistencyReport } from '@/quality'` instead of direct submodule paths.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 1101,
      "cacheReadTokens": 17114,
      "cacheCreationTokens": 9443,
      "latencyMs": 39428,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/quality/index.ts",
          "sizeBytes": 1936
        }
      ]
    },
    {
      "timestamp": "2026-02-09T16:11:01.007Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/docs/INPUT.md\n\n```markdown\n# Agents Reverse Engineer:\n\nA light-weight and powerful tool to reverse engineer a brownfield project and generate adequate AGENTS.md and CLAUDE.md among others... Compatible with Claude Code, OpenCode and many other agent tools.\n\n## Inception\n\nAgents Reverse Engineer was born out of the need to streamline the process of documenting and understanding existing projects. It aims to provide agents with a clear and concise overview of their codebase, making it easier to maintain the project over time.\nIt can be used in conjunction with other tools to enhance the overall development workflow like:\n- SpecKit: https://github.com/github/spec-kit\n- BMAD: https://github.com/bmad-code-org/BMAD-METHOD\n- Get Shit Done (GSD): https://github.com/glittercowboy/get-shit-done\n\n## Features\n\n- It should provide a command that will execute script usin Recursive Language Model with Claude Calude or and other LLM Agents tool. For Claude, it will be a command, ffor other, the available alternative.\n- It should be possible to add a hook on the end of a session to update the impacted files\n- It should generate AGENTS.md file in every directory of the structure\n- Each AGENTS.md file will be describing the content of the current directory and sub-structure. They can reference other files like ARCHITECTURE.md, STRUCTURE.md, STACK.md, INTEGRATIONS.md, INFRASTRUCTURE.md, CONVENTIONS.md, TESTING.md, PATTERNS.md, CONCERNS.md, etc...\n- The RLM will work as follow:\n    * build the project structure tree\n    * with start by executing the call at the first leaf and build recursively backward\n    * since the leaf is a file, analyse the file and generate a summary {filename}.sum\n    * when all leaf of a directory are summarized, analyse the directory and generate AGENTS.md file and other files if needed\n    * continue recursively until the root of the project is reached\n\n## Reaserch\n\nAnalyse in details GSD and BMAD.\n\nWe'll use a similar repository structure mainly to GSD and a bit of BMAD.\n\nLook at how they are approaching a brownfiled project, speical command and details about how they are doing. Check their codebase in details if needed.\n\n## Usage\n\nTo use Agents Reverse Engineer, simply run the following command in your terminal:\n\n```bash\n/are-generate\n/are-update\n```\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**INPUT.md documents the original design vision and inception rationale for the agents-reverse-engineer tool, defining the Recursive Language Model (RLM) algorithm for brownfield project documentation generation through post-order tree traversal of file summaries and directory aggregation.**\n\n## Original Vision\n\nCreated to streamline brownfield project documentation by providing AI agents with clear codebase overviews. Designed for compatibility with Claude Code, OpenCode, and other agent tools, complementing SpecKit (https://github.com/github/spec-kit), BMAD (https://github.com/bmad-code-org/BMAD-METHOD), and Get Shit Done (https://github.com/glittercowboy/get-shit-done).\n\n## RLM Algorithm Definition\n\nThe Recursive Language Model workflow executes as:\n1. Build project structure tree via directory traversal\n2. Start execution at first leaf (deepest file node) and build recursively backward toward root\n3. For leaf files: analyze content and generate `{filename}.sum` summary artifact\n4. For directories: once all child leaf summaries exist, analyze directory contents and generate `AGENTS.md` (plus optional `ARCHITECTURE.md`, `STRUCTURE.md`, `STACK.md`, `INTEGRATIONS.md`, `INFRASTRUCTURE.md`, `CONVENTIONS.md`, `TESTING.md`, `PATTERNS.md`, `CONCERNS.md`)\n5. Continue post-order traversal recursively until project root reached\n\n## Command Interface\n\nProposed command patterns:\n- `/are-generate` — Full documentation generation via RLM execution\n- `/are-update` — Incremental regeneration for modified files\n\n## Research Directives\n\nDocument instructed detailed analysis of GSD (Get Shit Done) and BMAD methodologies, specifically:\n- Repository structure patterns (primarily GSD-inspired with BMAD elements)\n- Brownfield project approach strategies\n- Special command implementations and execution details\n- Codebase architecture analysis requirements\n\n## Session Lifecycle Integration\n\nOriginal design specified session-end hook capability to automatically update impacted documentation files when agent sessions terminate, ensuring documentation remains synchronized with codebase changes without manual intervention.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 487,
      "cacheReadTokens": 0,
      "cacheCreationTokens": 26774,
      "latencyMs": 21728,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "docs/INPUT.md",
          "sizeBytes": 2272
        }
      ]
    },
    {
      "timestamp": "2026-02-09T16:11:00.032Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/update/orphan-cleaner.ts\n\n```typescript\n/**\n * Orphan cleanup for stale .sum and AGENTS.md files\n *\n * Handles:\n * - Deleting .sum files when source files are deleted\n * - Deleting .sum files for renamed files (at old path)\n * - Deleting AGENTS.md from directories with no remaining source files\n */\nimport { unlink, readdir, stat } from 'node:fs/promises';\nimport * as path from 'node:path';\nimport type { FileChange } from '../change-detection/types.js';\nimport type { CleanupResult } from './types.js';\n\n/**\n * Files to ignore when checking if a directory has source files.\n * These are generated files, not source files.\n */\nconst GENERATED_FILES = new Set([\n  'AGENTS.md',\n  'CLAUDE.md',\n]);\n\n/**\n * Clean up orphaned .sum files and empty AGENTS.md files.\n *\n * @param projectRoot - Absolute path to project root\n * @param changes - List of file changes (deleted and renamed files need cleanup)\n * @param dryRun - If true, don't actually delete files\n * @returns Cleanup result with lists of deleted files\n */\nexport async function cleanupOrphans(\n  projectRoot: string,\n  changes: FileChange[],\n  dryRun: boolean = false\n): Promise<CleanupResult> {\n  const result: CleanupResult = {\n    deletedSumFiles: [],\n    deletedAgentsMd: [],\n  };\n\n  // Collect paths that need .sum cleanup\n  const pathsToClean: string[] = [];\n\n  for (const change of changes) {\n    if (change.status === 'deleted') {\n      pathsToClean.push(change.path);\n    } else if (change.status === 'renamed' && change.oldPath) {\n      // For renames, clean up the old path's .sum\n      pathsToClean.push(change.oldPath);\n    }\n  }\n\n  // Delete .sum files for deleted/renamed source files\n  for (const relativePath of pathsToClean) {\n    const sumPath = path.join(projectRoot, `${relativePath}.sum`);\n    const deleted = await deleteIfExists(sumPath, dryRun);\n    if (deleted) {\n      result.deletedSumFiles.push(relativePath + '.sum');\n    }\n  }\n\n  // Collect affected directories for AGENTS.md cleanup\n  const affectedDirs = new Set<string>();\n  for (const relativePath of pathsToClean) {\n    const dir = path.dirname(relativePath);\n    if (dir && dir !== '.') {\n      affectedDirs.add(dir);\n    }\n  }\n\n  // Check each affected directory for empty AGENTS.md\n  for (const dir of affectedDirs) {\n    const dirPath = path.join(projectRoot, dir);\n    const cleaned = await cleanupEmptyDirectoryDocs(dirPath, dryRun);\n    if (cleaned) {\n      result.deletedAgentsMd.push(path.join(dir, 'AGENTS.md'));\n    }\n  }\n\n  return result;\n}\n\n/**\n * Delete a file if it exists.\n *\n * @returns true if file was deleted (or would be in dry run)\n */\nasync function deleteIfExists(filePath: string, dryRun: boolean): Promise<boolean> {\n  try {\n    await stat(filePath);\n    if (!dryRun) {\n      await unlink(filePath);\n    }\n    return true;\n  } catch {\n    // File doesn't exist\n    return false;\n  }\n}\n\n/**\n * Check if a directory has any source files remaining.\n * If not, delete its AGENTS.md.\n *\n * @param dirPath - Absolute path to directory\n * @param dryRun - If true, don't actually delete\n * @returns true if AGENTS.md was deleted\n */\nexport async function cleanupEmptyDirectoryDocs(\n  dirPath: string,\n  dryRun: boolean = false\n): Promise<boolean> {\n  try {\n    const entries = await readdir(dirPath);\n\n    // Check if directory has any source files\n    // Source files are: not .sum files, not generated docs, not hidden files\n    const hasSourceFiles = entries.some(entry => {\n      // Skip hidden files\n      if (entry.startsWith('.')) return false;\n      // Skip .sum files\n      if (entry.endsWith('.sum')) return false;\n      // Skip known generated files\n      if (GENERATED_FILES.has(entry)) return false;\n      // Everything else counts as a source file\n      return true;\n    });\n\n    if (!hasSourceFiles) {\n      const agentsPath = path.join(dirPath, 'AGENTS.md');\n      return await deleteIfExists(agentsPath, dryRun);\n    }\n\n    return false;\n  } catch {\n    // Directory doesn't exist or can't be read\n    return false;\n  }\n}\n\n/**\n * Get list of directories that should have their AGENTS.md regenerated.\n *\n * Includes parent directories of changed files up to project root.\n *\n * @param changes - List of file changes (non-deleted files)\n * @returns Set of relative directory paths\n */\nexport function getAffectedDirectories(changes: FileChange[]): Set<string> {\n  const dirs = new Set<string>();\n\n  for (const change of changes) {\n    // Skip deleted files - they don't affect directory docs\n    if (change.status === 'deleted') continue;\n\n    // Add all parent directories up to root\n    let dir = path.dirname(change.path);\n    while (dir && dir !== '.' && !path.isAbsolute(dir)) {\n      dirs.add(dir);\n      dir = path.dirname(dir);\n    }\n    // Include root directory\n    dirs.add('.');\n  }\n\n  return dirs;\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**orphan-cleaner.ts deletes stale `.sum` files and `AGENTS.md` files for deleted/renamed source files and empty directories.**\n\n## Exported Functions\n\n**`cleanupOrphans(projectRoot: string, changes: FileChange[], dryRun?: boolean): Promise<CleanupResult>`** orchestrates cleanup by extracting paths from `FileChange[]` where `status === 'deleted'` or `status === 'renamed'` (using `oldPath` for renames), deleting corresponding `.sum` files via `deleteIfExists()`, collecting affected directories via `path.dirname()`, and delegating empty directory cleanup to `cleanupEmptyDirectoryDocs()`. Returns `CleanupResult` with `deletedSumFiles[]` and `deletedAgentsMd[]` arrays.\n\n**`cleanupEmptyDirectoryDocs(dirPath: string, dryRun?: boolean): Promise<boolean>`** reads directory via `readdir()`, filters entries to detect source files (excluding hidden files starting with `.`, `.sum` extensions, and `GENERATED_FILES` set members `'AGENTS.md'` and `'CLAUDE.md'`), and deletes `AGENTS.md` via `deleteIfExists()` if no source files remain. Returns `true` if `AGENTS.md` was deleted.\n\n**`getAffectedDirectories(changes: FileChange[]): Set<string>`** extracts unique parent directories from non-deleted `FileChange` entries by walking up directory tree via `path.dirname()` until reaching `.` (root). Skips files with `status === 'deleted'` since deleted files don't affect directory documentation. Always includes `'.'` as root directory in returned set.\n\n## Cleanup Logic\n\n**Orphan detection** operates on two categories: `.sum` files at old paths for deleted source files (appends `.sum` to `change.path`), and `.sum` files at `oldPath` for renames (appends `.sum` to `change.oldPath`). Directory cleanup targets directories where all remaining entries are hidden files, `.sum` files, or members of `GENERATED_FILES` constant.\n\n**`deleteIfExists(filePath: string, dryRun: boolean): Promise<boolean>`** internal helper calls `stat()` to check existence, `unlink()` to delete (unless `dryRun === true`), returns `true` if file existed (or would be deleted in dry run), returns `false` if `stat()` throws.\n\n## Integration Points\n\nConsumes `FileChange` from `src/change-detection/types.ts` with discriminated `status` field (`'added' | 'modified' | 'deleted' | 'renamed'`) and optional `oldPath` string for renames. Returns `CleanupResult` from `src/update/types.ts` with relative path arrays for deleted artifacts. Called by `orchestrateUpdate()` in `src/update/orchestrator.ts` after change detection and before Phase 2 directory aggregation.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 691,
      "cacheReadTokens": 16334,
      "cacheCreationTokens": 11344,
      "latencyMs": 24437,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/update/orphan-cleaner.ts",
          "sizeBytes": 4751
        }
      ]
    },
    {
      "timestamp": "2026-02-09T16:10:54.069Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/specify/writer.ts\n\n```typescript\nimport { writeFile, mkdir, access } from 'node:fs/promises';\nimport { constants } from 'node:fs';\nimport * as path from 'node:path';\n\n/**\n * Options controlling spec output writing behavior.\n */\nexport interface WriteSpecOptions {\n  /** Full path to the output file (e.g., /project/specs/SPEC.md). */\n  outputPath: string;\n  /** Overwrite existing files without error. */\n  force: boolean;\n  /** Split AI output into multiple files by top-level `# ` headings. */\n  multiFile: boolean;\n}\n\n/**\n * Thrown when writeSpec() detects existing file(s) and force=false.\n * Callers should catch this and present a user-friendly message.\n */\nexport class SpecExistsError extends Error {\n  /** Paths of the files that already exist. */\n  readonly paths: string[];\n\n  constructor(paths: string[]) {\n    const list = paths.map((p) => `  - ${p}`).join('\\n');\n    super(`Spec file(s) already exist:\\n${list}\\nUse --force to overwrite.`);\n    this.name = 'SpecExistsError';\n    this.paths = paths;\n  }\n}\n\n/**\n * Check whether a file exists at the given path.\n */\nasync function fileExists(filePath: string): Promise<boolean> {\n  try {\n    await access(filePath, constants.F_OK);\n    return true;\n  } catch {\n    return false;\n  }\n}\n\n/**\n * Sanitize a heading string into a filename-safe slug.\n *\n * Lowercases, replaces whitespace with hyphens, strips non-alphanumeric\n * characters (except hyphens), collapses consecutive hyphens, and trims\n * leading/trailing hyphens.\n */\nfunction slugify(heading: string): string {\n  return heading\n    .toLowerCase()\n    .replace(/\\s+/g, '-')\n    .replace(/[^a-z0-9-]/g, '')\n    .replace(/-+/g, '-')\n    .replace(/^-|-$/g, '');\n}\n\n/**\n * Split markdown content on top-level `# ` headings into named sections.\n *\n * Returns an array of `{ filename, content }` pairs. Any content before\n * the first `# ` heading is placed into `00-preamble.md`.\n */\nfunction splitByHeadings(content: string): Array<{ filename: string; content: string }> {\n  const sections: Array<{ filename: string; content: string }> = [];\n\n  // Split on lines that start with exactly \"# \" (top-level heading)\n  const parts = content.split(/^(?=# )/m);\n\n  for (const part of parts) {\n    const trimmed = part.trimEnd();\n    if (!trimmed) continue;\n\n    const headingMatch = trimmed.match(/^# (.+)/);\n    if (headingMatch) {\n      const slug = slugify(headingMatch[1]);\n      const filename = slug ? `${slug}.md` : '00-preamble.md';\n      sections.push({ filename, content: trimmed + '\\n' });\n    } else {\n      // Content before the first heading\n      sections.push({ filename: '00-preamble.md', content: trimmed + '\\n' });\n    }\n  }\n\n  return sections;\n}\n\n/**\n * Write spec output to disk with overwrite protection.\n *\n * In single-file mode, writes content directly to `outputPath`.\n * In multi-file mode, splits content on top-level `# ` headings and\n * writes each section to a separate file in the directory of `outputPath`.\n *\n * @param content - The full AI-generated spec markdown\n * @param options - Output path, force flag, and multi-file mode\n * @returns Array of absolute paths to all written files\n * @throws SpecExistsError if files exist and force=false\n */\nexport async function writeSpec(content: string, options: WriteSpecOptions): Promise<string[]> {\n  const { outputPath, force, multiFile } = options;\n\n  if (!multiFile) {\n    // Single-file mode\n    if (!force && await fileExists(outputPath)) {\n      throw new SpecExistsError([outputPath]);\n    }\n\n    await mkdir(path.dirname(outputPath), { recursive: true });\n    await writeFile(outputPath, content, 'utf-8');\n    return [outputPath];\n  }\n\n  // Multi-file mode: split on top-level headings\n  const outputDir = path.dirname(outputPath);\n  const sections = splitByHeadings(content);\n\n  // Check all target files for existence before writing any\n  if (!force) {\n    const conflicts: string[] = [];\n    for (const section of sections) {\n      const filePath = path.join(outputDir, section.filename);\n      if (await fileExists(filePath)) {\n        conflicts.push(filePath);\n      }\n    }\n    if (conflicts.length > 0) {\n      throw new SpecExistsError(conflicts);\n    }\n  }\n\n  // Create output directory and write all sections\n  await mkdir(outputDir, { recursive: true });\n\n  const writtenPaths: string[] = [];\n  for (const section of sections) {\n    const filePath = path.join(outputDir, section.filename);\n    await writeFile(filePath, section.content, 'utf-8');\n    writtenPaths.push(filePath);\n  }\n\n  return writtenPaths;\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**writeSpec() writes AI-generated project specifications to disk with multi-file splitting, overwrite protection via SpecExistsError, and heading-based content segmentation.**\n\n## Exported Interface\n\n**WriteSpecOptions** configures output behavior:\n- `outputPath: string` — Full path to output file (e.g., `/project/specs/SPEC.md`)\n- `force: boolean` — Overwrite existing files without throwing SpecExistsError\n- `multiFile: boolean` — Split content on top-level `# ` headings into separate files\n\n**SpecExistsError** signals file collision when `force=false`:\n- Extends Error with `name: 'SpecExistsError'`\n- `paths: string[]` — Array of conflicting file paths\n- Constructor formats error message as bulleted list with instructions to use `--force`\n\n**writeSpec(content: string, options: WriteSpecOptions): Promise<string[]>** writes specification files:\n- Single-file mode: writes `content` directly to `outputPath` after verifying directory via `mkdir(path.dirname(outputPath), { recursive: true })`\n- Multi-file mode: calls `splitByHeadings(content)` to extract sections, writes each to `path.join(outputDir, section.filename)`\n- Returns array of absolute paths to all written files\n- Throws SpecExistsError if any target path exists and `force=false`\n- Preemptively checks all target paths before writing any files in multi-file mode to ensure atomic failure\n\n## Content Splitting Algorithm\n\n**splitByHeadings(content: string)** partitions markdown on top-level headings:\n- Splits on regex `/^(?=# )/m` (positive lookahead matching lines starting with `# `)\n- Extracts heading text via `/^# (.+)/` pattern match\n- Maps heading text to filename via `slugify()` transformation\n- Assigns preamble content (text before first `# ` heading) to `00-preamble.md`\n- Returns `Array<{ filename: string; content: string }>` with each section's filename and trimmed content plus newline\n\n**slugify(heading: string)** sanitizes heading text into filesystem-safe slug:\n- Lowercases via `.toLowerCase()`\n- Replaces whitespace with hyphens via `/\\s+/g` → `'-'`\n- Strips non-alphanumeric characters (except hyphens) via `/[^a-z0-9-]/g` → `''`\n- Collapses consecutive hyphens via `/-+/g` → `'-'`\n- Trims leading/trailing hyphens via `/^-|-$/g` → `''`\n- Example: `\"Project Overview & Goals\"` → `\"project-overview-goals\"`\n\n## File System Operations\n\n**fileExists(filePath: string): Promise<boolean>** checks file presence:\n- Calls `access(filePath, constants.F_OK)` from `node:fs/promises`\n- Returns `true` if file exists, `false` if access throws (no file)\n\n**Directory creation** via `mkdir(path.dirname(outputPath), { recursive: true })`:\n- Creates parent directories atomically before writing\n- Used in both single-file and multi-file modes\n\n**Write operations** via `writeFile(filePath, content, 'utf-8')`:\n- Single-file: writes once to `outputPath`\n- Multi-file: loops through sections array, writing each to `path.join(outputDir, section.filename)`\n\n## Integration Points\n\n**Caller expectations** (from `src/cli/specify.ts`):\n- Catch SpecExistsError and display `error.message` (contains formatted path list and `--force` hint)\n- Pass `WriteSpecOptions` with `outputPath` resolved from config, `force` from CLI flag, `multiFile` from `--multi-file` flag\n- Log returned paths array to confirm successful writes",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 928,
      "cacheReadTokens": 0,
      "cacheCreationTokens": 27589,
      "latencyMs": 31127,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/specify/writer.ts",
          "sizeBytes": 4490
        }
      ]
    },
    {
      "timestamp": "2026-02-09T16:10:46.054Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/specify/prompts.ts\n\n```typescript\nimport type { AgentsDocs } from '../generation/collector.js';\n\n/**\n * Prompt pair for spec generation: system instructions and user content.\n */\nexport interface SpecPrompt {\n  system: string;\n  user: string;\n}\n\n/**\n * System prompt for AI-driven specification synthesis.\n *\n * Enforces conceptual grouping by concern, prohibits folder-mirroring\n * and exact file path prescription, and targets AI agent consumption.\n */\nexport const SPEC_SYSTEM_PROMPT = `You produce software specifications from documentation.\n\nTASK:\nGenerate a comprehensive specification document from the provided AGENTS.md content. The specification must contain enough detail for an AI agent to reconstruct the entire project from scratch without seeing the original source code.\n\nAUDIENCE: AI agents (LLMs) — use structured, precise, instruction-oriented language. Every statement should be actionable.\n\nORGANIZATION (MANDATORY):\nGroup content by CONCERN, not by directory structure. Use these conceptual sections in order:\n\n1. Project Overview — purpose, core value proposition, problem solved, technology stack with versions\n2. Architecture — system design, module boundaries, data flow patterns, key design decisions and their rationale\n3. Public API Surface — all exported interfaces, function signatures with full parameter and return types, type definitions, error contracts\n4. Data Structures & State — key types, schemas, config objects, state management patterns, serialization formats\n5. Configuration — all config options with types, defaults, validation rules, environment variables\n6. Dependencies — each external dependency with exact version and rationale for inclusion\n7. Behavioral Contracts — Split into two subsections:\n   a. Runtime Behavior: error handling strategies (exact error types/codes and when thrown), retry logic (formulas, delay values), concurrency model, lifecycle hooks, resource management\n   b. Implementation Contracts: every regex pattern used for parsing/validation/extraction (verbatim in backticks), every format string and output template (exact structure with examples), every magic constant and sentinel value with its meaning, every environment variable with expected values, every file format specification (YAML schemas, NDJSON structures). These are reproduction-critical — an AI agent needs them to rebuild the system with identical observable behavior.\n8. Test Contracts — what each module's tests should verify: scenarios, edge cases, expected behaviors, error conditions\n9. Build Plan — phased implementation sequence: what to build first and why, dependency order between modules, incremental milestones\n\nRULES:\n- Describe MODULE BOUNDARIES and their interfaces — not file paths or directory layouts\n- Use exact function, type, and constant names as they appear in the documentation\n- Include FULL type signatures for all public APIs (parameters, return types, generics)\n- Do NOT prescribe exact filenames or file paths — describe what each module does and exports\n- Do NOT mirror the project's folder structure in your section organization\n- Do NOT use directory names as section headings\n- Include version numbers for ALL external dependencies\n- The Build Plan MUST list implementation phases with explicit dependency ordering\n- Each Build Plan phase must state what it depends on and what it enables\n- Behavioral Contracts must specify exact error types/codes and when they are thrown\n- Behavioral Contracts MUST include verbatim regex patterns, format strings, and magic constants from the source documents — do NOT paraphrase regex patterns into prose descriptions\n- When multiple modules reference the same constant or pattern, consolidate into a single definition with cross-references to the modules that use it\n\nOUTPUT: Raw markdown. No preamble. No meta-commentary. No \"Here is...\" or \"I've generated...\" prefix.`;\n\n/**\n * Build the system + user prompt pair for spec generation.\n *\n * Injects all collected AGENTS.md content with section delimiters.\n *\n * @param docs - Collected AGENTS.md documents from collectAgentsDocs()\n * @returns SpecPrompt with system and user prompt strings\n */\nexport function buildSpecPrompt(docs: AgentsDocs): SpecPrompt {\n  const agentsSections = docs.map(\n    (doc) => `### ${doc.relativePath}\\n\\n${doc.content}`,\n  );\n\n  const userSections: string[] = [\n    'Generate a comprehensive project specification from the following documentation.',\n    '',\n    `## AGENTS.md Files (${docs.length} directories)`,\n    '',\n    ...agentsSections,\n  ];\n\n  userSections.push(\n    '',\n    '## Output Requirements',\n    '',\n    'The specification MUST include these sections in order:',\n    '1. Project Overview (purpose, value, tech stack)',\n    '2. Architecture (module boundaries, data flow, design decisions)',\n    '3. Public API Surface (all exported interfaces, full type signatures)',\n    '4. Data Structures & State (types, schemas, config objects)',\n    '5. Configuration (options, types, defaults, validation)',\n    '6. Dependencies (each with version and rationale)',\n    '7. Behavioral Contracts (error handling, concurrency, lifecycle, PLUS verbatim regex patterns, format specs, magic constants, templates)',\n    '8. Test Contracts (per-module test scenarios and edge cases)',\n    '9. Build Plan (phased implementation order with dependencies)',\n    '',\n    'Output ONLY the markdown content. No preamble.',\n  );\n\n  return {\n    system: SPEC_SYSTEM_PROMPT,\n    user: userSections.join('\\n'),\n  };\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**prompts.ts constructs system and user prompts for AI-driven project specification synthesis from AGENTS.md corpus, enforcing concern-based organization and behavioral contract preservation.**\n\n## Exported Types\n\n`SpecPrompt` interface contains `system: string` and `user: string` properties representing the prompt pair for AI specification generation.\n\n## Exported Constants\n\n`SPEC_SYSTEM_PROMPT` defines the system instruction string for specification synthesis, containing 2,850+ characters of constraints and formatting rules.\n\n## Exported Functions\n\n`buildSpecPrompt(docs: AgentsDocs): SpecPrompt` assembles the prompt pair by injecting all AGENTS.md documents from `docs` array into user prompt with section delimiters formatted as `### ${doc.relativePath}`, appending output requirements list, and returning object with `system: SPEC_SYSTEM_PROMPT` and `user: userSections.join('\\n')`.\n\n## Prompt Architecture\n\n`SPEC_SYSTEM_PROMPT` enforces nine mandatory sections in sequence: (1) Project Overview with technology stack versions, (2) Architecture with module boundaries and design rationale, (3) Public API Surface with full type signatures, (4) Data Structures & State including serialization formats, (5) Configuration with types/defaults/validation, (6) Dependencies with exact versions and rationale, (7) Behavioral Contracts split into Runtime Behavior (error types/codes, retry formulas, concurrency model, lifecycle hooks) and Implementation Contracts (verbatim regex patterns, format strings, magic constants, environment variables, file format specs), (8) Test Contracts with per-module scenarios and edge cases, (9) Build Plan with phased implementation sequence and dependency ordering.\n\n## Organization Constraints\n\nSystem prompt prohibits folder-mirroring via rules: \"Describe MODULE BOUNDARIES and their interfaces — not file paths or directory layouts\", \"Do NOT prescribe exact filenames or file paths\", \"Do NOT mirror the project's folder structure\", \"Do NOT use directory names as section headings\". Mandates conceptual grouping by concern rather than directory structure.\n\n## Behavioral Contract Requirements\n\nSystem prompt mandates \"MUST include verbatim regex patterns, format strings, and magic constants from the source documents — do NOT paraphrase regex patterns into prose descriptions\". Specifies consolidation of duplicated constants across modules into single definition with cross-references. Implementation Contracts subsection must capture reproduction-critical details: regex patterns in backticks, format string exact structures with examples, magic constant meanings, environment variable expected values, YAML schemas, NDJSON structures.\n\n## User Prompt Construction\n\n`buildSpecPrompt` creates `userSections` array starting with task description \"Generate a comprehensive project specification from the following documentation\", followed by `## AGENTS.md Files (${docs.length} directories)` header, then maps `docs` array to section strings via `doc => '### ${doc.relativePath}\\n\\n${doc.content}'`, appends `## Output Requirements` section repeating the nine mandatory sections list with \"(error handling, concurrency, lifecycle, PLUS verbatim regex patterns, format specs, magic constants, templates)\" for Behavioral Contracts, and terminates with \"Output ONLY the markdown content. No preamble.\" instruction.\n\n## Output Format Enforcement\n\nBoth prompts enforce no-preamble output via terminal instructions: system prompt ends with \"OUTPUT: Raw markdown. No preamble. No meta-commentary. No 'Here is...' or 'I've generated...' prefix.\", user prompt ends with \"Output ONLY the markdown content. No preamble.\" This prevents conversational wrapper text in AI responses.\n\n## Integration Points\n\nConsumes `AgentsDocs` type from `../generation/collector.js` representing array of objects with `relativePath: string` and `content: string` properties. Called by `src/specify/index.ts` orchestrator which invokes `collectAgentsDocs()` to gather all AGENTS.md files, passes result to `buildSpecPrompt()`, then feeds resulting prompts to `AIService.call()`.\n\n## Audience Targeting\n\nSystem prompt specifies \"AUDIENCE: AI agents (LLMs) — use structured, precise, instruction-oriented language. Every statement should be actionable.\" This contrasts with human-targeted documentation by requiring all content to be formulated as executable instructions for code generation rather than explanatory prose.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 976,
      "cacheReadTokens": 17114,
      "cacheCreationTokens": 10353,
      "latencyMs": 40182,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/specify/prompts.ts",
          "sizeBytes": 5497
        }
      ]
    },
    {
      "timestamp": "2026-02-09T16:11:04.019Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/hooks/are-check-update.js\n\n```javascript\n#!/usr/bin/env node\n/**\n * ARE Update Check Hook\n *\n * Checks for ARE updates in background, writes result to cache.\n * Called by SessionStart hook - runs once per session.\n *\n * Cache file: ~/.claude/cache/are-update-check.json\n */\n\nimport { existsSync, mkdirSync, readFileSync, writeFileSync } from 'fs';\nimport { homedir } from 'os';\nimport { join } from 'path';\nimport { spawn, execSync } from 'child_process';\n\nconst homeDir = homedir();\nconst cwd = process.cwd();\nconst cacheDir = join(homeDir, '.claude', 'cache');\nconst cacheFile = join(cacheDir, 'are-update-check.json');\n\n// ARE-VERSION file locations (check project first, then global)\nconst projectVersionFile = join(cwd, '.claude', 'ARE-VERSION');\nconst globalVersionFile = join(homeDir, '.claude', 'ARE-VERSION');\n\n// Ensure cache directory exists\nif (!existsSync(cacheDir)) {\n  mkdirSync(cacheDir, { recursive: true });\n}\n\n// Run check in background (spawn background process)\nconst child = spawn(process.execPath, ['-e', `\n  const fs = require('fs');\n  const { execSync } = require('child_process');\n\n  const cacheFile = ${JSON.stringify(cacheFile)};\n  const projectVersionFile = ${JSON.stringify(projectVersionFile)};\n  const globalVersionFile = ${JSON.stringify(globalVersionFile)};\n\n  // Check project directory first (local install), then global\n  let installed = '0.0.0';\n  try {\n    if (fs.existsSync(projectVersionFile)) {\n      installed = fs.readFileSync(projectVersionFile, 'utf8').trim();\n    } else if (fs.existsSync(globalVersionFile)) {\n      installed = fs.readFileSync(globalVersionFile, 'utf8').trim();\n    }\n  } catch (e) {}\n\n  let latest = null;\n  try {\n    latest = execSync('npm view agents-reverse-engineer version', { encoding: 'utf8', timeout: 10000, windowsHide: true }).trim();\n  } catch (e) {}\n\n  const result = {\n    update_available: latest && installed !== latest,\n    installed,\n    latest: latest || 'unknown',\n    checked: Math.floor(Date.now() / 1000)\n  };\n\n  fs.writeFileSync(cacheFile, JSON.stringify(result));\n`], {\n  stdio: 'ignore',\n  detached: true,\n  windowsHide: true,\n});\n\nchild.unref();\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**are-check-update.js spawns a detached background process that queries npm for the latest `agents-reverse-engineer` version, compares it against the local `ARE-VERSION` file (checking project `.claude/ARE-VERSION` before global `~/.claude/ARE-VERSION`), and writes the update availability result to `~/.claude/cache/are-update-check.json`.**\n\n## Entry Point\n\nSessionStart hook script invoked by Claude Code IDE. Runs synchronously but immediately spawns detached subprocess for actual network check to avoid blocking session initialization.\n\n## Version File Resolution Strategy\n\nChecks version files in priority order:\n1. `${cwd}/.claude/ARE-VERSION` (project-local installation)\n2. `${homedir}/.claude/ARE-VERSION` (global installation)\n\nFalls back to `'0.0.0'` if neither exists.\n\n## Background Subprocess Spawning\n\nUses `spawn(process.execPath, ['-e', scriptString], { stdio: 'ignore', detached: true, windowsHide: true })` pattern followed by `child.unref()` to create detached Node.js process that survives parent termination. Script string is inline JavaScript code interpolated with `JSON.stringify()` for paths.\n\n## Network Check Behavior\n\nBackground process executes `npm view agents-reverse-engineer version` via `execSync()` with 10-second timeout. Network failures set `latest: 'unknown'` but do not throw. Comparison uses strict string equality between `installed` and `latest` versions.\n\n## Cache Output Format\n\nWrites JSON object to `~/.claude/cache/are-update-check.json`:\n```json\n{\n  \"update_available\": boolean,\n  \"installed\": string,\n  \"latest\": string | \"unknown\",\n  \"checked\": number  // Unix timestamp (seconds)\n}\n```\n\n## Directory Creation\n\nCalls `mkdirSync(cacheDir, { recursive: true })` to ensure `~/.claude/cache/` exists before spawning subprocess. Cache directory creation is synchronous and blocks hook execution.\n\n## Error Handling\n\nTry-catch blocks in spawned script swallow errors from:\n- Version file reads (defaults to `'0.0.0'`)\n- npm registry queries (sets `latest: null` → `'unknown'`)\n\nNo error propagation to parent process due to `stdio: 'ignore'` and detached execution.\n\n## Platform Compatibility\n\n`windowsHide: true` option prevents console window flash on Windows. Homedir resolution via `os.homedir()` handles cross-platform user directory lookup. `path.join()` ensures correct path separators.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 608,
      "cacheReadTokens": 17114,
      "cacheCreationTokens": 9759,
      "latencyMs": 23509,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "hooks/are-check-update.js",
          "sizeBytes": 2097
        }
      ]
    },
    {
      "timestamp": "2026-02-09T16:11:05.947Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/hooks/are-session-end.js\n\n```javascript\n#!/usr/bin/env node\n/**\n * ARE Session End Hook\n *\n * Triggers `are update` when session ends (if there are uncommitted changes).\n * Runs in background to avoid blocking session close.\n *\n * Disable via:\n * - Environment variable: ARE_DISABLE_HOOK=1\n * - Config file: hook_enabled: false in .agents-reverse-engineer.yaml\n */\n\nimport { execSync, spawn } from 'child_process';\nimport { existsSync, readFileSync } from 'fs';\n\n// Check for disable flag\nif (process.env.ARE_DISABLE_HOOK === '1') {\n  process.exit(0);\n}\n\n// Check config file for permanent disable\nconst configPath = '.agents-reverse-engineer.yaml';\nif (existsSync(configPath)) {\n  const config = readFileSync(configPath, 'utf-8');\n  if (config.includes('hook_enabled: false')) {\n    process.exit(0);\n  }\n}\n\n// Check git status - skip if no changes\ntry {\n  const status = execSync('git status --porcelain', { encoding: 'utf-8' });\n  if (!status.trim()) {\n    // No changes since last run - exit silently\n    process.exit(0);\n  }\n} catch {\n  // Not a git repo or git not available - exit silently\n  process.exit(0);\n}\n\n// Run update in background (don't block session close)\nconst child = spawn('npx', ['agents-reverse-engineer@latest', 'update', '--quiet'], {\n  stdio: 'ignore',\n  detached: true,\n});\nchild.unref();\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**are-session-end.js spawns detached background `npx agents-reverse-engineer@latest update --quiet` subprocess when session ends if uncommitted git changes detected, with disable mechanisms via `ARE_DISABLE_HOOK=1` environment variable or `hook_enabled: false` config substring match.**\n\n## Hook Lifecycle Integration\n\nExecutes during SessionEnd event in Claude Code/Gemini CLI runtime. Designed to terminate quickly without blocking session close via detached subprocess pattern.\n\n## Disable Mechanisms\n\nExits silently (status 0) if `process.env.ARE_DISABLE_HOOK === '1'` before performing any filesystem or git operations.\n\nReads `.agents-reverse-engineer.yaml` via `existsSync()` and `readFileSync(configPath, 'utf-8')`, performs substring search for `'hook_enabled: false'` (no YAML parser) and exits if match found.\n\n## Change Detection Strategy\n\nInvokes `execSync('git status --porcelain', { encoding: 'utf-8' })` to obtain working tree status. Exits silently if output is empty string after `trim()` (no changes since last commit) or if command throws (non-git repo, git unavailable).\n\n## Background Update Spawn\n\nCalls `spawn('npx', ['agents-reverse-engineer@latest', 'update', '--quiet'], { stdio: 'ignore', detached: true })` to execute incremental update workflow. Child process detached from parent via `detached: true` and unreferenced via `child.unref()` to allow parent exit without waiting for completion.\n\nUses `stdio: 'ignore'` to suppress all output streams, preventing stdio inheritance that could block session close.\n\n## Execution Environment\n\nShebang `#!/usr/bin/env node` enables direct execution as script. Imports `execSync`, `spawn` from `'child_process'` module and `existsSync`, `readFileSync` from `'fs'` module using ES module syntax.\n\nNo error handling for spawn failure—detached process errors are silently lost due to `stdio: 'ignore'`.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 485,
      "cacheReadTokens": 17114,
      "cacheCreationTokens": 9504,
      "latencyMs": 22276,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "hooks/are-session-end.js",
          "sizeBytes": 1276
        }
      ]
    },
    {
      "timestamp": "2026-02-09T16:11:00.755Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/update/types.ts\n\n```typescript\n/**\n * Types for incremental update workflow\n */\nimport type { FileChange } from '../change-detection/types.js';\n\n/**\n * Result of orphan cleanup\n */\nexport interface CleanupResult {\n  /** .sum files that were deleted */\n  deletedSumFiles: string[];\n  /** AGENTS.md files that were deleted (from empty directories) */\n  deletedAgentsMd: string[];\n}\n\n/**\n * Options for the update command\n */\nexport interface UpdateOptions {\n  /** Include uncommitted changes (staged + working directory) */\n  includeUncommitted?: boolean;\n  /** Dry run - show what would change without making changes */\n  dryRun?: boolean;\n}\n\n/**\n * Result of an update run\n */\nexport interface UpdateResult {\n  /** Files that were analyzed (added or modified) */\n  analyzedFiles: string[];\n  /** Files that were skipped (unchanged) */\n  skippedFiles: string[];\n  /** Cleanup result (deleted .sum and AGENTS.md files) */\n  cleanup: CleanupResult;\n  /** Directories whose AGENTS.md was regenerated */\n  regeneratedDirs: string[];\n  /** Git commit hash at start of update */\n  baseCommit: string;\n  /** Git commit hash at end of update */\n  currentCommit: string;\n  /** Whether this was a dry run */\n  dryRun: boolean;\n}\n\n/**\n * Progress callback for update operations\n */\nexport interface UpdateProgress {\n  /** Called when a file is about to be processed */\n  onFileStart?: (path: string, status: 'analyzing' | 'skipping') => void;\n  /** Called when a file is done processing */\n  onFileDone?: (path: string, status: 'analyzed' | 'skipped' | 'error') => void;\n  /** Called when cleanup deletes a file */\n  onCleanup?: (path: string, type: 'sum' | 'agents-md') => void;\n  /** Called when a directory AGENTS.md is regenerated */\n  onDirRegenerate?: (path: string) => void;\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**types.ts defines TypeScript interfaces for the incremental update workflow including change results, cleanup operations, progress callbacks, and command options.**\n\n## Exported Interfaces\n\n### CleanupResult\nRepresents the outcome of orphan cleanup operations performed during incremental updates.\n\n**Properties:**\n- `deletedSumFiles: string[]` — Paths to `.sum` files removed because their source files were deleted or renamed\n- `deletedAgentsMd: string[]` — Paths to `AGENTS.md` files removed from directories that became empty after cleanup\n\n### UpdateOptions\nConfiguration flags for the `are update` command execution.\n\n**Properties:**\n- `includeUncommitted?: boolean` — When true, includes both staged and working directory changes via `git status --porcelain` merge with `git diff`; when false, only processes committed changes since last run\n- `dryRun?: boolean` — When true, computes change set and displays preview without writing `.sum` files, regenerating `AGENTS.md`, or deleting orphans\n\n### UpdateResult\nComplete summary of an incremental update execution returned by update orchestrator.\n\n**Properties:**\n- `analyzedFiles: string[]` — Source files regenerated due to `content_hash` mismatch (status `'added'` or `'modified'` from `FileChange`)\n- `skippedFiles: string[]` — Source files with matching `content_hash` in YAML frontmatter (SHA-256 comparison passed)\n- `cleanup: CleanupResult` — Orphan removal results from `cleanupOrphans()` and `cleanupEmptyDirectoryDocs()`\n- `regeneratedDirs: string[]` — Directory paths whose `AGENTS.md` files were synthesized via Phase 2 aggregation after child `.sum` changes\n- `baseCommit: string` — Git commit SHA at update start (typically from previous update or HEAD~1)\n- `currentCommit: string` — Git commit SHA at update end (typically HEAD after processing changes)\n- `dryRun: boolean` — Mirrors the `UpdateOptions.dryRun` flag indicating preview-only execution\n\n### UpdateProgress\nCallback interface for streaming progress notifications during incremental update operations, consumed by `ProgressReporter` in `src/orchestration/progress.ts`.\n\n**Properties:**\n- `onFileStart?: (path: string, status: 'analyzing' | 'skipping') => void` — Invoked before processing each source file; `'analyzing'` for hash mismatch, `'skipping'` for hash match\n- `onFileDone?: (path: string, status: 'analyzed' | 'skipped' | 'error') => void` — Invoked after processing completes; `'error'` status indicates subprocess failure or timeout\n- `onCleanup?: (path: string, type: 'sum' | 'agents-md') => void` — Invoked when `cleanupOrphans()` deletes a `.sum` file or `cleanupEmptyDirectoryDocs()` removes an `AGENTS.md`\n- `onDirRegenerate?: (path: string) => void` — Invoked when directory aggregation rewrites an `AGENTS.md` file due to child `.sum` changes in `affectedDirs` set\n\n## Integration Points\n\nThis module re-exports `FileChange` from `../change-detection/types.js`, which represents git-detected or SHA-256-detected modifications with `status: 'added' | 'modified' | 'deleted' | 'renamed'` discriminated union. Update orchestrator (`src/update/orchestrator.ts`) consumes these types to coordinate hash-based change detection, Phase 1 pool execution for modified files, orphan cleanup via `orphan-cleaner.ts`, and targeted directory regeneration.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 877,
      "cacheReadTokens": 17114,
      "cacheCreationTokens": 9608,
      "latencyMs": 29363,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/update/types.ts",
          "sizeBytes": 1741
        }
      ]
    },
    {
      "timestamp": "2026-02-09T16:11:11.236Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/scripts/build-hooks.js\n\n```javascript\n#!/usr/bin/env node\n/**\n * Build Hooks Script\n *\n * Copies hook source files from hooks/ to hooks/dist/ for npm bundling.\n * Run via: npm run build:hooks\n * Called automatically during: npm run prepublishOnly\n */\n\nimport { copyFileSync, existsSync, mkdirSync, readdirSync } from 'fs';\nimport { join, dirname } from 'path';\nimport { fileURLToPath } from 'url';\n\nconst __filename = fileURLToPath(import.meta.url);\nconst __dirname = dirname(__filename);\nconst projectRoot = join(__dirname, '..');\n\nconst HOOKS_SRC = join(projectRoot, 'hooks');\nconst HOOKS_DIST = join(projectRoot, 'hooks', 'dist');\n\n// Ensure dist directory exists\nif (!existsSync(HOOKS_DIST)) {\n  mkdirSync(HOOKS_DIST, { recursive: true });\n}\n\n// Copy all .js files from hooks/ to hooks/dist/\nconst hookFiles = readdirSync(HOOKS_SRC).filter(\n  (f) => f.endsWith('.js') && f !== 'dist'\n);\n\nconsole.log('Building hooks...');\nfor (const file of hookFiles) {\n  const src = join(HOOKS_SRC, file);\n  const dest = join(HOOKS_DIST, file);\n  copyFileSync(src, dest);\n  console.log(`  Copied: ${file} -> hooks/dist/${file}`);\n}\n\nconsole.log(`Done. ${hookFiles.length} hook(s) built.`);\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**build-hooks.js copies JavaScript hook files from `hooks/` to `hooks/dist/` for npm package distribution.**\n\n## Execution Context\n\nRuns as Node.js script via `#!/usr/bin/env node` shebang. Invoked by `npm run build:hooks` and automatically during `npm run prepublishOnly` lifecycle hook (see package.json build pipeline).\n\n## File Operations\n\nUses synchronous fs operations: `copyFileSync()` for file copying, `existsSync()` for dist directory detection, `mkdirSync()` with `{ recursive: true }` for dist creation, `readdirSync()` for source file enumeration. Filters files via `.endsWith('.js') && f !== 'dist'` predicate to exclude non-JavaScript files and the dist directory itself.\n\n## Path Resolution\n\nResolves `__dirname` via `fileURLToPath(import.meta.url)` (ES module compatibility pattern). Constructs `projectRoot` as `join(__dirname, '..')`, `HOOKS_SRC` as `join(projectRoot, 'hooks')`, `HOOKS_DIST` as `join(projectRoot, 'hooks', 'dist')`.\n\n## Build Workflow\n\n1. Creates `hooks/dist/` directory if missing via `mkdirSync(HOOKS_DIST, { recursive: true })`\n2. Discovers `.js` files in `hooks/` via `readdirSync(HOOKS_SRC).filter()`\n3. Copies each file from `join(HOOKS_SRC, file)` to `join(HOOKS_DIST, file)` via `copyFileSync()`\n4. Logs progress: `\"Building hooks...\"` header, per-file `\"Copied: {file} -> hooks/dist/{file}\"` entries, summary `\"Done. {count} hook(s) built.\"`\n\n## Integration with Package Distribution\n\nEnsures session lifecycle hooks (`are-check-update.js`, `are-session-end.js`, `opencode-are-check-update.js`, `opencode-are-session-end.js`) are bundled in npm tarball for installation into IDE config directories (`~/.claude/hooks/`, `~/.config/opencode/plugins/`) via installer module (`src/installer/operations.ts`).",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 532,
      "cacheReadTokens": 17114,
      "cacheCreationTokens": 9485,
      "latencyMs": 20992,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "scripts/build-hooks.js",
          "sizeBytes": 1141
        }
      ]
    },
    {
      "timestamp": "2026-02-09T16:11:13.439Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/version.ts\n\n```typescript\nimport { readFileSync } from 'node:fs';\nimport { dirname, join } from 'node:path';\nimport { fileURLToPath } from 'node:url';\n\n/**\n * Get package version from package.json.\n */\nexport function getVersion(): string {\n  try {\n    const __dirname = dirname(fileURLToPath(import.meta.url));\n    const packagePath = join(__dirname, '..', 'package.json');\n    const packageJson = JSON.parse(readFileSync(packagePath, 'utf-8'));\n    return packageJson.version || 'unknown';\n  } catch {\n    return 'unknown';\n  }\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**getVersion() reads package.json from the filesystem to return the current package version string, with fallback to 'unknown' on error.**\n\n## Exported Functions\n\n- **getVersion(): string** — Synchronously reads `package.json` from the parent directory of the compiled module, parses JSON, extracts `version` field, returns 'unknown' on any error (file not found, JSON parse failure, missing version field)\n\n## Implementation Details\n\nUses `import.meta.url` → `fileURLToPath()` → `dirname()` to resolve current module directory, then `join(__dirname, '..', 'package.json')` to traverse one level up (from `dist/` to project root in compiled output). Reads file synchronously via `readFileSync(packagePath, 'utf-8')`, parses with `JSON.parse()`, accesses `packageJson.version` with `|| 'unknown'` fallback for nullish values. Wraps entire operation in try-catch returning 'unknown' on exception.\n\n## Usage Context\n\nCalled by CLI entry point (`src/cli/index.ts`) to display version in help text and version flag output. Called by session lifecycle hooks (`hooks/are-check-update.js`) to compare local version against npm registry during update checks. Read at runtime from compiled `dist/version.js` after build process (`npm run build` via tsc).\n\n## Path Resolution\n\nCompiled module location: `dist/version.js` (ES2022 target, NodeNext module resolution per tsconfig.json). Package.json location: project root `package.json`. Relative traversal: `dist/` → project root via `..` ensures correct resolution regardless of execution context (global install, npx, local node_modules).",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 395,
      "cacheReadTokens": 17114,
      "cacheCreationTokens": 9252,
      "latencyMs": 20796,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/version.ts",
          "sizeBytes": 506
        }
      ]
    },
    {
      "timestamp": "2026-02-09T16:11:18.533Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/LICENSE\n\n```text\nMIT License\n\nCopyright (c) 2026 GeoloeG-IsT\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**LICENSE grants MIT License permissions for agents-reverse-engineer software under Copyright (c) 2026 GeoloeG-IsT.**\n\n## License Type and Grant\n\nMIT License permits unrestricted use, modification, distribution, sublicensing, and sale of the Software and associated documentation files without royalties or fees, subject to preservation of copyright notice and permission notice in all copies or substantial portions.\n\n## Copyright Holder\n\nCopyright (c) 2026 GeoloeG-IsT owns all intellectual property rights to the agents-reverse-engineer codebase.\n\n## Warranty Disclaimer\n\nSOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT disclaims all warranties and representations.\n\n## Liability Limitation\n\nAUTHORS OR COPYRIGHT HOLDERS SHALL NOT BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE limits legal exposure for maintainers.\n\n## Required Attribution\n\n\"The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software\" mandates preservation of attribution in derivative works and distributions.\n\n## Integration Impact\n\nCI/CD workflow (`.github/workflows/publish.yml`) publishes npm package with `--provenance --access public` flags, making MIT License terms apply to all distributed artifacts. Package.json `\"license\": \"MIT\"` field declares license for npm registry. README.md references LICENSE file as source of legal terms for users and contributors.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 357,
      "cacheReadTokens": 17114,
      "cacheCreationTokens": 9320,
      "latencyMs": 16345,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "LICENSE",
          "sizeBytes": 1068
        }
      ]
    },
    {
      "timestamp": "2026-02-09T16:10:58.447Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/update/orchestrator.ts\n\n```typescript\n/**\n * Update orchestrator\n *\n * Coordinates incremental documentation updates using frontmatter-based change detection:\n * 1. Check git repository status\n * 2. Scan for existing .sum files\n * 3. Compare content hashes from frontmatter with current file hashes\n * 4. Clean up orphaned .sum files\n * 5. Prepare analysis tasks for changed files\n * 6. Track affected directories for AGENTS.md regeneration\n */\nimport * as path from 'node:path';\nimport pc from 'picocolors';\nimport type { Config } from '../config/schema.js';\nimport {\n  isGitRepo,\n  getCurrentCommit,\n  computeContentHash,\n  type FileChange,\n} from '../change-detection/index.js';\nimport { cleanupOrphans, getAffectedDirectories } from './orphan-cleaner.js';\nimport { readSumFile, getSumPath } from '../generation/writers/sum.js';\nimport type { UpdateOptions, CleanupResult } from './types.js';\nimport { discoverFiles as runDiscovery } from '../discovery/run.js';\nimport type { ITraceWriter } from '../orchestration/trace.js';\n\n/**\n * Result of update preparation (before analysis).\n */\nexport interface UpdatePlan {\n  /** Files to analyze (added or modified) */\n  filesToAnalyze: FileChange[];\n  /** Files to skip (unchanged based on content hash) */\n  filesToSkip: string[];\n  /** Cleanup result (files to delete) */\n  cleanup: CleanupResult;\n  /** Directories that need AGENTS.md regeneration */\n  affectedDirs: string[];\n  /** Base commit (not used in frontmatter mode, kept for compatibility) */\n  baseCommit: string;\n  /** Current commit */\n  currentCommit: string;\n  /** Whether this is first run (no .sum files exist) */\n  isFirstRun: boolean;\n}\n\n/**\n * Orchestrates incremental documentation updates using frontmatter-based change detection.\n */\nexport class UpdateOrchestrator {\n  private config: Config;\n  private projectRoot: string;\n  private tracer?: ITraceWriter;\n  private debug: boolean;\n\n  constructor(\n    config: Config,\n    projectRoot: string,\n    options?: { tracer?: ITraceWriter; debug?: boolean }\n  ) {\n    this.config = config;\n    this.projectRoot = projectRoot;\n    this.tracer = options?.tracer;\n    this.debug = options?.debug ?? false;\n  }\n\n  /**\n   * Close resources (no-op in frontmatter mode, kept for API compatibility).\n   */\n  close(): void {\n    // No database to close in frontmatter mode\n  }\n\n  /**\n   * Check prerequisites for update.\n   *\n   * @throws Error if not in a git repository\n   */\n  async checkPrerequisites(): Promise<void> {\n    const isRepo = await isGitRepo(this.projectRoot);\n    if (!isRepo) {\n      throw new Error(\n        `Not a git repository: ${this.projectRoot}\\n` +\n        'The update command requires a git repository for change detection.'\n      );\n    }\n  }\n\n  /**\n   * Discover all source files in the project.\n   */\n  private async discoverFiles(): Promise<string[]> {\n    const filterResult = await runDiscovery(this.projectRoot, this.config, {\n      tracer: this.tracer,\n      debug: this.debug,\n    });\n\n    // Walker returns absolute paths; convert to relative for consistent usage\n    return filterResult.included.map((f: string) => path.relative(this.projectRoot, f));\n  }\n\n  /**\n   * Prepare update plan without executing analysis.\n   *\n   * Uses frontmatter-based change detection:\n   * - Reads content_hash from each .sum file\n   * - Compares with current file content hash\n   * - Files with mismatched hashes need re-analysis\n   *\n   * @param options - Update options\n   * @returns Update plan with files to analyze and cleanup actions\n   */\n  async preparePlan(options: UpdateOptions = {}): Promise<UpdatePlan> {\n    const planStartTime = process.hrtime.bigint();\n\n    // Emit phase start\n    this.tracer?.emit({\n      type: 'phase:start',\n      phase: 'update-plan-creation',\n      taskCount: 0, // Will be determined after discovery\n      concurrency: 1,\n    });\n\n    if (this.debug) {\n      console.error(pc.dim('[debug] Creating update plan with change detection...'));\n    }\n\n    await this.checkPrerequisites();\n\n    // Get current commit for reference\n    const currentCommit = await getCurrentCommit(this.projectRoot);\n\n    if (this.debug) {\n      console.error(pc.dim(`[debug] Git commit: ${currentCommit.slice(0, 7)}`));\n    }\n\n    // Discover all source files\n    if (this.debug) {\n      console.error(pc.dim('[debug] Discovering files...'));\n    }\n\n    const allFiles = await this.discoverFiles();\n\n    const filesToAnalyze: FileChange[] = [];\n    const filesToSkip: string[] = [];\n    const deletedOrRenamed: FileChange[] = [];\n\n    // Track which .sum files we've seen (to detect orphans)\n    const seenSumFiles = new Set<string>();\n\n    // Check each file against its .sum file\n    for (const relativePath of allFiles) {\n      const filePath = path.join(this.projectRoot, relativePath);\n      const sumPath = getSumPath(filePath);\n      seenSumFiles.add(sumPath);\n\n      try {\n        // Read existing .sum file\n        const sumContent = await readSumFile(sumPath);\n\n        if (!sumContent) {\n          // No .sum file exists - file needs analysis\n          filesToAnalyze.push({ path: relativePath, status: 'added' });\n          continue;\n        }\n\n        // Compare content hashes\n        const currentHash = await computeContentHash(filePath);\n        const storedHash = sumContent.contentHash;\n\n        if (!storedHash || storedHash !== currentHash) {\n          // Hash mismatch or no hash stored - file needs re-analysis\n          filesToAnalyze.push({ path: relativePath, status: 'modified' });\n        } else {\n          // Hash matches - skip this file\n          filesToSkip.push(relativePath);\n        }\n      } catch {\n        // Error reading file - skip it\n        filesToSkip.push(relativePath);\n      }\n    }\n\n    // Cleanup orphans (deleted files whose .sum files still exist)\n    const cleanup = await cleanupOrphans(\n      this.projectRoot,\n      deletedOrRenamed,\n      options.dryRun ?? false\n    );\n\n    // Get directories affected by changes (for AGENTS.md regeneration)\n    // Sort by depth descending (deepest first) so children are processed before parents\n    const affectedDirs = Array.from(getAffectedDirectories(filesToAnalyze))\n      .sort((a, b) => {\n        const depthA = a === '.' ? 0 : a.split(path.sep).length;\n        const depthB = b === '.' ? 0 : b.split(path.sep).length;\n        return depthB - depthA;\n      });\n\n    if (this.debug) {\n      console.error(\n        pc.dim(\n          `[debug] Change detection: ${filesToAnalyze.length} changed, ${filesToSkip.length} unchanged, ${cleanup.deletedSumFiles.length} orphaned`\n        )\n      );\n      console.error(pc.dim(`[debug] Affected directories: ${affectedDirs.length}`));\n    }\n\n    // Emit plan created event\n    this.tracer?.emit({\n      type: 'plan:created',\n      planType: 'update',\n      fileCount: filesToAnalyze.length,\n      taskCount: filesToAnalyze.length + affectedDirs.length, // File tasks + dir regen tasks\n    });\n\n    // Emit phase end\n    const planEndTime = process.hrtime.bigint();\n    const planDurationMs = Number(planEndTime - planStartTime) / 1_000_000;\n    this.tracer?.emit({\n      type: 'phase:end',\n      phase: 'update-plan-creation',\n      durationMs: planDurationMs,\n      tasksCompleted: 1,\n      tasksFailed: 0,\n    });\n\n    // Determine if this is first run (no files to skip means no existing .sum files)\n    const isFirstRun = filesToSkip.length === 0 && filesToAnalyze.length > 0;\n    return {\n      filesToAnalyze,\n      filesToSkip,\n      cleanup,\n      affectedDirs,\n      baseCommit: currentCommit, // Not used in frontmatter mode\n      currentCommit,\n      isFirstRun,\n    };\n  }\n\n  /**\n   * Record file analyzed (no-op in frontmatter mode - hash is stored in .sum file).\n   * Kept for API compatibility.\n   */\n  async recordFileAnalyzed(\n    _relativePath: string,\n    _contentHash: string,\n    _currentCommit: string\n  ): Promise<void> {\n    // No-op: content hash is stored in .sum file frontmatter\n  }\n\n  /**\n   * Remove file from state (no-op in frontmatter mode).\n   * Kept for API compatibility.\n   */\n  async removeFileState(_relativePath: string): Promise<void> {\n    // No-op: .sum file cleanup is handled separately\n  }\n\n  /**\n   * Record a completed update run (no-op in frontmatter mode).\n   * Kept for API compatibility.\n   */\n  async recordRun(\n    _commitHash: string,\n    _filesAnalyzed: number,\n    _filesSkipped: number\n  ): Promise<number> {\n    // No-op: no run history in frontmatter mode\n    return 0;\n  }\n\n  /**\n   * Get last run information (not available in frontmatter mode).\n   * Kept for API compatibility.\n   */\n  async getLastRun(): Promise<undefined> {\n    // No run history in frontmatter mode\n    return undefined;\n  }\n\n  /**\n   * Check if this is the first run.\n   * In frontmatter mode, checks if any .sum files exist.\n   */\n  async isFirstRun(): Promise<boolean> {\n    const plan = await this.preparePlan({ dryRun: true });\n    return plan.isFirstRun;\n  }\n}\n\n/**\n * Create an update orchestrator.\n */\nexport function createUpdateOrchestrator(\n  config: Config,\n  projectRoot: string,\n  options?: { tracer?: ITraceWriter; debug?: boolean }\n): UpdateOrchestrator {\n  return new UpdateOrchestrator(config, projectRoot, options);\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**UpdateOrchestrator coordinates frontmatter-based incremental documentation updates by comparing SHA-256 content hashes in `.sum` file YAML frontmatter against current file content, cleaning orphaned artifacts, and tracking affected directories for `AGENTS.md` regeneration.**\n\n## Exported Symbols\n\n**UpdateOrchestrator** — Class managing incremental update workflow with methods:\n- `constructor(config: Config, projectRoot: string, options?: { tracer?: ITraceWriter; debug?: boolean })`\n- `close(): void` — No-op for API compatibility (no database resources to clean)\n- `checkPrerequisites(): Promise<void>` — Throws if `projectRoot` is not a git repository via `isGitRepo()`\n- `preparePlan(options?: UpdateOptions): Promise<UpdatePlan>` — Computes which files need re-analysis by hash comparison\n- `recordFileAnalyzed(_relativePath: string, _contentHash: string, _currentCommit: string): Promise<void>` — No-op (hash stored in frontmatter)\n- `removeFileState(_relativePath: string): Promise<void>` — No-op for API compatibility\n- `recordRun(_commitHash: string, _filesAnalyzed: number, _filesSkipped: number): Promise<number>` — No-op returning 0\n- `getLastRun(): Promise<undefined>` — No-op returning undefined (no run history in frontmatter mode)\n- `isFirstRun(): Promise<boolean>` — Returns true if no `.sum` files exist\n\n**createUpdateOrchestrator**(config: Config, projectRoot: string, options?: { tracer?: ITraceWriter; debug?: boolean }): UpdateOrchestrator — Factory function instantiating UpdateOrchestrator.\n\n**UpdatePlan** — Interface defining update strategy:\n```typescript\n{\n  filesToAnalyze: FileChange[];    // Added/modified files needing re-analysis\n  filesToSkip: string[];            // Unchanged files (hash match)\n  cleanup: CleanupResult;           // Orphaned .sum files to delete\n  affectedDirs: string[];           // Directories needing AGENTS.md regeneration\n  baseCommit: string;               // Current commit (not used, kept for compatibility)\n  currentCommit: string;            // Current commit SHA\n  isFirstRun: boolean;              // True if no .sum files exist\n}\n```\n\n## Hash-Based Change Detection Algorithm\n\n**preparePlan()** executes frontmatter-based change detection:\n1. Calls `checkPrerequisites()` to verify git repository exists\n2. Calls `getCurrentCommit()` to retrieve HEAD commit SHA\n3. Invokes `discoverFiles()` which wraps `runDiscovery()` from `discovery/run.js`, converting absolute paths to relative via `path.relative()`\n4. For each discovered file, calls `getSumPath()` to locate corresponding `.sum` file\n5. Reads `.sum` frontmatter via `readSumFile()` to extract `contentHash` field\n6. Computes current file hash via `computeContentHash()` (SHA-256)\n7. Hash mismatch or missing `.sum` → adds `FileChange` with `status: 'added'` or `'modified'` to `filesToAnalyze`\n8. Hash match → adds path to `filesToSkip`\n9. Calls `cleanupOrphans()` to delete `.sum` files for non-existent source files\n10. Calls `getAffectedDirectories()` to compute directory set from `filesToAnalyze`\n11. Sorts `affectedDirs` by depth descending via `split(path.sep).length` comparison (deepest first for bottom-up regeneration)\n\n## Trace Event Emission\n\nEmits trace events via `ITraceWriter` interface:\n- `phase:start` with `phase: 'update-plan-creation'` at start of `preparePlan()`\n- `plan:created` with `planType: 'update'`, `fileCount: filesToAnalyze.length`, `taskCount: filesToAnalyze.length + affectedDirs.length`\n- `phase:end` with `phase: 'update-plan-creation'`, `durationMs` computed via `process.hrtime.bigint()` delta, `tasksCompleted: 1`, `tasksFailed: 0`\n\n## Debug Logging\n\nWhen `debug: true` passed to constructor, emits `picocolors.dim()` formatted messages to stderr:\n- `'[debug] Creating update plan with change detection...'` at start\n- `'[debug] Git commit: <hash>'` showing first 7 characters of commit SHA\n- `'[debug] Discovering files...'` before discovery phase\n- `'[debug] Change detection: <changed> changed, <unchanged> unchanged, <orphaned> orphaned'` after comparison\n- `'[debug] Affected directories: <count>'` showing directory regeneration scope\n\n## Integration Points\n\n- **change-detection/index.js** — Imports `isGitRepo()`, `getCurrentCommit()`, `computeContentHash()`, `FileChange` type\n- **generation/writers/sum.js** — Imports `readSumFile()`, `getSumPath()` for frontmatter parsing and path resolution\n- **update/orphan-cleaner.js** — Imports `cleanupOrphans()`, `getAffectedDirectories()` for artifact cleanup and directory tracking\n- **discovery/run.js** — Imports `discoverFiles()` wrapping `runDiscovery()` for file scanning\n- **config/schema.js** — Accepts `Config` type defining exclude patterns and concurrency settings\n- **orchestration/trace.js** — Optional `ITraceWriter` dependency for NDJSON event emission\n\n## API Compatibility Pattern\n\nSeveral methods are no-ops with comment `// Kept for API compatibility`:\n- `close()` — No database resources in frontmatter mode (previous SQLite implementation removed)\n- `recordFileAnalyzed()` — Hash now stored in `.sum` YAML frontmatter instead of database\n- `removeFileState()` — `.sum` file cleanup handled by `cleanupOrphans()`\n- `recordRun()` — No run history tracking in frontmatter mode\n- `getLastRun()` — Returns undefined (no historical run data)\n\nThese preserve interface contract for consumers expecting SQLite-based state management from earlier versions.\n\n## First Run Detection\n\n`isFirstRun()` calls `preparePlan({ dryRun: true })` and checks if `filesToSkip.length === 0 && filesToAnalyze.length > 0`, indicating no existing `.sum` files with valid hashes.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 1586,
      "cacheReadTokens": 17114,
      "cacheCreationTokens": 11821,
      "latencyMs": 36807,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/update/orchestrator.ts",
          "sizeBytes": 9165
        }
      ]
    },
    {
      "timestamp": "2026-02-09T16:11:10.974Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/hooks/opencode-are-check-update.js\n\n```javascript\n#!/usr/bin/env node\n/**\n * ARE Update Check Plugin for OpenCode\n *\n * Checks for ARE updates in background, writes result to cache.\n * OpenCode equivalent of the Claude/Gemini SessionStart hook.\n *\n * Cache file: ~/.config/opencode/cache/are-update-check.json\n */\n\nimport { existsSync, mkdirSync, readFileSync, writeFileSync } from 'fs';\nimport { homedir } from 'os';\nimport { join } from 'path';\nimport { spawn } from 'child_process';\n\nexport const AreCheckUpdate = async () => {\n  return {\n    event: {\n      'session.created': async () => {\n        const homeDir = homedir();\n        const cwd = process.cwd();\n        const cacheDir = join(homeDir, '.config', 'opencode', 'cache');\n        const cacheFile = join(cacheDir, 'are-update-check.json');\n\n        // ARE-VERSION file locations (check project first, then global)\n        const projectVersionFile = join(cwd, '.opencode', 'ARE-VERSION');\n        const globalVersionFile = join(homeDir, '.config', 'opencode', 'ARE-VERSION');\n\n        // Ensure cache directory exists\n        if (!existsSync(cacheDir)) {\n          mkdirSync(cacheDir, { recursive: true });\n        }\n\n        // Run check in background (spawn background process)\n        const child = spawn(process.execPath, ['-e', `\n          const fs = require('fs');\n          const { execSync } = require('child_process');\n\n          const cacheFile = ${JSON.stringify(cacheFile)};\n          const projectVersionFile = ${JSON.stringify(projectVersionFile)};\n          const globalVersionFile = ${JSON.stringify(globalVersionFile)};\n\n          // Check project directory first (local install), then global\n          let installed = '0.0.0';\n          try {\n            if (fs.existsSync(projectVersionFile)) {\n              installed = fs.readFileSync(projectVersionFile, 'utf8').trim();\n            } else if (fs.existsSync(globalVersionFile)) {\n              installed = fs.readFileSync(globalVersionFile, 'utf8').trim();\n            }\n          } catch (e) {}\n\n          let latest = null;\n          try {\n            latest = execSync('npm view agents-reverse-engineer version', { encoding: 'utf8', timeout: 10000, windowsHide: true }).trim();\n          } catch (e) {}\n\n          const result = {\n            update_available: latest && installed !== latest,\n            installed,\n            latest: latest || 'unknown',\n            checked: Math.floor(Date.now() / 1000)\n          };\n\n          fs.writeFileSync(cacheFile, JSON.stringify(result));\n        `], {\n          stdio: 'ignore',\n          detached: true,\n          windowsHide: true,\n        });\n\n        child.unref();\n      },\n    },\n  };\n};\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**opencode-are-check-update.js spawns detached background process checking npm registry for agents-reverse-engineer version updates on OpenCode session creation, writes comparison result to `~/.config/opencode/cache/are-update-check.json`.**\n\n## Exported Interface\n\n- `AreCheckUpdate()` — Async factory returning OpenCode plugin object with `event['session.created']` handler\n\n## Plugin Lifecycle Hook\n\n`AreCheckUpdate()` returns plugin object with `event: { 'session.created': async () => {...} }` structure. OpenCode invokes handler when new session starts.\n\n## Version File Resolution Strategy\n\nChecks two locations in priority order:\n1. Project-local: `<cwd>/.opencode/ARE-VERSION`\n2. Global: `~/.config/opencode/ARE-VERSION`\n\nReads first existing file to determine installed version, defaults to `'0.0.0'` if neither exists.\n\n## Cache Directory Creation\n\nConstructs cache path: `join(homedir(), '.config', 'opencode', 'cache', 'are-update-check.json')`. Calls `mkdirSync(cacheDir, { recursive: true })` if directory missing via `!existsSync(cacheDir)` check.\n\n## Detached Subprocess Pattern\n\nSpawns background process using `spawn(process.execPath, ['-e', scriptString], { stdio: 'ignore', detached: true, windowsHide: true })`. Calls `child.unref()` to allow parent process exit without waiting. Subprocess runs Node.js inline script via `-e` flag.\n\n## Inline Script Execution\n\nSubprocess script uses synchronous Node.js APIs:\n- `fs.readFileSync(projectVersionFile, 'utf8')` or `fs.readFileSync(globalVersionFile, 'utf8')` for installed version\n- `execSync('npm view agents-reverse-engineer version', { encoding: 'utf8', timeout: 10000, windowsHide: true })` for latest registry version\n- `fs.writeFileSync(cacheFile, JSON.stringify(result))` for cache persistence\n\n## Cache Schema\n\nWrites JSON object with shape:\n```javascript\n{\n  update_available: boolean,  // true when latest !== installed\n  installed: string,          // version from ARE-VERSION file\n  latest: string,             // npm registry version or 'unknown'\n  checked: number             // Unix timestamp (seconds)\n}\n```\n\n## Error Handling\n\nTry-catch blocks around `fs.readFileSync()` (version file read) and `execSync()` (npm query) prevent subprocess crash. Failed version read defaults installed to `'0.0.0'`, failed npm query sets latest to `null` (serialized as `'unknown'`).\n\n## Platform-Specific Configuration\n\nUses OpenCode conventional paths:\n- Config directory: `~/.config/opencode/` (not overridable by environment)\n- Cache subdirectory: `.config/opencode/cache/`\n- Version file: `.config/opencode/ARE-VERSION`\n\nContrasts with Claude (`~/.claude/`) and Gemini (`~/.gemini/`) paths in sibling hook files.\n\n## Integration with ARE Installer\n\nInstaller writes `ARE-VERSION` file during setup (see `src/installer/operations.ts`). Hook reads this sentinel to compare against npm registry. Version file absence triggers `'0.0.0'` default, causing `update_available: true` for any published version.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 814,
      "cacheReadTokens": 17114,
      "cacheCreationTokens": 9837,
      "latencyMs": 26285,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "hooks/opencode-are-check-update.js",
          "sizeBytes": 2627
        }
      ]
    },
    {
      "timestamp": "2026-02-09T16:11:11.217Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/hooks/opencode-are-session-end.js\n\n```javascript\n#!/usr/bin/env node\n/**\n * ARE Session End Plugin for OpenCode\n *\n * Triggers `are update` when session ends (if there are uncommitted changes).\n * OpenCode equivalent of the Claude/Gemini SessionEnd hook.\n *\n * Runs in background to avoid blocking session close.\n *\n * Disable via:\n * - Environment variable: ARE_DISABLE_HOOK=1\n * - Config file: hook_enabled: false in .agents-reverse-engineer.yaml\n */\n\nimport { execSync, spawn } from 'child_process';\nimport { existsSync, readFileSync } from 'fs';\n\nexport const AreSessionEnd = async () => {\n  return {\n    event: {\n      'session.deleted': async () => {\n        // Check for disable flag\n        if (process.env.ARE_DISABLE_HOOK === '1') return;\n\n        // Check config file for permanent disable\n        const configPath = '.agents-reverse-engineer.yaml';\n        if (existsSync(configPath)) {\n          const config = readFileSync(configPath, 'utf-8');\n          if (config.includes('hook_enabled: false')) return;\n        }\n\n        // Check git status - skip if no changes\n        try {\n          const status = execSync('git status --porcelain', { encoding: 'utf-8' });\n          if (!status.trim()) return;\n        } catch {\n          // Not a git repo or git not available - exit silently\n          return;\n        }\n\n        // Run update in background (don't block session close)\n        const child = spawn('npx', ['agents-reverse-engineer@latest', 'update', '--quiet'], {\n          stdio: 'ignore',\n          detached: true,\n        });\n        child.unref();\n      },\n    },\n  };\n};\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**OpenCode session-end plugin that spawns background `are update` process when session closes if git working tree has uncommitted changes.**\n\n## Exported Symbol\n\n`AreSessionEnd` — Async factory function returning OpenCode plugin object with `event['session.deleted']` handler that executes update workflow in detached background process.\n\n**Signature:**\n```typescript\nexport const AreSessionEnd = async () => Promise<{\n  event: {\n    'session.deleted': () => Promise<void>\n  }\n}>\n```\n\n## Execution Flow\n\n`AreSessionEnd()` returns plugin with `'session.deleted'` event handler. Handler exits early if `process.env.ARE_DISABLE_HOOK === '1'` or if `.agents-reverse-engineer.yaml` exists and contains substring `'hook_enabled: false'` (uses `readFileSync()` + `includes()` without YAML parsing). Executes `execSync('git status --porcelain', { encoding: 'utf-8' })` and exits silently if output is empty or if `execSync()` throws (non-git repo). On non-empty status, spawns detached child process via `spawn('npx', ['agents-reverse-engineer@latest', 'update', '--quiet'], { stdio: 'ignore', detached: true })` followed by `child.unref()` to prevent blocking session close.\n\n## Behavioral Contracts\n\n**Git status command:** `'git status --porcelain'` with `{ encoding: 'utf-8' }` — detects uncommitted changes (modified/staged/untracked files).\n\n**Update invocation:** `['npx', 'agents-reverse-engineer@latest', 'update', '--quiet']` — spawns latest published version with `--quiet` flag to suppress terminal output.\n\n**Disable mechanisms:**\n- Environment variable check: `process.env.ARE_DISABLE_HOOK === '1'`\n- Config substring check: `config.includes('hook_enabled: false')` against `.agents-reverse-engineer.yaml` file contents\n\n**Config file path:** `'.agents-reverse-engineer.yaml'` (project root, checked via `existsSync(configPath)`)\n\n## Integration Point\n\nOpenCode plugin system loads this module and invokes `AreSessionEnd()` factory. Returned plugin object registers `'session.deleted'` lifecycle hook. Companion to `opencode-are-check-update.js` (SessionStart equivalent).\n\n## Process Management Pattern\n\nUses detached spawn with stdio ignore and unref to avoid blocking parent process termination:\n```javascript\nspawn(cmd, args, { stdio: 'ignore', detached: true }).unref()\n```\n\nNo timeout enforcement or SIGTERM/SIGKILL escalation (unlike `src/ai/subprocess.ts` resource-constrained subprocess execution). Update process runs in background indefinitely until natural completion.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 662,
      "cacheReadTokens": 17114,
      "cacheCreationTokens": 9542,
      "latencyMs": 28989,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "hooks/opencode-are-session-end.js",
          "sizeBytes": 1550
        }
      ]
    },
    {
      "timestamp": "2026-02-09T16:11:15.221Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/LANGUAGES-MANIFEST.md\n\n```markdown\n# Package Manifest Files by Language\n\n| Language | Manifest File(s) | Package Manager | Supported |\n|----------|------------------|-----------------|-----------|\n| JavaScript/TypeScript | `package.json` | npm, yarn, pnpm | ✓ |\n| Python | `requirements.txt`, `pyproject.toml`, `setup.py`, `Pipfile` | pip, poetry, pipenv | ✓ |\n| Ruby | `Gemfile` | bundler | |\n| Go | `go.mod` | go modules | ✓ |\n| Rust | `Cargo.toml` | cargo | ✓ |\n| Java | `pom.xml` (Maven), `build.gradle` (Gradle) | Maven, Gradle | |\n| Kotlin | `build.gradle.kts`, `build.gradle` | Gradle | |\n| C#/.NET | `*.csproj`, `packages.config`, `*.fsproj` | NuGet | |\n| PHP | `composer.json` | Composer | |\n| Swift | `Package.swift` | Swift PM | |\n| Elixir | `mix.exs` | Mix | |\n| Erlang | `rebar.config` | rebar3 | |\n| Scala | `build.sbt` | sbt | |\n| Clojure | `deps.edn`, `project.clj` | clj, Leiningen | |\n| Haskell | `package.yaml`, `*.cabal`, `stack.yaml` | cabal, stack | |\n| Dart/Flutter | `pubspec.yaml` | pub | |\n| Lua | `*.rockspec` | LuaRocks | |\n| R | `DESCRIPTION` | CRAN | |\n| Julia | `Project.toml` | Pkg | |\n| Zig | `build.zig.zon` | zig | |\n| Nim | `*.nimble` | nimble | |\n| OCaml | `dune-project`, `*.opam` | dune, opam | |\n| C/C++ | `CMakeLists.txt`, `conanfile.txt`, `vcpkg.json` | CMake, Conan, vcpkg | |\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**LANGUAGES-MANIFEST.md documents package manager manifest files across 26 programming languages, mapping each ecosystem's dependency declaration files to their respective package managers and indicating which formats ARE's directory aggregation phase currently supports.**\n\n## Supported Manifest Detection\n\nPhase 2 directory aggregation (`src/generation/prompts/builder.ts` via `detectManifest()`) recognizes 9 manifest types for inclusion in AGENTS.md prompts:\n\n- `package.json` (JavaScript/TypeScript: npm/yarn/pnpm)\n- `Cargo.toml` (Rust: cargo)\n- `go.mod` (Go: go modules)\n- `pyproject.toml` (Python: poetry)\n- `pom.xml` (Java: Maven)\n- `build.gradle` (Java/Kotlin: Gradle)\n- `Gemfile` (Ruby: bundler)\n- `composer.json` (PHP: Composer)\n- `CMakeLists.txt` and `Makefile` (C/C++: CMake/make)\n\nSupported languages marked with ✓ in table: JavaScript/TypeScript, Python, Go, Rust.\n\n## Markdown Table Structure\n\nColumn schema:\n- **Language**: Ecosystem name or language family (e.g., \"C#/.NET\", \"Dart/Flutter\")\n- **Manifest File(s)**: Comma-separated list of dependency declaration files, including alternatives (e.g., \"requirements.txt\", \"pyproject.toml\", \"setup.py\", \"Pipfile\" for Python)\n- **Package Manager**: Corresponding dependency resolution tools (e.g., \"pip, poetry, pipenv\")\n- **Supported**: ✓ indicates ARE's `detectManifest()` recognizes this format; empty cells indicate detection not implemented\n\n## Coverage Gaps\n\nUnsupported ecosystems with manifest entries but no detection logic:\n- Ruby (`Gemfile` listed but not in detection array despite being included in supported list)\n- Java/Kotlin Gradle variants (`build.gradle.kts`)\n- C#/.NET NuGet formats (`*.csproj`, `packages.config`, `*.fsproj`)\n- Swift (`Package.swift`)\n- Functional languages: Elixir (`mix.exs`), Erlang (`rebar.config`), Scala (`build.sbt`), Clojure (`deps.edn`, `project.clj`), Haskell (`package.yaml`, `*.cabal`, `stack.yaml`), OCaml (`dune-project`, `*.opam`)\n- Scientific/scripting: Dart (`pubspec.yaml`), Lua (`*.rockspec`), R (`DESCRIPTION`), Julia (`Project.toml`)\n- Systems languages: Zig (`build.zig.zon`), Nim (`*.nimble`)\n- C/C++ alternatives: Conan (`conanfile.txt`), vcpkg (`vcpkg.json`)\n\n## Integration Point\n\nReferenced by `buildDirectoryPrompt()` in `src/generation/prompts/builder.ts` when constructing Phase 2 aggregation prompts. Manifest content appended to prompt context via `fs.readFileSync(manifestPath, 'utf-8')` when `detectManifest(directoryPath)` returns non-null filename.\n\n## Extension Strategy\n\nAdding new manifest support requires:\n1. Append filename to `COMMON_MANIFESTS` array in `src/generation/prompts/builder.ts`\n2. Update Supported column (✓) in this reference table\n3. Verify `fs.existsSync(path.join(directoryPath, manifestFilename))` detection logic handles glob patterns (e.g., `*.csproj`) if wildcard matching needed",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 871,
      "cacheReadTokens": 17114,
      "cacheCreationTokens": 9631,
      "latencyMs": 27729,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "LANGUAGES-MANIFEST.md",
          "sizeBytes": 1293
        }
      ]
    },
    {
      "timestamp": "2026-02-09T16:10:32.221Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/orchestration/runner.ts\n\n```typescript\n/**\n * Three-phase command runner for AI-driven documentation generation.\n *\n * Wires together {@link AIService}, {@link ExecutionPlan}, the concurrency\n * pool, and the progress reporter into a cohesive execution engine.\n *\n * The three execution phases match the {@link ExecutionPlan} dependency graph:\n * 1. **File analysis** -- concurrent AI calls with configurable parallelism\n * 2. **Directory docs** -- concurrent per depth level, post-order AGENTS.md generation\n * 3. **Root documents** -- sequential AI calls for CLAUDE.md, ARCHITECTURE.md, etc.\n *\n * @module\n */\n\nimport * as path from 'node:path';\nimport { readFile, writeFile } from 'node:fs/promises';\nimport type { AIService } from '../ai/index.js';\nimport type { AIResponse } from '../ai/types.js';\nimport type { ExecutionPlan, ExecutionTask } from '../generation/executor.js';\nimport { writeSumFile, readSumFile } from '../generation/writers/sum.js';\nimport type { SumFileContent } from '../generation/writers/sum.js';\nimport { writeAgentsMd } from '../generation/writers/agents-md.js';\nimport { computeContentHashFromString } from '../change-detection/index.js';\nimport type { FileChange } from '../change-detection/types.js';\nimport { buildFilePrompt, buildDirectoryPrompt, buildRootPrompt } from '../generation/prompts/index.js';\nimport type { Config } from '../config/schema.js';\nimport { CONFIG_DIR } from '../config/loader.js';\nimport {\n  checkCodeVsDoc,\n  checkCodeVsCode,\n  checkPhantomPaths,\n  buildInconsistencyReport,\n  formatReportForCli,\n} from '../quality/index.js';\nimport type { Inconsistency } from '../quality/index.js';\nimport { formatExecutionPlanAsMarkdown } from '../generation/executor.js';\nimport { runPool } from './pool.js';\nimport { PlanTracker } from './plan-tracker.js';\nimport { ProgressReporter } from './progress.js';\nimport type { ITraceWriter } from './trace.js';\nimport type {\n  FileTaskResult,\n  RunSummary,\n  CommandRunOptions,\n} from './types.js';\nimport { getVersion } from '../version.js';\n\n// ---------------------------------------------------------------------------\n// CommandRunner\n// ---------------------------------------------------------------------------\n\n/**\n * Orchestrates AI-driven documentation generation.\n *\n * Create one instance per command invocation. The runner holds references\n * to the AI service and run options, then executes plans or file lists\n * through the three-phase pipeline.\n *\n * @example\n * ```typescript\n * const runner = new CommandRunner(aiService, {\n *   concurrency: 5,\n *   failFast: false,\n * });\n *\n * const summary = await runner.executeGenerate(plan);\n * console.log(`Processed ${summary.filesProcessed} files`);\n * ```\n */\nexport class CommandRunner {\n  /** AI service instance for making calls */\n  private readonly aiService: AIService;\n\n  /** Command execution options */\n  private readonly options: CommandRunOptions;\n\n  /** Trace writer for concurrency debugging */\n  private readonly tracer: ITraceWriter | undefined;\n\n  /**\n   * Create a new command runner.\n   *\n   * @param aiService - The AI service instance (should be created per CLI run)\n   * @param options - Execution options (concurrency, failFast, etc.)\n   */\n  constructor(aiService: AIService, options: CommandRunOptions) {\n    this.aiService = aiService;\n    this.options = options;\n    this.tracer = options.tracer;\n\n    // Wire the tracer into the AI service for subprocess/retry events\n    if (this.tracer) {\n      this.aiService.setTracer(this.tracer);\n    }\n  }\n\n  /** Progress log instance (if provided via options) for ProgressReporter mirroring */\n  private get progressLog() { return this.options.progressLog; }\n\n  /**\n   * Execute the `generate` command using a pre-built execution plan.\n   *\n   * Runs all three phases:\n   * 1. File tasks concurrently through the pool\n   * 2. Directory AGENTS.md generation (post-order)\n   * 3. Root document generation (sequential)\n   *\n   * @param plan - The execution plan from the generation orchestrator\n   * @returns Aggregated run summary\n   */\n  async executeGenerate(plan: ExecutionPlan): Promise<RunSummary> {\n    const reporter = new ProgressReporter(plan.fileTasks.length, plan.directoryTasks.length, this.progressLog);\n\n    // Initialize plan tracker (writes GENERATION-PLAN.md with checkboxes)\n    const planTracker = new PlanTracker(\n      plan.projectRoot,\n      formatExecutionPlanAsMarkdown(plan),\n    );\n    await planTracker.initialize();\n\n    const runStart = Date.now();\n    let filesProcessed = 0;\n    let filesFailed = 0;\n\n    // -------------------------------------------------------------------\n    // Pre-Phase 1: Cache old .sum content for stale documentation detection\n    // Throttled to avoid opening too many file descriptors at once.\n    // -------------------------------------------------------------------\n\n    const prePhase1Start = Date.now();\n    this.tracer?.emit({\n      type: 'phase:start',\n      phase: 'pre-phase-1-cache',\n      taskCount: plan.fileTasks.length,\n      concurrency: 20,\n    });\n\n    const oldSumCache = new Map<string, SumFileContent>();\n    const sumReadTasks = plan.fileTasks.map(\n      (task) => async () => {\n        try {\n          const existing = await readSumFile(`${task.absolutePath}.sum`);\n          if (existing) {\n            oldSumCache.set(task.path, existing);\n          }\n        } catch {\n          // No old .sum to compare -- skip\n        }\n      },\n    );\n    await runPool(sumReadTasks, {\n      concurrency: 20,\n      tracer: this.tracer,\n      phaseLabel: 'pre-phase-1-cache',\n      taskLabels: plan.fileTasks.map(t => t.path),\n    });\n\n    this.tracer?.emit({\n      type: 'phase:end',\n      phase: 'pre-phase-1-cache',\n      durationMs: Date.now() - prePhase1Start,\n      tasksCompleted: plan.fileTasks.length,\n      tasksFailed: 0,\n    });\n\n    // -------------------------------------------------------------------\n    // Phase 1: File analysis (concurrent)\n    // -------------------------------------------------------------------\n\n    const phase1Start = Date.now();\n    this.tracer?.emit({\n      type: 'phase:start',\n      phase: 'phase-1-files',\n      taskCount: plan.fileTasks.length,\n      concurrency: this.options.concurrency,\n    });\n\n    // Cache source content during Phase 1, reused for inconsistency detection\n    const sourceContentCache = new Map<string, string>();\n\n    const fileTasks = plan.fileTasks.map(\n      (task: ExecutionTask, taskIndex: number) => async (): Promise<FileTaskResult> => {\n        reporter.onFileStart(task.path);\n\n        const callStart = Date.now();\n\n        // Read the source file\n        const sourceContent = await readFile(task.absolutePath, 'utf-8');\n        sourceContentCache.set(task.path, sourceContent);\n\n        // Call AI with the task's prompts\n        const response: AIResponse = await this.aiService.call({\n          prompt: task.userPrompt,\n          systemPrompt: task.systemPrompt,\n          taskLabel: task.path,\n        });\n\n        // Track file size for telemetry (from in-memory content, avoids stat syscall)\n        this.aiService.addFilesReadToLastEntry([{\n          path: task.path,\n          sizeBytes: Buffer.byteLength(sourceContent, 'utf-8'),\n        }]);\n\n        // Compute content hash from already-loaded content (avoids second readFile)\n        const contentHash = computeContentHashFromString(sourceContent);\n\n        // Build .sum file content\n        const cleanedText = stripPreamble(response.text);\n        const sumContent: SumFileContent = {\n          summary: cleanedText,\n          metadata: {\n            purpose: extractPurpose(cleanedText),\n          },\n          generatedAt: new Date().toISOString(),\n          contentHash,\n        };\n\n        // Write .sum file\n        await writeSumFile(task.absolutePath, sumContent);\n\n        const durationMs = Date.now() - callStart;\n\n        return {\n          path: task.path,\n          success: true,\n          tokensIn: response.inputTokens,\n          tokensOut: response.outputTokens,\n          cacheReadTokens: response.cacheReadTokens,\n          cacheCreationTokens: response.cacheCreationTokens,\n          durationMs,\n          model: response.model,\n        };\n      },\n    );\n\n    const poolResults = await runPool(\n      fileTasks,\n      {\n        concurrency: this.options.concurrency,\n        failFast: this.options.failFast,\n        tracer: this.tracer,\n        phaseLabel: 'phase-1-files',\n        taskLabels: plan.fileTasks.map(t => t.path),\n      },\n      (result) => {\n        if (result.success && result.value) {\n          const v = result.value;\n          filesProcessed++;\n          reporter.onFileDone(v.path, v.durationMs, v.tokensIn, v.tokensOut, v.model, v.cacheReadTokens, v.cacheCreationTokens);\n          planTracker.markDone(v.path);\n        } else {\n          filesFailed++;\n          const errorMsg = result.error?.message ?? 'Unknown error';\n          const taskPath = plan.fileTasks[result.index]?.path ?? `task-${result.index}`;\n          reporter.onFileError(taskPath, errorMsg);\n        }\n      },\n    );\n\n    this.tracer?.emit({\n      type: 'phase:end',\n      phase: 'phase-1-files',\n      durationMs: Date.now() - phase1Start,\n      tasksCompleted: filesProcessed,\n      tasksFailed: filesFailed,\n    });\n\n    // -------------------------------------------------------------------\n    // Post-Phase 1: Inconsistency detection (non-throwing)\n    // -------------------------------------------------------------------\n\n    let inconsistenciesCodeVsDoc = 0;\n    let inconsistenciesCodeVsCode = 0;\n    let inconsistencyReport: import('../quality/index.js').InconsistencyReport | undefined;\n\n    try {\n      const inconsistencyStart = Date.now();\n      const allIssues: Inconsistency[] = [];\n\n      // Collect successfully processed file paths from pool results\n      const processedPaths: string[] = [];\n      for (const result of poolResults) {\n        if (result.success && result.value) {\n          processedPaths.push(result.value.path);\n        }\n      }\n\n      // Group files by directory\n      const dirGroups = new Map<string, string[]>();\n      for (const filePath of processedPaths) {\n        const dir = path.dirname(filePath);\n        const group = dirGroups.get(dir);\n        if (group) {\n          group.push(filePath);\n        } else {\n          dirGroups.set(dir, [filePath]);\n        }\n      }\n\n      // Run checks per directory group (throttled to avoid excessive parallel I/O)\n      const dirEntries = Array.from(dirGroups.entries());\n\n      this.tracer?.emit({\n        type: 'phase:start',\n        phase: 'post-phase-1-quality',\n        taskCount: dirEntries.length,\n        concurrency: 10,\n      });\n\n      const dirCheckResults: Inconsistency[][] = [];\n      const dirCheckTasks = dirEntries.map(\n        ([, groupPaths], groupIndex) => async () => {\n          const dirIssues: Inconsistency[] = [];\n          const filesForCodeVsCode: Array<{ path: string; content: string }> = [];\n\n          // Process files within this group sequentially to limit I/O\n          for (const filePath of groupPaths) {\n            const absoluteFilePath = `${plan.projectRoot}/${filePath}`;\n\n            // Use cached content from Phase 1 (avoids re-read)\n            let sourceContent = sourceContentCache.get(filePath);\n            if (!sourceContent) {\n              try {\n                sourceContent = await readFile(absoluteFilePath, 'utf-8');\n              } catch {\n                continue; // File unreadable, skip\n              }\n            }\n\n            filesForCodeVsCode.push({ path: filePath, content: sourceContent });\n\n            // Old-doc check: detects stale documentation\n            const oldSum = oldSumCache.get(filePath);\n            if (oldSum) {\n              const oldIssue = checkCodeVsDoc(sourceContent, oldSum, filePath);\n              if (oldIssue) {\n                oldIssue.description += ' (stale documentation)';\n                dirIssues.push(oldIssue);\n              }\n            }\n\n            // New-doc check: detects LLM omissions in freshly generated .sum\n            try {\n              const newSum = await readSumFile(`${absoluteFilePath}.sum`);\n              if (newSum) {\n                const newIssue = checkCodeVsDoc(sourceContent, newSum, filePath);\n                if (newIssue) {\n                  dirIssues.push(newIssue);\n                }\n              }\n            } catch {\n              // Freshly written .sum unreadable -- skip\n            }\n          }\n\n          // Code-vs-code check scoped to this directory group\n          const codeIssues = checkCodeVsCode(filesForCodeVsCode);\n          dirIssues.push(...codeIssues);\n\n          dirCheckResults[groupIndex] = dirIssues;\n        },\n      );\n      await runPool(dirCheckTasks, {\n        concurrency: 10,\n        tracer: this.tracer,\n        phaseLabel: 'post-phase-1-quality',\n        taskLabels: dirEntries.map(([dirPath]) => dirPath),\n      });\n\n      this.tracer?.emit({\n        type: 'phase:end',\n        phase: 'post-phase-1-quality',\n        durationMs: Date.now() - inconsistencyStart,\n        tasksCompleted: dirEntries.length,\n        tasksFailed: 0,\n      });\n\n      const allIssuesFlat = dirCheckResults.filter(Boolean).flat();\n      allIssues.push(...allIssuesFlat);\n\n      // Release cached source content to free memory\n      sourceContentCache.clear();\n\n      if (allIssues.length > 0) {\n        const report = buildInconsistencyReport(allIssues, {\n          projectRoot: plan.projectRoot,\n          filesChecked: processedPaths.length,\n          durationMs: Date.now() - inconsistencyStart,\n        });\n\n        inconsistenciesCodeVsDoc = report.summary.codeVsDoc;\n        inconsistenciesCodeVsCode = report.summary.codeVsCode;\n        inconsistencyReport = report;\n\n        console.error(formatReportForCli(report));\n      }\n    } catch (err) {\n      // Inconsistency detection must not break the pipeline\n      console.error(`[quality] Inconsistency detection failed: ${err instanceof Error ? err.message : String(err)}`);\n    }\n\n    // -------------------------------------------------------------------\n    // Phase 2: Directory docs (concurrent per depth level, post-order)\n    // -------------------------------------------------------------------\n\n    // Build set of directories in the plan (for filtering in buildDirectoryPrompt)\n    const knownDirs = new Set(plan.directoryTasks.map(t => t.path));\n\n    // Group directory tasks by depth so same-depth dirs run in parallel\n    // while maintaining post-order (children before parents)\n    const dirsByDepth = new Map<number, typeof plan.directoryTasks>();\n    for (const dirTask of plan.directoryTasks) {\n      const depth = (dirTask.metadata.depth as number) ?? 0;\n      const group = dirsByDepth.get(depth);\n      if (group) {\n        group.push(dirTask);\n      } else {\n        dirsByDepth.set(depth, [dirTask]);\n      }\n    }\n\n    // Process depth levels in descending order (deepest first = post-order)\n    const depthLevels = Array.from(dirsByDepth.keys()).sort((a, b) => b - a);\n\n    for (const depth of depthLevels) {\n      const dirsAtDepth = dirsByDepth.get(depth)!;\n      const phaseLabel = `phase-2-dirs-depth-${depth}`;\n      const dirConcurrency = Math.min(this.options.concurrency, dirsAtDepth.length);\n\n      const phase2Start = Date.now();\n      this.tracer?.emit({\n        type: 'phase:start',\n        phase: phaseLabel,\n        taskCount: dirsAtDepth.length,\n        concurrency: dirConcurrency,\n      });\n\n      const dirTasks = dirsAtDepth.map(\n        (dirTask) => async () => {\n          reporter.onDirectoryStart(dirTask.path);\n          const dirCallStart = Date.now();\n          const prompt = await buildDirectoryPrompt(dirTask.absolutePath, plan.projectRoot, this.options.debug, knownDirs, plan.projectStructure);\n          const dirResponse: AIResponse = await this.aiService.call({\n            prompt: prompt.user,\n            systemPrompt: prompt.system,\n            taskLabel: `${dirTask.path}/AGENTS.md`,\n          });\n          await writeAgentsMd(dirTask.absolutePath, plan.projectRoot, dirResponse.text);\n          const dirDurationMs = Date.now() - dirCallStart;\n          reporter.onDirectoryDone(\n            dirTask.path,\n            dirDurationMs,\n            dirResponse.inputTokens,\n            dirResponse.outputTokens,\n            dirResponse.model,\n            dirResponse.cacheReadTokens,\n            dirResponse.cacheCreationTokens,\n          );\n          planTracker.markDone(`${dirTask.path}/AGENTS.md`);\n        },\n      );\n\n      const phase2Results = await runPool(dirTasks, {\n        concurrency: dirConcurrency,\n        failFast: this.options.failFast,\n        tracer: this.tracer,\n        phaseLabel,\n        taskLabels: dirsAtDepth.map(t => t.path),\n      });\n\n      const phase2Succeeded = phase2Results.filter(r => r.success).length;\n      const phase2Failed = phase2Results.filter(r => !r.success).length;\n\n      this.tracer?.emit({\n        type: 'phase:end',\n        phase: phaseLabel,\n        durationMs: Date.now() - phase2Start,\n        tasksCompleted: phase2Succeeded,\n        tasksFailed: phase2Failed,\n      });\n    }\n\n    // -------------------------------------------------------------------\n    // Post-Phase 2: Phantom path validation (non-throwing)\n    // -------------------------------------------------------------------\n\n    let phantomPathCount = 0;\n    try {\n      const phantomIssues: Inconsistency[] = [];\n      for (const dirTask of plan.directoryTasks) {\n        const agentsMdPath = path.join(dirTask.absolutePath, 'AGENTS.md');\n        try {\n          const content = await readFile(agentsMdPath, 'utf-8');\n          const issues = checkPhantomPaths(agentsMdPath, content, plan.projectRoot);\n          phantomIssues.push(...issues);\n        } catch {\n          // AGENTS.md not yet written or read error — skip\n        }\n      }\n      phantomPathCount = phantomIssues.length;\n\n      if (phantomIssues.length > 0) {\n        const phantomReport = buildInconsistencyReport(phantomIssues, {\n          projectRoot: plan.projectRoot,\n          filesChecked: plan.directoryTasks.length,\n          durationMs: 0,\n        });\n        console.error(formatReportForCli(phantomReport));\n      }\n    } catch (err) {\n      console.error(`[quality] Phantom path validation failed: ${err instanceof Error ? err.message : String(err)}`);\n    }\n\n    // -------------------------------------------------------------------\n    // Phase 3: Root documents (sequential)\n    // -------------------------------------------------------------------\n\n    const phase3Start = Date.now();\n    this.tracer?.emit({\n      type: 'phase:start',\n      phase: 'phase-3-root',\n      taskCount: plan.rootTasks.length,\n      concurrency: 1,\n    });\n\n    let rootTasksCompleted = 0;\n    let rootTasksFailed = 0;\n    for (const rootTask of plan.rootTasks) {\n      const taskStart = Date.now();\n\n      // Emit task:start event\n      this.tracer?.emit({\n        type: 'task:start',\n        taskLabel: rootTask.path,\n        phase: 'phase-3-root',\n      });\n\n      try {\n        // Build prompt at runtime with all AGENTS.md content injected\n        const rootPrompt = await buildRootPrompt(plan.projectRoot, this.options.debug);\n\n        const response = await this.aiService.call({\n          prompt: rootPrompt.user,\n          systemPrompt: rootPrompt.system,\n          taskLabel: rootTask.path,\n          maxTurns: 1, // All context in prompt; no tool use needed\n        });\n\n        // Strip conversational preamble if the LLM still adds one\n        let content = response.text;\n        const mdStart = content.indexOf('# ');\n        if (mdStart > 0) {\n          const preamble = content.slice(0, mdStart).trim();\n          if (preamble && !preamble.startsWith('#') && !preamble.startsWith('<!--')) {\n            content = content.slice(mdStart);\n          }\n        }\n\n        await writeFile(rootTask.outputPath, content, 'utf-8');\n        reporter.onRootDone(rootTask.path);\n        planTracker.markDone(rootTask.path);\n        rootTasksCompleted++;\n\n        // Emit task:done event (success)\n        this.tracer?.emit({\n          type: 'task:done',\n          workerId: 0, // Sequential execution, single worker\n          taskIndex: rootTasksCompleted - 1,\n          taskLabel: rootTask.path,\n          durationMs: Date.now() - taskStart,\n          success: true,\n          activeTasks: 0, // Sequential, only one active at a time\n        });\n      } catch (error) {\n        rootTasksFailed++;\n\n        // Emit task:done event (failure)\n        this.tracer?.emit({\n          type: 'task:done',\n          workerId: 0,\n          taskIndex: rootTasksCompleted + rootTasksFailed - 1,\n          taskLabel: rootTask.path,\n          durationMs: Date.now() - taskStart,\n          success: false,\n          error: error instanceof Error ? error.message : String(error),\n          activeTasks: 0,\n        });\n        throw error; // Re-throw to maintain existing error handling\n      }\n    }\n\n    this.tracer?.emit({\n      type: 'phase:end',\n      phase: 'phase-3-root',\n      durationMs: Date.now() - phase3Start,\n      tasksCompleted: rootTasksCompleted,\n      tasksFailed: rootTasksFailed,\n    });\n\n    // Ensure all plan tracker writes are flushed\n    await planTracker.flush();\n\n    // -------------------------------------------------------------------\n    // Build and print summary\n    // -------------------------------------------------------------------\n\n    const aiSummary = this.aiService.getSummary();\n    const totalDurationMs = Date.now() - runStart;\n\n    const summary: RunSummary = {\n      version: getVersion(),\n      filesProcessed,\n      filesFailed,\n      filesSkipped: 0,\n      totalCalls: aiSummary.totalCalls,\n      totalInputTokens: aiSummary.totalInputTokens,\n      totalOutputTokens: aiSummary.totalOutputTokens,\n      totalCacheReadTokens: aiSummary.totalCacheReadTokens,\n      totalCacheCreationTokens: aiSummary.totalCacheCreationTokens,\n      totalDurationMs,\n      errorCount: aiSummary.errorCount,\n      retryCount: 0,\n      totalFilesRead: aiSummary.totalFilesRead,\n      uniqueFilesRead: aiSummary.uniqueFilesRead,\n      inconsistenciesCodeVsDoc,\n      inconsistenciesCodeVsCode,\n      phantomPaths: phantomPathCount,\n      inconsistencyReport,\n    };\n\n    reporter.printSummary(summary);\n\n    return summary;\n  }\n\n  /**\n   * Execute the `update` command for a set of changed files.\n   *\n   * Runs only Phase 1 (file analysis) for the specified files. Does NOT\n   * generate directory or root documents -- the update command handles\n   * AGENTS.md regeneration itself based on which directories were affected.\n   *\n   * @param filesToAnalyze - Array of changed files to re-analyze\n   * @param projectRoot - Absolute path to the project root\n   * @param config - Project configuration for prompt building\n   * @returns Aggregated run summary\n   */\n  async executeUpdate(\n    filesToAnalyze: FileChange[],\n    projectRoot: string,\n    config: Config,\n  ): Promise<RunSummary> {\n    const reporter = new ProgressReporter(filesToAnalyze.length, 0, this.progressLog);\n\n    const runStart = Date.now();\n    let filesProcessed = 0;\n    let filesFailed = 0;\n\n    // -------------------------------------------------------------------\n    // Phase 1: File analysis (concurrent)\n    // -------------------------------------------------------------------\n\n    const phase1Start = Date.now();\n    this.tracer?.emit({\n      type: 'phase:start',\n      phase: 'update-phase-1-files',\n      taskCount: filesToAnalyze.length,\n      concurrency: this.options.concurrency,\n    });\n\n    // Cache source content during update, reused for inconsistency detection\n    const updateSourceCache = new Map<string, string>();\n\n    // Attempt to read existing project plan for bird's-eye context\n    let projectPlan: string | undefined;\n    try {\n      const planPath = path.join(projectRoot, CONFIG_DIR, 'GENERATION-PLAN.md');\n      projectPlan = await readFile(planPath, 'utf-8');\n    } catch {\n      // No plan file from previous generate run — proceed without project structure context\n    }\n\n    const updateTasks = filesToAnalyze.map(\n      (file: FileChange, fileIndex: number) => async (): Promise<FileTaskResult> => {\n        reporter.onFileStart(file.path);\n\n        const callStart = Date.now();\n        const absolutePath = `${projectRoot}/${file.path}`;\n\n        // Read the source file\n        const sourceContent = await readFile(absolutePath, 'utf-8');\n        updateSourceCache.set(file.path, sourceContent);\n\n        // Read existing .sum for incremental update context\n        const existingSumContent = await readSumFile(`${absolutePath}.sum`);\n\n        // Build prompt\n        const prompt = buildFilePrompt({\n          filePath: file.path,\n          content: sourceContent,\n          projectPlan,\n          existingSum: existingSumContent?.summary,\n        }, this.options.debug);\n\n        // Call AI\n        const response: AIResponse = await this.aiService.call({\n          prompt: prompt.user,\n          systemPrompt: prompt.system,\n          taskLabel: file.path,\n        });\n\n        // Track file size for telemetry (from in-memory content, avoids stat syscall)\n        this.aiService.addFilesReadToLastEntry([{\n          path: file.path,\n          sizeBytes: Buffer.byteLength(sourceContent, 'utf-8'),\n        }]);\n\n        // Compute content hash from already-loaded content (avoids second readFile)\n        const contentHash = computeContentHashFromString(sourceContent);\n\n        // Build .sum file content\n        const cleanedText = stripPreamble(response.text);\n        const sumContent: SumFileContent = {\n          summary: cleanedText,\n          metadata: {\n            purpose: extractPurpose(cleanedText),\n          },\n          generatedAt: new Date().toISOString(),\n          contentHash,\n        };\n\n        // Write .sum file\n        await writeSumFile(absolutePath, sumContent);\n\n        const durationMs = Date.now() - callStart;\n\n        return {\n          path: file.path,\n          success: true,\n          tokensIn: response.inputTokens,\n          tokensOut: response.outputTokens,\n          cacheReadTokens: response.cacheReadTokens,\n          cacheCreationTokens: response.cacheCreationTokens,\n          durationMs,\n          model: response.model,\n        };\n      },\n    );\n\n    const poolResults = await runPool(\n      updateTasks,\n      {\n        concurrency: this.options.concurrency,\n        failFast: this.options.failFast,\n        tracer: this.tracer,\n        phaseLabel: 'update-phase-1-files',\n        taskLabels: filesToAnalyze.map(f => f.path),\n      },\n      (result) => {\n        if (result.success && result.value) {\n          const v = result.value;\n          filesProcessed++;\n          reporter.onFileDone(v.path, v.durationMs, v.tokensIn, v.tokensOut, v.model, v.cacheReadTokens, v.cacheCreationTokens);\n        } else {\n          filesFailed++;\n          const errorMsg = result.error?.message ?? 'Unknown error';\n          const filePath = filesToAnalyze[result.index]?.path ?? `file-${result.index}`;\n          reporter.onFileError(filePath, errorMsg);\n        }\n      },\n    );\n\n    this.tracer?.emit({\n      type: 'phase:end',\n      phase: 'update-phase-1-files',\n      durationMs: Date.now() - phase1Start,\n      tasksCompleted: filesProcessed,\n      tasksFailed: filesFailed,\n    });\n\n    // -------------------------------------------------------------------\n    // Post-analysis: Inconsistency detection (non-throwing)\n    // -------------------------------------------------------------------\n\n    let updateInconsistenciesCodeVsDoc = 0;\n    let updateInconsistenciesCodeVsCode = 0;\n    let updateInconsistencyReport: import('../quality/index.js').InconsistencyReport | undefined;\n\n    try {\n      const inconsistencyStart = Date.now();\n      const allIssues: Inconsistency[] = [];\n\n      // Collect successfully processed file paths\n      const processedPaths: string[] = [];\n      for (const result of poolResults) {\n        if (result.success && result.value) {\n          processedPaths.push(result.value.path);\n        }\n      }\n\n      // Group files by directory\n      const dirGroups = new Map<string, string[]>();\n      for (const filePath of processedPaths) {\n        const dir = path.dirname(filePath);\n        const group = dirGroups.get(dir);\n        if (group) {\n          group.push(filePath);\n        } else {\n          dirGroups.set(dir, [filePath]);\n        }\n      }\n\n      // Run checks per directory group (throttled to avoid excessive parallel I/O)\n      const updateDirEntries = Array.from(dirGroups.entries());\n\n      this.tracer?.emit({\n        type: 'phase:start',\n        phase: 'update-post-phase-1-quality',\n        taskCount: updateDirEntries.length,\n        concurrency: 10,\n      });\n\n      const updateDirResults: Inconsistency[][] = [];\n      const updateDirCheckTasks = updateDirEntries.map(\n        ([, groupPaths], groupIndex) => async () => {\n          const dirIssues: Inconsistency[] = [];\n          const filesForCodeVsCode: Array<{ path: string; content: string }> = [];\n\n          for (const filePath of groupPaths) {\n            const absoluteFilePath = `${projectRoot}/${filePath}`;\n\n            // Use cached content from update phase (avoids re-read)\n            let sourceContent = updateSourceCache.get(filePath);\n            if (!sourceContent) {\n              try {\n                sourceContent = await readFile(absoluteFilePath, 'utf-8');\n              } catch {\n                continue;\n              }\n            }\n\n            filesForCodeVsCode.push({ path: filePath, content: sourceContent });\n\n            // New-doc check: detects LLM omissions in freshly generated .sum\n            try {\n              const newSum = await readSumFile(`${absoluteFilePath}.sum`);\n              if (newSum) {\n                const newIssue = checkCodeVsDoc(sourceContent, newSum, filePath);\n                if (newIssue) {\n                  dirIssues.push(newIssue);\n                }\n              }\n            } catch {\n              // .sum unreadable -- skip\n            }\n          }\n\n          // Code-vs-code check scoped to this directory group\n          const codeIssues = checkCodeVsCode(filesForCodeVsCode);\n          dirIssues.push(...codeIssues);\n\n          updateDirResults[groupIndex] = dirIssues;\n        },\n      );\n      await runPool(updateDirCheckTasks, {\n        concurrency: 10,\n        tracer: this.tracer,\n        phaseLabel: 'update-post-phase-1-quality',\n        taskLabels: updateDirEntries.map(([dirPath]) => dirPath),\n      });\n\n      this.tracer?.emit({\n        type: 'phase:end',\n        phase: 'update-post-phase-1-quality',\n        durationMs: Date.now() - inconsistencyStart,\n        tasksCompleted: updateDirEntries.length,\n        tasksFailed: 0,\n      });\n\n      const allIssuesFlat = updateDirResults.filter(Boolean).flat();\n      allIssues.push(...allIssuesFlat);\n\n      // Release cached source content to free memory\n      updateSourceCache.clear();\n\n      if (allIssues.length > 0) {\n        const report = buildInconsistencyReport(allIssues, {\n          projectRoot,\n          filesChecked: processedPaths.length,\n          durationMs: Date.now() - inconsistencyStart,\n        });\n\n        updateInconsistenciesCodeVsDoc = report.summary.codeVsDoc;\n        updateInconsistenciesCodeVsCode = report.summary.codeVsCode;\n        updateInconsistencyReport = report;\n\n        console.error(formatReportForCli(report));\n      }\n    } catch (err) {\n      console.error(`[quality] Inconsistency detection failed: ${err instanceof Error ? err.message : String(err)}`);\n    }\n\n    // Build and print summary\n    const aiSummary = this.aiService.getSummary();\n    const totalDurationMs = Date.now() - runStart;\n\n    const summary: RunSummary = {\n      version: getVersion(),\n      filesProcessed,\n      filesFailed,\n      filesSkipped: 0,\n      totalCalls: aiSummary.totalCalls,\n      totalInputTokens: aiSummary.totalInputTokens,\n      totalOutputTokens: aiSummary.totalOutputTokens,\n      totalCacheReadTokens: aiSummary.totalCacheReadTokens,\n      totalCacheCreationTokens: aiSummary.totalCacheCreationTokens,\n      totalDurationMs,\n      errorCount: aiSummary.errorCount,\n      retryCount: 0,\n      totalFilesRead: aiSummary.totalFilesRead,\n      uniqueFilesRead: aiSummary.uniqueFilesRead,\n      inconsistenciesCodeVsDoc: updateInconsistenciesCodeVsDoc,\n      inconsistenciesCodeVsCode: updateInconsistenciesCodeVsCode,\n      inconsistencyReport: updateInconsistencyReport,\n    };\n\n    reporter.printSummary(summary);\n\n    return summary;\n  }\n}\n\n// ---------------------------------------------------------------------------\n// Helpers\n// ---------------------------------------------------------------------------\n\n/**\n * Strip LLM preamble from response text.\n * Detects common preamble patterns and removes everything before the actual content.\n */\nfunction stripPreamble(responseText: string): string {\n  // Pattern 1: Content after a --- separator (LLM uses --- before real content)\n  const separatorIndex = responseText.indexOf('\\n---\\n');\n  if (separatorIndex >= 0 && separatorIndex < 500) {\n    const afterSeparator = responseText.slice(separatorIndex + 5).trim();\n    if (afterSeparator.length > 0) {\n      return afterSeparator;\n    }\n  }\n\n  // Pattern 2: Content starts with a bold purpose line (**)\n  const boldMatch = responseText.match(/^[\\s\\S]{0,500}?(\\*\\*[A-Z])/);\n  if (boldMatch && boldMatch.index !== undefined) {\n    const before = responseText.slice(0, boldMatch.index).trim();\n    // Only strip if what comes before looks like preamble (no identifiers, short)\n    if (before.length > 0 && before.length < 300 && !before.includes('##')) {\n      return responseText.slice(boldMatch.index);\n    }\n  }\n\n  return responseText;\n}\n\nconst PREAMBLE_PREFIXES = [\n  'now i', 'perfect', 'based on', 'let me', 'here is', 'i\\'ll', 'i will',\n  'great', 'okay', 'sure', 'certainly', 'alright',\n];\n\n/**\n * Extract the purpose from AI response text.\n *\n * Skips lines that look like LLM preamble, markdown headers, or separators.\n * Falls back to empty string if the response is empty.\n *\n * @param responseText - The AI-generated summary text\n * @returns A single-line purpose string\n */\nfunction extractPurpose(responseText: string): string {\n  const lines = responseText.split('\\n');\n  for (const line of lines) {\n    const trimmed = line.trim();\n    if (!trimmed || trimmed.startsWith('#') || trimmed === '---') continue;\n\n    // Skip lines that look like LLM preamble\n    const lower = trimmed.toLowerCase();\n    if (PREAMBLE_PREFIXES.some(p => lower.startsWith(p))) continue;\n\n    // Strip bold markdown wrapper if present\n    const purpose = trimmed.replace(/^\\*\\*(.+)\\*\\*$/, '$1');\n    return purpose.length > 120 ? purpose.slice(0, 117) + '...' : purpose;\n  }\n  return '';\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**CommandRunner orchestrates AI-driven three-phase documentation generation (file analysis, directory AGENTS.md aggregation, root document synthesis) with concurrency pool execution, quality validation, telemetry integration, and trace emission.**\n\n## Exported Classes\n\n**CommandRunner** — Main orchestration class accepting `AIService` instance and `CommandRunOptions`, exposes `executeGenerate(plan: ExecutionPlan): Promise<RunSummary>` for full three-phase generation and `executeUpdate(filesToAnalyze: FileChange[], projectRoot: string, config: Config): Promise<RunSummary>` for incremental file re-analysis.\n\nConstructor accepts `aiService: AIService` and `options: CommandRunOptions`, optionally wires `options.tracer` into AI service via `setTracer()` for subprocess/retry event emission.\n\n## Three-Phase Execution Pipeline\n\n**Phase 1: File Analysis (executeGenerate lines 84-259)** — Pre-phase caches existing `.sum` files via `readSumFile()` at concurrency=20 for stale-doc detection. Maps file tasks to async functions reading source via `readFile()`, computing `contentHash` via `computeContentHashFromString()`, calling `aiService.call()` with `buildFilePrompt()` output, stripping preamble via `stripPreamble()`, extracting purpose via `extractPurpose()`, constructing `SumFileContent` with `generatedAt`/`contentHash`/`summary`/`metadata.purpose`, writing via `writeSumFile()`. Executes via `runPool(fileTasks, { concurrency, failFast, tracer, phaseLabel: 'phase-1-files', taskLabels })` with progress callback invoking `reporter.onFileDone()` and `planTracker.markDone()`. Caches `sourceContent` in `Map<string, string>` for quality checks.\n\n**Post-Phase 1 Quality (lines 261-362)** — Groups processed files by directory via `path.dirname()` into `Map<string, string[]>`. Runs throttled (concurrency=10) validation per directory group: reads cached source content, compares against old `.sum` via `checkCodeVsDoc(sourceContent, oldSum, filePath)` appending `(stale documentation)` label, reads fresh `.sum` via `readSumFile()` and runs second `checkCodeVsDoc()` pass, aggregates `filesForCodeVsCode` array and runs `checkCodeVsCode()` for duplicate symbol detection. Flattens results, builds `InconsistencyReport` via `buildInconsistencyReport()`, prints via `formatReportForCli()`, clears `sourceContentCache`.\n\n**Phase 2: Directory AGENTS.md (lines 364-445)** — Groups `plan.directoryTasks` by `metadata.depth` into `Map<number, ExecutionTask[]>`, sorts depth levels descending (deepest-first post-order traversal). For each depth level: builds `phaseLabel` as `phase-2-dirs-depth-${depth}`, computes `dirConcurrency = Math.min(options.concurrency, dirsAtDepth.length)`, emits `phase:start` trace event, maps directory tasks to async functions calling `buildDirectoryPrompt(dirTask.absolutePath, projectRoot, debug, knownDirs, projectStructure)` with `knownDirs` set for filtering, calls `aiService.call()` with prompt, writes via `writeAgentsMd(dirTask.absolutePath, projectRoot, dirResponse.text)`, updates reporter and plan tracker. Executes via `runPool()` with `phaseLabel`/`taskLabels`, emits `phase:end` with `tasksCompleted`/`tasksFailed` counts.\n\n**Post-Phase 2 Quality (lines 447-467)** — Iterates `plan.directoryTasks`, reads `AGENTS.md` via `readFile(path.join(dirTask.absolutePath, 'AGENTS.md'))`, runs `checkPhantomPaths(agentsMdPath, content, projectRoot)`, aggregates issues, builds `phantomReport` via `buildInconsistencyReport()`, prints via `formatReportForCli()`. Non-throwing with catch-all error handler.\n\n**Phase 3: Root Documents (lines 469-532)** — Sequential execution (concurrency=1) over `plan.rootTasks`. Emits `task:start` trace event, calls `buildRootPrompt(projectRoot, debug)` with all AGENTS.md injected, calls `aiService.call({ prompt, systemPrompt, maxTurns: 1 })`, strips conversational preamble via `content.indexOf('# ')` slice if leading content lacks `#` or `<!--`, writes via `writeFile(rootTask.outputPath, content)`, emits `task:done` with `workerId: 0`/`success`/`error`, updates reporter and plan tracker. Flushes `planTracker` via `await planTracker.flush()` after loop.\n\n## Incremental Update Execution\n\n**executeUpdate(filesToAnalyze, projectRoot, config)** — Phase 1 only implementation for changed file re-analysis. Attempts to load `GENERATION-PLAN.md` from `.agents-reverse-engineer/` via `path.join(projectRoot, CONFIG_DIR, 'GENERATION-PLAN.md')` for `projectPlan` context (falls through on error). Maps `filesToAnalyze` to async tasks reading source via `readFile()`, reading existing `.sum` via `readSumFile()`, building prompt via `buildFilePrompt({ filePath, content, projectPlan, existingSum })`, calling AI service, writing `.sum` with hash. Caches source in `updateSourceCache`. Runs post-analysis quality checks (lines 616-717) identical to generate flow but without stale-doc comparison. Builds `RunSummary` with `updateInconsistenciesCodeVsDoc`/`updateInconsistenciesCodeVsCode` from `report.summary`.\n\n## Helper Functions\n\n**stripPreamble(responseText: string): string** — Pattern 1: detects `\\n---\\n` separator within first 500 chars, returns content after. Pattern 2: regex `/^[\\s\\S]{0,500}?(\\*\\*[A-Z])/` matches bold uppercase start, strips preceding content if <300 chars and no `##` markdown headers. Returns original text if no patterns match.\n\n**extractPurpose(responseText: string): string** — Splits on newlines, skips empty/`#`/`---` lines, filters lines starting with `PREAMBLE_PREFIXES` array (`['now i', 'perfect', 'based on', 'let me', 'here is', 'i\\'ll', 'i will', 'great', 'okay', 'sure', 'certainly', 'alright']`) via `lower.startsWith()` checks, strips bold markdown via `.replace(/^\\*\\*(.+)\\*\\*$/, '$1')`, truncates to 120 chars with `...` suffix. Returns empty string if no valid line found.\n\n## Integration Points\n\n**Dependencies:** `AIService` from `../ai/index.js`, `ExecutionPlan`/`ExecutionTask` from `../generation/executor.js`, `writeSumFile`/`readSumFile` from `../generation/writers/sum.js`, `writeAgentsMd` from `../generation/writers/agents-md.js`, `buildFilePrompt`/`buildDirectoryPrompt`/`buildRootPrompt` from `../generation/prompts/index.js`, `runPool` from `./pool.js`, `PlanTracker` from `./plan-tracker.js`, `ProgressReporter` from `./progress.js`, `ITraceWriter` from `./trace.js`, `checkCodeVsDoc`/`checkCodeVsCode`/`checkPhantomPaths`/`buildInconsistencyReport`/`formatReportForCli` from `../quality/index.js`, `computeContentHashFromString` from `../change-detection/index.js`, `getVersion` from `../version.js`.\n\n**Configuration:** `CommandRunOptions` with `concurrency` (worker pool size), `failFast` (abort on first error), `debug` (verbose prompts), `tracer` (ITraceWriter instance), `progressLog` (Writable stream for mirrored output). `Config` type threaded through `executeUpdate` for prompt building.\n\n**Telemetry:** Calls `aiService.addFilesReadToLastEntry([{ path, sizeBytes }])` after each AI call to track file metadata. Calls `aiService.getSummary()` for aggregated token/cost/duration counts. Emits trace events via `tracer?.emit()`: `phase:start/end` with `taskCount`/`concurrency`/`durationMs`/`tasksCompleted`/`tasksFailed`, `task:start` with `taskLabel`/`phase`, `task:done` with `workerId`/`taskIndex`/`durationMs`/`success`/`error`/`activeTasks`.\n\n## Error Handling\n\n**Quality Validation Non-Throwing:** Both post-Phase 1 inconsistency detection (lines 261-362 generate, 616-717 update) and post-Phase 2 phantom path validation (lines 447-467) wrapped in try-catch blocks logging `[quality] Inconsistency detection failed: ${err.message}` or `[quality] Phantom path validation failed:` without propagating errors.\n\n**Pool Error Handling:** Delegates to `runPool()` with `failFast` option. Progress callback distinguishes `result.success` vs `result.error`, invokes `reporter.onFileDone()` for success and `reporter.onFileError(taskPath, errorMsg)` for failure, increments `filesProcessed`/`filesFailed` counters.\n\n**Root Task Errors:** Phase 3 sequential loop wraps each task in try-catch, increments `rootTasksFailed`, emits `task:done` with `success: false`/`error`, re-throws to maintain existing error handling.\n\n## RunSummary Construction\n\n**Fields:** `version` (from `getVersion()`), `filesProcessed`/`filesFailed`/`filesSkipped`, `totalCalls`/`totalInputTokens`/`totalOutputTokens`/`totalCacheReadTokens`/`totalCacheCreationTokens` (from `aiService.getSummary()`), `totalDurationMs` (`Date.now() - runStart`), `errorCount`, `retryCount: 0` (hardcoded), `totalFilesRead`/`uniqueFilesRead`, `inconsistenciesCodeVsDoc`/`inconsistenciesCodeVsCode`/`phantomPaths`, `inconsistencyReport`. Printed via `reporter.printSummary(summary)`.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 2608,
      "cacheReadTokens": 17114,
      "cacheCreationTokens": 18997,
      "latencyMs": 71046,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/orchestration/runner.ts",
          "sizeBytes": 34827
        }
      ]
    },
    {
      "timestamp": "2026-02-09T16:11:25.206Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/tsconfig.json\n\n```json\n{\n  \"compilerOptions\": {\n    \"target\": \"ES2022\",\n    \"module\": \"NodeNext\",\n    \"moduleResolution\": \"NodeNext\",\n    \"lib\": [\"ES2022\"],\n    \"outDir\": \"dist\",\n    \"rootDir\": \"src\",\n    \"strict\": true,\n    \"esModuleInterop\": true,\n    \"skipLibCheck\": true,\n    \"forceConsistentCasingInFileNames\": true,\n    \"declaration\": true,\n    \"declarationMap\": true,\n    \"sourceMap\": true,\n    \"resolveJsonModule\": true,\n    \"isolatedModules\": true\n  },\n  \"include\": [\"src/**/*\"],\n  \"exclude\": [\"node_modules\", \"dist\"]\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**tsconfig.json configures TypeScript compilation to ES2022 with NodeNext module resolution, emitting source maps and declarations from `src/` to `dist/` with strict type checking.**\n\n## Compiler Target and Module System\n\n`target: \"ES2022\"` emits JavaScript compatible with ES2022 runtime features (top-level await, class fields, private methods). `module: \"NodeNext\"` and `moduleResolution: \"NodeNext\"` enforce Node.js ESM conventions with explicit `.js` extensions in import statements and `package.json` `type: \"module\"` semantics. `lib: [\"ES2022\"]` provides ES2022 standard library type definitions.\n\n## Output Configuration\n\n`outDir: \"dist\"` emits compiled JavaScript to `dist/` directory. `rootDir: \"src\"` preserves source tree structure during compilation. `declaration: true` generates `.d.ts` type definition files for package consumers. `declarationMap: true` creates `.d.ts.map` files mapping declarations back to source for IDE navigation. `sourceMap: true` emits `.js.map` files for runtime debugging.\n\n## Type Checking Strictness\n\n`strict: true` enables all strict type-checking options: `strictNullChecks`, `strictFunctionTypes`, `strictBindCallApply`, `strictPropertyInitialization`, `noImplicitAny`, `noImplicitThis`, `alwaysStrict`. `forceConsistentCasingInFileNames: true` prevents case-insensitive import mismatches across filesystems.\n\n## Module Interoperability\n\n`esModuleInterop: true` enables synthetic default imports from CommonJS modules via `import x from 'cjs'` instead of `import * as x from 'cjs'`. `resolveJsonModule: true` allows importing JSON files with type inference (used for `package.json` metadata parsing in `src/generation/prompts/builder.ts`). `isolatedModules: true` ensures each file can be transpiled independently (required for TypeScript-on-demand bundlers like tsx).\n\n## Compilation Scope\n\n`include: [\"src/**/*\"]` compiles all files under `src/` directory. `exclude: [\"node_modules\", \"dist\"]` prevents recursive compilation of dependencies and output artifacts. `skipLibCheck: true` skips type checking of `.d.ts` files in `node_modules/` to improve compilation performance.\n\n## Build Integration\n\nReferenced by `npm run build` script invoking `tsc` compiler. Emitted `dist/` output structure mirrors `src/` tree: `src/cli/index.ts` → `dist/cli/index.js` + `dist/cli/index.d.ts` + `dist/cli/index.js.map`. Binary entry points in `package.json` (`bin.are`, `bin.agents-reverse-engineer`) resolve to `dist/cli/index.js`. Pre-publish hook (`prepublishOnly`) executes `npm run build` ensuring fresh compilation before npm tarball creation.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 694,
      "cacheReadTokens": 17114,
      "cacheCreationTokens": 9273,
      "latencyMs": 23426,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "tsconfig.json",
          "sizeBytes": 506
        }
      ]
    },
    {
      "timestamp": "2026-02-09T16:11:24.474Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/package.json\n\n```json\n{\n  \"name\": \"agents-reverse-engineer\",\n  \"version\": \"0.6.5\",\n  \"description\": \"CLI tool for reverse-engineering codebase documentation for AI agents\",\n  \"type\": \"module\",\n  \"bin\": {\n    \"agents-reverse-engineer\": \"dist/cli/index.js\",\n    \"are\": \"dist/cli/index.js\"\n  },\n  \"scripts\": {\n    \"build\": \"tsc\",\n    \"build:hooks\": \"node scripts/build-hooks.js\",\n    \"prepack\": \"rm -f LICENSE.sum README.md.sum\",\n    \"prepublishOnly\": \"npm run build && npm run build:hooks\",\n    \"dev\": \"tsx watch src/cli/index.ts\"\n  },\n  \"dependencies\": {\n    \"fast-glob\": \"^3.3.3\",\n    \"ignore\": \"^7.0.3\",\n    \"isbinaryfile\": \"^5.0.4\",\n    \"ora\": \"^8.1.1\",\n    \"picocolors\": \"^1.1.1\",\n    \"simple-git\": \"^3.27.0\",\n    \"yaml\": \"^2.7.0\",\n    \"zod\": \"^3.24.1\"\n  },\n  \"devDependencies\": {\n    \"@types/node\": \"^22.10.7\",\n    \"tsx\": \"^4.19.2\",\n    \"typescript\": \"^5.7.3\"\n  },\n  \"engines\": {\n    \"node\": \">=18.0.0\"\n  },\n  \"keywords\": [\n    \"documentation\",\n    \"codebase\",\n    \"ai\",\n    \"agents\",\n    \"reverse-engineering\"\n  ],\n  \"author\": \"GeoloeG-IsT\",\n  \"license\": \"MIT\",\n  \"main\": \"dist/cli/index.js\",\n  \"files\": [\n    \"dist\",\n    \"hooks/dist\",\n    \"README.md\",\n    \"LICENSE\"\n  ],\n  \"repository\": {\n    \"type\": \"git\",\n    \"url\": \"git+https://github.com/GeoloeG-IsT/agents-reverse-engineer.git\"\n  },\n  \"bugs\": {\n    \"url\": \"https://github.com/GeoloeG-IsT/agents-reverse-engineer/issues\"\n  },\n  \"homepage\": \"https://github.com/GeoloeG-IsT/agents-reverse-engineer#readme\"\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**package.json defines npm package metadata for agents-reverse-engineer 0.6.5, a CLI tool that reverse-engineers codebase documentation for AI agents via ES module-based TypeScript compilation and dual binary entry points.**\n\n## Binary Entry Points\n\n`bin` declares two executable names (`agents-reverse-engineer`, `are`) both resolving to `dist/cli/index.js` after TypeScript compilation.\n\n## Build Scripts\n\n`build` invokes `tsc` to compile TypeScript sources from `src/` to `dist/`.\n\n`build:hooks` executes `node scripts/build-hooks.js` to copy hook files from `hooks/` to `hooks/dist/` for npm tarball inclusion.\n\n`prepack` removes documentation artifacts `LICENSE.sum` and `README.md.sum` before packaging to prevent pollution of published package.\n\n`prepublishOnly` chains `npm run build && npm run build:hooks` ensuring TypeScript compilation and hook preparation execute before `npm publish`.\n\n`dev` launches `tsx watch src/cli/index.ts` for hot-reloading development mode.\n\n## Runtime Dependencies\n\n`fast-glob` (^3.3.3) provides file discovery with glob pattern matching for codebase traversal.\n\n`ignore` (^7.0.3) parses `.gitignore` files to filter discovered files.\n\n`isbinaryfile` (^5.0.4) detects binary files via extension and content analysis to exclude from text-based processing.\n\n`ora` (^8.1.1) renders terminal spinners for progress indication during long-running operations.\n\n`picocolors` (^1.1.1) adds ANSI color codes to CLI output for formatted logging.\n\n`simple-git` (^3.27.0) executes git commands for change detection via `git diff`, `git status`, SHA-256 fallback.\n\n`yaml` (^2.7.0) parses `.agents-reverse-engineer/config.yaml` configuration files.\n\n`zod` (^3.24.1) validates configuration schema against defined TypeScript types in `src/config/schema.ts`.\n\n## Development Dependencies\n\n`@types/node` (^22.10.7) provides Node.js API type definitions for TypeScript compilation.\n\n`tsx` (^4.19.2) executes TypeScript files directly without precompilation for dev script.\n\n`typescript` (^5.7.3) compiles TypeScript sources to JavaScript in `dist/` output directory.\n\n## Engine Requirements\n\n`node` field specifies `>=18.0.0` as minimum required Node.js version for ES module support.\n\n## Package Distribution\n\n`type: \"module\"` declares package uses ES modules instead of CommonJS.\n\n`main` points to `dist/cli/index.js` as default entry when required as module.\n\n`files` array whitelists `dist`, `hooks/dist`, `README.md`, `LICENSE` for inclusion in published tarball (excludes `src/`, `scripts/`, raw `hooks/` before build).\n\n## Repository Metadata\n\n`repository.url` links to `git+https://github.com/GeoloeG-IsT/agents-reverse-engineer.git` for source browsing.\n\n`bugs.url` directs issue reports to `https://github.com/GeoloeG-IsT/agents-reverse-engineer/issues`.\n\n`homepage` references `https://github.com/GeoloeG-IsT/agents-reverse-engineer#readme` for documentation.\n\n`author` identifies package maintainer as \"GeoloeG-IsT\".\n\n`license` declares \"MIT\" license for open-source distribution.\n\n`keywords` array includes `[\"documentation\", \"codebase\", \"ai\", \"agents\", \"reverse-engineering\"]` for npm registry search indexing.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 9,
      "outputTokens": 939,
      "cacheReadTokens": 26806,
      "cacheCreationTokens": 27748,
      "latencyMs": 32581,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "package.json",
          "sizeBytes": 1445
        }
      ]
    },
    {
      "timestamp": "2026-02-09T16:11:22.743Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/README.md\n\n```markdown\n<div align=\"center\">\n\n# AGENTS REVERSE ENGINEER (ARE)\n\n**Reverse engineer your codebase into AI-friendly documentation.**\n\n**Generate `.sum` files, `AGENTS.md`, and root docs for Claude Code, OpenCode, and any AI assistant that supports `AGENTS.md`.**\n\n[![npm version](https://img.shields.io/npm/v/agents-reverse-engineer?style=for-the-badge&logo=npm&logoColor=white&color=CB3837)](https://www.npmjs.com/package/agents-reverse-engineer)\n[![License](https://img.shields.io/badge/license-MIT-blue?style=for-the-badge)](LICENSE)\n\n<br>\n\n```bash\nnpx agents-reverse-engineer@latest\n```\n\n**Interactive installer with runtime and location selection.**\n\n**Works on Mac, Windows, and Linux.**\n\n<br>\n\n_\"Finally, my AI assistant actually understands my codebase structure.\"_\n\n_\"No more explaining the same architecture in every conversation.\"_\n\n<br>\n\n[Why This Exists](#why-this-exists) · [How It Works](#how-it-works) · [Commands](#commands) · [Generated Docs](#generated-documentation)\n\n</div>\n\n---\n\n## Why This Exists\n\nAI coding assistants are powerful, but they don't know your codebase. Every session starts fresh. You explain the same architecture, the same patterns, the same file locations — over and over.\n\n**agents-reverse-engineer** fixes that. It generates documentation that AI assistants actually read:\n\n- **`.sum` files** — Per-file summaries with purpose, exports, dependencies\n- **`AGENTS.md`** — Per-directory overviews with file organization (standard format)\n- **`CLAUDE.md`** / **`GEMINI.md`** / **`OPENCODE.md`** — Runtime-specific project entry points\n\nThe result: Your AI assistant understands your codebase from the first message.\n\n---\n\n## Who This Is For\n\nDevelopers using AI coding assistants (Claude Code, OpenCode, Gemini CLI, or any tool supporting `AGENTS.md`) who want their assistant to actually understand their project structure — without manually writing documentation or repeating context every session.\n\n---\n\n## Getting Started\n\n### 1. Install Commands\n\n```bash\nnpx agents-reverse-engineer@latest\n```\n\nThe interactive installer prompts you to:\n\n1. **Select runtime** — Claude Code, OpenCode, Gemini CLI, or all\n2. **Select location** — Global (`~/.claude/`) or local (`./.claude/`)\n\nThis installs:\n\n- **Commands** — `/are-init`, `/are-discover`, `/are-generate`, `/are-update`, `/are-specify`, `/are-clean`\n- **Session hook** — Auto-updates docs when session ends (Claude/Gemini)\n\n### 2. Initialize Configuration\n\nAfter installation, create the configuration file in your AI assistant:\n\n```bash\n/are-init\n```\n\nThis creates `.agents-reverse-engineer/config.yaml` with default settings.\n\n### 3. Generate Documentation\n\nIn your AI assistant:\n\n```\n/are-discover\n/are-generate\n```\n\nThe assistant creates the plan and generates all documentation.\n\n### Non-Interactive Installation\n\n```bash\n# Install for Claude Code globally\nnpx agents-reverse-engineer@latest --runtime claude -g\n\n# Install for all runtimes locally\nnpx agents-reverse-engineer@latest --runtime all -l\n```\n\n### Uninstall\n\n```bash\nnpx agents-reverse-engineer@latest uninstall\n```\n\nRemoves:\n- Command files (`/are-*` commands)\n- Session hooks (Claude/Gemini)\n- ARE permissions from settings.json\n- `.agents-reverse-engineer` folder (local installs only)\n\nUse `--runtime` and `-g`/`-l` flags for specific targets.\n\n### Checking Version\n\n```bash\nnpx agents-reverse-engineer@latest --version\n```\n\n---\n\n## How It Works\n\n### 1. Install Commands\n\n```bash\nnpx agents-reverse-engineer@latest\n```\n\nInteractive installer installs commands and hooks for your chosen runtime(s).\n\n**Runtimes:** Claude Code, OpenCode, Gemini CLI (or all at once)\n\n---\n\n### 2. Initialize Configuration\n\n```\n/are-init\n```\n\nCreates `.agents-reverse-engineer/config.yaml` with exclusion patterns and options.\n\n---\n\n### 3. Discover & Plan\n\n```\n/are-discover\n```\n\nScans your codebase (respecting `.gitignore`), detects file types, and creates `GENERATION-PLAN.md` with all files to analyze.\n\nUses **post-order traversal** — deepest directories first, so child documentation exists before parent directories are documented.\n\n---\n\n### 4. Generate (in your AI assistant)\n\n```\n/are-generate\n```\n\nYour AI assistant executes the plan:\n\n1. **File Analysis** — Creates `.sum` file for each source file\n2. **Directory Docs** — Creates `AGENTS.md` for each directory\n3. **Root Docs** — Creates `CLAUDE.md`, `GEMINI.md`, `OPENCODE.md`\n\n---\n\n### 5. Update Incrementally\n\n```\n/are-update\n```\n\nOnly regenerates documentation for files that changed since last run.\n\n---\n\n### 6. Generate Specification\n\n```\n/are-specify\n```\n\nSynthesizes all AGENTS.md documentation into a single project specification document (`specs/SPEC.md`). Use `--multi-file` to split into separate files, or `--dry-run` to preview without calling the AI.\n\n---\n\n## Commands\n\n| Command                         | Description                      |\n| ------------------------------- | -------------------------------- |\n| `are`                           | Interactive installer (default)  |\n| `are install`                   | Install with prompts             |\n| `are install --runtime <rt> -g` | Install to runtime globally      |\n| `are install --runtime <rt> -l` | Install to runtime locally       |\n| `are install -u`                | Uninstall (remove files/hooks)   |\n| `are init`                      | Create configuration file        |\n| `are discover`                  | List files that will be analyzed |\n| `are discover --plan`           | Create GENERATION-PLAN.md        |\n| `are discover --show-excluded`  | Show excluded files with reasons |\n| `are generate`                  | Generate all documentation       |\n| `are update`                    | Update changed files only        |\n| `are specify`                   | Generate project specification   |\n| `are clean`                     | Remove all generated docs        |\n\n**Runtimes:** `claude`, `opencode`, `gemini`, `all`\n\n### AI Assistant Commands\n\n| Command         | Description                    | Supported Runtimes       |\n| --------------- | ------------------------------ | ------------------------ |\n| `/are-init`     | Initialize config and commands | Claude, OpenCode, Gemini |\n| `/are-discover` | Rediscover and regenerate plan | Claude, OpenCode, Gemini |\n| `/are-generate` | Generate all documentation     | Claude, OpenCode, Gemini |\n| `/are-update`   | Update changed files only      | Claude, OpenCode, Gemini |\n| `/are-specify`  | Generate project specification | Claude, OpenCode, Gemini |\n| `/are-clean`    | Remove all generated docs      | Claude, OpenCode, Gemini |\n\n---\n\n## Generated Documentation\n\n### `.sum` Files (Per File)\n\n```yaml\n---\nfile_type: service\ngenerated_at: 2026-01-30T12:00:00Z\n---\n\n## Purpose\nHandles user authentication via JWT tokens.\n\n## Public Interface\n- `authenticate(token: string): User`\n- `generateToken(user: User): string`\n\n## Dependencies\n- jsonwebtoken: Token signing/verification\n- ./user-repository: User data access\n\n## Implementation Notes\nTokens expire after 24 hours. Refresh handled by client.\n```\n\n### `AGENTS.md` (Per Directory)\n\nDirectory overview with:\n\n- Description of the directory's role\n- Files grouped by purpose (Types, Services, Utils, etc.)\n- Subdirectories with brief descriptions\n\n### Root Documents\n\n- **`CLAUDE.md`** — Project entry point for Claude Code (auto-loaded)\n- **`GEMINI.md`** — Project entry point for Gemini CLI\n- **`OPENCODE.md`** — Project entry point for OpenCode\n- **`AGENTS.md`** — Root directory overview (universal format)\n\n---\n\n## Configuration\n\nEdit `.agents-reverse-engineer/config.yaml`:\n\n```yaml\n# File and directory exclusions\nexclude:\n  patterns: []              # Custom glob patterns (e.g., [\"*.log\", \"temp/**\"])\n  vendorDirs:               # Directories to skip\n    - node_modules\n    - dist\n    - .git\n  binaryExtensions:         # File types to skip\n    - .png\n    - .jpg\n    - .pdf\n\n# Discovery options\noptions:\n  followSymlinks: false     # Follow symbolic links during traversal\n  maxFileSize: 1048576      # Max file size in bytes (1MB default)\n\n# Output formatting\noutput:\n  colors: true              # Use colors in terminal output\n  verbose: true             # Show each file as processed\n\n# AI service configuration\nai:\n  backend: auto             # Backend: 'claude', 'gemini', 'opencode', 'auto'\n  model: sonnet             # Model identifier (backend-specific)\n  timeoutMs: 300000         # Subprocess timeout in ms (5 minutes)\n  maxRetries: 3             # Max retries for transient errors\n  concurrency: 5            # Parallel AI calls (1-10, lower for WSL/constrained envs)\n\n  telemetry:\n    keepRuns: 50            # Number of run logs to keep\n    costThresholdUsd: 10.0  # Optional: warn when cost exceeds this (USD)\n\n  # Optional: Custom model pricing (override defaults)\n  pricing:\n    claude-opus-4:\n      inputCostPerMTok: 15.0    # USD per 1M input tokens\n      outputCostPerMTok: 75.0   # USD per 1M output tokens\n```\n\n### Key Config Options\n\n**Concurrency (`ai.concurrency`)**\n- Default: `5` (changed from 5 to 2 in WSL environments)\n- Range: `1-10`\n- Lower values recommended for resource-constrained environments\n- Higher values speed up generation but use more memory\n\n**Timeout (`ai.timeoutMs`)**\n- Default: `300000` (5 minutes)\n- AI subprocess timeout for each file analysis\n- Increase for very large files or slow connections\n\n---\n\n## Requirements\n\n- **Node.js 18+**\n- **AI Coding Assistant** — One of:\n  - [Claude Code](https://claude.ai/claude-code) (full support + session hooks)\n  - [Gemini CLI](https://github.com/google-gemini/gemini-cli) (full support + session hooks)\n  - [OpenCode](https://github.com/opencode-ai/opencode) (AGENTS.md supported)\n  - Any assistant supporting `AGENTS.md` format\n\n---\n\n## License\n\nMIT\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**README.md serves as the project marketing page, installation guide, and CLI reference for agents-reverse-engineer, documenting npx-based interactive installation, IDE command workflows (`/are-generate`, `/are-update`), three-phase generation pipeline (file analysis → directory docs → root docs), and configuration schema for `.agents-reverse-engineer/config.yaml`.**\n\n## Installation Patterns\n\n**Interactive installer invocation:** `npx agents-reverse-engineer@latest` launches prompt-based flow selecting runtime (`claude`, `opencode`, `gemini`, `all`) and location scope (global `-g` targeting `~/.claude/`, local `-l` targeting `./.claude/`).\n\n**Non-interactive installation flags:** `--runtime <rt>` plus `-g`/`-l` bypass prompts for CI/CD scenarios. Example: `npx agents-reverse-engineer@latest --runtime claude -g` installs to `~/.claude/commands/` and `~/.claude/hooks/`.\n\n**Uninstall command:** `npx agents-reverse-engineer@latest uninstall` removes command files (`/are-*`), session hooks (Claude/Gemini only), ARE permission entries from `settings.json`, and `.agents-reverse-engineer/` folder (local installs only).\n\n**Version check:** `npx agents-reverse-engineer@latest --version` displays installed version.\n\n## CLI Commands\n\n**Entry point:** `are` binary (alias: `agents-reverse-engineer`) invokes `src/cli/index.ts`.\n\n**Command surface:**\n- `are install` — Interactive installer with runtime/location prompts\n- `are install --runtime <rt> -g/-l` — Non-interactive installation\n- `are install -u` — Uninstall (removes files/hooks/permissions)\n- `are init` — Creates `.agents-reverse-engineer/config.yaml` with defaults\n- `are discover` — Scans codebase, lists discoverable files\n- `are discover --plan` — Writes `GENERATION-PLAN.md` with post-order directory traversal\n- `are discover --show-excluded` — Displays excluded files with exclusion reasons\n- `are generate` — Executes three-phase pipeline: `.sum` files → `AGENTS.md` per directory → root docs (`CLAUDE.md`/`GEMINI.md`/`OPENCODE.md`)\n- `are update` — Regenerates only files with SHA-256 content hash mismatches\n- `are specify` — Synthesizes all `AGENTS.md` into `specs/SPEC.md` (use `--multi-file` for split output, `--dry-run` for preview)\n- `are clean` — Deletes `.sum`, `AGENTS.md` (generated only), `CLAUDE.md`, `GEMINI.md`, `OPENCODE.md`, `GENERATION-PLAN.md`\n\n## IDE Command Integration\n\n**Claude Code commands:** Installed to `~/.claude/skills/are-<command>/SKILL.md` with frontmatter `name: /are-<command>`.\n\n**OpenCode commands:** Installed to `~/.config/opencode/commands/are-<command>.md` with frontmatter `agent: build`.\n\n**Gemini CLI commands:** Installed to `~/.gemini/commands/are-<command>.toml` with TOML format (`description`, `prompt` triple-quoted strings).\n\n**Supported commands:**\n- `/are-init` — Initialize configuration and commands\n- `/are-discover` — Rediscover files and regenerate plan\n- `/are-generate` — Full documentation generation (long-running: remove stale `progress.log`, use `run_in_background: true`, poll via `tail -5` every 10-15s, check `TaskOutput` with `block: false`)\n- `/are-update` — Incremental update for changed files\n- `/are-specify` — Generate project specification from aggregated docs\n- `/are-clean` — Remove all generated artifacts\n\n## Generated Documentation Artifacts\n\n**`.sum` files:** Per-file summaries with YAML frontmatter containing `file_type`, `generated_at` timestamp. Body includes purpose statement, public interface (function/class signatures), dependencies (package imports + local file paths), implementation notes. Example: `authenticate(token: string): User`, `generateToken(user: User): string`.\n\n**`AGENTS.md`:** Per-directory overviews with directory role description, files grouped by purpose (Types, Services, Utils), subdirectory brief descriptions. User-authored `AGENTS.md` renamed to `AGENTS.local.md` and prepended to generated content.\n\n**Root documents:**\n- `CLAUDE.md` — Project entry point for Claude Code (auto-loaded via platform convention)\n- `GEMINI.md` — Project entry point for Gemini CLI\n- `OPENCODE.md` — Project entry point for OpenCode\n- Root `AGENTS.md` — Universal format directory overview\n\n**`GENERATION-PLAN.md`:** Post-order directory traversal plan listing all files to analyze, grouped by phase (file analysis, directory aggregation, root synthesis).\n\n**`specs/SPEC.md`:** Synthesized project specification from all `AGENTS.md` files (generated via `are specify`).\n\n## Configuration Schema\n\n**Config file location:** `.agents-reverse-engineer/config.yaml`\n\n**Section: `exclude`**\n- `patterns: string[]` — Gitignore-style globs (e.g., `[\"*.log\", \"temp/**\"]`)\n- `vendorDirs: string[]` — Third-party directories to skip (default: `node_modules`, `dist`, `.git`, etc.)\n- `binaryExtensions: string[]` — Non-text file extensions (default: `.png`, `.jpg`, `.pdf`, etc.)\n\n**Section: `options`**\n- `followSymlinks: boolean` — Follow symbolic links during traversal (default: `false`)\n- `maxFileSize: number` — Binary detection threshold in bytes (default: `1048576` = 1MB)\n\n**Section: `output`**\n- `colors: boolean` — Enable ANSI color codes in terminal (default: `true`)\n- `verbose: boolean` — Show each file as processed (default: `true`)\n\n**Section: `ai`**\n- `backend: 'claude' | 'gemini' | 'opencode' | 'auto'` — AI service selection (default: `auto` detects first available)\n- `model: string | null` — Model identifier override (backend-specific, default: `sonnet`)\n- `timeoutMs: number` — Subprocess timeout in milliseconds (default: `300000` = 5 minutes)\n- `maxRetries: number` — Exponential backoff retry attempts (default: `3`)\n- `concurrency: number` — Worker pool size 1-10 (default: `5`, reduced to `2` for WSL environments due to resource constraints)\n\n**Subsection: `ai.telemetry`**\n- `keepRuns: number` — Retention limit for historical run logs (default: `50`)\n- `costThresholdUsd: number | null` — Warning threshold for cumulative costs in USD (optional)\n\n**Subsection: `ai.pricing` (per-backend overrides)**\n- `inputCostPerMTok: number` — USD per 1 million input tokens\n- `outputCostPerMTok: number` — USD per 1 million output tokens\n\n**Example pricing entry:**\n```yaml\npricing:\n  claude-opus-4:\n    inputCostPerMTok: 15.0\n    outputCostPerMTok: 75.0\n```\n\n## Three-Phase Generation Pipeline\n\n**Phase 1: File Analysis**\n- Concurrent execution via iterator-based worker pool (`src/orchestration/pool.ts`)\n- Generates `.sum` file for each source file via AI subprocess\n- Respects `.gitignore`, excludes vendor directories and binary files\n- SHA-256 content hashing in YAML frontmatter for incremental updates\n\n**Phase 2: Directory Aggregation**\n- Post-order traversal (deepest directories first)\n- Waits for all child `.sum` files via `isDirectoryComplete()` predicate\n- Generates `AGENTS.md` per directory consuming child summaries and subdirectory `AGENTS.md` files\n- Import maps via `extractDirectoryImports()` with path verification\n- Manifest detection (9 types: `package.json`, `Cargo.toml`, `go.mod`, `pyproject.toml`, `pom.xml`, `build.gradle`, `Gemfile`, `composer.json`, `CMakeLists.txt`/`Makefile`)\n\n**Phase 3: Root Document Synthesis**\n- Sequential execution (concurrency=1)\n- Generates `CLAUDE.md`, `GEMINI.md`, `OPENCODE.md`\n- Consumes all `AGENTS.md` files via `collectAgentsDocs()` recursive traversal\n- Parses root `package.json` for project metadata\n- Enforces synthesis-only constraint (no invention of features not in source documents)\n\n## Incremental Update Workflow\n\n**Hash comparison strategy:**\n1. Read `content_hash` from `.sum` YAML frontmatter via `readSumFile()`\n2. Compute current SHA-256 hash via `computeContentHash()`\n3. Mismatch → add to `filesToAnalyze` with `status: 'modified'` or `'added'`\n4. Match → add to `filesToSkip`\n5. Detect orphans: `.sum` files for deleted sources or renamed oldPaths\n6. Call `cleanupOrphans()` to delete stale summaries\n7. Call `cleanupEmptyDirectoryDocs()` to remove `AGENTS.md` from directories with no remaining sources\n8. Compute `affectedDirs` via `getAffectedDirectories()` by walking parent directories\n9. Regenerate `.sum` for changed files via Phase 1 pool\n10. Regenerate `AGENTS.md` for affected directories sequentially\n\n**Git integration:** Supports `git diff <baseCommit>..HEAD` for committed changes, `git status --porcelain` merge via `--uncommitted` flag, rename detection via `git diff -M` (50% similarity threshold).\n\n## Subprocess Resource Management\n\n**Problem:** Claude CLI spawns excessive Node.js threads (GitHub issue #5771: 200 instances reported). With concurrency=5, each subprocess spawns internal threads causing RAM exhaustion in WSL environments.\n\n**Mitigations applied:**\n- `NODE_OPTIONS='--max-old-space-size=512'` limits heap to 512MB per subprocess\n- `UV_THREADPOOL_SIZE='4'` constrains libuv thread pool to 4 threads\n- `CLAUDE_CODE_DISABLE_BACKGROUND_TASKS='1'` prevents background task spawning\n- `--disallowedTools Task` prevents subprocess from spawning subagents\n- Process group killing: `kill(-pid)` terminates entire subprocess tree\n- SIGTERM sent at `timeoutMs`, SIGKILL escalation after 5s grace period\n- Default concurrency reduced from 5 → 2 for resource-constrained environments\n\n## Session Hooks\n\n**Claude/Gemini lifecycle:**\n- `are-check-update.js` (SessionStart) — Spawns detached process querying `npm view agents-reverse-engineer version`, compares against `~/.claude/ARE-VERSION`, caches result to `~/.claude/cache/are-update-check.json`\n- `are-session-end.js` (SessionEnd) — Runs `git status --porcelain`, spawns `npx agents-reverse-engineer@latest update --quiet` as detached background process if changes detected\n\n**OpenCode plugins:**\n- `opencode-are-check-update.js` — Exports `AreCheckUpdate()` async factory returning plugin with `event['session.created']` handler\n- `opencode-are-session-end.js` — Exports `AreSessionEnd()` async factory returning plugin with `event['session.deleted']` handler\n\n**Disable mechanisms:**\n- Environment variable: `ARE_DISABLE_HOOK=1`\n- Config flag: `hook_enabled: false` in `.agents-reverse-engineer.yaml` (substring search, no YAML parser)\n\n## Requirements\n\n**Runtime:** Node.js ≥18.0.0\n\n**AI Coding Assistant (at least one):**\n- Claude Code — Full support with session hooks, auto-loaded `CLAUDE.md`\n- Gemini CLI — Full support with session hooks, manual `GEMINI.md` loading\n- OpenCode — `AGENTS.md` format supported, manual `OPENCODE.md` loading\n- Any assistant supporting `AGENTS.md` format (universal compatibility)\n\n## Marketing Positioning\n\n**Value proposition:** Eliminates repetitive context explanation in AI assistant sessions by generating persistent, assistant-readable documentation (`.sum` files, `AGENTS.md`, root docs).\n\n**Target audience:** Developers using AI coding assistants (Claude Code, OpenCode, Gemini CLI) who want codebase understanding from first message without manual documentation overhead.\n\n**Pain points addressed:**\n- \"AI assistant doesn't know my codebase structure\"\n- \"Every session starts fresh\"\n- \"Explaining same architecture repeatedly\"\n\n**User testimonials (quoted):**\n- \"Finally, my AI assistant actually understands my codebase structure.\"\n- \"No more explaining the same architecture in every conversation.\"\n\n## License\n\nMIT",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 3181,
      "cacheReadTokens": 17114,
      "cacheCreationTokens": 11770,
      "latencyMs": 70104,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "README.md",
          "sizeBytes": 9778
        }
      ]
    },
    {
      "timestamp": "2026-02-09T16:12:32.905Z",
      "prompt": "Generate AGENTS.md for directory: \"src/quality/density\" (density)\n\n## File Summaries (1 files)\n\n### validator.ts\n**Purpose:** validator.ts provides a disabled stub for findability validation that previously checked whether exported symbols fro...\n\n**validator.ts provides a disabled stub for findability validation that previously checked whether exported symbols from .sum files appeared in parent AGENTS.md content, retained for future structured metadata extraction support.**\n\n## Exported Interface\n\n**FindabilityResult** - Validation result for a single .sum file containing:\n- `filePath: string` - Path to the validated .sum file\n- `symbolsTested: string[]` - Symbol names tested for presence\n- `symbolsFound: string[]` - Symbol names found in AGENTS.md content\n- `symbolsMissing: string[]` - Symbol names absent from AGENTS.md content\n- `score: number` - Ratio of found to tested symbols (0.0 to 1.0 range)\n\n## Exported Function\n\n**validateFindability** - `(agentsMdContent: string, sumFiles: Map<string, SumFileContent>) => FindabilityResult[]`\n\nReturns empty array since structured metadata extraction removed. Previously validated that key symbols from .sum files appeared in AGENTS.md by:\n1. Extracting symbol names from `metadata.publicInterface` field in `SumFileContent`\n2. Searching for symbol presence in `agentsMdContent` via substring matching\n3. Computing per-file `FindabilityResult` with score calculation\n\nParameters ignored (prefixed with underscore): `_agentsMdContent` consumed full AGENTS.md text, `_sumFiles` consumed map of file paths to parsed `SumFileContent` objects.\n\n## Integration Context\n\n**Disabled Feature** - Function signature preserved but implementation gutted after `SumFileContent.metadata.publicInterface` field removed from schema (`src/generation/writers/sum.ts`). No callers in codebase invoke this validator (disabled in `src/quality/index.ts` quality validation pipeline).\n\n**Historical Design** - LLM-free validation using string-based symbol matching. No AI subprocess calls, purely heuristic checking via substring search. Intended to detect when directory-level AGENTS.md aggregation failed to preserve critical symbol names from child .sum file summaries.\n\n**Restoration Path** - Future re-implementation requires:\n1. Adding structured export extraction to .sum file generation (Phase 1 of three-phase pipeline)\n2. Parsing YAML frontmatter in .sum files via `src/generation/writers/sum.ts:readSumFile()`\n3. Implementing symbol presence checks against AGENTS.md content\n4. Re-enabling in `src/quality/index.ts` quality validator orchestrator\n\n## Type Dependencies\n\nImports `SumFileContent` from `src/generation/writers/sum.ts` - Interface for parsed .sum file containing YAML frontmatter (`generated_at`, `content_hash`, `purpose`, `critical_todos[]`, `related_files[]`) plus markdown summary content.\n\n## Import Map (verified — use these exact paths)\n\nvalidator.ts:\n  ../../generation/writers/sum.js → SumFileContent (type)\n\n## Project Directory Structure\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>",
      "systemPrompt": "You are generating an AGENTS.md file — a directory-level overview for AI coding assistants.\n\nCRITICAL: Output ONLY the raw markdown content. No code fences, no preamble, no explanations, no conversational text. Your entire response IS the AGENTS.md file content.\n\nOUTPUT FORMAT:\n- First line MUST be exactly: <!-- Generated by agents-reverse-engineer -->\n- Use a # heading with the directory name\n- Write a one-paragraph purpose statement for the directory\nusage\nADAPTIVE SECTIONS:\nAnalyze the directory contents and choose the most relevant sections. Do NOT use a fixed template. Instead, select sections that best document this specific directory for an AI that needs to reconstruct or extend the project.\n\nConsider these section types (choose what applies):\n- **Contents**: Group files by purpose/category under ## headings. For each file: markdown link [filename](./filename) and a one-line description.\n- **Subdirectories**: If subdirectories exist, list them with links [dirname/](./dirname/) and brief summaries.\n- **Architecture / Data Flow**: If files form a pipeline, request/response chain, or layered architecture, document it.\n- **Stack**: If this is a package root (has package.json, Cargo.toml, go.mod, etc.), document the technology stack, key scripts, and entry points.\n- **Structure**: If the directory layout follows a convention (feature-sliced, domain-driven, MVC, etc.), document it.\n- **Patterns**: If files share recurring design patterns (factory, strategy, middleware, barrel re-export), name and document them.\n- **Configuration**: If the directory contains config files, schemas, or environment definitions, document the config surface area.\n- **API Surface**: If the directory exports a public API (barrel index, route definitions, SDK), document the interface contract.\n- **File Relationships**: How files collaborate, depend on each other, or share state.\n- **Behavioral Contracts**: If files contain regex patterns, format specifications, magic constants, or template strings that define observable behavior, collect them in a dedicated section. Preserve verbatim patterns from file summaries — do NOT paraphrase regex into prose. This section is MANDATORY when file summaries contain behavioral artifacts.\n\nChoose any relevant sections or create your own based on the directory contents. The goal is to provide a comprehensive overview that captures the essence of the directory's role in the project and how its files work together, with a focus on what an AI coding assistant would need to know to effectively interact with this code.\n\nSCOPE:\n- AGENTS.md is a NAVIGATIONAL INDEX — help an AI find the right file quickly\n- Focus on: what each file does, how files relate, directory-level patterns\n- Do NOT reproduce full architecture sections — those belong in the root CLAUDE.md\n\nPATH ACCURACY (MANDATORY):\n- When referencing files or modules outside this directory, use ONLY paths from the \"Import Map\" section\n- Do NOT invent, rename, or guess module paths — if a path isn't in the Import Map, don't reference it\n- Use the exact directory names from \"Project Directory Structure\" — do NOT rename directories\n  (e.g., if the directory is called \"cli\", write \"src/cli/\", NOT \"src/commands/\")\n- Cross-module references must use the specifier format from actual import statements\n  (e.g., \"../generation/writers/sum.js\", NOT \"../fs/sum-file.js\")\n- If you are unsure about a path, omit the cross-reference rather than guessing\n\nCONSISTENCY (MANDATORY):\n- Do not contradict yourself within the same document\n- If you describe a technique (e.g., \"regex-based\"), do not call it something else later (e.g., \"AST-based\")\n- When stating version numbers, engines, or config fields, use ONLY values present in the file summaries\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this directory\", \"this module\", \"provides\", \"responsible for\", \"is used to\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Per-file descriptions in Contents sections: 1-2 sentences maximum. Reference key symbols but do not reproduce full summaries.\n- Behavioral contracts (regex patterns, format specs, constants) belong in a separate Behavioral Contracts section, not in per-file descriptions.\n- Subdirectory descriptions: 1-2 sentences maximum. Capture the directory's role, not its full contents.\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- Key exported symbols from file summaries MUST appear in the directory overview\n- Preserve exact casing of identifiers\n\nUSER NOTES:\n- If \"User Notes\" are provided in the prompt, they contain user-defined instructions that will be automatically prepended to your output\n- Do NOT repeat or paraphrase user notes in your generated content — they are included separately\n- You may reference information from user notes for context",
      "response": "<!-- Generated by agents-reverse-engineer -->\n\n# src/quality/density\n\nDisabled findability validation stub that previously verified exported symbols from .sum files appeared in parent AGENTS.md content, retained for future structured metadata extraction support.\n\n## Contents\n\n**[validator.ts](./validator.ts)** — Exports `validateFindability()` returning empty array; previously extracted symbol names from `SumFileContent.metadata.publicInterface` and performed substring search in AGENTS.md content to compute per-file `FindabilityResult` with score calculation.\n\n## Exported Interface\n\n**FindabilityResult** — Validation result containing `filePath`, `symbolsTested[]`, `symbolsFound[]`, `symbolsMissing[]`, and `score` (0.0–1.0 ratio of found to tested symbols).\n\n**validateFindability()** — `(agentsMdContent: string, sumFiles: Map<string, SumFileContent>) => FindabilityResult[]` — Returns empty array; signature preserved but implementation gutted after `SumFileContent.metadata.publicInterface` removal.\n\n## Disabled Feature Context\n\nvalidator.ts implements LLM-free validation using string-based symbol matching with no AI subprocess calls. Disabled in `../index.ts` quality validation pipeline when `publicInterface` field removed from `SumFileContent` schema in `../../generation/writers/sum.ts`. Previously detected when directory-level AGENTS.md aggregation failed to preserve critical symbol names from child .sum file summaries.\n\n## Restoration Path\n\nRe-implementation requires:\n1. Adding structured export extraction to .sum file generation (Phase 1 of three-phase pipeline in `../../generation/orchestrator.ts`)\n2. Parsing YAML frontmatter in .sum files via `readSumFile()` from `../../generation/writers/sum.ts`\n3. Implementing symbol presence checks against AGENTS.md content\n4. Re-enabling in `../index.ts` quality validator orchestrator\n\n## Type Dependencies\n\nImports `SumFileContent` from `../../generation/writers/sum.ts` — Interface for parsed .sum file containing YAML frontmatter (`generated_at`, `content_hash`, `purpose`, `critical_todos[]`, `related_files[]`) and markdown summary content.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 513,
      "cacheReadTokens": 16334,
      "cacheCreationTokens": 10838,
      "latencyMs": 28113,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": []
    },
    {
      "timestamp": "2026-02-09T16:12:32.932Z",
      "prompt": "Generate AGENTS.md for directory: \"src/quality/inconsistency\" (inconsistency)\n\n## File Summaries (3 files)\n\n### code-vs-code.ts\n**Purpose:** checkCodeVsCode() detects duplicate exports across source files by aggregating symbol names per file and flagging sym...\n\n**checkCodeVsCode() detects duplicate exports across source files by aggregating symbol names per file and flagging symbols exported from multiple files within a scoped group.**\n\n## Exported Functions\n\n**checkCodeVsCode(files: Array<{ path: string; content: string }>): CodeCodeInconsistency[]**\nDetects duplicate exports across file groups by extracting symbol names via `extractExports()` from `code-vs-doc.ts`, building a `Map<string, string[]>` where keys are export names and values are file paths, and returning inconsistency records for symbols appearing in multiple files. Each inconsistency has `type: 'code-vs-code'`, `severity: 'warning'`, `pattern: 'duplicate-export'`, `files` array of paths, and `description` string with count.\n\n## Dependencies\n\n**extractExports()** imported from `./code-vs-doc.js` performs regex-based symbol extraction using pattern `/^[ \\t]*export\\s+(?:default\\s+)?(?:function|class|const|let|var|type|interface|enum)\\s+(\\w+)/gm`.\n\n**CodeCodeInconsistency** type imported from `../types.js` represents cross-file inconsistencies with discriminator `type: 'code-vs-code'`.\n\n## Algorithm\n\nIterates `files` array, calls `extractExports(file.content)` to get symbol set, populates `exportMap` by appending `file.path` to existing symbol arrays or initializing new entries. Second pass filters `exportMap` entries where `paths.length > 1` and constructs inconsistency objects with formatted description `\"Symbol \\\"${name}\\\" exported from ${paths.length} files\"`.\n\n## Scoping Contract\n\nCaller must scope input to per-directory file groups to avoid false positives across unrelated modules. No AI calls or AST analysis—purely heuristic substring matching. Intentional duplication (e.g., factory patterns, parallel implementations) cannot be distinguished without semantic analysis.\n\n## Integration Points\n\nUsed by `reporter.ts` in quality validation pipeline alongside `checkCodeVsDoc()` and phantom path validators. Output format matches `InconsistencyReport.issues[]` discriminated union schema.\n### code-vs-doc.ts\n**Purpose:** code-vs-doc.ts detects documentation drift by extracting exported identifiers from TypeScript/JavaScript source and v...\n\n**code-vs-doc.ts detects documentation drift by extracting exported identifiers from TypeScript/JavaScript source and verifying their presence in corresponding .sum file content.**\n\n## Exported Functions\n\n**extractExports(sourceContent: string): string[]** — Extracts exported identifiers from TypeScript/JavaScript source using regex pattern `/^[ \\t]*export\\s+(?:default\\s+)?(?:function|class|const|let|var|type|interface|enum)\\s+(\\w+)/gm`. Matches declarations like `export function foo`, `export const BAR`, `export default class App` while ignoring re-exports and commented-out lines. Returns array of captured identifier names from match group 1.\n\n**checkCodeVsDoc(sourceContent: string, sumContent: SumFileContent, filePath: string): CodeDocInconsistency | null** — Compares source exports against parsed .sum file content to detect documentation drift. Extracts exports via `extractExports()`, then filters for symbols not present in `sumContent.summary` text via case-sensitive substring matching (`!sumText.includes(e)`). Returns `CodeDocInconsistency` with `type: 'code-vs-doc'`, `severity: 'warning'`, `details.missingFromDoc[]` array when drift detected, or `null` when documentation is consistent. The `details.missingFromCode` array is always empty (legacy field from removed `publicInterface` schema).\n\n## Detection Algorithm\n\nPerforms one-way consistency check: exports present in source but undocumented. Regex captures identifier from declaration keywords (`function`, `class`, `const`, `let`, `var`, `type`, `interface`, `enum`) following `export` or `export default`. Verification uses substring search against entire `.summary` text rather than structured fields, which yields false negatives when symbols appear only in prose unrelated to API surface.\n\n## Type Dependencies\n\nImports `SumFileContent` from `../../generation/writers/sum.js` for parsed .sum structure with `summary: string` field. Imports `CodeDocInconsistency` from `../types.js` for discriminated union member with required fields: `type`, `severity`, `filePath`, `sumPath`, `description`, `details: { missingFromDoc: string[], missingFromCode: string[] }`.\n\n## Known Limitations\n\nRegex-based export extraction misses complex patterns including destructured exports (`export { foo, bar }`), namespace exports (`export * from './module'`), dynamic exports, and multiline declarations. Substring matching produces false negatives when export names appear in comments, prose descriptions, or unrelated context rather than as documented API surface. No AST analysis performed to distinguish intentional API surface from implementation details.\n### reporter.ts\n**Purpose:** reporter.ts builds structured inconsistency reports from validation results and formats them as plain-text CLI output.\n\n**reporter.ts builds structured inconsistency reports from validation results and formats them as plain-text CLI output.**\n\n## Exported Functions\n\n### buildInconsistencyReport\n```typescript\nfunction buildInconsistencyReport(\n  issues: Inconsistency[],\n  metadata: { projectRoot: string; filesChecked: number; durationMs: number }\n): InconsistencyReport\n```\n\nAggregates `Inconsistency[]` array into `InconsistencyReport` with summary counts by type (`code-vs-doc`, `code-vs-code`, `phantom-path`) and severity (`error`, `warning`, `info`). Populates `metadata` object with ISO 8601 `timestamp` via `new Date().toISOString()`, `projectRoot`, `filesChecked`, `durationMs`. Returns `InconsistencyReport` containing `metadata`, `issues`, and `summary` fields with counts: `total`, `codeVsDoc`, `codeVsCode`, `phantomPaths`, `errors`, `warnings`, `info`.\n\n### formatReportForCli\n```typescript\nfunction formatReportForCli(report: InconsistencyReport): string\n```\n\nTransforms `InconsistencyReport` into human-readable plain text for stderr output. Uses no color dependencies (picocolors-free for testability). Outputs format:\n\n```\n=== Inconsistency Report ===\nChecked 42 files in 150ms\nFound 3 issue(s)\n\n[ERROR] <description>\n  File: <filePath>                    (code-vs-doc type)\n  Doc: <agentsMdPath>                 (phantom-path type)\n  Path: <referencedPath>\n  Files: <file1>, <file2>, ...        (code-vs-code type)\n```\n\nSeverity tags: `[ERROR]`, `[WARN]`, `[INFO]` map from `issue.severity`. Discriminates union via `issue.type` to render type-specific fields: `filePath` for `code-vs-doc`, `agentsMdPath` and `details.referencedPath` for `phantom-path`, `files.join(', ')` for `code-vs-code`.\n\n## Integration with Quality Pipeline\n\nCalled by `src/quality/index.ts` validation orchestrator after `validateCodeVsDoc()`, `validateCodeVsCode()`, `validatePhantomPaths()` return `Inconsistency[]` arrays. `buildInconsistencyReport()` receives merged issues array and run metadata (files checked, duration). `formatReportForCli()` output written to `progress.log` and stderr, providing structured summary of code-documentation inconsistencies, duplicate exports, and unresolved path references detected during post-generation validation phase.\n\n## Types\n\nConsumes `Inconsistency` discriminated union (type: `'code-vs-doc' | 'code-vs-code' | 'phantom-path'`) and `InconsistencyReport` structure from `../types.js`. `Inconsistency` members:\n- `CodeDocInconsistency`: `filePath`, `missingFromDoc` array, `missingFromCode` array, `pattern: 'undocumented-exports' | 'undocumented-only'`\n- `CodeCodeInconsistency`: `symbol`, `files` array, `pattern: 'duplicate-export'`\n- `PhantomPathInconsistency`: `agentsMdPath`, `details.referencedPath`, `details.attemptedPaths` array, `pattern: 'unresolved-path'`\n\nAll share `severity: 'error' | 'warning' | 'info'` and `description` string fields.\n\n## Import Map (verified — use these exact paths)\n\ncode-vs-code.ts:\n  ../types.js → CodeCodeInconsistency (type)\n\ncode-vs-doc.ts:\n  ../../generation/writers/sum.js → SumFileContent (type)\n  ../types.js → CodeDocInconsistency (type)\n\nreporter.ts:\n  ../types.js → Inconsistency, InconsistencyReport (type)\n\n## Project Directory Structure\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>",
      "systemPrompt": "You are generating an AGENTS.md file — a directory-level overview for AI coding assistants.\n\nCRITICAL: Output ONLY the raw markdown content. No code fences, no preamble, no explanations, no conversational text. Your entire response IS the AGENTS.md file content.\n\nOUTPUT FORMAT:\n- First line MUST be exactly: <!-- Generated by agents-reverse-engineer -->\n- Use a # heading with the directory name\n- Write a one-paragraph purpose statement for the directory\nusage\nADAPTIVE SECTIONS:\nAnalyze the directory contents and choose the most relevant sections. Do NOT use a fixed template. Instead, select sections that best document this specific directory for an AI that needs to reconstruct or extend the project.\n\nConsider these section types (choose what applies):\n- **Contents**: Group files by purpose/category under ## headings. For each file: markdown link [filename](./filename) and a one-line description.\n- **Subdirectories**: If subdirectories exist, list them with links [dirname/](./dirname/) and brief summaries.\n- **Architecture / Data Flow**: If files form a pipeline, request/response chain, or layered architecture, document it.\n- **Stack**: If this is a package root (has package.json, Cargo.toml, go.mod, etc.), document the technology stack, key scripts, and entry points.\n- **Structure**: If the directory layout follows a convention (feature-sliced, domain-driven, MVC, etc.), document it.\n- **Patterns**: If files share recurring design patterns (factory, strategy, middleware, barrel re-export), name and document them.\n- **Configuration**: If the directory contains config files, schemas, or environment definitions, document the config surface area.\n- **API Surface**: If the directory exports a public API (barrel index, route definitions, SDK), document the interface contract.\n- **File Relationships**: How files collaborate, depend on each other, or share state.\n- **Behavioral Contracts**: If files contain regex patterns, format specifications, magic constants, or template strings that define observable behavior, collect them in a dedicated section. Preserve verbatim patterns from file summaries — do NOT paraphrase regex into prose. This section is MANDATORY when file summaries contain behavioral artifacts.\n\nChoose any relevant sections or create your own based on the directory contents. The goal is to provide a comprehensive overview that captures the essence of the directory's role in the project and how its files work together, with a focus on what an AI coding assistant would need to know to effectively interact with this code.\n\nSCOPE:\n- AGENTS.md is a NAVIGATIONAL INDEX — help an AI find the right file quickly\n- Focus on: what each file does, how files relate, directory-level patterns\n- Do NOT reproduce full architecture sections — those belong in the root CLAUDE.md\n\nPATH ACCURACY (MANDATORY):\n- When referencing files or modules outside this directory, use ONLY paths from the \"Import Map\" section\n- Do NOT invent, rename, or guess module paths — if a path isn't in the Import Map, don't reference it\n- Use the exact directory names from \"Project Directory Structure\" — do NOT rename directories\n  (e.g., if the directory is called \"cli\", write \"src/cli/\", NOT \"src/commands/\")\n- Cross-module references must use the specifier format from actual import statements\n  (e.g., \"../generation/writers/sum.js\", NOT \"../fs/sum-file.js\")\n- If you are unsure about a path, omit the cross-reference rather than guessing\n\nCONSISTENCY (MANDATORY):\n- Do not contradict yourself within the same document\n- If you describe a technique (e.g., \"regex-based\"), do not call it something else later (e.g., \"AST-based\")\n- When stating version numbers, engines, or config fields, use ONLY values present in the file summaries\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this directory\", \"this module\", \"provides\", \"responsible for\", \"is used to\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Per-file descriptions in Contents sections: 1-2 sentences maximum. Reference key symbols but do not reproduce full summaries.\n- Behavioral contracts (regex patterns, format specs, constants) belong in a separate Behavioral Contracts section, not in per-file descriptions.\n- Subdirectory descriptions: 1-2 sentences maximum. Capture the directory's role, not its full contents.\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- Key exported symbols from file summaries MUST appear in the directory overview\n- Preserve exact casing of identifiers\n\nUSER NOTES:\n- If \"User Notes\" are provided in the prompt, they contain user-defined instructions that will be automatically prepended to your output\n- Do NOT repeat or paraphrase user notes in your generated content — they are included separately\n- You may reference information from user notes for context",
      "response": "<!-- Generated by agents-reverse-engineer -->\n\n# src/quality/inconsistency\n\nValidates code-documentation consistency by extracting exports from source files, verifying their presence in `.sum` documentation, detecting duplicate exports across file groups, and aggregating validation results into structured reports with CLI-formatted output.\n\n## Contents\n\n### Validators\n\n**[code-vs-doc.ts](./code-vs-doc.ts)** — `extractExports()` extracts exported identifiers from TypeScript/JavaScript source via regex pattern `/^[ \\t]*export\\s+(?:default\\s+)?(?:function|class|const|let|var|type|interface|enum)\\s+(\\w+)/gm`. `checkCodeVsDoc()` verifies extracted exports appear in `.sum` file `summary` text via substring search, returning `CodeDocInconsistency` with `missingFromDoc[]` array when drift detected, or `null` when consistent.\n\n**[code-vs-code.ts](./code-vs-code.ts)** — `checkCodeVsCode()` detects duplicate exports across file groups by aggregating `extractExports()` results into `Map<symbol, paths[]>`, returning `CodeCodeInconsistency[]` array for symbols exported from multiple files with `pattern: 'duplicate-export'` and `severity: 'warning'`.\n\n### Report Generation\n\n**[reporter.ts](./reporter.ts)** — `buildInconsistencyReport()` aggregates `Inconsistency[]` into `InconsistencyReport` with summary counts by type (`code-vs-doc`, `code-vs-code`, `phantom-path`) and severity (`error`, `warning`, `info`). `formatReportForCli()` transforms report into plain-text output with severity tags `[ERROR]`/`[WARN]`/`[INFO]` and type-specific field rendering for stderr and `progress.log` output.\n\n## Validation Algorithm\n\n**Phase 1: Export Extraction**  \nRegex-based identifier capture from source declarations (`export function foo`, `export const BAR`, `export default class App`). Matches capture group 1 containing symbol name, filtering lines matching `/^[ \\t]*export\\s+(?:default\\s+)?(?:function|class|const|let|var|type|interface|enum)\\s+(\\w+)/gm`.\n\n**Phase 2: Documentation Verification**  \nSubstring search against `.sum` file `summary` text. Export name must appear anywhere in summary content to pass validation. Missing symbols populate `CodeDocInconsistency.details.missingFromDoc[]` array.\n\n**Phase 3: Cross-File Duplicate Detection**  \nAggregates exports into `Map<symbol, string[]>` where keys are export names and values are file paths. Filters map entries where `paths.length > 1`, constructing `CodeCodeInconsistency` records with formatted description `\"Symbol \\\"${name}\\\" exported from ${paths.length} files\"`.\n\n**Phase 4: Report Aggregation**  \nMerges inconsistency arrays from code-vs-doc, code-vs-code, and phantom-path validators into single `InconsistencyReport` with summary counts. Populates metadata (timestamp, projectRoot, filesChecked, durationMs) and discriminates union via `issue.type` for CLI rendering.\n\n## Behavioral Contracts\n\n**Regex Export Pattern (code-vs-doc.ts, code-vs-code.ts):**  \n`/^[ \\t]*export\\s+(?:default\\s+)?(?:function|class|const|let|var|type|interface|enum)\\s+(\\w+)/gm`\n\n**CLI Report Severity Tags (reporter.ts):**  \n- `severity: 'error'` → `[ERROR]`  \n- `severity: 'warning'` → `[WARN]`  \n- `severity: 'info'` → `[INFO]`\n\n**Type-Specific Field Rendering (reporter.ts):**  \n- `code-vs-doc`: `File: ${filePath}`  \n- `code-vs-code`: `Files: ${files.join(', ')}`  \n- `phantom-path`: `Doc: ${agentsMdPath}`, `Path: ${referencedPath}`\n\n## Integration with Quality Pipeline\n\nCalled by `src/quality/index.ts` orchestrator after per-file validation. `checkCodeVsDoc()` invoked for each source file with parsed `.sum` content. `checkCodeVsCode()` receives scoped per-directory file groups to avoid false positives across unrelated modules. Results merged with phantom-path validation output, passed to `buildInconsistencyReport()` with run metadata (filesChecked, durationMs), then formatted via `formatReportForCli()` for stderr and `progress.log` output.\n\n## Known Limitations\n\n**Regex-Based Export Extraction:**  \nMisses destructured exports (`export { foo, bar }`), namespace exports (`export * from './module'`), dynamic exports, multiline declarations. No AST analysis—purely heuristic pattern matching.\n\n**Substring Verification:**  \nProduces false negatives when export names appear in comments, prose descriptions, or unrelated context rather than as documented API surface. No structured field verification after `publicInterface` schema removal.\n\n**Intentional Duplication:**  \nCode-vs-code validator cannot distinguish factory patterns, parallel implementations, or other intentional duplication without semantic analysis. Caller must scope input to per-directory groups to minimize false positives.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 1282,
      "cacheReadTokens": 16334,
      "cacheCreationTokens": 12359,
      "latencyMs": 34275,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": []
    },
    {
      "timestamp": "2026-02-09T16:12:32.912Z",
      "prompt": "Generate AGENTS.md for directory: \"src/quality/phantom-paths\" (phantom-paths)\n\n## File Summaries (2 files)\n\n### index.ts\n**Purpose:** Re-exports `checkPhantomPaths` validator for phantom path detection in generated documentation.\n\n**Re-exports `checkPhantomPaths` validator for phantom path detection in generated documentation.**\n\n## Exported Interface\n\n- `checkPhantomPaths` — Re-exported from `./validator.js`, function implementing phantom path validation for `AGENTS.md` files\n\n## Module Purpose\n\nBarrel export consolidating phantom path validation functionality. Provides single import point for consumers needing to detect unresolvable file path references embedded in generated documentation.\n\n## Integration Context\n\nPart of quality validation subsystem (`src/quality/`) alongside code-vs-doc consistency checking (`src/quality/inconsistency/code-vs-doc.ts`) and code-vs-code duplicate detection (`src/quality/inconsistency/code-vs-code.ts`). Invoked during post-generation validation phase to identify broken path references that may mislead AI coding assistants.\n\n## Implementation Location\n\nCore validation logic resides in `./validator.ts` which extracts path-like strings from `AGENTS.md` using three regex patterns:\n- Markdown link targets: `/\\[(?:[^\\]]*)\\]\\((\\.[^)]+)\\)/g`\n- Backtick-quoted paths: `` /`((?:src\\/|\\.\\.?\\/)[^`]+\\.[a-z]{1,4})`/g ``\n- Prose-embedded paths: `/(?:from|in|by|via|see)\\s+`?(src\\/[\\w\\-./]+)`?/gi`\n\nResolves paths against AGENTS.md directory and project root with `.ts`/`.js` extension fallback, reporting `PhantomPathInconsistency` for unresolved references.\n### validator.ts\n**Purpose:** validator.ts detects unresolvable path references in AGENTS.md files by extracting path-like strings via regex patter...\n\n**validator.ts detects unresolvable path references in AGENTS.md files by extracting path-like strings via regex patterns, resolving them against both the AGENTS.md directory and project root with .ts/.js fallback logic, and returning PhantomPathInconsistency objects for non-existent paths.**\n\n## Exported Functions\n\n### checkPhantomPaths\n```typescript\nfunction checkPhantomPaths(\n  agentsMdPath: string,\n  content: string,\n  projectRoot: string,\n): PhantomPathInconsistency[]\n```\n\nExtracts path-like references from AGENTS.md content using `PATH_PATTERNS`, resolves each unique path against `agentsMdDir` (via `path.dirname(agentsMdPath)`) and `projectRoot`, verifies existence via `existsSync()`, and returns `PhantomPathInconsistency[]` for unresolved paths with `type: 'phantom-path'`, `severity: 'warning'`, relative `agentsMdPath`, `description`, and `details` object containing `referencedPath`, `resolvedTo` (relative to projectRoot), and `context` (first 120 chars of containing line).\n\n## Path Extraction Patterns\n\n### PATH_PATTERNS\n```typescript\nconst PATH_PATTERNS: RegExp[] = [\n  /\\[(?:[^\\]]*)\\]\\((\\.[^)]+)\\)/g,              // Markdown links: [text](./path)\n  /`((?:src\\/|\\.\\.?\\/)[^`]+\\.[a-z]{1,4})`/g,  // Backtick paths: `src/foo/bar.ts`\n  /(?:from|in|by|via|see)\\s+`?(src\\/[\\w\\-./]+)`?/gi, // Prose paths: \"from src/foo/\"\n]\n```\n\nThree regex patterns capture: (1) markdown link targets starting with `.`, (2) backtick-quoted paths beginning with `src/`, `./`, or `../` with 1-4 letter extensions, (3) prose-embedded `src/` paths following keywords \"from\", \"in\", \"by\", \"via\", or \"see\" (case-insensitive via `i` flag).\n\n### SKIP_PATTERNS\n```typescript\nconst SKIP_PATTERNS: RegExp[] = [\n  /node_modules/,\n  /\\.git\\//,\n  /^https?:/,\n  /\\{\\{/,                  // template placeholders\n  /\\$\\{/,                  // template literals\n  /\\*/,                    // glob patterns\n  /\\{[^}]*,[^}]*\\}/,      // brace expansion: {a,b,c}\n]\n```\n\nExcludes vendor directories (`node_modules`, `.git/`), URLs (`http://`, `https://`), template syntax (`{{`, `${`), glob wildcards (`*`), and brace expansions (`{a,b,c}`) from validation via `SKIP_PATTERNS.some((p) => p.test(rawPath))`.\n\n## Path Resolution Strategy\n\nAttempts resolution in order: (1) `path.resolve(agentsMdDir, rawPath)`, (2) `path.resolve(projectRoot, rawPath)`, (3) `.js` → `.ts` substitution via `fromAgentsMd.replace(/\\.js$/, '.ts')` and `fromRoot.replace(/\\.js$/, '.ts')` for TypeScript import convention support. Uses `tryPaths.some((p) => existsSync(p))` to validate at least one candidate exists.\n\n## Deduplication and Context Extraction\n\nMaintains `seen` Set tracking `rawPath` strings to skip duplicate pattern matches via `seen.has(rawPath)` check before processing. Extracts context line via `content.split('\\n')` and `lines.find((l) => l.includes(rawPath))`, truncating to 120 characters via `contextLine.trim().slice(0, 120)` for inclusion in `PhantomPathInconsistency.details.context`.\n\n## Integration Points\n\nConsumed by `src/quality/phantom-paths/index.ts` as part of quality validation pipeline. Returns `PhantomPathInconsistency` discriminated union member conforming to `src/quality/types.ts` schema with required fields: `type`, `severity`, `agentsMdPath`, `description`, `details`.\n\n## Import Map (verified — use these exact paths)\n\nvalidator.ts:\n  ../types.js → PhantomPathInconsistency (type)\n\n## Project Directory Structure\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>",
      "systemPrompt": "You are generating an AGENTS.md file — a directory-level overview for AI coding assistants.\n\nCRITICAL: Output ONLY the raw markdown content. No code fences, no preamble, no explanations, no conversational text. Your entire response IS the AGENTS.md file content.\n\nOUTPUT FORMAT:\n- First line MUST be exactly: <!-- Generated by agents-reverse-engineer -->\n- Use a # heading with the directory name\n- Write a one-paragraph purpose statement for the directory\nusage\nADAPTIVE SECTIONS:\nAnalyze the directory contents and choose the most relevant sections. Do NOT use a fixed template. Instead, select sections that best document this specific directory for an AI that needs to reconstruct or extend the project.\n\nConsider these section types (choose what applies):\n- **Contents**: Group files by purpose/category under ## headings. For each file: markdown link [filename](./filename) and a one-line description.\n- **Subdirectories**: If subdirectories exist, list them with links [dirname/](./dirname/) and brief summaries.\n- **Architecture / Data Flow**: If files form a pipeline, request/response chain, or layered architecture, document it.\n- **Stack**: If this is a package root (has package.json, Cargo.toml, go.mod, etc.), document the technology stack, key scripts, and entry points.\n- **Structure**: If the directory layout follows a convention (feature-sliced, domain-driven, MVC, etc.), document it.\n- **Patterns**: If files share recurring design patterns (factory, strategy, middleware, barrel re-export), name and document them.\n- **Configuration**: If the directory contains config files, schemas, or environment definitions, document the config surface area.\n- **API Surface**: If the directory exports a public API (barrel index, route definitions, SDK), document the interface contract.\n- **File Relationships**: How files collaborate, depend on each other, or share state.\n- **Behavioral Contracts**: If files contain regex patterns, format specifications, magic constants, or template strings that define observable behavior, collect them in a dedicated section. Preserve verbatim patterns from file summaries — do NOT paraphrase regex into prose. This section is MANDATORY when file summaries contain behavioral artifacts.\n\nChoose any relevant sections or create your own based on the directory contents. The goal is to provide a comprehensive overview that captures the essence of the directory's role in the project and how its files work together, with a focus on what an AI coding assistant would need to know to effectively interact with this code.\n\nSCOPE:\n- AGENTS.md is a NAVIGATIONAL INDEX — help an AI find the right file quickly\n- Focus on: what each file does, how files relate, directory-level patterns\n- Do NOT reproduce full architecture sections — those belong in the root CLAUDE.md\n\nPATH ACCURACY (MANDATORY):\n- When referencing files or modules outside this directory, use ONLY paths from the \"Import Map\" section\n- Do NOT invent, rename, or guess module paths — if a path isn't in the Import Map, don't reference it\n- Use the exact directory names from \"Project Directory Structure\" — do NOT rename directories\n  (e.g., if the directory is called \"cli\", write \"src/cli/\", NOT \"src/commands/\")\n- Cross-module references must use the specifier format from actual import statements\n  (e.g., \"../generation/writers/sum.js\", NOT \"../fs/sum-file.js\")\n- If you are unsure about a path, omit the cross-reference rather than guessing\n\nCONSISTENCY (MANDATORY):\n- Do not contradict yourself within the same document\n- If you describe a technique (e.g., \"regex-based\"), do not call it something else later (e.g., \"AST-based\")\n- When stating version numbers, engines, or config fields, use ONLY values present in the file summaries\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this directory\", \"this module\", \"provides\", \"responsible for\", \"is used to\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Per-file descriptions in Contents sections: 1-2 sentences maximum. Reference key symbols but do not reproduce full summaries.\n- Behavioral contracts (regex patterns, format specs, constants) belong in a separate Behavioral Contracts section, not in per-file descriptions.\n- Subdirectory descriptions: 1-2 sentences maximum. Capture the directory's role, not its full contents.\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- Key exported symbols from file summaries MUST appear in the directory overview\n- Preserve exact casing of identifiers\n\nUSER NOTES:\n- If \"User Notes\" are provided in the prompt, they contain user-defined instructions that will be automatically prepended to your output\n- Do NOT repeat or paraphrase user notes in your generated content — they are included separately\n- You may reference information from user notes for context",
      "response": "<!-- Generated by agents-reverse-engineer -->\n\n# src/quality/phantom-paths\n\nDetects unresolvable path references in `AGENTS.md` files by extracting path-like strings via regex patterns, resolving them against both the `AGENTS.md` directory and project root with `.ts`/`.js` fallback logic, and returning `PhantomPathInconsistency` objects for non-existent paths.\n\n## Contents\n\n### [index.ts](./index.ts)\nBarrel re-export consolidating `checkPhantomPaths` validator for single import point.\n\n### [validator.ts](./validator.ts)\nCore validation logic extracting paths via `PATH_PATTERNS`, resolving with extension fallback, reporting `PhantomPathInconsistency` for unresolved references.\n\n## Exported Interface\n\n- **`checkPhantomPaths(agentsMdPath, content, projectRoot)`** — Validates path references in `AGENTS.md` content, returns `PhantomPathInconsistency[]` with `type: 'phantom-path'`, `severity: 'warning'`, `referencedPath`, `resolvedTo`, and 120-char `context` excerpt\n\n## Path Extraction Patterns\n\nThree regex patterns (`PATH_PATTERNS`) capture references:\n\n```javascript\n/\\[(?:[^\\]]*)\\]\\((\\.[^)]+)\\)/g              // Markdown links: [text](./path)\n/`((?:src\\/|\\.\\.?\\/)[^`]+\\.[a-z]{1,4})`/g  // Backtick paths: `src/foo/bar.ts`\n/(?:from|in|by|via|see)\\s+`?(src\\/[\\w\\-./]+)`?/gi // Prose paths: \"from src/foo/\"\n```\n\n`SKIP_PATTERNS` excludes `node_modules`, `.git/`, URLs (`https?:`), template syntax (`{{`, `${`), glob wildcards (`*`), brace expansions (`{a,b,c}`).\n\n## Resolution Strategy\n\nAttempts resolution in order: (1) relative to `agentsMdDir` via `path.dirname(agentsMdPath)`, (2) relative to `projectRoot`, (3) `.js` → `.ts` substitution for TypeScript import convention. Uses `existsSync()` to validate at least one candidate exists. Deduplicates via `seen` Set tracking `rawPath` strings.\n\n## Integration Context\n\nPart of quality validation subsystem (`src/quality/`) alongside code-vs-doc consistency (`../inconsistency/code-vs-doc.js`) and code-vs-code duplicate detection (`../inconsistency/code-vs-code.js`). Invoked during post-generation validation phase via `src/quality/index.ts` to identify broken path references that may mislead AI coding assistants.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 666,
      "cacheReadTokens": 16334,
      "cacheCreationTokens": 11652,
      "latencyMs": 34520,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": []
    },
    {
      "timestamp": "2026-02-09T16:12:32.920Z",
      "prompt": "Generate AGENTS.md for directory: \"src/ai/telemetry\" (telemetry)\n\n## File Summaries (3 files)\n\n### cleanup.ts\n**Purpose:** cleanupOldLogs() deletes old run log JSON files from `.agents-reverse-engineer/logs/`, retaining only the N most rece...\n\n**cleanupOldLogs() deletes old run log JSON files from `.agents-reverse-engineer/logs/`, retaining only the N most recent files sorted by ISO timestamp filename.**\n\n## Exported Functions\n\n### cleanupOldLogs\n\n```typescript\nasync function cleanupOldLogs(projectRoot: string, keepCount: number): Promise<number>\n```\n\nRemoves old telemetry log files exceeding `keepCount` retention limit. Reads `path.join(projectRoot, '.agents-reverse-engineer/logs')`, filters entries matching pattern `run-*.json` via `name.startsWith('run-') && name.endsWith('.json')`, sorts lexicographically via `entries.sort()` then reverses to newest-first ordering, slices beyond `keepCount` into `toDelete[]`, and unlinks each via `fs.unlink()`. Returns count of deleted files. Returns `0` without error if logs directory does not exist (catches `ENOENT` errno).\n\n## Constants\n\n- `LOGS_DIR`: String literal `'.agents-reverse-engineer/logs'` defining telemetry log directory relative to project root.\n\n## Error Handling\n\nENOENT errors from `fs.readdir()` are caught and return `0` (graceful handling when logs directory missing). All other errors propagate to caller.\n\n## Integration Points\n\nCalled by `TelemetryLogger.finalize()` in `src/ai/telemetry/logger.ts` after writing run log JSON, using `config.ai.telemetry.keepRuns` as retention limit (default 50). Complements trace cleanup in `src/orchestration/trace.ts` which retains 500 NDJSON trace files.\n\n## Sorting Strategy\n\nLexicographic sort on ISO 8601 timestamp filenames (format `run-<timestamp>.json`) produces correct chronological ordering without parsing timestamps. Reversal via `entries.reverse()` yields newest-first sequence, enabling slice-based retention via `entries.slice(keepCount)`.\n### logger.ts\n**Purpose:** TelemetryLogger accumulates per-call AI service telemetry entries in memory during CLI run execution and computes agg...\n\n**TelemetryLogger accumulates per-call AI service telemetry entries in memory during CLI run execution and computes aggregate summary statistics for serialization to RunLog.**\n\n## Exported Class\n\n`TelemetryLogger` — in-memory telemetry accumulator created once per CLI invocation.\n\n**Constructor:**\n```typescript\nconstructor(runId: string)\n```\nAccepts `runId` (typically ISO timestamp string), captures `startTime` as `new Date().toISOString()`, initializes empty `entries: TelemetryEntry[]` array.\n\n**Read-only Properties:**\n- `runId: string` — unique identifier for this run\n- `startTime: string` — ISO 8601 timestamp when run started\n\n## Core Methods\n\n`addEntry(entry: TelemetryEntry): void` — appends telemetry entry to in-memory `entries` array. Called by AI service after each subprocess completion.\n\n`getEntries(): readonly TelemetryEntry[]` — returns immutable view of accumulated entries.\n\n`setFilesReadOnLastEntry(filesRead: FileRead[]): void` — mutates `filesRead` property of most recent entry. Called by AI service after command runner attaches file metadata. No-op if `entries.length === 0`.\n\n## Summary Computation\n\n`getSummary(): RunLog['summary']` — computes aggregate statistics from all entries on every invocation (not cached). Iterates `entries` array to sum:\n- `totalInputTokens`, `totalOutputTokens`, `totalCacheReadTokens`, `totalCacheCreationTokens` (token counts from `TelemetryEntry` fields)\n- `totalDurationMs` (sum of `latencyMs` values)\n- `errorCount` (count of entries where `error !== undefined`)\n- `totalFilesRead` (sum of `filesRead.length` across entries)\n- `uniqueFilesRead` (size of `Set<string>` populated with `file.path` from all `filesRead` arrays)\n\nReturns object with `totalCalls: this.entries.length` plus computed totals.\n\n## Run Finalization\n\n`toRunLog(): RunLog` — assembles complete `RunLog` structure for serialization. Sets `endTime` to `new Date().toISOString()`, spreads `entries` array, invokes `getSummary()`. Called once when CLI run completes before writing to `.agents-reverse-engineer/logs/run-<timestamp>.json`.\n\n## Type Dependencies\n\nImports `TelemetryEntry`, `RunLog`, `FileRead` from `../types.js`. `TelemetryEntry` contains per-call metrics: `inputTokens`, `outputTokens`, `cacheReadTokens`, `cacheCreationTokens`, `latencyMs`, `error?: string`, `filesRead: FileRead[]`. `RunLog` structure includes `runId`, `startTime`, `endTime`, `entries`, `summary`. `FileRead` contains `path`, `sizeBytes`, `linesRead` metadata.\n\n## Integration Point\n\nTelemetryLogger instance passed to AIService via constructor, threaded through `CommandRunOptions.telemetryLogger`. AIService calls `addEntry()` after each subprocess completion, `setFilesReadOnLastEntry()` after file metadata attachment, `toRunLog()` before serialization via `src/ai/telemetry/run-log.ts` writer.\n### run-log.ts\n**Purpose:** Writes completed `RunLog` objects to disk as timestamped JSON files in `.agents-reverse-engineer/logs/` with director...\n\n**Writes completed `RunLog` objects to disk as timestamped JSON files in `.agents-reverse-engineer/logs/` with directory creation and filename sanitization.**\n\n## Exported Functions\n\n### writeRunLog\n\n```typescript\nasync function writeRunLog(projectRoot: string, runLog: RunLog): Promise<string>\n```\n\nWrites a completed `RunLog` to disk as pretty-printed JSON (2-space indentation). Creates `<projectRoot>/.agents-reverse-engineer/logs/` directory via `fs.mkdir()` with `recursive: true` if absent. Derives filename from `runLog.startTime` by replacing `:` and `.` characters with `-` to produce cross-platform-safe filenames, yielding pattern `run-<ISO8601-sanitized>.json` (e.g., `run-2026-02-07T12-00-00-000Z.json`). Returns absolute path to written file.\n\n## Constants\n\n### LOGS_DIR\n\n```typescript\nconst LOGS_DIR = '.agents-reverse-engineer/logs'\n```\n\nRelative directory path for telemetry log storage. Combined with `projectRoot` parameter to form absolute log destination via `path.join()`.\n\n## Dependencies\n\n- `node:fs/promises`: `mkdir()`, `writeFile()` for async filesystem operations\n- `node:path`: `join()` for cross-platform path construction\n- `RunLog` type from `../types.js`: Structured run metadata with `startTime` (ISO 8601 string), token counts, costs, durations, errors, file read metadata\n\n## Integration Points\n\nCalled by `TelemetryLogger.finalize()` in `src/ai/telemetry/logger.ts` after aggregating per-call metrics into cumulative `RunLog` structure. Works in tandem with `cleanupOldLogs()` from `src/ai/telemetry/cleanup.ts` for retention management (keeps N most recent logs based on `config.ai.telemetry.keepRuns`).\n\n## Filename Sanitization Pattern\n\nApplies `runLog.startTime.replace(/[:.]/g, '-')` to transform ISO 8601 timestamps like `2026-02-07T12:00:00.000Z` into filesystem-safe `2026-02-07T12-00-00-000Z`, avoiding Windows path restrictions and shell escaping issues.\n\n## Import Map (verified — use these exact paths)\n\nlogger.ts:\n  ../types.js → TelemetryEntry, RunLog, FileRead (type)\n\nrun-log.ts:\n  ../types.js → RunLog (type)\n\n## Project Directory Structure\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>",
      "systemPrompt": "You are generating an AGENTS.md file — a directory-level overview for AI coding assistants.\n\nCRITICAL: Output ONLY the raw markdown content. No code fences, no preamble, no explanations, no conversational text. Your entire response IS the AGENTS.md file content.\n\nOUTPUT FORMAT:\n- First line MUST be exactly: <!-- Generated by agents-reverse-engineer -->\n- Use a # heading with the directory name\n- Write a one-paragraph purpose statement for the directory\nusage\nADAPTIVE SECTIONS:\nAnalyze the directory contents and choose the most relevant sections. Do NOT use a fixed template. Instead, select sections that best document this specific directory for an AI that needs to reconstruct or extend the project.\n\nConsider these section types (choose what applies):\n- **Contents**: Group files by purpose/category under ## headings. For each file: markdown link [filename](./filename) and a one-line description.\n- **Subdirectories**: If subdirectories exist, list them with links [dirname/](./dirname/) and brief summaries.\n- **Architecture / Data Flow**: If files form a pipeline, request/response chain, or layered architecture, document it.\n- **Stack**: If this is a package root (has package.json, Cargo.toml, go.mod, etc.), document the technology stack, key scripts, and entry points.\n- **Structure**: If the directory layout follows a convention (feature-sliced, domain-driven, MVC, etc.), document it.\n- **Patterns**: If files share recurring design patterns (factory, strategy, middleware, barrel re-export), name and document them.\n- **Configuration**: If the directory contains config files, schemas, or environment definitions, document the config surface area.\n- **API Surface**: If the directory exports a public API (barrel index, route definitions, SDK), document the interface contract.\n- **File Relationships**: How files collaborate, depend on each other, or share state.\n- **Behavioral Contracts**: If files contain regex patterns, format specifications, magic constants, or template strings that define observable behavior, collect them in a dedicated section. Preserve verbatim patterns from file summaries — do NOT paraphrase regex into prose. This section is MANDATORY when file summaries contain behavioral artifacts.\n\nChoose any relevant sections or create your own based on the directory contents. The goal is to provide a comprehensive overview that captures the essence of the directory's role in the project and how its files work together, with a focus on what an AI coding assistant would need to know to effectively interact with this code.\n\nSCOPE:\n- AGENTS.md is a NAVIGATIONAL INDEX — help an AI find the right file quickly\n- Focus on: what each file does, how files relate, directory-level patterns\n- Do NOT reproduce full architecture sections — those belong in the root CLAUDE.md\n\nPATH ACCURACY (MANDATORY):\n- When referencing files or modules outside this directory, use ONLY paths from the \"Import Map\" section\n- Do NOT invent, rename, or guess module paths — if a path isn't in the Import Map, don't reference it\n- Use the exact directory names from \"Project Directory Structure\" — do NOT rename directories\n  (e.g., if the directory is called \"cli\", write \"src/cli/\", NOT \"src/commands/\")\n- Cross-module references must use the specifier format from actual import statements\n  (e.g., \"../generation/writers/sum.js\", NOT \"../fs/sum-file.js\")\n- If you are unsure about a path, omit the cross-reference rather than guessing\n\nCONSISTENCY (MANDATORY):\n- Do not contradict yourself within the same document\n- If you describe a technique (e.g., \"regex-based\"), do not call it something else later (e.g., \"AST-based\")\n- When stating version numbers, engines, or config fields, use ONLY values present in the file summaries\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this directory\", \"this module\", \"provides\", \"responsible for\", \"is used to\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Per-file descriptions in Contents sections: 1-2 sentences maximum. Reference key symbols but do not reproduce full summaries.\n- Behavioral contracts (regex patterns, format specs, constants) belong in a separate Behavioral Contracts section, not in per-file descriptions.\n- Subdirectory descriptions: 1-2 sentences maximum. Capture the directory's role, not its full contents.\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- Key exported symbols from file summaries MUST appear in the directory overview\n- Preserve exact casing of identifiers\n\nUSER NOTES:\n- If \"User Notes\" are provided in the prompt, they contain user-defined instructions that will be automatically prepended to your output\n- Do NOT repeat or paraphrase user notes in your generated content — they are included separately\n- You may reference information from user notes for context",
      "response": "<!-- Generated by agents-reverse-engineer -->\n\n# src/ai/telemetry\n\nPersistent telemetry accumulation and retention management: in-memory RunLog assembly via TelemetryLogger, JSON serialization with sanitized ISO 8601 filenames, and log rotation enforcing configurable retention limits.\n\n## Contents\n\n**[cleanup.ts](./cleanup.ts)** — `cleanupOldLogs(projectRoot, keepCount)` deletes old run log files from `.agents-reverse-engineer/logs/`, retaining N most recent via lexicographic sort on ISO 8601 filenames, handles ENOENT gracefully.\n\n**[logger.ts](./logger.ts)** — `TelemetryLogger` class accumulates per-call `TelemetryEntry` objects in memory, computes aggregate summary statistics (total tokens/costs/errors, unique files read), exports `toRunLog()` for serialization.\n\n**[run-log.ts](./run-log.ts)** — `writeRunLog(projectRoot, runLog)` writes RunLog JSON to `.agents-reverse-engineer/logs/run-<timestamp>.json` with directory creation and filename sanitization (`/[:.]/g` → `-`).\n\n## Architecture\n\n**Single-Run Lifecycle:**\n1. CLI invokes `new TelemetryLogger(runId)` with ISO timestamp, captures `startTime`\n2. AIService calls `addEntry(entry)` after each subprocess completion, `setFilesReadOnLastEntry(filesRead)` after file metadata attachment\n3. CLI invokes `toRunLog()` to freeze in-memory state into RunLog structure with computed summary\n4. `writeRunLog()` persists JSON to disk with 2-space indentation\n5. `cleanupOldLogs(projectRoot, config.ai.telemetry.keepRuns)` removes oldest files beyond retention limit\n\n**Cross-Run Retention:**\n- Log filenames encode sortable timestamps via `run-2026-02-07T12-00-00-000Z.json` pattern\n- Lexicographic sort produces correct chronological ordering without timestamp parsing\n- Retention enforced via `entries.slice(keepCount)` after newest-first reversal\n\n## Data Flow\n\n```\nAIService subprocess\n  ↓ (per-call metrics)\nTelemetryLogger.addEntry()\n  ↓ (in-memory accumulation)\nTelemetryLogger.toRunLog()\n  ↓ (RunLog structure with summary)\nwriteRunLog()\n  ↓ (filesystem persistence)\n.agents-reverse-engineer/logs/run-<timestamp>.json\n  ↓ (retention enforcement)\ncleanupOldLogs() removes oldest files\n```\n\n## RunLog Summary Computation\n\n`TelemetryLogger.getSummary()` iterates all `entries[]` to compute:\n- `totalInputTokens`, `totalOutputTokens`, `totalCacheReadTokens`, `totalCacheCreationTokens` (sum of per-call counts)\n- `totalDurationMs` (sum of `latencyMs` values)\n- `errorCount` (count where `error !== undefined`)\n- `totalFilesRead` (sum of `filesRead.length` across entries)\n- `uniqueFilesRead` (cardinality of `Set<string>` from all `file.path` values)\n\nRecomputes on every invocation without caching (safe because typically called once at run end).\n\n## Filesystem Safety\n\n**Filename Sanitization:**\n- ISO 8601 contains `:` (Windows path separator) and `.` (shell metacharacter)\n- `runLog.startTime.replace(/[:.]/g, '-')` produces cross-platform-safe filenames\n- Example: `2026-02-07T12:00:00.000Z` → `run-2026-02-07T12-00-00-000Z.json`\n\n**Directory Creation:**\n- `fs.mkdir()` with `recursive: true` creates `.agents-reverse-engineer/logs/` if absent\n- `cleanupOldLogs()` catches ENOENT from `fs.readdir()` and returns `0` when logs directory missing\n\n## Integration Points\n\n- **TelemetryLogger** threaded via `CommandRunOptions.telemetryLogger` → AIService constructor → per-call hooks\n- **writeRunLog** called by command runners after run completion when `config.ai.telemetry.enabled === true`\n- **cleanupOldLogs** invoked immediately after `writeRunLog()` with `config.ai.telemetry.keepRuns` (default 50)\n- Complements `src/orchestration/trace.ts` which retains 500 NDJSON trace files via similar lexicographic sort + slice pattern\n\n## Type Dependencies\n\nAll modules import from `../types.js`:\n- `TelemetryEntry`: per-call metrics with `inputTokens`, `outputTokens`, `cacheReadTokens`, `cacheCreationTokens`, `latencyMs`, `error?: string`, `filesRead: FileRead[]`\n- `RunLog`: container with `runId`, `startTime`, `endTime`, `entries: TelemetryEntry[]`, `summary` (aggregate statistics)\n- `FileRead`: file metadata with `path`, `sizeBytes`, `linesRead`\n\n## Behavioral Contracts\n\n**Filename Pattern:**\n```regex\n^run-\\d{4}-\\d{2}-\\d{2}T\\d{2}-\\d{2}-\\d{2}-\\d{3}Z\\.json$\n```\nMatches sanitized ISO 8601 timestamps where `:` and `.` replaced with `-`.\n\n**Log File Filtering:**\n```typescript\nname.startsWith('run-') && name.endsWith('.json')\n```\nIsolates run log files from other potential artifacts in `.agents-reverse-engineer/logs/`.\n\n**Timestamp Sanitization Transform:**\n```javascript\nrunLog.startTime.replace(/[:.]/g, '-')\n```\nConverts ISO 8601 string to filesystem-safe filename component.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 1427,
      "cacheReadTokens": 16334,
      "cacheCreationTokens": 12104,
      "latencyMs": 36457,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": []
    },
    {
      "timestamp": "2026-02-09T16:12:32.925Z",
      "prompt": "Generate AGENTS.md for directory: \"src/generation/writers\" (writers)\n\n## File Summaries (3 files)\n\n### agents-md.ts\n**Purpose:** agents-md.ts manages `AGENTS.md` file lifecycle during directory aggregation, preserving user-authored content by ren...\n\n**agents-md.ts manages `AGENTS.md` file lifecycle during directory aggregation, preserving user-authored content by renaming existing files to `AGENTS.local.md` and prepending them above generated documentation with marker-based ownership detection.**\n\n## Exported Functions\n\n**writeAgentsMd(dirPath: string, _projectRoot: string, content: string): Promise<string>**\nWrites `AGENTS.md` for a directory with LLM-generated content, returns path to written file. Executes four-step preservation workflow: (1) reads existing `AGENTS.md` at `dirPath`, checks for `GENERATED_MARKER` absence to detect user authorship, renames to `AGENTS.local.md` via `rename()` if user-authored; (2) if no user content found in step 1, attempts to read already-renamed `AGENTS.local.md` from previous run; (3) strips `GENERATED_MARKER` prefix from incoming `content` parameter via `slice()` + regex `/^\\n+/` to normalize LLM output; (4) constructs final content with structure: `GENERATED_MARKER` → newline → optional user content block with `<!-- User-defined AGENTS.md preserved as AGENTS.local.md -->` header + `---` separator → LLM content, writes to `AGENTS.md` via `writeFile()` with `mkdir()` ensuring parent directory exists. Parameter `_projectRoot` unused (underscore-prefixed per linter convention). Called by Phase 2 orchestrator (`src/generation/orchestrator.ts`) during post-order directory aggregation after child `.sum` files processed.\n\n**isGeneratedAgentsMd(filePath: string): Promise<boolean>**\nDetects whether `AGENTS.md` at `filePath` was generated by ARE via substring search for `GENERATED_MARKER` in file content read via `readFile()`. Returns `false` on read errors (file not found, permission denied) via empty catch block. Used by `writeAgentsMd()` to determine preservation logic and by cleanup commands (`src/cli/clean.ts`) to distinguish generated vs. user-authored files for selective deletion.\n\n## Marker Constant\n\n**GENERATED_MARKER: string**\nHTML comment string `'<!-- Generated by agents-reverse-engineer -->'` injected at top of all generated `AGENTS.md` files. Serves as ownership identifier for idempotent regeneration: presence indicates generated content safe to overwrite, absence indicates user authorship requiring preservation via `AGENTS.local.md` rename. Pattern avoids YAML frontmatter to maintain Markdown readability and compatibility with viewers that don't parse frontmatter.\n\n## User Content Preservation Strategy\n\nUser-authored `AGENTS.md` files (lacking `GENERATED_MARKER`) renamed to `AGENTS.local.md` on first `writeAgentsMd()` invocation. Subsequent runs read `AGENTS.local.md` directly without re-checking original path, preventing repeated file system operations. Preserved content prepended above generated content with visual separator (`---`) ensuring AI assistants encounter project-specific context before auto-generated summaries. If `AGENTS.local.md` exists from previous run and user deletes/modifies `AGENTS.md` in interim, the `AGENTS.local.md` version takes precedence (step 2 fallback logic).\n\n## Output Structure\n\nFinal `AGENTS.md` format:\n```\n<!-- Generated by agents-reverse-engineer -->\n\n<!-- User-defined AGENTS.md preserved as AGENTS.local.md -->\n\n[user content verbatim with leading/trailing whitespace trimmed]\n\n---\n\n[LLM-generated content with GENERATED_MARKER prefix stripped]\n```\n\nIf no user content exists, omits comment header and separator, outputs:\n```\n<!-- Generated by agents-reverse-engineer -->\n\n[LLM-generated content]\n```\n\n## Integration Points\n\n- **src/generation/orchestrator.ts**: Calls `writeAgentsMd()` in Phase 2 (`runDirectoryGeneration`) after aggregating child `.sum` files and subdirectory `AGENTS.md` files via `collectAgentsDocs()`, passes `dirPath` from sorted directory list and `content` from `AIService.call()` response\n- **src/cli/clean.ts**: Calls `isGeneratedAgentsMd()` to filter deletion targets, skips user-authored files, restores `AGENTS.local.md` → `AGENTS.md` after removing generated version\n- **src/update/orphan-cleaner.ts**: Uses `isGeneratedAgentsMd()` in `cleanupEmptyDirectoryDocs()` to determine whether to delete `AGENTS.md` from directories with no remaining source files\n\n## Error Handling\n\nBoth functions use empty catch blocks with fallback behavior: `isGeneratedAgentsMd()` returns `false` on read errors (treats unreadable files as non-generated), `writeAgentsMd()` treats read errors as \"no existing file\" and proceeds with writing marker + LLM content only. No explicit error logging since failures indicate normal conditions (file not found on first run). Parent orchestrator (`src/generation/executor.ts`) captures write errors during task execution and reports via progress logger.\n### index.ts\n**Purpose:** index.ts re-exports writer utilities for .sum files and AGENTS.md generation.\n\n**index.ts re-exports writer utilities for .sum files and AGENTS.md generation.**\n\n## Exported Symbols\n\n**From `./sum.js`:**\n- `writeSumFile` — Writes .sum files with YAML frontmatter containing `generated_at`, `content_hash`, `purpose`, `critical_todos`, `related_files`\n- `readSumFile` — Parses existing .sum files, extracts YAML frontmatter and markdown body\n- `getSumPath` — Computes .sum file path from source file path\n- `sumFileExists` — Checks if .sum file exists for given source file\n- `SumFileContent` — Type interface for .sum file structure with frontmatter fields and markdown content\n\n**From `./agents-md.js`:**\n- `writeAgentsMd` — Writes directory-level AGENTS.md files from aggregated child .sum content and subdirectory AGENTS.md files during Phase 2 post-order traversal\n\n## Module Role\n\nindex.ts serves as public API boundary for `src/generation/writers/` directory. Consumers import writer functions via `import { writeSumFile, readSumFile, writeAgentsMd } from './generation/writers/index.js'` without accessing implementation modules directly. Central to Phase 1 file analysis (writeSumFile) and Phase 2 directory aggregation (writeAgentsMd) in three-phase pipeline. SumFileContent type defines contract for frontmatter schema including SHA-256 content_hash used by incremental update system.\n### sum.ts\n**Purpose:** sum.ts manages `.sum` file I/O with YAML frontmatter serialization, parsing, and path resolution for per-file documen...\n\n**sum.ts manages `.sum` file I/O with YAML frontmatter serialization, parsing, and path resolution for per-file documentation artifacts.**\n\n## Exported Types\n\n**SumFileContent** interface structures `.sum` file data with `summary: string` (main documentation text), `metadata: SummaryMetadata` (purpose/todos/related files), `generatedAt: string` (ISO 8601 timestamp), `contentHash: string` (SHA-256 hex digest for change detection).\n\n## File Format\n\n`.sum` files use YAML frontmatter delimited by `---` markers containing:\n- `generated_at: <ISO8601>` — timestamp\n- `content_hash: <sha256-hex>` — SHA-256 digest\n- `purpose: <single-line-string>` — one-line purpose statement\n- `critical_todos: [...]` or multi-line list — optional security/breaking issues\n- `related_files: [...]` or multi-line list — optional dependency paths\n\nFrontmatter ends with `---\\n` followed by markdown summary body.\n\n## Core Functions\n\n**writeSumFile(sourcePath: string, content: SumFileContent): Promise<string>** creates `.sum` file at `${sourcePath}.sum`, calls `mkdir(dir, {recursive: true})` to ensure parent directory exists, invokes `formatSumFile()` for serialization, returns written path.\n\n**readSumFile(sumPath: string): Promise<SumFileContent | null>** reads file via `readFile(sumPath, 'utf-8')`, delegates to `parseSumFile()`, returns `null` on missing file or parse failure.\n\n**getSumPath(sourcePath: string): string** returns `${sourcePath}.sum` path for given source file.\n\n**sumFileExists(sourcePath: string): Promise<boolean>** calls `readSumFile(getSumPath(sourcePath))` and returns `content !== null`.\n\n## Parsing Logic\n\n**parseSumFile(content: string): SumFileContent | null** extracts frontmatter via `/^---\\n([\\s\\S]*?)\\n---\\n/` regex, parses fields using `String.match()` with patterns:\n- `generated_at:\\s*(.+)` captures ISO timestamp\n- `content_hash:\\s*(.+)` captures SHA-256 hash\n- `purpose:\\s*(.+)` captures single-line purpose\n\nCalls `parseYamlArray(frontmatter, key)` for `critical_todos` and `related_files`, returns `null` on exception or missing frontmatter delimiter.\n\n**parseYamlArray(frontmatter: string, key: string): string[]** supports two formats:\n1. Inline: `key: [a, b, c]` matched via `/key:\\s*\\[([^\\]]*)\\]/`, splits on commas, trims quotes via `.replace(/^[\"']|[\"']$/g, '')`\n2. Multi-line: `key:\\n  - item1\\n  - item2` matched via `/key:\\s*\\n((?:\\s+-\\s+.+\\n?)+)/m`, extracts lines, strips `^\\s*-\\s*` prefix\n\nReturns empty array if neither pattern matches.\n\n## Serialization Logic\n\n**formatSumFile(content: SumFileContent): string** constructs frontmatter lines array with mandatory fields `generated_at`, `content_hash`, `purpose`, conditionally appends `formatYamlArray()` output for `critical_todos` and `related_files` if arrays non-empty, joins with `\\n`, appends delimiter `---\\n`, concatenates with `content.summary`.\n\n**formatYamlArray(key: string, values: string[]): string** chooses format by heuristics:\n- Empty: `key: []`\n- Inline: `key: [${values.join(', ')}]` if `values.length <= 3` and all items `< 40` chars\n- Multi-line: `key:\\n${values.map(v => '  - ' + v).join('\\n')}` otherwise\n\n## Integration Points\n\nUsed by `src/update/orphan-cleaner.ts` via `readSumFile()` for `content_hash` extraction during incremental updates. Called by `src/generation/executor.ts` Phase 1 workers via `writeSumFile()` after AI subprocess returns summary. Consumed by Phase 2 directory aggregation via `readSumFile()` to load child `.sum` files for `AGENTS.md` synthesis prompts.\n\n## Dependencies\n\nImports `writeFile`, `readFile`, `mkdir` from `node:fs/promises`, `path` from `node:path`, `SummaryMetadata` from `../types.js` (defines `purpose`, `criticalTodos?`, `relatedFiles?` fields).\n\n## Import Map (verified — use these exact paths)\n\nsum.ts:\n  ../types.js → SummaryMetadata (type)\n\n## Project Directory Structure\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>",
      "systemPrompt": "You are generating an AGENTS.md file — a directory-level overview for AI coding assistants.\n\nCRITICAL: Output ONLY the raw markdown content. No code fences, no preamble, no explanations, no conversational text. Your entire response IS the AGENTS.md file content.\n\nOUTPUT FORMAT:\n- First line MUST be exactly: <!-- Generated by agents-reverse-engineer -->\n- Use a # heading with the directory name\n- Write a one-paragraph purpose statement for the directory\nusage\nADAPTIVE SECTIONS:\nAnalyze the directory contents and choose the most relevant sections. Do NOT use a fixed template. Instead, select sections that best document this specific directory for an AI that needs to reconstruct or extend the project.\n\nConsider these section types (choose what applies):\n- **Contents**: Group files by purpose/category under ## headings. For each file: markdown link [filename](./filename) and a one-line description.\n- **Subdirectories**: If subdirectories exist, list them with links [dirname/](./dirname/) and brief summaries.\n- **Architecture / Data Flow**: If files form a pipeline, request/response chain, or layered architecture, document it.\n- **Stack**: If this is a package root (has package.json, Cargo.toml, go.mod, etc.), document the technology stack, key scripts, and entry points.\n- **Structure**: If the directory layout follows a convention (feature-sliced, domain-driven, MVC, etc.), document it.\n- **Patterns**: If files share recurring design patterns (factory, strategy, middleware, barrel re-export), name and document them.\n- **Configuration**: If the directory contains config files, schemas, or environment definitions, document the config surface area.\n- **API Surface**: If the directory exports a public API (barrel index, route definitions, SDK), document the interface contract.\n- **File Relationships**: How files collaborate, depend on each other, or share state.\n- **Behavioral Contracts**: If files contain regex patterns, format specifications, magic constants, or template strings that define observable behavior, collect them in a dedicated section. Preserve verbatim patterns from file summaries — do NOT paraphrase regex into prose. This section is MANDATORY when file summaries contain behavioral artifacts.\n\nChoose any relevant sections or create your own based on the directory contents. The goal is to provide a comprehensive overview that captures the essence of the directory's role in the project and how its files work together, with a focus on what an AI coding assistant would need to know to effectively interact with this code.\n\nSCOPE:\n- AGENTS.md is a NAVIGATIONAL INDEX — help an AI find the right file quickly\n- Focus on: what each file does, how files relate, directory-level patterns\n- Do NOT reproduce full architecture sections — those belong in the root CLAUDE.md\n\nPATH ACCURACY (MANDATORY):\n- When referencing files or modules outside this directory, use ONLY paths from the \"Import Map\" section\n- Do NOT invent, rename, or guess module paths — if a path isn't in the Import Map, don't reference it\n- Use the exact directory names from \"Project Directory Structure\" — do NOT rename directories\n  (e.g., if the directory is called \"cli\", write \"src/cli/\", NOT \"src/commands/\")\n- Cross-module references must use the specifier format from actual import statements\n  (e.g., \"../generation/writers/sum.js\", NOT \"../fs/sum-file.js\")\n- If you are unsure about a path, omit the cross-reference rather than guessing\n\nCONSISTENCY (MANDATORY):\n- Do not contradict yourself within the same document\n- If you describe a technique (e.g., \"regex-based\"), do not call it something else later (e.g., \"AST-based\")\n- When stating version numbers, engines, or config fields, use ONLY values present in the file summaries\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this directory\", \"this module\", \"provides\", \"responsible for\", \"is used to\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Per-file descriptions in Contents sections: 1-2 sentences maximum. Reference key symbols but do not reproduce full summaries.\n- Behavioral contracts (regex patterns, format specs, constants) belong in a separate Behavioral Contracts section, not in per-file descriptions.\n- Subdirectory descriptions: 1-2 sentences maximum. Capture the directory's role, not its full contents.\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- Key exported symbols from file summaries MUST appear in the directory overview\n- Preserve exact casing of identifiers\n\nUSER NOTES:\n- If \"User Notes\" are provided in the prompt, they contain user-defined instructions that will be automatically prepended to your output\n- Do NOT repeat or paraphrase user notes in your generated content — they are included separately\n- You may reference information from user notes for context",
      "response": "<!-- Generated by agents-reverse-engineer -->\n\n# src/generation/writers\n\n`.sum` file I/O with YAML frontmatter serialization, `AGENTS.md` lifecycle management with user content preservation, and path resolution utilities for Phase 1/Phase 2 writer operations.\n\n## Contents\n\n**[sum.ts](./sum.ts)** — Serializes `SumFileContent` to `.sum` files with YAML frontmatter (`generated_at`, `content_hash`, `purpose`, `critical_todos`, `related_files`), parses frontmatter via regex extraction of inline/multi-line arrays, computes `.sum` paths via `getSumPath()`, validates existence via `sumFileExists()`.\n\n**[agents-md.ts](./agents-md.ts)** — Writes directory-level `AGENTS.md` with `GENERATED_MARKER` ownership detection, preserves user-authored content by renaming to `AGENTS.local.md` and prepending above LLM output, strips marker prefix from incoming content, constructs final structure with optional user content block and `---` separator.\n\n**[index.ts](./index.ts)** — Re-exports `writeSumFile`, `readSumFile`, `getSumPath`, `sumFileExists`, `SumFileContent` from `sum.ts` and `writeAgentsMd` from `agents-md.ts` as public API boundary.\n\n## YAML Frontmatter Format\n\nsum.ts serializes frontmatter with adaptive array formatting: inline `[a, b, c]` for ≤3 items under 40 chars each, multi-line `- item` otherwise. Parses via dual-pattern regex supporting both syntaxes. Mandatory fields: `generated_at` (ISO 8601), `content_hash` (SHA-256 hex), `purpose` (single-line string). Optional arrays: `critical_todos`, `related_files`.\n\n## User Content Preservation\n\nagents-md.ts implements idempotent preservation via four-step workflow:\n\n1. Read existing `AGENTS.md`, detect user authorship via `GENERATED_MARKER` absence, rename to `AGENTS.local.md` if user-authored\n2. Attempt read of already-renamed `AGENTS.local.md` from previous run if step 1 found no user content\n3. Strip `GENERATED_MARKER` prefix from incoming LLM content via `slice()` + `/^\\n+/` normalization\n4. Construct final content: `GENERATED_MARKER` → optional user block with `<!-- User-defined AGENTS.md preserved as AGENTS.local.md -->` header and `---` separator → LLM content, write via `writeFile()` with `mkdir({recursive: true})`\n\nPreserved content prepended above generated content ensures AI assistants encounter project-specific context before auto-generated summaries.\n\n## Path Resolution\n\n`getSumPath(sourcePath)` returns `${sourcePath}.sum` for given source file. No directory transformation — `.sum` files colocate with source files throughout tree. `sumFileExists()` delegates to `readSumFile()` non-null check, avoiding redundant `access()` syscalls.\n\n## Integration with Pipeline Phases\n\n**Phase 1 (File Analysis):** `src/generation/executor.ts` workers call `writeSumFile()` after AI subprocess returns `SumFileContent`, writes to path from `getSumPath(sourceFile.path)`.\n\n**Phase 2 (Directory Aggregation):** `src/generation/orchestrator.ts` calls `readSumFile()` via `collectAgentsDocs()` to load child `.sum` files for prompt construction, calls `writeAgentsMd()` with LLM response from `AIService.call()`.\n\n**Incremental Updates:** `src/update/orphan-cleaner.ts` calls `readSumFile()` to extract `content_hash` from YAML frontmatter for SHA-256 comparison, `cleanupEmptyDirectoryDocs()` uses `isGeneratedAgentsMd()` to determine deletion eligibility.\n\n**Cleanup:** `src/cli/clean.ts` calls `isGeneratedAgentsMd()` to filter deletion targets, skips user-authored files, restores `AGENTS.local.md` → `AGENTS.md` after removing generated version.\n\n## Behavioral Contracts\n\n**GENERATED_MARKER constant:**\n```javascript\n'<!-- Generated by agents-reverse-engineer -->'\n```\n\n**Frontmatter extraction regex (parseSumFile):**\n```javascript\n/^---\\n([\\s\\S]*?)\\n---\\n/\n```\n\n**Field patterns:**\n```javascript\n/generated_at:\\s*(.+)/\n/content_hash:\\s*(.+)/\n/purpose:\\s*(.+)/\n```\n\n**Array parsing patterns (parseYamlArray):**\n```javascript\n/key:\\s*\\[([^\\]]*)\\]/  // Inline: key: [a, b, c]\n/key:\\s*\\n((?:\\s+-\\s+.+\\n?)+)/m  // Multi-line: key:\\n  - item\n```\n\n**Quote stripping (inline arrays):**\n```javascript\n.replace(/^[\"']|[\"']$/g, '')\n```\n\n**Multi-line item prefix:**\n```javascript\n/^\\s*-\\s*/  // Strips leading '  - '\n```\n\n**Array format heuristic (formatYamlArray):**\n- Inline if `values.length <= 3` and all items `< 40` chars\n- Multi-line otherwise\n- Empty: `key: []`\n\n**Newline normalization (writeAgentsMd):**\n```javascript\ncontent.slice(GENERATED_MARKER.length).replace(/^\\n+/, '')\n```",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 1350,
      "cacheReadTokens": 16334,
      "cacheCreationTokens": 12967,
      "latencyMs": 44761,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": []
    },
    {
      "timestamp": "2026-02-09T16:12:32.916Z",
      "prompt": "Generate AGENTS.md for directory: \"src/ai/backends\" (backends)\n\n## File Summaries (3 files)\n\n### claude.ts\n**Purpose:** ClaudeBackend implements AIBackend interface for Claude Code CLI (`claude`), handling CLI argument construction, JSON...\n\n**ClaudeBackend implements AIBackend interface for Claude Code CLI (`claude`), handling CLI argument construction, JSON response parsing with Zod validation, PATH availability detection, and install instructions.**\n\n## Exported Symbols\n\n**ClaudeBackend** class implements `AIBackend` interface with properties:\n- `name: 'claude'` — Backend identifier\n- `cliCommand: 'claude'` — CLI executable name\n\nMethods:\n- `async isAvailable(): Promise<boolean>` — Delegates to `isCommandOnPath('claude')` for PATH detection\n- `buildArgs(options: AICallOptions): string[]` — Constructs CLI arguments array for `claude` command\n- `parseResponse(stdout: string, durationMs: number, exitCode: number): AIResponse` — Parses Claude CLI JSON output into normalized `AIResponse`\n- `getInstallInstructions(): string` — Returns multiline string with npm install command and URL\n\n**isCommandOnPath** function `async (command: string): Promise<boolean>` — Cross-platform PATH lookup checking if command exists as file in any PATH directory. Splits `process.env.PATH` by `path.delimiter`, iterates directories, checks `process.env.PATHEXT` extensions on Windows (`.exe`, `.cmd`, `.bat`), uses `fs.stat().isFile()` for detection (not `fs.access()` to avoid Unix permission assumptions).\n\n**ClaudeResponseSchema** Zod schema validates Claude CLI v2.1.31 JSON output structure:\n- Root: `{ type: 'result', subtype: 'success'|'error', is_error: boolean, duration_ms: number, duration_api_ms: number, num_turns: number, result: string, session_id: string, total_cost_usd: number, usage: {...}, modelUsage: {...} }`\n- `usage` object: `{ input_tokens, cache_creation_input_tokens, cache_read_input_tokens, output_tokens }` (all numbers)\n- `modelUsage` record: `Map<modelName, { inputTokens, outputTokens, cacheReadInputTokens, cacheCreationInputTokens, costUSD }>`\n\n## CLI Argument Construction\n\n`buildArgs()` returns fixed base arguments:\n- `-p` — Non-interactive print mode\n- `--output-format json` — Structured JSON output\n- `--no-session-persistence` — Prevents disk session saves\n- `--permission-mode bypassPermissions` — Skips interactive permission prompts (references PITFALLS.md §8)\n\nConditionally appends:\n- `--model <value>` if `options.model` provided\n- `--system-prompt <value>` if `options.systemPrompt` provided\n- `--max-turns <value>` if `options.maxTurns !== undefined`\n\nPrompt text excluded from args array — passed via stdin by `runSubprocess()` caller.\n\n## JSON Response Parsing\n\n`parseResponse()` implements defensive parsing:\n1. Finds first `{` character via `stdout.indexOf('{')` to skip non-JSON prefix text (upgrade notices, warnings)\n2. Throws `AIServiceError` with code `PARSE_ERROR` if no JSON object found\n3. Parses substring `stdout.slice(jsonStart)` and validates against `ClaudeResponseSchema`\n4. Extracts model name from `Object.keys(parsed.modelUsage)[0]` (first key is active model)\n5. Maps to `AIResponse` structure:\n   - `text` ← `parsed.result`\n   - `model` ← first modelUsage key or `'unknown'`\n   - `inputTokens` ← `parsed.usage.input_tokens`\n   - `outputTokens` ← `parsed.usage.output_tokens`\n   - `cacheReadTokens` ← `parsed.usage.cache_read_input_tokens`\n   - `cacheCreationTokens` ← `parsed.usage.cache_creation_input_tokens`\n   - `durationMs` ← function parameter (wall-clock time)\n   - `exitCode` ← function parameter\n   - `raw` ← entire `parsed` object\n\n## Error Handling\n\nThrows `AIServiceError` with discriminated codes:\n- `PARSE_ERROR` when `stdout.indexOf('{')` returns `-1` (includes first 200 chars of raw output in message)\n- `PARSE_ERROR` when `ClaudeResponseSchema.parse()` throws (includes Zod validation error message)\n\n## Dependencies\n\n- `node:fs/promises` — `fs.stat()` for PATH detection\n- `node:path` — `path.join()`, `path.delimiter` for cross-platform path manipulation\n- `zod` — `ClaudeResponseSchema` validation, `z.infer<>` type extraction\n- `../types.js` — `AIBackend`, `AICallOptions`, `AIResponse`, `AIServiceError` imports\n\n## Integration Pattern\n\nIntended usage flow (documented in class example):\n1. Instantiate `new ClaudeBackend()`\n2. Call `await backend.isAvailable()` for availability check\n3. Call `backend.buildArgs({ prompt, model?, systemPrompt?, maxTurns? })` to construct CLI args\n4. Pass args to `runSubprocess('claude', args, { input: prompt, timeoutMs })` (from `../subprocess.ts`)\n5. Call `backend.parseResponse(result.stdout, result.durationMs, result.exitCode)` to normalize response\n\n## Platform-Specific Behavior\n\nWindows PATH detection differences:\n- Reads `process.env.PATHEXT` (typically `\".COM;.EXE;.BAT;.CMD;.VBS;.VBE;.JS;.JSE;.WSF;.WSH;.MSC\"`)\n- Checks each extension appended to command name: `claude.exe`, `claude.cmd`, `claude.bat`, etc.\n- Uses `fs.stat()` rather than Unix execute bit checking for cross-platform compatibility\n\nUnix/Linux/macOS:\n- `process.env.PATHEXT` unset → single iteration with bare command name\n- Checks `claude` (no extension)\n\nPATH string cleaning:\n- Strips double quotes via `.replace(/[\"]+/g, '')` before splitting (handles Windows quoted paths)\n- Splits by `path.delimiter` (`:` on Unix, `;` on Windows)\n- Filters empty strings via `.filter(Boolean)`\n### gemini.ts\n**Purpose:** GeminiBackend implements AIBackend interface as a stub demonstrating backend extension pattern, returning build confi...\n\n**GeminiBackend implements AIBackend interface as a stub demonstrating backend extension pattern, returning build configuration for Gemini CLI but throwing SUBPROCESS_ERROR in parseResponse until Gemini JSON output format stabilizes.**\n\n## Exported Interface\n\n**GeminiBackend** — Stub class implementing AIBackend interface with:\n- `name: 'gemini'` — Backend identifier constant\n- `cliCommand: 'gemini'` — CLI executable name\n- `isAvailable(): Promise<boolean>` — Delegates to `isCommandOnPath(this.cliCommand)` to check PATH availability\n- `buildArgs(_options: AICallOptions): string[]` — Returns `['-p', '--output-format', 'json']` argument array for CLI invocation\n- `parseResponse(_stdout: string, _durationMs: number, _exitCode: number): AIResponse` — Throws `AIServiceError` with code `'SUBPROCESS_ERROR'` and message `'Gemini backend is not yet implemented. Use Claude backend.'`\n- `getInstallInstructions(): string` — Returns multiline instructions referencing `npm install -g @anthropic-ai/gemini-cli` and GitHub URL `https://github.com/google-gemini/gemini-cli`\n\n## Dependencies\n\nImports `AIBackend`, `AICallOptions`, `AIResponse`, `AIServiceError` from `'../types.js'`. Imports `isCommandOnPath` utility from `'./claude.js'` for PATH detection.\n\n## Implementation Status\n\nBackend deferred to future phase pending Gemini CLI JSON stability (references `RESEARCH.md Open Question 2` in module docstring). All methods except `parseResponse` provide operational implementations. Prompt delivery occurs via stdin through subprocess wrapper (noted in `buildArgs` docstring).\n\n## Integration Pattern\n\nFollows same AIBackend contract as ClaudeBackend and OpenCodeBackend, enabling registry-based detection via `AIBackendRegistry.detectAvailableBackend()` in `src/ai/registry.ts`. Backend registered in registry's `backends` array for automatic `--backend auto` resolution.\n### opencode.ts\n**Purpose:** OpenCodeBackend implements AIBackend interface as stub demonstrating extension pattern for future OpenCode CLI integr...\n\n**OpenCodeBackend implements AIBackend interface as stub demonstrating extension pattern for future OpenCode CLI integration, currently throwing SUBPROCESS_ERROR in parseResponse() until JSONL output parsing implementation completes.**\n\n## Exported Symbols\n\n**OpenCodeBackend** class implements AIBackend interface with readonly properties `name: 'opencode'` and `cliCommand: 'opencode'`.\n\n## Public Interface\n\n**isAvailable(): Promise<boolean>** checks if `opencode` CLI exists on PATH by delegating to `isCommandOnPath(this.cliCommand)` imported from `claude.js`.\n\n**buildArgs(_options: AICallOptions): string[]** constructs CLI argument array `['run', '--format', 'json']` based on documented OpenCode CLI flags from RESEARCH.md. Prompt content passes via stdin through subprocess wrapper, not as CLI argument.\n\n**parseResponse(_stdout: string, _durationMs: number, _exitCode: number): AIResponse** always throws `AIServiceError('SUBPROCESS_ERROR', 'OpenCode backend is not yet implemented. Use Claude backend.')` since JSONL parsing remains unimplemented per RESEARCH.md Open Question 3.\n\n**getInstallInstructions(): string** returns multi-line string with curl-based install command `curl -fsSL https://opencode.ai/install | bash` and URL reference `https://opencode.ai`.\n\n## Integration Pattern\n\nDemonstrates backend extension architecture where AIBackend interface enables pluggable AI CLI adapters. OpenCodeBackend mirrors ClaudeBackend and GeminiBackend structure (sibling files `claude.ts`, `gemini.ts`) but defers output parsing implementation to future development phase.\n\n## Dependencies\n\nImports `AIBackend`, `AICallOptions`, `AIResponse` interfaces and `AIServiceError` class from `../types.js`. Imports `isCommandOnPath()` utility from `./claude.js` for PATH existence checking.\n\n## Implementation Status\n\nStub implementation allows registry detection and argument construction but blocks execution at response parsing stage. Full implementation requires JSONL format specification and parser matching OpenCode CLI output structure.\n\n## Import Map (verified — use these exact paths)\n\nclaude.ts:\n  ../types.js → AIBackend, AICallOptions, AIResponse (type)\n  ../types.js → AIServiceError\n\ngemini.ts:\n  ../types.js → AIBackend, AICallOptions, AIResponse (type)\n  ../types.js → AIServiceError\n\nopencode.ts:\n  ../types.js → AIBackend, AICallOptions, AIResponse (type)\n  ../types.js → AIServiceError\n\n## Project Directory Structure\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>",
      "systemPrompt": "You are generating an AGENTS.md file — a directory-level overview for AI coding assistants.\n\nCRITICAL: Output ONLY the raw markdown content. No code fences, no preamble, no explanations, no conversational text. Your entire response IS the AGENTS.md file content.\n\nOUTPUT FORMAT:\n- First line MUST be exactly: <!-- Generated by agents-reverse-engineer -->\n- Use a # heading with the directory name\n- Write a one-paragraph purpose statement for the directory\nusage\nADAPTIVE SECTIONS:\nAnalyze the directory contents and choose the most relevant sections. Do NOT use a fixed template. Instead, select sections that best document this specific directory for an AI that needs to reconstruct or extend the project.\n\nConsider these section types (choose what applies):\n- **Contents**: Group files by purpose/category under ## headings. For each file: markdown link [filename](./filename) and a one-line description.\n- **Subdirectories**: If subdirectories exist, list them with links [dirname/](./dirname/) and brief summaries.\n- **Architecture / Data Flow**: If files form a pipeline, request/response chain, or layered architecture, document it.\n- **Stack**: If this is a package root (has package.json, Cargo.toml, go.mod, etc.), document the technology stack, key scripts, and entry points.\n- **Structure**: If the directory layout follows a convention (feature-sliced, domain-driven, MVC, etc.), document it.\n- **Patterns**: If files share recurring design patterns (factory, strategy, middleware, barrel re-export), name and document them.\n- **Configuration**: If the directory contains config files, schemas, or environment definitions, document the config surface area.\n- **API Surface**: If the directory exports a public API (barrel index, route definitions, SDK), document the interface contract.\n- **File Relationships**: How files collaborate, depend on each other, or share state.\n- **Behavioral Contracts**: If files contain regex patterns, format specifications, magic constants, or template strings that define observable behavior, collect them in a dedicated section. Preserve verbatim patterns from file summaries — do NOT paraphrase regex into prose. This section is MANDATORY when file summaries contain behavioral artifacts.\n\nChoose any relevant sections or create your own based on the directory contents. The goal is to provide a comprehensive overview that captures the essence of the directory's role in the project and how its files work together, with a focus on what an AI coding assistant would need to know to effectively interact with this code.\n\nSCOPE:\n- AGENTS.md is a NAVIGATIONAL INDEX — help an AI find the right file quickly\n- Focus on: what each file does, how files relate, directory-level patterns\n- Do NOT reproduce full architecture sections — those belong in the root CLAUDE.md\n\nPATH ACCURACY (MANDATORY):\n- When referencing files or modules outside this directory, use ONLY paths from the \"Import Map\" section\n- Do NOT invent, rename, or guess module paths — if a path isn't in the Import Map, don't reference it\n- Use the exact directory names from \"Project Directory Structure\" — do NOT rename directories\n  (e.g., if the directory is called \"cli\", write \"src/cli/\", NOT \"src/commands/\")\n- Cross-module references must use the specifier format from actual import statements\n  (e.g., \"../generation/writers/sum.js\", NOT \"../fs/sum-file.js\")\n- If you are unsure about a path, omit the cross-reference rather than guessing\n\nCONSISTENCY (MANDATORY):\n- Do not contradict yourself within the same document\n- If you describe a technique (e.g., \"regex-based\"), do not call it something else later (e.g., \"AST-based\")\n- When stating version numbers, engines, or config fields, use ONLY values present in the file summaries\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this directory\", \"this module\", \"provides\", \"responsible for\", \"is used to\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Per-file descriptions in Contents sections: 1-2 sentences maximum. Reference key symbols but do not reproduce full summaries.\n- Behavioral contracts (regex patterns, format specs, constants) belong in a separate Behavioral Contracts section, not in per-file descriptions.\n- Subdirectory descriptions: 1-2 sentences maximum. Capture the directory's role, not its full contents.\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- Key exported symbols from file summaries MUST appear in the directory overview\n- Preserve exact casing of identifiers\n\nUSER NOTES:\n- If \"User Notes\" are provided in the prompt, they contain user-defined instructions that will be automatically prepended to your output\n- Do NOT repeat or paraphrase user notes in your generated content — they are included separately\n- You may reference information from user notes for context",
      "response": "<!-- Generated by agents-reverse-engineer -->\n\n# src/ai/backends\n\n**Backend adapter implementations for Claude Code, Gemini CLI, and OpenCode providing AIBackend interface compliance with CLI argument construction, JSON response parsing, PATH availability detection, and installation instructions.**\n\n## Contents\n\n### Backend Implementations\n\n**[claude.ts](./claude.ts)** — ClaudeBackend adapter with Zod-validated JSON parsing extracting `text`/`model`/`inputTokens`/`outputTokens`/`cacheReadTokens`/`cacheCreationTokens` from Claude CLI v2.1.31 output, `buildArgs()` constructing `['-p', '--output-format', 'json', '--no-session-persistence', '--permission-mode', 'bypassPermissions']` base arguments with conditional `--model`/`--system-prompt`/`--max-turns` appends, `isCommandOnPath()` cross-platform PATH detection handling Windows `PATHEXT` extension iteration.\n\n**[gemini.ts](./gemini.ts)** — GeminiBackend stub returning `['-p', '--output-format', 'json']` from `buildArgs()` but throwing `SUBPROCESS_ERROR` in `parseResponse()` until Gemini JSON output format stabilizes per RESEARCH.md Open Question 2.\n\n**[opencode.ts](./opencode.ts)** — OpenCodeBackend stub returning `['run', '--format', 'json']` from `buildArgs()` but throwing `SUBPROCESS_ERROR` in `parseResponse()` until JSONL parsing implementation completes per RESEARCH.md Open Question 3.\n\n## AIBackend Interface Contract\n\nAll backends implement five required methods:\n\n- **`isAvailable(): Promise<boolean>`** — Delegates to `isCommandOnPath(this.cliCommand)` checking PATH directories via `fs.stat().isFile()` with Windows PATHEXT handling\n- **`buildArgs(options: AICallOptions): string[]`** — Constructs CLI argument arrays (prompt content delivered via stdin by `runSubprocess()` caller, not CLI arg)\n- **`parseResponse(stdout: string, durationMs: number, exitCode: number): AIResponse`** — Normalizes CLI output to `{ text, model, inputTokens, outputTokens, cacheReadTokens, cacheCreationTokens, durationMs, exitCode, raw }` structure\n- **`getInstallInstructions(): string`** — Returns multiline installation guidance with npm/curl commands and documentation URLs\n- **Properties:** `name: string` (backend identifier), `cliCommand: string` (executable name)\n\n## Claude Response Schema\n\n`ClaudeResponseSchema` validates JSON structure:\n\n```typescript\n{\n  type: 'result',\n  subtype: 'success'|'error',\n  is_error: boolean,\n  duration_ms: number,\n  duration_api_ms: number,\n  num_turns: number,\n  result: string,\n  session_id: string,\n  total_cost_usd: number,\n  usage: {\n    input_tokens: number,\n    cache_creation_input_tokens: number,\n    cache_read_input_tokens: number,\n    output_tokens: number\n  },\n  modelUsage: Record<string, {\n    inputTokens: number,\n    outputTokens: number,\n    cacheReadInputTokens: number,\n    cacheCreationInputTokens: number,\n    costUSD: number\n  }>\n}\n```\n\n## Integration with Registry\n\nBackends registered in `src/ai/registry.ts` `AIBackendRegistry.backends` array enabling `detectAvailableBackend()` auto-detection via parallel `isAvailable()` checks. Registry returns first available backend when `config.ai.backend === 'auto'`.\n\n## PATH Detection Strategy\n\n`isCommandOnPath()` implements cross-platform availability checking:\n\n1. Reads `process.env.PATH`, strips double quotes, splits by `path.delimiter` (`:` Unix, `;` Windows)\n2. Reads `process.env.PATHEXT` on Windows (e.g., `.COM;.EXE;.BAT;.CMD`), defaults to `['']` on Unix\n3. Iterates PATH directories × PATHEXT extensions constructing candidate paths\n4. Calls `fs.stat(candidatePath).isFile()` returning `true` on first match (avoids Unix execute bit assumptions)\n5. Returns `false` if no candidates exist as files\n\n## Error Handling\n\nClaudeBackend throws `AIServiceError` with codes:\n\n- **`PARSE_ERROR`** — When `stdout.indexOf('{')` returns `-1` (includes first 200 chars of raw output)\n- **`PARSE_ERROR`** — When Zod validation fails (includes validation error message)\n\nStub backends throw:\n\n- **`SUBPROCESS_ERROR`** — GeminiBackend/OpenCodeBackend until parsing implementation completes\n\n## Behavioral Contracts\n\n### ClaudeBackend CLI Arguments\n\nBase arguments (always included):\n```\n-p\n--output-format json\n--no-session-persistence\n--permission-mode bypassPermissions\n```\n\nConditional arguments:\n```\n--model <value>               # if options.model present\n--system-prompt <value>       # if options.systemPrompt present\n--max-turns <value>           # if options.maxTurns !== undefined\n```\n\n### JSON Parsing Defensive Pattern\n\n```javascript\nconst jsonStart = stdout.indexOf('{')\nif (jsonStart === -1) {\n  throw new AIServiceError('PARSE_ERROR', `No JSON in output: ${stdout.slice(0, 200)}`)\n}\nconst parsed = ClaudeResponseSchema.parse(JSON.parse(stdout.slice(jsonStart)))\n```\n\n### Windows PATHEXT Extensions\n\n```\n.COM;.EXE;.BAT;.CMD;.VBS;.VBE;.JS;.JSE;.WSF;.WSH;.MSC\n```",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 1419,
      "cacheReadTokens": 16334,
      "cacheCreationTokens": 12849,
      "latencyMs": 48469,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": []
    },
    {
      "timestamp": "2026-02-09T16:12:32.952Z",
      "prompt": "Generate AGENTS.md for directory: \"src/discovery/filters\" (filters)\n\n## File Summaries (5 files)\n\n### binary.ts\n**Purpose:** Binary file filter implementing two-phase detection (extension fast-path, content analysis fallback) with size-based ...\n\n**Binary file filter implementing two-phase detection (extension fast-path, content analysis fallback) with size-based exclusion for file discovery.**\n\n## Exported Interface\n\n`createBinaryFilter(options?: BinaryFilterOptions): FileFilter` — Factory returning a `FileFilter` with `name: 'binary'` and async `shouldExclude(absolutePath: string): Promise<boolean>` method. Combines extension-based detection with content analysis via `isbinaryfile.isBinaryFile()` and size threshold checks via `fs.stat()`.\n\n`BinaryFilterOptions` — Configuration interface with optional `maxFileSize?: number` (bytes, default 1048576) and `additionalExtensions?: string[]` for supplementing default binary extension set.\n\n`BINARY_EXTENSIONS: Set<string>` — Exported set of 82 known binary extensions across 9 categories: images (`.png`, `.jpg`, `.jpeg`, `.gif`, `.bmp`, `.ico`, `.webp`, `.svg`, `.tiff`, `.tif`, `.psd`, `.raw`, `.heif`, `.heic`), archives (`.zip`, `.tar`, `.gz`, `.rar`, `.7z`, `.bz2`, `.xz`, `.tgz`), executables (`.exe`, `.dll`, `.so`, `.dylib`, `.bin`, `.msi`, `.app`, `.dmg`), media (`.mp3`, `.mp4`, `.wav`, `.avi`, `.mov`, `.mkv`, `.flac`, `.ogg`, `.webm`, `.m4a`, `.aac`, `.wma`, `.wmv`, `.flv`), binary documents (`.pdf`, `.doc`, `.docx`, `.xls`, `.xlsx`, `.ppt`, `.pptx`, `.odt`, `.ods`, `.odp`), fonts (`.woff`, `.woff2`, `.ttf`, `.eot`, `.otf`), compiled bytecode (`.class`, `.pyc`, `.pyo`, `.o`, `.obj`, `.a`, `.lib`, `.wasm`), database (`.db`, `.sqlite`, `.sqlite3`, `.mdb`), and miscellaneous (`.ico`, `.icns`, `.cur`, `.deb`, `.rpm`, `.jar`, `.war`, `.ear`).\n\n`DEFAULT_MAX_FILE_SIZE: number` — Constant set to `1024 * 1024` (1MB).\n\n## Detection Algorithm\n\n`shouldExclude()` executes three-phase check:\n\n1. **Fast path (extension)**: Extracts `path.extname(absolutePath).toLowerCase()`, checks membership in `binaryExtensions` set (merge of `BINARY_EXTENSIONS` and `additionalExtensions` normalized with leading dot via `ext.startsWith('.') ? ext : '.' + ext`), returns `true` on match.\n\n2. **Size threshold**: Invokes `fs.stat(absolutePath)`, compares `stats.size > maxFileSize`, returns `true` if exceeded.\n\n3. **Slow path (content)**: Delegates to `isBinaryFile(absolutePath)` from `isbinaryfile` package for unknown extensions.\n\nReturns `true` on `fs.stat()` failure (file unreadable/missing). All operations are async via `async/await` pattern.\n\n## Integration with Discovery Pipeline\n\nImplements `FileFilter` interface from `../types.js` with `name` property for identification in filter chains. Composed with other filters (gitignore, vendor, custom) via `discovery/filters/index.ts` exports. Configured via `config.yaml` schema fields: `options.maxFileSize` and `exclude.binaryExtensions`.\n\n## Dependencies\n\n- `isbinaryfile` — Provides `isBinaryFile(path)` for content-based binary detection\n- `node:fs/promises` — Supplies `fs.stat()` for size retrieval\n- `node:path` — Supplies `path.extname()` for extension extraction\n### custom.ts\n**Purpose:** createCustomFilter constructs a FileFilter that excludes files matching user-provided gitignore-style patterns during...\n\n**createCustomFilter constructs a FileFilter that excludes files matching user-provided gitignore-style patterns during discovery.**\n\n## Exports\n\n- `createCustomFilter(patterns: string[], root: string): FileFilter` — constructs filter instance with `ignore` library matcher, returns FileFilter with `name: 'custom'` and `shouldExclude` method\n\n## Implementation Details\n\ncreateCustomFilter instantiates `Ignore` object via `ignore()` factory, normalizes root via `path.resolve(root)`, adds all patterns via `ig.add(patterns)` if array non-empty. Returns FileFilter with shouldExclude implementation that converts absolute paths to relative via `path.relative(normalizedRoot, absolutePath)`, returns false for empty patterns array or paths outside root (relative path starts with `..` or is empty string), otherwise delegates to `ig.ignores(relativePath)`.\n\n## Integration Points\n\nConsumed by file discovery pipeline in `src/discovery/walker.ts` as part of composable filter chain. Patterns sourced from `config.exclude.patterns` array in `.agents-reverse-engineer/config.yaml` loaded via `src/config/loader.ts`. Works alongside gitignore filter (`src/discovery/filters/gitignore.ts`), binary filter (`src/discovery/filters/binary.ts`), and vendor filter (`src/discovery/filters/vendor.ts`) exported from `src/discovery/filters/index.ts`.\n\n## Path Handling\n\nRequires relative paths for `ignore` library compatibility. Absolute input paths resolved via normalizedRoot baseline. Edge case: paths outside root (e.g., symlinks resolving to parent directories) bypass filtering by returning false when relativePath starts with `..`. Empty relative paths also bypass exclusion.\n\n## Dependencies\n\n- `ignore` library (Ignore type) for gitignore-style pattern matching with `ig.ignores(relativePath)` predicate\n- `node:path` for `path.relative()` and `path.resolve()` conversions\n- FileFilter interface from `../types.js` defining `name: string` and `shouldExclude(absolutePath: string): boolean` contract\n### gitignore.ts\n**Purpose:** createGitignoreFilter produces FileFilter instances that exclude paths matching .gitignore patterns via the `ignore` ...\n\n**createGitignoreFilter produces FileFilter instances that exclude paths matching .gitignore patterns via the `ignore` library.**\n\n## Exported Function\n\n### createGitignoreFilter\n```typescript\nasync function createGitignoreFilter(root: string): Promise<FileFilter>\n```\nReads `.gitignore` from `root` directory, parses patterns via `ignore()` library, returns `FileFilter` with `shouldExclude()` method. Missing `.gitignore` file caught silently (filter passes all paths). Normalizes `root` to absolute path via `path.resolve()`.\n\n## FileFilter Implementation\n\nReturned object implements `FileFilter` interface with:\n- `name: 'gitignore'` — Filter identifier for debugging/logging\n- `shouldExclude(absolutePath: string): boolean` — Path exclusion predicate\n\n## Path Handling\n\n`shouldExclude()` converts absolute paths to relative via `path.relative(normalizedRoot, absolutePath)`:\n- Returns `false` for paths outside root (starting with `..`)\n- Returns `false` for empty relative paths\n- Delegates to `ig.ignores(relativePath)` for pattern matching\n\n**Critical constraint**: `ignore` library requires relative paths without leading slash. Trailing slash semantics apply to directories; this implementation omits trailing slash since file walker returns files only.\n\n## Dependencies\n\n- `ignore` library (`Ignore` type) — Parses and matches .gitignore patterns\n- `node:fs/promises` — Async `.gitignore` file reading via `readFile()`\n- `node:path` — Path normalization and relative path computation\n- `../types.js` — `FileFilter` interface definition\n\n## Integration Points\n\nUsed by discovery pipeline (`src/discovery/walker.ts`) as composable filter in filter chain alongside binary filter, vendor filter, custom pattern filter. Consumed during Phase 1 file analysis to respect repository gitignore rules.\n### index.ts\n**Purpose:** Exports filter chain orchestration via `applyFilters()` function that runs files through sequential `FileFilter` pred...\n\n**Exports filter chain orchestration via `applyFilters()` function that runs files through sequential `FileFilter` predicates with bounded concurrency, records exclusion metadata per filter, and re-exports all filter creators.**\n\n## Exported Functions\n\n**`applyFilters(files: string[], filters: FileFilter[], options?: { tracer?: ITraceWriter; debug?: boolean }): Promise<FilterResult>`**\n\nApplies filter chain to file list with short-circuit evaluation (stops at first exclusion). Spawns 30 concurrent workers sharing single iterator to bound file descriptor usage (critical for binary detection I/O via `isBinaryFile()`). Each worker processes files serially through filter array, breaks on first `shouldExclude()` true return. Collects results as `Array<{ index: number; file: string; excluded?: ExcludedFile }>`, sorts by original index to preserve input order. Populates `filterStats` map tracking per-filter `matched`/`rejected` counts, emits `filter:applied` trace events with `filterName`, `filesMatched`, `filesRejected` fields. Debug mode logs rejected counts via `console.error(pc.dim(...))` pattern. Returns `FilterResult` with `included: string[]` and `excluded: ExcludedFile[]` where `ExcludedFile` contains `path`, `filter`, `reason: 'Excluded by {filter.name} filter'`.\n\n## Re-Exported Filter Creators\n\n- `createGitignoreFilter` from `./gitignore.js`\n- `createVendorFilter`, `DEFAULT_VENDOR_DIRS` from `./vendor.js`\n- `createBinaryFilter`, `BINARY_EXTENSIONS`, `BinaryFilterOptions` from `./binary.js`\n- `createCustomFilter` from `./custom.js`\n\n## Key Dependencies\n\n- `picocolors` (`pc`) for debug logging color formatting\n- `FileFilter`, `FilterResult`, `ExcludedFile` types from `../types.js`\n- `ITraceWriter` interface from `../../orchestration/trace.js`\n\n## Concurrency Strategy\n\nFixed concurrency limit of 30 workers sharing iterator via `files.entries()` iterator pattern. Worker count capped at `Math.min(CONCURRENCY, files.length)` to avoid spawning unnecessary workers for small file sets. Workers drain shared iterator via `for...of` loop, no work-stealing or queue coordination required beyond iterator protocol.\n\n## Filter Statistics\n\nTracks per-filter statistics in `Map<string, { matched: number; rejected: number }>` where:\n- `rejected` increments when filter excludes file\n- `matched` increments for all filters when file passes entire chain (not when individual filter passes)\n\nStatistics emitted as `filter:applied` trace events after all workers complete, enabling downstream analysis of filter effectiveness.\n### vendor.ts\n**Purpose:** createVendorFilter() constructs a FileFilter that excludes files within third-party dependency directories and build ...\n\n**createVendorFilter() constructs a FileFilter that excludes files within third-party dependency directories and build output paths using path segment matching and path pattern substring matching.**\n\n## Exported Symbols\n\n### DEFAULT_VENDOR_DIRS\n```typescript\nconst DEFAULT_VENDOR_DIRS: readonly [\n  'node_modules', 'vendor', '.git', 'dist', 'build',\n  '__pycache__', '.next', 'venv', '.venv', 'target'\n]\n```\nArray of common vendor directory names: `node_modules`, `vendor`, `.git`, `dist`, `build`, `__pycache__`, `.next`, `venv`, `.venv`, `target`. Used as default exclusion list for third-party code, build artifacts, Python cache, Next.js build output, Python virtual environments, and Rust/Java build directories.\n\n### createVendorFilter\n```typescript\nfunction createVendorFilter(vendorDirs: string[]): FileFilter\n```\nReturns a `FileFilter` instance that excludes paths containing any directory from `vendorDirs`. Implements two matching strategies: (1) single-segment matching where directory names like `'node_modules'` match any path segment (e.g., `/project/node_modules/lodash/index.js`), (2) path-pattern matching where patterns like `'.agents/skills'` or `'apps/vendor'` match via substring search after normalizing separators to `path.sep`. The returned filter has `name: 'vendor'` and `shouldExclude(absolutePath: string): boolean` method.\n\n## Pattern Matching Algorithm\n\nThe `shouldExclude()` method splits `absolutePath` by `path.sep`, iterates segments checking membership in `singleSegments` Set (O(1) lookup), then iterates `pathPatterns` array checking `absolutePath.includes(pattern)` for substring match. Returns `true` on first match, `false` if no matches found.\n\nPath patterns undergo normalization via `dir.replace(/[\\\\/]/g, path.sep)` to convert both forward and backslashes to OS-specific separator before classification. Classification uses `normalized.includes(path.sep)` to distinguish path patterns (containing separators) from single segments (no separators).\n\n## Integration with Discovery Pipeline\n\nImplements `FileFilter` interface from `../types.js` with `name: string` and `shouldExclude(absolutePath: string): boolean` signature. Composed with other filters (gitignore, binary, custom) in discovery filter chain orchestrated by `src/discovery/walker.ts`.\n\n## Import Map (verified — use these exact paths)\n\nbinary.ts:\n  ../types.js → FileFilter (type)\n\ncustom.ts:\n  ../types.js → FileFilter (type)\n\ngitignore.ts:\n  ../types.js → FileFilter (type)\n\nindex.ts:\n  ../types.js → FileFilter, FilterResult, ExcludedFile (type)\n  ../../orchestration/trace.js → ITraceWriter (type)\n\nvendor.ts:\n  ../types.js → FileFilter (type)\n\n## Project Directory Structure\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>",
      "systemPrompt": "You are generating an AGENTS.md file — a directory-level overview for AI coding assistants.\n\nCRITICAL: Output ONLY the raw markdown content. No code fences, no preamble, no explanations, no conversational text. Your entire response IS the AGENTS.md file content.\n\nOUTPUT FORMAT:\n- First line MUST be exactly: <!-- Generated by agents-reverse-engineer -->\n- Use a # heading with the directory name\n- Write a one-paragraph purpose statement for the directory\nusage\nADAPTIVE SECTIONS:\nAnalyze the directory contents and choose the most relevant sections. Do NOT use a fixed template. Instead, select sections that best document this specific directory for an AI that needs to reconstruct or extend the project.\n\nConsider these section types (choose what applies):\n- **Contents**: Group files by purpose/category under ## headings. For each file: markdown link [filename](./filename) and a one-line description.\n- **Subdirectories**: If subdirectories exist, list them with links [dirname/](./dirname/) and brief summaries.\n- **Architecture / Data Flow**: If files form a pipeline, request/response chain, or layered architecture, document it.\n- **Stack**: If this is a package root (has package.json, Cargo.toml, go.mod, etc.), document the technology stack, key scripts, and entry points.\n- **Structure**: If the directory layout follows a convention (feature-sliced, domain-driven, MVC, etc.), document it.\n- **Patterns**: If files share recurring design patterns (factory, strategy, middleware, barrel re-export), name and document them.\n- **Configuration**: If the directory contains config files, schemas, or environment definitions, document the config surface area.\n- **API Surface**: If the directory exports a public API (barrel index, route definitions, SDK), document the interface contract.\n- **File Relationships**: How files collaborate, depend on each other, or share state.\n- **Behavioral Contracts**: If files contain regex patterns, format specifications, magic constants, or template strings that define observable behavior, collect them in a dedicated section. Preserve verbatim patterns from file summaries — do NOT paraphrase regex into prose. This section is MANDATORY when file summaries contain behavioral artifacts.\n\nChoose any relevant sections or create your own based on the directory contents. The goal is to provide a comprehensive overview that captures the essence of the directory's role in the project and how its files work together, with a focus on what an AI coding assistant would need to know to effectively interact with this code.\n\nSCOPE:\n- AGENTS.md is a NAVIGATIONAL INDEX — help an AI find the right file quickly\n- Focus on: what each file does, how files relate, directory-level patterns\n- Do NOT reproduce full architecture sections — those belong in the root CLAUDE.md\n\nPATH ACCURACY (MANDATORY):\n- When referencing files or modules outside this directory, use ONLY paths from the \"Import Map\" section\n- Do NOT invent, rename, or guess module paths — if a path isn't in the Import Map, don't reference it\n- Use the exact directory names from \"Project Directory Structure\" — do NOT rename directories\n  (e.g., if the directory is called \"cli\", write \"src/cli/\", NOT \"src/commands/\")\n- Cross-module references must use the specifier format from actual import statements\n  (e.g., \"../generation/writers/sum.js\", NOT \"../fs/sum-file.js\")\n- If you are unsure about a path, omit the cross-reference rather than guessing\n\nCONSISTENCY (MANDATORY):\n- Do not contradict yourself within the same document\n- If you describe a technique (e.g., \"regex-based\"), do not call it something else later (e.g., \"AST-based\")\n- When stating version numbers, engines, or config fields, use ONLY values present in the file summaries\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this directory\", \"this module\", \"provides\", \"responsible for\", \"is used to\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Per-file descriptions in Contents sections: 1-2 sentences maximum. Reference key symbols but do not reproduce full summaries.\n- Behavioral contracts (regex patterns, format specs, constants) belong in a separate Behavioral Contracts section, not in per-file descriptions.\n- Subdirectory descriptions: 1-2 sentences maximum. Capture the directory's role, not its full contents.\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- Key exported symbols from file summaries MUST appear in the directory overview\n- Preserve exact casing of identifiers\n\nUSER NOTES:\n- If \"User Notes\" are provided in the prompt, they contain user-defined instructions that will be automatically prepended to your output\n- Do NOT repeat or paraphrase user notes in your generated content — they are included separately\n- You may reference information from user notes for context",
      "response": "<!-- Generated by agents-reverse-engineer -->\n\n# src/discovery/filters\n\nComposable file exclusion filters implementing gitignore semantics, binary detection, vendor directory skipping, and custom glob patterns for the file discovery pipeline.\n\n## Contents\n\n### Filter Implementations\n\n**[binary.ts](./binary.ts)** — Three-phase binary file detector: extension fast-path lookup in 82-member `BINARY_EXTENSIONS` set (images/archives/executables/media/fonts/bytecode/databases), size threshold comparison via `fs.stat()` (default 1MB), content analysis fallback via `isbinaryfile.isBinaryFile()`.\n\n**[custom.ts](./custom.ts)** — User-defined gitignore-style pattern matcher via `ignore` library, converts absolute paths to relative via `path.relative()`, bypasses filtering for paths outside root (relative path starts with `..`).\n\n**[gitignore.ts](./gitignore.ts)** — `.gitignore` parser consuming `ignore` library, silently passes all paths when `.gitignore` missing, converts absolute paths to relative for pattern matching.\n\n**[vendor.ts](./vendor.ts)** — Vendor directory excluder with dual matching: single-segment Set lookup (`node_modules`, `.git`, `dist`, `build`, `__pycache__`, `.next`, `venv`, `.venv`, `target`) and path-pattern substring search for multi-segment patterns (`.agents/skills`, `apps/vendor`).\n\n**[index.ts](./index.ts)** — Filter chain orchestrator with `applyFilters()` executing short-circuit evaluation (stops at first exclusion) across 30 concurrent workers sharing iterator, collects per-filter statistics (`matched`/`rejected` counts), emits `filter:applied` trace events, re-exports all filter creators.\n\n## Architecture\n\n### Filter Chain Execution\n\n`applyFilters()` bounds concurrency to 30 workers via `Math.min(CONCURRENCY, files.length)` to prevent file descriptor exhaustion during binary detection I/O. Each worker drains shared `files.entries()` iterator, processes files serially through filter array with short-circuit semantics (breaks on first `shouldExclude()` true return). Results sorted by original index to preserve input order despite concurrent execution.\n\n### Path Normalization Contract\n\nAll filters receive absolute paths, convert to relative via `path.relative(normalizedRoot, absolutePath)` for pattern matching libraries (`ignore` requires relative paths without leading slash). Paths outside root (starting with `..`) bypass filtering by returning `false`. Empty relative paths also bypass exclusion.\n\n### Binary Detection Strategy\n\n`createBinaryFilter()` prioritizes performance via three-tiered approach:\n1. Extension lookup in pre-allocated Set (O(1), no I/O)\n2. Size threshold check via `fs.stat()` (single syscall)\n3. Content analysis via `isbinaryfile` heuristics (reads file header bytes)\n\nReturns `true` on `fs.stat()` failure (file unreadable/missing) to fail-safe exclude corrupt/inaccessible files.\n\n### Vendor Path Pattern Classification\n\n`createVendorFilter()` splits patterns into `singleSegments` Set and `pathPatterns` array based on presence of path separator after normalization (`dir.replace(/[\\\\/]/g, path.sep)`). Single segments use Set membership check, path patterns use substring search. Normalization ensures cross-platform compatibility (Windows backslashes converted to `path.sep`).\n\n## Filter Statistics\n\n`applyFilters()` aggregates statistics in `Map<string, { matched: number; rejected: number }>`:\n- `rejected` increments when filter excludes file\n- `matched` increments for all filters when file passes entire chain (not per individual filter pass)\n\nStatistics emitted as `filter:applied` trace events with `filterName`, `filesMatched`, `filesRejected` fields for downstream effectiveness analysis.\n\n## Behavioral Contracts\n\n### Binary Extension Set (82 entries)\nImages: `.png`, `.jpg`, `.jpeg`, `.gif`, `.bmp`, `.ico`, `.webp`, `.svg`, `.tiff`, `.tif`, `.psd`, `.raw`, `.heif`, `.heic`\nArchives: `.zip`, `.tar`, `.gz`, `.rar`, `.7z`, `.bz2`, `.xz`, `.tgz`\nExecutables: `.exe`, `.dll`, `.so`, `.dylib`, `.bin`, `.msi`, `.app`, `.dmg`\nMedia: `.mp3`, `.mp4`, `.wav`, `.avi`, `.mov`, `.mkv`, `.flac`, `.ogg`, `.webm`, `.m4a`, `.aac`, `.wma`, `.wmv`, `.flv`\nDocuments: `.pdf`, `.doc`, `.docx`, `.xls`, `.xlsx`, `.ppt`, `.pptx`, `.odt`, `.ods`, `.odp`\nFonts: `.woff`, `.woff2`, `.ttf`, `.eot`, `.otf`\nBytecode: `.class`, `.pyc`, `.pyo`, `.o`, `.obj`, `.a`, `.lib`, `.wasm`\nDatabases: `.db`, `.sqlite`, `.sqlite3`, `.mdb`\nMisc: `.ico`, `.icns`, `.cur`, `.deb`, `.rpm`, `.jar`, `.war`, `.ear`\n\n### Default Vendor Directories (10 entries)\n`node_modules`, `vendor`, `.git`, `dist`, `build`, `__pycache__`, `.next`, `venv`, `.venv`, `target`\n\n### Default Max File Size\n`1024 * 1024` (1MB)\n\n### Filter Chain Concurrency\n30 workers (constant `CONCURRENCY` in `index.ts`)\n\n## Integration Points\n\nConsumed by `src/discovery/walker.ts` as composable filter chain during Phase 1 file analysis. Configured via `config.exclude` fields in `.agents-reverse-engineer/config.yaml`:\n- `patterns` → `createCustomFilter()`\n- `vendorDirs` → `createVendorFilter()`\n- `binaryExtensions` → `createBinaryFilter()` (merged with `BINARY_EXTENSIONS`)\n- `.maxFileSize` → `createBinaryFilter()`\n\nImplements `FileFilter` interface from `../types.js` with `name: string` and `shouldExclude(absolutePath: string): Promise<boolean>` signature.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 1501,
      "cacheReadTokens": 16334,
      "cacheCreationTokens": 13511,
      "latencyMs": 51588,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": []
    },
    {
      "timestamp": "2026-02-09T16:12:32.942Z",
      "prompt": "Generate AGENTS.md for directory: \"src/generation/prompts\" (prompts)\n\n## File Summaries (4 files)\n\n### builder.ts\n**Purpose:** buildFilePrompt, buildDirectoryPrompt, buildRootPrompt construct system+user prompt pairs for file analysis, director...\n\n**buildFilePrompt, buildDirectoryPrompt, buildRootPrompt construct system+user prompt pairs for file analysis, directory aggregation, and root synthesis by assembling templates with runtime context (source code, .sum summaries, AGENTS.md, import maps, project structure).**\n\n## Exported Functions\n\n**buildFilePrompt(context: PromptContext, debug = false): { system: string; user: string }**  \nConstructs file analysis prompts by substituting `{{FILE_PATH}}`, `{{CONTENT}}`, `{{LANG}}`, `{{PROJECT_PLAN_SECTION}}` placeholders in `FILE_USER_PROMPT` template. Detects language via `detectLanguage()` for syntax highlighting. Appends contextFiles section if `context.contextFiles` populated. Returns `FILE_UPDATE_SYSTEM_PROMPT` when `context.existingSum` present (incremental update mode), otherwise `FILE_SYSTEM_PROMPT`.\n\n**buildDirectoryPrompt(dirPath: string, projectRoot: string, debug = false, knownDirs?: Set<string>, projectStructure?: string, existingAgentsMd?: string): Promise<{ system: string; user: string }>**  \nAssembles directory aggregation prompts by reading all `.sum` files via `readSumFile(getSumPath(entryPath))`, collecting child `AGENTS.md` files, extracting directory imports via `extractDirectoryImports()` + `formatImportMap()`, detecting manifest files (9 types: `package.json`, `Cargo.toml`, `go.mod`, `pyproject.toml`, `pom.xml`, `build.gradle`, `Gemfile`, `composer.json`, `CMakeLists.txt`, `Makefile`), and checking for `AGENTS.local.md` or non-generated `AGENTS.md` (lacking `GENERATED_MARKER`). Returns `DIRECTORY_UPDATE_SYSTEM_PROMPT` when `existingAgentsMd` supplied (incremental), otherwise `DIRECTORY_SYSTEM_PROMPT`. Filters subdirectories against `knownDirs` set when provided to skip non-source directories. Embeds `projectStructure` in `<project-structure>` tags when available.\n\n**buildRootPrompt(projectRoot: string, debug = false): Promise<{ system: string; user: string }>**  \nConstructs root synthesis prompts by collecting all `AGENTS.md` files via `collectAgentsDocs(projectRoot)`, parsing root `package.json` for metadata (name, version, description, packageManager, scripts), and embedding synthesis constraints: \"Use ONLY the information provided below. Do NOT invent features, hooks, patterns, or APIs that are not explicitly mentioned in the AGENTS.md content.\" Returns `ROOT_SYSTEM_PROMPT` paired with user prompt containing all AGENTS.md content as `### ${relativePath}` sections, package metadata, and output requirements checklist (purpose, architecture, directories table, getting started, technologies).\n\n**detectLanguage(filePath: string): string**  \nMaps file extensions to syntax highlighting identifiers via lookup table. Supports 22 extensions: `.ts` → `'typescript'`, `.tsx` → `'tsx'`, `.js` → `'javascript'`, `.jsx` → `'jsx'`, `.py` → `'python'`, `.rb` → `'ruby'`, `.go` → `'go'`, `.rs` → `'rust'`, `.java` → `'java'`, `.kt` → `'kotlin'`, `.swift` → `'swift'`, `.cs` → `'csharp'`, `.php` → `'php'`, `.vue` → `'vue'`, `.svelte` → `'svelte'`, `.json` → `'json'`, `.yaml`/`.yml` → `'yaml'`, `.md` → `'markdown'`, `.css` → `'css'`, `.scss` → `'scss'`, `.html` → `'html'`. Defaults to `'text'` for unknown extensions.\n\n## Internal Helpers\n\n**logTemplate(debug: boolean, action: string, filePath: string, extra?: string): void**  \nEmits debug logs to stderr with format: `[prompt] ${action} → ${relativePath} ${extra}`. Only executes when `debug=true`. Uses picocolors for formatting: `pc.dim()` for brackets/arrows, `pc.cyan()` for action labels.\n\n## Integration Points\n\nImports template constants from `./templates.js`: `FILE_SYSTEM_PROMPT`, `FILE_USER_PROMPT`, `FILE_UPDATE_SYSTEM_PROMPT`, `DIRECTORY_SYSTEM_PROMPT`, `DIRECTORY_UPDATE_SYSTEM_PROMPT`, `ROOT_SYSTEM_PROMPT`. Calls `readSumFile()` and `getSumPath()` from `../writers/sum.js` to retrieve `.sum` file metadata+content. References `GENERATED_MARKER` from `../writers/agents-md.js` to detect user-authored vs. generated AGENTS.md. Invokes `extractDirectoryImports()` + `formatImportMap()` from `../../imports/index.js` to build verified import maps. Calls `collectAgentsDocs(projectRoot)` from `../collector.js` to aggregate all AGENTS.md files recursively.\n\n## Behavioral Contracts\n\n**Source file extension regex for import extraction:**  \n`/\\.(ts|tsx|js|jsx|py|go|rs|java|kt)$/` — only files matching these extensions processed by `extractDirectoryImports()`.\n\n**Manifest file names for package root detection:**  \n`['package.json', 'Cargo.toml', 'go.mod', 'pyproject.toml', 'pom.xml', 'build.gradle', 'Gemfile', 'composer.json', 'CMakeLists.txt', 'Makefile']` — presence triggers \"likely a package or project root\" hint in directory prompts.\n\n**File summary format in directory prompts:**  \n```\n### ${fileName}\n**Purpose:** ${metadata.purpose}\n\n${summaryContent}\n```\n\n**Subdirectory section format:**  \n```\n### ${dirName}/\n${childAgentsMdContent}\n```\n\n**User notes section header:**  \n`\"## User Notes (AGENTS.local.md)\"` when AGENTS.local.md exists, `\"## User Notes (existing AGENTS.md)\"` when pre-existing non-generated AGENTS.md detected on first run.\n\n**Incremental update instruction appended to prompts:**  \n`\"## Existing Summary (update this — preserve stable content, modify only what changed)\"` for file prompts with `context.existingSum`, `\"## Existing AGENTS.md (update this — preserve stable content, modify only what changed)\"` for directory prompts with `existingAgentsMd`.\n\n**Root prompt synthesis constraint:**  \n`\"Use ONLY the information provided below. Do NOT invent features, hooks, patterns, or APIs that are not explicitly mentioned in the AGENTS.md content.\"` — enforces faithful synthesis without hallucination.\n\n**Root prompt output requirements:**  \n```\nThe document MUST include:\n- Project purpose and description\n- Architecture overview with directory structure\n- Key directories table\n- Getting started (install, build, run commands)\n- Key technologies and dependencies\n```\n\n**Root prompt de-duplication directive:**  \n`\"Individual file details belong in directory-level AGENTS.md files — reference them, don't duplicate them.\"` — prevents redundant file-level detail in root document.\n\n## Data Flow\n\n1. **File analysis:** `buildFilePrompt()` receives `PromptContext` (filePath, content, contextFiles, existingSum), detects language, substitutes template placeholders, appends related files section, returns system+user pair selecting update vs. fresh template based on `existingSum` presence.\n\n2. **Directory aggregation:** `buildDirectoryPrompt()` reads directory via `readdir()`, filters known directories via `knownDirs` set, reads `.sum` files in parallel via `Promise.all()`, reads child `AGENTS.md` files in parallel, extracts imports from source files matching `/\\.(ts|tsx|js|jsx|py|go|rs|java|kt)$/`, detects manifest files, checks for user documentation (AGENTS.local.md or non-generated AGENTS.md), assembles sections in order: file summaries, import map, project structure, subdirectories, directory hints, user notes, existing AGENTS.md (update mode), returns system+user pair with `DIRECTORY_UPDATE_SYSTEM_PROMPT` if `existingAgentsMd` supplied.\n\n3. **Root synthesis:** `buildRootPrompt()` calls `collectAgentsDocs()` to recursively gather all `AGENTS.md` files, reads root `package.json` and extracts metadata fields (name, version, description, packageManager, scripts), embeds all AGENTS.md content as `### ${relativePath}` sections, appends package metadata, includes synthesis constraints and output requirements checklist, returns `ROOT_SYSTEM_PROMPT` + user prompt.\n\n## Error Handling\n\nSilent fallback on missing files: `try/catch` blocks around `readFile()` for child AGENTS.md, AGENTS.local.md, existing AGENTS.md, package.json — logs skipped files to stderr when `debug=true` but proceeds without throwing. `readSumFile()` returns null on missing `.sum` files, filtered out via `.filter((r): r is string => r !== null)` type guard.\n### index.ts\n**Purpose:** index.ts re-exports prompt construction functions, type definitions, and validation guidelines from the generation/pr...\n\n**index.ts re-exports prompt construction functions, type definitions, and validation guidelines from the generation/prompts module, serving as the public interface for three-phase documentation generation prompt engineering.**\n\n## Exported Types\n\n- `PromptContext` — Type re-exported from `./types.js`, contains contextual data passed to prompt builders (file metadata, import maps, directory structure, aggregated child content)\n- `SUMMARY_GUIDELINES` — Constant re-exported from `./types.js`, contains the behavioral contract text embedded in all AI prompts defining documentation density rules, identifier preservation requirements, and forbidden filler phrases\n\n## Exported Functions\n\n- `buildFilePrompt(context: PromptContext): string` — Re-exported from `./builder.js`, constructs Phase 1 prompts for individual file analysis including import maps and language-specific guidance\n- `buildDirectoryPrompt(context: PromptContext): string` — Re-exported from `./builder.js`, constructs Phase 2 prompts for directory-level `AGENTS.md` aggregation consuming child `.sum` files and subdirectory `AGENTS.md` content\n- `buildRootPrompt(context: PromptContext): string` — Re-exported from `./builder.js`, constructs Phase 3 prompts for root integration document synthesis (`CLAUDE.md`, `GEMINI.md`, `OPENCODE.md`) consuming all `AGENTS.md` files\n- `detectLanguage(filePath: string): string` — Re-exported from `./builder.js`, maps file extensions to language names for prompt customization (e.g., `.ts` → `\"TypeScript\"`, `.py` → `\"Python\"`)\n\n## Module Architecture\n\nThis barrel export consolidates the prompt engineering subsystem, separating public API (`index.ts`) from implementation details across three internal modules:\n- `types.ts` — Type definitions and embedded guideline text\n- `builder.ts` — Prompt construction logic with phase-specific templating\n- `templates.ts` — Raw prompt text fragments (not re-exported, internal only)\n\n## Integration Points\n\nConsumed by `src/generation/executor.ts` which invokes `buildFilePrompt()`, `buildDirectoryPrompt()`, and `buildRootPrompt()` during three-phase pipeline execution, passing `PromptContext` instances populated by `src/generation/collector.ts` with aggregated content, import maps from `src/imports/extractor.ts`, and manifest detection results.\n### templates.ts\n**Purpose:** templates.ts defines system and user prompt templates for AI-driven documentation generation across three pipeline ph...\n\n**templates.ts defines system and user prompt templates for AI-driven documentation generation across three pipeline phases: file analysis (FILE_SYSTEM_PROMPT, FILE_USER_PROMPT), directory aggregation (DIRECTORY_SYSTEM_PROMPT), and root synthesis (ROOT_SYSTEM_PROMPT), plus incremental update variants (FILE_UPDATE_SYSTEM_PROMPT, DIRECTORY_UPDATE_SYSTEM_PROMPT).**\n\n## Exported Constants\n\n- `FILE_SYSTEM_PROMPT: string` — System prompt for per-file `.sum` generation instructing AI to produce identifier-rich summaries with mandatory density rules, anchor term preservation, and behavioral contract extraction\n- `FILE_USER_PROMPT: string` — User prompt template for file analysis containing placeholders `{{FILE_PATH}}` and `{{CONTENT}}`, injected with project structure tree and file source code\n- `DIRECTORY_SYSTEM_PROMPT: string` — System prompt for `AGENTS.md` generation enforcing navigational index structure, adaptive section selection, path accuracy constraints, and behavioral contract aggregation\n- `FILE_UPDATE_SYSTEM_PROMPT: string` — Incremental update variant of FILE_SYSTEM_PROMPT instructing AI to preserve unchanged sections verbatim and modify only code-affected content\n- `DIRECTORY_UPDATE_SYSTEM_PROMPT: string` — Incremental update variant of DIRECTORY_SYSTEM_PROMPT instructing AI to preserve structure and modify only entries for changed files/subdirectories\n- `ROOT_SYSTEM_PROMPT: string` — System prompt for root document synthesis (CLAUDE.md, GEMINI.md, OPENCODE.md) constraining AI to synthesize only from provided AGENTS.md content without invention\n\n## Behavioral Contracts\n\n### File Analysis Density Rules\nFILE_SYSTEM_PROMPT mandates: \"Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\" and bans filler phrases: `\"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"`. Enforces pattern `\"[ExportName] does X\"` instead of `\"The ExportName function is responsible for doing X\"`.\n\n### Anchor Term Preservation\nFILE_SYSTEM_PROMPT requires: \"All exported function/class/type/const names MUST appear in the summary exactly as written in source\", \"Preserve exact casing of identifiers (e.g., buildAgentsMd, not 'build agents md')\", \"Missing any exported identifier is a failure\".\n\n### Output Format Requirements\nFILE_SYSTEM_PROMPT enforces: \"Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\", \"Do NOT include any preamble, thinking, or meta-commentary\", \"Do NOT say 'Here is...', 'Now I'll...', 'Based on my analysis...', 'Let me create...', 'Perfect.'\", \"Your response IS the documentation — not a message about the documentation\".\n\n### Behavioral Contract Extraction\nFILE_SYSTEM_PROMPT section \"BEHAVIORAL CONTRACTS (NEVER EXCLUDE)\" lists verbatim patterns required: \"Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\", \"Format strings, output templates, serialization structures — show exact format\", \"Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\", \"Prompt text or template strings that control AI/LLM behavior\", \"Error message patterns and error code strings used for matching\", \"Environment variable names and their expected values\", \"File format specifications (YAML frontmatter schemas, NDJSON line formats)\". States: \"These define observable behavior that must be reproduced exactly.\"\n\n### Directory Path Accuracy\nDIRECTORY_SYSTEM_PROMPT section \"PATH ACCURACY (MANDATORY)\" enforces: \"When referencing files or modules outside this directory, use ONLY paths from the 'Import Map' section\", \"Do NOT invent, rename, or guess module paths — if a path isn't in the Import Map, don't reference it\", \"Use the exact directory names from 'Project Directory Structure' — do NOT rename directories (e.g., if the directory is called 'cli', write 'src/cli/', NOT 'src/commands/')\", \"Cross-module references must use the specifier format from actual import statements (e.g., '../generation/writers/sum.js', NOT '../fs/sum-file.js')\", \"If you are unsure about a path, omit the cross-reference rather than guessing\".\n\n### Directory Output Format\nDIRECTORY_SYSTEM_PROMPT requires: \"Output ONLY the raw markdown content. No code fences, no preamble, no explanations, no conversational text. Your entire response IS the AGENTS.md file content.\", \"First line MUST be exactly: `<!-- Generated by agents-reverse-engineer -->`\".\n\n### Incremental Update Preservation\nFILE_UPDATE_SYSTEM_PROMPT mandates: \"Preserve the structure, section headings, and phrasing of the existing summary wherever the underlying code is unchanged\", \"Only modify content that is directly affected by the code changes\", \"If a section describes code that has not changed, keep it VERBATIM — do not rephrase, reorganize, or 'improve' stable text\", \"Regex patterns, format strings, magic constants, and template content from the existing summary MUST be preserved verbatim unless the source code changed them\", \"Never summarize or paraphrase regex patterns — always show the exact pattern in backticks\".\n\n### Root Synthesis Constraints\nROOT_SYSTEM_PROMPT enforces: \"Synthesize ONLY from the AGENTS.md content provided in the user prompt\", \"Do NOT invent, extrapolate, or hallucinate features, hooks, APIs, patterns, or dependencies not explicitly mentioned\", \"If information is missing, omit that section rather than guessing\", \"Every claim must be traceable to a specific AGENTS.md file provided\", \"Output ONLY the raw markdown content\", \"Do NOT include any conversational text, preamble, or meta-commentary\", \"Do NOT say 'Here is...' or 'I've generated...' — just output the document itself\".\n\n## Template Structure\n\nFILE_USER_PROMPT contains three interpolation regions: project structure tree embedded in `<project-structure>` tags, file path substitution via `{{FILE_PATH}}` placeholder, and source code injection via `{{CONTENT}}` placeholder within triple-backtick TypeScript code fence. Template concludes with instruction: \"Lead with a single bold purpose statement: **[FileName] does X.** Then use ## headings to organize the remaining content.\"\n\nDIRECTORY_SYSTEM_PROMPT section \"ADAPTIVE SECTIONS\" lists optional documentation sections: \"Contents (group files by purpose with markdown links and one-line descriptions)\", \"Subdirectories (list with links and summaries)\", \"Architecture / Data Flow (pipelines, request/response chains, layered architecture)\", \"Stack (technology stack for package roots with package.json/Cargo.toml/go.mod)\", \"Structure (layout conventions: feature-sliced, domain-driven, MVC)\", \"Patterns (factory, strategy, middleware, barrel re-export)\", \"Configuration (config files, schemas, environment definitions)\", \"API Surface (barrel index, route definitions, SDK interface contracts)\", \"File Relationships (collaboration, dependencies, shared state)\", \"Behavioral Contracts (regex patterns, format specifications, magic constants, template strings — MANDATORY when file summaries contain behavioral artifacts)\".\n\n## Integration Points\n\nTemplates consumed by `buildFilePrompt()` and `buildDirectoryPrompt()` in `src/generation/prompts/builder.ts`, which substitute placeholders with project context, source content, import maps, and directory structures. FILE_USER_PROMPT references project file tree embedded at build time. DIRECTORY_SYSTEM_PROMPT references \"Import Map\" and \"Project Directory Structure\" sections injected by prompt builder. ROOT_SYSTEM_PROMPT consumed by root document generation phase expecting aggregated AGENTS.md content from directory traversal.\n### types.ts\n**Purpose:** Defines prompt context structure and summary generation guidelines for AI-driven file analysis.\n\n**Defines prompt context structure and summary generation guidelines for AI-driven file analysis.**\n\n## Exported Types\n\n### PromptContext\nInterface supplying input data for prompt construction in file/directory/root analysis phases.\n\n```typescript\ninterface PromptContext {\n  filePath: string;              // Absolute path to analyzed file\n  content: string;               // Raw file content for AI processing\n  contextFiles?: Array<{         // Optional related files for cross-reference\n    path: string;\n    content: string;\n  }>;\n  projectPlan?: string;          // GENERATION-PLAN.md content for hierarchical context\n  existingSum?: string;          // Prior .sum summary text for incremental updates\n}\n```\n\nConsumed by `buildFilePrompt()`, `buildDirectoryPrompt()`, `buildRootPrompt()` in `src/generation/prompts/builder.ts` to construct AI subprocess prompts with file content, import maps, child AGENTS.md aggregations, and project metadata.\n\n### SUMMARY_GUIDELINES\nConst object encoding documentation density rules and inclusion/exclusion constraints for `.sum` generation.\n\n```typescript\nconst SUMMARY_GUIDELINES = {\n  targetLength: { min: 300, max: 500 },  // Word count range for summaries\n  include: [\n    'Purpose and responsibility',\n    'Public interface (exports, key functions)',\n    'Key patterns and notable algorithms',\n    'Dependencies with usage context',\n    'Key function signatures as code snippets',\n    'Tightly coupled sibling files',\n    'Behavioral contracts: verbatim regex patterns, format strings, magic constants, sentinel values, output templates, environment variables',\n  ],\n  exclude: [\n    'Control flow minutiae (loop structures, variable naming, temporary state)',\n    'Generic TODOs/FIXMEs (keep only security/breaking)',\n    'Broad architectural relationships (handled by AGENTS.md)',\n  ],\n}\n```\n\nEmbedded in `buildFilePrompt()` prompt template (`FILE_SUMMARY_TEMPLATE` in `src/generation/prompts/templates.ts`) to enforce consistency across AI backends (Claude Code, Gemini CLI, OpenCode). The `include` array explicitly mandates capturing behavioral contracts (regex patterns, format strings, magic constants, sentinel values, environment variable names) that define reproducible behavior.\n\n## Integration Points\n\n- **Prompt Builder**: `buildFilePrompt()` destructures `PromptContext` fields and injects `SUMMARY_GUIDELINES.include`/`exclude` arrays into template string literals.\n- **Update Workflow**: `existingSum` field populated by `readSumFile()` in `src/generation/writers/sum.ts` for incremental regeneration when `content_hash` mismatches.\n- **Template System**: `SUMMARY_GUIDELINES` constants referenced in `FILE_SUMMARY_TEMPLATE`, `DIRECTORY_AGGREGATION_TEMPLATE`, `ROOT_SYNTHESIS_TEMPLATE` via string interpolation.\n- **Quality Validation**: `targetLength` range not enforced programmatically but used as heuristic during `validateFindability()` (disabled in v0.6.5 after `publicInterface` schema field removal).\n\n## Design Rationale\n\nSeparates data contracts (`PromptContext`) from prompt engineering constraints (`SUMMARY_GUIDELINES`) to enable independent evolution. The `as const` assertion on `SUMMARY_GUIDELINES` provides TypeScript literal type inference for readonly arrays, preventing accidental mutations during template construction. The explicit enumeration of \"behavioral contracts\" in `include` array addresses phantom path and code-vs-doc inconsistencies by mandating verbatim capture of regex patterns, format strings, and sentinel values that define observable system behavior.\n\n## Import Map (verified — use these exact paths)\n\nbuilder.ts:\n  ../writers/sum.js → readSumFile, getSumPath\n  ../writers/agents-md.js → GENERATED_MARKER\n  ../../imports/index.js → extractDirectoryImports, formatImportMap\n  ../collector.js → collectAgentsDocs\n\n## Project Directory Structure\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>",
      "systemPrompt": "You are generating an AGENTS.md file — a directory-level overview for AI coding assistants.\n\nCRITICAL: Output ONLY the raw markdown content. No code fences, no preamble, no explanations, no conversational text. Your entire response IS the AGENTS.md file content.\n\nOUTPUT FORMAT:\n- First line MUST be exactly: <!-- Generated by agents-reverse-engineer -->\n- Use a # heading with the directory name\n- Write a one-paragraph purpose statement for the directory\nusage\nADAPTIVE SECTIONS:\nAnalyze the directory contents and choose the most relevant sections. Do NOT use a fixed template. Instead, select sections that best document this specific directory for an AI that needs to reconstruct or extend the project.\n\nConsider these section types (choose what applies):\n- **Contents**: Group files by purpose/category under ## headings. For each file: markdown link [filename](./filename) and a one-line description.\n- **Subdirectories**: If subdirectories exist, list them with links [dirname/](./dirname/) and brief summaries.\n- **Architecture / Data Flow**: If files form a pipeline, request/response chain, or layered architecture, document it.\n- **Stack**: If this is a package root (has package.json, Cargo.toml, go.mod, etc.), document the technology stack, key scripts, and entry points.\n- **Structure**: If the directory layout follows a convention (feature-sliced, domain-driven, MVC, etc.), document it.\n- **Patterns**: If files share recurring design patterns (factory, strategy, middleware, barrel re-export), name and document them.\n- **Configuration**: If the directory contains config files, schemas, or environment definitions, document the config surface area.\n- **API Surface**: If the directory exports a public API (barrel index, route definitions, SDK), document the interface contract.\n- **File Relationships**: How files collaborate, depend on each other, or share state.\n- **Behavioral Contracts**: If files contain regex patterns, format specifications, magic constants, or template strings that define observable behavior, collect them in a dedicated section. Preserve verbatim patterns from file summaries — do NOT paraphrase regex into prose. This section is MANDATORY when file summaries contain behavioral artifacts.\n\nChoose any relevant sections or create your own based on the directory contents. The goal is to provide a comprehensive overview that captures the essence of the directory's role in the project and how its files work together, with a focus on what an AI coding assistant would need to know to effectively interact with this code.\n\nSCOPE:\n- AGENTS.md is a NAVIGATIONAL INDEX — help an AI find the right file quickly\n- Focus on: what each file does, how files relate, directory-level patterns\n- Do NOT reproduce full architecture sections — those belong in the root CLAUDE.md\n\nPATH ACCURACY (MANDATORY):\n- When referencing files or modules outside this directory, use ONLY paths from the \"Import Map\" section\n- Do NOT invent, rename, or guess module paths — if a path isn't in the Import Map, don't reference it\n- Use the exact directory names from \"Project Directory Structure\" — do NOT rename directories\n  (e.g., if the directory is called \"cli\", write \"src/cli/\", NOT \"src/commands/\")\n- Cross-module references must use the specifier format from actual import statements\n  (e.g., \"../generation/writers/sum.js\", NOT \"../fs/sum-file.js\")\n- If you are unsure about a path, omit the cross-reference rather than guessing\n\nCONSISTENCY (MANDATORY):\n- Do not contradict yourself within the same document\n- If you describe a technique (e.g., \"regex-based\"), do not call it something else later (e.g., \"AST-based\")\n- When stating version numbers, engines, or config fields, use ONLY values present in the file summaries\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this directory\", \"this module\", \"provides\", \"responsible for\", \"is used to\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Per-file descriptions in Contents sections: 1-2 sentences maximum. Reference key symbols but do not reproduce full summaries.\n- Behavioral contracts (regex patterns, format specs, constants) belong in a separate Behavioral Contracts section, not in per-file descriptions.\n- Subdirectory descriptions: 1-2 sentences maximum. Capture the directory's role, not its full contents.\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- Key exported symbols from file summaries MUST appear in the directory overview\n- Preserve exact casing of identifiers\n\nUSER NOTES:\n- If \"User Notes\" are provided in the prompt, they contain user-defined instructions that will be automatically prepended to your output\n- Do NOT repeat or paraphrase user notes in your generated content — they are included separately\n- You may reference information from user notes for context",
      "response": "<!-- Generated by agents-reverse-engineer -->\n\n# generation/prompts\n\n**This directory exports `buildFilePrompt()`, `buildDirectoryPrompt()`, `buildRootPrompt()` for constructing system+user prompt pairs fed to AI CLI subprocesses during three-phase documentation generation: per-file `.sum` analysis, directory-level `AGENTS.md` aggregation, and root integration document synthesis (`CLAUDE.md`, `GEMINI.md`, `OPENCODE.md`).**\n\n## Contents\n\n### [builder.ts](./builder.ts)\nImplements `buildFilePrompt()` (substitutes `{{FILE_PATH}}`, `{{CONTENT}}`, `{{LANG}}`, `{{PROJECT_PLAN_SECTION}}` placeholders, appends contextFiles, selects `FILE_UPDATE_SYSTEM_PROMPT` when `context.existingSum` present), `buildDirectoryPrompt()` (reads `.sum` files via `readSumFile()`, collects child `AGENTS.md`, extracts imports via `extractDirectoryImports()`, detects 9 manifest types: `package.json`, `Cargo.toml`, `go.mod`, `pyproject.toml`, `pom.xml`, `build.gradle`, `Gemfile`, `composer.json`, `CMakeLists.txt`, `Makefile`), `buildRootPrompt()` (calls `collectAgentsDocs()`, parses root `package.json`, embeds synthesis constraints), and `detectLanguage()` (maps 22 file extensions to syntax identifiers).\n\n### [index.ts](./index.ts)\nBarrel export re-exporting `PromptContext`, `SUMMARY_GUIDELINES`, `buildFilePrompt()`, `buildDirectoryPrompt()`, `buildRootPrompt()`, `detectLanguage()` from `types.ts` and `builder.ts` to form public API consumed by `src/generation/executor.ts`.\n\n### [templates.ts](./templates.ts)\nDefines `FILE_SYSTEM_PROMPT` (density rules, anchor term preservation, behavioral contract extraction), `FILE_USER_PROMPT` (placeholders: `{{FILE_PATH}}`, `{{CONTENT}}`), `DIRECTORY_SYSTEM_PROMPT` (path accuracy constraints, adaptive section selection), `FILE_UPDATE_SYSTEM_PROMPT` (incremental preservation directives), `DIRECTORY_UPDATE_SYSTEM_PROMPT` (structure preservation), `ROOT_SYSTEM_PROMPT` (synthesis constraints forbidding invention).\n\n### [types.ts](./types.ts)\nExports `PromptContext` (fields: `filePath`, `content`, `contextFiles`, `projectPlan`, `existingSum`) consumed by prompt builders and `SUMMARY_GUIDELINES` (targetLength: 300-500 words, include array mandating behavioral contracts: verbatim regex patterns, format strings, magic constants, sentinel values, environment variables).\n\n## Architecture\n\n### Three-Phase Prompt Construction\n\n**Phase 1: File Analysis**  \n`buildFilePrompt()` receives `PromptContext`, detects language via extension lookup (`detectLanguage()`), substitutes placeholders in `FILE_USER_PROMPT` template, appends related files section when `contextFiles` populated, selects `FILE_UPDATE_SYSTEM_PROMPT` for incremental updates (when `existingSum` present) or `FILE_SYSTEM_PROMPT` for fresh analysis.\n\n**Phase 2: Directory Aggregation**  \n`buildDirectoryPrompt()` reads directory via `readdir()`, filters against `knownDirs` set (skips non-source directories), reads `.sum` files in parallel via `Promise.all(readSumFile(getSumPath(entryPath)))`, collects child `AGENTS.md`, extracts imports from source files matching `/\\.(ts|tsx|js|jsx|py|go|rs|java|kt)$/` via `extractDirectoryImports()`, detects manifest files, checks for user documentation (AGENTS.local.md or non-generated AGENTS.md lacking `GENERATED_MARKER`), assembles sections: file summaries, import map via `formatImportMap()`, project structure, subdirectories, directory hints, user notes, existing AGENTS.md (update mode), returns `DIRECTORY_UPDATE_SYSTEM_PROMPT` when `existingAgentsMd` supplied.\n\n**Phase 3: Root Synthesis**  \n`buildRootPrompt()` calls `collectAgentsDocs()` to recursively gather all `AGENTS.md` files, reads root `package.json` and extracts metadata (name, version, description, packageManager, scripts), embeds all AGENTS.md content as `### ${relativePath}` sections, appends package metadata, includes synthesis constraint: \"Use ONLY the information provided below. Do NOT invent features, hooks, patterns, or APIs that are not explicitly mentioned in the AGENTS.md content.\"\n\n### Template Substitution Pattern\n\nAll prompts follow structure: system prompt (behavioral constraints) + user prompt (context injection). Builder functions perform string interpolation with runtime data:\n- `buildFilePrompt()` injects file path, source code, language identifier, project plan section\n- `buildDirectoryPrompt()` injects `.sum` summaries, import maps, child AGENTS.md, manifest hints, user notes\n- `buildRootPrompt()` injects all AGENTS.md content, package metadata, output requirements checklist\n\n## Behavioral Contracts\n\n### Density Rules (FILE_SYSTEM_PROMPT)\n`\"Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\"`. Banned filler phrases: `\"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"`.\n\n### Anchor Term Preservation (FILE_SYSTEM_PROMPT)\n`\"All exported function/class/type/const names MUST appear in the summary exactly as written in source\"`, `\"Preserve exact casing of identifiers\"`, `\"Missing any exported identifier is a failure\"`.\n\n### Output Format Requirements (FILE_SYSTEM_PROMPT, DIRECTORY_SYSTEM_PROMPT)\n`\"Start your response DIRECTLY with the purpose statement\"` for file prompts. `\"Output ONLY the raw markdown content. No code fences, no preamble\"` for directory prompts. `\"First line MUST be exactly: <!-- Generated by agents-reverse-engineer -->\"` for AGENTS.md.\n\n### Behavioral Contract Extraction (FILE_SYSTEM_PROMPT)\nRequired captures: `\"Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\"`, `\"Format strings, output templates, serialization structures — show exact format\"`, `\"Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\"`, `\"Prompt text or template strings that control AI/LLM behavior\"`, `\"Error message patterns and error code strings\"`, `\"Environment variable names and their expected values\"`, `\"File format specifications (YAML frontmatter schemas, NDJSON line formats)\"`.\n\n### Path Accuracy Constraints (DIRECTORY_SYSTEM_PROMPT)\n`\"When referencing files or modules outside this directory, use ONLY paths from the 'Import Map' section\"`, `\"Do NOT invent, rename, or guess module paths\"`, `\"Use the exact directory names from 'Project Directory Structure' — do NOT rename directories\"`, `\"Cross-module references must use the specifier format from actual import statements\"`, `\"If you are unsure about a path, omit the cross-reference rather than guessing\"`.\n\n### Incremental Update Preservation (FILE_UPDATE_SYSTEM_PROMPT, DIRECTORY_UPDATE_SYSTEM_PROMPT)\n`\"Preserve the structure, section headings, and phrasing of the existing summary wherever the underlying code is unchanged\"`, `\"Only modify content that is directly affected by the code changes\"`, `\"Regex patterns, format strings, magic constants, and template content from the existing summary MUST be preserved verbatim unless the source code changed them\"`, `\"Never summarize or paraphrase regex patterns — always show the exact pattern in backticks\"`.\n\n### Root Synthesis Constraints (ROOT_SYSTEM_PROMPT)\n`\"Synthesize ONLY from the AGENTS.md content provided in the user prompt\"`, `\"Do NOT invent, extrapolate, or hallucinate features, hooks, APIs, patterns, or dependencies not explicitly mentioned\"`, `\"Every claim must be traceable to a specific AGENTS.md file provided\"`.\n\n### Source File Extension Filter (builder.ts)\n`/\\.(ts|tsx|js|jsx|py|go|rs|java|kt)$/` — only files matching these extensions processed by `extractDirectoryImports()` for import map generation.\n\n### Manifest Detection Pattern (builder.ts)\n`['package.json', 'Cargo.toml', 'go.mod', 'pyproject.toml', 'pom.xml', 'build.gradle', 'Gemfile', 'composer.json', 'CMakeLists.txt', 'Makefile']` — presence triggers \"likely a package or project root\" hint in directory prompts.\n\n### Language Extension Map (builder.ts detectLanguage())\n22 extensions: `.ts` → `'typescript'`, `.tsx` → `'tsx'`, `.js` → `'javascript'`, `.jsx` → `'jsx'`, `.py` → `'python'`, `.rb` → `'ruby'`, `.go` → `'go'`, `.rs` → `'rust'`, `.java` → `'java'`, `.kt` → `'kotlin'`, `.swift` → `'swift'`, `.cs` → `'csharp'`, `.php` → `'php'`, `.vue` → `'vue'`, `.svelte` → `'svelte'`, `.json` → `'json'`, `.yaml`/`.yml` → `'yaml'`, `.md` → `'markdown'`, `.css` → `'css'`, `.scss` → `'scss'`, `.html` → `'html'`. Defaults to `'text'`.\n\n## File Relationships\n\n`builder.ts` calls `readSumFile()` + `getSumPath()` from `../writers/sum.js` to retrieve `.sum` frontmatter+content, references `GENERATED_MARKER` from `../writers/agents-md.js` to detect user-authored vs. generated AGENTS.md, invokes `extractDirectoryImports()` + `formatImportMap()` from `../../imports/index.js` for verified import maps, calls `collectAgentsDocs()` from `../collector.js` to aggregate AGENTS.md recursively. `templates.ts` provides raw prompt text consumed by `builder.ts` via import. `types.ts` defines `PromptContext` structure threaded through `builder.ts` functions. `index.ts` re-exports public API consumed by `src/generation/executor.ts` during three-phase pipeline execution.\n\n## Integration Points\n\nConsumed by `src/generation/executor.ts` which invokes prompt builders during phase execution: `buildFilePrompt()` called for each file in Phase 1 concurrent pool, `buildDirectoryPrompt()` called for each directory in Phase 2 post-order traversal, `buildRootPrompt()` called once per root document in Phase 3 sequential execution. Prompts passed as `{ system, user }` pairs to `AIService.call()` in `src/ai/service.ts` which spawns CLI subprocesses (`claude`, `gemini`, `opencode`) with resource limits (`NODE_OPTIONS='--max-old-space-size=512'`, `UV_THREADPOOL_SIZE='4'`, `CLAUDE_CODE_DISABLE_BACKGROUND_TASKS='1'`, `--disallowedTools Task`).",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 2764,
      "cacheReadTokens": 16334,
      "cacheCreationTokens": 15799,
      "latencyMs": 65853,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": []
    },
    {
      "timestamp": "2026-02-09T16:13:38.853Z",
      "prompt": "Generate AGENTS.md for directory: \"src/types\" (types)\n\n## File Summaries (1 files)\n\n### index.ts\n**Purpose:** index.ts defines shared TypeScript interfaces for file discovery results, exclusion tracking, and statistics aggregat...\n\n**index.ts defines shared TypeScript interfaces for file discovery results, exclusion tracking, and statistics aggregation used across ARE modules.**\n\n## Exported Interfaces\n\n### ExcludedFile\nRepresents a file excluded during discovery with metadata:\n- `path: string` — Absolute or relative path to excluded file\n- `reason: string` — Exclusion rationale (e.g., \"gitignore pattern\", \"binary file\", \"vendor directory\")\n\n### DiscoveryResult\nOutput contract for file discovery process:\n- `files: string[]` — Paths of files selected for analysis\n- `excluded: ExcludedFile[]` — Files rejected with explanatory reasons\n\n### DiscoveryStats\nAggregated metrics from discovery execution:\n- `totalFiles: number` — Count of all files encountered\n- `includedFiles: number` — Count selected for analysis\n- `excludedFiles: number` — Count rejected by filters\n- `exclusionReasons: Record<string, number>` — Histogram mapping reason strings to occurrence counts\n\n## Integration Points\n\nConsumed by `src/discovery/walker.ts` as return types for directory traversal functions. Used by `src/cli/discover.ts` to format console output and write `GENERATION-PLAN.md`. Referenced in `src/orchestration/runner.ts` for telemetry logging of discovery phase metrics.\n\n## Project Directory Structure\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>",
      "systemPrompt": "You are generating an AGENTS.md file — a directory-level overview for AI coding assistants.\n\nCRITICAL: Output ONLY the raw markdown content. No code fences, no preamble, no explanations, no conversational text. Your entire response IS the AGENTS.md file content.\n\nOUTPUT FORMAT:\n- First line MUST be exactly: <!-- Generated by agents-reverse-engineer -->\n- Use a # heading with the directory name\n- Write a one-paragraph purpose statement for the directory\nusage\nADAPTIVE SECTIONS:\nAnalyze the directory contents and choose the most relevant sections. Do NOT use a fixed template. Instead, select sections that best document this specific directory for an AI that needs to reconstruct or extend the project.\n\nConsider these section types (choose what applies):\n- **Contents**: Group files by purpose/category under ## headings. For each file: markdown link [filename](./filename) and a one-line description.\n- **Subdirectories**: If subdirectories exist, list them with links [dirname/](./dirname/) and brief summaries.\n- **Architecture / Data Flow**: If files form a pipeline, request/response chain, or layered architecture, document it.\n- **Stack**: If this is a package root (has package.json, Cargo.toml, go.mod, etc.), document the technology stack, key scripts, and entry points.\n- **Structure**: If the directory layout follows a convention (feature-sliced, domain-driven, MVC, etc.), document it.\n- **Patterns**: If files share recurring design patterns (factory, strategy, middleware, barrel re-export), name and document them.\n- **Configuration**: If the directory contains config files, schemas, or environment definitions, document the config surface area.\n- **API Surface**: If the directory exports a public API (barrel index, route definitions, SDK), document the interface contract.\n- **File Relationships**: How files collaborate, depend on each other, or share state.\n- **Behavioral Contracts**: If files contain regex patterns, format specifications, magic constants, or template strings that define observable behavior, collect them in a dedicated section. Preserve verbatim patterns from file summaries — do NOT paraphrase regex into prose. This section is MANDATORY when file summaries contain behavioral artifacts.\n\nChoose any relevant sections or create your own based on the directory contents. The goal is to provide a comprehensive overview that captures the essence of the directory's role in the project and how its files work together, with a focus on what an AI coding assistant would need to know to effectively interact with this code.\n\nSCOPE:\n- AGENTS.md is a NAVIGATIONAL INDEX — help an AI find the right file quickly\n- Focus on: what each file does, how files relate, directory-level patterns\n- Do NOT reproduce full architecture sections — those belong in the root CLAUDE.md\n\nPATH ACCURACY (MANDATORY):\n- When referencing files or modules outside this directory, use ONLY paths from the \"Import Map\" section\n- Do NOT invent, rename, or guess module paths — if a path isn't in the Import Map, don't reference it\n- Use the exact directory names from \"Project Directory Structure\" — do NOT rename directories\n  (e.g., if the directory is called \"cli\", write \"src/cli/\", NOT \"src/commands/\")\n- Cross-module references must use the specifier format from actual import statements\n  (e.g., \"../generation/writers/sum.js\", NOT \"../fs/sum-file.js\")\n- If you are unsure about a path, omit the cross-reference rather than guessing\n\nCONSISTENCY (MANDATORY):\n- Do not contradict yourself within the same document\n- If you describe a technique (e.g., \"regex-based\"), do not call it something else later (e.g., \"AST-based\")\n- When stating version numbers, engines, or config fields, use ONLY values present in the file summaries\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this directory\", \"this module\", \"provides\", \"responsible for\", \"is used to\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Per-file descriptions in Contents sections: 1-2 sentences maximum. Reference key symbols but do not reproduce full summaries.\n- Behavioral contracts (regex patterns, format specs, constants) belong in a separate Behavioral Contracts section, not in per-file descriptions.\n- Subdirectory descriptions: 1-2 sentences maximum. Capture the directory's role, not its full contents.\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- Key exported symbols from file summaries MUST appear in the directory overview\n- Preserve exact casing of identifiers\n\nUSER NOTES:\n- If \"User Notes\" are provided in the prompt, they contain user-defined instructions that will be automatically prepended to your output\n- Do NOT repeat or paraphrase user notes in your generated content — they are included separately\n- You may reference information from user notes for context",
      "response": "<!-- Generated by agents-reverse-engineer -->\n\n# src/types\n\nShared TypeScript interface definitions for file discovery results, exclusion tracking, and statistics aggregation across ARE modules.\n\n## Contents\n\n### [index.ts](./index.ts)\nExports `ExcludedFile` (path + reason), `DiscoveryResult` (files + excluded arrays), `DiscoveryStats` (totalFiles, includedFiles, excludedFiles, exclusionReasons histogram).\n\n## Usage Pattern\n\n`src/discovery/walker.ts` returns `DiscoveryResult` from traversal functions. `src/cli/discover.ts` consumes `DiscoveryResult` to format console output and write `GENERATION-PLAN.md`. `src/orchestration/runner.ts` logs `DiscoveryStats` to telemetry during discovery phase.\n\n## Data Model\n\n### ExcludedFile\n- `path: string` — Absolute/relative file path\n- `reason: string` — Exclusion rationale (\"gitignore pattern\", \"binary file\", \"vendor directory\")\n\n### DiscoveryResult\n- `files: string[]` — Selected for analysis\n- `excluded: ExcludedFile[]` — Rejected with reasons\n\n### DiscoveryStats\n- `totalFiles: number`, `includedFiles: number`, `excludedFiles: number` — Counts\n- `exclusionReasons: Record<string, number>` — Reason → occurrence count map",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 337,
      "cacheReadTokens": 17477,
      "cacheCreationTokens": 9324,
      "latencyMs": 30775,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": []
    },
    {
      "timestamp": "2026-02-09T16:13:38.840Z",
      "prompt": "Generate AGENTS.md for directory: \"src/output\" (output)\n\n## File Summaries (1 files)\n\n### logger.ts\n**Purpose:** Terminal logging facade with picocolors-based output formatting for CLI discovery, exclusion, summary, warning, and e...\n\n**Terminal logging facade with picocolors-based output formatting for CLI discovery, exclusion, summary, warning, and error messages.**\n\n## Exported Interface\n\n`Logger` interface defines six methods:\n- `info(message: string): void` — Log informational message\n- `file(path: string): void` — Log discovered file\n- `excluded(path: string, reason: string, filter: string): void` — Log excluded file with reason and filter name\n- `summary(included: number, excluded: number): void` — Log discovery summary with counts\n- `warn(message: string): void` — Log warning message\n- `error(message: string): void` — Log error message\n\n`LoggerOptions` interface:\n- `colors: boolean` — Enable ANSI color codes (default true)\n\n## Factory Functions\n\n`createLogger(options: LoggerOptions): Logger` constructs logger with conditional color support. When `options.colors` is true, uses `picocolors` (`pc`) for formatting; otherwise uses `noColor` identity functions. Output format per CONTEXT.md specification:\n- `file()`: green `\"  +\"` prefix + path\n- `excluded()`: dim `\"  -\"` prefix + path + dim `\" (${reason}: ${filter})\"`\n- `summary()`: bold `\"\\nDiscovered ${included} files\"` + dim `\" (${excluded} excluded)\"`\n- `warn()`: yellow `\"Warning: \"` prefix + message via `console.warn()`\n- `error()`: red `\"Error: \"` prefix + message via `console.error()`\n- `info()`: plain `console.log()` without formatting\n\n`createSilentLogger(): Logger` returns no-op logger with all methods as empty functions. Used for testing or programmatic usage where output suppression is required.\n\n## Internal Types\n\n`ColorFunctions` interface defines five formatting functions:\n- `green(s: string): string`\n- `dim(s: string): string`\n- `red(s: string): string`\n- `bold(s: string): string`\n- `yellow(s: string): string`\n\n`identity(s: string): string` returns input unchanged.\n\n`noColor: ColorFunctions` constant binds all five color functions to `identity` for no-color mode.\n\n## Dependencies\n\nImports `picocolors` as `pc` for ANSI color code generation. Falls back to `noColor` identity functions when `options.colors` is false.\n\n## Project Directory Structure\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>",
      "systemPrompt": "You are generating an AGENTS.md file — a directory-level overview for AI coding assistants.\n\nCRITICAL: Output ONLY the raw markdown content. No code fences, no preamble, no explanations, no conversational text. Your entire response IS the AGENTS.md file content.\n\nOUTPUT FORMAT:\n- First line MUST be exactly: <!-- Generated by agents-reverse-engineer -->\n- Use a # heading with the directory name\n- Write a one-paragraph purpose statement for the directory\nusage\nADAPTIVE SECTIONS:\nAnalyze the directory contents and choose the most relevant sections. Do NOT use a fixed template. Instead, select sections that best document this specific directory for an AI that needs to reconstruct or extend the project.\n\nConsider these section types (choose what applies):\n- **Contents**: Group files by purpose/category under ## headings. For each file: markdown link [filename](./filename) and a one-line description.\n- **Subdirectories**: If subdirectories exist, list them with links [dirname/](./dirname/) and brief summaries.\n- **Architecture / Data Flow**: If files form a pipeline, request/response chain, or layered architecture, document it.\n- **Stack**: If this is a package root (has package.json, Cargo.toml, go.mod, etc.), document the technology stack, key scripts, and entry points.\n- **Structure**: If the directory layout follows a convention (feature-sliced, domain-driven, MVC, etc.), document it.\n- **Patterns**: If files share recurring design patterns (factory, strategy, middleware, barrel re-export), name and document them.\n- **Configuration**: If the directory contains config files, schemas, or environment definitions, document the config surface area.\n- **API Surface**: If the directory exports a public API (barrel index, route definitions, SDK), document the interface contract.\n- **File Relationships**: How files collaborate, depend on each other, or share state.\n- **Behavioral Contracts**: If files contain regex patterns, format specifications, magic constants, or template strings that define observable behavior, collect them in a dedicated section. Preserve verbatim patterns from file summaries — do NOT paraphrase regex into prose. This section is MANDATORY when file summaries contain behavioral artifacts.\n\nChoose any relevant sections or create your own based on the directory contents. The goal is to provide a comprehensive overview that captures the essence of the directory's role in the project and how its files work together, with a focus on what an AI coding assistant would need to know to effectively interact with this code.\n\nSCOPE:\n- AGENTS.md is a NAVIGATIONAL INDEX — help an AI find the right file quickly\n- Focus on: what each file does, how files relate, directory-level patterns\n- Do NOT reproduce full architecture sections — those belong in the root CLAUDE.md\n\nPATH ACCURACY (MANDATORY):\n- When referencing files or modules outside this directory, use ONLY paths from the \"Import Map\" section\n- Do NOT invent, rename, or guess module paths — if a path isn't in the Import Map, don't reference it\n- Use the exact directory names from \"Project Directory Structure\" — do NOT rename directories\n  (e.g., if the directory is called \"cli\", write \"src/cli/\", NOT \"src/commands/\")\n- Cross-module references must use the specifier format from actual import statements\n  (e.g., \"../generation/writers/sum.js\", NOT \"../fs/sum-file.js\")\n- If you are unsure about a path, omit the cross-reference rather than guessing\n\nCONSISTENCY (MANDATORY):\n- Do not contradict yourself within the same document\n- If you describe a technique (e.g., \"regex-based\"), do not call it something else later (e.g., \"AST-based\")\n- When stating version numbers, engines, or config fields, use ONLY values present in the file summaries\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this directory\", \"this module\", \"provides\", \"responsible for\", \"is used to\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Per-file descriptions in Contents sections: 1-2 sentences maximum. Reference key symbols but do not reproduce full summaries.\n- Behavioral contracts (regex patterns, format specs, constants) belong in a separate Behavioral Contracts section, not in per-file descriptions.\n- Subdirectory descriptions: 1-2 sentences maximum. Capture the directory's role, not its full contents.\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- Key exported symbols from file summaries MUST appear in the directory overview\n- Preserve exact casing of identifiers\n\nUSER NOTES:\n- If \"User Notes\" are provided in the prompt, they contain user-defined instructions that will be automatically prepended to your output\n- Do NOT repeat or paraphrase user notes in your generated content — they are included separately\n- You may reference information from user notes for context",
      "response": "<!-- Generated by agents-reverse-engineer -->\n\n# src/output/\n\nTerminal output formatting layer providing dependency-injectable logger abstraction with conditional color support via picocolors.\n\n## Contents\n\n### [logger.ts](./logger.ts)\nExports `Logger` interface (six methods: `info()`, `file()`, `excluded()`, `summary()`, `warn()`, `error()`), `createLogger(options)` factory with picocolors-based ANSI formatting, and `createSilentLogger()` no-op factory for test/programmatic contexts.\n\n## Architecture\n\n**Dependency Injection Pattern**: Consumers receive `Logger` interface rather than direct `console` access, enabling output capture in tests and silence in programmatic workflows.\n\n**Conditional Color Support**: `createLogger(options)` binds `ColorFunctions` to picocolors `pc` when `options.colors` is true, otherwise to `noColor` identity functions. Single conditional at factory construction avoids per-call branching.\n\n**Output Routing**: `info()/file()/excluded()/summary()` write to `console.log()`, `warn()` to `console.warn()`, `error()` to `console.error()` for stream separation in shell redirects.\n\n## Behavioral Contracts\n\n**File Discovery Formatting**:\n- Included file: `` `  +` `` (green) + `path`\n- Excluded file: `` `  -` `` (dim) + `path` + ` (${reason}: ${filter})` (dim)\n- Summary: `Discovered ${included} files (${excluded} excluded)` (bold count, dim excluded)\n\n**Warning/Error Prefixes**:\n- `warn()`: `\"Warning: \"` (yellow) + message → stderr\n- `error()`: `\"Error: \"` (red) + message → stderr\n\n## Usage Context\n\nThreaded through `CommandRunOptions` → `DiscoveryOptions` → `run()` in `src/discovery/run.ts`. CLI commands instantiate via `createLogger({ colors: config.output.colors })`, test suites use `createSilentLogger()`.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 493,
      "cacheReadTokens": 17477,
      "cacheCreationTokens": 9585,
      "latencyMs": 35982,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": []
    },
    {
      "timestamp": "2026-02-09T16:13:38.830Z",
      "prompt": "Generate AGENTS.md for directory: \".github/workflows\" (workflows)\n\n## File Summaries (1 files)\n\n### publish.yml\n**Purpose:** GitHub Actions workflow automating npm package publication with Sigstore provenance attestation on release events.\n\n**GitHub Actions workflow automating npm package publication with Sigstore provenance attestation on release events.**\n\n## Workflow Triggers\n\nExecutes on two event patterns:\n- `release[published]` event when GitHub release published\n- `workflow_dispatch` for manual triggering via Actions UI\n\n## Job Configuration\n\n`publish` job runs on `ubuntu-latest` with permission grants:\n- `contents: read` — checkout repository access\n- `id-token: write` — enables Sigstore-signed provenance generation\n\n## Build Pipeline Steps\n\n**Checkout Step:**\n- `actions/checkout@v4` clones repository at release commit SHA\n\n**Node.js Setup:**\n- `actions/setup-node@v4` installs Node.js version 20\n- Configures npm registry URL as `https://registry.npmjs.org` via `registry-url` parameter\n\n**Dependency Installation:**\n- `npm ci` performs clean install from package-lock.json\n\n**Build Execution:**\n- `npm run build` invokes TypeScript compilation and hook copying (triggers `prepublishOnly` script: `tsc` + `build:hooks` from `scripts/build-hooks.js`)\n\n**Publication:**\n- `npm publish --provenance --access public` publishes package with cryptographic attestation\n- `--provenance` flag generates Sigstore-signed build provenance linking published tarball to source commit SHA via SLSA attestation\n- `--access public` ensures scoped package visibility\n- `NODE_AUTH_TOKEN` environment variable sourced from `secrets.NPM_TOKEN` repository secret for npm registry authentication\n\n## Security Features\n\nCryptographic provenance attestation creates verifiable supply chain metadata:\n- Links published artifact to GitHub Actions workflow execution\n- Records commit SHA, workflow file path, repository coordinates\n- Signed via Sigstore's keyless signing using GitHub's OIDC token\n- Consumers verify authenticity via `npm audit signatures` or `sigstore-js` tooling\n\n## Project Directory Structure\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>",
      "systemPrompt": "You are generating an AGENTS.md file — a directory-level overview for AI coding assistants.\n\nCRITICAL: Output ONLY the raw markdown content. No code fences, no preamble, no explanations, no conversational text. Your entire response IS the AGENTS.md file content.\n\nOUTPUT FORMAT:\n- First line MUST be exactly: <!-- Generated by agents-reverse-engineer -->\n- Use a # heading with the directory name\n- Write a one-paragraph purpose statement for the directory\nusage\nADAPTIVE SECTIONS:\nAnalyze the directory contents and choose the most relevant sections. Do NOT use a fixed template. Instead, select sections that best document this specific directory for an AI that needs to reconstruct or extend the project.\n\nConsider these section types (choose what applies):\n- **Contents**: Group files by purpose/category under ## headings. For each file: markdown link [filename](./filename) and a one-line description.\n- **Subdirectories**: If subdirectories exist, list them with links [dirname/](./dirname/) and brief summaries.\n- **Architecture / Data Flow**: If files form a pipeline, request/response chain, or layered architecture, document it.\n- **Stack**: If this is a package root (has package.json, Cargo.toml, go.mod, etc.), document the technology stack, key scripts, and entry points.\n- **Structure**: If the directory layout follows a convention (feature-sliced, domain-driven, MVC, etc.), document it.\n- **Patterns**: If files share recurring design patterns (factory, strategy, middleware, barrel re-export), name and document them.\n- **Configuration**: If the directory contains config files, schemas, or environment definitions, document the config surface area.\n- **API Surface**: If the directory exports a public API (barrel index, route definitions, SDK), document the interface contract.\n- **File Relationships**: How files collaborate, depend on each other, or share state.\n- **Behavioral Contracts**: If files contain regex patterns, format specifications, magic constants, or template strings that define observable behavior, collect them in a dedicated section. Preserve verbatim patterns from file summaries — do NOT paraphrase regex into prose. This section is MANDATORY when file summaries contain behavioral artifacts.\n\nChoose any relevant sections or create your own based on the directory contents. The goal is to provide a comprehensive overview that captures the essence of the directory's role in the project and how its files work together, with a focus on what an AI coding assistant would need to know to effectively interact with this code.\n\nSCOPE:\n- AGENTS.md is a NAVIGATIONAL INDEX — help an AI find the right file quickly\n- Focus on: what each file does, how files relate, directory-level patterns\n- Do NOT reproduce full architecture sections — those belong in the root CLAUDE.md\n\nPATH ACCURACY (MANDATORY):\n- When referencing files or modules outside this directory, use ONLY paths from the \"Import Map\" section\n- Do NOT invent, rename, or guess module paths — if a path isn't in the Import Map, don't reference it\n- Use the exact directory names from \"Project Directory Structure\" — do NOT rename directories\n  (e.g., if the directory is called \"cli\", write \"src/cli/\", NOT \"src/commands/\")\n- Cross-module references must use the specifier format from actual import statements\n  (e.g., \"../generation/writers/sum.js\", NOT \"../fs/sum-file.js\")\n- If you are unsure about a path, omit the cross-reference rather than guessing\n\nCONSISTENCY (MANDATORY):\n- Do not contradict yourself within the same document\n- If you describe a technique (e.g., \"regex-based\"), do not call it something else later (e.g., \"AST-based\")\n- When stating version numbers, engines, or config fields, use ONLY values present in the file summaries\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this directory\", \"this module\", \"provides\", \"responsible for\", \"is used to\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Per-file descriptions in Contents sections: 1-2 sentences maximum. Reference key symbols but do not reproduce full summaries.\n- Behavioral contracts (regex patterns, format specs, constants) belong in a separate Behavioral Contracts section, not in per-file descriptions.\n- Subdirectory descriptions: 1-2 sentences maximum. Capture the directory's role, not its full contents.\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- Key exported symbols from file summaries MUST appear in the directory overview\n- Preserve exact casing of identifiers\n\nUSER NOTES:\n- If \"User Notes\" are provided in the prompt, they contain user-defined instructions that will be automatically prepended to your output\n- Do NOT repeat or paraphrase user notes in your generated content — they are included separately\n- You may reference information from user notes for context",
      "response": "<!-- Generated by agents-reverse-engineer -->\n\n# .github/workflows\n\nGitHub Actions CI/CD automation for npm package publication with Sigstore-signed provenance attestation.\n\n## Contents\n\n### [publish.yml](./publish.yml)\nWorkflow automating npm publication on `release[published]` events or manual `workflow_dispatch`. Executes `actions/checkout@v4`, `actions/setup-node@v4` (Node.js 20, registry `https://registry.npmjs.org`), `npm ci`, `npm run build` (invokes `prepublishOnly`: TypeScript compilation + `build:hooks` script from `scripts/build-hooks.js`), and `npm publish --provenance --access public` with `NODE_AUTH_TOKEN` from `secrets.NPM_TOKEN`. Grants `id-token: write` permission enabling Sigstore keyless signing to generate SLSA attestation linking published tarball to source commit SHA.\n\n## CI/CD Pipeline Architecture\n\n**Trigger Strategy:**  \nExecutes on GitHub release publication or manual dispatch, ensuring controlled release cadence.\n\n**Build Steps:**  \nSequential execution: repository checkout at release commit SHA → Node.js 20 installation with npm registry configuration → clean dependency install via `npm ci` → TypeScript compilation and hook file preparation via `npm run build` → authenticated publication with cryptographic attestation.\n\n**Security Model:**  \nOIDC token-based authentication (`id-token: write`) enables Sigstore keyless signing without long-lived credentials. Generated provenance attestation creates verifiable supply chain metadata recording workflow execution context (commit SHA, repository coordinates, workflow file path) consumable via `npm audit signatures` or `sigstore-js` tooling.\n\n## Integration Points\n\n**Build System:**  \nDepends on `prepublishOnly` script in root `package.json` executing TypeScript compiler (`tsc`) and hook file copier (`scripts/build-hooks.js`). Outputs `dist/` directory tree and `hooks/dist/` session lifecycle hook bundles included in npm tarball.\n\n**Publication Authentication:**  \nRequires `NPM_TOKEN` repository secret storing npm automation token with publish permissions for package scope. Token injected as `NODE_AUTH_TOKEN` environment variable during `npm publish` execution.\n\n**Provenance Attestation:**  \n`--provenance` flag triggers GitHub Actions attestation generation via `@actions/attest-build-provenance` internal API, uploading signed SLSA v0.2 attestation to npm registry metadata storage. Consumers verify authenticity by inspecting `attestations` field in registry metadata or using `npm audit signatures` command.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 596,
      "cacheReadTokens": 17477,
      "cacheCreationTokens": 9461,
      "latencyMs": 37072,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": []
    },
    {
      "timestamp": "2026-02-09T16:13:38.882Z",
      "prompt": "Generate AGENTS.md for directory: \"src/change-detection\" (change-detection)\n\n## File Summaries (3 files)\n\n### detector.ts\n**Purpose:** detector.ts implements Git-based change detection via simple-git and SHA-256 content hashing via Node.js crypto for i...\n\n**detector.ts implements Git-based change detection via simple-git and SHA-256 content hashing via Node.js crypto for incremental documentation updates.**\n\n## Exported Functions\n\n### isGitRepo\n```typescript\nasync function isGitRepo(projectRoot: string): Promise<boolean>\n```\nChecks if `projectRoot` contains a Git repository by calling `simpleGit(projectRoot).checkIsRepo()`.\n\n### getCurrentCommit\n```typescript\nasync function getCurrentCommit(projectRoot: string): Promise<string>\n```\nReturns HEAD commit hash via `git.revparse(['HEAD'])`, trimmed to remove trailing whitespace.\n\n### getChangedFiles\n```typescript\nasync function getChangedFiles(\n  projectRoot: string,\n  baseCommit: string,\n  options: ChangeDetectionOptions = {}\n): Promise<ChangeDetectionResult>\n```\nDetects files changed between `baseCommit` and HEAD using `git diff --name-status -M` (rename detection with 50% similarity threshold). Parses diff output line-by-line with tab-separated status codes: `'A'` (added), `'M'` (modified), `'D'` (deleted), `'R'` prefix (renamed). For renames, extracts `oldPath` from `parts[1]` and new path from `parts[parts.length - 1]`. When `options.includeUncommitted` is true, merges uncommitted changes from `git.status()` arrays: `modified`, `deleted`, `not_added` (untracked), `staged`. Returns `ChangeDetectionResult` with `currentCommit`, `baseCommit`, `changes[]` array of `FileChange`, and `includesUncommitted` boolean.\n\n### computeContentHash\n```typescript\nasync function computeContentHash(filePath: string): Promise<string>\n```\nReads file content via `readFile(filePath)`, computes SHA-256 hash via `createHash('sha256').update(content).digest('hex')`, returns hex-encoded digest. Used for comparing `.sum` frontmatter `content_hash` against current file state.\n\n### computeContentHashFromString\n```typescript\nfunction computeContentHashFromString(content: string): string\n```\nSynchronous variant accepting in-memory string content, returns hex-encoded SHA-256 digest. Avoids redundant disk read when file content already loaded.\n\n## Git Diff Parsing\n\nParses `git diff --name-status -M` output with format `STATUS\\tFILE` or `STATUS\\tOLD\\tNEW` for renames. Status codes:\n- `'A'` → `status: 'added'`\n- `'M'` → `status: 'modified'`\n- `'D'` → `status: 'deleted'`\n- `'R'` prefix (e.g., `'R100'`) → `status: 'renamed'` with `oldPath` extracted from second tab-delimited part\n\n## Uncommitted Changes Merge\n\nWhen `options.includeUncommitted` is true, calls `git.status()` and merges:\n- `status.modified[]` → `status: 'modified'`\n- `status.deleted[]` → `status: 'deleted'`\n- `status.not_added[]` → `status: 'added'` (untracked files)\n- `status.staged[]` → `status: 'added'` (staged additions)\n\nDeduplicates via `changes.some(c => c.path === file)` check before pushing to `changes[]` array.\n\n## Integration with Update Workflow\n\nUsed by `src/update/orchestrator.ts` to compute `ChangeDetectionResult`, which drives incremental update decisions: hash mismatches trigger `.sum` regeneration, renamed `oldPath` values enable orphan cleanup, uncommitted changes support `--uncommitted` flag behavior.\n### index.ts\n**Purpose:** src/change-detection/index.ts exports the public API for git-based change detection, providing hash computation and f...\n\n**src/change-detection/index.ts exports the public API for git-based change detection, providing hash computation and file change detection for incremental updates.**\n\n## Exported Functions\n\n- `isGitRepo(): Promise<boolean>` — Checks if current directory is a git repository\n- `getCurrentCommit(): Promise<string>` — Returns current HEAD commit SHA\n- `getChangedFiles(options: ChangeDetectionOptions): Promise<ChangeDetectionResult>` — Detects added/modified/deleted/renamed files between commits or in working tree\n- `computeContentHash(filePath: string): Promise<string>` — Computes SHA-256 hex hash of file content at `filePath`\n- `computeContentHashFromString(content: string): string` — Computes SHA-256 hex hash of string content\n\n## Exported Types\n\n- `ChangeType` — Union literal: `'added' | 'modified' | 'deleted' | 'renamed'`\n- `FileChange` — Object with `path: string`, `status: ChangeType`, optional `oldPath?: string` (for renames), optional `contentHash?: string`\n- `ChangeDetectionResult` — Object with `changes: FileChange[]` array and `baseCommit?: string`\n- `ChangeDetectionOptions` — Configuration object for `getChangedFiles()` with optional `baseCommit?: string`, `includeUncommitted?: boolean`\n\n## Module Role\n\nActs as public-facing barrel export for `./detector.js` implementation module. All git operations (`git diff`, `git status`, rename detection via `-M` flag) and SHA-256 hashing logic reside in `detector.ts`. Consumers import from this index for change detection workflow in `src/update/orchestrator.ts` and hash verification in `.sum` YAML frontmatter processing.\n\n## Integration Points\n\n- **src/update/orchestrator.ts**: Calls `getChangedFiles()` with `includeUncommitted` flag from CLI `--uncommitted` option, compares `FileChange.contentHash` against `.sum` frontmatter `content_hash` field to build `filesToAnalyze` and `filesToSkip` sets\n- **src/generation/writers/sum.ts**: Calls `computeContentHash()` before writing `.sum` files to populate YAML frontmatter `content_hash` field\n- **src/update/orphan-cleaner.ts**: Uses `ChangeType === 'deleted'` or `'renamed'` to identify `.sum` files requiring cleanup via `FileChange.path` and `FileChange.oldPath` matching\n### types.ts\n**Purpose:** Defines TypeScript types and interfaces for git-based change detection, representing file modifications tracked via `...\n\n**Defines TypeScript types and interfaces for git-based change detection, representing file modifications tracked via `ChangeType` discriminants ('added' | 'modified' | 'deleted' | 'renamed') and structured as `FileChange` objects with path information and optional rename tracking.**\n\n## Exported Types\n\n**`ChangeType`** — String literal union type representing the four possible file change states:\n```typescript\ntype ChangeType = 'added' | 'modified' | 'deleted' | 'renamed'\n```\n\n## Exported Interfaces\n\n**`FileChange`** — Represents a single file modification detected from git diff output:\n- `path: string` — Relative path to the file (for renamed files, contains the new path)\n- `status: ChangeType` — Discriminant field indicating change type\n- `oldPath?: string` — Optional field present only when `status === 'renamed'`, stores original pre-rename path\n\n**`ChangeDetectionResult`** — Aggregated output from git diff analysis:\n- `currentCommit: string` — SHA-1 hash of HEAD or current working tree state\n- `baseCommit: string` — SHA-1 hash of comparison baseline (e.g., previous commit, tag, or branch)\n- `changes: FileChange[]` — Array of detected modifications\n- `includesUncommitted: boolean` — Flag indicating whether working tree and staged changes were merged into results (corresponds to `git status --porcelain` inclusion)\n\n**`ChangeDetectionOptions`** — Configuration object for change detection behavior:\n- `includeUncommitted?: boolean` — When `true`, merges uncommitted working tree and staged changes with committed diff results via `git status --porcelain` parsing\n\n## Usage Context\n\nThese types support the incremental update workflow (`src/update/orchestrator.ts`) where `FileChange` objects drive selective `.sum` regeneration. The `status` field enables conditional logic:\n- `'added'` or `'modified'` → queue for Phase 1 analysis\n- `'deleted'` → trigger orphan cleanup via `cleanupOrphans()`\n- `'renamed'` → use `oldPath` to locate stale `.sum` file and `path` for new generation target\n\nThe `includesUncommitted` flag in `ChangeDetectionResult` determines whether `getAffectedDirectories()` must account for working tree modifications not yet committed, ensuring `AGENTS.md` regeneration covers all dirty state.\n\n## Project Directory Structure\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>",
      "systemPrompt": "You are generating an AGENTS.md file — a directory-level overview for AI coding assistants.\n\nCRITICAL: Output ONLY the raw markdown content. No code fences, no preamble, no explanations, no conversational text. Your entire response IS the AGENTS.md file content.\n\nOUTPUT FORMAT:\n- First line MUST be exactly: <!-- Generated by agents-reverse-engineer -->\n- Use a # heading with the directory name\n- Write a one-paragraph purpose statement for the directory\nusage\nADAPTIVE SECTIONS:\nAnalyze the directory contents and choose the most relevant sections. Do NOT use a fixed template. Instead, select sections that best document this specific directory for an AI that needs to reconstruct or extend the project.\n\nConsider these section types (choose what applies):\n- **Contents**: Group files by purpose/category under ## headings. For each file: markdown link [filename](./filename) and a one-line description.\n- **Subdirectories**: If subdirectories exist, list them with links [dirname/](./dirname/) and brief summaries.\n- **Architecture / Data Flow**: If files form a pipeline, request/response chain, or layered architecture, document it.\n- **Stack**: If this is a package root (has package.json, Cargo.toml, go.mod, etc.), document the technology stack, key scripts, and entry points.\n- **Structure**: If the directory layout follows a convention (feature-sliced, domain-driven, MVC, etc.), document it.\n- **Patterns**: If files share recurring design patterns (factory, strategy, middleware, barrel re-export), name and document them.\n- **Configuration**: If the directory contains config files, schemas, or environment definitions, document the config surface area.\n- **API Surface**: If the directory exports a public API (barrel index, route definitions, SDK), document the interface contract.\n- **File Relationships**: How files collaborate, depend on each other, or share state.\n- **Behavioral Contracts**: If files contain regex patterns, format specifications, magic constants, or template strings that define observable behavior, collect them in a dedicated section. Preserve verbatim patterns from file summaries — do NOT paraphrase regex into prose. This section is MANDATORY when file summaries contain behavioral artifacts.\n\nChoose any relevant sections or create your own based on the directory contents. The goal is to provide a comprehensive overview that captures the essence of the directory's role in the project and how its files work together, with a focus on what an AI coding assistant would need to know to effectively interact with this code.\n\nSCOPE:\n- AGENTS.md is a NAVIGATIONAL INDEX — help an AI find the right file quickly\n- Focus on: what each file does, how files relate, directory-level patterns\n- Do NOT reproduce full architecture sections — those belong in the root CLAUDE.md\n\nPATH ACCURACY (MANDATORY):\n- When referencing files or modules outside this directory, use ONLY paths from the \"Import Map\" section\n- Do NOT invent, rename, or guess module paths — if a path isn't in the Import Map, don't reference it\n- Use the exact directory names from \"Project Directory Structure\" — do NOT rename directories\n  (e.g., if the directory is called \"cli\", write \"src/cli/\", NOT \"src/commands/\")\n- Cross-module references must use the specifier format from actual import statements\n  (e.g., \"../generation/writers/sum.js\", NOT \"../fs/sum-file.js\")\n- If you are unsure about a path, omit the cross-reference rather than guessing\n\nCONSISTENCY (MANDATORY):\n- Do not contradict yourself within the same document\n- If you describe a technique (e.g., \"regex-based\"), do not call it something else later (e.g., \"AST-based\")\n- When stating version numbers, engines, or config fields, use ONLY values present in the file summaries\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this directory\", \"this module\", \"provides\", \"responsible for\", \"is used to\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Per-file descriptions in Contents sections: 1-2 sentences maximum. Reference key symbols but do not reproduce full summaries.\n- Behavioral contracts (regex patterns, format specs, constants) belong in a separate Behavioral Contracts section, not in per-file descriptions.\n- Subdirectory descriptions: 1-2 sentences maximum. Capture the directory's role, not its full contents.\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- Key exported symbols from file summaries MUST appear in the directory overview\n- Preserve exact casing of identifiers\n\nUSER NOTES:\n- If \"User Notes\" are provided in the prompt, they contain user-defined instructions that will be automatically prepended to your output\n- Do NOT repeat or paraphrase user notes in your generated content — they are included separately\n- You may reference information from user notes for context",
      "response": "<!-- Generated by agents-reverse-engineer -->\n\n# src/change-detection\n\nGit-based change detection and SHA-256 content hashing for incremental documentation updates. Parses `git diff --name-status -M` with rename detection, merges uncommitted working tree changes, and computes file content hashes for `.sum` frontmatter verification.\n\n## Contents\n\n### Core Implementation\n\n**[detector.ts](./detector.ts)** — Implements `isGitRepo()`, `getCurrentCommit()`, `getChangedFiles()` with git diff parsing, `computeContentHash()` and `computeContentHashFromString()` for SHA-256 hex digest generation. Parses status codes (`'A'`/`'M'`/`'D'`/`'R'` prefix) from `git diff --name-status -M` (50% similarity threshold), merges `status.modified[]`, `status.deleted[]`, `status.not_added[]`, `status.staged[]` when `includeUncommitted` is true. Returns `ChangeDetectionResult` with `currentCommit`, `baseCommit`, `changes[]`, `includesUncommitted`.\n\n**[types.ts](./types.ts)** — Defines `ChangeType` union (`'added' | 'modified' | 'deleted' | 'renamed'`), `FileChange` interface with `path`, `status`, optional `oldPath` (renames), optional `contentHash`, `ChangeDetectionResult` interface with `currentCommit`, `baseCommit`, `changes[]`, `includesUncommitted`, and `ChangeDetectionOptions` interface with `includeUncommitted` flag.\n\n**[index.ts](./index.ts)** — Barrel export re-exporting `isGitRepo()`, `getCurrentCommit()`, `getChangedFiles()`, `computeContentHash()`, `computeContentHashFromString()` from `detector.ts` and all types from `types.ts`.\n\n## Integration Points\n\n- **src/update/orchestrator.ts**: Calls `getChangedFiles()` with `includeUncommitted` flag, compares `FileChange.contentHash` against `.sum` YAML frontmatter `content_hash` field, builds `filesToAnalyze` and `filesToSkip` sets, uses `ChangeType === 'deleted' | 'renamed'` for orphan cleanup targeting.\n- **src/generation/writers/sum.ts**: Calls `computeContentHash()` before writing `.sum` files to populate YAML frontmatter `content_hash` field.\n- **src/update/orphan-cleaner.ts**: Uses `FileChange.oldPath` (renames) and `FileChange.path` (deletes) to identify stale `.sum` files requiring removal.\n\n## Behavioral Contracts\n\n**Git diff parsing:** Splits `git diff --name-status -M` output on newlines, splits lines on tab separator. Status codes map to `ChangeType`:\n- `'A'` → `status: 'added'`\n- `'M'` → `status: 'modified'`\n- `'D'` → `status: 'deleted'`\n- `startsWith('R')` → `status: 'renamed'` with `oldPath = parts[1]`, `path = parts[parts.length - 1]`\n\n**SHA-256 hashing:** Uses `crypto.createHash('sha256').update(content).digest('hex')` producing lowercase hex string (64 characters). Matches YAML frontmatter pattern `content_hash: [a-f0-9]{64}`.\n\n**Uncommitted merge:** When `includeUncommitted` is true, deduplicates via `changes.some(c => c.path === file)` before pushing `status.modified[]`, `status.deleted[]`, `status.not_added[]`, `status.staged[]` entries.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 911,
      "cacheReadTokens": 17477,
      "cacheCreationTokens": 11168,
      "latencyMs": 44815,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": []
    },
    {
      "timestamp": "2026-02-09T16:13:38.892Z",
      "prompt": "Generate AGENTS.md for directory: \"src/imports\" (imports)\n\n## File Summaries (3 files)\n\n### extractor.ts\n**Purpose:** extractor.ts extracts and classifies TypeScript/JavaScript import statements from source files using regex parsing, p...\n\n**extractor.ts extracts and classifies TypeScript/JavaScript import statements from source files using regex parsing, producing structured import maps for LLM prompt inclusion.**\n\n## Exported Functions\n\n**extractImports(sourceContent: string): ImportEntry[]**\nParses source content using `IMPORT_REGEX` pattern `/^import\\s+(type\\s+)?(?:\\{([^}]*)\\}|(\\*\\s+as\\s+\\w+)|(\\w+))\\s+from\\s+['\"]([^'\"]+)['\"]/gm` to extract import statements. Captures five groups: (1) `type` keyword for type-only imports, (2) named symbols between braces, (3) namespace imports (`* as name`), (4) default imports, (5) module specifier. Returns `ImportEntry[]` with `specifier`, `symbols[]`, and `typeOnly` boolean. Handles aliased named imports via regex replacement `/\\s+as\\s+\\w+/` removal.\n\n**extractDirectoryImports(dirPath: string, fileNames: string[]): Promise<FileImports[]>**\nReads first 100 lines of each file in `fileNames` array (performance optimization for import-region scanning), calls `extractImports()` on content, filters out bare specifiers (npm packages like `'react'`) and `node:` built-ins. Classifies relative imports into `internal` (starts with `./`) and `external` (starts with `../`). Returns `FileImports[]` containing `fileName`, `externalImports[]`, and `internalImports[]`. Silently catches read errors to skip unreadable files.\n\n**formatImportMap(fileImports: FileImports[]): string**\nConverts `FileImports[]` into human-readable text block for LLM prompt injection. Format template:\n```\nrunner.ts:\n  ../ai/index.js → AIService\n  ../generation/executor.js → ExecutionPlan, ExecutionTask\n\npool.ts:\n  ./trace.js → ITraceWriter (type)\n```\nAppends `(type)` suffix for type-only imports. Filters out files with no external imports (excludes internal-only dependencies from output).\n\n## Import Regex Pattern\n\n`IMPORT_REGEX` constant matches ES module import syntax with five capture groups:\n- Group 1: `type` keyword (`import type { Foo }` patterns)\n- Group 2: Named symbols within braces (`{ Foo, Bar as Baz }`)\n- Group 3: Namespace imports (`* as name`)\n- Group 4: Default imports (bare identifier after `import`)\n- Group 5: Module specifier (quoted string after `from`)\n\nPattern anchored with `^` to match line start, uses `gm` flags for global multiline matching. Does not capture dynamic imports (`import('...')`) or side-effect imports (`import './styles.css'`).\n\n## Integration Points\n\nDepends on `node:fs/promises.readFile` for file I/O and `node:path` for path joining. Consumed by `src/generation/prompts/builder.ts` which calls `extractDirectoryImports()` during Phase 2 directory aggregation to populate import maps in `AGENTS.md` generation prompts. Verifies external import paths exist via constraints defined in prompt templates to prevent phantom path references.\n\n## Data Structures\n\nUses `ImportEntry` type from `./types.js` with shape `{ specifier: string, symbols: string[], typeOnly: boolean }`. `FileImports` type contains `{ fileName: string, externalImports: ImportEntry[], internalImports: ImportEntry[] }`. No internal state beyond regex lastIndex cursor during extraction loop.\n### index.ts\n**Purpose:** Exports type definitions and functions for static import analysis via `extractor.js`, exposing `extractImports()`, `e...\n\n**Exports type definitions and functions for static import analysis via `extractor.js`, exposing `extractImports()`, `extractDirectoryImports()`, `formatImportMap()`, plus `ImportEntry` and `FileImports` types used throughout the generation pipeline for dependency graph construction and import map rendering in AI prompts.**\n\n## Public Interface\n\n```typescript\nfunction extractImports(filePath: string, content: string): FileImports\n```\nExtracts import statements from a single source file, returning `FileImports` object containing the file path and an array of `ImportEntry` objects.\n\n```typescript\nfunction extractDirectoryImports(directoryPath: string, discoveredFiles: string[]): Map<string, FileImports>\n```\nAggregates imports across all files within a directory by filtering `discoveredFiles` to those starting with `directoryPath + path.sep`, invoking `extractImports()` on each file, returning `Map<filePath, FileImports>` for directory-level dependency analysis.\n\n```typescript\nfunction formatImportMap(importMap: Map<string, FileImports>): string\n```\nSerializes `Map<string, FileImports>` into human-readable Markdown format for injection into Phase 2 directory aggregation prompts, enabling AI context on intra-directory dependencies.\n\n```typescript\ninterface ImportEntry {\n  source: string;          // Import specifier (relative path or package name)\n  isRelative: boolean;     // true if source starts with './' or '../'\n  importedNames: string[]; // Named imports/namespaces\n  lineNumber: number;      // Source line for debugging\n}\n```\nDiscriminates relative file imports from package imports via `isRelative` boolean, stores extracted identifiers in `importedNames[]` for API surface tracking.\n\n```typescript\ninterface FileImports {\n  file: string;            // Absolute or project-relative file path\n  imports: ImportEntry[];  // Extracted import statements\n}\n```\nContainer type mapping file paths to their import dependency lists, consumed by `extractDirectoryImports()` aggregation and `formatImportMap()` serialization.\n\n## Integration Points\n\n- **Generation Phase 2**: `src/generation/prompts/builder.ts` calls `extractDirectoryImports()` during `buildDirectoryPrompt()` to construct import context for `AGENTS.md` synthesis\n- **Orchestration**: `src/generation/orchestrator.ts` passes `discoveredFiles` list to enable directory-level filtering without redundant filesystem scans\n- **Prompt Templates**: `src/generation/prompts/templates.ts` consumes formatted import maps via `formatImportMap()` output injected into `directoryPromptTemplate`\n- **Quality Validation**: `src/quality/inconsistency/code-vs-doc.ts` may leverage `ImportEntry.importedNames[]` for cross-file symbol resolution (currently unused)\n\n## Module Organization\n\nBarrel export pattern delegates implementation to `extractor.ts` and type definitions to `types.ts`, maintaining single public interface at `src/imports/index.ts` for external consumption. All three functions and both types exported without namespace wrapping for direct destructured imports.\n### types.ts\n**Purpose:** ImportEntry and FileImports type definitions model static import statement extraction results for dependency graph co...\n\n**ImportEntry and FileImports type definitions model static import statement extraction results for dependency graph construction and import map generation in AGENTS.md prompts.**\n\n## Exported Types\n\n**ImportEntry** represents a single parsed import statement with three fields:\n- `specifier: string` — Raw import path as written in source (e.g., `'../ai/index.js'`, `'lodash'`)\n- `symbols: string[]` — Named/default imports extracted from statement (e.g., `['AIService', 'AIResponse']`)\n- `typeOnly: boolean` — Discriminates `import type` statements from runtime imports\n\n**FileImports** aggregates all imports discovered in a single source file with three fields:\n- `fileName: string` — Relative file path from discovery root (e.g., `'runner.ts'`, `'ai/service.ts'`)\n- `externalImports: ImportEntry[]` — Cross-directory or external module imports (used for inter-module dependency tracking)\n- `internalImports: ImportEntry[]` — Same-directory imports (used for intra-module coupling analysis)\n\n## Integration Points\n\nConsumed by `src/imports/extractor.ts` which implements regex-based import parsing producing `FileImports[]` arrays. `src/generation/prompts/builder.ts` calls `extractDirectoryImports()` to construct import maps embedded in directory aggregation prompts for `AGENTS.md` generation. The `externalImports` vs `internalImports` partitioning enables prompts to emphasize cross-module dependencies while suppressing local coupling noise.\n\n## Design Rationale\n\nThe `typeOnly` field supports TypeScript-specific workflows where type imports don't affect runtime dependency graphs. The `symbols: string[]` array enables precise tracking of which exports are actually consumed, supporting future dead code detection or import optimization features. The internal/external split allows `AGENTS.md` prompts to prioritize architectural boundaries over implementation details.\n\n## Project Directory Structure\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>",
      "systemPrompt": "You are generating an AGENTS.md file — a directory-level overview for AI coding assistants.\n\nCRITICAL: Output ONLY the raw markdown content. No code fences, no preamble, no explanations, no conversational text. Your entire response IS the AGENTS.md file content.\n\nOUTPUT FORMAT:\n- First line MUST be exactly: <!-- Generated by agents-reverse-engineer -->\n- Use a # heading with the directory name\n- Write a one-paragraph purpose statement for the directory\nusage\nADAPTIVE SECTIONS:\nAnalyze the directory contents and choose the most relevant sections. Do NOT use a fixed template. Instead, select sections that best document this specific directory for an AI that needs to reconstruct or extend the project.\n\nConsider these section types (choose what applies):\n- **Contents**: Group files by purpose/category under ## headings. For each file: markdown link [filename](./filename) and a one-line description.\n- **Subdirectories**: If subdirectories exist, list them with links [dirname/](./dirname/) and brief summaries.\n- **Architecture / Data Flow**: If files form a pipeline, request/response chain, or layered architecture, document it.\n- **Stack**: If this is a package root (has package.json, Cargo.toml, go.mod, etc.), document the technology stack, key scripts, and entry points.\n- **Structure**: If the directory layout follows a convention (feature-sliced, domain-driven, MVC, etc.), document it.\n- **Patterns**: If files share recurring design patterns (factory, strategy, middleware, barrel re-export), name and document them.\n- **Configuration**: If the directory contains config files, schemas, or environment definitions, document the config surface area.\n- **API Surface**: If the directory exports a public API (barrel index, route definitions, SDK), document the interface contract.\n- **File Relationships**: How files collaborate, depend on each other, or share state.\n- **Behavioral Contracts**: If files contain regex patterns, format specifications, magic constants, or template strings that define observable behavior, collect them in a dedicated section. Preserve verbatim patterns from file summaries — do NOT paraphrase regex into prose. This section is MANDATORY when file summaries contain behavioral artifacts.\n\nChoose any relevant sections or create your own based on the directory contents. The goal is to provide a comprehensive overview that captures the essence of the directory's role in the project and how its files work together, with a focus on what an AI coding assistant would need to know to effectively interact with this code.\n\nSCOPE:\n- AGENTS.md is a NAVIGATIONAL INDEX — help an AI find the right file quickly\n- Focus on: what each file does, how files relate, directory-level patterns\n- Do NOT reproduce full architecture sections — those belong in the root CLAUDE.md\n\nPATH ACCURACY (MANDATORY):\n- When referencing files or modules outside this directory, use ONLY paths from the \"Import Map\" section\n- Do NOT invent, rename, or guess module paths — if a path isn't in the Import Map, don't reference it\n- Use the exact directory names from \"Project Directory Structure\" — do NOT rename directories\n  (e.g., if the directory is called \"cli\", write \"src/cli/\", NOT \"src/commands/\")\n- Cross-module references must use the specifier format from actual import statements\n  (e.g., \"../generation/writers/sum.js\", NOT \"../fs/sum-file.js\")\n- If you are unsure about a path, omit the cross-reference rather than guessing\n\nCONSISTENCY (MANDATORY):\n- Do not contradict yourself within the same document\n- If you describe a technique (e.g., \"regex-based\"), do not call it something else later (e.g., \"AST-based\")\n- When stating version numbers, engines, or config fields, use ONLY values present in the file summaries\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this directory\", \"this module\", \"provides\", \"responsible for\", \"is used to\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Per-file descriptions in Contents sections: 1-2 sentences maximum. Reference key symbols but do not reproduce full summaries.\n- Behavioral contracts (regex patterns, format specs, constants) belong in a separate Behavioral Contracts section, not in per-file descriptions.\n- Subdirectory descriptions: 1-2 sentences maximum. Capture the directory's role, not its full contents.\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- Key exported symbols from file summaries MUST appear in the directory overview\n- Preserve exact casing of identifiers\n\nUSER NOTES:\n- If \"User Notes\" are provided in the prompt, they contain user-defined instructions that will be automatically prepended to your output\n- Do NOT repeat or paraphrase user notes in your generated content — they are included separately\n- You may reference information from user notes for context",
      "response": "<!-- Generated by agents-reverse-engineer -->\n\n# src/imports\n\n**Static import analysis module extracting TypeScript/JavaScript import statements via regex parsing to construct dependency graphs and import maps for AI-driven documentation prompts.**\n\n## Contents\n\n### Core Extraction\n\n**[extractor.ts](./extractor.ts)** — Regex-based import parser extracting ES module syntax into `ImportEntry[]` arrays. `extractImports()` applies `IMPORT_REGEX` (`/^import\\s+(type\\s+)?(?:\\{([^}]*)\\}|(\\*\\s+as\\s+\\w+)|(\\w+))\\s+from\\s+['\"]([^'\"]+)['\"]/gm`) to capture five groups: (1) `type` keyword, (2) named symbols, (3) namespace imports, (4) default imports, (5) module specifier. `extractDirectoryImports()` scans first 100 lines of each file to optimize import-region parsing, filters bare specifiers (`'react'`, `node:*`), partitions relative imports into `internal` (`./`) and `external` (`../`). `formatImportMap()` serializes `FileImports[]` into LLM prompt text blocks with `(type)` suffix for type-only imports.\n\n**[types.ts](./types.ts)** — Type definitions for import extraction results. `ImportEntry` models single import statement with `specifier`, `symbols[]`, `typeOnly` discriminator. `FileImports` aggregates directory-level imports via `externalImports[]` (cross-directory dependencies) and `internalImports[]` (same-directory coupling). External/internal partitioning enables `AGENTS.md` prompts to emphasize architectural boundaries.\n\n**[index.ts](./index.ts)** — Barrel export exposing `extractImports()`, `extractDirectoryImports()`, `formatImportMap()`, `ImportEntry`, `FileImports` for integration with Phase 2 directory aggregation pipeline (`src/generation/prompts/builder.ts`).\n\n## Integration Points\n\n**Generation Phase 2 (Directory Aggregation):**\n- `src/generation/prompts/builder.ts` calls `extractDirectoryImports()` during `AGENTS.md` synthesis\n- Import maps injected into prompts via `formatImportMap()` output in `src/generation/prompts/templates.ts`\n- External import paths verified against filesystem constraints to prevent phantom references\n\n**Discovery Pipeline:**\n- `src/generation/orchestrator.ts` passes `discoveredFiles[]` list enabling directory-level filtering without redundant scans\n- Skips binary files, vendor directories, custom exclude patterns from upstream discovery filters\n\n**Data Flow:**\n- Runner invokes `extractDirectoryImports(dirPath, fileNames)` → reads first 100 lines → regex extraction → internal/external partitioning → `FileImports[]` return\n- Prompt builder calls `formatImportMap(fileImports)` → text serialization → template injection\n\n## Behavioral Contracts\n\n**IMPORT_REGEX Pattern:**\n```regex\n/^import\\s+(type\\s+)?(?:\\{([^}]*)\\}|(\\*\\s+as\\s+\\w+)|(\\w+))\\s+from\\s+['\"]([^'\"]+)['\"]/gm\n```\n- Group 1: `type` keyword (`import type { Foo }`)\n- Group 2: Named symbols within braces (`{ Foo, Bar as Baz }`)\n- Group 3: Namespace imports (`* as name`)\n- Group 4: Default imports (bare identifier after `import`)\n- Group 5: Module specifier (quoted string after `from`)\n- Anchored `^` for line start, `gm` flags for global multiline\n- Does NOT capture dynamic imports (`import('...')`) or side-effect imports (`import './styles.css'`)\n\n**Format Template (formatImportMap output):**\n```\nrunner.ts:\n  ../ai/index.js → AIService\n  ../generation/executor.js → ExecutionPlan, ExecutionTask\n\npool.ts:\n  ./trace.js → ITraceWriter (type)\n```\n\n**File Read Optimization:**\n- Reads only first 100 lines via `content.split('\\n').slice(0, 100).join('\\n')` before regex scanning\n- Assumption: ES module imports typically concentrated in file header region\n- Trade-off: Skips late-file dynamic import detection for 95%+ file I/O reduction\n\n**Import Classification:**\n- Bare specifiers (`'react'`, `'lodash'`) → filtered out (external packages)\n- `node:` prefixed (`'node:fs'`) → filtered out (built-in modules)\n- Relative starting `./` → `internalImports[]` (same-directory coupling)\n- Relative starting `../` → `externalImports[]` (cross-directory dependencies)",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 1147,
      "cacheReadTokens": 17477,
      "cacheCreationTokens": 11112,
      "latencyMs": 48705,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": []
    },
    {
      "timestamp": "2026-02-09T16:13:38.929Z",
      "prompt": "Generate AGENTS.md for directory: \"src/discovery\" (discovery)\n\n## File Summaries (3 files)\n\n### run.ts\n**Purpose:** `run.ts` exports `discoverFiles()`, the high-level entry point orchestrating four-filter pipeline execution (gitignor...\n\n**`run.ts` exports `discoverFiles()`, the high-level entry point orchestrating four-filter pipeline execution (gitignore → vendor → binary → custom) after directory walking, returning `FilterResult` with included/excluded file lists and filter attribution.**\n\n## Exported Interface\n\n**`DiscoveryConfig`** — Structural subset of full `Config` schema from `src/config/schema.ts` containing:\n- `exclude.vendorDirs: string[]` — Third-party directories to skip\n- `exclude.binaryExtensions: string[]` — Non-text file extensions\n- `exclude.patterns: string[]` — Gitignore-style globs\n- `options.maxFileSize: number` — Binary detection threshold in bytes\n- `options.followSymlinks: boolean` — Symlink traversal flag\n\n**`DiscoverFilesOptions`** — Optional parameters for pipeline execution:\n- `tracer?: ITraceWriter` — Trace event emitter for telemetry\n- `debug?: boolean` — Verbose logging flag\n\n**`discoverFiles(root: string, config: DiscoveryConfig, options?: DiscoverFilesOptions): Promise<FilterResult>`** — Async pipeline function creating filters in deterministic order (gitignore, vendor, binary, custom), invoking `walkDirectory()` with `cwd` and `followSymlinks` options, then applying filter chain via `applyFilters()` with tracer/debug propagation. Returns `FilterResult` containing `included: FileInfo[]`, `excluded: FileInfo[]` arrays with per-file filter attribution.\n\n## Filter Chain Composition\n\nConstructs four filters via factory functions:\n1. **`createGitignoreFilter(root)`** — Parses `.gitignore` files using `ignore` library\n2. **`createVendorFilter(config.exclude.vendorDirs)`** — Blocks third-party directories (node_modules, .git, dist)\n3. **`createBinaryFilter({ maxFileSize, additionalExtensions })`** — Detects binary files via extension + content analysis\n4. **`createCustomFilter(config.exclude.patterns, root)`** — Applies user-defined glob patterns\n\nFilters applied sequentially in declared order via `applyFilters()` from `./filters/index.js`.\n\n## Integration Points\n\n**Callers:** All CLI commands requiring file discovery (`src/cli/discover.ts`, `src/cli/generate.ts`, `src/cli/update.ts`) invoke `discoverFiles()` with config loaded from `loadConfig()`.\n\n**Dependencies:**\n- `walkDirectory()` from `./walker.js` — Filesystem traversal with symlink control\n- `applyFilters()` from `./filters/index.js` — Sequential filter application with attribution\n- `ITraceWriter` from `../orchestration/trace.js` — Telemetry event emission\n\n**Output consumption:** Returned `FilterResult.included` feeds into Phase 1 file analysis worker pool, `FilterResult.excluded` populates `GENERATION-PLAN.md` statistics.\n\n## Design Pattern\n\nFacade pattern consolidating filter creation, directory walking, and filter application into single async function with config-driven parameterization. Promotes structural compatibility via `DiscoveryConfig` interface instead of tight coupling to full `Config` type, enabling unit testing with minimal mock objects.\n### types.ts\n**Purpose:** types.ts defines TypeScript interfaces for the file discovery system's filter chain architecture, including filter co...\n\n**types.ts defines TypeScript interfaces for the file discovery system's filter chain architecture, including filter contracts, exclusion metadata, and walker configuration.**\n\n## Exported Interfaces\n\n**FileFilter** — Filter contract with two properties:\n- `name: string` (readonly) identifies which filter excluded a file for logging\n- `shouldExclude(path: string, stats?: Stats): Promise<boolean> | boolean` determines file inclusion (returns true to exclude)\n\nTakes absolute file path and optional `Stats` from `node:fs` for size-based or metadata filtering. Supports both sync and async implementations. Known implementations: GitignoreFilter, BinaryFilter, VendorFilter, CustomPatternFilter (per inline documentation).\n\n**ExcludedFile** — Exclusion record with three properties:\n- `path: string` absolute path to excluded file\n- `reason: string` human-readable exclusion explanation\n- `filter: string` name of the filter that excluded the file\n\nUsed to track why files were rejected during discovery pipeline execution.\n\n**FilterResult** — Discovery pipeline output with two properties:\n- `included: string[]` files passing all filters (ready for analysis)\n- `excluded: ExcludedFile[]` rejected files with metadata\n\nAggregates results from filter chain execution across all discovered files.\n\n**WalkerOptions** — Directory traversal configuration with three properties:\n- `cwd: string` root directory for walking (absolute path, required)\n- `followSymlinks?: boolean` whether to traverse symlinks (default false per CONTEXT.md)\n- `dot?: boolean` whether to include dotfiles starting with `.` (default true for analysis)\n\nPassed to directory walker to control filesystem traversal behavior.\n\n## Integration Points\n\nAll interfaces consumed by `src/discovery/walker.ts` (directory walker) and `src/discovery/filters/` implementations (gitignore.ts, binary.ts, vendor.ts, custom.ts). The `FileFilter` interface enables composable filter chains where each filter can independently reject files. The `Stats` parameter in `shouldExclude()` allows filters to inspect file metadata without additional filesystem calls.\n### walker.ts\n**Purpose:** walker.ts implements directory traversal via fast-glob for ARE's file discovery pipeline.\n\n**walker.ts implements directory traversal via fast-glob for ARE's file discovery pipeline.**\n\n## Exported Function\n\n`walkDirectory(options: WalkerOptions): Promise<string[]>` returns absolute paths to all files in directory tree. Uses `fg.glob('**/*', ...)` with cwd-relative traversal. Returns unfiltered file list—filtering applied separately by filter chain in `src/discovery/filters/`.\n\n## Configuration Options\n\nConsumes `WalkerOptions` interface with:\n- `cwd: string` — Base directory for glob traversal\n- `dot?: boolean` — Include dotfiles (default `true`)\n- `followSymlinks?: boolean` — Follow symbolic links (default `false`)\n\n## fast-glob Parameters\n\nPasses these options to `fg.glob()`:\n- `absolute: true` — Return absolute paths (not relative)\n- `onlyFiles: true` — Exclude directories from results\n- `suppressErrors: true` — Ignore permission errors per RESEARCH.md requirement\n- `ignore: ['**/.git/**']` — Hard-coded exclusion for `.git` internals (performance optimization)\n\n## Integration Points\n\nCalled by `src/discovery/run.ts` which applies post-traversal filters: gitignore patterns via `src/discovery/filters/gitignore.ts`, binary detection via `src/discovery/filters/binary.ts`, vendor directories via `src/discovery/filters/vendor.ts`, custom glob patterns via `src/discovery/filters/custom.ts`. Walker does not inspect file contents or apply business logic—purely delegates to fast-glob's optimized filesystem traversal.\n\n## Import Map (verified — use these exact paths)\n\nrun.ts:\n  ../orchestration/trace.js → ITraceWriter (type)\n\n## Project Directory Structure\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\n\n## Subdirectories\n\n### filters/\n<!-- Generated by agents-reverse-engineer -->\n\n# src/discovery/filters\n\nComposable file exclusion filters implementing gitignore semantics, binary detection, vendor directory skipping, and custom glob patterns for the file discovery pipeline.\n\n## Contents\n\n### Filter Implementations\n\n**[binary.ts](./binary.ts)** — Three-phase binary file detector: extension fast-path lookup in 82-member `BINARY_EXTENSIONS` set (images/archives/executables/media/fonts/bytecode/databases), size threshold comparison via `fs.stat()` (default 1MB), content analysis fallback via `isbinaryfile.isBinaryFile()`.\n\n**[custom.ts](./custom.ts)** — User-defined gitignore-style pattern matcher via `ignore` library, converts absolute paths to relative via `path.relative()`, bypasses filtering for paths outside root (relative path starts with `..`).\n\n**[gitignore.ts](./gitignore.ts)** — `.gitignore` parser consuming `ignore` library, silently passes all paths when `.gitignore` missing, converts absolute paths to relative for pattern matching.\n\n**[vendor.ts](./vendor.ts)** — Vendor directory excluder with dual matching: single-segment Set lookup (`node_modules`, `.git`, `dist`, `build`, `__pycache__`, `.next`, `venv`, `.venv`, `target`) and path-pattern substring search for multi-segment patterns (`.agents/skills`, `apps/vendor`).\n\n**[index.ts](./index.ts)** — Filter chain orchestrator with `applyFilters()` executing short-circuit evaluation (stops at first exclusion) across 30 concurrent workers sharing iterator, collects per-filter statistics (`matched`/`rejected` counts), emits `filter:applied` trace events, re-exports all filter creators.\n\n## Architecture\n\n### Filter Chain Execution\n\n`applyFilters()` bounds concurrency to 30 workers via `Math.min(CONCURRENCY, files.length)` to prevent file descriptor exhaustion during binary detection I/O. Each worker drains shared `files.entries()` iterator, processes files serially through filter array with short-circuit semantics (breaks on first `shouldExclude()` true return). Results sorted by original index to preserve input order despite concurrent execution.\n\n### Path Normalization Contract\n\nAll filters receive absolute paths, convert to relative via `path.relative(normalizedRoot, absolutePath)` for pattern matching libraries (`ignore` requires relative paths without leading slash). Paths outside root (starting with `..`) bypass filtering by returning `false`. Empty relative paths also bypass exclusion.\n\n### Binary Detection Strategy\n\n`createBinaryFilter()` prioritizes performance via three-tiered approach:\n1. Extension lookup in pre-allocated Set (O(1), no I/O)\n2. Size threshold check via `fs.stat()` (single syscall)\n3. Content analysis via `isbinaryfile` heuristics (reads file header bytes)\n\nReturns `true` on `fs.stat()` failure (file unreadable/missing) to fail-safe exclude corrupt/inaccessible files.\n\n### Vendor Path Pattern Classification\n\n`createVendorFilter()` splits patterns into `singleSegments` Set and `pathPatterns` array based on presence of path separator after normalization (`dir.replace(/[\\\\/]/g, path.sep)`). Single segments use Set membership check, path patterns use substring search. Normalization ensures cross-platform compatibility (Windows backslashes converted to `path.sep`).\n\n## Filter Statistics\n\n`applyFilters()` aggregates statistics in `Map<string, { matched: number; rejected: number }>`:\n- `rejected` increments when filter excludes file\n- `matched` increments for all filters when file passes entire chain (not per individual filter pass)\n\nStatistics emitted as `filter:applied` trace events with `filterName`, `filesMatched`, `filesRejected` fields for downstream effectiveness analysis.\n\n## Behavioral Contracts\n\n### Binary Extension Set (82 entries)\nImages: `.png`, `.jpg`, `.jpeg`, `.gif`, `.bmp`, `.ico`, `.webp`, `.svg`, `.tiff`, `.tif`, `.psd`, `.raw`, `.heif`, `.heic`\nArchives: `.zip`, `.tar`, `.gz`, `.rar`, `.7z`, `.bz2`, `.xz`, `.tgz`\nExecutables: `.exe`, `.dll`, `.so`, `.dylib`, `.bin`, `.msi`, `.app`, `.dmg`\nMedia: `.mp3`, `.mp4`, `.wav`, `.avi`, `.mov`, `.mkv`, `.flac`, `.ogg`, `.webm`, `.m4a`, `.aac`, `.wma`, `.wmv`, `.flv`\nDocuments: `.pdf`, `.doc`, `.docx`, `.xls`, `.xlsx`, `.ppt`, `.pptx`, `.odt`, `.ods`, `.odp`\nFonts: `.woff`, `.woff2`, `.ttf`, `.eot`, `.otf`\nBytecode: `.class`, `.pyc`, `.pyo`, `.o`, `.obj`, `.a`, `.lib`, `.wasm`\nDatabases: `.db`, `.sqlite`, `.sqlite3`, `.mdb`\nMisc: `.ico`, `.icns`, `.cur`, `.deb`, `.rpm`, `.jar`, `.war`, `.ear`\n\n### Default Vendor Directories (10 entries)\n`node_modules`, `vendor`, `.git`, `dist`, `build`, `__pycache__`, `.next`, `venv`, `.venv`, `target`\n\n### Default Max File Size\n`1024 * 1024` (1MB)\n\n### Filter Chain Concurrency\n30 workers (constant `CONCURRENCY` in `index.ts`)\n\n## Integration Points\n\nConsumed by `src/discovery/walker.ts` as composable filter chain during Phase 1 file analysis. Configured via `config.exclude` fields in `.agents-reverse-engineer/config.yaml`:\n- `patterns` → `createCustomFilter()`\n- `vendorDirs` → `createVendorFilter()`\n- `binaryExtensions` → `createBinaryFilter()` (merged with `BINARY_EXTENSIONS`)\n- `.maxFileSize` → `createBinaryFilter()`\n\nImplements `FileFilter` interface from `../types.js` with `name: string` and `shouldExclude(absolutePath: string): Promise<boolean>` signature.",
      "systemPrompt": "You are generating an AGENTS.md file — a directory-level overview for AI coding assistants.\n\nCRITICAL: Output ONLY the raw markdown content. No code fences, no preamble, no explanations, no conversational text. Your entire response IS the AGENTS.md file content.\n\nOUTPUT FORMAT:\n- First line MUST be exactly: <!-- Generated by agents-reverse-engineer -->\n- Use a # heading with the directory name\n- Write a one-paragraph purpose statement for the directory\nusage\nADAPTIVE SECTIONS:\nAnalyze the directory contents and choose the most relevant sections. Do NOT use a fixed template. Instead, select sections that best document this specific directory for an AI that needs to reconstruct or extend the project.\n\nConsider these section types (choose what applies):\n- **Contents**: Group files by purpose/category under ## headings. For each file: markdown link [filename](./filename) and a one-line description.\n- **Subdirectories**: If subdirectories exist, list them with links [dirname/](./dirname/) and brief summaries.\n- **Architecture / Data Flow**: If files form a pipeline, request/response chain, or layered architecture, document it.\n- **Stack**: If this is a package root (has package.json, Cargo.toml, go.mod, etc.), document the technology stack, key scripts, and entry points.\n- **Structure**: If the directory layout follows a convention (feature-sliced, domain-driven, MVC, etc.), document it.\n- **Patterns**: If files share recurring design patterns (factory, strategy, middleware, barrel re-export), name and document them.\n- **Configuration**: If the directory contains config files, schemas, or environment definitions, document the config surface area.\n- **API Surface**: If the directory exports a public API (barrel index, route definitions, SDK), document the interface contract.\n- **File Relationships**: How files collaborate, depend on each other, or share state.\n- **Behavioral Contracts**: If files contain regex patterns, format specifications, magic constants, or template strings that define observable behavior, collect them in a dedicated section. Preserve verbatim patterns from file summaries — do NOT paraphrase regex into prose. This section is MANDATORY when file summaries contain behavioral artifacts.\n\nChoose any relevant sections or create your own based on the directory contents. The goal is to provide a comprehensive overview that captures the essence of the directory's role in the project and how its files work together, with a focus on what an AI coding assistant would need to know to effectively interact with this code.\n\nSCOPE:\n- AGENTS.md is a NAVIGATIONAL INDEX — help an AI find the right file quickly\n- Focus on: what each file does, how files relate, directory-level patterns\n- Do NOT reproduce full architecture sections — those belong in the root CLAUDE.md\n\nPATH ACCURACY (MANDATORY):\n- When referencing files or modules outside this directory, use ONLY paths from the \"Import Map\" section\n- Do NOT invent, rename, or guess module paths — if a path isn't in the Import Map, don't reference it\n- Use the exact directory names from \"Project Directory Structure\" — do NOT rename directories\n  (e.g., if the directory is called \"cli\", write \"src/cli/\", NOT \"src/commands/\")\n- Cross-module references must use the specifier format from actual import statements\n  (e.g., \"../generation/writers/sum.js\", NOT \"../fs/sum-file.js\")\n- If you are unsure about a path, omit the cross-reference rather than guessing\n\nCONSISTENCY (MANDATORY):\n- Do not contradict yourself within the same document\n- If you describe a technique (e.g., \"regex-based\"), do not call it something else later (e.g., \"AST-based\")\n- When stating version numbers, engines, or config fields, use ONLY values present in the file summaries\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this directory\", \"this module\", \"provides\", \"responsible for\", \"is used to\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Per-file descriptions in Contents sections: 1-2 sentences maximum. Reference key symbols but do not reproduce full summaries.\n- Behavioral contracts (regex patterns, format specs, constants) belong in a separate Behavioral Contracts section, not in per-file descriptions.\n- Subdirectory descriptions: 1-2 sentences maximum. Capture the directory's role, not its full contents.\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- Key exported symbols from file summaries MUST appear in the directory overview\n- Preserve exact casing of identifiers\n\nUSER NOTES:\n- If \"User Notes\" are provided in the prompt, they contain user-defined instructions that will be automatically prepended to your output\n- Do NOT repeat or paraphrase user notes in your generated content — they are included separately\n- You may reference information from user notes for context",
      "response": "<!-- Generated by agents-reverse-engineer -->\n\n# src/discovery\n\nFile discovery pipeline orchestrating four-filter chain execution (gitignore → vendor → binary → custom) after fast-glob directory traversal, returning attributed inclusion/exclusion results.\n\n## Contents\n\n**[run.ts](./run.ts)** — Exports `discoverFiles()` facade creating filter chain (GitignoreFilter → VendorFilter → BinaryFilter → CustomPatternFilter) via factory functions, invoking `walkDirectory()` with symlink control, applying filters via `applyFilters()` with trace/debug propagation, returning `FilterResult` with per-file attribution.\n\n**[types.ts](./types.ts)** — Defines `FileFilter` interface (`name: string`, `shouldExclude(path, stats?): boolean | Promise<boolean>`), `ExcludedFile` record (`path`, `reason`, `filter`), `FilterResult` aggregate (`included: string[]`, `excluded: ExcludedFile[]`), `WalkerOptions` config (`cwd`, `followSymlinks?`, `dot?`).\n\n**[walker.ts](./walker.ts)** — Implements `walkDirectory(options: WalkerOptions): Promise<string[]>` via `fg.glob('**/*', {absolute: true, onlyFiles: true, suppressErrors: true, ignore: ['**/.git/**']})` with cwd-relative traversal, dotfile inclusion (default true), symlink control (default false).\n\n## Subdirectories\n\n**[filters/](./filters/)** — Four filter implementations: `binary.ts` (extension Set + size threshold + content analysis), `gitignore.ts` (`.gitignore` parser via `ignore` library), `vendor.ts` (Set lookup for single-segment + substring for path patterns), `custom.ts` (user-defined glob patterns). Orchestrator `index.ts` exports `applyFilters()` with 30-worker concurrency, short-circuit evaluation, per-filter statistics (`matched`/`rejected`), `filter:applied` trace events.\n\n## Architecture\n\n### Discovery Pipeline Flow\n\n1. **Configuration** — Caller (CLI commands) loads `DiscoveryConfig` subset from full config schema via `loadConfig()`\n2. **Filter Creation** — `discoverFiles()` constructs four filters in deterministic order: `createGitignoreFilter(root)`, `createVendorFilter(config.exclude.vendorDirs)`, `createBinaryFilter({maxFileSize, additionalExtensions: config.exclude.binaryExtensions})`, `createCustomFilter(config.exclude.patterns, root)`\n3. **Directory Walking** — Delegates to `walkDirectory({cwd: root, followSymlinks: config.options.followSymlinks})` returning absolute paths via fast-glob\n4. **Filter Application** — Invokes `applyFilters(files, filters, {tracer, debug})` from `filters/index.ts` executing short-circuit filter chain across 30 concurrent workers\n5. **Result Aggregation** — Returns `FilterResult` with `included` files passing all filters, `excluded` array containing `ExcludedFile` records with `filter`/`reason` attribution\n\n### Path Normalization Contract\n\nFilters receive absolute paths from walker, convert to relative via `path.relative(normalizedRoot, absolutePath)` for pattern matching (gitignore/custom patterns). Paths outside root (relative path starts with `..`) bypass filtering. Walker emits absolute paths via `fast-glob` option `{absolute: true}`.\n\n### Filter Interface Polymorphism\n\n`FileFilter.shouldExclude()` supports both sync/async implementations via union return type `boolean | Promise<boolean>`. Binary filter uses async `fs.stat()` + `isbinaryfile.isBinaryFile()`, others use sync string matching. Filter chain executor awaits all promises via `Promise.resolve()` wrapper.\n\n### Configuration Structural Typing\n\n`DiscoveryConfig` interface in `run.ts` defines minimal subset of full `Config` schema from `src/config/schema.ts` containing only `exclude.*`, `options.maxFileSize`, `options.followSymlinks` fields. Enables unit testing with minimal mock objects, prevents tight coupling to entire config schema.\n\n## Integration Points\n\n**Callers**: `src/cli/discover.ts`, `src/cli/generate.ts`, `src/cli/update.ts` invoke `discoverFiles()` with config from `loadConfig()`.\n\n**Consumers**: `FilterResult.included` feeds Phase 1 file analysis worker pool in `src/generation/executor.ts`, `FilterResult.excluded` populates `GENERATION-PLAN.md` statistics via `PlanTracker.writeGenerationPlan()`.\n\n**Telemetry**: `ITraceWriter` from `src/orchestration/trace.ts` emits `filter:applied` events with per-filter match/rejection counts when `--trace` flag enabled.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 1144,
      "cacheReadTokens": 17477,
      "cacheCreationTokens": 12235,
      "latencyMs": 49480,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": []
    },
    {
      "timestamp": "2026-02-09T16:13:38.909Z",
      "prompt": "Generate AGENTS.md for directory: \"src/quality\" (quality)\n\n## File Summaries (2 files)\n\n### index.ts\n**Purpose:** Public API barrel module for quality analysis subsystem, re-exporting inconsistency detection functions (`checkCodeVs...\n\n**Public API barrel module for quality analysis subsystem, re-exporting inconsistency detection functions (`checkCodeVsDoc`, `checkCodeVsCode`, `checkPhantomPaths`), reporting utilities (`buildInconsistencyReport`, `formatReportForCli`), density validation (`validateFindability`), and all discriminated union types for inconsistency classification.**\n\n## Exported Types\n\nRe-exports from `./types.js`:\n- `InconsistencySeverity` — Severity classification type\n- `CodeDocInconsistency` — Discriminated union member for missing exported symbols in `.sum` documentation\n- `CodeCodeInconsistency` — Discriminated union member for duplicate symbol exports across multiple files\n- `PhantomPathInconsistency` — Discriminated union member for unresolved path references in `AGENTS.md`\n- `Inconsistency` — Discriminated union of all inconsistency types\n- `InconsistencyReport` — Container type with `metadata`, `issues[]`, and `summary` aggregation\n\nRe-exports from `./density/validator.js`:\n- `FindabilityResult` — Return type from `validateFindability()` containing density metrics\n\n## Code-vs-Doc Validation\n\nExports from `./inconsistency/code-vs-doc.js`:\n- `extractExports(filePath: string): string[]` — Regex-based extraction of exported symbols via pattern `/^[ \\t]*export\\s+(?:default\\s+)?(?:function|class|const|let|var|type|interface|enum)\\s+(\\w+)/gm`\n- `checkCodeVsDoc(filePath: string, sumContent: string): CodeDocInconsistency | null` — Verifies all extracted exports appear in `.sum` summary text via substring search, returns `null` on pass or `CodeDocInconsistency` with `missingFromDoc` array on failure\n\n## Code-vs-Code Validation\n\nExports from `./inconsistency/code-vs-code.js`:\n- `checkCodeVsCode(files: string[]): CodeCodeInconsistency[]` — Aggregates exports across per-directory file groups into `Map<symbol, string[]>`, returns array of `CodeCodeInconsistency` for symbols appearing in multiple files with pattern `'duplicate-export'`\n\n## Phantom Path Detection\n\nExports from `./phantom-paths/index.js`:\n- `checkPhantomPaths(agentsMdPath: string): PhantomPathInconsistency[]` — Extracts path-like strings from `AGENTS.md` via three regex patterns (markdown links `/\\[(?:[^\\]]*)\\]\\((\\.[^)]+)\\)/g`, backtick-quoted paths `` /`((?:src\\/|\\.\\.?\\/)[^`]+\\.[a-z]{1,4})`/g ``, prose-embedded `/(?:from|in|by|via|see)\\s+`?(src\\/[\\w\\-./]+)`?/gi`), resolves against AGENTS.md directory and project root with `.ts`/`.js` fallback, returns array of unresolved references\n\n## Inconsistency Reporting\n\nExports from `./inconsistency/reporter.js`:\n- `buildInconsistencyReport(issues: Inconsistency[], metadata: object): InconsistencyReport` — Constructs report with timestamp, projectRoot, filesChecked, durationMs metadata and summary counts by type/severity\n- `formatReportForCli(report: InconsistencyReport): string` — Formats `InconsistencyReport` for terminal display with ANSI color codes via picocolors\n\n## Density Validation\n\nExports from `./density/validator.js`:\n- `validateFindability(sumContent: string): FindabilityResult` — Disabled feature after removal of structured `publicInterface` from `SumFileContent` schema, returns density metrics for identifier-rich summary validation\n\n## Module Organization\n\nBarrel pattern aggregates four subsystems:\n1. **Inconsistency detection** — code-vs-doc (exports missing from summaries), code-vs-code (duplicate symbols), phantom-paths (unresolved references in `AGENTS.md`)\n2. **Reporting** — `buildInconsistencyReport()` aggregation, `formatReportForCli()` terminal rendering\n3. **Density validation** — `validateFindability()` checks for identifier density in summaries (currently disabled)\n4. **Shared types** — Discriminated unions for inconsistency classification with `pattern` discriminator field\n\nConsumer modules import via `import { checkCodeVsDoc, buildInconsistencyReport } from '@/quality'` instead of direct submodule paths.\n### types.ts\n**Purpose:** types.ts defines the discriminated union types for quality validation inconsistencies detected across code-vs-doc, co...\n\n**types.ts defines the discriminated union types for quality validation inconsistencies detected across code-vs-doc, code-vs-code, and phantom-path analysis, plus the structured report format for validation output.**\n\n## Exported Types\n\n**InconsistencySeverity**: String literal union `'info' | 'warning' | 'error'` classifying inconsistency impact levels.\n\n**CodeDocInconsistency**: Interface with `type: 'code-vs-doc'` discriminator representing mismatches between exported symbols in source files and their `.sum` documentation. Contains `severity: InconsistencySeverity`, `filePath: string` (source file), `sumPath: string` (corresponding `.sum` file), `description: string`, and `details` object with `missingFromDoc: string[]` (symbols exported but undocumented), `missingFromCode: string[]` (symbols documented but not exported), and optional `purposeMismatch?: string` for contradictory purpose statements.\n\n**CodeCodeInconsistency**: Interface with `type: 'code-vs-code'` discriminator representing conflicts between multiple source files. Contains `severity: InconsistencySeverity`, `files: string[]` (conflicting file paths), `description: string`, and `pattern: string` identifying the detected inconsistency pattern (e.g., `'duplicate-export'` for symbols appearing across multiple files).\n\n**PhantomPathInconsistency**: Interface with `type: 'phantom-path'` discriminator representing unresolvable path references in generated `AGENTS.md` documentation. Contains `severity: InconsistencySeverity`, `agentsMdPath: string` (document containing the reference), `description: string`, and `details` object with `referencedPath: string` (raw phantom path), `resolvedTo: string` (resolution attempt context: project root or AGENTS.md location), and `context: string` (line of text containing the reference).\n\n**Inconsistency**: Discriminated union `CodeDocInconsistency | CodeCodeInconsistency | PhantomPathInconsistency` enabling type-safe pattern matching via the `type` discriminator field.\n\n**InconsistencyReport**: Interface structuring validation output with `metadata` object (contains `timestamp: string`, `projectRoot: string`, `filesChecked: number`, `durationMs: number`), `issues: Inconsistency[]` array of all detected problems, and `summary` object with aggregate counts (`total: number`, `codeVsDoc: number`, `codeVsCode: number`, `phantomPaths: number`, `errors: number`, `warnings: number`, `info: number`).\n\n## Integration Points\n\nConsumed by quality validation modules:\n- `src/quality/inconsistency/code-vs-doc.ts`: Produces `CodeDocInconsistency` instances via regex-based export extraction and substring matching\n- `src/quality/inconsistency/code-vs-code.ts`: Produces `CodeCodeInconsistency` instances via cross-file export aggregation into `Map<symbol, string[]>`\n- `src/quality/phantom-paths/validator.ts`: Produces `PhantomPathInconsistency` instances via regex path extraction and `existsSync()` resolution\n- `src/quality/inconsistency/reporter.ts`: Aggregates all inconsistency types into `InconsistencyReport` with formatted output\n\n## Design Pattern\n\nUses discriminated union pattern with `type` literal field enabling exhaustive type narrowing:\n```typescript\nswitch (inconsistency.type) {\n  case 'code-vs-doc': /* access missingFromDoc, missingFromCode */ break;\n  case 'code-vs-code': /* access files, pattern */ break;\n  case 'phantom-path': /* access referencedPath, resolvedTo */ break;\n}\n```\n\n## Project Directory Structure\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\n\n## Subdirectories\n\n### density/\n<!-- Generated by agents-reverse-engineer -->\n\n# src/quality/density\n\nDisabled findability validation stub that previously verified exported symbols from .sum files appeared in parent AGENTS.md content, retained for future structured metadata extraction support.\n\n## Contents\n\n**[validator.ts](./validator.ts)** — Exports `validateFindability()` returning empty array; previously extracted symbol names from `SumFileContent.metadata.publicInterface` and performed substring search in AGENTS.md content to compute per-file `FindabilityResult` with score calculation.\n\n## Exported Interface\n\n**FindabilityResult** — Validation result containing `filePath`, `symbolsTested[]`, `symbolsFound[]`, `symbolsMissing[]`, and `score` (0.0–1.0 ratio of found to tested symbols).\n\n**validateFindability()** — `(agentsMdContent: string, sumFiles: Map<string, SumFileContent>) => FindabilityResult[]` — Returns empty array; signature preserved but implementation gutted after `SumFileContent.metadata.publicInterface` removal.\n\n## Disabled Feature Context\n\nvalidator.ts implements LLM-free validation using string-based symbol matching with no AI subprocess calls. Disabled in `../index.ts` quality validation pipeline when `publicInterface` field removed from `SumFileContent` schema in `../../generation/writers/sum.ts`. Previously detected when directory-level AGENTS.md aggregation failed to preserve critical symbol names from child .sum file summaries.\n\n## Restoration Path\n\nRe-implementation requires:\n1. Adding structured export extraction to .sum file generation (Phase 1 of three-phase pipeline in `../../generation/orchestrator.ts`)\n2. Parsing YAML frontmatter in .sum files via `readSumFile()` from `../../generation/writers/sum.ts`\n3. Implementing symbol presence checks against AGENTS.md content\n4. Re-enabling in `../index.ts` quality validator orchestrator\n\n## Type Dependencies\n\nImports `SumFileContent` from `../../generation/writers/sum.ts` — Interface for parsed .sum file containing YAML frontmatter (`generated_at`, `content_hash`, `purpose`, `critical_todos[]`, `related_files[]`) and markdown summary content.\n### inconsistency/\n<!-- Generated by agents-reverse-engineer -->\n\n# src/quality/inconsistency\n\nValidates code-documentation consistency by extracting exports from source files, verifying their presence in `.sum` documentation, detecting duplicate exports across file groups, and aggregating validation results into structured reports with CLI-formatted output.\n\n## Contents\n\n### Validators\n\n**[code-vs-doc.ts](./code-vs-doc.ts)** — `extractExports()` extracts exported identifiers from TypeScript/JavaScript source via regex pattern `/^[ \\t]*export\\s+(?:default\\s+)?(?:function|class|const|let|var|type|interface|enum)\\s+(\\w+)/gm`. `checkCodeVsDoc()` verifies extracted exports appear in `.sum` file `summary` text via substring search, returning `CodeDocInconsistency` with `missingFromDoc[]` array when drift detected, or `null` when consistent.\n\n**[code-vs-code.ts](./code-vs-code.ts)** — `checkCodeVsCode()` detects duplicate exports across file groups by aggregating `extractExports()` results into `Map<symbol, paths[]>`, returning `CodeCodeInconsistency[]` array for symbols exported from multiple files with `pattern: 'duplicate-export'` and `severity: 'warning'`.\n\n### Report Generation\n\n**[reporter.ts](./reporter.ts)** — `buildInconsistencyReport()` aggregates `Inconsistency[]` into `InconsistencyReport` with summary counts by type (`code-vs-doc`, `code-vs-code`, `phantom-path`) and severity (`error`, `warning`, `info`). `formatReportForCli()` transforms report into plain-text output with severity tags `[ERROR]`/`[WARN]`/`[INFO]` and type-specific field rendering for stderr and `progress.log` output.\n\n## Validation Algorithm\n\n**Phase 1: Export Extraction**  \nRegex-based identifier capture from source declarations (`export function foo`, `export const BAR`, `export default class App`). Matches capture group 1 containing symbol name, filtering lines matching `/^[ \\t]*export\\s+(?:default\\s+)?(?:function|class|const|let|var|type|interface|enum)\\s+(\\w+)/gm`.\n\n**Phase 2: Documentation Verification**  \nSubstring search against `.sum` file `summary` text. Export name must appear anywhere in summary content to pass validation. Missing symbols populate `CodeDocInconsistency.details.missingFromDoc[]` array.\n\n**Phase 3: Cross-File Duplicate Detection**  \nAggregates exports into `Map<symbol, string[]>` where keys are export names and values are file paths. Filters map entries where `paths.length > 1`, constructing `CodeCodeInconsistency` records with formatted description `\"Symbol \\\"${name}\\\" exported from ${paths.length} files\"`.\n\n**Phase 4: Report Aggregation**  \nMerges inconsistency arrays from code-vs-doc, code-vs-code, and phantom-path validators into single `InconsistencyReport` with summary counts. Populates metadata (timestamp, projectRoot, filesChecked, durationMs) and discriminates union via `issue.type` for CLI rendering.\n\n## Behavioral Contracts\n\n**Regex Export Pattern (code-vs-doc.ts, code-vs-code.ts):**  \n`/^[ \\t]*export\\s+(?:default\\s+)?(?:function|class|const|let|var|type|interface|enum)\\s+(\\w+)/gm`\n\n**CLI Report Severity Tags (reporter.ts):**  \n- `severity: 'error'` → `[ERROR]`  \n- `severity: 'warning'` → `[WARN]`  \n- `severity: 'info'` → `[INFO]`\n\n**Type-Specific Field Rendering (reporter.ts):**  \n- `code-vs-doc`: `File: ${filePath}`  \n- `code-vs-code`: `Files: ${files.join(', ')}`  \n- `phantom-path`: `Doc: ${agentsMdPath}`, `Path: ${referencedPath}`\n\n## Integration with Quality Pipeline\n\nCalled by `src/quality/index.ts` orchestrator after per-file validation. `checkCodeVsDoc()` invoked for each source file with parsed `.sum` content. `checkCodeVsCode()` receives scoped per-directory file groups to avoid false positives across unrelated modules. Results merged with phantom-path validation output, passed to `buildInconsistencyReport()` with run metadata (filesChecked, durationMs), then formatted via `formatReportForCli()` for stderr and `progress.log` output.\n\n## Known Limitations\n\n**Regex-Based Export Extraction:**  \nMisses destructured exports (`export { foo, bar }`), namespace exports (`export * from './module'`), dynamic exports, multiline declarations. No AST analysis—purely heuristic pattern matching.\n\n**Substring Verification:**  \nProduces false negatives when export names appear in comments, prose descriptions, or unrelated context rather than as documented API surface. No structured field verification after `publicInterface` schema removal.\n\n**Intentional Duplication:**  \nCode-vs-code validator cannot distinguish factory patterns, parallel implementations, or other intentional duplication without semantic analysis. Caller must scope input to per-directory groups to minimize false positives.\n### phantom-paths/\n<!-- Generated by agents-reverse-engineer -->\n\n# src/quality/phantom-paths\n\nDetects unresolvable path references in `AGENTS.md` files by extracting path-like strings via regex patterns, resolving them against both the `AGENTS.md` directory and project root with `.ts`/`.js` fallback logic, and returning `PhantomPathInconsistency` objects for non-existent paths.\n\n## Contents\n\n### [index.ts](./index.ts)\nBarrel re-export consolidating `checkPhantomPaths` validator for single import point.\n\n### [validator.ts](./validator.ts)\nCore validation logic extracting paths via `PATH_PATTERNS`, resolving with extension fallback, reporting `PhantomPathInconsistency` for unresolved references.\n\n## Exported Interface\n\n- **`checkPhantomPaths(agentsMdPath, content, projectRoot)`** — Validates path references in `AGENTS.md` content, returns `PhantomPathInconsistency[]` with `type: 'phantom-path'`, `severity: 'warning'`, `referencedPath`, `resolvedTo`, and 120-char `context` excerpt\n\n## Path Extraction Patterns\n\nThree regex patterns (`PATH_PATTERNS`) capture references:\n\n```javascript\n/\\[(?:[^\\]]*)\\]\\((\\.[^)]+)\\)/g              // Markdown links: [text](./path)\n/`((?:src\\/|\\.\\.?\\/)[^`]+\\.[a-z]{1,4})`/g  // Backtick paths: `src/foo/bar.ts`\n/(?:from|in|by|via|see)\\s+`?(src\\/[\\w\\-./]+)`?/gi // Prose paths: \"from src/foo/\"\n```\n\n`SKIP_PATTERNS` excludes `node_modules`, `.git/`, URLs (`https?:`), template syntax (`{{`, `${`), glob wildcards (`*`), brace expansions (`{a,b,c}`).\n\n## Resolution Strategy\n\nAttempts resolution in order: (1) relative to `agentsMdDir` via `path.dirname(agentsMdPath)`, (2) relative to `projectRoot`, (3) `.js` → `.ts` substitution for TypeScript import convention. Uses `existsSync()` to validate at least one candidate exists. Deduplicates via `seen` Set tracking `rawPath` strings.\n\n## Integration Context\n\nPart of quality validation subsystem (`src/quality/`) alongside code-vs-doc consistency (`../inconsistency/code-vs-doc.js`) and code-vs-code duplicate detection (`../inconsistency/code-vs-code.js`). Invoked during post-generation validation phase via `src/quality/index.ts` to identify broken path references that may mislead AI coding assistants.",
      "systemPrompt": "You are generating an AGENTS.md file — a directory-level overview for AI coding assistants.\n\nCRITICAL: Output ONLY the raw markdown content. No code fences, no preamble, no explanations, no conversational text. Your entire response IS the AGENTS.md file content.\n\nOUTPUT FORMAT:\n- First line MUST be exactly: <!-- Generated by agents-reverse-engineer -->\n- Use a # heading with the directory name\n- Write a one-paragraph purpose statement for the directory\nusage\nADAPTIVE SECTIONS:\nAnalyze the directory contents and choose the most relevant sections. Do NOT use a fixed template. Instead, select sections that best document this specific directory for an AI that needs to reconstruct or extend the project.\n\nConsider these section types (choose what applies):\n- **Contents**: Group files by purpose/category under ## headings. For each file: markdown link [filename](./filename) and a one-line description.\n- **Subdirectories**: If subdirectories exist, list them with links [dirname/](./dirname/) and brief summaries.\n- **Architecture / Data Flow**: If files form a pipeline, request/response chain, or layered architecture, document it.\n- **Stack**: If this is a package root (has package.json, Cargo.toml, go.mod, etc.), document the technology stack, key scripts, and entry points.\n- **Structure**: If the directory layout follows a convention (feature-sliced, domain-driven, MVC, etc.), document it.\n- **Patterns**: If files share recurring design patterns (factory, strategy, middleware, barrel re-export), name and document them.\n- **Configuration**: If the directory contains config files, schemas, or environment definitions, document the config surface area.\n- **API Surface**: If the directory exports a public API (barrel index, route definitions, SDK), document the interface contract.\n- **File Relationships**: How files collaborate, depend on each other, or share state.\n- **Behavioral Contracts**: If files contain regex patterns, format specifications, magic constants, or template strings that define observable behavior, collect them in a dedicated section. Preserve verbatim patterns from file summaries — do NOT paraphrase regex into prose. This section is MANDATORY when file summaries contain behavioral artifacts.\n\nChoose any relevant sections or create your own based on the directory contents. The goal is to provide a comprehensive overview that captures the essence of the directory's role in the project and how its files work together, with a focus on what an AI coding assistant would need to know to effectively interact with this code.\n\nSCOPE:\n- AGENTS.md is a NAVIGATIONAL INDEX — help an AI find the right file quickly\n- Focus on: what each file does, how files relate, directory-level patterns\n- Do NOT reproduce full architecture sections — those belong in the root CLAUDE.md\n\nPATH ACCURACY (MANDATORY):\n- When referencing files or modules outside this directory, use ONLY paths from the \"Import Map\" section\n- Do NOT invent, rename, or guess module paths — if a path isn't in the Import Map, don't reference it\n- Use the exact directory names from \"Project Directory Structure\" — do NOT rename directories\n  (e.g., if the directory is called \"cli\", write \"src/cli/\", NOT \"src/commands/\")\n- Cross-module references must use the specifier format from actual import statements\n  (e.g., \"../generation/writers/sum.js\", NOT \"../fs/sum-file.js\")\n- If you are unsure about a path, omit the cross-reference rather than guessing\n\nCONSISTENCY (MANDATORY):\n- Do not contradict yourself within the same document\n- If you describe a technique (e.g., \"regex-based\"), do not call it something else later (e.g., \"AST-based\")\n- When stating version numbers, engines, or config fields, use ONLY values present in the file summaries\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this directory\", \"this module\", \"provides\", \"responsible for\", \"is used to\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Per-file descriptions in Contents sections: 1-2 sentences maximum. Reference key symbols but do not reproduce full summaries.\n- Behavioral contracts (regex patterns, format specs, constants) belong in a separate Behavioral Contracts section, not in per-file descriptions.\n- Subdirectory descriptions: 1-2 sentences maximum. Capture the directory's role, not its full contents.\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- Key exported symbols from file summaries MUST appear in the directory overview\n- Preserve exact casing of identifiers\n\nUSER NOTES:\n- If \"User Notes\" are provided in the prompt, they contain user-defined instructions that will be automatically prepended to your output\n- Do NOT repeat or paraphrase user notes in your generated content — they are included separately\n- You may reference information from user notes for context",
      "response": "<!-- Generated by agents-reverse-engineer -->\n\n# src/quality\n\nCode-documentation consistency validation subsystem executing three detection strategies: regex-based export extraction with substring matching (`code-vs-doc`), cross-file duplicate symbol aggregation (`code-vs-code`), and regex-based path reference resolution with filesystem verification (`phantom-paths`).\n\n## Contents\n\n**[index.ts](./index.ts)** — Barrel module re-exporting `checkCodeVsDoc()`, `checkCodeVsCode()`, `checkPhantomPaths()` validators, `buildInconsistencyReport()`, `formatReportForCli()` reporters, disabled `validateFindability()` density validator, and discriminated union types (`CodeDocInconsistency`, `CodeCodeInconsistency`, `PhantomPathInconsistency`, `Inconsistency`, `InconsistencyReport`).\n\n**[types.ts](./types.ts)** — Defines `InconsistencySeverity` (`'info' | 'warning' | 'error'`), `CodeDocInconsistency` (exported symbols missing from `.sum` with `missingFromDoc[]`), `CodeCodeInconsistency` (duplicate exports across files with `pattern: 'duplicate-export'`), `PhantomPathInconsistency` (unresolved paths in `AGENTS.md` with `referencedPath`, `resolvedTo`, `context`), `Inconsistency` discriminated union, `InconsistencyReport` container with metadata (`timestamp`, `projectRoot`, `filesChecked`, `durationMs`) and summary counts.\n\n## Subdirectories\n\n**[inconsistency/](./inconsistency/)** — code-vs-doc validator extracting exports via `/^[ \\t]*export\\s+(?:default\\s+)?(?:function|class|const|let|var|type|interface|enum)\\s+(\\w+)/gm` with substring verification against `.sum` content; code-vs-code detector aggregating exports into `Map<symbol, paths[]>` to identify duplicates; reporter building aggregated `InconsistencyReport` with formatted CLI output.\n\n**[phantom-paths/](./phantom-paths/)** — Path reference validator extracting strings via three regex patterns (markdown links, backtick paths, prose references), resolving against `AGENTS.md` directory and project root with `.ts`/`.js` fallback, returning `PhantomPathInconsistency[]` for unresolved references.\n\n**[density/](./density/)** — Disabled `validateFindability()` stub previously verifying exported symbols from `.sum` files appeared in parent `AGENTS.md`; implementation gutted after `SumFileContent.metadata.publicInterface` removal.\n\n## Validation Workflow\n\n**Code-vs-Doc Consistency:**  \n`extractExports(filePath)` scans source via regex capturing export identifiers → `checkCodeVsDoc(filePath, sumContent)` performs substring search against `.sum` summary text → returns `CodeDocInconsistency` with `missingFromDoc[]` when drift detected or `null` when consistent.\n\n**Code-vs-Code Duplicate Detection:**  \n`checkCodeVsCode(files[])` aggregates `extractExports()` results into `Map<symbol, string[]>` → filters entries with `paths.length > 1` → returns `CodeCodeInconsistency[]` with `pattern: 'duplicate-export'` and `severity: 'warning'`.\n\n**Phantom Path Resolution:**  \n`checkPhantomPaths(agentsMdPath, content, projectRoot)` applies three regex patterns to extract path references → attempts resolution via `existsSync()` against `AGENTS.md` directory and project root with extension substitution → returns `PhantomPathInconsistency[]` for unresolved paths with 120-char context excerpt.\n\n**Report Aggregation:**  \n`buildInconsistencyReport(issues[], metadata)` merges inconsistency arrays → computes summary counts by `type` (`code-vs-doc`, `code-vs-code`, `phantom-path`) and `severity` (`error`, `warning`, `info`) → returns `InconsistencyReport` with timestamp metadata. `formatReportForCli(report)` transforms into plain-text output with severity tags (`[ERROR]`, `[WARN]`, `[INFO]`) and type-specific field rendering for stderr/`progress.log`.\n\n## Behavioral Contracts\n\n**Export Extraction Regex (inconsistency/):**  \n`/^[ \\t]*export\\s+(?:default\\s+)?(?:function|class|const|let|var|type|interface|enum)\\s+(\\w+)/gm`\n\n**Path Extraction Patterns (phantom-paths/validator.ts):**  \n```javascript\n/\\[(?:[^\\]]*)\\]\\((\\.[^)]+)\\)/g              // Markdown: [text](./path)\n/`((?:src\\/|\\.\\.?\\/)[^`]+\\.[a-z]{1,4})`/g  // Backtick: `src/foo.ts`\n/(?:from|in|by|via|see)\\s+`?(src\\/[\\w\\-./]+)`?/gi // Prose: \"from src/foo/\"\n```\n\n**Severity Tags (inconsistency/reporter.ts):**  \n`severity: 'error'` → `[ERROR]`, `severity: 'warning'` → `[WARN]`, `severity: 'info'` → `[INFO]`\n\n**Type Discriminator Field:**  \n`type: 'code-vs-doc' | 'code-vs-code' | 'phantom-path'` enables exhaustive switch narrowing.\n\n## Known Limitations\n\n**Regex-Based Export Extraction:**  \nMisses destructured (`export { foo }`), namespace (`export *`), dynamic exports, multiline declarations. No AST analysis—purely heuristic pattern matching.\n\n**Substring Verification:**  \nFalse negatives when export names appear in comments/prose rather than documented API surface. No structured field verification after `publicInterface` schema removal.\n\n**Intentional Duplication:**  \nCode-vs-code cannot distinguish factory patterns, parallel implementations, intentional re-exports. Caller must scope input to per-directory groups to minimize false positives.\n\n**Extension Fallback Logic:**  \nPath resolution attempts `.js` → `.ts` substitution but doesn't handle `.jsx`/`.tsx`, `.cjs`/`.mjs`, or index file resolution (`./foo` → `./foo/index.ts`).",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 1541,
      "cacheReadTokens": 17477,
      "cacheCreationTokens": 13527,
      "latencyMs": 51858,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": []
    },
    {
      "timestamp": "2026-02-09T16:13:38.948Z",
      "prompt": "Generate AGENTS.md for directory: \"src/integration\" (integration)\n\n## File Summaries (4 files)\n\n### detect.ts\n**Purpose:** detectEnvironments() and hasEnvironment() identify AI coding assistant runtime environments (Claude Code, OpenCode, A...\n\n**detectEnvironments() and hasEnvironment() identify AI coding assistant runtime environments (Claude Code, OpenCode, Aider) via filesystem marker detection.**\n\n## Exported Functions\n\n**detectEnvironments(projectRoot: string): DetectedEnvironment[]**\nScans `projectRoot` for AI assistant configuration markers and returns array of detected environments. Checks three patterns:\n- Claude Code: `.claude/` directory OR `CLAUDE.md` file existence\n- OpenCode: `.opencode/` directory existence\n- Aider: `.aider.conf.yml` file OR `.aider/` directory existence\n\nReturns `DetectedEnvironment[]` objects with fields: `type: EnvironmentType`, `configDir: string`, `detected: true`.\n\n**hasEnvironment(projectRoot: string, type: EnvironmentType): boolean**\nPredicate wrapper around `detectEnvironments()` that returns `true` if specified `EnvironmentType` exists in project. Uses `Array.some()` to match `env.type === type`.\n\n## Detection Logic\n\nUses `existsSync()` from `node:fs` and `path.join()` for absolute path construction. No async I/O—all checks synchronous.\n\n**Claude Code detection:**\n- Primary marker: `.claude/` directory\n- Fallback marker: `CLAUDE.md` root file\n- Returns `configDir: '.claude'` regardless of detection path\n\n**OpenCode detection:**\n- Single marker: `.opencode/` directory\n- Returns `configDir: '.opencode'`\n\n**Aider detection:**\n- Primary marker: `.aider.conf.yml` file\n- Fallback marker: `.aider/` directory\n- Returns `configDir: '.aider'` regardless of detection path\n\n## Type Dependencies\n\nImports `DetectedEnvironment` and `EnvironmentType` from `./types.js`. `DetectedEnvironment` contains discriminated union with `type` field matching `'claude' | 'opencode' | 'aider'`.\n\n## Integration Points\n\nCalled by `src/integration/generate.ts` to determine which platform-specific templates to generate (`CLAUDE.md`, `OPENCODE.md`, etc.). Used by installer modules (`src/installer/`) to validate target environment presence before hook installation.\n### generate.ts\n**Purpose:** generateIntegrationFiles() orchestrates AI assistant integration setup by detecting environments, retrieving platform...\n\n**generateIntegrationFiles() orchestrates AI assistant integration setup by detecting environments, retrieving platform-specific templates, writing command files, and copying bundled hook files from `hooks/dist/` to project directories.**\n\n## Exported Functions\n\n**generateIntegrationFiles(projectRoot: string, options?: GenerateOptions): Promise<IntegrationResult[]>**\n- Detects AI environments via `detectEnvironments()` unless `options.environment` overrides detection\n- Maps `EnvironmentType` to config directories: `claude` → `.claude`, `opencode` → `.opencode`, `aider` → `.aider`, `gemini` → `.gemini`\n- Iterates detected environments, retrieves templates via `getTemplatesForEnvironment()`, writes files via `writeFileSync()` with `ensureDir()` parent directory creation\n- Skips existing files unless `options.force` is true, populates `IntegrationResult.filesSkipped[]`\n- For `claude` environment specifically, copies bundled hook `are-session-end.js` from `hooks/dist/` to `.claude/hooks/are-session-end.js` via `readBundledHook()` → `writeFileSync()`\n- Returns `IntegrationResult[]` with `filesCreated[]` and `filesSkipped[]` per environment\n- Respects `options.dryRun` to skip actual writes\n\n## Internal Functions\n\n**getBundledHookPath(hookName: string): string**\n- Resolves path from `dist/integration/` up two levels to project root, then into `hooks/dist/{hookName}`\n- Uses `fileURLToPath(import.meta.url)` for ES module `__dirname` equivalent\n\n**readBundledHook(hookName: string): string**\n- Reads hook file content from path returned by `getBundledHookPath()`\n- Throws `Error` if `existsSync()` check fails with message pattern `\"Bundled hook not found: {hookPath}\"`\n\n**ensureDir(filePath: string): void**\n- Extracts directory via `path.dirname()`, creates recursively via `mkdirSync({ recursive: true })` if `existsSync()` returns false\n\n**getTemplatesForEnvironment(type: EnvironmentType): ReturnType<typeof getClaudeTemplates>**\n- Switch statement dispatching on `EnvironmentType`: `claude` → `getClaudeTemplates()`, `opencode` → `getOpenCodeTemplates()`, `gemini` → `getGeminiTemplates()`, `aider` → empty array `[]`\n- Returns array of template objects with `path` and `content` properties\n\n## Types\n\n**GenerateOptions interface**\n- `dryRun?: boolean` — Preview mode, no file writes\n- `force?: boolean` — Overwrite existing files instead of skipping\n- `environment?: EnvironmentType` — Bypass auto-detection, force specific environment\n\n## Integration Points\n\n- Imports `detectEnvironments()` from `./detect.js` for environment auto-detection\n- Imports `getClaudeTemplates()`, `getOpenCodeTemplates()`, `getGeminiTemplates()` from `./templates.js` for platform-specific command file content\n- Imports `IntegrationResult` and `EnvironmentType` from `./types.js`\n- File operations via Node.js `fs` module: `existsSync()`, `mkdirSync()`, `writeFileSync()`, `readFileSync()`\n- Path manipulation via `node:path` module and `fileURLToPath()` from `node:url`\n\n## File Path Patterns\n\n**Hook bundling convention:**\n- Hooks copied from `hooks/dist/{hookName}` (post-build artifacts from `scripts/build-hooks.js`)\n- Destination: `.claude/hooks/are-session-end.js` (only for `claude` environment)\n\n**Command file destinations:**\n- Platform-specific via templates: `.claude/skills/`, `.opencode/commands/`, `.gemini/commands/`\n### templates.ts\n**Purpose:** templates.ts generates platform-specific command file content (SKILL.md, .md, .toml) for Claude Code, OpenCode, and G...\n\n**templates.ts generates platform-specific command file content (SKILL.md, .md, .toml) for Claude Code, OpenCode, and Gemini CLI by substituting command prefix and version file path placeholders into shared command templates, then exporting via getClaudeTemplates(), getOpenCodeTemplates(), and getGeminiTemplates().**\n\n## Exported Functions\n\n- `getClaudeTemplates(): IntegrationTemplate[]` — returns Claude Code SKILL.md templates for all commands\n- `getOpenCodeTemplates(): IntegrationTemplate[]` — returns OpenCode .md command templates\n- `getGeminiTemplates(): IntegrationTemplate[]` — returns Gemini .toml command templates\n\n## Command Definitions\n\n`COMMANDS` object defines six command templates: `generate`, `update`, `init`, `discover`, `clean`, `specify`, `help`. Each has `description`, `argumentHint`, and `content` properties. Content uses placeholder strings `COMMAND_PREFIX` (replaced with `/are-` for all platforms), `VERSION_FILE_PATH` (replaced with `.claude/ARE-VERSION`, `.opencode/ARE-VERSION`, or `.gemini/ARE-VERSION`), and `$ARGUMENTS` (preserves user-supplied CLI flags).\n\n## Platform Configuration\n\n`PLATFORM_CONFIGS` maps `Platform` type (`'claude' | 'opencode' | 'gemini'`) to `PlatformConfig` objects:\n\n- **Claude**: `commandPrefix: '/are-'`, `pathPrefix: '.claude/skills/'`, `filenameSeparator: '.'`, `usesName: true`, `versionFilePath: '.claude/ARE-VERSION'`\n- **OpenCode**: `commandPrefix: '/are-'`, `pathPrefix: '.opencode/commands/'`, `filenameSeparator: '-'`, `extraFrontmatter: 'agent: build'`, `usesName: false`, `versionFilePath: '.opencode/ARE-VERSION'`\n- **Gemini**: `commandPrefix: '/are-'`, `pathPrefix: '.gemini/commands/'`, `filenameSeparator: '-'`, `usesName: false`, `versionFilePath: '.gemini/ARE-VERSION'`\n\n## Template Generation Strategy\n\n`buildTemplate()` constructs `IntegrationTemplate` with `filename`, `path`, and `content` properties. Claude produces `.claude/skills/are-{command}/SKILL.md`, OpenCode produces `.opencode/commands/are-{command}.md`, Gemini produces `.gemini/commands/are-{command}.toml`. `buildFrontmatter()` inserts YAML frontmatter with `name:` field only when `PlatformConfig.usesName` is true (Claude only), adds `description:` field for all platforms, appends `extraFrontmatter` if present (OpenCode's `agent: build`). `buildGeminiToml()` generates TOML format with `description = \"...\"` and triple-quoted `prompt = \"\"\"...\"\"\"` fields wrapping command content.\n\n## Command Content Patterns\n\nAll command templates except `help` and `clean` include background execution instructions using `run_in_background: true` with `TaskOutput` polling. Monitoring pattern appears verbatim in `generate`, `update`, `discover`, `specify` templates:\n\n```\n3. **Monitor progress by polling** `.agents-reverse-engineer/progress.log`:\n   - Wait ~15 seconds (use `sleep 15` in Bash), then use the **Read** tool to read `.agents-reverse-engineer/progress.log` (use the `offset` parameter to read only the last ~20 lines for long files)\n   - Show the user a brief progress update\n   - Check whether the background task has completed using `TaskOutput` with `block: false`\n   - Repeat until the background task finishes\n   - **Important**: Keep polling even if progress.log doesn't exist yet (the command takes a few seconds to start writing)\n```\n\n`discover` and `clean` commands include strict rule blocks enforcing zero flag addition beyond user input:\n\n```\n## STRICT RULES - VIOLATION IS FORBIDDEN\n\n1. Run ONLY this exact command: `npx agents-reverse-engineer@latest COMMAND $ARGUMENTS`\n2. DO NOT add ANY flags the user did not explicitly type\n3. If user typed nothing after `COMMAND_PREFIXcommand`, run with ZERO flags\n```\n\n`help` command template contains full reference documentation with command tables, configuration schema, generated file examples, and common workflows. Uses `<objective>` and `<reference>` XML tags. Instructs AI to read `VERSION_FILE_PATH` first and output reference without project-specific analysis.\n\n## Path Construction\n\nClaude commands use nested directory structure: `are-{command}/SKILL.md` under `.claude/skills/`. OpenCode and Gemini use flat structure: `are-{command}.md` or `are-{command}.toml` directly under `.opencode/commands/` or `.gemini/commands/`. `getTemplatesForPlatform()` iterates `COMMANDS` object entries, calls `buildTemplate()` for each, returns array of `IntegrationTemplate` objects consumed by installer module for file writes.\n### types.ts\n**Purpose:** Defines TypeScript types for AI coding assistant environment detection and integration template generation across Cla...\n\n**Defines TypeScript types for AI coding assistant environment detection and integration template generation across Claude Code, OpenCode, Gemini, and Aider platforms.**\n\n## Exported Types\n\n### EnvironmentType\n```typescript\ntype EnvironmentType = 'claude' | 'opencode' | 'aider' | 'gemini'\n```\nString literal union representing supported AI assistant platforms.\n\n### DetectedEnvironment\n```typescript\ninterface DetectedEnvironment {\n  type: EnvironmentType;\n  configDir: string;\n  detected: boolean;\n}\n```\nRepresents discovery result from environment detection: `type` identifies the AI platform, `configDir` stores the configuration directory path (e.g., `.claude`, `.opencode`), `detected` boolean indicates presence in project.\n\n### IntegrationTemplate\n```typescript\ninterface IntegrationTemplate {\n  filename: string;\n  path: string;\n  content: string;\n}\n```\nDescribes a single integration file to be written: `filename` is the base name (e.g., `'generate.md'`), `path` is the relative path from project root (e.g., `.claude/commands/ar/generate.md`), `content` holds the template text to write.\n\n### IntegrationResult\n```typescript\ninterface IntegrationResult {\n  environment: EnvironmentType;\n  filesCreated: string[];\n  filesSkipped: string[];\n}\n```\nAggregates outcome of integration file generation: `environment` identifies the target platform, `filesCreated` lists successfully written file paths, `filesSkipped` tracks paths that existed and were not overwritten.\n\n## Integration Points\n\nUsed by `src/integration/detect.ts` for environment detection return values and by `src/integration/generate.ts` for template construction and file writing results. Consumed by `src/installer/operations.ts` during npx-based command/hook installation workflow.\n\n## Project Directory Structure\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>",
      "systemPrompt": "You are generating an AGENTS.md file — a directory-level overview for AI coding assistants.\n\nCRITICAL: Output ONLY the raw markdown content. No code fences, no preamble, no explanations, no conversational text. Your entire response IS the AGENTS.md file content.\n\nOUTPUT FORMAT:\n- First line MUST be exactly: <!-- Generated by agents-reverse-engineer -->\n- Use a # heading with the directory name\n- Write a one-paragraph purpose statement for the directory\nusage\nADAPTIVE SECTIONS:\nAnalyze the directory contents and choose the most relevant sections. Do NOT use a fixed template. Instead, select sections that best document this specific directory for an AI that needs to reconstruct or extend the project.\n\nConsider these section types (choose what applies):\n- **Contents**: Group files by purpose/category under ## headings. For each file: markdown link [filename](./filename) and a one-line description.\n- **Subdirectories**: If subdirectories exist, list them with links [dirname/](./dirname/) and brief summaries.\n- **Architecture / Data Flow**: If files form a pipeline, request/response chain, or layered architecture, document it.\n- **Stack**: If this is a package root (has package.json, Cargo.toml, go.mod, etc.), document the technology stack, key scripts, and entry points.\n- **Structure**: If the directory layout follows a convention (feature-sliced, domain-driven, MVC, etc.), document it.\n- **Patterns**: If files share recurring design patterns (factory, strategy, middleware, barrel re-export), name and document them.\n- **Configuration**: If the directory contains config files, schemas, or environment definitions, document the config surface area.\n- **API Surface**: If the directory exports a public API (barrel index, route definitions, SDK), document the interface contract.\n- **File Relationships**: How files collaborate, depend on each other, or share state.\n- **Behavioral Contracts**: If files contain regex patterns, format specifications, magic constants, or template strings that define observable behavior, collect them in a dedicated section. Preserve verbatim patterns from file summaries — do NOT paraphrase regex into prose. This section is MANDATORY when file summaries contain behavioral artifacts.\n\nChoose any relevant sections or create your own based on the directory contents. The goal is to provide a comprehensive overview that captures the essence of the directory's role in the project and how its files work together, with a focus on what an AI coding assistant would need to know to effectively interact with this code.\n\nSCOPE:\n- AGENTS.md is a NAVIGATIONAL INDEX — help an AI find the right file quickly\n- Focus on: what each file does, how files relate, directory-level patterns\n- Do NOT reproduce full architecture sections — those belong in the root CLAUDE.md\n\nPATH ACCURACY (MANDATORY):\n- When referencing files or modules outside this directory, use ONLY paths from the \"Import Map\" section\n- Do NOT invent, rename, or guess module paths — if a path isn't in the Import Map, don't reference it\n- Use the exact directory names from \"Project Directory Structure\" — do NOT rename directories\n  (e.g., if the directory is called \"cli\", write \"src/cli/\", NOT \"src/commands/\")\n- Cross-module references must use the specifier format from actual import statements\n  (e.g., \"../generation/writers/sum.js\", NOT \"../fs/sum-file.js\")\n- If you are unsure about a path, omit the cross-reference rather than guessing\n\nCONSISTENCY (MANDATORY):\n- Do not contradict yourself within the same document\n- If you describe a technique (e.g., \"regex-based\"), do not call it something else later (e.g., \"AST-based\")\n- When stating version numbers, engines, or config fields, use ONLY values present in the file summaries\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this directory\", \"this module\", \"provides\", \"responsible for\", \"is used to\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Per-file descriptions in Contents sections: 1-2 sentences maximum. Reference key symbols but do not reproduce full summaries.\n- Behavioral contracts (regex patterns, format specs, constants) belong in a separate Behavioral Contracts section, not in per-file descriptions.\n- Subdirectory descriptions: 1-2 sentences maximum. Capture the directory's role, not its full contents.\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- Key exported symbols from file summaries MUST appear in the directory overview\n- Preserve exact casing of identifiers\n\nUSER NOTES:\n- If \"User Notes\" are provided in the prompt, they contain user-defined instructions that will be automatically prepended to your output\n- Do NOT repeat or paraphrase user notes in your generated content — they are included separately\n- You may reference information from user notes for context",
      "response": "<!-- Generated by agents-reverse-engineer -->\n\n# src/integration/\n\n**Platform-specific AI assistant integration layer performing environment detection, template generation, and file installation for Claude Code, OpenCode, Gemini CLI, and Aider via filesystem marker scanning and bundled hook deployment.**\n\n## Contents\n\n### Environment Detection\n\n**[detect.ts](./detect.ts)** — Exports `detectEnvironments()` and `hasEnvironment()` for AI runtime discovery via filesystem markers. `detectEnvironments()` scans `projectRoot` for Claude (`.claude/` or `CLAUDE.md`), OpenCode (`.opencode/`), Aider (`.aider.conf.yml` or `.aider/`) using synchronous `existsSync()` checks. Returns `DetectedEnvironment[]` with `type`, `configDir`, `detected` fields. `hasEnvironment()` provides predicate wrapper filtering by `EnvironmentType`.\n\n**[types.ts](./types.ts)** — Defines `EnvironmentType` literal union `'claude' | 'opencode' | 'aider' | 'gemini'`, `DetectedEnvironment` with `type`/`configDir`/`detected` fields, `IntegrationTemplate` with `filename`/`path`/`content` fields, `IntegrationResult` with `environment`/`filesCreated`/`filesSkipped` arrays.\n\n### Template Generation\n\n**[templates.ts](./templates.ts)** — Exports `getClaudeTemplates()`, `getOpenCodeTemplates()`, `getGeminiTemplates()` producing `IntegrationTemplate[]` arrays with platform-specific command files. Defines `COMMANDS` object with six templates (`generate`, `update`, `init`, `discover`, `clean`, `specify`, `help`) containing placeholder substitution for `COMMAND_PREFIX` (`/are-`), `VERSION_FILE_PATH` (`.claude/ARE-VERSION` | `.opencode/ARE-VERSION` | `.gemini/ARE-VERSION`), `$ARGUMENTS`. `PLATFORM_CONFIGS` maps `Platform` to `PlatformConfig` specifying `pathPrefix` (`.claude/skills/` | `.opencode/commands/` | `.gemini/commands/`), `filenameSeparator`, `usesName` (Claude-only YAML frontmatter), `extraFrontmatter` (OpenCode `agent: build`). `buildTemplate()` constructs paths, `buildFrontmatter()` injects YAML, `buildGeminiToml()` generates TOML format. Command content embeds background execution pattern with `run_in_background: true`, `TaskOutput` polling, `.agents-reverse-engineer/progress.log` monitoring via `Read` tool with `offset` parameter. `discover`/`clean` templates include strict rule blocks forbidding flag injection beyond `$ARGUMENTS`.\n\n### File Installation\n\n**[generate.ts](./generate.ts)** — Exports `generateIntegrationFiles(projectRoot, options?)` orchestrating setup via `detectEnvironments()` → `getTemplatesForEnvironment()` → `writeFileSync()` chain. Maps `EnvironmentType` to config directories (`claude` → `.claude`, `opencode` → `.opencode`, `aider` → `.aider`, `gemini` → `.gemini`). Creates parent directories via `ensureDir()` using `mkdirSync({ recursive: true })`. Skips existing files unless `options.force` overrides, populates `IntegrationResult.filesSkipped[]`. For `claude` environment specifically, copies bundled hook `are-session-end.js` from `hooks/dist/` to `.claude/hooks/are-session-end.js` via `readBundledHook()` → `writeFileSync()`. `getBundledHookPath()` resolves hook paths from `dist/integration/` up two levels to project root then into `hooks/dist/{hookName}`. Throws `Error` if bundled hook missing. Returns `IntegrationResult[]` with `filesCreated`/`filesSkipped` per environment. Respects `options.dryRun` for preview mode. Defines `GenerateOptions` interface with `dryRun?`, `force?`, `environment?` fields.\n\n## Integration Points\n\nConsumed by `src/installer/operations.ts` during npx-based installation workflow. `detectEnvironments()` validates target runtime presence before hook registration. Template generators provide command file content for `src/installer/operations.ts` writes. Generated files reference `VERSION_FILE_PATH` for session hooks' version checking logic.\n\n## File Path Patterns\n\n**Claude Code:**\n- Commands: `.claude/skills/are-{command}/SKILL.md`\n- Hooks: `.claude/hooks/are-session-end.js`\n- Version cache: `.claude/ARE-VERSION`\n\n**OpenCode:**\n- Commands: `.opencode/commands/are-{command}.md`\n- Version cache: `.opencode/ARE-VERSION`\n\n**Gemini CLI:**\n- Commands: `.gemini/commands/are-{command}.toml`\n- Version cache: `.gemini/ARE-VERSION`\n\n**Aider:**\n- Markers: `.aider.conf.yml`, `.aider/`\n- No template generation (detection only)\n\n## Hook Bundling Convention\n\nHooks copied from `hooks/dist/{hookName}` (post-build artifacts from `scripts/build-hooks.js`). Claude `are-session-end.js` hook spawns detached `npx agents-reverse-engineer@latest update --quiet` process on `git status --porcelain` detecting changes. Disable via `ARE_DISABLE_HOOK=1` environment variable or `hook_enabled: false` config flag.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 1337,
      "cacheReadTokens": 17477,
      "cacheCreationTokens": 12225,
      "latencyMs": 53929,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": []
    },
    {
      "timestamp": "2026-02-09T16:13:38.898Z",
      "prompt": "Generate AGENTS.md for directory: \"src/specify\" (specify)\n\n## File Summaries (3 files)\n\n### index.ts\n**Purpose:** Barrel module exporting project specification generation capabilities: buildSpecPrompt for prompt construction, write...\n\n**Barrel module exporting project specification generation capabilities: buildSpecPrompt for prompt construction, writeSpec for file system output with collision detection.**\n\n## Exported Functions\n\n- `buildSpecPrompt` — constructs AI prompt from AGENTS.md corpus (from `./prompts.js`)\n- `writeSpec` — writes synthesized specification to disk with overwrite protection (from `./writer.js`)\n\n## Exported Types\n\n- `SpecPrompt` — prompt structure returned by buildSpecPrompt (from `./prompts.js`)\n- `WriteSpecOptions` — configuration for writeSpec operation (from `./writer.js`)\n\n## Exported Errors\n\n- `SpecExistsError` — thrown by writeSpec when target file exists and `force: false` (from `./writer.js`)\n\n## Module Role\n\nEntry point for `/are-specify` command implementation (invoked from `src/cli/specify.ts`). Aggregates AGENTS.md files via buildSpecPrompt, generates unified project specification via AI synthesis, writes output to `specs/` directory via writeSpec. Supports single-file (`specs/SPEC.md`) and multi-file (`specs/<dirname>.md`) output modes.\n### prompts.ts\n**Purpose:** prompts.ts constructs system and user prompts for AI-driven project specification synthesis from AGENTS.md corpus, en...\n\n**prompts.ts constructs system and user prompts for AI-driven project specification synthesis from AGENTS.md corpus, enforcing concern-based organization and behavioral contract preservation.**\n\n## Exported Types\n\n`SpecPrompt` interface contains `system: string` and `user: string` properties representing the prompt pair for AI specification generation.\n\n## Exported Constants\n\n`SPEC_SYSTEM_PROMPT` defines the system instruction string for specification synthesis, containing 2,850+ characters of constraints and formatting rules.\n\n## Exported Functions\n\n`buildSpecPrompt(docs: AgentsDocs): SpecPrompt` assembles the prompt pair by injecting all AGENTS.md documents from `docs` array into user prompt with section delimiters formatted as `### ${doc.relativePath}`, appending output requirements list, and returning object with `system: SPEC_SYSTEM_PROMPT` and `user: userSections.join('\\n')`.\n\n## Prompt Architecture\n\n`SPEC_SYSTEM_PROMPT` enforces nine mandatory sections in sequence: (1) Project Overview with technology stack versions, (2) Architecture with module boundaries and design rationale, (3) Public API Surface with full type signatures, (4) Data Structures & State including serialization formats, (5) Configuration with types/defaults/validation, (6) Dependencies with exact versions and rationale, (7) Behavioral Contracts split into Runtime Behavior (error types/codes, retry formulas, concurrency model, lifecycle hooks) and Implementation Contracts (verbatim regex patterns, format strings, magic constants, environment variables, file format specs), (8) Test Contracts with per-module scenarios and edge cases, (9) Build Plan with phased implementation sequence and dependency ordering.\n\n## Organization Constraints\n\nSystem prompt prohibits folder-mirroring via rules: \"Describe MODULE BOUNDARIES and their interfaces — not file paths or directory layouts\", \"Do NOT prescribe exact filenames or file paths\", \"Do NOT mirror the project's folder structure\", \"Do NOT use directory names as section headings\". Mandates conceptual grouping by concern rather than directory structure.\n\n## Behavioral Contract Requirements\n\nSystem prompt mandates \"MUST include verbatim regex patterns, format strings, and magic constants from the source documents — do NOT paraphrase regex patterns into prose descriptions\". Specifies consolidation of duplicated constants across modules into single definition with cross-references. Implementation Contracts subsection must capture reproduction-critical details: regex patterns in backticks, format string exact structures with examples, magic constant meanings, environment variable expected values, YAML schemas, NDJSON structures.\n\n## User Prompt Construction\n\n`buildSpecPrompt` creates `userSections` array starting with task description \"Generate a comprehensive project specification from the following documentation\", followed by `## AGENTS.md Files (${docs.length} directories)` header, then maps `docs` array to section strings via `doc => '### ${doc.relativePath}\\n\\n${doc.content}'`, appends `## Output Requirements` section repeating the nine mandatory sections list with \"(error handling, concurrency, lifecycle, PLUS verbatim regex patterns, format specs, magic constants, templates)\" for Behavioral Contracts, and terminates with \"Output ONLY the markdown content. No preamble.\" instruction.\n\n## Output Format Enforcement\n\nBoth prompts enforce no-preamble output via terminal instructions: system prompt ends with \"OUTPUT: Raw markdown. No preamble. No meta-commentary. No 'Here is...' or 'I've generated...' prefix.\", user prompt ends with \"Output ONLY the markdown content. No preamble.\" This prevents conversational wrapper text in AI responses.\n\n## Integration Points\n\nConsumes `AgentsDocs` type from `../generation/collector.js` representing array of objects with `relativePath: string` and `content: string` properties. Called by `src/specify/index.ts` orchestrator which invokes `collectAgentsDocs()` to gather all AGENTS.md files, passes result to `buildSpecPrompt()`, then feeds resulting prompts to `AIService.call()`.\n\n## Audience Targeting\n\nSystem prompt specifies \"AUDIENCE: AI agents (LLMs) — use structured, precise, instruction-oriented language. Every statement should be actionable.\" This contrasts with human-targeted documentation by requiring all content to be formulated as executable instructions for code generation rather than explanatory prose.\n### writer.ts\n**Purpose:** writeSpec() writes AI-generated project specifications to disk with multi-file splitting, overwrite protection via Sp...\n\n**writeSpec() writes AI-generated project specifications to disk with multi-file splitting, overwrite protection via SpecExistsError, and heading-based content segmentation.**\n\n## Exported Interface\n\n**WriteSpecOptions** configures output behavior:\n- `outputPath: string` — Full path to output file (e.g., `/project/specs/SPEC.md`)\n- `force: boolean` — Overwrite existing files without throwing SpecExistsError\n- `multiFile: boolean` — Split content on top-level `# ` headings into separate files\n\n**SpecExistsError** signals file collision when `force=false`:\n- Extends Error with `name: 'SpecExistsError'`\n- `paths: string[]` — Array of conflicting file paths\n- Constructor formats error message as bulleted list with instructions to use `--force`\n\n**writeSpec(content: string, options: WriteSpecOptions): Promise<string[]>** writes specification files:\n- Single-file mode: writes `content` directly to `outputPath` after verifying directory via `mkdir(path.dirname(outputPath), { recursive: true })`\n- Multi-file mode: calls `splitByHeadings(content)` to extract sections, writes each to `path.join(outputDir, section.filename)`\n- Returns array of absolute paths to all written files\n- Throws SpecExistsError if any target path exists and `force=false`\n- Preemptively checks all target paths before writing any files in multi-file mode to ensure atomic failure\n\n## Content Splitting Algorithm\n\n**splitByHeadings(content: string)** partitions markdown on top-level headings:\n- Splits on regex `/^(?=# )/m` (positive lookahead matching lines starting with `# `)\n- Extracts heading text via `/^# (.+)/` pattern match\n- Maps heading text to filename via `slugify()` transformation\n- Assigns preamble content (text before first `# ` heading) to `00-preamble.md`\n- Returns `Array<{ filename: string; content: string }>` with each section's filename and trimmed content plus newline\n\n**slugify(heading: string)** sanitizes heading text into filesystem-safe slug:\n- Lowercases via `.toLowerCase()`\n- Replaces whitespace with hyphens via `/\\s+/g` → `'-'`\n- Strips non-alphanumeric characters (except hyphens) via `/[^a-z0-9-]/g` → `''`\n- Collapses consecutive hyphens via `/-+/g` → `'-'`\n- Trims leading/trailing hyphens via `/^-|-$/g` → `''`\n- Example: `\"Project Overview & Goals\"` → `\"project-overview-goals\"`\n\n## File System Operations\n\n**fileExists(filePath: string): Promise<boolean>** checks file presence:\n- Calls `access(filePath, constants.F_OK)` from `node:fs/promises`\n- Returns `true` if file exists, `false` if access throws (no file)\n\n**Directory creation** via `mkdir(path.dirname(outputPath), { recursive: true })`:\n- Creates parent directories atomically before writing\n- Used in both single-file and multi-file modes\n\n**Write operations** via `writeFile(filePath, content, 'utf-8')`:\n- Single-file: writes once to `outputPath`\n- Multi-file: loops through sections array, writing each to `path.join(outputDir, section.filename)`\n\n## Integration Points\n\n**Caller expectations** (from `src/cli/specify.ts`):\n- Catch SpecExistsError and display `error.message` (contains formatted path list and `--force` hint)\n- Pass `WriteSpecOptions` with `outputPath` resolved from config, `force` from CLI flag, `multiFile` from `--multi-file` flag\n- Log returned paths array to confirm successful writes\n\n## Import Map (verified — use these exact paths)\n\nprompts.ts:\n  ../generation/collector.js → AgentsDocs (type)\n\n## Project Directory Structure\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>",
      "systemPrompt": "You are generating an AGENTS.md file — a directory-level overview for AI coding assistants.\n\nCRITICAL: Output ONLY the raw markdown content. No code fences, no preamble, no explanations, no conversational text. Your entire response IS the AGENTS.md file content.\n\nOUTPUT FORMAT:\n- First line MUST be exactly: <!-- Generated by agents-reverse-engineer -->\n- Use a # heading with the directory name\n- Write a one-paragraph purpose statement for the directory\nusage\nADAPTIVE SECTIONS:\nAnalyze the directory contents and choose the most relevant sections. Do NOT use a fixed template. Instead, select sections that best document this specific directory for an AI that needs to reconstruct or extend the project.\n\nConsider these section types (choose what applies):\n- **Contents**: Group files by purpose/category under ## headings. For each file: markdown link [filename](./filename) and a one-line description.\n- **Subdirectories**: If subdirectories exist, list them with links [dirname/](./dirname/) and brief summaries.\n- **Architecture / Data Flow**: If files form a pipeline, request/response chain, or layered architecture, document it.\n- **Stack**: If this is a package root (has package.json, Cargo.toml, go.mod, etc.), document the technology stack, key scripts, and entry points.\n- **Structure**: If the directory layout follows a convention (feature-sliced, domain-driven, MVC, etc.), document it.\n- **Patterns**: If files share recurring design patterns (factory, strategy, middleware, barrel re-export), name and document them.\n- **Configuration**: If the directory contains config files, schemas, or environment definitions, document the config surface area.\n- **API Surface**: If the directory exports a public API (barrel index, route definitions, SDK), document the interface contract.\n- **File Relationships**: How files collaborate, depend on each other, or share state.\n- **Behavioral Contracts**: If files contain regex patterns, format specifications, magic constants, or template strings that define observable behavior, collect them in a dedicated section. Preserve verbatim patterns from file summaries — do NOT paraphrase regex into prose. This section is MANDATORY when file summaries contain behavioral artifacts.\n\nChoose any relevant sections or create your own based on the directory contents. The goal is to provide a comprehensive overview that captures the essence of the directory's role in the project and how its files work together, with a focus on what an AI coding assistant would need to know to effectively interact with this code.\n\nSCOPE:\n- AGENTS.md is a NAVIGATIONAL INDEX — help an AI find the right file quickly\n- Focus on: what each file does, how files relate, directory-level patterns\n- Do NOT reproduce full architecture sections — those belong in the root CLAUDE.md\n\nPATH ACCURACY (MANDATORY):\n- When referencing files or modules outside this directory, use ONLY paths from the \"Import Map\" section\n- Do NOT invent, rename, or guess module paths — if a path isn't in the Import Map, don't reference it\n- Use the exact directory names from \"Project Directory Structure\" — do NOT rename directories\n  (e.g., if the directory is called \"cli\", write \"src/cli/\", NOT \"src/commands/\")\n- Cross-module references must use the specifier format from actual import statements\n  (e.g., \"../generation/writers/sum.js\", NOT \"../fs/sum-file.js\")\n- If you are unsure about a path, omit the cross-reference rather than guessing\n\nCONSISTENCY (MANDATORY):\n- Do not contradict yourself within the same document\n- If you describe a technique (e.g., \"regex-based\"), do not call it something else later (e.g., \"AST-based\")\n- When stating version numbers, engines, or config fields, use ONLY values present in the file summaries\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this directory\", \"this module\", \"provides\", \"responsible for\", \"is used to\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Per-file descriptions in Contents sections: 1-2 sentences maximum. Reference key symbols but do not reproduce full summaries.\n- Behavioral contracts (regex patterns, format specs, constants) belong in a separate Behavioral Contracts section, not in per-file descriptions.\n- Subdirectory descriptions: 1-2 sentences maximum. Capture the directory's role, not its full contents.\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- Key exported symbols from file summaries MUST appear in the directory overview\n- Preserve exact casing of identifiers\n\nUSER NOTES:\n- If \"User Notes\" are provided in the prompt, they contain user-defined instructions that will be automatically prepended to your output\n- Do NOT repeat or paraphrase user notes in your generated content — they are included separately\n- You may reference information from user notes for context",
      "response": "<!-- Generated by agents-reverse-engineer -->\n\n# specify/\n\nSpecification synthesis from AGENTS.md corpus via AI-driven prompt construction (`buildSpecPrompt`) and filesystem output (`writeSpec`) with multi-file splitting and overwrite protection.\n\n## Contents\n\n### Files\n\n**[index.ts](./index.ts)** — Barrel module re-exporting `buildSpecPrompt`, `writeSpec`, `SpecPrompt`, `WriteSpecOptions`, `SpecExistsError` for `/are-specify` command integration.\n\n**[prompts.ts](./prompts.ts)** — `buildSpecPrompt(docs: AgentsDocs): SpecPrompt` constructs system/user prompts enforcing nine-section structure (Overview, Architecture, API Surface, Data Structures, Configuration, Dependencies, Behavioral Contracts, Test Contracts, Build Plan) with verbatim regex/format/constant preservation. `SPEC_SYSTEM_PROMPT` prohibits folder-mirroring, mandates concern-based organization, targets AI agents with actionable instructions.\n\n**[writer.ts](./writer.ts)** — `writeSpec(content: string, options: WriteSpecOptions): Promise<string[]>` writes single-file or multi-file specifications via heading-based splitting. `splitByHeadings(content)` partitions on `/^# /m` regex, `slugify()` sanitizes heading text to filenames (e.g., `\"Project Overview\"` → `\"project-overview.md\"`). Throws `SpecExistsError` with `paths: string[]` when targets exist and `force: false`.\n\n## Data Flow\n\n1. **Orchestration** (`src/cli/specify.ts`) invokes `collectAgentsDocs()` to gather all `AGENTS.md` files recursively\n2. **Prompt Construction** (`prompts.ts`) injects corpus into `SPEC_SYSTEM_PROMPT` template with section delimiters `### ${relativePath}`, appends nine-section requirements\n3. **AI Synthesis** (`AIService.call()` from `src/ai/service.ts`) processes prompt pair, returns specification markdown\n4. **Output** (`writer.ts`) writes single `specs/SPEC.md` or splits into `specs/<slugified-heading>.md` per top-level heading\n5. **Error Handling** catches `SpecExistsError` and displays `--force` hint\n\n## Behavioral Contracts\n\n### Prompt Constraints (SPEC_SYSTEM_PROMPT)\n\n**Organization rules:**\n- Regex `/^# (Project Overview|Architecture|Public API Surface|Data Structures & State|Configuration|Dependencies|Behavioral Contracts|Test Contracts|Build Plan)$/m` defines mandatory section headings\n- Prohibition pattern: \"Do NOT prescribe exact filenames or file paths\" / \"Do NOT mirror the project's folder structure\" / \"Do NOT use directory names as section headings\"\n- Behavioral Contracts split: \"Runtime Behavior (error handling, concurrency, lifecycle)\" + \"Implementation Contracts (verbatim regex patterns, format specs, magic constants)\"\n\n**Output format enforcement:**\n- Terminal instruction: `\"OUTPUT: Raw markdown. No preamble. No meta-commentary. No 'Here is...' or 'I've generated...' prefix.\"`\n- User prompt: `\"Output ONLY the markdown content. No preamble.\"`\n\n### File System Patterns (writer.ts)\n\n**Heading splitting regex:** `/^(?=# )/m` (positive lookahead for top-level headings)  \n**Heading extraction:** `/^# (.+)/` (capture heading text)  \n**Slugification chain:**\n1. `.toLowerCase()`\n2. `/\\s+/g` → `'-'` (whitespace to hyphens)\n3. `/[^a-z0-9-]/g` → `''` (strip non-alphanumeric except hyphens)\n4. `/-+/g` → `'-'` (collapse consecutive hyphens)\n5. `/^-|-$/g` → `''` (trim leading/trailing hyphens)\n\n**Preamble filename:** `00-preamble.md` (content before first `# ` heading)\n\n## Integration Points\n\n**Consumed by:** `src/cli/specify.ts` (command orchestrator)  \n**Consumes:**\n- `collectAgentsDocs()` from `../generation/collector.js` → `AgentsDocs` (array of `{relativePath, content}`)\n- `AIService.call()` from `../ai/service.js` (prompt pair execution)\n- `mkdir()`, `writeFile()`, `access()` from `node:fs/promises` (file operations)\n- `path.dirname()`, `path.join()` from `node:path` (path resolution)\n\n**Error propagation:** `SpecExistsError` thrown when `force: false` and target paths exist, caught by CLI layer for `--force` hint display",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 1152,
      "cacheReadTokens": 17477,
      "cacheCreationTokens": 11289,
      "latencyMs": 54196,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": []
    },
    {
      "timestamp": "2026-02-09T16:14:09.640Z",
      "prompt": "Generate AGENTS.md for directory: \"src/update\" (update)\n\n## File Summaries (4 files)\n\n### index.ts\n**Purpose:** Module barrel for incremental update functionality exporting UpdateOrchestrator, orchestrator factory, cleanup utilit...\n\n**Module barrel for incremental update functionality exporting UpdateOrchestrator, orchestrator factory, cleanup utilities, and type definitions.**\n\n## Exported Symbols\n\n### From `./orchestrator.js`\n- **UpdateOrchestrator** (class) — Coordinates incremental update workflow with state management and change detection\n- **createUpdateOrchestrator** (factory function) — Creates UpdateOrchestrator instance\n- **UpdatePlan** (type) — Describes files to analyze/skip and directories to regenerate\n\n### From `./orphan-cleaner.js`\n- **cleanupOrphans** (function) — Deletes `.sum` files for deleted source files or renamed oldPaths\n- **cleanupEmptyDirectoryDocs** (function) — Removes `AGENTS.md` from directories with no remaining sources\n- **getAffectedDirectories** (function) — Computes parent directory paths requiring AGENTS.md regeneration by walking up from changed files\n\n### From `./types.js`\n- **UpdateOptions** (type) — Configuration for update command including uncommitted changes flag\n- **UpdateResult** (type) — Aggregated outcome with counts for analyzed/skipped/orphaned files\n- **UpdateProgress** (type) — Streaming progress events during update execution\n- **CleanupResult** (type) — Counts of orphaned `.sum` files and empty `AGENTS.md` files removed\n\n## Integration Points\n\nRe-exports types and functions consumed by:\n- `src/cli/update.ts` — CLI entry point invoking `createUpdateOrchestrator()` with parsed options\n- `src/update/orchestrator.ts` — Core incremental update logic calling `cleanupOrphans()` and `getAffectedDirectories()`\n- `hooks/are-session-end.js` — Session hook spawning `npx agents-reverse-engineer@latest update --quiet` on uncommitted changes\n\n## Module Role\n\nServes as public API boundary for the `src/update/` directory, exposing only the orchestrator, factory, cleanup utilities, and type contracts while hiding internal implementation details of SHA-256 hash comparison, git diff parsing, and orphan detection logic.\n### orchestrator.ts\n**Purpose:** UpdateOrchestrator coordinates frontmatter-based incremental documentation updates by comparing SHA-256 content hashe...\n\n**UpdateOrchestrator coordinates frontmatter-based incremental documentation updates by comparing SHA-256 content hashes in `.sum` file YAML frontmatter against current file content, cleaning orphaned artifacts, and tracking affected directories for `AGENTS.md` regeneration.**\n\n## Exported Symbols\n\n**UpdateOrchestrator** — Class managing incremental update workflow with methods:\n- `constructor(config: Config, projectRoot: string, options?: { tracer?: ITraceWriter; debug?: boolean })`\n- `close(): void` — No-op for API compatibility (no database resources to clean)\n- `checkPrerequisites(): Promise<void>` — Throws if `projectRoot` is not a git repository via `isGitRepo()`\n- `preparePlan(options?: UpdateOptions): Promise<UpdatePlan>` — Computes which files need re-analysis by hash comparison\n- `recordFileAnalyzed(_relativePath: string, _contentHash: string, _currentCommit: string): Promise<void>` — No-op (hash stored in frontmatter)\n- `removeFileState(_relativePath: string): Promise<void>` — No-op for API compatibility\n- `recordRun(_commitHash: string, _filesAnalyzed: number, _filesSkipped: number): Promise<number>` — No-op returning 0\n- `getLastRun(): Promise<undefined>` — No-op returning undefined (no run history in frontmatter mode)\n- `isFirstRun(): Promise<boolean>` — Returns true if no `.sum` files exist\n\n**createUpdateOrchestrator**(config: Config, projectRoot: string, options?: { tracer?: ITraceWriter; debug?: boolean }): UpdateOrchestrator — Factory function instantiating UpdateOrchestrator.\n\n**UpdatePlan** — Interface defining update strategy:\n```typescript\n{\n  filesToAnalyze: FileChange[];    // Added/modified files needing re-analysis\n  filesToSkip: string[];            // Unchanged files (hash match)\n  cleanup: CleanupResult;           // Orphaned .sum files to delete\n  affectedDirs: string[];           // Directories needing AGENTS.md regeneration\n  baseCommit: string;               // Current commit (not used, kept for compatibility)\n  currentCommit: string;            // Current commit SHA\n  isFirstRun: boolean;              // True if no .sum files exist\n}\n```\n\n## Hash-Based Change Detection Algorithm\n\n**preparePlan()** executes frontmatter-based change detection:\n1. Calls `checkPrerequisites()` to verify git repository exists\n2. Calls `getCurrentCommit()` to retrieve HEAD commit SHA\n3. Invokes `discoverFiles()` which wraps `runDiscovery()` from `discovery/run.js`, converting absolute paths to relative via `path.relative()`\n4. For each discovered file, calls `getSumPath()` to locate corresponding `.sum` file\n5. Reads `.sum` frontmatter via `readSumFile()` to extract `contentHash` field\n6. Computes current file hash via `computeContentHash()` (SHA-256)\n7. Hash mismatch or missing `.sum` → adds `FileChange` with `status: 'added'` or `'modified'` to `filesToAnalyze`\n8. Hash match → adds path to `filesToSkip`\n9. Calls `cleanupOrphans()` to delete `.sum` files for non-existent source files\n10. Calls `getAffectedDirectories()` to compute directory set from `filesToAnalyze`\n11. Sorts `affectedDirs` by depth descending via `split(path.sep).length` comparison (deepest first for bottom-up regeneration)\n\n## Trace Event Emission\n\nEmits trace events via `ITraceWriter` interface:\n- `phase:start` with `phase: 'update-plan-creation'` at start of `preparePlan()`\n- `plan:created` with `planType: 'update'`, `fileCount: filesToAnalyze.length`, `taskCount: filesToAnalyze.length + affectedDirs.length`\n- `phase:end` with `phase: 'update-plan-creation'`, `durationMs` computed via `process.hrtime.bigint()` delta, `tasksCompleted: 1`, `tasksFailed: 0`\n\n## Debug Logging\n\nWhen `debug: true` passed to constructor, emits `picocolors.dim()` formatted messages to stderr:\n- `'[debug] Creating update plan with change detection...'` at start\n- `'[debug] Git commit: <hash>'` showing first 7 characters of commit SHA\n- `'[debug] Discovering files...'` before discovery phase\n- `'[debug] Change detection: <changed> changed, <unchanged> unchanged, <orphaned> orphaned'` after comparison\n- `'[debug] Affected directories: <count>'` showing directory regeneration scope\n\n## Integration Points\n\n- **change-detection/index.js** — Imports `isGitRepo()`, `getCurrentCommit()`, `computeContentHash()`, `FileChange` type\n- **generation/writers/sum.js** — Imports `readSumFile()`, `getSumPath()` for frontmatter parsing and path resolution\n- **update/orphan-cleaner.js** — Imports `cleanupOrphans()`, `getAffectedDirectories()` for artifact cleanup and directory tracking\n- **discovery/run.js** — Imports `discoverFiles()` wrapping `runDiscovery()` for file scanning\n- **config/schema.js** — Accepts `Config` type defining exclude patterns and concurrency settings\n- **orchestration/trace.js** — Optional `ITraceWriter` dependency for NDJSON event emission\n\n## API Compatibility Pattern\n\nSeveral methods are no-ops with comment `// Kept for API compatibility`:\n- `close()` — No database resources in frontmatter mode (previous SQLite implementation removed)\n- `recordFileAnalyzed()` — Hash now stored in `.sum` YAML frontmatter instead of database\n- `removeFileState()` — `.sum` file cleanup handled by `cleanupOrphans()`\n- `recordRun()` — No run history tracking in frontmatter mode\n- `getLastRun()` — Returns undefined (no historical run data)\n\nThese preserve interface contract for consumers expecting SQLite-based state management from earlier versions.\n\n## First Run Detection\n\n`isFirstRun()` calls `preparePlan({ dryRun: true })` and checks if `filesToSkip.length === 0 && filesToAnalyze.length > 0`, indicating no existing `.sum` files with valid hashes.\n### orphan-cleaner.ts\n**Purpose:** orphan-cleaner.ts deletes stale `.sum` files and `AGENTS.md` files for deleted/renamed source files and empty directo...\n\n**orphan-cleaner.ts deletes stale `.sum` files and `AGENTS.md` files for deleted/renamed source files and empty directories.**\n\n## Exported Functions\n\n**`cleanupOrphans(projectRoot: string, changes: FileChange[], dryRun?: boolean): Promise<CleanupResult>`** orchestrates cleanup by extracting paths from `FileChange[]` where `status === 'deleted'` or `status === 'renamed'` (using `oldPath` for renames), deleting corresponding `.sum` files via `deleteIfExists()`, collecting affected directories via `path.dirname()`, and delegating empty directory cleanup to `cleanupEmptyDirectoryDocs()`. Returns `CleanupResult` with `deletedSumFiles[]` and `deletedAgentsMd[]` arrays.\n\n**`cleanupEmptyDirectoryDocs(dirPath: string, dryRun?: boolean): Promise<boolean>`** reads directory via `readdir()`, filters entries to detect source files (excluding hidden files starting with `.`, `.sum` extensions, and `GENERATED_FILES` set members `'AGENTS.md'` and `'CLAUDE.md'`), and deletes `AGENTS.md` via `deleteIfExists()` if no source files remain. Returns `true` if `AGENTS.md` was deleted.\n\n**`getAffectedDirectories(changes: FileChange[]): Set<string>`** extracts unique parent directories from non-deleted `FileChange` entries by walking up directory tree via `path.dirname()` until reaching `.` (root). Skips files with `status === 'deleted'` since deleted files don't affect directory documentation. Always includes `'.'` as root directory in returned set.\n\n## Cleanup Logic\n\n**Orphan detection** operates on two categories: `.sum` files at old paths for deleted source files (appends `.sum` to `change.path`), and `.sum` files at `oldPath` for renames (appends `.sum` to `change.oldPath`). Directory cleanup targets directories where all remaining entries are hidden files, `.sum` files, or members of `GENERATED_FILES` constant.\n\n**`deleteIfExists(filePath: string, dryRun: boolean): Promise<boolean>`** internal helper calls `stat()` to check existence, `unlink()` to delete (unless `dryRun === true`), returns `true` if file existed (or would be deleted in dry run), returns `false` if `stat()` throws.\n\n## Integration Points\n\nConsumes `FileChange` from `src/change-detection/types.ts` with discriminated `status` field (`'added' | 'modified' | 'deleted' | 'renamed'`) and optional `oldPath` string for renames. Returns `CleanupResult` from `src/update/types.ts` with relative path arrays for deleted artifacts. Called by `orchestrateUpdate()` in `src/update/orchestrator.ts` after change detection and before Phase 2 directory aggregation.\n### types.ts\n**Purpose:** types.ts defines TypeScript interfaces for the incremental update workflow including change results, cleanup operatio...\n\n**types.ts defines TypeScript interfaces for the incremental update workflow including change results, cleanup operations, progress callbacks, and command options.**\n\n## Exported Interfaces\n\n### CleanupResult\nRepresents the outcome of orphan cleanup operations performed during incremental updates.\n\n**Properties:**\n- `deletedSumFiles: string[]` — Paths to `.sum` files removed because their source files were deleted or renamed\n- `deletedAgentsMd: string[]` — Paths to `AGENTS.md` files removed from directories that became empty after cleanup\n\n### UpdateOptions\nConfiguration flags for the `are update` command execution.\n\n**Properties:**\n- `includeUncommitted?: boolean` — When true, includes both staged and working directory changes via `git status --porcelain` merge with `git diff`; when false, only processes committed changes since last run\n- `dryRun?: boolean` — When true, computes change set and displays preview without writing `.sum` files, regenerating `AGENTS.md`, or deleting orphans\n\n### UpdateResult\nComplete summary of an incremental update execution returned by update orchestrator.\n\n**Properties:**\n- `analyzedFiles: string[]` — Source files regenerated due to `content_hash` mismatch (status `'added'` or `'modified'` from `FileChange`)\n- `skippedFiles: string[]` — Source files with matching `content_hash` in YAML frontmatter (SHA-256 comparison passed)\n- `cleanup: CleanupResult` — Orphan removal results from `cleanupOrphans()` and `cleanupEmptyDirectoryDocs()`\n- `regeneratedDirs: string[]` — Directory paths whose `AGENTS.md` files were synthesized via Phase 2 aggregation after child `.sum` changes\n- `baseCommit: string` — Git commit SHA at update start (typically from previous update or HEAD~1)\n- `currentCommit: string` — Git commit SHA at update end (typically HEAD after processing changes)\n- `dryRun: boolean` — Mirrors the `UpdateOptions.dryRun` flag indicating preview-only execution\n\n### UpdateProgress\nCallback interface for streaming progress notifications during incremental update operations, consumed by `ProgressReporter` in `src/orchestration/progress.ts`.\n\n**Properties:**\n- `onFileStart?: (path: string, status: 'analyzing' | 'skipping') => void` — Invoked before processing each source file; `'analyzing'` for hash mismatch, `'skipping'` for hash match\n- `onFileDone?: (path: string, status: 'analyzed' | 'skipped' | 'error') => void` — Invoked after processing completes; `'error'` status indicates subprocess failure or timeout\n- `onCleanup?: (path: string, type: 'sum' | 'agents-md') => void` — Invoked when `cleanupOrphans()` deletes a `.sum` file or `cleanupEmptyDirectoryDocs()` removes an `AGENTS.md`\n- `onDirRegenerate?: (path: string) => void` — Invoked when directory aggregation rewrites an `AGENTS.md` file due to child `.sum` changes in `affectedDirs` set\n\n## Integration Points\n\nThis module re-exports `FileChange` from `../change-detection/types.js`, which represents git-detected or SHA-256-detected modifications with `status: 'added' | 'modified' | 'deleted' | 'renamed'` discriminated union. Update orchestrator (`src/update/orchestrator.ts`) consumes these types to coordinate hash-based change detection, Phase 1 pool execution for modified files, orphan cleanup via `orphan-cleaner.ts`, and targeted directory regeneration.\n\n## Import Map (verified — use these exact paths)\n\norchestrator.ts:\n  ../config/schema.js → Config (type)\n  ../change-detection/index.js → isGitRepo, getCurrentCommit, computeContentHash, type FileChange\n  ../generation/writers/sum.js → readSumFile, getSumPath\n  ../discovery/run.js → discoverFiles\n  ../orchestration/trace.js → ITraceWriter (type)\n\norphan-cleaner.ts:\n  ../change-detection/types.js → FileChange (type)\n\ntypes.ts:\n  ../change-detection/types.js → FileChange (type)\n\n## Project Directory Structure\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>",
      "systemPrompt": "You are generating an AGENTS.md file — a directory-level overview for AI coding assistants.\n\nCRITICAL: Output ONLY the raw markdown content. No code fences, no preamble, no explanations, no conversational text. Your entire response IS the AGENTS.md file content.\n\nOUTPUT FORMAT:\n- First line MUST be exactly: <!-- Generated by agents-reverse-engineer -->\n- Use a # heading with the directory name\n- Write a one-paragraph purpose statement for the directory\nusage\nADAPTIVE SECTIONS:\nAnalyze the directory contents and choose the most relevant sections. Do NOT use a fixed template. Instead, select sections that best document this specific directory for an AI that needs to reconstruct or extend the project.\n\nConsider these section types (choose what applies):\n- **Contents**: Group files by purpose/category under ## headings. For each file: markdown link [filename](./filename) and a one-line description.\n- **Subdirectories**: If subdirectories exist, list them with links [dirname/](./dirname/) and brief summaries.\n- **Architecture / Data Flow**: If files form a pipeline, request/response chain, or layered architecture, document it.\n- **Stack**: If this is a package root (has package.json, Cargo.toml, go.mod, etc.), document the technology stack, key scripts, and entry points.\n- **Structure**: If the directory layout follows a convention (feature-sliced, domain-driven, MVC, etc.), document it.\n- **Patterns**: If files share recurring design patterns (factory, strategy, middleware, barrel re-export), name and document them.\n- **Configuration**: If the directory contains config files, schemas, or environment definitions, document the config surface area.\n- **API Surface**: If the directory exports a public API (barrel index, route definitions, SDK), document the interface contract.\n- **File Relationships**: How files collaborate, depend on each other, or share state.\n- **Behavioral Contracts**: If files contain regex patterns, format specifications, magic constants, or template strings that define observable behavior, collect them in a dedicated section. Preserve verbatim patterns from file summaries — do NOT paraphrase regex into prose. This section is MANDATORY when file summaries contain behavioral artifacts.\n\nChoose any relevant sections or create your own based on the directory contents. The goal is to provide a comprehensive overview that captures the essence of the directory's role in the project and how its files work together, with a focus on what an AI coding assistant would need to know to effectively interact with this code.\n\nSCOPE:\n- AGENTS.md is a NAVIGATIONAL INDEX — help an AI find the right file quickly\n- Focus on: what each file does, how files relate, directory-level patterns\n- Do NOT reproduce full architecture sections — those belong in the root CLAUDE.md\n\nPATH ACCURACY (MANDATORY):\n- When referencing files or modules outside this directory, use ONLY paths from the \"Import Map\" section\n- Do NOT invent, rename, or guess module paths — if a path isn't in the Import Map, don't reference it\n- Use the exact directory names from \"Project Directory Structure\" — do NOT rename directories\n  (e.g., if the directory is called \"cli\", write \"src/cli/\", NOT \"src/commands/\")\n- Cross-module references must use the specifier format from actual import statements\n  (e.g., \"../generation/writers/sum.js\", NOT \"../fs/sum-file.js\")\n- If you are unsure about a path, omit the cross-reference rather than guessing\n\nCONSISTENCY (MANDATORY):\n- Do not contradict yourself within the same document\n- If you describe a technique (e.g., \"regex-based\"), do not call it something else later (e.g., \"AST-based\")\n- When stating version numbers, engines, or config fields, use ONLY values present in the file summaries\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this directory\", \"this module\", \"provides\", \"responsible for\", \"is used to\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Per-file descriptions in Contents sections: 1-2 sentences maximum. Reference key symbols but do not reproduce full summaries.\n- Behavioral contracts (regex patterns, format specs, constants) belong in a separate Behavioral Contracts section, not in per-file descriptions.\n- Subdirectory descriptions: 1-2 sentences maximum. Capture the directory's role, not its full contents.\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- Key exported symbols from file summaries MUST appear in the directory overview\n- Preserve exact casing of identifiers\n\nUSER NOTES:\n- If \"User Notes\" are provided in the prompt, they contain user-defined instructions that will be automatically prepended to your output\n- Do NOT repeat or paraphrase user notes in your generated content — they are included separately\n- You may reference information from user notes for context",
      "response": "<!-- Generated by agents-reverse-engineer -->\n\n# src/update/\n\nIncremental documentation synchronization via frontmatter SHA-256 hash comparison, orphaned artifact cleanup, and targeted directory regeneration.\n\n## Contents\n\n### Core Orchestration\n\n**[index.ts](./index.ts)** — Barrel export aggregating `UpdateOrchestrator` class, `createUpdateOrchestrator()` factory, cleanup functions (`cleanupOrphans`, `cleanupEmptyDirectoryDocs`, `getAffectedDirectories`), and type interfaces (`UpdateOptions`, `UpdateResult`, `UpdateProgress`, `CleanupResult`).\n\n**[orchestrator.ts](./orchestrator.ts)** — `UpdateOrchestrator` class executing hash-based change detection via `preparePlan()`: discovers files through `runDiscovery()`, reads `.sum` YAML frontmatter via `readSumFile()` to extract `contentHash`, computes current SHA-256 via `computeContentHash()`, produces `UpdatePlan` with `filesToAnalyze[]` (hash mismatch/missing), `filesToSkip[]` (hash match), `cleanup: CleanupResult` (orphaned `.sum` files), `affectedDirs[]` (depth-sorted parent directories). Emits trace events (`phase:start`, `plan:created`, `phase:end`) via `ITraceWriter`. No-op methods (`close()`, `recordFileAnalyzed()`, `removeFileState()`, `recordRun()`, `getLastRun()`) preserved for API compatibility after SQLite state manager removal.\n\n**[orphan-cleaner.ts](./orphan-cleaner.ts)** — `cleanupOrphans()` deletes `.sum` files for `FileChange` entries with `status === 'deleted'` (uses `change.path`) or `status === 'renamed'` (uses `change.oldPath`), returns `CleanupResult` with `deletedSumFiles[]` and `deletedAgentsMd[]`. `cleanupEmptyDirectoryDocs()` removes `AGENTS.md` from directories containing only hidden files (`.` prefix), `.sum` files, or `GENERATED_FILES` set members. `getAffectedDirectories()` walks parent directories via `path.dirname()` for non-deleted `FileChange` entries, excludes `status === 'deleted'`, always includes `'.'` root.\n\n**[types.ts](./types.ts)** — `UpdateOptions` interface with `includeUncommitted?: boolean` (merges `git status --porcelain` into change set) and `dryRun?: boolean` (preview-only). `UpdateResult` interface with `analyzedFiles[]`, `skippedFiles[]`, `cleanup`, `regeneratedDirs[]`, `baseCommit`, `currentCommit`, `dryRun` fields. `UpdateProgress` callback interface with `onFileStart()`, `onFileDone()`, `onCleanup()`, `onDirRegenerate()` hooks. `CleanupResult` interface with `deletedSumFiles[]`, `deletedAgentsMd[]` arrays. Re-exports `FileChange` from `../change-detection/types.js`.\n\n## Update Algorithm\n\n**Hash-Based Change Detection** (orchestrator.ts `preparePlan()` method):\n1. `checkPrerequisites()` verifies git repository via `isGitRepo()`\n2. `getCurrentCommit()` retrieves HEAD commit SHA\n3. `discoverFiles()` wraps `runDiscovery()` from `../discovery/run.js`, converts absolute paths to relative via `path.relative()`\n4. For each file: `getSumPath()` locates `.sum` file, `readSumFile()` extracts `contentHash` from YAML frontmatter, `computeContentHash()` computes current SHA-256\n5. Hash mismatch or missing `.sum` → `FileChange` with `status: 'added'|'modified'` added to `filesToAnalyze[]`\n6. Hash match → path added to `filesToSkip[]`\n7. `cleanupOrphans()` deletes `.sum` files for deleted/renamed sources\n8. `getAffectedDirectories()` computes unique parent directory set from `filesToAnalyze[]`\n9. `affectedDirs[]` sorted by depth descending via `split(path.sep).length` (deepest-first for bottom-up regeneration)\n\n**Orphan Cleanup** (orphan-cleaner.ts):\n- Deleted files: Appends `.sum` to `change.path`, calls `deleteIfExists()`\n- Renamed files: Appends `.sum` to `change.oldPath`, calls `deleteIfExists()`\n- Empty directories: `cleanupEmptyDirectoryDocs()` removes `AGENTS.md` if directory contains no source files (excludes hidden files, `.sum` files, `AGENTS.md`, `CLAUDE.md`)\n- `deleteIfExists()` internal helper: `stat()` existence check, `unlink()` delete (skipped if `dryRun === true`), returns boolean\n\n## Integration Points\n\n**Upstream Dependencies:**\n- `../change-detection/index.js` — `isGitRepo()`, `getCurrentCommit()`, `computeContentHash()`, `FileChange` type\n- `../generation/writers/sum.js` — `readSumFile()` (parses YAML frontmatter), `getSumPath()` (resolves `.sum` path)\n- `../discovery/run.js` — `discoverFiles()` wrapping `runDiscovery()` for file scanning\n- `../config/schema.js` — `Config` type with exclude patterns and concurrency settings\n- `../orchestration/trace.js` — `ITraceWriter` interface for NDJSON trace emission\n\n**Downstream Consumers:**\n- `../cli/update.ts` — CLI entry point invoking `createUpdateOrchestrator()` with parsed options\n- `hooks/are-session-end.js` — Session hook spawning `npx agents-reverse-engineer@latest update --quiet` on uncommitted changes\n\n## API Compatibility Pattern\n\norchestrator.ts no-op methods preserve interface contract for consumers expecting SQLite-based state management (removed in frontmatter migration):\n- `close()` — No database resources to clean\n- `recordFileAnalyzed()` — Hash now stored in `.sum` YAML frontmatter\n- `removeFileState()` — Cleanup handled by `cleanupOrphans()`\n- `recordRun()` — Returns `0` (no run history tracking)\n- `getLastRun()` — Returns `undefined` (no historical run data)\n\n## Trace Events\n\norchestrator.ts emits via `ITraceWriter`:\n- `{ type: 'phase:start', phase: 'update-plan-creation' }` — Start of `preparePlan()`\n- `{ type: 'plan:created', planType: 'update', fileCount, taskCount }` — Plan finalized\n- `{ type: 'phase:end', phase: 'update-plan-creation', durationMs, tasksCompleted: 1, tasksFailed: 0 }` — Plan complete\n\n## Debug Logging\n\norchestrator.ts stderr output (when `debug: true`):\n- `'[debug] Creating update plan with change detection...'` — Start signal\n- `'[debug] Git commit: <hash>'` — First 7 characters of commit SHA\n- `'[debug] Discovering files...'` — Before discovery phase\n- `'[debug] Change detection: <changed> changed, <unchanged> unchanged, <orphaned> orphaned'` — After hash comparison\n- `'[debug] Affected directories: <count>'` — Directory regeneration scope",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 1831,
      "cacheReadTokens": 17477,
      "cacheCreationTokens": 12088,
      "latencyMs": 34580,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": []
    },
    {
      "timestamp": "2026-02-09T16:13:38.888Z",
      "prompt": "Generate AGENTS.md for directory: \"src/config\" (config)\n\n## File Summaries (3 files)\n\n### defaults.ts\n**Purpose:** defaults.ts computes adaptive concurrency limits based on CPU cores and system memory, exports default exclusion list...\n\n**defaults.ts computes adaptive concurrency limits based on CPU cores and system memory, exports default exclusion lists for vendor directories, file patterns, and binary extensions, and provides the complete default configuration object structure.**\n\n## Exported Functions\n\n**getDefaultConcurrency(): number** computes default worker pool size using formula `clamp(cores * 5, MIN, min(memCap, MAX))` where cores comes from `os.availableParallelism()` or `os.cpus().length` fallback, memCap equals `Math.floor((totalMemGB * 0.5) / 0.512)` capping memory usage at 50% of total RAM divided by 512MB subprocess heap budget, MIN_CONCURRENCY is 2, MAX_CONCURRENCY is 20, and CONCURRENCY_MULTIPLIER is 5.\n\n## Exported Constants\n\n**DEFAULT_VENDOR_DIRS** is readonly array of 18 directory names: `['node_modules', 'vendor', '.git', 'dist', 'build', '__pycache__', '.next', 'venv', '.venv', 'target', '.cargo', '.gradle', '.agents-reverse-engineer', '.agents', '.planning', '.claude', '.opencode', '.gemini']`.\n\n**DEFAULT_EXCLUDE_PATTERNS** is readonly array of 26 gitignore-style glob patterns excluding AI assistant docs (`AGENTS.md`, `CLAUDE.md`, `OPENCODE.md`, `GEMINI.md`, `**/AGENTS.md`, `**/CLAUDE.md`, `**/OPENCODE.md`, `**/GEMINI.md`), lock files (`*.lock`, `package-lock.json`, `yarn.lock`, `pnpm-lock.yaml`, `bun.lock`, `bun.lockb`, `Gemfile.lock`, `Cargo.lock`, `poetry.lock`, `composer.lock`, `go.sum`), and dotfiles/artifacts (`.gitignore`, `.gitattributes`, `.gitkeep`, `.env`, `**/.env`, `**/.env.*`, `*.log`, `*.sum`, `**/*.sum`, `**/SKILL.md`).\n\n**DEFAULT_BINARY_EXTENSIONS** is readonly array of 26 extensions: images (`.png`, `.jpg`, `.jpeg`, `.gif`, `.bmp`, `.ico`, `.webp`), archives (`.zip`, `.tar`, `.gz`, `.rar`, `.7z`), executables (`.exe`, `.dll`, `.so`, `.dylib`), media (`.mp3`, `.mp4`, `.wav`), documents (`.pdf`), fonts (`.woff`, `.woff2`, `.ttf`, `.eot`), compiled (`.class`, `.pyc`).\n\n**DEFAULT_MAX_FILE_SIZE** equals `1024 * 1024` (1MB byte threshold for binary detection).\n\n**DEFAULT_CONFIG** is readonly object with structure `{ exclude: { patterns, vendorDirs, binaryExtensions }, options: { followSymlinks: false, maxFileSize }, output: { colors: true } }` spreading all default arrays and using DEFAULT_MAX_FILE_SIZE value.\n\n## Internal Constants\n\n**CONCURRENCY_MULTIPLIER** is 5 (factor applied to CPU core count).\n\n**MIN_CONCURRENCY** is 2 (floor enforced by getDefaultConcurrency).\n\n**MAX_CONCURRENCY** is 20 (ceiling matching Zod schema `.max(20)` constraint).\n\n**SUBPROCESS_HEAP_GB** is 0.512 (512MB heap budget per subprocess matching `NODE_OPTIONS='--max-old-space-size=512'` in subprocess.ts).\n\n**MEMORY_FRACTION** is 0.5 (allocates 50% of total system memory to subprocess pool).\n\n## Memory-Aware Concurrency Logic\n\ngetDefaultConcurrency checks if `totalMemGB > 1` before computing memCap to avoid division-by-zero or invalid floor operations on systems with less than 1GB RAM (returns Infinity bypassing memory constraint). Returns `Math.max(MIN_CONCURRENCY, Math.min(computed, memCap, MAX_CONCURRENCY))` ensuring result falls within [2, 20] interval while respecting memory capacity ceiling.\n\n## Integration Points\n\nReferenced by `src/config/schema.ts` (merges with user config via `config.ai?.concurrency ?? getDefaultConcurrency()`), `src/config/loader.ts` (spreads DEFAULT_CONFIG as base object before YAML overlay), and indirectly by `src/orchestration/pool.ts` (consumes final concurrency value for worker pool sizing).\n### loader.ts\n**Purpose:** loadConfig() loads `.agents-reverse-engineer/config.yaml` via YAML parsing, validates with ConfigSchema Zod schema, r...\n\n**loadConfig() loads `.agents-reverse-engineer/config.yaml` via YAML parsing, validates with ConfigSchema Zod schema, returns Config with defaults when file absent, and throws ConfigError on parse/validation failures; writeDefaultConfig() generates commented YAML with DEFAULT_VENDOR_DIRS/DEFAULT_BINARY_EXTENSIONS/DEFAULT_EXCLUDE_PATTERNS arrays; configExists() checks file presence via fs.access().**\n\n## Exported Functions\n\n**loadConfig(root: string, options?: { tracer?: ITraceWriter; debug?: boolean }): Promise<Config>**\nReads `${root}/${CONFIG_DIR}/${CONFIG_FILE}` (resolves to `.agents-reverse-engineer/config.yaml`), parses YAML via `parse()`, validates with `ConfigSchema.parse(raw)`, returns validated Config object. When ENOENT caught, returns `ConfigSchema.parse({})` triggering default population. Emits `config:loaded` trace event via `options.tracer?.emit()` with fields `{ type, configPath, model, concurrency }`. Writes debug output via `console.error(pc.dim())` when `options.debug` enabled. Throws ConfigError wrapping ZodError with formatted `issue.path.join('.')` and `issue.message` for validation failures. Throws ConfigError for YAML parse errors with underlying error as `cause`.\n\n**configExists(root: string): Promise<boolean>**\nChecks presence of `${root}/${CONFIG_DIR}/${CONFIG_FILE}` via `access(configPath, constants.F_OK)`, returns true on success, false on catch.\n\n**writeDefaultConfig(root: string): Promise<void>**\nCreates `${root}/${CONFIG_DIR}` via `mkdir(configDir, { recursive: true })`, writes YAML template to `${root}/${CONFIG_DIR}/${CONFIG_FILE}` via `writeFile()`. Template includes comment headers (`# FILE & DIRECTORY EXCLUSIONS`, `# DISCOVERY OPTIONS`, `# OUTPUT FORMATTING`, `# AI SERVICE CONFIGURATION`) with inline defaults: `exclude.patterns` array from DEFAULT_EXCLUDE_PATTERNS (mapped with `yamlScalar(pattern)` quoting), `exclude.vendorDirs` from DEFAULT_VENDOR_DIRS, `exclude.binaryExtensions` from DEFAULT_BINARY_EXTENSIONS, `options.maxFileSize: ${DEFAULT_MAX_FILE_SIZE}` interpolation, `ai.timeoutMs: 300000`, `ai.maxRetries: 3`, `ai.concurrency` commented with `${getDefaultConcurrency()}` current machine value, `ai.telemetry.keepRuns: 50`.\n\n## Exported Constants\n\n**CONFIG_DIR = '.agents-reverse-engineer'**\nDirectory name for configuration root.\n\n**CONFIG_FILE = 'config.yaml'**\nConfiguration filename within CONFIG_DIR.\n\n## Exported Classes\n\n**ConfigError extends Error**\nConstructor signature: `(message: string, filePath: string, cause?: Error)`. Public readonly property `filePath: string` stores config file path. Public readonly property `cause?: Error` stores underlying ZodError or YAML parse error. Sets `name` property to `'ConfigError'` string literal.\n\n## Dependencies\n\nImports `readFile`, `writeFile`, `mkdir`, `access` from `node:fs/promises`, `constants` from `node:fs`, `path` from `node:path`, `parse`/`stringify` from `yaml` package, `ZodError` from `zod`, `pc` (picocolors) for `pc.dim()` formatting. Imports `ConfigSchema` and `Config` type from `./schema.js`. Imports DEFAULT_VENDOR_DIRS, DEFAULT_BINARY_EXTENSIONS, DEFAULT_MAX_FILE_SIZE, DEFAULT_EXCLUDE_PATTERNS, getDefaultConcurrency from `./defaults.js`. Imports `ITraceWriter` type from `../orchestration/trace.js`.\n\n## Helper Functions\n\n**yamlScalar(value: string): string**\nInternal function wrapping strings containing YAML metacharacters in double quotes. Regex pattern `/[*{}\\[\\]?,:#&!|>'\"%@`]/` detects characters requiring quoting (asterisk for aliases, braces for flow collections, square brackets for sequences, question mark/colon for mappings, hash for comments, ampersand for anchors, pipe/greater for block scalars, quotes, percent for directives, at for reserved indicators, backtick). Escapes backslashes via `.replace(/\\\\/g, '\\\\\\\\')` before escaping quotes via `.replace(/\"/g, '\\\\\"')`. Returns unquoted value when regex test fails.\n\n## Error Handling Patterns\n\nConfigError wraps two error types: ZodError from schema validation (extracts `err.issues[]` array, maps to formatted strings `${issue.path.join('.')}: ${issue.message}`, joins with newlines in error message), and generic Error from YAML parse failures (includes `(err as Error).message` in error message). ENOENT catch block for missing config files returns defaults without throwing. ConfigError instances rethrown as-is during loadConfig() error handling. All other errors cast to Error and wrapped in ConfigError with `configPath` and `cause` properties.\n\n## Trace Event Schema\n\n`config:loaded` event emitted via `tracer.emit()` with fields:\n- `type: 'config:loaded'` (string literal discriminator)\n- `configPath: string` (relative path via `path.relative(root, configPath)` or literal `'(defaults)'` when file absent)\n- `model: string` (from `config.ai.model`)\n- `concurrency: number` (from `config.ai.concurrency`)\n\n## YAML Template Structure\n\nGenerated config includes four section headers with inline documentation:\n1. `# FILE & DIRECTORY EXCLUSIONS` — maps `exclude.patterns`, `exclude.vendorDirs`, `exclude.binaryExtensions` arrays\n2. `# DISCOVERY OPTIONS` — sets `options.followSymlinks: false`, `options.maxFileSize: ${DEFAULT_MAX_FILE_SIZE}`\n3. `# OUTPUT FORMATTING` — sets `output.colors: true`\n4. `# AI SERVICE CONFIGURATION` — maps `ai.backend: auto`, `ai.model: sonnet`, `ai.timeoutMs: 300000`, `ai.maxRetries: 3`, commented `ai.concurrency` line with current default, nested `ai.telemetry.keepRuns: 50`\n### schema.ts\n**Purpose:** ConfigSchema defines Zod validation schema for `.agents-reverse-engineer/config.yaml` with nested object schemas for ...\n\n**ConfigSchema defines Zod validation schema for `.agents-reverse-engineer/config.yaml` with nested object schemas for exclusion rules, discovery options, output formatting, and AI service configuration, providing type-safe defaults via z.default() chaining.**\n\n## Exported Schemas\n\n**ConfigSchema** (z.ZodDefault<z.ZodObject>) — Root configuration schema with four nested sections: `exclude`, `options`, `output`, `ai`. Accepts empty object `{}` which populates all fields with defaults via `.default({})` at schema level.\n\n**ExcludeSchema** (z.ZodDefault<z.ZodObject>) — Defines `patterns` (array of gitignore-style globs from `DEFAULT_EXCLUDE_PATTERNS`), `vendorDirs` (array from `DEFAULT_VENDOR_DIRS`), `binaryExtensions` (array from `DEFAULT_BINARY_EXTENSIONS`). All arrays cloned via spread operator `[...DEFAULT_*]` to prevent mutation.\n\n**OptionsSchema** (z.ZodDefault<z.ZodObject>) — Defines `followSymlinks` (boolean, default false), `maxFileSize` (positive number from `DEFAULT_MAX_FILE_SIZE`). Controls file discovery traversal behavior.\n\n**OutputSchema** (z.ZodDefault<z.ZodObject>) — Defines `colors` (boolean, default true) for ANSI terminal color code enablement.\n\n**AISchema** (z.ZodDefault<z.ZodObject>) — Defines `backend` (enum: 'claude' | 'gemini' | 'opencode' | 'auto', default 'auto'), `model` (string, default 'sonnet'), `timeoutMs` (positive number, default 300000 = 5 minutes), `maxRetries` (min 0, default 3), `concurrency` (min 1, max 20, default from `getDefaultConcurrency()` function), `telemetry` nested object with `keepRuns` (min 0, default 50). Concurrency default is computed dynamically via function invocation during schema construction.\n\n## Exported TypeScript Types\n\n**Config** — Inferred via `z.infer<typeof ConfigSchema>` representing fully parsed configuration object with all defaults applied.\n\n**ExcludeConfig** — Inferred from ExcludeSchema: `{ patterns: string[], vendorDirs: string[], binaryExtensions: string[] }`.\n\n**OptionsConfig** — Inferred from OptionsSchema: `{ followSymlinks: boolean, maxFileSize: number }`.\n\n**OutputConfig** — Inferred from OutputSchema: `{ colors: boolean }`.\n\n**AIConfig** — Inferred from AISchema: `{ backend: 'claude' | 'gemini' | 'opencode' | 'auto', model: string, timeoutMs: number, maxRetries: number, concurrency: number, telemetry: { keepRuns: number } }`.\n\n## Import Dependencies\n\nImports `z` from 'zod' for schema construction. Imports constants `DEFAULT_VENDOR_DIRS`, `DEFAULT_BINARY_EXTENSIONS`, `DEFAULT_MAX_FILE_SIZE`, `DEFAULT_EXCLUDE_PATTERNS` and function `getDefaultConcurrency` from `./defaults.js`. These provide platform-specific defaults (e.g., concurrency adjusted for WSL vs. macOS/Linux based on CPU cores and available memory).\n\n## Schema Validation Pattern\n\nAll schemas use `.default()` chaining at both field level (individual fields) and object level (entire section). This enables partial configuration merging: user provides `{ ai: { backend: 'claude' } }`, Zod merges with defaults to produce complete AIConfig with timeoutMs, maxRetries, concurrency, telemetry.keepRuns populated from schema defaults.\n\nArray defaults use spread operator `[...DEFAULT_PATTERNS]` to clone imported arrays, preventing accidental mutation of shared constant references during schema validation.\n\n## Dynamic Default Resolution\n\nAISchema.concurrency uses function invocation `.default(getDefaultConcurrency)` instead of static value. Zod evaluates function during parse to compute platform-specific concurrency (2 for WSL, 5 for others, adjusted for CPU cores/memory). Function must return number matching `.min(1).max(20)` constraint.\n\n## Import Map (verified — use these exact paths)\n\nloader.ts:\n  ../orchestration/trace.js → ITraceWriter (type)\n\n## Project Directory Structure\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>",
      "systemPrompt": "You are generating an AGENTS.md file — a directory-level overview for AI coding assistants.\n\nCRITICAL: Output ONLY the raw markdown content. No code fences, no preamble, no explanations, no conversational text. Your entire response IS the AGENTS.md file content.\n\nOUTPUT FORMAT:\n- First line MUST be exactly: <!-- Generated by agents-reverse-engineer -->\n- Use a # heading with the directory name\n- Write a one-paragraph purpose statement for the directory\nusage\nADAPTIVE SECTIONS:\nAnalyze the directory contents and choose the most relevant sections. Do NOT use a fixed template. Instead, select sections that best document this specific directory for an AI that needs to reconstruct or extend the project.\n\nConsider these section types (choose what applies):\n- **Contents**: Group files by purpose/category under ## headings. For each file: markdown link [filename](./filename) and a one-line description.\n- **Subdirectories**: If subdirectories exist, list them with links [dirname/](./dirname/) and brief summaries.\n- **Architecture / Data Flow**: If files form a pipeline, request/response chain, or layered architecture, document it.\n- **Stack**: If this is a package root (has package.json, Cargo.toml, go.mod, etc.), document the technology stack, key scripts, and entry points.\n- **Structure**: If the directory layout follows a convention (feature-sliced, domain-driven, MVC, etc.), document it.\n- **Patterns**: If files share recurring design patterns (factory, strategy, middleware, barrel re-export), name and document them.\n- **Configuration**: If the directory contains config files, schemas, or environment definitions, document the config surface area.\n- **API Surface**: If the directory exports a public API (barrel index, route definitions, SDK), document the interface contract.\n- **File Relationships**: How files collaborate, depend on each other, or share state.\n- **Behavioral Contracts**: If files contain regex patterns, format specifications, magic constants, or template strings that define observable behavior, collect them in a dedicated section. Preserve verbatim patterns from file summaries — do NOT paraphrase regex into prose. This section is MANDATORY when file summaries contain behavioral artifacts.\n\nChoose any relevant sections or create your own based on the directory contents. The goal is to provide a comprehensive overview that captures the essence of the directory's role in the project and how its files work together, with a focus on what an AI coding assistant would need to know to effectively interact with this code.\n\nSCOPE:\n- AGENTS.md is a NAVIGATIONAL INDEX — help an AI find the right file quickly\n- Focus on: what each file does, how files relate, directory-level patterns\n- Do NOT reproduce full architecture sections — those belong in the root CLAUDE.md\n\nPATH ACCURACY (MANDATORY):\n- When referencing files or modules outside this directory, use ONLY paths from the \"Import Map\" section\n- Do NOT invent, rename, or guess module paths — if a path isn't in the Import Map, don't reference it\n- Use the exact directory names from \"Project Directory Structure\" — do NOT rename directories\n  (e.g., if the directory is called \"cli\", write \"src/cli/\", NOT \"src/commands/\")\n- Cross-module references must use the specifier format from actual import statements\n  (e.g., \"../generation/writers/sum.js\", NOT \"../fs/sum-file.js\")\n- If you are unsure about a path, omit the cross-reference rather than guessing\n\nCONSISTENCY (MANDATORY):\n- Do not contradict yourself within the same document\n- If you describe a technique (e.g., \"regex-based\"), do not call it something else later (e.g., \"AST-based\")\n- When stating version numbers, engines, or config fields, use ONLY values present in the file summaries\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this directory\", \"this module\", \"provides\", \"responsible for\", \"is used to\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Per-file descriptions in Contents sections: 1-2 sentences maximum. Reference key symbols but do not reproduce full summaries.\n- Behavioral contracts (regex patterns, format specs, constants) belong in a separate Behavioral Contracts section, not in per-file descriptions.\n- Subdirectory descriptions: 1-2 sentences maximum. Capture the directory's role, not its full contents.\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- Key exported symbols from file summaries MUST appear in the directory overview\n- Preserve exact casing of identifiers\n\nUSER NOTES:\n- If \"User Notes\" are provided in the prompt, they contain user-defined instructions that will be automatically prepended to your output\n- Do NOT repeat or paraphrase user notes in your generated content — they are included separately\n- You may reference information from user notes for context",
      "response": "<!-- Generated by agents-reverse-engineer -->\n\n# src/config\n\nConfiguration management layer providing YAML-based project configuration with Zod schema validation, adaptive concurrency computation based on system resources, and default exclusion rules for file discovery.\n\n## Contents\n\n### Core Modules\n\n**[schema.ts](./schema.ts)** — Exports `ConfigSchema` (root Zod schema accepting empty object), `ExcludeSchema` (patterns/vendorDirs/binaryExtensions arrays), `OptionsSchema` (followSymlinks/maxFileSize), `OutputSchema` (colors boolean), `AISchema` (backend enum/model/timeoutMs/maxRetries/concurrency/telemetry with dynamic default via `getDefaultConcurrency()` invocation), and inferred TypeScript types `Config`, `ExcludeConfig`, `OptionsConfig`, `OutputConfig`, `AIConfig`.\n\n**[loader.ts](./loader.ts)** — Exports `loadConfig(root, options)` reading `.agents-reverse-engineer/config.yaml` via YAML parsing and `ConfigSchema.parse()` validation (returns defaults on ENOENT, emits `config:loaded` trace event, throws `ConfigError` on ZodError/parse failures), `configExists(root)` checking file presence via `fs.access()`, `writeDefaultConfig(root)` generating commented YAML template with inline defaults, `ConfigError` class extending Error with `filePath` and `cause` properties, and constants `CONFIG_DIR` ('.agents-reverse-engineer'), `CONFIG_FILE` ('config.yaml').\n\n**[defaults.ts](./defaults.ts)** — Exports `getDefaultConcurrency()` computing worker pool size via formula `clamp(cores * 5, 2, min(memCap, 20))` where `memCap = floor((totalMemGB * 0.5) / 0.512)` caps memory usage at 50% of RAM divided by 512MB subprocess heap budget, constants `DEFAULT_VENDOR_DIRS` (18 directories: node_modules/.git/dist/build/__pycache__/.next/venv/.venv/target/.cargo/.gradle/.agents-reverse-engineer/.agents/.planning/.claude/.opencode/.gemini/vendor), `DEFAULT_EXCLUDE_PATTERNS` (26 globs: AI docs/lock files/dotfiles), `DEFAULT_BINARY_EXTENSIONS` (26 extensions: images/archives/executables/media/documents/fonts/compiled), `DEFAULT_MAX_FILE_SIZE` (1MB threshold), and `DEFAULT_CONFIG` object spreading all default arrays.\n\n## Configuration Schema Hierarchy\n\nConfigSchema composes four nested schemas via Zod object merging:\n\n1. **exclude** (ExcludeSchema) — File/directory filtering rules\n   - `patterns: string[]` — Gitignore-style globs (default: 26 patterns excluding AGENTS.md/CLAUDE.md/lock files/*.sum/*.log)\n   - `vendorDirs: string[]` — Third-party directories to skip (default: 18 directories)\n   - `binaryExtensions: string[]` — Non-text file extensions (default: 26 extensions)\n\n2. **options** (OptionsSchema) — Discovery traversal behavior\n   - `followSymlinks: boolean` — Enable symlink following (default: false)\n   - `maxFileSize: number` — Binary detection threshold in bytes (default: 1048576 = 1MB)\n\n3. **output** (OutputSchema) — CLI rendering preferences\n   - `colors: boolean` — Enable ANSI color codes (default: true)\n\n4. **ai** (AISchema) — AI service orchestration parameters\n   - `backend: 'claude' | 'gemini' | 'opencode' | 'auto'` — CLI backend selection (default: 'auto')\n   - `model: string` — Backend-specific model identifier (default: 'sonnet')\n   - `timeoutMs: number` — Subprocess timeout milliseconds (default: 300000 = 5 minutes)\n   - `maxRetries: number` — Exponential backoff retry attempts (default: 3, min: 0)\n   - `concurrency: number` — Worker pool size (default: dynamic via `getDefaultConcurrency()`, min: 1, max: 20)\n   - `telemetry.keepRuns: number` — Run log retention count (default: 50, min: 0)\n\n## Adaptive Concurrency Algorithm\n\n`getDefaultConcurrency()` implements memory-aware concurrency calculation:\n\n1. **CPU-based baseline**: `cores * 5` where `cores = os.availableParallelism() || os.cpus().length`\n2. **Memory capacity ceiling**: `memCap = floor((totalMemGB * 0.5) / 0.512)` allocating 50% of system RAM with 512MB per subprocess\n3. **Clamping**: `max(2, min(baseline, memCap, 20))` enforcing [2, 20] interval\n4. **Edge case handling**: Returns Infinity when `totalMemGB <= 1` to bypass memory constraint on low-RAM systems\n\nConstants:\n- `CONCURRENCY_MULTIPLIER = 5` (CPU core scaling factor)\n- `MIN_CONCURRENCY = 2` (floor matching WSL default)\n- `MAX_CONCURRENCY = 20` (ceiling matching Zod `.max(20)` constraint)\n- `SUBPROCESS_HEAP_GB = 0.512` (512MB heap budget from `NODE_OPTIONS='--max-old-space-size=512'` in subprocess.ts)\n- `MEMORY_FRACTION = 0.5` (allocate 50% of total RAM to pool)\n\n## Configuration Loading Workflow\n\n`loadConfig(root, options)` executes sequence:\n\n1. **Path resolution**: Constructs `${root}/.agents-reverse-engineer/config.yaml`\n2. **File read**: `readFile(configPath, 'utf-8')` with ENOENT catch returning `ConfigSchema.parse({})`\n3. **YAML parsing**: `parse(rawYaml)` with Error catch wrapped in ConfigError\n4. **Schema validation**: `ConfigSchema.parse(raw)` with ZodError catch formatted as `${path}: ${message}`\n5. **Trace emission**: `tracer?.emit({ type: 'config:loaded', configPath, model, concurrency })`\n6. **Debug output**: `console.error(pc.dim(JSON.stringify(config, null, 2)))` when `options.debug` enabled\n\nError paths throw ConfigError with:\n- `message`: Formatted validation errors or YAML parse message\n- `filePath`: Absolute config file path\n- `cause`: Underlying ZodError or Error instance\n\n## Default Configuration Template\n\n`writeDefaultConfig(root)` generates YAML with four comment-delimited sections:\n\n```yaml\n# FILE & DIRECTORY EXCLUSIONS\nexclude:\n  patterns: [...26 quoted globs...]\n  vendorDirs: [...18 directory names...]\n  binaryExtensions: [...26 extensions...]\n\n# DISCOVERY OPTIONS\noptions:\n  followSymlinks: false\n  maxFileSize: 1048576\n\n# OUTPUT FORMATTING\noutput:\n  colors: true\n\n# AI SERVICE CONFIGURATION\nai:\n  backend: auto\n  model: sonnet\n  timeoutMs: 300000\n  maxRetries: 3\n  # concurrency: <current machine default>\n  telemetry:\n    keepRuns: 50\n```\n\n`yamlScalar(value)` helper quotes strings containing YAML metacharacters (`[*{}\\[\\]?,:#&!|>'\"%@` ]`) via regex `/[*{}\\[\\]?,:#&!|>'\"%@`]/` test, escapes backslashes (`\\\\` → `\\\\\\\\`) and quotes (`\"` → `\\\\\"`) before wrapping in double quotes.\n\n## Integration with Codebase\n\n**Consumed by**:\n- `src/cli/init.ts` — Calls `writeDefaultConfig()` for initialization command\n- `src/cli/generate.ts`, `src/cli/update.ts`, `src/cli/discover.ts` — Call `loadConfig()` to retrieve runtime configuration\n- `src/orchestration/pool.ts` — Uses `config.ai.concurrency` for worker count\n- `src/ai/service.ts` — Uses `config.ai.backend`, `config.ai.model`, `config.ai.timeoutMs`, `config.ai.maxRetries`\n- `src/discovery/walker.ts` — Uses `config.exclude.*` and `config.options.*` for file filtering\n- `src/output/logger.ts` — Uses `config.output.colors` for picocolors enablement\n- `src/ai/telemetry/cleanup.ts` — Uses `config.ai.telemetry.keepRuns` for log retention\n\n**Provides types to**:\n- All CLI entry points requiring type-safe configuration access\n- Orchestration modules needing concurrency/timeout parameters\n- Discovery filters requiring exclusion rule arrays\n\n## Default Exclusion Patterns\n\n**DEFAULT_EXCLUDE_PATTERNS** (26 globs):\n- AI docs: `AGENTS.md`, `CLAUDE.md`, `OPENCODE.md`, `GEMINI.md` (root + recursive `**/*`)\n- Lock files: `*.lock`, `package-lock.json`, `yarn.lock`, `pnpm-lock.yaml`, `bun.lock`, `bun.lockb`, `Gemfile.lock`, `Cargo.lock`, `poetry.lock`, `composer.lock`, `go.sum`\n- Dotfiles: `.gitignore`, `.gitattributes`, `.gitkeep`, `.env` (root + recursive `**/.env*`)\n- Artifacts: `*.log`, `*.sum` (root + recursive), `**/SKILL.md`\n\n**DEFAULT_VENDOR_DIRS** (18 directories):\n- Package managers: `node_modules`, `vendor`\n- Build outputs: `dist`, `build`, `target`, `.next`, `__pycache__`\n- Version control: `.git`\n- Python venvs: `venv`, `.venv`\n- Cargo/Gradle caches: `.cargo`, `.gradle`\n- AI assistant directories: `.agents-reverse-engineer`, `.agents`, `.planning`, `.claude`, `.opencode`, `.gemini`\n\n**DEFAULT_BINARY_EXTENSIONS** (26 extensions):\n- Images: `.png`, `.jpg`, `.jpeg`, `.gif`, `.bmp`, `.ico`, `.webp`\n- Archives: `.zip`, `.tar`, `.gz`, `.rar`, `.7z`\n- Executables: `.exe`, `.dll`, `.so`, `.dylib`\n- Media: `.mp3`, `.mp4`, `.wav`\n- Documents: `.pdf`\n- Fonts: `.woff`, `.woff2`, `.ttf`, `.eot`\n- Compiled: `.class`, `.pyc`",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 2589,
      "cacheReadTokens": 17477,
      "cacheCreationTokens": 12710,
      "latencyMs": 75758,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": []
    },
    {
      "timestamp": "2026-02-09T16:13:39.152Z",
      "prompt": "Generate AGENTS.md for directory: \"src/installer\" (installer)\n\n## File Summaries (7 files)\n\n### banner.ts\n**Purpose:** banner.ts provides styled terminal output functions for the ARE installer with ASCII art display, help text formattin...\n\n**banner.ts provides styled terminal output functions for the ARE installer with ASCII art display, help text formatting, and colorized message helpers using picocolors.**\n\n## Exported Constants\n\n- `VERSION: string` — Package version string obtained via `getVersion()` from `../version.js`\n\n## Exported Functions\n\n### Display Functions\n\n- `displayBanner(): void` — Renders ASCII art \"ARE\" logo in green via `pc.green()` with version number and tagline \"AI-friendly codebase documentation\" in dimmed text via `pc.dim()`\n\n- `showHelp(): void` — Prints usage instructions with `pc.bold()` section headers covering CLI options (`--runtime`, `-g`/`--global`, `-l`/`--local`, `-u`/`--uninstall`, `--force`, `-q`/`--quiet`, `-h`/`--help`), example invocations with runtime selectors, and installation mode combinations\n\n- `showNextSteps(runtime: string, filesCreated: number): void` — Displays post-installation instructions with bold \"Installation complete!\" header, file count summary via `pc.dim()`, numbered steps referencing ARE commands (`/are-help`, `/are-init`, `/are-discover`, `/are-generate`, `/are-update`, `/are-specify`, `/are-clean`) in cyan via `pc.cyan()`, and GitHub documentation URL\n\n### Message Helpers\n\n- `showSuccess(msg: string): void` — Prints message prefixed with green checkmark `pc.green('✓')`\n\n- `showError(msg: string): void` — Prints message prefixed with red X `pc.red('✗')`\n\n- `showWarning(msg: string): void` — Prints message prefixed with yellow exclamation `pc.yellow('!')`\n\n- `showInfo(msg: string): void` — Prints message prefixed with cyan arrow `pc.cyan('>')`\n\n## Dependencies\n\n- `picocolors` (imported as `pc`) — Terminal color library for ANSI escape sequences\n- `../version.js` — Provides `getVersion()` function returning package version from package.json\n\n## ASCII Art Format\n\nThe ASCII banner uses Unicode box-drawing characters (U+2550–U+255D range) to render block letters \"ARE\" across 6 lines with 25–27 characters per line, followed by version/tagline footer lines.\n### index.ts\n**Purpose:** Orchestrates npx installation workflow for IDE command/hook registration with runtime-specific path resolution, inter...\n\n**Orchestrates npx installation workflow for IDE command/hook registration with runtime-specific path resolution, interactive prompts for missing parameters, and parallel support for Claude Code, OpenCode, and Gemini backends.**\n\n## Exported Functions\n\n**`runInstaller(args: InstallerArgs): Promise<InstallerResult[]>`** — Main entry point executing install/uninstall workflow with location determination, runtime selection, file operations via `installFiles()`/`uninstallFiles()`, verification via `verifyInstallation()`, and styled output formatting. Returns empty array if `args.help` is true. Exits with code 1 in non-interactive mode when `--runtime` or location flags missing.\n\n**`parseInstallerArgs(args: string[]): InstallerArgs`** — Parses CLI arguments supporting short flags (`-g`, `-l`, `-h`, `-q`) and long flags (`--global`, `--local`, `--force`, `--quiet`, `--help`), extracts `--runtime` value with validation against `validRuntimes: Runtime[] = ['claude', 'opencode', 'gemini', 'all']`, returns `InstallerArgs` with boolean flags and optional `runtime` field.\n\n## Re-exported Symbols\n\nExports types from `./types.js`: `InstallerArgs`, `InstallerResult`, `Runtime`, `Location`, `RuntimePaths`\n\nExports functions from module dependencies:\n- `./paths.js`: `getRuntimePaths`, `getAllRuntimes`, `resolveInstallPath`, `getSettingsPath`\n- `./banner.js`: `displayBanner`, `showHelp`, `showSuccess`, `showError`, `showWarning`, `showInfo`, `showNextSteps`, `VERSION`\n- `./prompts.js`: `selectRuntime`, `selectLocation`, `confirmAction`, `isInteractive`\n\n## Internal Workflow Functions\n\n**`determineLocation(args: InstallerArgs): Location | undefined`** — Returns `'global'` if `args.global && !args.local`, returns `'local'` if `args.local && !args.global`, returns `undefined` (triggers interactive prompt) if both or neither flags set.\n\n**`determineRuntimes(runtime: Runtime | undefined): Array<Exclude<Runtime, 'all'>>`** — Returns empty array if `runtime` is undefined (triggers prompt), calls `getAllRuntimes()` if `runtime === 'all'`, otherwise returns single-element array with specific runtime.\n\n**`runInstall(runtime: Runtime, location: Location, force: boolean, quiet: boolean): Promise<InstallerResult[]>`** — Executes installation by calling `installFiles(runtime, location, { force, dryRun: false })`, flattens `results.flatMap((r) => r.filesCreated)` for verification input, calls `verifyInstallation(allCreatedFiles)`, logs errors for missing files via `verification.missing`, invokes `displayInstallResults(results)` unless `quiet` is true.\n\n**`runUninstall(runtime: Runtime, location: Location, quiet: boolean): InstallerResult[]`** — Executes uninstallation by calling `uninstallFiles(runtime, location, false)`, calls `deleteConfigFolder(location, false)` returning `configDeleted` boolean, invokes `displayUninstallResults(results, configDeleted)` unless `quiet` is true.\n\n## Display Functions\n\n**`displayInstallResults(results: InstallerResult[]): void`** — Iterates over results accumulating `totalCreated`, `totalSkipped`, `hooksRegistered` counters, calls `showSuccess()`/`showError()` per result, displays summary with `showSuccess()` for created files and registered hooks, calls `showWarning()` for skipped files with `--force` hint, invokes `showNextSteps(primaryRuntime, totalCreated)` where `primaryRuntime = results[0]?.runtime || 'claude'`, outputs GitHub docs link via `showInfo('Docs: https://github.com/GeoloeG-IsT/agents-reverse-engineer')`.\n\n**`displayUninstallResults(results: InstallerResult[], configDeleted: boolean = false): void`** — Iterates results tracking `totalDeleted` (from `result.filesCreated.length` repurposed for deleted files), `hooksUnregistered` (from `result.hookRegistered` repurposed for unregister count), calls `showSuccess()` for non-zero deletions, `showInfo()` when no files found, displays summary with removal counts and config folder deletion status.\n\n## Non-Interactive Mode Constraints\n\nRequires `--runtime` flag with validated value (`'claude'` | `'opencode'` | `'gemini'` | `'all'`), requires `-g`/`--global` or `-l`/`--local` flag, calls `showError()` and `process.exit(1)` if either missing when `!isInteractive()`, skips prompts entirely in non-interactive context.\n\n## Integration Points\n\nDepends on `./operations.js` for `installFiles()`, `verifyInstallation()`, `formatInstallResult()`, depends on `./uninstall.js` for `uninstallFiles()`, `deleteConfigFolder()`, depends on `./prompts.js` for `selectRuntime()`, `selectLocation()`, `isInteractive()` interactive mode detection, depends on `./banner.js` for all styled terminal output functions, depends on `./paths.js` for `getAllRuntimes()` and `resolveInstallPath()`.\n\n## Runtime Type Definition\n\nType `Runtime = 'claude' | 'opencode' | 'gemini' | 'all'` validated in `parseInstallerArgs()` via `validRuntimes` array, expanded to specific runtime array via `determineRuntimes()` when value is `'all'`, passed to `installFiles()`/`uninstallFiles()` which handle multi-runtime fan-out internally.\n### operations.ts\n**Purpose:** `operations.ts` installs ARE command templates and session hooks to IDE runtime directories (~/.claude, ~/.opencode, ...\n\n**`operations.ts` installs ARE command templates and session hooks to IDE runtime directories (~/.claude, ~/.opencode, ~/.gemini), registers hooks in settings.json, writes ARE-VERSION files, and verifies installations.**\n\n## Exported Functions\n\n**`installFiles(runtime: Runtime, location: Location, options: InstallOptions): InstallerResult[]`**\nInstalls command templates and hooks for one or all runtimes. When `runtime === 'all'`, calls `getAllRuntimes()` and maps `installFilesForRuntime()` across all supported runtimes. Returns array of `InstallerResult` objects with `filesCreated`, `filesSkipped`, `errors`, `hookRegistered`, `versionWritten` fields.\n\n**`verifyInstallation(files: string[]): { success: boolean; missing: string[] }`**\nChecks existence of file paths via `existsSync()`. Returns object with `success: true` if all files exist, otherwise `missing` array of absent paths.\n\n**`registerHooks(basePath: string, runtime: Exclude<Runtime, 'all'>, dryRun: boolean): boolean`**\nRegisters ARE session hooks in `settings.json` for Claude and Gemini runtimes. Dispatches to `registerClaudeHooks()` or `registerGeminiHooks()` based on runtime. Returns `true` if any hooks were added, `false` if all already existed. Skips for OpenCode (uses plugin system).\n\n**`registerPermissions(settingsPath: string, dryRun: boolean): boolean`**\nAdds `ARE_PERMISSIONS` bash command patterns to Claude Code `settings.json` permissions.allow array. Returns `true` if any permissions added. Used to reduce friction for ARE commands during execution.\n\n**`getPackageVersion(): string`**\nReads version from `package.json` via `fileURLToPath(import.meta.url)` → `path.join(__dirname, '..', '..', 'package.json')`. Returns version string or `'unknown'` on parse failure.\n\n**`writeVersionFile(basePath: string, dryRun: boolean): void`**\nWrites `ARE-VERSION` file containing result of `getPackageVersion()` to `basePath`. Used by session hooks to detect installed version. Skips if `dryRun === true`.\n\n**`formatInstallResult(result: InstallerResult): string[]`**\nGenerates human-readable display lines showing runtime/location header, created/skipped file paths, hook registration status, and summary counts.\n\n## Exported Interfaces\n\n**`InstallOptions`**\n```typescript\n{\n  force: boolean;    // Overwrite existing files\n  dryRun: boolean;   // Preview without writing\n}\n```\n\n## Internal Functions\n\n**`installFilesForRuntime(runtime: Exclude<Runtime, 'all'>, location: Location, options: InstallOptions): InstallerResult`**\nCore installation logic. Calls `resolveInstallPath()` to get base directory, `getTemplatesForRuntime()` for command templates, writes template files via `writeFileSync()` after `ensureDir()`, copies hooks/plugins via `readBundledHook()` → `writeFileSync()`, registers hooks in settings.json, calls `writeVersionFile()`. Returns `InstallerResult` with success status and file lists.\n\n**`getTemplatesForRuntime(runtime: Exclude<Runtime, 'all'>): Template[]`**\nDispatches to `getClaudeTemplates()`, `getOpenCodeTemplates()`, or `getGeminiTemplates()` from `src/integration/templates.ts` based on runtime.\n\n**`ensureDir(filePath: string): void`**\nCreates parent directory for `filePath` via `mkdirSync(path.dirname(filePath), { recursive: true })` if it doesn't exist.\n\n**`getBundledHookPath(hookName: string): string`**\nResolves path to bundled hook file in `hooks/dist/` relative to compiled module location. Uses `fileURLToPath(import.meta.url)` → `path.join(__dirname, '..', '..', 'hooks', 'dist', hookName)`.\n\n**`readBundledHook(hookName: string): string`**\nReads hook file content via `getBundledHookPath()` and `readFileSync()`. Throws error if hook not found.\n\n**`registerClaudeHooks(settingsPath: string, runtimeDir: string, dryRun: boolean): boolean`**\nRegisters hooks in Claude Code format (nested `hooks` array). For each `ARE_HOOKS` entry, constructs `hookCommand: \"node ${runtimeDir}/hooks/${filename}\"`, checks if hook exists via command string match, appends `HookEvent` object with `hooks: [{ type: 'command', command }]` structure. Writes merged settings to `settingsPath`.\n\n**`registerGeminiHooks(settingsPath: string, runtimeDir: string, dryRun: boolean): boolean`**\nRegisters hooks in Gemini CLI format (flat object with `name` field). Constructs `GeminiHook: { name, type: 'command', command }`, checks existence, appends to event array. Writes merged settings.\n\n## Hook/Plugin Definitions\n\n**`ARE_HOOKS: HookDefinition[]`**\nArray defining session hooks for Claude and Gemini. Currently empty (hooks disabled due to issues). Format:\n```typescript\n{ event: 'SessionStart' | 'SessionEnd', filename: string, name: string }\n```\nPreviously included:\n- `'are-check-update.js'` for SessionStart\n- `'are-session-end.js'` for SessionEnd (commented out)\n\n**`ARE_PLUGINS: PluginDefinition[]`**\nArray defining plugins for OpenCode runtime. Format:\n```typescript\n{ srcFilename: string, destFilename: string }\n```\nIncludes:\n- `{ srcFilename: 'opencode-are-check-update.js', destFilename: 'are-check-update.js' }`\n- SessionEnd plugin commented out\n\n**`ARE_PERMISSIONS: string[]`**\nBash command permission patterns for Claude Code auto-allow. Glob patterns:\n- `'Bash(npx agents-reverse-engineer@latest init*)'`\n- `'Bash(npx agents-reverse-engineer@latest discover*)'`\n- `'Bash(npx agents-reverse-engineer@latest generate*)'`\n- `'Bash(npx agents-reverse-engineer@latest update*)'`\n- `'Bash(npx agents-reverse-engineer@latest clean*)'`\n- `'Bash(rm -f .agents-reverse-engineer/progress.log*)'`\n- `'Bash(sleep *)'`\n\n## Settings.json Schema\n\n**`SettingsJson` (Claude Code format)**\n```typescript\n{\n  hooks?: {\n    SessionStart?: HookEvent[];\n    SessionEnd?: HookEvent[];\n  };\n  permissions?: {\n    allow?: string[];\n    deny?: string[];\n  };\n}\n```\nWhere `HookEvent = { hooks: SessionHook[] }` and `SessionHook = { type: 'command', command: string }`.\n\n**`GeminiSettingsJson` (Gemini CLI format)**\n```typescript\n{\n  hooks?: {\n    SessionStart?: GeminiHook[];\n    SessionEnd?: GeminiHook[];\n  };\n}\n```\nWhere `GeminiHook = { name: string, type: 'command', command: string }`.\n\n## Integration Points\n\nImports `resolveInstallPath()` and `getAllRuntimes()` from `./paths.js` for runtime directory resolution. Imports `getClaudeTemplates()`, `getOpenCodeTemplates()`, `getGeminiTemplates()` from `../integration/templates.js` for command template content. Uses `Runtime`, `Location`, `InstallerResult` types from `./types.js`.\n\n## File Writing Pattern\n\nAll writes follow pattern: `ensureDir(fullPath)` → `writeFileSync(fullPath, content, 'utf-8')`. Errors caught individually and pushed to `errors[]` array. Template files written from in-memory template objects, hook files copied from bundled `hooks/dist/` directory. Settings.json merges with existing content, preserving user hooks/permissions.\n\n## Installation Workflow\n\nFor each runtime: resolve base path → get templates → iterate templates (skip if exists and !force, else write) → iterate hooks/plugins (skip if exists and !force, else copy from bundled) → register in settings.json → write VERSION file if files created. OpenCode installs to `plugins/` directory (auto-loaded), Claude/Gemini install to `hooks/` directory (requires settings.json registration).\n### paths.ts\n**Purpose:** paths.ts provides cross-platform path resolution for AI coding assistant runtime installations (Claude, OpenCode, Gem...\n\n**paths.ts provides cross-platform path resolution for AI coding assistant runtime installations (Claude, OpenCode, Gemini) with environment variable overrides and installation detection.**\n\n## Exported Functions\n\n**getAllRuntimes(): Array<Exclude<Runtime, 'all'>>**\nReturns `['claude', 'opencode', 'gemini']` array of concrete runtime identifiers, excluding the `'all'` meta-runtime.\n\n**getRuntimePaths(runtime: Exclude<Runtime, 'all'>): RuntimePaths**\nReturns `RuntimePaths` object containing `global` (absolute path), `local` (relative path), and `settingsFile` (absolute path to settings.json) for specified runtime. Uses `os.homedir()` and `path.join()` for cross-platform compatibility.\n\n**resolveInstallPath(runtime: Exclude<Runtime, 'all'>, location: Location, projectRoot?: string): string**\nReturns absolute installation path. For `location === 'global'`, returns `getRuntimePaths(runtime).global`. For local installations, joins `paths.local` with `projectRoot || process.cwd()`.\n\n**getSettingsPath(runtime: Exclude<Runtime, 'all'>): string**\nReturns absolute path to runtime's settings.json file via `getRuntimePaths(runtime).settingsFile`.\n\n**isRuntimeInstalledLocally(runtime: Exclude<Runtime, 'all'>, projectRoot: string): Promise<boolean>**\nChecks if local config directory exists (e.g., `.claude`, `.opencode`, `.gemini`) via `stat()` and `stats.isDirectory()`. Returns false if stat throws.\n\n**isRuntimeInstalledGlobally(runtime: Exclude<Runtime, 'all'>): Promise<boolean>**\nChecks if global config directory exists via `stat()` on `paths.global`. Returns false if stat throws.\n\n**getInstalledRuntimes(projectRoot: string): Promise<Array<Exclude<Runtime, 'all'>>>**\nIterates through `getAllRuntimes()`, filters via `isRuntimeInstalledLocally()`, returns array of installed runtime identifiers.\n\n## Environment Variable Overrides\n\n**Claude:**\n- `CLAUDE_CONFIG_DIR` overrides default `~/.claude`\n\n**OpenCode:**\n- `OPENCODE_CONFIG_DIR` highest priority\n- Falls back to `XDG_CONFIG_HOME/opencode`\n- Falls back to `~/.config/opencode`\n\n**Gemini:**\n- `GEMINI_CONFIG_DIR` overrides default `~/.gemini`\n\n## Runtime Path Mappings\n\n| Runtime | Global Default | Local Path | Settings File |\n|---------|---------------|------------|---------------|\n| claude | `~/.claude` | `.claude` | `~/.claude/settings.json` |\n| opencode | `~/.config/opencode` | `.opencode` | `~/.config/opencode/settings.json` |\n| gemini | `~/.gemini` | `.gemini` | `~/.gemini/settings.json` |\n\n## Dependencies\n\nUses `node:os` for `homedir()`, `node:path` for `join()`, `node:fs/promises` for `stat()`. Imports `Runtime`, `Location`, `RuntimePaths` types from `./types.js`.\n### prompts.ts\n**Purpose:** Interactive terminal prompt system providing arrow-key selection in TTY mode with numbered fallback for non-interacti...\n\n**Interactive terminal prompt system providing arrow-key selection in TTY mode with numbered fallback for non-interactive environments, enforcing raw mode cleanup via try/finally blocks and process exit handlers to prevent terminal state corruption.**\n\n## Exported Functions\n\n### isInteractive\n\n```typescript\nfunction isInteractive(): boolean\n```\n\nReturns `true` if `process.stdin.isTTY === true`, indicating an interactive terminal capable of arrow-key navigation. Returns `false` for CI environments or piped stdin.\n\n### selectOption\n\n```typescript\nasync function selectOption<T>(\n  prompt: string,\n  options: SelectOption<T>[],\n): Promise<T>\n```\n\nGeneric option selector dispatching to `arrowKeySelect()` in TTY mode or `numberedSelect()` in non-TTY mode. Accepts array of `SelectOption<T>` with `{label: string, value: T}` structure. Returns selected value of type `T`.\n\n### selectRuntime\n\n```typescript\nasync function selectRuntime(mode: 'install' | 'uninstall' = 'install'): Promise<Runtime>\n```\n\nPrompts user to select from `'claude'`, `'opencode'`, `'gemini'`, or `'all'` runtime values. Customizes prompt text based on `mode` parameter: `\"Select runtime to install:\"` or `\"Select runtime to uninstall:\"`. Returns `Runtime` type from `./types.js`.\n\n### selectLocation\n\n```typescript\nasync function selectLocation(mode: 'install' | 'uninstall' = 'install'): Promise<Location>\n```\n\nPrompts user to select between `'global'` (user config directories like `~/.claude`) or `'local'` (project-level directories like `./.claude`). Returns `Location` type from `./types.js`.\n\n### confirmAction\n\n```typescript\nasync function confirmAction(message: string): Promise<boolean>\n```\n\nPrompts user with custom message and Yes/No options. Returns `true` for confirmed, `false` for declined.\n\n## Types\n\n### SelectOption\n\n```typescript\ninterface SelectOption<T> {\n  label: string;\n  value: T;\n}\n```\n\nGeneric option structure for `selectOption()` mapping display labels to typed values.\n\n## Terminal Raw Mode Management\n\n### cleanupRawMode\n\n```typescript\nfunction cleanupRawMode(): void\n```\n\nRestores terminal state by calling `process.stdin.setRawMode(false)` and `process.stdin.pause()` when `rawModeActive` flag is true. Ignores errors during cleanup. Sets `rawModeActive = false`.\n\n### Process Exit Handlers\n\nRegisters `cleanupRawMode()` on `process.on('exit')` and `process.on('SIGINT')` to guarantee terminal state restoration even on Ctrl+C interruption or unexpected termination.\n\n### rawModeActive\n\nModule-level `boolean` state tracker preventing double-cleanup attempts. Set to `true` when `process.stdin.setRawMode(true)` succeeds, reset to `false` by `cleanupRawMode()`.\n\n## Arrow Key Selection (TTY Mode)\n\n### arrowKeySelect\n\n```typescript\nasync function arrowKeySelect<T>(\n  prompt: string,\n  options: SelectOption<T>[],\n): Promise<T>\n```\n\nCaptures keypress events via `readline.emitKeypressEvents(process.stdin)` after enabling raw mode with `process.stdin.setRawMode(true)`. Maintains `selectedIndex` state responding to:\n\n- `key.name === 'up'` → `selectedIndex = Math.max(0, selectedIndex - 1)`\n- `key.name === 'down'` → `selectedIndex = Math.min(options.length - 1, selectedIndex + 1)`\n- `key.name === 'return'` → cleanup and resolve `options[selectedIndex].value`\n- `key.ctrl && key.name === 'c'` → cleanup and `process.exit(0)`\n\nRenders selected option with cyan color via `pc.cyan('> ')` prefix and `pc.cyan(opt.label)`. Re-renders on arrow key by clearing lines with ANSI escape sequences: `\\x1b[${options.length + 1}A` (move cursor up), `\\x1b[2K` (clear line), `\\x1b[1B` (move down).\n\nAlways executes `cleanupRawMode()` via `process.stdin.off('keypress', handleKeypress)` before resolving promise. Wraps setup in try/catch calling `cleanupRawMode()` on error.\n\n## Numbered Selection (Non-TTY Mode)\n\n### numberedSelect\n\n```typescript\nasync function numberedSelect<T>(\n  prompt: string,\n  options: SelectOption<T>[],\n): Promise<T>\n```\n\nPrints numbered list `1. ${opt.label}` and reads integer input via `readline.createInterface()` with `rl.question('Enter number: ')`. Validates input with `parseInt(answer, 10)` rejecting if `isNaN(num) || num < 1 || num > options.length`. Returns `options[num - 1].value`.\n\n## Dependencies\n\n- **node:readline**: Provides `emitKeypressEvents()`, `createInterface()` for stdin handling\n- **picocolors** (`pc`): Terminal color formatting with `pc.bold()`, `pc.cyan()`\n- **./types.js**: Imports `Runtime` and `Location` types for installer-specific prompts\n\n## Critical Design Constraints\n\nRaw mode cleanup enforced via:\n1. Try/finally blocks in `arrowKeySelect()` setup\n2. Global process event handlers for `'exit'` and `'SIGINT'`\n3. Explicit cleanup before promise resolution in `arrowKeySelect()`\n4. Module-level `rawModeActive` flag preventing double-cleanup races\n\nPrevents terminal state corruption where raw mode persists after process termination, breaking user's shell input echoing.\n### types.ts\n**Purpose:** types.ts defines TypeScript interfaces and union types for the npx installer workflow supporting multi-runtime (Claud...\n\n**types.ts defines TypeScript interfaces and union types for the npx installer workflow supporting multi-runtime (Claude/OpenCode/Gemini) installation with global/local targeting, operation result tracking, and path resolution.**\n\n## Exported Types\n\n**Runtime** — Union type: `'claude' | 'opencode' | 'gemini' | 'all'`\n- Maps to platform-specific directories: `~/.claude`, `~/.config/opencode`, `~/.gemini`\n- Special value `'all'` triggers installation to all three runtimes\n\n**Location** — Union type: `'global' | 'local'`\n- `'global'` targets user-level directories (home directory)\n- `'local'` targets project-level directories (working directory)\n\n**InstallerArgs** — Interface for CLI argument parsing\n```typescript\ninterface InstallerArgs {\n  runtime?: Runtime;      // Target platform (optional, prompts if missing)\n  global: boolean;        // Install to ~/.claude, ~/.config/opencode, ~/.gemini\n  local: boolean;         // Install to .claude, .opencode, .gemini\n  uninstall: boolean;     // Remove instead of install\n  force: boolean;         // Overwrite existing files\n  help: boolean;          // Show usage and exit\n  quiet: boolean;         // Suppress banner/info output\n}\n```\nSupports both interactive mode (prompts when `runtime` undefined) and non-interactive mode (all flags set).\n\n**InstallerResult** — Interface for per-runtime operation outcomes\n```typescript\ninterface InstallerResult {\n  success: boolean;                  // Overall operation status\n  runtime: Exclude<Runtime, 'all'>; // Resolved runtime (never 'all')\n  location: Location;                // Target location\n  filesCreated: string[];            // Successfully written file paths\n  filesSkipped: string[];            // Existing files (no --force)\n  errors: string[];                  // Failure messages\n  hookRegistered?: boolean;          // Claude-specific: SessionEnd hook in settings.json\n  versionWritten?: boolean;          // ~/.claude/ARE-VERSION existence\n}\n```\nUsed by installer operations (`src/installer/operations.ts`) for aggregated reporting.\n\n**RuntimePaths** — Interface for resolved directory paths\n```typescript\ninterface RuntimePaths {\n  global: string;        // ~/.claude, ~/.config/opencode, ~/.gemini\n  local: string;         // .claude, .opencode, .gemini\n  settingsFile: string;  // settings.json path for hook registration\n}\n```\nEnvironment overrides respected via `CLAUDE_CONFIG_DIR`, `OPENCODE_CONFIG_DIR`, `GEMINI_CONFIG_DIR` (see `src/installer/paths.ts`).\n\n## Integration Points\n\n- `src/installer/prompts.ts` constructs `InstallerArgs` from interactive prompts\n- `src/installer/paths.ts` returns `RuntimePaths` for each runtime with environment variable overrides\n- `src/installer/operations.ts` produces `InstallerResult[]` arrays from installation operations\n- `src/installer/index.ts` consumes these types for CLI entry point orchestration\n\n## Usage Patterns\n\n**Parallel installation to all runtimes:**\n```typescript\nconst results: InstallerResult[] = installToRuntime('all', 'global', args);\n// Expands to three operations: claude, opencode, gemini\n```\n\n**Location precedence validation:**\n```typescript\nif (args.global && args.local) {\n  throw new Error('Cannot specify both --global and --local');\n}\n```\n\n**Path resolution with environment override:**\n```typescript\nconst paths: RuntimePaths = getRuntimePaths('claude');\n// Uses CLAUDE_CONFIG_DIR if set, else ~/.claude\n```\n### uninstall.ts\n**Purpose:** Uninstall module removes installed ARE command files, hooks, hook registrations from settings.json, and permissions f...\n\n**Uninstall module removes installed ARE command files, hooks, hook registrations from settings.json, and permissions for Claude Code/Gemini CLI/OpenCode runtimes with dry-run support and empty directory cleanup.**\n\n## Exported Functions\n\n**`uninstallFiles(runtime: Runtime, location: Location, dryRun: boolean = false): InstallerResult[]`**\nUninstalls files for one or all runtimes, returning array of `InstallerResult` objects (one per runtime processed). When `runtime === 'all'`, calls `getAllRuntimes()` and maps over each runtime with `uninstallFilesForRuntime()`. Otherwise returns single-element array with result for specified runtime.\n\n**`unregisterHooks(basePath: string, runtime: Exclude<Runtime, 'all'>, dryRun: boolean): boolean`**\nRemoves ARE hook entries from `settings.json` for Claude/Gemini runtimes, dispatching to `unregisterClaudeHooks()` or `unregisterGeminiHooks()`. Returns `true` if any hook was removed. Handles both current hook paths (`node ${runtimeDir}/hooks/${filename}`) and legacy paths (`node hooks/${filename}`).\n\n**`unregisterPermissions(basePath: string, dryRun: boolean): boolean`**\nRemoves ARE Bash command permissions from Claude Code `settings.json` allow list. Filters out all entries matching `ARE_PERMISSIONS` array, cleans up empty `permissions.allow`/`permissions` structures. Returns `true` if any permissions were removed.\n\n**`deleteConfigFolder(location: Location, dryRun: boolean): boolean`**\nDeletes `.agents-reverse-engineer` directory from `process.cwd()` when `location === 'local'`. Uses `rmSync(configPath, { recursive: true, force: true })`. Returns `false` for global installations or missing folders.\n\n## Internal Functions\n\n**`uninstallFilesForRuntime(runtime: Exclude<Runtime, 'all'>, location: Location, dryRun: boolean): InstallerResult`**\nCore uninstallation logic for single runtime. Resolves `basePath` via `resolveInstallPath()`, retrieves templates via `getTemplatesForRuntime()`. Deletes command template files by extracting relative path (`template.path.split('/').slice(1).join('/')`) and joining with `basePath`. For Claude/Gemini, deletes hook files matching `ARE_HOOKS` definitions, calls `unregisterHooks()` and (Claude only) `unregisterPermissions()`. For OpenCode, deletes plugin files matching `ARE_PLUGIN_FILENAMES`. Removes `ARE-VERSION` file. Calls cleanup functions: `cleanupAreSkillDirs()` (Claude), `cleanupLegacyGeminiFiles()` (Gemini), `cleanupEmptyDirs()` (all runtimes). Returns `InstallerResult` with `filesCreated` tracking deleted files, `hookRegistered` repurposed to indicate hook unregistration success.\n\n**`getTemplatesForRuntime(runtime: Exclude<Runtime, 'all'>)`**\nDispatches to `getClaudeTemplates()`, `getOpenCodeTemplates()`, or `getGeminiTemplates()` based on runtime parameter.\n\n**`getHookPatterns(runtimeDir: string): string[]`**\nBuilds array of hook command patterns for matching in `settings.json`. Iterates `ARE_HOOKS`, generates current format (`node ${runtimeDir}/hooks/${filename}`) and legacy format (`node hooks/${filename}`) for each hook.\n\n**`unregisterClaudeHooks(basePath: string, dryRun: boolean): boolean`**\nLoads `settings.json` as `SettingsJson` type. Filters `settings.hooks.SessionStart` and `settings.hooks.SessionEnd` arrays, removing events where `event.hooks` contains any command matching `getHookPatterns('.claude')`. Deletes empty event arrays and empty `hooks` object. Writes updated JSON with 2-space indentation unless `dryRun`. Returns `true` if any hook removed.\n\n**`unregisterGeminiHooks(basePath: string, dryRun: boolean): boolean`**\nSimilar to Claude variant but operates on `GeminiSettingsJson` with simplified hook structure (direct `GeminiHook` objects vs nested `HookEvent` wrappers). Filters by `h.command` matching `getHookPatterns('.gemini')`.\n\n**`cleanupAreSkillDirs(skillsDir: string): void`**\nIterates entries in Claude skills directory, calling `cleanupEmptyDirs()` on subdirectories whose names start with `'are-'`.\n\n**`cleanupEmptyDirs(dirPath: string): void`**\nRecursively removes empty directories via `rmdirSync()`, then attempts parent directory cleanup. Stops recursion at runtime root directories (basenames: `.claude`, `.opencode`, `.gemini`, `.config`).\n\n**`cleanupLegacyGeminiFiles(commandsDir: string): void`**\nRemoves legacy files from old Gemini installations: `are-*.md` files (pre-TOML format), `.toml` files from nested `are/` subdirectory (pre-flat structure). Calls `cleanupEmptyDirs()` on `are/` subdirectory after deletion.\n\n## Hook and Permission Definitions\n\n**`ARE_HOOKS: HookDefinition[]`**\nArray defining two hooks: `{ event: 'SessionStart', filename: 'are-check-update.js' }`, `{ event: 'SessionEnd', filename: 'are-session-end.js' }`. Must match `operations.ts` definitions.\n\n**`ARE_PLUGIN_FILENAMES: string[]`**\nArray of OpenCode plugin filenames: `['are-check-update.js', 'are-session-end.js']`. Must match `operations.ts`.\n\n**`ARE_PERMISSIONS: string[]`**\nArray of five Bash permission strings for Claude Code: patterns like `'Bash(npx agents-reverse-engineer@latest init*)'` covering init/discover/generate/update/clean commands. Must match `operations.ts`.\n\n**`CONFIG_DIR: string`**\nConstant `'.agents-reverse-engineer'` for local configuration directory name (matches `config/loader.ts`).\n\n## Type Interfaces\n\n**`SessionHook`**\nClaude hook entry: `{ type: 'command', command: string }`.\n\n**`HookEvent`**\nClaude hook event wrapper: `{ hooks: SessionHook[] }`.\n\n**`SettingsJson`**\nClaude settings schema: `hooks?: { SessionStart?: HookEvent[], SessionEnd?: HookEvent[] }`, `permissions?: { allow?: string[], deny?: string[] }`.\n\n**`GeminiHook`**\nGemini hook entry: `{ name: string, type: 'command', command: string }`.\n\n**`GeminiSettingsJson`**\nGemini settings schema: `hooks?: { SessionStart?: GeminiHook[], SessionEnd?: GeminiHook[] }`.\n\n**`HookDefinition`**\nHook metadata: `{ event: 'SessionStart' | 'SessionEnd', filename: string }`.\n\n## Integration Points\n\nImports `Runtime`, `Location`, `InstallerResult` from `./types.js`, `resolveInstallPath()`, `getAllRuntimes()`, `getRuntimePaths()` from `./paths.js`, template getters from `../integration/templates.js`. Mirrors installation logic from `operations.ts` for clean reversal. Uses Node.js `fs` module (`existsSync`, `unlinkSync`, `readFileSync`, `writeFileSync`, `readdirSync`, `rmdirSync`, `rmSync`) and `path` module for filesystem operations.\n\n## Behavioral Contracts\n\nHook command pattern format: `node ${runtimeDir}/hooks/${filename}` (current), `node hooks/${filename}` (legacy). Permission pattern format: `Bash(npx agents-reverse-engineer@latest <command>*)`. Settings JSON written with `JSON.stringify(settings, null, 2)` for 2-space indentation. Empty array/object cleanup order: delete empty event arrays first, then empty `hooks`/`permissions` parent objects. Runtime root basenames for recursion stopping: `.claude`, `.opencode`, `.gemini`, `.config`. Legacy Gemini file patterns: `are-*.md` (commands directory), `*.toml` (nested `are/` subdirectory).\n\n## Import Map (verified — use these exact paths)\n\nbanner.ts:\n  ../version.js → getVersion\n\noperations.ts:\n  ../integration/templates.js → getClaudeTemplates, getOpenCodeTemplates, getGeminiTemplates\n\nuninstall.ts:\n  ../integration/templates.js → getClaudeTemplates, getOpenCodeTemplates, getGeminiTemplates\n\n## Project Directory Structure\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>",
      "systemPrompt": "You are generating an AGENTS.md file — a directory-level overview for AI coding assistants.\n\nCRITICAL: Output ONLY the raw markdown content. No code fences, no preamble, no explanations, no conversational text. Your entire response IS the AGENTS.md file content.\n\nOUTPUT FORMAT:\n- First line MUST be exactly: <!-- Generated by agents-reverse-engineer -->\n- Use a # heading with the directory name\n- Write a one-paragraph purpose statement for the directory\nusage\nADAPTIVE SECTIONS:\nAnalyze the directory contents and choose the most relevant sections. Do NOT use a fixed template. Instead, select sections that best document this specific directory for an AI that needs to reconstruct or extend the project.\n\nConsider these section types (choose what applies):\n- **Contents**: Group files by purpose/category under ## headings. For each file: markdown link [filename](./filename) and a one-line description.\n- **Subdirectories**: If subdirectories exist, list them with links [dirname/](./dirname/) and brief summaries.\n- **Architecture / Data Flow**: If files form a pipeline, request/response chain, or layered architecture, document it.\n- **Stack**: If this is a package root (has package.json, Cargo.toml, go.mod, etc.), document the technology stack, key scripts, and entry points.\n- **Structure**: If the directory layout follows a convention (feature-sliced, domain-driven, MVC, etc.), document it.\n- **Patterns**: If files share recurring design patterns (factory, strategy, middleware, barrel re-export), name and document them.\n- **Configuration**: If the directory contains config files, schemas, or environment definitions, document the config surface area.\n- **API Surface**: If the directory exports a public API (barrel index, route definitions, SDK), document the interface contract.\n- **File Relationships**: How files collaborate, depend on each other, or share state.\n- **Behavioral Contracts**: If files contain regex patterns, format specifications, magic constants, or template strings that define observable behavior, collect them in a dedicated section. Preserve verbatim patterns from file summaries — do NOT paraphrase regex into prose. This section is MANDATORY when file summaries contain behavioral artifacts.\n\nChoose any relevant sections or create your own based on the directory contents. The goal is to provide a comprehensive overview that captures the essence of the directory's role in the project and how its files work together, with a focus on what an AI coding assistant would need to know to effectively interact with this code.\n\nSCOPE:\n- AGENTS.md is a NAVIGATIONAL INDEX — help an AI find the right file quickly\n- Focus on: what each file does, how files relate, directory-level patterns\n- Do NOT reproduce full architecture sections — those belong in the root CLAUDE.md\n\nPATH ACCURACY (MANDATORY):\n- When referencing files or modules outside this directory, use ONLY paths from the \"Import Map\" section\n- Do NOT invent, rename, or guess module paths — if a path isn't in the Import Map, don't reference it\n- Use the exact directory names from \"Project Directory Structure\" — do NOT rename directories\n  (e.g., if the directory is called \"cli\", write \"src/cli/\", NOT \"src/commands/\")\n- Cross-module references must use the specifier format from actual import statements\n  (e.g., \"../generation/writers/sum.js\", NOT \"../fs/sum-file.js\")\n- If you are unsure about a path, omit the cross-reference rather than guessing\n\nCONSISTENCY (MANDATORY):\n- Do not contradict yourself within the same document\n- If you describe a technique (e.g., \"regex-based\"), do not call it something else later (e.g., \"AST-based\")\n- When stating version numbers, engines, or config fields, use ONLY values present in the file summaries\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this directory\", \"this module\", \"provides\", \"responsible for\", \"is used to\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Per-file descriptions in Contents sections: 1-2 sentences maximum. Reference key symbols but do not reproduce full summaries.\n- Behavioral contracts (regex patterns, format specs, constants) belong in a separate Behavioral Contracts section, not in per-file descriptions.\n- Subdirectory descriptions: 1-2 sentences maximum. Capture the directory's role, not its full contents.\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- Key exported symbols from file summaries MUST appear in the directory overview\n- Preserve exact casing of identifiers\n\nUSER NOTES:\n- If \"User Notes\" are provided in the prompt, they contain user-defined instructions that will be automatically prepended to your output\n- Do NOT repeat or paraphrase user notes in your generated content — they are included separately\n- You may reference information from user notes for context",
      "response": "<!-- Generated by agents-reverse-engineer -->\n\n# src/installer\n\nnpx installation orchestrator for deploying ARE command templates and session hooks into AI coding assistant runtime directories (`~/.claude`, `~/.config/opencode`, `~/.gemini`), with interactive prompts for missing parameters, parallel multi-runtime support, hook registration in settings.json, and permission preauthorization for Claude Code.\n\n## Contents\n\n### Core Installation Logic\n\n**[operations.ts](./operations.ts)** — Writes command templates from `src/integration/templates.ts` to runtime directories, copies bundled hook files from `hooks/dist/`, registers hooks in `settings.json` via `registerClaudeHooks()`/`registerGeminiHooks()`, writes `ARE-VERSION` file, adds Bash permission patterns to Claude Code allow list. Exports `installFiles()` fan-out across runtimes, `verifyInstallation()` checking file existence via `existsSync()`, `getPackageVersion()` reading from package.json, `formatInstallResult()` for human-readable output.\n\n**[uninstall.ts](./uninstall.ts)** — Removes command templates, hooks, hook registrations from `settings.json`, Bash permission entries from allow list, and `ARE-VERSION` file. Calls `cleanupAreSkillDirs()` deleting empty `are-*` skill directories for Claude, `cleanupLegacyGeminiFiles()` removing pre-TOML `*.md` files, `cleanupEmptyDirs()` recursively deleting empty parent directories stopping at runtime roots. Exports `uninstallFiles()`, `deleteConfigFolder()` for `.agents-reverse-engineer` removal.\n\n**[paths.ts](./paths.ts)** — Resolves runtime installation paths with environment variable overrides (`CLAUDE_CONFIG_DIR`, `OPENCODE_CONFIG_DIR`, `GEMINI_CONFIG_DIR`). Exports `getRuntimePaths()` returning `RuntimePaths` objects with `global`/`local`/`settingsFile` fields, `resolveInstallPath()` joining runtime paths with project root, `isRuntimeInstalledLocally()`/`isRuntimeInstalledGlobally()` existence checks via `stat()`.\n\n### Workflow Orchestration\n\n**[index.ts](./index.ts)** — Parses CLI args via `parseInstallerArgs()` supporting `-g`/`--global`, `-l`/`--local`, `--runtime`, `--force`, `-u`/`--uninstall`, `-q`/`--quiet`, dispatches to `runInstall()`/`runUninstall()` after resolving runtime and location via `determineRuntimes()`/`determineLocation()` with interactive prompt fallback. Calls `verifyInstallation()` checking created file existence, formats results via `displayInstallResults()`/`displayUninstallResults()` showing file counts, hook registration status, and `showNextSteps()` with ARE command list. Exits with code 1 in non-interactive mode when required flags missing.\n\n**[prompts.ts](./prompts.ts)** — Arrow-key selection in TTY mode via `arrowKeySelect()` with `readline.emitKeypressEvents()`, numbered fallback via `numberedSelect()` for non-TTY. Exports `selectRuntime()` prompting for `'claude'`/`'opencode'`/`'gemini'`/`'all'`, `selectLocation()` for `'global'`/`'local'`, `confirmAction()` for yes/no prompts, `isInteractive()` checking `process.stdin.isTTY`. Enforces raw mode cleanup via `cleanupRawMode()` in try/finally blocks and process exit handlers (`'exit'`, `'SIGINT'`) preventing terminal state corruption.\n\n### Display Formatting\n\n**[banner.ts](./banner.ts)** — Styled terminal output with ASCII art \"ARE\" logo in green via `pc.green()`, version footer via `getVersion()`, usage instructions with `pc.bold()` section headers covering CLI options and example invocations. Exports `displayBanner()`, `showHelp()`, `showNextSteps()` listing ARE commands (`/are-help`, `/are-init`, `/are-discover`, `/are-generate`, `/are-update`, `/are-specify`, `/are-clean`) in cyan, message helpers `showSuccess()`/`showError()`/`showWarning()`/`showInfo()` with prefixed symbols.\n\n### Type Definitions\n\n**[types.ts](./types.ts)** — Defines `Runtime = 'claude' | 'opencode' | 'gemini' | 'all'`, `Location = 'global' | 'local'`, `InstallerArgs` interface with boolean flags and optional runtime field, `InstallerResult` interface tracking `filesCreated[]`, `filesSkipped[]`, `errors[]`, `hookRegistered`, `versionWritten`, `RuntimePaths` interface with `global`/`local`/`settingsFile` path strings.\n\n## Installation Flow\n\n1. **Argument Parsing** — `parseInstallerArgs()` extracts flags, validates `--runtime` against `['claude', 'opencode', 'gemini', 'all']`\n2. **Interactive Prompts** — `determineLocation()` calls `selectLocation()` if both/neither `-g`/`-l` set, `determineRuntimes()` calls `selectRuntime()` if `--runtime` missing\n3. **Path Resolution** — `resolveInstallPath()` combines `getRuntimePaths(runtime)` with project root for local installs, uses global path for `-g`\n4. **File Operations** — `installFilesForRuntime()` calls `getTemplatesForRuntime()` from `src/integration/templates.ts`, writes templates via `ensureDir()` → `writeFileSync()`, copies hooks from `hooks/dist/` via `readBundledHook()` → `writeFileSync()`\n5. **Hook Registration** — `registerHooks()` dispatches to `registerClaudeHooks()` or `registerGeminiHooks()`, merges hooks into `settings.json` under `hooks.SessionStart`/`hooks.SessionEnd`, writes ARE Bash permission patterns via `registerPermissions()` for Claude only\n6. **Verification** — `verifyInstallation()` checks all file paths via `existsSync()`, reports `missing[]` array for absent files\n7. **Display Results** — `displayInstallResults()` accumulates `totalCreated`/`totalSkipped`/`hooksRegistered` counters, calls `showNextSteps()` with ARE command list unless `--quiet` flag set\n\n## Uninstallation Flow\n\n1. **File Deletion** — `uninstallFilesForRuntime()` reads templates for path extraction, deletes command files and hooks via `unlinkSync()`, removes `ARE-VERSION` file\n2. **Hook Cleanup** — `unregisterHooks()` filters `settings.json` event arrays removing hooks matching `getHookPatterns()` for current (`node ${runtimeDir}/hooks/${filename}`) and legacy (`node hooks/${filename}`) formats, deletes empty event arrays and empty `hooks` object\n3. **Permission Cleanup** — `unregisterPermissions()` filters `permissions.allow` removing all `ARE_PERMISSIONS` entries, cleans up empty `permissions.allow`/`permissions` structures\n4. **Directory Cleanup** — `cleanupAreSkillDirs()` removes empty `are-*` skill subdirectories for Claude, `cleanupLegacyGeminiFiles()` deletes pre-TOML `*.md` files and nested `are/*.toml` files, `cleanupEmptyDirs()` recursively deletes empty parents stopping at runtime roots (`.claude`, `.opencode`, `.gemini`, `.config`)\n5. **Config Removal** — `deleteConfigFolder()` removes `.agents-reverse-engineer` directory for local installs via `rmSync({ recursive: true, force: true })`\n\n## Runtime-Specific Behaviors\n\n**Claude Code:**\n- Installs to `~/.claude` (global) or `.claude` (local), overridable via `CLAUDE_CONFIG_DIR`\n- Registers hooks in nested format: `settings.hooks[event] = [{ hooks: [{ type: 'command', command: string }] }]`\n- Adds five Bash permission patterns to `settings.permissions.allow`: `Bash(npx agents-reverse-engineer@latest init*)`, `discover*`, `generate*`, `update*`, `clean*`\n- Cleans up skill directories matching `are-*` prefix during uninstall\n\n**OpenCode:**\n- Installs to `~/.config/opencode` (global) or `.opencode` (local), respects `OPENCODE_CONFIG_DIR` → `XDG_CONFIG_HOME/opencode` → default fallback chain\n- Copies plugins to `plugins/` directory (auto-loaded by OpenCode runtime)\n- No settings.json registration (plugin system auto-discovers)\n- Supports plugin filenames: `are-check-update.js`, `are-session-end.js`\n\n**Gemini:**\n- Installs to `~/.gemini` (global) or `.gemini` (local), overridable via `GEMINI_CONFIG_DIR`\n- Registers hooks in flat format: `settings.hooks[event] = [{ name: string, type: 'command', command: string }]`\n- Cleans up legacy files during uninstall: `are-*.md` (pre-TOML commands), `are/*.toml` (pre-flat structure)\n\n## Behavioral Contracts\n\n**Hook Command Pattern:**\n```\nCurrent: node ${runtimeDir}/hooks/${filename}\nLegacy:  node hooks/${filename}\n```\n\n**Permission Pattern:**\n```\nBash(npx agents-reverse-engineer@latest <command>*)\n```\n\n**Settings JSON Indentation:**\n```javascript\nJSON.stringify(settings, null, 2)  // 2-space indent\n```\n\n**Runtime Root Basenames (recursion stop):**\n```\n.claude, .opencode, .gemini, .config\n```\n\n**Hook File Sources:**\n```\nhooks/dist/are-check-update.js\nhooks/dist/are-session-end.js\nhooks/dist/opencode-are-check-update.js\nhooks/dist/opencode-are-session-end.js\n```\n\n**Version File Location:**\n```\n${basePath}/ARE-VERSION\n```",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 2503,
      "cacheReadTokens": 17477,
      "cacheCreationTokens": 18442,
      "latencyMs": 79517,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": []
    },
    {
      "timestamp": "2026-02-09T16:13:38.984Z",
      "prompt": "Generate AGENTS.md for directory: \"src/generation\" (generation)\n\n## File Summaries (5 files)\n\n### collector.ts\n**Purpose:** collector.ts recursively discovers and aggregates all `AGENTS.md` files from a project tree into a sorted array of re...\n\n**collector.ts recursively discovers and aggregates all `AGENTS.md` files from a project tree into a sorted array of relative paths and content, skipping vendor/build/meta directories.**\n\n## Exported Types\n\n**`AgentsDocs`**: Type alias for `Array<{ relativePath: string; content: string }>` representing collected documentation files with project-relative paths.\n\n## Exported Functions\n\n**`collectAgentsDocs(projectRoot: string): Promise<AgentsDocs>`**: Recursively walks directory tree starting at `projectRoot`, collects all `AGENTS.md` files (ignoring unreadable directories/files), returns alphabetically sorted array by `relativePath`. Uses `readdir()` with `withFileTypes: true` for efficient type checking, `readFile()` for UTF-8 content loading, `path.relative()` for path normalization, and `Array.sort()` with `localeCompare()` for lexicographic ordering.\n\n## Directory Exclusion\n\n**`SKIP_DIRS`**: `Set<string>` containing 13 directory names to exclude during traversal: `'node_modules'`, `'.git'`, `'.agents-reverse-engineer'`, `'vendor'`, `'dist'`, `'build'`, `'__pycache__'`, `'.next'`, `'venv'`, `'.venv'`, `'target'`, `'.cargo'`, `'.gradle'`. Prevents recursion into build artifacts, version control, Python/Rust/Gradle caches, and ARE's own metadata directory.\n\n## Implementation Pattern\n\n**Recursive walker**: Inner `async walk(currentDir: string)` function implements depth-first traversal with try-catch around `readdir()` and `readFile()` to gracefully handle permission errors or inaccessible paths (continues execution without throwing). Checks `entry.isDirectory()` and `!SKIP_DIRS.has(entry.name)` before recursion. Matches filename via exact string equality `entry.name === 'AGENTS.md'`.\n\n## Integration Context\n\nUsed by Phase 3 root document synthesis in `src/generation/orchestrator.ts` to aggregate all directory-level documentation before generating platform-specific integration files (`CLAUDE.md`, `GEMINI.md`, `OPENCODE.md`). Consumed by `src/specify/index.ts` for project specification synthesis across entire documentation corpus.\n### complexity.ts\n**Purpose:** complexity.ts computes codebase complexity metrics (file count, directory depth, unique directories) from discovered ...\n\n**complexity.ts computes codebase complexity metrics (file count, directory depth, unique directories) from discovered file paths for documentation generation planning.**\n\n## Exported Interface\n\n**ComplexityMetrics** - Aggregated complexity measurements\n- `fileCount: number` - Total source files discovered\n- `directoryDepth: number` - Maximum nested directory depth\n- `files: string[]` - Complete list of source file absolute paths\n- `directories: Set<string>` - Unique directory paths extracted from files\n\n## Exported Function\n\n**analyzeComplexity(files: string[], projectRoot: string): ComplexityMetrics** - Orchestrates complexity computation by invoking `calculateDirectoryDepth()` and `extractDirectories()` on input file array, returns populated ComplexityMetrics object.\n\n## Internal Functions\n\n**calculateDirectoryDepth(files: string[], projectRoot: string): number** - Computes maximum depth by calling `path.relative(projectRoot, file)` for each file, splitting on `path.sep`, subtracting 1 (excludes filename from depth count), tracking `Math.max()` across all files.\n\n**extractDirectories(files: string[]): Set<string>** - Walks parent chain via `path.dirname()` loop for each file, terminating when `dir === '.'` or `parent === dir` (root reached), accumulates unique directories in Set to deduplicate shared ancestor paths.\n\n## Integration Points\n\nUsed by `src/generation/orchestrator.ts` or `src/cli/discover.ts` to compute metrics for GENERATION-PLAN.md phase breakdown and concurrency decisions. The `directoryDepth` metric influences post-order directory aggregation sorting (Phase 2 processes deepest directories first). The `directories` Set drives `AGENTS.md` generation target list.\n\n## Algorithm Details\n\n**Depth calculation**: Relative path splitting produces array like `['src', 'generation', 'complexity.ts']`, length 3 minus 1 equals depth 2 for the `src/generation/` directory.\n\n**Directory extraction**: For path `/home/pascal/wks/are/src/generation/complexity.ts`, extracts `/home/pascal/wks/are/src/generation`, `/home/pascal/wks/are/src`, `/home/pascal/wks/are`, stops before root `/home` when `parent === dir`.\n### executor.ts\n**Purpose:** Transforms GenerationPlan into ExecutionPlan with dependency-ordered tasks for the three-phase documentation pipeline...\n\n**Transforms GenerationPlan into ExecutionPlan with dependency-ordered tasks for the three-phase documentation pipeline, enforcing post-order directory traversal via depth-based sorting.**\n\n## Exported Types\n\n**ExecutionTask** — Individual work unit for AI processing with dependency tracking\n- `id: string` — Unique identifier (format: `file:<path>` | `dir:<path>` | `root:<filename>`)\n- `type: 'file' | 'directory' | 'root-doc'` — Task category determining pipeline phase\n- `path: string` — Relative path from project root\n- `absolutePath: string` — Resolved filesystem path\n- `systemPrompt: string` — AI system context (placeholder for dir/root, actual for file)\n- `userPrompt: string` — AI generation instructions (placeholder for dir/root, actual for file)\n- `dependencies: string[]` — Task IDs that must complete before this task can execute\n- `outputPath: string` — Target file for generated content (`.sum`, `AGENTS.md`, `CLAUDE.md`)\n- `metadata: { directoryFiles?: string[]; depth?: number; packageRoot?: string }` — Task-specific context\n\n**ExecutionPlan** — Complete dependency graph for pipeline execution\n- `projectRoot: string` — Absolute project base path\n- `tasks: ExecutionTask[]` — All tasks in dependency-satisfying order\n- `fileTasks: ExecutionTask[]` — Phase 1 tasks (parallel-eligible)\n- `directoryTasks: ExecutionTask[]` — Phase 2 tasks (post-order sorted by depth descending)\n- `rootTasks: ExecutionTask[]` — Phase 3 tasks (sequential, depends on all directories)\n- `directoryFileMap: Record<string, string[]>` — Directory to file paths mapping for completion tracking\n- `projectStructure?: string` — Compact file tree for directory prompt context\n\n## Core Functions\n\n**buildExecutionPlan(plan: GenerationPlan, projectRoot: string): ExecutionPlan** — Constructs execution plan with post-order directory traversal\n\nCreates three task categories:\n1. **fileTasks**: Maps `plan.tasks` with `type === 'file'` to ExecutionTask, generates IDs via `file:${task.filePath}`, outputs to `${absolutePath}.sum`, zero dependencies, sorts by directory depth descending via `getDirectoryDepth(path.dirname(a.path))` comparison\n2. **directoryTasks**: Builds from `directoryFileMap` entries sorted by `getDirectoryDepth(dirB) - getDirectoryDepth(dirA)`, assigns `dependencies` as `files.map(f => 'file:${f}')`, placeholder prompts (\"Built at execution time by buildDirectoryPrompt()\"), outputs to `${dirAbsPath}/AGENTS.md`, stores `metadata.depth` and `metadata.directoryFiles`\n3. **rootTasks**: Single task `root:CLAUDE.md` depending on all directory task IDs, placeholder prompts (\"Built at runtime by buildRootPrompt()\"), outputs to `${projectRoot}/CLAUDE.md`\n\n**isDirectoryComplete(dirPath: string, expectedFiles: string[], projectRoot: string): Promise<{ complete: boolean; missing: string[] }>** — Verifies all files in directory have `.sum` artifacts via `sumFileExists()`\n\nIterates `expectedFiles`, resolves each via `path.join(projectRoot, relativePath)`, checks `sumFileExists(absolutePath)`, accumulates missing paths, returns completion status and missing array.\n\n**getReadyDirectories(executionPlan: ExecutionPlan): Promise<string[]>** — Filters directories with all `.sum` files present\n\nIterates `executionPlan.directoryFileMap` entries, calls `isDirectoryComplete(dir, files, executionPlan.projectRoot)`, collects directories where `complete === true`.\n\n**formatExecutionPlanAsMarkdown(plan: ExecutionPlan): string** — Generates GENERATION-PLAN.md with post-order task breakdown\n\nProduces markdown with:\n- Header: `# Documentation Generation Plan` with ISO date from `new Date().toISOString().split('T')[0]` and `plan.projectRoot`\n- Summary: Task counts (`plan.tasks.length`, `plan.fileTasks.length`, `plan.directoryTasks.length`, `plan.rootTasks.length`) and traversal strategy (\"Post-order (children before parents)\")\n- Phase 1: Groups `plan.fileTasks` by directory via `task.path.lastIndexOf('/')`, outputs sections `### Depth ${depth}: ${dir}/ (${files.length} files)` using `plan.directoryTasks` order, checkbox items `- [ ] \\`${file}\\``\n- Phase 2: Groups `plan.directoryTasks` by `metadata.depth`, sorts depths descending, outputs `### Depth ${depth}` sections with `- [ ] \\`${dir}/AGENTS.md\\`` items\n- Phase 3: Hardcoded `- [ ] \\`CLAUDE.md\\`` entry\n\n## Dependency Relationships\n\n**Input:** GenerationPlan from `src/generation/orchestrator.ts` containing `tasks[]` with `type/filePath/systemPrompt/userPrompt`, `files[]` with `relativePath`\n\n**Output:** ExecutionPlan consumed by `src/orchestration/runner.ts` for phase execution\n\n**External dependencies:**\n- `sumFileExists()` from `src/generation/writers/sum.ts` — Checks `.sum` file existence for completion tracking\n- `path.join()`, `path.dirname()`, `path.sep` — Path manipulation for task ID generation and depth calculation\n\n## Post-Order Traversal Strategy\n\n**getDirectoryDepth(dir: string): number** — Calculates directory nesting level\n\nReturns 0 for root `\".\"`, otherwise counts path segments via `dir.split(path.sep).length`. Examples: `\"src\"` → 1, `\"src/cli\"` → 2, `\"src/generation/prompts\"` → 3.\n\nSorting pattern: `(a, b) => getDirectoryDepth(dirB) - getDirectoryDepth(dirA)` ensures deepest directories process first, guaranteeing child `AGENTS.md` exist before parent synthesis (Phase 2 requirement documented in `src/generation/orchestrator.ts`).\n\n## Task ID Format\n\n**File tasks:** `file:<relativePath>` (e.g., `file:src/cli/index.ts`)\n**Directory tasks:** `dir:<relativePath>` (e.g., `dir:src/cli`)\n**Root tasks:** `root:<filename>` (e.g., `root:CLAUDE.md`)\n\nDependency arrays reference these IDs exactly: `directoryTask.dependencies = files.map(f => 'file:${f}')`, `rootTask.dependencies = allDirTaskIds`.\n\n## Placeholder Prompts\n\nDirectory and root tasks store placeholder strings for `systemPrompt`/`userPrompt`:\n- Directory: `\"Built at execution time by buildDirectoryPrompt()\"` / `\"Directory \"${dir}\" — ${files.length} files. Prompt populated from .sum files at runtime.\"`\n- Root: `\"Built at runtime by buildRootPrompt()\"` / `\"Root document — prompt populated from AGENTS.md files at runtime.\"`\n\nActual prompts constructed in `src/orchestration/runner.ts` Phase 2/3 via `buildDirectoryPrompt()` and `buildRootPrompt()` consuming `.sum` and `AGENTS.md` files. Placeholders exist only for plan display via `formatExecutionPlanAsMarkdown()` and dependency graph structure.\n### orchestrator.ts\n**Purpose:** GenerationOrchestrator coordinates three-phase documentation generation: prepares files from DiscoveryResult, creates...\n\n**GenerationOrchestrator coordinates three-phase documentation generation: prepares files from DiscoveryResult, creates AnalysisTask lists for concurrent file analysis and post-file directory aggregation, computes ComplexityMetrics, and emits trace events for plan lifecycle.**\n\n## Exported Types\n\n**PreparedFile** interface contains `filePath: string` (absolute), `relativePath: string` (from project root), `content: string` (file text).\n\n**AnalysisTask** discriminated union via `type: 'file' | 'directory'`:\n- `filePath: string` (relative path for files, directory path for directories)\n- `systemPrompt?: string` and `userPrompt?: string` (populated for file tasks via buildFilePrompt, built at execution time for directory tasks via buildDirectoryPrompt)\n- `directoryInfo?: { sumFiles: string[], fileCount: number }` (populated only for directory tasks)\n\n**GenerationPlan** interface aggregates:\n- `files: PreparedFile[]` (all prepared files with content initially populated, then cleared)\n- `tasks: AnalysisTask[]` (file tasks followed by directory tasks)\n- `complexity: ComplexityMetrics` (computed via analyzeComplexity)\n- `projectStructure?: string` (compact directory tree listing)\n\n## Core Class\n\n**GenerationOrchestrator** constructor accepts `config: Config`, `projectRoot: string`, `options?: { tracer?: ITraceWriter, debug?: boolean }`. Stores as private fields `this.config`, `this.projectRoot`, `this.tracer`, `this.debug`.\n\n**prepareFiles(discoveryResult: DiscoveryResult): Promise<PreparedFile[]>** reads content via `readFile(filePath, 'utf-8')`, computes `relativePath` via `path.relative(this.projectRoot, filePath)`, silently skips unreadable files (permission errors), returns array of PreparedFile objects with populated content.\n\n**createFileTasks(files: PreparedFile[], projectStructure?: string): AnalysisTask[]** invokes `buildFilePrompt({ filePath, content, projectPlan: projectStructure }, this.debug)` for each file, returns AnalysisTask array with `type: 'file'`, `filePath: relativePath`, `systemPrompt: prompt.system`, `userPrompt: prompt.user`.\n\n**createDirectoryTasks(files: PreparedFile[]): AnalysisTask[]** groups files via `Map<string, PreparedFile[]>` keyed by `path.dirname(relativePath)`, generates one AnalysisTask per directory with `type: 'directory'`, `filePath: dir || '.'`, `directoryInfo: { sumFiles: string[], fileCount: number }` where sumFiles contains `${relativePath}.sum` paths. Prompts intentionally omitted (built at execution time).\n\n**createPlan(discoveryResult: DiscoveryResult): Promise<GenerationPlan>** orchestrates full planning:\n1. Emits `phase:start` trace event with `phase: 'plan-creation'`, `taskCount: discoveryResult.files.length`, `concurrency: 1`\n2. Calls `prepareFiles(discoveryResult)` to read file content\n3. Calls `analyzeComplexity(files.map(f => f.filePath), projectRoot)` from `./complexity.js`\n4. Calls `buildProjectStructure(files)` to generate compact directory tree string\n5. Calls `createFileTasks(files, projectStructure)` to generate file analysis tasks\n6. Calls `createDirectoryTasks(files)` to generate directory aggregation tasks\n7. Concatenates `[...fileTasks, ...dirTasks]` into single tasks array\n8. Clears `content` field from PreparedFile objects via `(file as { content: string }).content = ''` to free memory (content already embedded in prompts)\n9. Emits `plan:created` trace event with `planType: 'generate'`, `fileCount`, `taskCount: tasks.length + 1` (+1 for root CLAUDE.md task added by buildExecutionPlan)\n10. Emits `phase:end` trace event with `phase: 'plan-creation'`, `durationMs`, `tasksCompleted: 1`, `tasksFailed: 0`\n11. Returns GenerationPlan object\n\n## Factory Function\n\n**createOrchestrator(config: Config, projectRoot: string, options?: { tracer?: ITraceWriter, debug?: boolean }): GenerationOrchestrator** constructs and returns new GenerationOrchestrator instance with provided parameters.\n\n## Private Methods\n\n**buildProjectStructure(files: PreparedFile[]): string** groups files via `Map<string, string[]>` keyed by `path.dirname(relativePath) || '.'`, sorts directories alphabetically, formats output as:\n```\ndir1/\n  file1.ts\n  file2.ts\ndir2/\n  file3.ts\n```\nReturns multi-line string for LLM bird's-eye context.\n\n## Integration Points\n\nImports `buildFilePrompt` from `./prompts/index.js` (called per-file to generate system/user prompts with content and project structure).\n\nImports `analyzeComplexity` from `./complexity.js` (returns ComplexityMetrics with directoryDepth and other metrics).\n\nImports `Config` from `../config/schema.js` (Zod-validated configuration object).\n\nImports `DiscoveryResult` from `../types/index.js` (contains `files: string[]` array from file discovery phase).\n\nImports `ITraceWriter` from `../orchestration/trace.js` (optional tracer for NDJSON event emission with `.emit()` method).\n\n## Memory Management\n\nAfter createFileTasks embeds file content into task prompts, createPlan clears PreparedFile.content fields via cast `(file as { content: string }).content = ''` to release memory. Comment notes \"The runner re-reads files from disk\" during execution phase.\n\n## Task Ordering\n\nTasks array contains file analysis tasks first (concurrent execution), followed by directory aggregation tasks (post-file execution). Directory tasks intentionally omit systemPrompt/userPrompt fields — buildDirectoryPrompt constructs these at execution time after .sum files exist.\n\n## Trace Events\n\nEmits three trace event types via `this.tracer?.emit()`:\n- `phase:start` with `phase: 'plan-creation'`, `taskCount`, `concurrency: 1`\n- `plan:created` with `planType: 'generate'`, `fileCount`, `taskCount`\n- `phase:end` with `phase: 'plan-creation'`, `durationMs`, `tasksCompleted: 1`, `tasksFailed: 0`\n\nDuration computed via `process.hrtime.bigint()` delta converted to milliseconds via `Number(delta) / 1_000_000`.\n\n## Debug Logging\n\nWhen `this.debug === true`, emits stderr messages via `console.error(pc.dim())`:\n- `\"[debug] Preparing files: reading and detecting types...\"`\n- `\"[debug] Analyzing complexity...\"`\n- `\"[debug] Complexity analysis: depth=${complexity.directoryDepth}\"`\n- `\"[debug] Generation plan: ${files.length} files, ${tasks.length} tasks (${dirTasks.length} directories)\"`\n\nUses `picocolors` (imported as `pc`) for dimmed output.\n### types.ts\n**Purpose:** types.ts defines the core TypeScript interfaces for the documentation generation pipeline: file analysis results (`An...\n\n**types.ts defines the core TypeScript interfaces for the documentation generation pipeline: file analysis results (`AnalysisResult`), extracted metadata schemas (`SummaryMetadata`), and generation options (`SummaryOptions`).**\n\n## Exported Interfaces\n\n**`AnalysisResult`** — Result structure returned after LLM-based file analysis, containing:\n- `summary: string` — Generated markdown summary text\n- `metadata: SummaryMetadata` — Extracted structured metadata\n\n**`SummaryMetadata`** — Metadata schema extracted during file analysis, with fields:\n- `purpose: string` — Primary purpose statement (mandatory)\n- `criticalTodos?: string[]` — Optional array of security/breaking issues only\n- `relatedFiles?: string[]` — Optional array of tightly-coupled sibling file paths\n\n**`SummaryOptions`** — Configuration object for summary generation behavior:\n- `targetLength: 'short' | 'standard' | 'detailed'` — Controls output verbosity\n- `includeCodeSnippets: boolean` — Toggles code snippet inclusion in generated summaries\n\n## Integration Points\n\nThese types flow through the three-phase pipeline defined in `src/generation/orchestrator.ts`:\n- Phase 1 (file analysis): AI backends in `src/ai/backends/` return `AnalysisResult` via `AIService.call()`\n- Phase 2 (directory aggregation): `SummaryMetadata` fields (`relatedFiles`, `criticalTodos`) aggregated into `AGENTS.md` via `src/generation/writers/agents-md.ts`\n- Phase 3 (root synthesis): `purpose` field extracted from `.sum` YAML frontmatter via `src/generation/collector.ts`\n\n`SummaryOptions` consumed by prompt builders in `src/generation/prompts/builder.ts` to adjust template verbosity and code snippet inclusion rules.\n\n## Design Constraints\n\n- `SummaryMetadata.criticalTodos` intentionally limited to security/breaking issues (not general TODOs) per density rules in project instructions\n- `SummaryMetadata.relatedFiles` paths must resolve to actual source files to avoid phantom path validation errors from `src/quality/phantom-paths/validator.ts`\n- `AnalysisResult` populated by LLM via subprocess output parsing in `src/ai/subprocess.ts` (expects JSON-serializable structure)\n\n## Import Map (verified — use these exact paths)\n\norchestrator.ts:\n  ../config/schema.js → Config (type)\n  ../types/index.js → DiscoveryResult (type)\n  ../orchestration/trace.js → ITraceWriter (type)\n\n## Project Directory Structure\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\n\n## Subdirectories\n\n### prompts/\n<!-- Generated by agents-reverse-engineer -->\n\n# generation/prompts\n\n**This directory exports `buildFilePrompt()`, `buildDirectoryPrompt()`, `buildRootPrompt()` for constructing system+user prompt pairs fed to AI CLI subprocesses during three-phase documentation generation: per-file `.sum` analysis, directory-level `AGENTS.md` aggregation, and root integration document synthesis (`CLAUDE.md`, `GEMINI.md`, `OPENCODE.md`).**\n\n## Contents\n\n### [builder.ts](./builder.ts)\nImplements `buildFilePrompt()` (substitutes `{{FILE_PATH}}`, `{{CONTENT}}`, `{{LANG}}`, `{{PROJECT_PLAN_SECTION}}` placeholders, appends contextFiles, selects `FILE_UPDATE_SYSTEM_PROMPT` when `context.existingSum` present), `buildDirectoryPrompt()` (reads `.sum` files via `readSumFile()`, collects child `AGENTS.md`, extracts imports via `extractDirectoryImports()`, detects 9 manifest types: `package.json`, `Cargo.toml`, `go.mod`, `pyproject.toml`, `pom.xml`, `build.gradle`, `Gemfile`, `composer.json`, `CMakeLists.txt`, `Makefile`), `buildRootPrompt()` (calls `collectAgentsDocs()`, parses root `package.json`, embeds synthesis constraints), and `detectLanguage()` (maps 22 file extensions to syntax identifiers).\n\n### [index.ts](./index.ts)\nBarrel export re-exporting `PromptContext`, `SUMMARY_GUIDELINES`, `buildFilePrompt()`, `buildDirectoryPrompt()`, `buildRootPrompt()`, `detectLanguage()` from `types.ts` and `builder.ts` to form public API consumed by `src/generation/executor.ts`.\n\n### [templates.ts](./templates.ts)\nDefines `FILE_SYSTEM_PROMPT` (density rules, anchor term preservation, behavioral contract extraction), `FILE_USER_PROMPT` (placeholders: `{{FILE_PATH}}`, `{{CONTENT}}`), `DIRECTORY_SYSTEM_PROMPT` (path accuracy constraints, adaptive section selection), `FILE_UPDATE_SYSTEM_PROMPT` (incremental preservation directives), `DIRECTORY_UPDATE_SYSTEM_PROMPT` (structure preservation), `ROOT_SYSTEM_PROMPT` (synthesis constraints forbidding invention).\n\n### [types.ts](./types.ts)\nExports `PromptContext` (fields: `filePath`, `content`, `contextFiles`, `projectPlan`, `existingSum`) consumed by prompt builders and `SUMMARY_GUIDELINES` (targetLength: 300-500 words, include array mandating behavioral contracts: verbatim regex patterns, format strings, magic constants, sentinel values, environment variables).\n\n## Architecture\n\n### Three-Phase Prompt Construction\n\n**Phase 1: File Analysis**  \n`buildFilePrompt()` receives `PromptContext`, detects language via extension lookup (`detectLanguage()`), substitutes placeholders in `FILE_USER_PROMPT` template, appends related files section when `contextFiles` populated, selects `FILE_UPDATE_SYSTEM_PROMPT` for incremental updates (when `existingSum` present) or `FILE_SYSTEM_PROMPT` for fresh analysis.\n\n**Phase 2: Directory Aggregation**  \n`buildDirectoryPrompt()` reads directory via `readdir()`, filters against `knownDirs` set (skips non-source directories), reads `.sum` files in parallel via `Promise.all(readSumFile(getSumPath(entryPath)))`, collects child `AGENTS.md`, extracts imports from source files matching `/\\.(ts|tsx|js|jsx|py|go|rs|java|kt)$/` via `extractDirectoryImports()`, detects manifest files, checks for user documentation (AGENTS.local.md or non-generated AGENTS.md lacking `GENERATED_MARKER`), assembles sections: file summaries, import map via `formatImportMap()`, project structure, subdirectories, directory hints, user notes, existing AGENTS.md (update mode), returns `DIRECTORY_UPDATE_SYSTEM_PROMPT` when `existingAgentsMd` supplied.\n\n**Phase 3: Root Synthesis**  \n`buildRootPrompt()` calls `collectAgentsDocs()` to recursively gather all `AGENTS.md` files, reads root `package.json` and extracts metadata (name, version, description, packageManager, scripts), embeds all AGENTS.md content as `### ${relativePath}` sections, appends package metadata, includes synthesis constraint: \"Use ONLY the information provided below. Do NOT invent features, hooks, patterns, or APIs that are not explicitly mentioned in the AGENTS.md content.\"\n\n### Template Substitution Pattern\n\nAll prompts follow structure: system prompt (behavioral constraints) + user prompt (context injection). Builder functions perform string interpolation with runtime data:\n- `buildFilePrompt()` injects file path, source code, language identifier, project plan section\n- `buildDirectoryPrompt()` injects `.sum` summaries, import maps, child AGENTS.md, manifest hints, user notes\n- `buildRootPrompt()` injects all AGENTS.md content, package metadata, output requirements checklist\n\n## Behavioral Contracts\n\n### Density Rules (FILE_SYSTEM_PROMPT)\n`\"Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\"`. Banned filler phrases: `\"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"`.\n\n### Anchor Term Preservation (FILE_SYSTEM_PROMPT)\n`\"All exported function/class/type/const names MUST appear in the summary exactly as written in source\"`, `\"Preserve exact casing of identifiers\"`, `\"Missing any exported identifier is a failure\"`.\n\n### Output Format Requirements (FILE_SYSTEM_PROMPT, DIRECTORY_SYSTEM_PROMPT)\n`\"Start your response DIRECTLY with the purpose statement\"` for file prompts. `\"Output ONLY the raw markdown content. No code fences, no preamble\"` for directory prompts. `\"First line MUST be exactly: <!-- Generated by agents-reverse-engineer -->\"` for AGENTS.md.\n\n### Behavioral Contract Extraction (FILE_SYSTEM_PROMPT)\nRequired captures: `\"Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\"`, `\"Format strings, output templates, serialization structures — show exact format\"`, `\"Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\"`, `\"Prompt text or template strings that control AI/LLM behavior\"`, `\"Error message patterns and error code strings\"`, `\"Environment variable names and their expected values\"`, `\"File format specifications (YAML frontmatter schemas, NDJSON line formats)\"`.\n\n### Path Accuracy Constraints (DIRECTORY_SYSTEM_PROMPT)\n`\"When referencing files or modules outside this directory, use ONLY paths from the 'Import Map' section\"`, `\"Do NOT invent, rename, or guess module paths\"`, `\"Use the exact directory names from 'Project Directory Structure' — do NOT rename directories\"`, `\"Cross-module references must use the specifier format from actual import statements\"`, `\"If you are unsure about a path, omit the cross-reference rather than guessing\"`.\n\n### Incremental Update Preservation (FILE_UPDATE_SYSTEM_PROMPT, DIRECTORY_UPDATE_SYSTEM_PROMPT)\n`\"Preserve the structure, section headings, and phrasing of the existing summary wherever the underlying code is unchanged\"`, `\"Only modify content that is directly affected by the code changes\"`, `\"Regex patterns, format strings, magic constants, and template content from the existing summary MUST be preserved verbatim unless the source code changed them\"`, `\"Never summarize or paraphrase regex patterns — always show the exact pattern in backticks\"`.\n\n### Root Synthesis Constraints (ROOT_SYSTEM_PROMPT)\n`\"Synthesize ONLY from the AGENTS.md content provided in the user prompt\"`, `\"Do NOT invent, extrapolate, or hallucinate features, hooks, APIs, patterns, or dependencies not explicitly mentioned\"`, `\"Every claim must be traceable to a specific AGENTS.md file provided\"`.\n\n### Source File Extension Filter (builder.ts)\n`/\\.(ts|tsx|js|jsx|py|go|rs|java|kt)$/` — only files matching these extensions processed by `extractDirectoryImports()` for import map generation.\n\n### Manifest Detection Pattern (builder.ts)\n`['package.json', 'Cargo.toml', 'go.mod', 'pyproject.toml', 'pom.xml', 'build.gradle', 'Gemfile', 'composer.json', 'CMakeLists.txt', 'Makefile']` — presence triggers \"likely a package or project root\" hint in directory prompts.\n\n### Language Extension Map (builder.ts detectLanguage())\n22 extensions: `.ts` → `'typescript'`, `.tsx` → `'tsx'`, `.js` → `'javascript'`, `.jsx` → `'jsx'`, `.py` → `'python'`, `.rb` → `'ruby'`, `.go` → `'go'`, `.rs` → `'rust'`, `.java` → `'java'`, `.kt` → `'kotlin'`, `.swift` → `'swift'`, `.cs` → `'csharp'`, `.php` → `'php'`, `.vue` → `'vue'`, `.svelte` → `'svelte'`, `.json` → `'json'`, `.yaml`/`.yml` → `'yaml'`, `.md` → `'markdown'`, `.css` → `'css'`, `.scss` → `'scss'`, `.html` → `'html'`. Defaults to `'text'`.\n\n## File Relationships\n\n`builder.ts` calls `readSumFile()` + `getSumPath()` from `../writers/sum.js` to retrieve `.sum` frontmatter+content, references `GENERATED_MARKER` from `../writers/agents-md.js` to detect user-authored vs. generated AGENTS.md, invokes `extractDirectoryImports()` + `formatImportMap()` from `../../imports/index.js` for verified import maps, calls `collectAgentsDocs()` from `../collector.js` to aggregate AGENTS.md recursively. `templates.ts` provides raw prompt text consumed by `builder.ts` via import. `types.ts` defines `PromptContext` structure threaded through `builder.ts` functions. `index.ts` re-exports public API consumed by `src/generation/executor.ts` during three-phase pipeline execution.\n\n## Integration Points\n\nConsumed by `src/generation/executor.ts` which invokes prompt builders during phase execution: `buildFilePrompt()` called for each file in Phase 1 concurrent pool, `buildDirectoryPrompt()` called for each directory in Phase 2 post-order traversal, `buildRootPrompt()` called once per root document in Phase 3 sequential execution. Prompts passed as `{ system, user }` pairs to `AIService.call()` in `src/ai/service.ts` which spawns CLI subprocesses (`claude`, `gemini`, `opencode`) with resource limits (`NODE_OPTIONS='--max-old-space-size=512'`, `UV_THREADPOOL_SIZE='4'`, `CLAUDE_CODE_DISABLE_BACKGROUND_TASKS='1'`, `--disallowedTools Task`).\n### writers/\n<!-- Generated by agents-reverse-engineer -->\n\n# src/generation/writers\n\n`.sum` file I/O with YAML frontmatter serialization, `AGENTS.md` lifecycle management with user content preservation, and path resolution utilities for Phase 1/Phase 2 writer operations.\n\n## Contents\n\n**[sum.ts](./sum.ts)** — Serializes `SumFileContent` to `.sum` files with YAML frontmatter (`generated_at`, `content_hash`, `purpose`, `critical_todos`, `related_files`), parses frontmatter via regex extraction of inline/multi-line arrays, computes `.sum` paths via `getSumPath()`, validates existence via `sumFileExists()`.\n\n**[agents-md.ts](./agents-md.ts)** — Writes directory-level `AGENTS.md` with `GENERATED_MARKER` ownership detection, preserves user-authored content by renaming to `AGENTS.local.md` and prepending above LLM output, strips marker prefix from incoming content, constructs final structure with optional user content block and `---` separator.\n\n**[index.ts](./index.ts)** — Re-exports `writeSumFile`, `readSumFile`, `getSumPath`, `sumFileExists`, `SumFileContent` from `sum.ts` and `writeAgentsMd` from `agents-md.ts` as public API boundary.\n\n## YAML Frontmatter Format\n\nsum.ts serializes frontmatter with adaptive array formatting: inline `[a, b, c]` for ≤3 items under 40 chars each, multi-line `- item` otherwise. Parses via dual-pattern regex supporting both syntaxes. Mandatory fields: `generated_at` (ISO 8601), `content_hash` (SHA-256 hex), `purpose` (single-line string). Optional arrays: `critical_todos`, `related_files`.\n\n## User Content Preservation\n\nagents-md.ts implements idempotent preservation via four-step workflow:\n\n1. Read existing `AGENTS.md`, detect user authorship via `GENERATED_MARKER` absence, rename to `AGENTS.local.md` if user-authored\n2. Attempt read of already-renamed `AGENTS.local.md` from previous run if step 1 found no user content\n3. Strip `GENERATED_MARKER` prefix from incoming LLM content via `slice()` + `/^\\n+/` normalization\n4. Construct final content: `GENERATED_MARKER` → optional user block with `<!-- User-defined AGENTS.md preserved as AGENTS.local.md -->` header and `---` separator → LLM content, write via `writeFile()` with `mkdir({recursive: true})`\n\nPreserved content prepended above generated content ensures AI assistants encounter project-specific context before auto-generated summaries.\n\n## Path Resolution\n\n`getSumPath(sourcePath)` returns `${sourcePath}.sum` for given source file. No directory transformation — `.sum` files colocate with source files throughout tree. `sumFileExists()` delegates to `readSumFile()` non-null check, avoiding redundant `access()` syscalls.\n\n## Integration with Pipeline Phases\n\n**Phase 1 (File Analysis):** `src/generation/executor.ts` workers call `writeSumFile()` after AI subprocess returns `SumFileContent`, writes to path from `getSumPath(sourceFile.path)`.\n\n**Phase 2 (Directory Aggregation):** `src/generation/orchestrator.ts` calls `readSumFile()` via `collectAgentsDocs()` to load child `.sum` files for prompt construction, calls `writeAgentsMd()` with LLM response from `AIService.call()`.\n\n**Incremental Updates:** `src/update/orphan-cleaner.ts` calls `readSumFile()` to extract `content_hash` from YAML frontmatter for SHA-256 comparison, `cleanupEmptyDirectoryDocs()` uses `isGeneratedAgentsMd()` to determine deletion eligibility.\n\n**Cleanup:** `src/cli/clean.ts` calls `isGeneratedAgentsMd()` to filter deletion targets, skips user-authored files, restores `AGENTS.local.md` → `AGENTS.md` after removing generated version.\n\n## Behavioral Contracts\n\n**GENERATED_MARKER constant:**\n```javascript\n'<!-- Generated by agents-reverse-engineer -->'\n```\n\n**Frontmatter extraction regex (parseSumFile):**\n```javascript\n/^---\\n([\\s\\S]*?)\\n---\\n/\n```\n\n**Field patterns:**\n```javascript\n/generated_at:\\s*(.+)/\n/content_hash:\\s*(.+)/\n/purpose:\\s*(.+)/\n```\n\n**Array parsing patterns (parseYamlArray):**\n```javascript\n/key:\\s*\\[([^\\]]*)\\]/  // Inline: key: [a, b, c]\n/key:\\s*\\n((?:\\s+-\\s+.+\\n?)+)/m  // Multi-line: key:\\n  - item\n```\n\n**Quote stripping (inline arrays):**\n```javascript\n.replace(/^[\"']|[\"']$/g, '')\n```\n\n**Multi-line item prefix:**\n```javascript\n/^\\s*-\\s*/  // Strips leading '  - '\n```\n\n**Array format heuristic (formatYamlArray):**\n- Inline if `values.length <= 3` and all items `< 40` chars\n- Multi-line otherwise\n- Empty: `key: []`\n\n**Newline normalization (writeAgentsMd):**\n```javascript\ncontent.slice(GENERATED_MARKER.length).replace(/^\\n+/, '')\n```",
      "systemPrompt": "You are generating an AGENTS.md file — a directory-level overview for AI coding assistants.\n\nCRITICAL: Output ONLY the raw markdown content. No code fences, no preamble, no explanations, no conversational text. Your entire response IS the AGENTS.md file content.\n\nOUTPUT FORMAT:\n- First line MUST be exactly: <!-- Generated by agents-reverse-engineer -->\n- Use a # heading with the directory name\n- Write a one-paragraph purpose statement for the directory\nusage\nADAPTIVE SECTIONS:\nAnalyze the directory contents and choose the most relevant sections. Do NOT use a fixed template. Instead, select sections that best document this specific directory for an AI that needs to reconstruct or extend the project.\n\nConsider these section types (choose what applies):\n- **Contents**: Group files by purpose/category under ## headings. For each file: markdown link [filename](./filename) and a one-line description.\n- **Subdirectories**: If subdirectories exist, list them with links [dirname/](./dirname/) and brief summaries.\n- **Architecture / Data Flow**: If files form a pipeline, request/response chain, or layered architecture, document it.\n- **Stack**: If this is a package root (has package.json, Cargo.toml, go.mod, etc.), document the technology stack, key scripts, and entry points.\n- **Structure**: If the directory layout follows a convention (feature-sliced, domain-driven, MVC, etc.), document it.\n- **Patterns**: If files share recurring design patterns (factory, strategy, middleware, barrel re-export), name and document them.\n- **Configuration**: If the directory contains config files, schemas, or environment definitions, document the config surface area.\n- **API Surface**: If the directory exports a public API (barrel index, route definitions, SDK), document the interface contract.\n- **File Relationships**: How files collaborate, depend on each other, or share state.\n- **Behavioral Contracts**: If files contain regex patterns, format specifications, magic constants, or template strings that define observable behavior, collect them in a dedicated section. Preserve verbatim patterns from file summaries — do NOT paraphrase regex into prose. This section is MANDATORY when file summaries contain behavioral artifacts.\n\nChoose any relevant sections or create your own based on the directory contents. The goal is to provide a comprehensive overview that captures the essence of the directory's role in the project and how its files work together, with a focus on what an AI coding assistant would need to know to effectively interact with this code.\n\nSCOPE:\n- AGENTS.md is a NAVIGATIONAL INDEX — help an AI find the right file quickly\n- Focus on: what each file does, how files relate, directory-level patterns\n- Do NOT reproduce full architecture sections — those belong in the root CLAUDE.md\n\nPATH ACCURACY (MANDATORY):\n- When referencing files or modules outside this directory, use ONLY paths from the \"Import Map\" section\n- Do NOT invent, rename, or guess module paths — if a path isn't in the Import Map, don't reference it\n- Use the exact directory names from \"Project Directory Structure\" — do NOT rename directories\n  (e.g., if the directory is called \"cli\", write \"src/cli/\", NOT \"src/commands/\")\n- Cross-module references must use the specifier format from actual import statements\n  (e.g., \"../generation/writers/sum.js\", NOT \"../fs/sum-file.js\")\n- If you are unsure about a path, omit the cross-reference rather than guessing\n\nCONSISTENCY (MANDATORY):\n- Do not contradict yourself within the same document\n- If you describe a technique (e.g., \"regex-based\"), do not call it something else later (e.g., \"AST-based\")\n- When stating version numbers, engines, or config fields, use ONLY values present in the file summaries\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this directory\", \"this module\", \"provides\", \"responsible for\", \"is used to\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Per-file descriptions in Contents sections: 1-2 sentences maximum. Reference key symbols but do not reproduce full summaries.\n- Behavioral contracts (regex patterns, format specs, constants) belong in a separate Behavioral Contracts section, not in per-file descriptions.\n- Subdirectory descriptions: 1-2 sentences maximum. Capture the directory's role, not its full contents.\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- Key exported symbols from file summaries MUST appear in the directory overview\n- Preserve exact casing of identifiers\n\nUSER NOTES:\n- If \"User Notes\" are provided in the prompt, they contain user-defined instructions that will be automatically prepended to your output\n- Do NOT repeat or paraphrase user notes in your generated content — they are included separately\n- You may reference information from user notes for context",
      "response": "<!-- Generated by agents-reverse-engineer -->\n\n# generation\n\n**Phase orchestration, file/directory task preparation, complexity analysis, prompt construction, `.sum` + `AGENTS.md` I/O, and root document collection for the three-phase AI-driven documentation pipeline.**\n\n## Contents\n\n### [collector.ts](./collector.ts)\n`collectAgentsDocs()` recursively walks project tree via `readdir()` with `withFileTypes: true`, accumulates all `AGENTS.md` files into `Array<{ relativePath, content }>`, skips 13 directories (`node_modules`, `.git`, `.agents-reverse-engineer`, `vendor`, `dist`, `build`, `__pycache__`, `.next`, `venv`, `.venv`, `target`, `.cargo`, `.gradle`), returns lexicographically sorted array via `localeCompare()`.\n\n### [complexity.ts](./complexity.ts)\n`analyzeComplexity()` computes `ComplexityMetrics` from file array: `fileCount`, `directoryDepth` via `path.relative().split(path.sep).length - 1`, `directories` set via `path.dirname()` parent chain walk until root `.`, `files` array passthrough.\n\n### [executor.ts](./executor.ts)\n`buildExecutionPlan()` transforms `GenerationPlan` into `ExecutionPlan` with three task categories: `fileTasks` (format `file:<path>`, outputs `.sum`, zero dependencies), `directoryTasks` (format `dir:<path>`, outputs `AGENTS.md`, depends on child file IDs, sorted by `getDirectoryDepth()` descending for post-order traversal), `rootTasks` (format `root:CLAUDE.md`, depends on all directory IDs). `isDirectoryComplete()` verifies all `.sum` files exist via `sumFileExists()`. `formatExecutionPlanAsMarkdown()` generates `GENERATION-PLAN.md`.\n\n### [orchestrator.ts](./orchestrator.ts)\n`GenerationOrchestrator.createPlan()` executes seven-step pipeline: reads file content via `readFile()`, calls `analyzeComplexity()`, invokes `buildProjectStructure()` for compact tree, calls `buildFilePrompt()` per file with `projectPlan` injection, groups files by `path.dirname()` for directory task creation, concatenates `fileTasks + dirTasks`, clears `PreparedFile.content` fields to free memory, emits `phase:start`/`plan:created`/`phase:end` trace events.\n\n### [types.ts](./types.ts)\nDefines `AnalysisResult` (fields: `summary`, `metadata: SummaryMetadata`), `SummaryMetadata` (fields: `purpose`, `criticalTodos?`, `relatedFiles?`), `SummaryOptions` (fields: `targetLength: 'short'|'standard'|'detailed'`, `includeCodeSnippets: boolean`).\n\n## Subdirectories\n\n### [prompts/](./prompts/)\n`buildFilePrompt()` substitutes `{{FILE_PATH}}`, `{{CONTENT}}`, `{{LANG}}`, selects `FILE_UPDATE_SYSTEM_PROMPT` when `existingSum` present. `buildDirectoryPrompt()` reads `.sum` via `readSumFile()`, collects child `AGENTS.md`, extracts imports via `extractDirectoryImports()`, detects 9 manifest types (`package.json`, `Cargo.toml`, `go.mod`, `pyproject.toml`, `pom.xml`, `build.gradle`, `Gemfile`, `composer.json`, `CMakeLists.txt`/`Makefile`). `buildRootPrompt()` calls `collectAgentsDocs()`, parses root `package.json`, embeds synthesis constraints. Templates enforce density rules, anchor term preservation, behavioral contract extraction (verbatim regex patterns, format strings, magic constants).\n\n### [writers/](./writers/)\n`writeSumFile()` serializes YAML frontmatter (`generated_at`, `content_hash`, `purpose`, `critical_todos`, `related_files`) with adaptive array formatting (inline `[a,b,c]` for ≤3 items <40 chars, multi-line `- item` otherwise). `readSumFile()` parses via dual-pattern regex. `writeAgentsMd()` implements four-step user content preservation: detect ownership via `GENERATED_MARKER` absence, rename to `AGENTS.local.md`, prepend above LLM output with `---` separator. `getSumPath()` returns `${sourcePath}.sum`.\n\n## Architecture\n\n### Three-Phase Pipeline\n\n**Phase 1 (File Analysis):** `GenerationOrchestrator.createFileTasks()` invokes `buildFilePrompt()` per file, embeds content + project structure, populates `AnalysisTask[]` with `type: 'file'`, `systemPrompt`, `userPrompt`. Workers execute concurrently via `src/orchestration/pool.ts`, call `AIService.call()` → `runSubprocess()` → `execFile()` spawning AI CLI subprocesses with resource limits (`NODE_OPTIONS='--max-old-space-size=512'`, `UV_THREADPOOL_SIZE='4'`, `CLAUDE_CODE_DISABLE_BACKGROUND_TASKS='1'`, `--disallowedTools Task`). Results written via `writeSumFile()` to `.sum` paths from `getSumPath()`.\n\n**Phase 2 (Directory Aggregation):** `GenerationOrchestrator.createDirectoryTasks()` groups files via `Map<dirname, PreparedFile[]>`, creates `AnalysisTask[]` with `type: 'directory'`, `directoryInfo: { sumFiles, fileCount }`. Executor sorts tasks by `getDirectoryDepth()` descending (deepest first), waits for child `.sum` files via `isDirectoryComplete()`, constructs prompts via `buildDirectoryPrompt()` consuming `.sum` frontmatter + child `AGENTS.md` + import maps from `extractDirectoryImports()`. Writes via `writeAgentsMd()` preserving user content from `AGENTS.local.md`.\n\n**Phase 3 (Root Synthesis):** Sequential execution (concurrency=1) generates `CLAUDE.md`, `GEMINI.md`, `OPENCODE.md` via `buildRootPrompt()` aggregating all `AGENTS.md` via `collectAgentsDocs()`, parsing root `package.json`, enforcing synthesis-only constraints (no invention). Strips conversational preamble via pattern matching before writing output.\n\n### Dependency Graph\n\n`buildExecutionPlan()` constructs DAG with three layers:\n1. File tasks: zero dependencies, parallel-eligible\n2. Directory tasks: depend on child file IDs (`files.map(f => 'file:${f}')`), sorted by depth descending\n3. Root tasks: depend on all directory IDs\n\n`isDirectoryComplete()` predicate blocks directory processing until all child `.sum` files exist, ensuring post-order traversal guarantees data availability for parent synthesis.\n\n### Memory Management\n\n`createPlan()` clears `PreparedFile.content` fields via cast `(file as { content: string }).content = ''` after embedding content into file task prompts. Comment: \"The runner re-reads files from disk\" during execution phase. Reduces peak memory from O(total codebase size) to O(largest file × concurrency).\n\n## File Relationships\n\n`orchestrator.ts` calls `buildFilePrompt()` from `./prompts/index.js`, `analyzeComplexity()` from `./complexity.js`, `buildProjectStructure()` internally. `executor.ts` imports `sumFileExists()` from `./writers/sum.ts` for completion checks. `collector.ts` supplies `collectAgentsDocs()` to `./prompts/builder.ts` for root prompt construction. `writers/sum.ts` writes files consumed by `writers/agents-md.ts` prompt builder, both consumed by `executor.ts` via `./writers/index.js` barrel export.\n\n## Integration Points\n\nConsumed by `src/orchestration/runner.ts` which invokes `GenerationOrchestrator.createPlan()` → `buildExecutionPlan()` → three-phase execution via worker pools. Prompts passed to `AIService.call()` in `src/ai/service.ts`. Results written to filesystem via `writeSumFile()` + `writeAgentsMd()`. Trace events emitted to `src/orchestration/trace.ts`. Complexity metrics logged via `src/output/logger.ts`. Discovery input from `src/discovery/run.ts` as `DiscoveryResult`. Configuration from `src/config/loader.ts` as `Config`.\n\n## Behavioral Contracts\n\n**GENERATED_MARKER** (writers/agents-md.ts):\n```javascript\n'<!-- Generated by agents-reverse-engineer -->'\n```\n\n**Frontmatter extraction** (writers/sum.ts):\n```javascript\n/^---\\n([\\s\\S]*?)\\n---\\n/\n```\n\n**YAML field patterns**:\n```javascript\n/generated_at:\\s*(.+)/\n/content_hash:\\s*(.+)/\n/purpose:\\s*(.+)/\n/key:\\s*\\[([^\\]]*)\\]/  // Inline arrays\n/key:\\s*\\n((?:\\s+-\\s+.+\\n?)+)/m  // Multi-line arrays\n```\n\n**Array formatting heuristic** (writers/sum.ts):\n- Inline if `length <= 3` and all items `< 40` chars\n- Multi-line otherwise\n\n**Directory depth calculation** (executor.ts):\n```javascript\ndir.split(path.sep).length  // Returns 0 for '.', 1 for 'src', 2 for 'src/cli'\n```\n\n**Post-order sort comparator** (executor.ts):\n```javascript\n(a, b) => getDirectoryDepth(dirB) - getDirectoryDepth(dirA)  // Descending\n```\n\n**Task ID formats** (executor.ts):\n- File: `file:<relativePath>`\n- Directory: `dir:<relativePath>`\n- Root: `root:<filename>`\n\n**Skip directories** (collector.ts):\n```javascript\nSet(['node_modules', '.git', '.agents-reverse-engineer', 'vendor', 'dist', 'build', '__pycache__', '.next', 'venv', '.venv', 'target', '.cargo', '.gradle'])\n```\n\n**Manifest detection** (prompts/builder.ts):\n```javascript\n['package.json', 'Cargo.toml', 'go.mod', 'pyproject.toml', 'pom.xml', 'build.gradle', 'Gemfile', 'composer.json', 'CMakeLists.txt', 'Makefile']\n```\n\n**Source file filter for imports** (prompts/builder.ts):\n```javascript\n/\\.(ts|tsx|js|jsx|py|go|rs|java|kt)$/\n```\n\n**Language extension map** (prompts/builder.ts):\n22 extensions: `.ts` → `'typescript'`, `.py` → `'python'`, `.rs` → `'rust'`, etc., defaults to `'text'`.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 2701,
      "cacheReadTokens": 17477,
      "cacheCreationTokens": 18573,
      "latencyMs": 81214,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": []
    },
    {
      "timestamp": "2026-02-09T16:13:39.057Z",
      "prompt": "Generate AGENTS.md for directory: \"src/ai\" (ai)\n\n## File Summaries (6 files)\n\n### index.ts\n**Purpose:** Barrel export consolidating AI service layer into single public interface: AIService, BackendRegistry, createBackendR...\n\n**Barrel export consolidating AI service layer into single public interface: AIService, BackendRegistry, createBackendRegistry, resolveBackend, detectBackend, getInstallInstructions, withRetry, runSubprocess, isCommandOnPath, plus type exports (AIBackend, AIResponse, AICallOptions, SubprocessResult, RetryOptions, TelemetryEntry, RunLog, FileRead) and AIServiceError.**\n\n## Public API Surface\n\nAll imports from `src/ai/` must go through this barrel export. Direct imports from `src/ai/backends/` or `src/ai/telemetry/` violate encapsulation.\n\n### Service Orchestration\n\n- **AIService** (class) — Main orchestrator for AI backend calls with telemetry, retries, timeout enforcement\n- **AIServiceOptions** (type) — Configuration interface: `timeoutMs`, `maxRetries`, `telemetry: { keepRuns }`, optional tracer\n\n### Backend Registry\n\n- **BackendRegistry** (class) — Container for backend implementations with registration and lookup\n- **createBackendRegistry(): BackendRegistry** — Factory initializing registry with Claude, Gemini, OpenCode backends\n- **resolveBackend(registry: BackendRegistry, name: string | 'auto'): Promise<AIBackend>** — Returns backend by name or auto-detects first available\n- **detectBackend(registry: BackendRegistry): Promise<AIBackend | null>** — Iterates registry to find first backend with available command\n- **getInstallInstructions(backendName: string): string** — Returns installation instructions for specified backend\n\n### Retry Logic\n\n- **withRetry<T>(fn: () => Promise<T>, options?: RetryOptions): Promise<T>** — Exponential backoff retry wrapper\n- **DEFAULT_RETRY_OPTIONS** (const) — Default retry configuration\n\n### Subprocess Management\n\n- **runSubprocess(options: SubprocessOptions): Promise<SubprocessResult>** — Spawns AI CLI child process with timeout, resource limits, process group killing\n\n### Backend Utilities\n\n- **isCommandOnPath(command: string): Promise<boolean>** — Checks command availability via `which` (exported from `backends/claude.ts`)\n\n## Type Exports\n\n- **AIBackend** — Interface with `name`, `command`, `buildArgs()`, `parseResponse()`, `isAvailable()`\n- **AIResponse** — Standardized response: `content: string`, `finishReason: string`, `usage?: TokenUsage`\n- **AICallOptions** — Call parameters: `prompt: string`, `temperature?: number`, `maxTokens?: number`, `systemPrompt?: string`\n- **SubprocessResult** — Execution result: `stdout: string`, `stderr: string`, `exitCode: number | null`, `signal: string | null`, `pid?: number`, `durationMs: number`\n- **RetryOptions** — Retry config: `maxRetries: number`, `initialDelayMs: number`, `maxDelayMs: number`, `backoffFactor: number`\n- **TelemetryEntry** — Per-call telemetry: `timestamp`, `backend`, `model`, `durationMs`, `tokens`, `cost`, `error?`, `filesRead[]`\n- **RunLog** — Aggregated run log: `startTime`, `endTime`, `calls[]`, `summary: { totalCost, errorCount, uniqueFilesRead }`\n- **FileRead** — File metadata: `path: string`, `sizeBytes: number`, `linesRead: number`\n\n## Error Types\n\n- **AIServiceError** (class extends Error) — Discriminated error with `code` property values: `'BACKEND_NOT_FOUND'`, `'SUBPROCESS_ERROR'`, `'TIMEOUT'`, `'RATE_LIMIT'`, `'PARSE_ERROR'`, `'CONFIG_ERROR'`\n\n## Usage Pattern\n\n```typescript\nimport { AIService, createBackendRegistry, resolveBackend } from './ai/index.js';\n\nconst registry = createBackendRegistry();\nconst backend = await resolveBackend(registry, 'auto');\nconst service = new AIService(backend, {\n  timeoutMs: 120_000,\n  maxRetries: 3,\n  telemetry: { keepRuns: 10 },\n});\n\nconst response = await service.call({ prompt: 'Hello' });\n```\n\n## Module Constraints\n\nThis barrel export enforces layering: consumers import from `ai/index.js`, never reaching into `ai/backends/` or `ai/telemetry/` directly. Internal modules remain encapsulated.\n### registry.ts\n**Purpose:** registry.ts manages AI backend registration, auto-detection, and resolution with actionable error messages when CLIs ...\n\n**registry.ts manages AI backend registration, auto-detection, and resolution with actionable error messages when CLIs are not found.**\n\n## Exported Classes\n\n**BackendRegistry** stores AI backend adapters keyed by name with insertion-order preservation for detection priority. Methods:\n- `register(backend: AIBackend): void` — adds backend to internal `Map<string, AIBackend>` keyed by `backend.name`\n- `get(name: string): AIBackend | undefined` — retrieves backend by name\n- `getAll(): AIBackend[]` — returns all backends in registration order\n\n## Exported Functions\n\n**createBackendRegistry(): BackendRegistry** returns pre-populated registry with backends in priority order: ClaudeBackend, GeminiBackend, OpenCodeBackend.\n\n**detectBackend(registry: BackendRegistry): Promise<AIBackend | null>** iterates `registry.getAll()` calling `isAvailable()` on each backend, returns first where CLI is found on PATH or `null` if none available.\n\n**getInstallInstructions(registry: BackendRegistry): string** concatenates `backend.getInstallInstructions()` from all registered backends with `\\n\\n` separators for error message formatting.\n\n**resolveBackend(registry: BackendRegistry, requested: string | 'auto'): Promise<AIBackend>** selects backend via auto-detection (when `requested === 'auto'`) or explicit lookup. Throws `AIServiceError` with code `'CLI_NOT_FOUND'` if:\n- Auto-detection finds no available backend (includes full install instructions via `getInstallInstructions()`)\n- Requested backend name not in registry (lists known backends)\n- Requested backend CLI not on PATH (includes backend-specific install instructions)\n\n## Dependencies\n\nImports `AIBackend`, `AIServiceError` from `./types.js`, `ClaudeBackend` from `./backends/claude.js`, `GeminiBackend` from `./backends/gemini.js`, `OpenCodeBackend` from `./backends/opencode.js`.\n\n## Detection Priority\n\nBackend registration order in `createBackendRegistry()` determines `detectBackend()` iteration sequence: Claude (recommended, fully implemented) → Gemini (experimental, stub) → OpenCode (experimental, stub).\n\n## Error Message Templates\n\nCLI_NOT_FOUND errors follow template: `\"No AI CLI found on your system.\\n\\nInstall one of the following:\\n\\n${instructions}\\n\\nThen run this command again.\"` for auto-detection failures, `\"Backend \\\"${requested}\\\" is not available. The \\\"${backend.cliCommand}\\\" CLI was not found on PATH.\\n\\n${backend.getInstallInstructions()}\"` for explicit backend unavailability.\n### retry.ts\n**Purpose:** retry.ts implements exponential backoff retry logic with jitter for handling transient AI service failures during sub...\n\n**retry.ts implements exponential backoff retry logic with jitter for handling transient AI service failures during subprocess calls.**\n\n## Exported Functions\n\n**withRetry\\<T>(fn: () => Promise<T>, options: RetryOptions): Promise<T>** executes async function `fn` with exponential backoff retry on transient failures. Returns result on success, throws last error after exhausting `maxRetries`, throws immediately if `options.isRetryable(error)` returns false. Delay formula: `min(baseDelayMs * multiplier^attempt, maxDelayMs) + jitter` where jitter is random [0, 500ms].\n\n## Exported Constants\n\n**DEFAULT_RETRY_OPTIONS** provides base configuration: `maxRetries: 3`, `baseDelayMs: 1_000`, `maxDelayMs: 8_000`, `multiplier: 2`. Typed as `Omit<RetryOptions, 'isRetryable' | 'onRetry'>` requiring callers to supply error predicate and retry callback.\n\n## Retry Behavior\n\n**withRetry** attempts operation up to `maxRetries + 1` total times (initial attempt + retries). On each failure checks `options.isRetryable(error)` before sleeping—permanent errors (rate limits with \"retry after\" headers, auth failures) throw immediately without backoff. Invokes optional `options.onRetry?.(attempt + 1, error)` callback before each delay to enable telemetry emission or logging.\n\n## Jitter Strategy\n\n**Delay calculation** adds random 0-500ms jitter to capped exponential delay via `Math.random() * 500` to prevent thundering herd when multiple worker pool processes hit same rate limit simultaneously. Jitter applied after computing `min(baseDelayMs * multiplier^attempt, maxDelayMs)`.\n\n## Integration Points\n\n**withRetry** consumed by `AIService.call()` in `src/ai/service.ts` which wraps `runSubprocess()` invocations. Caller provides `isRetryable` predicate checking stderr patterns (\"rate limit\", \"429\", \"too many requests\", \"overloaded\") and `onRetry` callback emitting trace events via `ITraceWriter.retry()`. Referenced in subprocess resource management context (CLAUDE.md §\"Subprocess Resource Management\").\n\n## Type Dependencies\n\n**RetryOptions** interface imported from `./types.js` defines configuration schema with required fields `maxRetries`, `baseDelayMs`, `maxDelayMs`, `multiplier`, `isRetryable: (error: unknown) => boolean`, optional `onRetry?: (attempt: number, error: unknown) => void`.\n### service.ts\n**Purpose:** AIService orchestrates AI CLI subprocess invocations with retry logic, timeout enforcement, telemetry recording, and ...\n\n**AIService orchestrates AI CLI subprocess invocations with retry logic, timeout enforcement, telemetry recording, and trace emission for concurrent file analysis workflows.**\n\n## Exported API\n\n**AIService** class coordinates AI backend interactions through:\n- `constructor(backend: AIBackend, options: AIServiceOptions)` — Initializes service with resolved backend adapter and config (timeoutMs, maxRetries, telemetry.keepRuns)\n- `call(options: AICallOptions): Promise<AIResponse>` — Executes AI invocation with retry wrapper, records TelemetryEntry, returns normalized AIResponse\n- `finalize(projectRoot: string): Promise<{ logPath: string; summary: RunLog['summary'] }>` — Writes RunLog to `.agents-reverse-engineer/logs/run-<timestamp>.json`, prunes old logs via cleanupOldLogs()\n- `setTracer(tracer: ITraceWriter): void` — Attaches trace writer for subprocess:spawn/exit/retry events\n- `setDebug(enabled: boolean): void` — Enables verbose subprocess logging with heap/RSS metrics to stderr\n- `setSubprocessLogDir(dir: string): void` — Configures directory for writing per-subprocess `.log` files (stdout+stderr+metadata)\n- `addFilesReadToLastEntry(filesRead: FileRead[]): void` — Attaches file context metadata to most recent telemetry entry\n- `getSummary(): RunLog['summary']` — Returns current run statistics (totalInputTokens, totalCacheReadTokens, errorCount, etc.) without finalizing\n\n**AIServiceOptions** interface specifies:\n- `timeoutMs: number` — Default subprocess timeout (from config.ai.timeoutMs, typically 120000)\n- `maxRetries: number` — Retry attempt limit for transient errors (from config.ai.maxRetries, typically 3)\n- `model?: string` — Default model identifier applied to all calls unless overridden per AICallOptions.model\n- `telemetry.keepRuns: number` — Retention count for historical run logs (from config.ai.telemetry.keepRuns, typically 50)\n\n**AIServiceError** type (re-exported from `./types.js`) with discriminated codes: `'TIMEOUT' | 'RATE_LIMIT' | 'SUBPROCESS_ERROR' | 'PARSE_ERROR'`\n\n## Retry Strategy\n\n**withRetry()** invoked from `./retry.js` with custom configuration:\n- `maxRetries: options.maxRetries` (typically 3)\n- `isRetryable: (error) => error.code === 'RATE_LIMIT'` — ONLY retries rate limits; timeouts are permanent failures to avoid resource exhaustion\n- `onRetry: (attempt, error) => ...` — Emits `retry` trace event and stderr warning with `[warn] Retrying \"${taskLabel}\" (attempt ${attempt}/${maxRetries}, reason: ${errorCode})`\n\n**isRateLimitStderr(stderr: string): boolean** detects transient errors via pattern matching:\n- Patterns: `['rate limit', '429', 'too many requests', 'overloaded']` (case-insensitive substring search)\n- Maps subprocess stderr to `AIServiceError('RATE_LIMIT', ...)` for retry eligibility\n\n## Subprocess Lifecycle\n\n**call()** execution flow:\n1. Merges `options.model ?? this.options.model` into effectiveOptions\n2. Calls `backend.buildArgs(effectiveOptions)` to construct CLI argv\n3. Increments `activeSubprocesses` counter before spawn\n4. Invokes `runSubprocess(backend.cliCommand, args, { timeoutMs, input: prompt, onSpawn: (pid) => tracer.emit('subprocess:spawn', ...) })`\n5. Decrements `activeSubprocesses` after completion\n6. Emits `subprocess:exit` trace event with childPid, exitCode, signal, durationMs, timedOut\n7. Enqueues subprocess log write via `enqueueSubprocessLog()` (fire-and-forget, non-critical)\n8. Checks `result.timedOut` → throws `AIServiceError('TIMEOUT', ...)` with stderr warning showing PID and duration\n9. Checks `exitCode !== 0` → applies `isRateLimitStderr()` detection → throws RATE_LIMIT or SUBPROCESS_ERROR\n10. Calls `backend.parseResponse(stdout, durationMs, exitCode)` with try/catch wrapping parse errors as `AIServiceError('PARSE_ERROR', ...)`\n11. Records TelemetryEntry via `logger.addEntry()` with model, tokens, latencyMs, retryCount, filesRead (empty array initially)\n\n## Telemetry Recording\n\n**TelemetryLogger** (from `./telemetry/logger.js`) accumulates entries in-memory:\n- Each `call()` invocation adds entry with timestamp, prompt, systemPrompt, response, model, inputTokens, outputTokens, cacheReadTokens, cacheCreationTokens, latencyMs, exitCode, retryCount, thinking='not supported', filesRead=[]\n- Failed calls record empty response, 0 tokens, error message string, exitCode=1\n- `addFilesReadToLastEntry(filesRead)` mutates most recent entry to attach FileRead[] metadata (path + sizeBytes)\n- `toRunLog()` converts entries to RunLog with aggregated summary: totalInputTokens, totalOutputTokens, totalCacheReadTokens, totalCacheCreationTokens, totalLatencyMs, callCount, errorCount, uniqueFilesRead\n\n**writeRunLog(projectRoot, runLog)** (from `./telemetry/run-log.js`) serializes to `.agents-reverse-engineer/logs/run-<timestamp>.json`\n\n**cleanupOldLogs(projectRoot, keepRuns)** (from `./telemetry/cleanup.js`) enforces retention policy by deleting oldest logs beyond keepRuns limit\n\n## Debug Logging\n\n**setDebug(true)** enables stderr output:\n- Before spawn: `[debug] Spawning subprocess for \"${taskLabel}\" (active: ${activeSubprocesses}, heapUsed: ${formatBytes(mem.heapUsed)}, rss: ${formatBytes(mem.rss)}, timeout: ${timeoutMs/1000}s)`\n- After exit: `[debug] Subprocess exited for \"${taskLabel}\" (PID ${childPid}, exitCode: ${exitCode}, duration: ${durationMs/1000}s, active: ${activeSubprocesses})`\n- Uses `formatBytes(bytes): string` helper producing `\"123B\"` / `\"4.5KB\"` / `\"67.8MB\"` strings\n\n**setSubprocessLogDir(dir)** writes `.log` files asynchronously:\n- Filename pattern: `${taskLabel.replace(/\\//g, '--').replace(/[^a-zA-Z0-9._-]/g, '_')}_pid${childPid}.log`\n- Content format: metadata header (`task:`, `pid:`, `command:`, `exit:`, `signal:`, `duration:`, `timed_out:`) followed by `--- stdout ---` and `--- stderr ---` sections\n- Serialized via promise chain (`logWriteQueue`) to prevent concurrent mkdir races from worker pool\n- Failures silently swallowed (non-critical operation)\n\n## Trace Event Emission\n\n**setTracer(tracer)** enables ITraceWriter event emission:\n- `subprocess:spawn` — Emitted synchronously in `onSpawn` callback with childPid, command, taskLabel\n- `subprocess:exit` — Emitted after subprocess completion with childPid, exitCode, signal, durationMs, timedOut\n- `retry` — Emitted in retry handler with attempt number, taskLabel, errorCode\n\n## Backend Integration\n\n**AIBackend** interface (from `./types.js`) requires:\n- `name: string` — Backend identifier (e.g., \"Claude\", \"Gemini\", \"OpenCode\")\n- `cliCommand: string` — Executable name (e.g., \"claude\", \"gemini\", \"opencode\")\n- `buildArgs(options: AICallOptions): string[]` — Constructs argv from options (model, prompt, systemPrompt)\n- `parseResponse(stdout: string, durationMs: number, exitCode: number): AIResponse` — Extracts text, model, tokens, cacheReadTokens, cacheCreationTokens\n\n**Backend adapter examples:**\n- `./backends/claude.ts` — Parses JSON with `usage.input_tokens`, `usage.output_tokens`, `usage.cache_read_input_tokens`, `usage.cache_creation_input_tokens`\n- `./backends/gemini.ts` — Stub implementation throwing SUBPROCESS_ERROR\n- `./backends/opencode.ts` — Stub implementation throwing SUBPROCESS_ERROR\n\n## Dependencies\n\n- `runSubprocess()` from `./subprocess.js` — Spawns child processes with timeout, process group killing, SIGTERM/SIGKILL escalation\n- `withRetry()` from `./retry.js` — Exponential backoff wrapper with isRetryable predicate and onRetry callback\n- `TelemetryLogger` from `./telemetry/logger.js` — In-memory entry accumulation with summary aggregation\n- `writeRunLog()` from `./telemetry/run-log.js` — Persists RunLog JSON to disk\n- `cleanupOldLogs()` from `./telemetry/cleanup.js` — Prunes old logs via readdir + stat + unlink\n- `ITraceWriter` from `../orchestration/trace.js` — Trace event emission interface (NullTraceWriter or TraceWriter with NDJSON output)\n\n## Design Patterns\n\n**Template Method pattern** — `call()` defines subprocess invocation skeleton, delegates backend-specific logic to `backend.buildArgs()` and `backend.parseResponse()`\n\n**Strategy pattern** — Backend adapters (Claude, Gemini, OpenCode) implement AIBackend interface for pluggable CLI invocation\n\n**Promise chaining for serialization** — `logWriteQueue` ensures sequential execution of async mkdir+writeFile to avoid race conditions from concurrent workers\n\n**Resource tracking** — `activeSubprocesses` counter enables debug logging of subprocess concurrency for diagnosing resource exhaustion (see CLAUDE.md subprocess management section)\n### subprocess.ts\n**Purpose:** subprocess.ts spawns AI CLI child processes with timeout enforcement, SIGKILL escalation, stdin piping, and concurren...\n\n**subprocess.ts spawns AI CLI child processes with timeout enforcement, SIGKILL escalation, stdin piping, and concurrent subprocess tracking.**\n\n## Exported Functions\n\n`runSubprocess(command: string, args: string[], options: SubprocessOptions): Promise<SubprocessResult>` spawns AI CLI subprocess via `execFile()` with timeout enforcement. Always resolves (never rejects). Returns `SubprocessResult` containing `stdout`, `stderr`, `exitCode`, `signal`, `durationMs`, `timedOut`, `childPid`. Sends SIGTERM at `options.timeoutMs`, escalates to SIGKILL after 5s grace period if process doesn't exit. Writes `options.input` to stdin if provided, then closes stream via `child.stdin.end()`. Invokes `options.onSpawn(pid)` callback synchronously after spawn for trace emission. Kills process group via `process.kill(-child.pid, 'SIGKILL')` on exit to terminate subprocess trees. Clears SIGKILL timer via `clearTimeout(sigkillTimer)` after callback fires. Detects timeout via `error.killed === true`. Extracts exit code from `error.code` (if number), else `child.exitCode`, else defaults to 1 for errors or 0 for success.\n\n`getActiveSubprocessCount(): number` returns size of `activeSubprocesses` Map for concurrency debugging.\n\n`getActiveSubprocesses(): Array<{ pid: number; command: string; spawnedAt: number; runningMs: number }>` returns array of active subprocess details computed from `activeSubprocesses` Map entries. Calculates `runningMs` as `Date.now() - info.spawnedAt`.\n\n## Exported Types\n\n`SubprocessOptions` interface with fields:\n- `timeoutMs: number` — subprocess timeout in milliseconds\n- `input?: string` — stdin content to pipe\n- `onSpawn?: (pid: number | undefined) => void` — callback fired synchronously after spawn\n\n## Resource Management\n\nTracks active subprocesses in `Map<number, { command: string; spawnedAt: number }>` keyed by PID. Registers subprocess via `activeSubprocesses.set(child.pid, { command, spawnedAt })` after spawn. Removes via `activeSubprocesses.delete(child.pid)` in execFile callback. Map enables concurrency monitoring via exported count/detail getters.\n\n## Timeout Enforcement Strategy\n\nSIGTERM sent at `options.timeoutMs` via `execFile` timeout option with `killSignal: 'SIGTERM'`. SIGKILL escalation timer set for `timeoutMs + SIGKILL_GRACE_MS` (5000ms). Timer unreffed via `sigkillTimer.unref()` to prevent blocking event loop. Cleared via `clearTimeout()` in callback if process exits before escalation. Process group killing via negative PID (`process.kill(-child.pid, 'SIGKILL')`) terminates entire subprocess tree. Falls back to single-process kill if process group signal fails.\n\n## Exit Code Extraction Logic\n\nChecks `error === null` → exit code 0. Checks `typeof error.code === 'number'` → use `error.code`. Checks `child.exitCode !== null` → use `child.exitCode`. Else defaults to 1. Required because `error.code` can be string like `'ERR_CHILD_PROCESS_STDIO_MAXBUFFER'` instead of numeric exit code.\n\n## Stdin Handling\n\nWrites `options.input` to `child.stdin` if provided and `child.stdin !== null`. Computes byte length via `Buffer.byteLength(options.input, 'utf-8')` for logging. Always calls `child.stdin.end()` to send EOF, preventing child process deadlock waiting for stdin closure.\n\n## Process Environment\n\nPasses `maxBuffer: 10 * 1024 * 1024` (10MB) for large AI responses. Sets `encoding: 'utf-8'` and spreads `...process.env` into child environment. Used by `src/ai/service.ts` to inject resource limit variables: `NODE_OPTIONS='--max-old-space-size=512'`, `UV_THREADPOOL_SIZE='4'`, `CLAUDE_CODE_DISABLE_BACKGROUND_TASKS='1'`.\n\n## Constants\n\n`SIGKILL_GRACE_MS = 5_000` — milliseconds between SIGTERM and SIGKILL escalation.\n\n## Integration Points\n\nConsumed by `AIService.call()` in `src/ai/service.ts` which populates `SubprocessOptions.onSpawn` for trace event emission. Result `SubprocessResult` type defined in `src/ai/types.ts`. Centralizes all AI CLI subprocess spawning for Claude Code, Gemini CLI, OpenCode backends defined in `src/ai/backends/`.\n### types.ts\n**Purpose:** Defines TypeScript interfaces, type aliases, and the AIServiceError class for the AI service layer, establishing cont...\n\n**Defines TypeScript interfaces, type aliases, and the AIServiceError class for the AI service layer, establishing contracts for backend adapters, subprocess results, API responses, retry configuration, and telemetry logging structures.**\n\n## Exported Types\n\n### Subprocess Result\n\n**SubprocessResult** captures the outcome of a CLI subprocess execution with fields:\n- `stdout: string` — standard output captured from child process\n- `stderr: string` — standard error captured from child process  \n- `exitCode: number` — numeric exit code (0 = success, non-zero = failure)\n- `signal: string | null` — signal that terminated the process, or null for normal exit\n- `durationMs: number` — wall-clock duration in milliseconds\n- `timedOut: boolean` — whether process exceeded its timeout\n- `childPid?: number` — OS PID of child process (undefined if spawn failed)\n\n### AI Call Interfaces\n\n**AICallOptions** defines input parameters for AI service calls:\n- `prompt: string` — required prompt text sent to AI model\n- `systemPrompt?: string` — optional system prompt for context/behavior\n- `model?: string` — model identifier (e.g., \"sonnet\", \"opus\") interpreted by backend\n- `timeoutMs?: number` — subprocess timeout override in milliseconds\n- `maxTurns?: number` — maximum agentic turns (backend-specific)\n- `taskLabel?: string` — label for tracing (e.g., file path being processed)\n\n**AIResponse** normalizes backend CLI outputs into a unified structure:\n- `text: string` — AI model's text response\n- `model: string` — model identifier reported by backend\n- `inputTokens: number` — input tokens consumed\n- `outputTokens: number` — output tokens generated\n- `cacheReadTokens: number` — tokens served from cache reads\n- `cacheCreationTokens: number` — tokens written to cache\n- `durationMs: number` — wall-clock duration in milliseconds\n- `exitCode: number` — process exit code from CLI\n- `raw: unknown` — original CLI JSON output for debugging\n\n### Backend Contract\n\n**AIBackend** interface defines the contract for AI CLI backend adapters (Claude, Gemini, OpenCode):\n- `readonly name: string` — human-readable backend name (e.g., \"Claude\", \"Gemini\")\n- `readonly cliCommand: string` — CLI executable name on PATH (e.g., \"claude\", \"gemini\")\n- `isAvailable(): Promise<boolean>` — checks whether CLI is available on PATH\n- `buildArgs(options: AICallOptions): string[]` — builds CLI argument array for a call\n- `parseResponse(stdout: string, durationMs: number, exitCode: number): AIResponse` — parses CLI stdout into normalized AIResponse\n- `getInstallInstructions(): string` — returns user-facing install instructions when CLI not found\n\n### Retry Configuration\n\n**RetryOptions** controls exponential backoff retry behavior:\n- `maxRetries: number` — maximum retry attempts (e.g., 3 means up to 4 total attempts)\n- `baseDelayMs: number` — base delay in milliseconds before first retry\n- `maxDelayMs: number` — maximum delay cap in milliseconds\n- `multiplier: number` — exponential multiplier applied to base delay\n- `isRetryable: (error: unknown) => boolean` — predicate returning true if error is transient and retryable\n- `onRetry?: (attempt: number, error: unknown) => void` — optional callback invoked before each retry\n\n### Telemetry Structures\n\n**FileRead** records a single file read sent as context to an AI call:\n- `path: string` — file path relative to project root\n- `sizeBytes: number` — file size in bytes at time of read\n\n**TelemetryEntry** captures per-call telemetry for a single AI invocation:\n- `timestamp: string` — ISO 8601 timestamp when call initiated\n- `prompt: string` — prompt text sent\n- `systemPrompt?: string` — system prompt if used\n- `response: string` — AI model's text response\n- `model: string` — model identifier\n- `inputTokens: number` — input tokens consumed\n- `outputTokens: number` — output tokens generated\n- `cacheReadTokens: number` — tokens served from cache reads\n- `cacheCreationTokens: number` — tokens written to cache\n- `latencyMs: number` — wall-clock latency in milliseconds\n- `exitCode: number` — process exit code\n- `error?: string` — error message if call failed\n- `retryCount: number` — number of retries before this result\n- `thinking: string` — AI thinking/reasoning content, \"not supported\" when backend doesn't provide it\n- `filesRead: FileRead[]` — files sent as context for this call\n\n**RunLog** aggregates all TelemetryEntry instances for a single CLI run:\n- `runId: string` — unique run identifier (ISO timestamp-based)\n- `startTime: string` — ISO 8601 timestamp when run started\n- `endTime: string` — ISO 8601 timestamp when run finished\n- `entries: TelemetryEntry[]` — all individual call entries\n- `summary` — aggregated summary object with fields:\n  - `totalCalls: number` — total AI calls made\n  - `totalInputTokens: number` — sum of input tokens across all calls\n  - `totalOutputTokens: number` — sum of output tokens across all calls\n  - `totalDurationMs: number` — total wall-clock duration in milliseconds\n  - `errorCount: number` — calls resulting in errors\n  - `totalCacheReadTokens: number` — sum of cache read tokens\n  - `totalCacheCreationTokens: number` — sum of cache creation tokens\n  - `totalFilesRead: number` — total file reads including duplicates\n  - `uniqueFilesRead: number` — unique files read, deduped by path\n\n### Error Types\n\n**AIServiceErrorCode** is a discriminated union type alias for typed error handling:\n```typescript\ntype AIServiceErrorCode = 'CLI_NOT_FOUND' | 'TIMEOUT' | 'PARSE_ERROR' | 'SUBPROCESS_ERROR' | 'RATE_LIMIT'\n```\n\n**AIServiceError** extends Error with a machine-readable code field:\n- `readonly code: AIServiceErrorCode` — machine-readable error code\n- `constructor(code: AIServiceErrorCode, message: string)` — constructs typed error with code and message\n- Sets `name` property to `'AIServiceError'`\n\nEnables branching on error.code without parsing message strings (e.g., `if (error.code === 'RATE_LIMIT')`).\n\n## Integration Points\n\nThis types module is imported by all AI service modules:\n- `src/ai/service.ts` — main AIService class consuming AIBackend and AICallOptions\n- `src/ai/subprocess.ts` — runSubprocess() returning SubprocessResult\n- `src/ai/registry.ts` — backend registry managing AIBackend implementations\n- `src/ai/retry.ts` — retry utility consuming RetryOptions\n- `src/ai/telemetry/logger.ts` — telemetry logger writing TelemetryEntry and RunLog structures\n- `src/ai/backends/*.ts` — Claude/Gemini/OpenCode adapters implementing AIBackend interface\n\n## Import Map (verified — use these exact paths)\n\nservice.ts:\n  ../orchestration/trace.js → ITraceWriter (type)\n\n## Project Directory Structure\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\n\n## Subdirectories\n\n### backends/\n<!-- Generated by agents-reverse-engineer -->\n\n# src/ai/backends\n\n**Backend adapter implementations for Claude Code, Gemini CLI, and OpenCode providing AIBackend interface compliance with CLI argument construction, JSON response parsing, PATH availability detection, and installation instructions.**\n\n## Contents\n\n### Backend Implementations\n\n**[claude.ts](./claude.ts)** — ClaudeBackend adapter with Zod-validated JSON parsing extracting `text`/`model`/`inputTokens`/`outputTokens`/`cacheReadTokens`/`cacheCreationTokens` from Claude CLI v2.1.31 output, `buildArgs()` constructing `['-p', '--output-format', 'json', '--no-session-persistence', '--permission-mode', 'bypassPermissions']` base arguments with conditional `--model`/`--system-prompt`/`--max-turns` appends, `isCommandOnPath()` cross-platform PATH detection handling Windows `PATHEXT` extension iteration.\n\n**[gemini.ts](./gemini.ts)** — GeminiBackend stub returning `['-p', '--output-format', 'json']` from `buildArgs()` but throwing `SUBPROCESS_ERROR` in `parseResponse()` until Gemini JSON output format stabilizes per RESEARCH.md Open Question 2.\n\n**[opencode.ts](./opencode.ts)** — OpenCodeBackend stub returning `['run', '--format', 'json']` from `buildArgs()` but throwing `SUBPROCESS_ERROR` in `parseResponse()` until JSONL parsing implementation completes per RESEARCH.md Open Question 3.\n\n## AIBackend Interface Contract\n\nAll backends implement five required methods:\n\n- **`isAvailable(): Promise<boolean>`** — Delegates to `isCommandOnPath(this.cliCommand)` checking PATH directories via `fs.stat().isFile()` with Windows PATHEXT handling\n- **`buildArgs(options: AICallOptions): string[]`** — Constructs CLI argument arrays (prompt content delivered via stdin by `runSubprocess()` caller, not CLI arg)\n- **`parseResponse(stdout: string, durationMs: number, exitCode: number): AIResponse`** — Normalizes CLI output to `{ text, model, inputTokens, outputTokens, cacheReadTokens, cacheCreationTokens, durationMs, exitCode, raw }` structure\n- **`getInstallInstructions(): string`** — Returns multiline installation guidance with npm/curl commands and documentation URLs\n- **Properties:** `name: string` (backend identifier), `cliCommand: string` (executable name)\n\n## Claude Response Schema\n\n`ClaudeResponseSchema` validates JSON structure:\n\n```typescript\n{\n  type: 'result',\n  subtype: 'success'|'error',\n  is_error: boolean,\n  duration_ms: number,\n  duration_api_ms: number,\n  num_turns: number,\n  result: string,\n  session_id: string,\n  total_cost_usd: number,\n  usage: {\n    input_tokens: number,\n    cache_creation_input_tokens: number,\n    cache_read_input_tokens: number,\n    output_tokens: number\n  },\n  modelUsage: Record<string, {\n    inputTokens: number,\n    outputTokens: number,\n    cacheReadInputTokens: number,\n    cacheCreationInputTokens: number,\n    costUSD: number\n  }>\n}\n```\n\n## Integration with Registry\n\nBackends registered in `src/ai/registry.ts` `AIBackendRegistry.backends` array enabling `detectAvailableBackend()` auto-detection via parallel `isAvailable()` checks. Registry returns first available backend when `config.ai.backend === 'auto'`.\n\n## PATH Detection Strategy\n\n`isCommandOnPath()` implements cross-platform availability checking:\n\n1. Reads `process.env.PATH`, strips double quotes, splits by `path.delimiter` (`:` Unix, `;` Windows)\n2. Reads `process.env.PATHEXT` on Windows (e.g., `.COM;.EXE;.BAT;.CMD`), defaults to `['']` on Unix\n3. Iterates PATH directories × PATHEXT extensions constructing candidate paths\n4. Calls `fs.stat(candidatePath).isFile()` returning `true` on first match (avoids Unix execute bit assumptions)\n5. Returns `false` if no candidates exist as files\n\n## Error Handling\n\nClaudeBackend throws `AIServiceError` with codes:\n\n- **`PARSE_ERROR`** — When `stdout.indexOf('{')` returns `-1` (includes first 200 chars of raw output)\n- **`PARSE_ERROR`** — When Zod validation fails (includes validation error message)\n\nStub backends throw:\n\n- **`SUBPROCESS_ERROR`** — GeminiBackend/OpenCodeBackend until parsing implementation completes\n\n## Behavioral Contracts\n\n### ClaudeBackend CLI Arguments\n\nBase arguments (always included):\n```\n-p\n--output-format json\n--no-session-persistence\n--permission-mode bypassPermissions\n```\n\nConditional arguments:\n```\n--model <value>               # if options.model present\n--system-prompt <value>       # if options.systemPrompt present\n--max-turns <value>           # if options.maxTurns !== undefined\n```\n\n### JSON Parsing Defensive Pattern\n\n```javascript\nconst jsonStart = stdout.indexOf('{')\nif (jsonStart === -1) {\n  throw new AIServiceError('PARSE_ERROR', `No JSON in output: ${stdout.slice(0, 200)}`)\n}\nconst parsed = ClaudeResponseSchema.parse(JSON.parse(stdout.slice(jsonStart)))\n```\n\n### Windows PATHEXT Extensions\n\n```\n.COM;.EXE;.BAT;.CMD;.VBS;.VBE;.JS;.JSE;.WSF;.WSH;.MSC\n```\n### telemetry/\n<!-- Generated by agents-reverse-engineer -->\n\n# src/ai/telemetry\n\nPersistent telemetry accumulation and retention management: in-memory RunLog assembly via TelemetryLogger, JSON serialization with sanitized ISO 8601 filenames, and log rotation enforcing configurable retention limits.\n\n## Contents\n\n**[cleanup.ts](./cleanup.ts)** — `cleanupOldLogs(projectRoot, keepCount)` deletes old run log files from `.agents-reverse-engineer/logs/`, retaining N most recent via lexicographic sort on ISO 8601 filenames, handles ENOENT gracefully.\n\n**[logger.ts](./logger.ts)** — `TelemetryLogger` class accumulates per-call `TelemetryEntry` objects in memory, computes aggregate summary statistics (total tokens/costs/errors, unique files read), exports `toRunLog()` for serialization.\n\n**[run-log.ts](./run-log.ts)** — `writeRunLog(projectRoot, runLog)` writes RunLog JSON to `.agents-reverse-engineer/logs/run-<timestamp>.json` with directory creation and filename sanitization (`/[:.]/g` → `-`).\n\n## Architecture\n\n**Single-Run Lifecycle:**\n1. CLI invokes `new TelemetryLogger(runId)` with ISO timestamp, captures `startTime`\n2. AIService calls `addEntry(entry)` after each subprocess completion, `setFilesReadOnLastEntry(filesRead)` after file metadata attachment\n3. CLI invokes `toRunLog()` to freeze in-memory state into RunLog structure with computed summary\n4. `writeRunLog()` persists JSON to disk with 2-space indentation\n5. `cleanupOldLogs(projectRoot, config.ai.telemetry.keepRuns)` removes oldest files beyond retention limit\n\n**Cross-Run Retention:**\n- Log filenames encode sortable timestamps via `run-2026-02-07T12-00-00-000Z.json` pattern\n- Lexicographic sort produces correct chronological ordering without timestamp parsing\n- Retention enforced via `entries.slice(keepCount)` after newest-first reversal\n\n## Data Flow\n\n```\nAIService subprocess\n  ↓ (per-call metrics)\nTelemetryLogger.addEntry()\n  ↓ (in-memory accumulation)\nTelemetryLogger.toRunLog()\n  ↓ (RunLog structure with summary)\nwriteRunLog()\n  ↓ (filesystem persistence)\n.agents-reverse-engineer/logs/run-<timestamp>.json\n  ↓ (retention enforcement)\ncleanupOldLogs() removes oldest files\n```\n\n## RunLog Summary Computation\n\n`TelemetryLogger.getSummary()` iterates all `entries[]` to compute:\n- `totalInputTokens`, `totalOutputTokens`, `totalCacheReadTokens`, `totalCacheCreationTokens` (sum of per-call counts)\n- `totalDurationMs` (sum of `latencyMs` values)\n- `errorCount` (count where `error !== undefined`)\n- `totalFilesRead` (sum of `filesRead.length` across entries)\n- `uniqueFilesRead` (cardinality of `Set<string>` from all `file.path` values)\n\nRecomputes on every invocation without caching (safe because typically called once at run end).\n\n## Filesystem Safety\n\n**Filename Sanitization:**\n- ISO 8601 contains `:` (Windows path separator) and `.` (shell metacharacter)\n- `runLog.startTime.replace(/[:.]/g, '-')` produces cross-platform-safe filenames\n- Example: `2026-02-07T12:00:00.000Z` → `run-2026-02-07T12-00-00-000Z.json`\n\n**Directory Creation:**\n- `fs.mkdir()` with `recursive: true` creates `.agents-reverse-engineer/logs/` if absent\n- `cleanupOldLogs()` catches ENOENT from `fs.readdir()` and returns `0` when logs directory missing\n\n## Integration Points\n\n- **TelemetryLogger** threaded via `CommandRunOptions.telemetryLogger` → AIService constructor → per-call hooks\n- **writeRunLog** called by command runners after run completion when `config.ai.telemetry.enabled === true`\n- **cleanupOldLogs** invoked immediately after `writeRunLog()` with `config.ai.telemetry.keepRuns` (default 50)\n- Complements `src/orchestration/trace.ts` which retains 500 NDJSON trace files via similar lexicographic sort + slice pattern\n\n## Type Dependencies\n\nAll modules import from `../types.js`:\n- `TelemetryEntry`: per-call metrics with `inputTokens`, `outputTokens`, `cacheReadTokens`, `cacheCreationTokens`, `latencyMs`, `error?: string`, `filesRead: FileRead[]`\n- `RunLog`: container with `runId`, `startTime`, `endTime`, `entries: TelemetryEntry[]`, `summary` (aggregate statistics)\n- `FileRead`: file metadata with `path`, `sizeBytes`, `linesRead`\n\n## Behavioral Contracts\n\n**Filename Pattern:**\n```regex\n^run-\\d{4}-\\d{2}-\\d{2}T\\d{2}-\\d{2}-\\d{2}-\\d{3}Z\\.json$\n```\nMatches sanitized ISO 8601 timestamps where `:` and `.` replaced with `-`.\n\n**Log File Filtering:**\n```typescript\nname.startsWith('run-') && name.endsWith('.json')\n```\nIsolates run log files from other potential artifacts in `.agents-reverse-engineer/logs/`.\n\n**Timestamp Sanitization Transform:**\n```javascript\nrunLog.startTime.replace(/[:.]/g, '-')\n```\nConverts ISO 8601 string to filesystem-safe filename component.",
      "systemPrompt": "You are generating an AGENTS.md file — a directory-level overview for AI coding assistants.\n\nCRITICAL: Output ONLY the raw markdown content. No code fences, no preamble, no explanations, no conversational text. Your entire response IS the AGENTS.md file content.\n\nOUTPUT FORMAT:\n- First line MUST be exactly: <!-- Generated by agents-reverse-engineer -->\n- Use a # heading with the directory name\n- Write a one-paragraph purpose statement for the directory\nusage\nADAPTIVE SECTIONS:\nAnalyze the directory contents and choose the most relevant sections. Do NOT use a fixed template. Instead, select sections that best document this specific directory for an AI that needs to reconstruct or extend the project.\n\nConsider these section types (choose what applies):\n- **Contents**: Group files by purpose/category under ## headings. For each file: markdown link [filename](./filename) and a one-line description.\n- **Subdirectories**: If subdirectories exist, list them with links [dirname/](./dirname/) and brief summaries.\n- **Architecture / Data Flow**: If files form a pipeline, request/response chain, or layered architecture, document it.\n- **Stack**: If this is a package root (has package.json, Cargo.toml, go.mod, etc.), document the technology stack, key scripts, and entry points.\n- **Structure**: If the directory layout follows a convention (feature-sliced, domain-driven, MVC, etc.), document it.\n- **Patterns**: If files share recurring design patterns (factory, strategy, middleware, barrel re-export), name and document them.\n- **Configuration**: If the directory contains config files, schemas, or environment definitions, document the config surface area.\n- **API Surface**: If the directory exports a public API (barrel index, route definitions, SDK), document the interface contract.\n- **File Relationships**: How files collaborate, depend on each other, or share state.\n- **Behavioral Contracts**: If files contain regex patterns, format specifications, magic constants, or template strings that define observable behavior, collect them in a dedicated section. Preserve verbatim patterns from file summaries — do NOT paraphrase regex into prose. This section is MANDATORY when file summaries contain behavioral artifacts.\n\nChoose any relevant sections or create your own based on the directory contents. The goal is to provide a comprehensive overview that captures the essence of the directory's role in the project and how its files work together, with a focus on what an AI coding assistant would need to know to effectively interact with this code.\n\nSCOPE:\n- AGENTS.md is a NAVIGATIONAL INDEX — help an AI find the right file quickly\n- Focus on: what each file does, how files relate, directory-level patterns\n- Do NOT reproduce full architecture sections — those belong in the root CLAUDE.md\n\nPATH ACCURACY (MANDATORY):\n- When referencing files or modules outside this directory, use ONLY paths from the \"Import Map\" section\n- Do NOT invent, rename, or guess module paths — if a path isn't in the Import Map, don't reference it\n- Use the exact directory names from \"Project Directory Structure\" — do NOT rename directories\n  (e.g., if the directory is called \"cli\", write \"src/cli/\", NOT \"src/commands/\")\n- Cross-module references must use the specifier format from actual import statements\n  (e.g., \"../generation/writers/sum.js\", NOT \"../fs/sum-file.js\")\n- If you are unsure about a path, omit the cross-reference rather than guessing\n\nCONSISTENCY (MANDATORY):\n- Do not contradict yourself within the same document\n- If you describe a technique (e.g., \"regex-based\"), do not call it something else later (e.g., \"AST-based\")\n- When stating version numbers, engines, or config fields, use ONLY values present in the file summaries\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this directory\", \"this module\", \"provides\", \"responsible for\", \"is used to\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Per-file descriptions in Contents sections: 1-2 sentences maximum. Reference key symbols but do not reproduce full summaries.\n- Behavioral contracts (regex patterns, format specs, constants) belong in a separate Behavioral Contracts section, not in per-file descriptions.\n- Subdirectory descriptions: 1-2 sentences maximum. Capture the directory's role, not its full contents.\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- Key exported symbols from file summaries MUST appear in the directory overview\n- Preserve exact casing of identifiers\n\nUSER NOTES:\n- If \"User Notes\" are provided in the prompt, they contain user-defined instructions that will be automatically prepended to your output\n- Do NOT repeat or paraphrase user notes in your generated content — they are included separately\n- You may reference information from user notes for context",
      "response": "<!-- Generated by agents-reverse-engineer -->\n\n# src/ai\n\nBackend-agnostic AI service orchestration layer: subprocess-spawning adapters for Claude Code/Gemini/OpenCode CLIs, exponential backoff retry logic with rate limit detection, timeout enforcement via SIGTERM/SIGKILL escalation, NDJSON telemetry logging with token cost tracking, and trace event emission for concurrent pool workflows.\n\n## Contents\n\n**[index.ts](./index.ts)** — Barrel export consolidating AIService, BackendRegistry, createBackendRegistry, resolveBackend, detectBackend, getInstallInstructions, withRetry, runSubprocess, isCommandOnPath with type re-exports (AIBackend, AIResponse, AICallOptions, SubprocessResult, RetryOptions, TelemetryEntry, RunLog, FileRead, AIServiceError).\n\n**[registry.ts](./registry.ts)** — BackendRegistry stores AIBackend implementations in insertion-order Map with `register(backend)`, `get(name)`, `getAll()` methods; createBackendRegistry() pre-populates ClaudeBackend/GeminiBackend/OpenCodeBackend; resolveBackend(registry, 'auto'|name) performs auto-detection or explicit lookup throwing CLI_NOT_FOUND with install instructions.\n\n**[retry.ts](./retry.ts)** — withRetry() executes async function with exponential backoff (delay = min(baseDelayMs × multiplier^attempt, maxDelayMs) + jitter[0..500ms]) calling isRetryable(error) predicate before each retry, invoking onRetry(attempt, error) callback for telemetry.\n\n**[service.ts](./service.ts)** — AIService orchestrates call(options) via runSubprocess() wrapped in withRetry(), detects rate limits via stderr pattern matching (['rate limit', '429', 'too many requests', 'overloaded']), emits subprocess:spawn/exit/retry trace events, accumulates TelemetryEntry records via internal TelemetryLogger, writes RunLog to `.agents-reverse-engineer/logs/run-<timestamp>.json` on finalize(), enforces cleanup via cleanupOldLogs(keepRuns).\n\n**[subprocess.ts](./subprocess.ts)** — runSubprocess() spawns child process via execFile() with stdin piping, timeout enforcement (SIGTERM at timeoutMs, SIGKILL after 5s grace), process group killing (`kill(-pid)`) for tree termination, concurrent subprocess tracking via Map<pid, {command, spawnedAt}>, returns SubprocessResult with stdout/stderr/exitCode/signal/durationMs/timedOut/childPid.\n\n**[types.ts](./types.ts)** — Defines AIBackend interface (isAvailable/buildArgs/parseResponse/getInstallInstructions), AICallOptions (prompt/systemPrompt/model/timeoutMs/maxTurns/taskLabel), AIResponse (text/model/inputTokens/outputTokens/cacheReadTokens/cacheCreationTokens/durationMs/exitCode/raw), SubprocessResult, RetryOptions, TelemetryEntry, RunLog with summary aggregation, FileRead, AIServiceError with discriminated codes ('CLI_NOT_FOUND'|'TIMEOUT'|'PARSE_ERROR'|'SUBPROCESS_ERROR'|'RATE_LIMIT').\n\n## Subdirectories\n\n**[backends/](./backends/)** — ClaudeBackend with Zod-validated JSON parsing extracting usage.input_tokens/output_tokens/cache_read_input_tokens/cache_creation_input_tokens, GeminiBackend/OpenCodeBackend stubs throwing SUBPROCESS_ERROR, isCommandOnPath() cross-platform PATH detection with Windows PATHEXT extension iteration.\n\n**[telemetry/](./telemetry/)** — TelemetryLogger accumulates per-call entries computing aggregate summary (totalInputTokens/totalCacheReadTokens/errorCount/uniqueFilesRead), writeRunLog() serializes to `.agents-reverse-engineer/logs/run-<timestamp>.json` with filename sanitization (`/[:.]/g → '-'`), cleanupOldLogs() enforces retention via lexicographic sort on ISO 8601 filenames.\n\n## Architecture\n\n### Three-Layer Design\n\n**Backend Adapter Layer** (backends/): AIBackend implementations translate AICallOptions into CLI-specific argv arrays via buildArgs(), parse stdout JSON into normalized AIResponse via parseResponse(), detect availability via isCommandOnPath() checking PATH directories with Windows PATHEXT handling.\n\n**Subprocess Execution Layer** (subprocess.ts): runSubprocess() spawns execFile() with 10MB maxBuffer, writes input to stdin via Buffer.byteLength() computed payload, enforces timeout via SIGTERM then SIGKILL escalation with unref()'d timer, kills process groups via negative PID (`process.kill(-child.pid, 'SIGKILL')`), tracks active subprocesses in Map for concurrency monitoring.\n\n**Service Orchestration Layer** (service.ts): AIService wraps runSubprocess() calls in withRetry() with isRateLimitStderr() predicate detecting ['rate limit', '429', 'too many requests', 'overloaded'] patterns, emits subprocess:spawn/exit trace events via ITraceWriter, accumulates TelemetryEntry[] via TelemetryLogger, finalizes RunLog with summary (totalInputTokens, totalCacheReadTokens, errorCount, uniqueFilesRead).\n\n### Retry Strategy\n\nwithRetry() executes fn() up to maxRetries+1 times with exponential backoff: `min(baseDelayMs * multiplier^attempt, maxDelayMs) + random(0..500ms)`. Checks isRetryable(error) before each sleep—permanently fails on auth errors or non-retryable signals. Invokes onRetry(attempt, error) before delay for trace emission. AIService.call() only retries RATE_LIMIT errors (code === 'RATE_LIMIT'), treats TIMEOUT as permanent failure to prevent resource exhaustion.\n\n### Timeout Enforcement\n\nrunSubprocess() sends SIGTERM at timeoutMs via execFile killSignal option. Sets unref()'d SIGKILL timer at `timeoutMs + 5000ms`. Clears timer in callback if process exits before escalation. Process group killing (`kill(-pid)`) terminates entire subprocess tree. Falls back to single-process kill if group signal fails.\n\n### Telemetry Pipeline\n\nAIService.call() records TelemetryEntry after each subprocess completion with timestamp/prompt/systemPrompt/response/model/inputTokens/outputTokens/cacheReadTokens/cacheCreationTokens/latencyMs/exitCode/error/retryCount/thinking/filesRead. TelemetryLogger.addEntry() appends to in-memory entries[]. AIService.addFilesReadToLastEntry(filesRead) mutates most recent entry to attach FileRead[] metadata (path + sizeBytes). AIService.finalize() calls TelemetryLogger.toRunLog() computing summary, writes to `.agents-reverse-engineer/logs/run-<timestamp>.json` via writeRunLog(), enforces retention via cleanupOldLogs(keepRuns).\n\n### Resource Management\n\nSubprocess limits injected by AIService via environment variables (set in src/ai/service.ts, executed in subprocess.ts):\n- `NODE_OPTIONS='--max-old-space-size=512'` — limits heap to 512MB per subprocess\n- `UV_THREADPOOL_SIZE='4'` — constrains libuv thread pool to 4 threads\n- `CLAUDE_CODE_DISABLE_BACKGROUND_TASKS='1'` — prevents background task spawning\n- CLI args: `--disallowedTools Task` — blocks subagent spawning\n\nactiveSubprocesses Map tracks concurrent processes keyed by PID with {command, spawnedAt}. getActiveSubprocessCount() returns Map size. getActiveSubprocesses() computes runningMs as `Date.now() - spawnedAt`.\n\n### Debug Logging\n\nAIService.setDebug(true) enables stderr output before/after subprocess with heapUsed/rss metrics via formatBytes(). AIService.setSubprocessLogDir(dir) writes per-subprocess `.log` files with metadata header (task/pid/command/exit/signal/duration/timed_out) followed by stdout/stderr sections. Serialized via logWriteQueue promise chain preventing concurrent mkdir races. Failures silently swallowed (non-critical).\n\n## Integration Points\n\n**Phase 1 Orchestration**: `src/generation/executor.ts` calls AIService.call() for each .sum file generation passing prompt from `src/generation/prompts/builder.ts`, attaches filesRead metadata via addFilesReadToLastEntry() after file analysis.\n\n**Worker Pool**: `src/orchestration/pool.ts` shares single AIService instance across N workers (default 2 for WSL, 5 elsewhere), monitors concurrency via getActiveSubprocessCount(), emits trace events via AIService.setTracer(tracer).\n\n**Trace Emission**: AIService subprocess:spawn/exit events serialized to `.agents-reverse-engineer/traces/trace-<timestamp>.ndjson` via `src/orchestration/trace.ts` TraceWriter with promise-chain ordering guarantees.\n\n**Registry Auto-Detection**: `src/cli/generate.ts` calls createBackendRegistry() then resolveBackend(registry, config.ai.backend) for backend selection, throws CLI_NOT_FOUND with install instructions when backend unavailable.\n\n**Incremental Updates**: `src/update/orchestrator.ts` reuses same AIService instance for modified file regeneration, shares telemetry accumulation across discovery + analysis phases.\n\n## Behavioral Contracts\n\n### Rate Limit Detection Patterns\n\n```javascript\n['rate limit', '429', 'too many requests', 'overloaded'].some(p => \n  stderr.toLowerCase().includes(p)\n)\n```\n\n### Timeout Detection\n\n```javascript\nresult.timedOut === true  // set when error.killed === true in execFile callback\n```\n\n### Exit Code Extraction\n\n```javascript\nerror === null ? 0\n: typeof error.code === 'number' ? error.code\n: child.exitCode !== null ? child.exitCode\n: 1\n```\n\n### Process Group Killing\n\n```javascript\nprocess.kill(-child.pid, 'SIGKILL')  // negative PID targets process group\n```\n\n### Filename Sanitization\n\n```javascript\nrunLog.startTime.replace(/[:.]/g, '-')  // ISO 8601 → filesystem-safe\n// Example: 2026-02-07T12:00:00.000Z → run-2026-02-07T12-00-00-000Z.json\n```\n\n### Log File Filtering\n\n```javascript\nname.startsWith('run-') && name.endsWith('.json')\n```",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 2559,
      "cacheReadTokens": 17477,
      "cacheCreationTokens": 18999,
      "latencyMs": 86098,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": []
    },
    {
      "timestamp": "2026-02-09T16:13:39.107Z",
      "prompt": "Generate AGENTS.md for directory: \"src/cli\" (cli)\n\n## File Summaries (7 files)\n\n### clean.ts\n**Purpose:** cleanCommand() deletes generated ARE documentation artifacts (.sum files, AGENTS.md with marker, CLAUDE.md, GENERATIO...\n\n**cleanCommand() deletes generated ARE documentation artifacts (.sum files, AGENTS.md with marker, CLAUDE.md, GENERATION-PLAN.md) while preserving user-authored AGENTS.md and restoring AGENTS.local.md backups.**\n\n## Exported Interface\n\n**CleanOptions** interface with:\n- `dryRun: boolean` — Preview deletions without modifying filesystem (default: false)\n\n**cleanCommand(targetPath: string, options: CleanOptions): Promise<void>** — Entry point for `are clean` command, resolves targetPath to absolute directory, discovers artifacts via fast-glob, filters AGENTS.md by `GENERATED_MARKER` presence, deletes generated files, restores AGENTS.local.md → AGENTS.md.\n\n## Discovery Strategy\n\nUses `fast-glob` with three parallel glob calls:\n- `**/*.sum` — All summary files\n- `**/AGENTS.md` — Directory documentation (requires marker filtering)\n- `**/AGENTS.local.md` — User-authored backups to restore\n\nAll globs use `{ cwd: resolvedPath, absolute: true, onlyFiles: true, dot: true, ignore: ['**/node_modules/**', '**/.git/**'] }` options.\n\nSingle-file checks via `access(filePath, constants.F_OK)` for:\n- `CLAUDE.md` at project root\n- `.agents-reverse-engineer/GENERATION-PLAN.md`\n\n## User Content Preservation\n\n**Generated vs. User-Authored Distinction:**\n- Reads each `AGENTS.md` file with `readFile(file, 'utf-8')`\n- Checks for substring `GENERATED_MARKER` (imported from `src/generation/writers/agents-md.js`)\n- Marker present → add to `generatedAgentsFiles[]` for deletion\n- Marker absent → add to `skippedAgentsFiles[]`, log preservation message\n- Pattern prevents deletion of SDK documentation or manually written AGENTS.md files\n\n**AGENTS.local.md Restoration:**\n- `rename(localFile, agentsPath)` moves AGENTS.local.md → AGENTS.md\n- Reverses the backup operation performed during generation phase\n- Tracked separately in `restored` counter\n\n## Error Handling\n\n**Directory access validation:**\n- `access(resolvedPath, constants.R_OK)` before operation\n- `ENOENT` → log error, `process.exit(1)`\n- `EACCES`/`EPERM` → log error, `process.exit(1)`\n- Other errors → re-throw\n\n**Per-file deletion:**\n- Wraps `unlink()` and `rename()` in try-catch\n- Logs failure message with `relativePath()` and error message\n- Continues processing remaining files (no fail-fast)\n\n## Output Format\n\n**Discovery summary:**\n```\nFiles that would be deleted: (if dryRun)\n  path/to/file.sum\n  path/to/AGENTS.md\n\nPreserving user-authored AGENTS.md:\n  docs/AGENTS.md\n\nFiles that would be restored: (if dryRun)\n  path/AGENTS.local.md → path/AGENTS.md\n\nN .sum file(s), M AGENTS.md file(s), P root doc(s), Q AGENTS.local.md to restore\n```\n\n**Completion message:**\n- Dry run: `Dry run — no files were changed.` (yellow)\n- Success: `Deleted N file(s), restored M AGENTS.local.md file(s).` (green)\n- Failure messages logged individually per file\n\n## Integration Points\n\n**Imports:**\n- `GENERATED_MARKER` from `src/generation/writers/agents-md.js` — HTML comment string for filtering\n- `createLogger({ colors: true })` from `src/output/logger.js` — Terminal output with picocolors\n- `fast-glob` — Artifact discovery with ignore patterns\n- Node.js `fs/promises`: `access`, `readFile`, `rename`, `unlink`\n\n**Invoked by:** `src/cli/index.ts` command dispatcher for `/are-clean` skill and `are clean` CLI command.\n### discover.ts\n**Purpose:** discoverCommand() orchestrates file discovery with filter application, GENERATION-PLAN.md creation via post-order tra...\n\n**discoverCommand() orchestrates file discovery with filter application, GENERATION-PLAN.md creation via post-order traversal, and dual-stream progress reporting (console + progress.log).**\n\n## Exported Interface\n\n**discoverCommand(targetPath: string, options: DiscoverOptions): Promise<void>**\n- Resolves `targetPath` to absolute path (defaults to `process.cwd()`)\n- Loads config via `loadConfig(resolvedPath)` with Zod validation defaults\n- Verifies target path exists via `access(resolvedPath, constants.R_OK)`, exits on `ENOENT`/`EACCES`/`EPERM`\n- Invokes `discoverFiles(resolvedPath, config, { tracer, debug })` for gitignore/vendor/binary/custom filtering\n- Emits `discovery:start` and `discovery:end` trace events with `filesIncluded`/`filesExcluded`/`durationMs`\n- Logs included files via `logger.file(rel)`, excluded files via `logger.excluded(rel, excluded.reason, excluded.filter)`\n- Creates `DiscoveryResult` from `result.included` and `result.excluded` arrays\n- Calls `createOrchestrator(config, resolvedPath).createPlan(discoveryResult)` to build `GenerationPlan`\n- Transforms plan via `buildExecutionPlan(generationPlan, resolvedPath)` applying post-order directory traversal\n- Formats plan via `formatExecutionPlanAsMarkdown(executionPlan)` with phase breakdown (file analysis, directory docs, root synthesis)\n- Writes markdown to `.agents-reverse-engineer/GENERATION-PLAN.md` via `mkdir(configDir, { recursive: true })` + `writeFile(planPath, markdown, 'utf8')`\n- Writes parallel progress entries to `ProgressLog` for `tail -f` monitoring\n\n**DiscoverOptions**\n- `tracer?: ITraceWriter` — Optional NDJSON trace emitter for `discovery:start/end` events\n- `debug?: boolean` — Enables `console.error(pc.dim('[debug] ...'))` output with discovery counts\n\n## Progress Reporting Strategy\n\nDual-stream logging writes identical output to console (via `createLogger()`) and `.agents-reverse-engineer/progress.log` (via `ProgressLog.create(resolvedPath)`). Progress log includes ISO 8601 timestamp header `=== ARE Discover (${new Date().toISOString()}) ===`, project path, file inclusion/exclusion details with relative paths, summary counts, and plan creation status. Finalizes via `progressLog.finalize()` before exit.\n\n## Error Handling\n\nThree filesystem error codes handled explicitly:\n- `ENOENT` → `logger.error('Directory not found')` + `process.exit(1)`\n- `EACCES`/`EPERM` → `logger.error('Permission denied')` + `process.exit(1)`\n- Plan write failure → `logger.error('Failed to write plan')` + `progressLog.finalize()` + `process.exit(1)`\n\nAll other errors rethrown via `throw error`.\n\n## Trace Event Emission\n\nEmits two trace event types via `options.tracer?.emit()`:\n- `discovery:start` with `targetPath: resolvedPath`\n- `discovery:end` with `filesIncluded: result.included.length`, `filesExcluded: result.excluded.length`, `durationMs: Number(discoveryEndTime - discoveryStartTime) / 1_000_000` computed from `process.hrtime.bigint()` delta\n\n## Path Relativization\n\nDefines inline helper `relativePath(absPath: string): string => path.relative(resolvedPath, absPath)` to convert discovered file absolute paths to project-relative paths for cleaner console/log output. Applied to all included files (`logger.file(rel)`), excluded files (`logger.excluded(rel, ...)`), and plan path (`logger.info('Created ' + planRelPath)`).\n\n## Dependencies\n\n- `discoverFiles()` from `../discovery/run.js` — Executes filter chain (gitignore/vendor/binary/custom)\n- `createOrchestrator()` from `../generation/orchestrator.js` — Factory for generation plan builder\n- `buildExecutionPlan()` from `../generation/executor.js` — Post-order directory traversal with phase separation\n- `formatExecutionPlanAsMarkdown()` from `../generation/executor.js` — Markdown formatter with file count/phase summaries\n- `loadConfig()` from `../config/loader.js` — YAML config loader with Zod schema defaults\n- `createLogger()` from `../output/logger.js` — Console logger with picocolors formatting\n- `ProgressLog` from `../orchestration/index.js` — Append-only log file writer for `tail -f` monitoring\n- `ITraceWriter` from `../orchestration/trace.js` — NDJSON trace event emitter interface\n### generate.ts\n**Purpose:** generateCommand() orchestrates full three-phase documentation pipeline: discovers files via discoverFiles(), builds G...\n\n**generateCommand() orchestrates full three-phase documentation pipeline: discovers files via discoverFiles(), builds GenerationPlan via createOrchestrator().createPlan(), resolves AI backend via resolveBackend(), creates AIService and CommandRunner, executes concurrent file analysis → directory aggregation → root synthesis via runner.executeGenerate(), writes telemetry/traces, exits with status codes (0=success, 1=partial failure, 2=total failure).**\n\n## Exported Interface\n\n```typescript\ninterface GenerateOptions {\n  dryRun?: boolean;        // Show plan without AI calls\n  concurrency?: number;    // Worker pool size override\n  failFast?: boolean;      // Abort on first failure\n  debug?: boolean;         // Log AI prompts/backend details\n  trace?: boolean;         // Emit NDJSON trace events\n}\n\nasync function generateCommand(\n  targetPath: string,\n  options: GenerateOptions\n): Promise<void>\n```\n\n## Execution Flow\n\ngenerateCommand() executes seven sequential phases:\n\n1. **Path resolution**: `path.resolve(targetPath)` converts relative to absolute path\n2. **Trace initialization**: `createTraceWriter(absolutePath, options.trace ?? false)` creates NDJSON tracer before config/discovery\n3. **Config loading**: `loadConfig(absolutePath, { tracer, debug })` reads `.agents-reverse-engineer/config.yaml` with Zod validation\n4. **File discovery**: `discoverFiles(absolutePath, config, { tracer, debug })` applies filter chain (gitignore/binary/vendor/custom) producing `{ included, excluded }`\n5. **Plan creation**: `createOrchestrator(config, absolutePath, { tracer, debug }).createPlan(discoveryResult)` returns GenerationPlan with `files[]`, `tasks[]`, `complexity`\n6. **Backend resolution**: `resolveBackend(createBackendRegistry(), config.ai.backend)` auto-detects Claude/Gemini/OpenCode CLI availability, throws `AIServiceError` with `code: 'CLI_NOT_FOUND'` if none found, logs `getInstallInstructions(registry)` on stderr\n7. **Pipeline execution**: `buildExecutionPlan(plan, absolutePath)` splits tasks into `fileTasks`/`directoryTasks`/`rootTasks`, `new CommandRunner(aiService, { concurrency, failFast, debug, tracer, progressLog }).executeGenerate(executionPlan)` runs three-phase pool concurrency\n\n## Dry-Run Mode\n\nWhen `options.dryRun === true`, formatPlan() displays plan summary and buildExecutionPlan() shows task breakdown without calling AIService:\n\n```\n--- Dry Run Summary ---\nFiles to analyze:     <fileTasks.length>\nDirectories:          <directoryFileMap keys>\nRoot documents:       <rootTasks.length>\nEstimated AI calls:   <total tasks.length>\n```\n\nExits without backend resolution or AIService instantiation.\n\n## AI Service Configuration\n\nAIService instantiated with backend-specific options merged from config and CLI:\n\n```typescript\nconst aiService = new AIService(backend, {\n  timeoutMs: config.ai.timeoutMs,           // SIGTERM timeout (default 120000ms)\n  maxRetries: config.ai.maxRetries,         // Exponential backoff retries (default 3)\n  model: config.ai.model,                   // Override backend default\n  telemetry: { keepRuns: config.ai.telemetry.keepRuns }  // Retention limit (default 50)\n});\n```\n\nIf `options.debug === true`, calls `aiService.setDebug(true)` enabling subprocess stdout/stderr logging.\n\nIf `options.trace === true`, calls `aiService.setSubprocessLogDir(logDir)` where `logDir = path.join(absolutePath, '.agents-reverse-engineer', 'subprocess-logs', <ISO timestamp>)` to capture per-call output files.\n\n## Progress Logging\n\nProgressLog.create() writes human-readable streaming log to `.agents-reverse-engineer/progress.log`:\n\n```typescript\nconst progressLog = ProgressLog.create(absolutePath);\nprogressLog.write(`=== ARE Generate (${new Date().toISOString()}) ===`);\nprogressLog.write(`Project: ${absolutePath}`);\nprogressLog.write(`Files: ${fileTasks.length} | Directories: ${directoryTasks.length}`);\n```\n\nPassed to CommandRunner constructor, receives task pickup/completion events with ETA calculations. Finalized via `progressLog.finalize()` before exit.\n\n## Exit Codes\n\nProcess exit status determined by RunSummary from runner.executeGenerate():\n\n- **0**: `summary.filesFailed === 0` (all files succeeded or no files to process)\n- **1**: `summary.filesFailed > 0 && summary.filesProcessed > 0` (partial failure)\n- **2**: `summary.filesProcessed === 0 && summary.filesFailed > 0` (total failure, no progress)\n- **2** (CLI not found): `AIServiceError.code === 'CLI_NOT_FOUND'` triggers stderr instructions via `getInstallInstructions(registry)`, calls `process.exit(2)` before pipeline execution\n\n## Trace Cleanup\n\nWhen `options.trace === true`, calls `cleanupOldTraces(absolutePath)` after `tracer.finalize()` to enforce retention limit (default 500 trace files) via mtime-based pruning.\n\n## Dependencies\n\n- **loadConfig**: Zod-validated config loading from `.agents-reverse-engineer/config.yaml`\n- **discoverFiles**: Filter chain execution returning `{ included, excluded }` discovery result\n- **createOrchestrator**: Factory producing orchestrator with `createPlan()` method\n- **buildExecutionPlan**: Splits GenerationPlan into phase-specific task groups (`fileTasks`/`directoryTasks`/`rootTasks`)\n- **resolveBackend**: Auto-detects first available AI CLI (Claude/Gemini/OpenCode) via sequential `isAvailable()` checks\n- **AIService**: Subprocess manager with exponential backoff retry, telemetry logging, resource limits (`NODE_OPTIONS`, `UV_THREADPOOL_SIZE`)\n- **CommandRunner**: Worker pool executor with three-phase dispatch (`executeGenerate()`), progress reporting, trace emission\n- **ProgressLog**: Streaming append-only log writer for `tail -f` monitoring\n- **createTraceWriter**: NDJSON trace event emitter with promise-chain serialization\n- **cleanupOldTraces**: Mtime-based retention enforcement for `.agents-reverse-engineer/traces/` directory\n### index.ts\n**Purpose:** index.ts orchestrates CLI command routing, argument parsing, and interactive installer invocation for the agents-reve...\n\n**index.ts orchestrates CLI command routing, argument parsing, and interactive installer invocation for the agents-reverse-engineer tool.**\n\n## Exported Symbols\n\nNone (entry point script with shebang `#!/usr/bin/env node`).\n\n## Command Routing\n\n`main()` async function parses `process.argv.slice(2)` via `parseArgs()`, routes to command handlers:\n\n- `install` → `runInstaller()` with `parseInstallerArgs()`\n- `uninstall` → `runInstaller()` with `uninstall: true` override\n- `init` → `initCommand(positional[0] || '.', { force })`\n- `clean` → `cleanCommand(positional[0] || '.', { dryRun })`\n- `discover` → `discoverCommand(positional[0] || '.', {})`\n- `generate` → `generateCommand(positional[0] || '.', GenerateOptions)`\n- `update` → `updateCommand(positional[0] || '.', UpdateCommandOptions)`\n- `specify` → `specifyCommand(positional[0] || '.', SpecifyOptions)`\n\n## Argument Parsing\n\n`parseArgs(args: string[])` returns `{ command: string | undefined, positional: string[], flags: Set<string>, values: Map<string, string> }`. Splits args into:\n\n- First non-flag arg → `command`\n- Subsequent non-flag args → `positional[]`\n- `--flagName value` → `values.set('flagName', 'value')`\n- `--flagName` (no value) → `flags.add('flagName')`\n- Short flags `-h`, `-g`, `-l`, `-V` → mapped to long forms (`help`, `global`, `local`, `version`)\n\n## Installer Invocation Modes\n\nThree entry paths to installer:\n\n1. **Interactive mode:** `args.length === 0` → `runInstaller({ global: false, local: false, uninstall: false, force: false, help: false, quiet: false })`\n2. **Direct installer flags:** No command + `hasInstallerFlags()` returns `true` → `runInstaller(parseInstallerArgs(args))` (supports `npx agents-reverse-engineer --runtime claude -g`)\n3. **Explicit install/uninstall command:** `command === 'install'` or `'uninstall'` → `runInstaller(parseInstallerArgs(args))`\n\n`hasInstallerFlags(flags, values)` checks for `global`, `local`, `force`, or `runtime` presence.\n\n## Flag Mappings\n\n`GenerateOptions`:\n- `dryRun: flags.has('dry-run')`\n- `concurrency: values.has('concurrency') ? parseInt(values.get('concurrency')!, 10) : undefined`\n- `failFast: flags.has('fail-fast')`\n- `debug: flags.has('debug')`\n- `trace: flags.has('trace')`\n\n`UpdateCommandOptions` extends `GenerateOptions` with:\n- `uncommitted: flags.has('uncommitted')`\n\n`SpecifyOptions`:\n- `output: values.get('output')`\n- `force: flags.has('force')`\n- `dryRun: flags.has('dry-run')`\n- `multiFile: flags.has('multi-file')`\n- `debug: flags.has('debug')`\n- `trace: flags.has('trace')`\n\n`CleanOptions`:\n- `dryRun: flags.has('dry-run')`\n\n## Version and Help Handling\n\n`showVersion()` prints `agents-reverse-engineer v${VERSION}` via `getVersion()` and exits with code 0.\n\n`showHelp()` prints `USAGE` constant and exits with code 0.\n\n`showVersionBanner()` prints `agents-reverse-engineer v${VERSION}\\n` without exiting (shown before command execution).\n\n`showUnknownCommand(command)` prints error message and exits with code 1.\n\n## Usage Documentation\n\n`USAGE` constant (multiline string) documents:\n\n- Commands: `install`, `uninstall`, `init`, `discover`, `generate`, `update`, `specify`, `clean`\n- Install/Uninstall options: `--runtime <name>`, `-g, --global`, `-l, --local`, `--force`\n- General options: `--debug`, `--trace`, `--dry-run`, `--output <path>`, `--multi-file`, `--concurrency <n>`, `--fail-fast`, `--uncommitted`, `--help, -h`, `--version, -V`\n- Examples: `are install`, `are install --runtime claude -g`, `are uninstall`, `are init`, `are discover`, `are generate --dry-run`, `are generate ./my-project --concurrency 3`, `are update --uncommitted`, `are specify --output ./docs/spec.md --force`\n\n## Error Handling\n\n`main().catch((err: Error) => { console.error(\\`Error: ${err.message}\\`); process.exit(1); })` catches uncaught errors and exits with code 1.\n\n## Dependencies\n\nImports command handlers from:\n- `./init.js` → `initCommand`\n- `./discover.js` → `discoverCommand`\n- `./generate.js` → `generateCommand, GenerateOptions`\n- `./update.js` → `updateCommand, UpdateCommandOptions`\n- `./clean.js` → `cleanCommand, CleanOptions`\n- `./specify.js` → `specifyCommand, SpecifyOptions`\n\nImports installer from `../installer/index.js` → `runInstaller, parseInstallerArgs`.\n\nImports version from `../version.js` → `getVersion()`.\n### init.ts\n**Purpose:** initCommand() creates `.agents-reverse-engineer/config.yaml` with defaults via writeDefaultConfig(), warns if configE...\n\n**initCommand() creates `.agents-reverse-engineer/config.yaml` with defaults via writeDefaultConfig(), warns if configExists() returns true, and exits on EACCES/EPERM errors.**\n\n## Exported Interface\n\n```typescript\nasync function initCommand(root: string, options?: { force?: boolean }): Promise<void>\n```\n\nAccepts `root` (resolved via `path.resolve()`) and optional `options.force` flag (default: `false`). When `force` is false and `configExists(resolvedRoot)` returns true, logs warning and skips creation. Otherwise calls `writeDefaultConfig(resolvedRoot)` to generate configuration file.\n\n## Dependencies\n\n- `configExists()` — Checks for existing configuration at `${root}/${CONFIG_DIR}/${CONFIG_FILE}`\n- `writeDefaultConfig()` — Writes YAML config with documented defaults from `src/config/loader.ts`\n- `CONFIG_DIR` — Constant `'.agents-reverse-engineer'` (config directory name)\n- `CONFIG_FILE` — Constant `'config.yaml'` (config file name)\n- `createLogger()` — Factory from `src/output/logger.ts` instantiated with `{ colors: true }`\n\n## Error Handling\n\nCatches `NodeJS.ErrnoException` and discriminates on `error.code`:\n\n- `'EACCES'` or `'EPERM'` → Logs `\"Permission denied: Cannot create ${configPath}\"` with permission guidance, calls `process.exit(1)`\n- Other errors → Logs `\"Failed to create configuration: ${error.message}\"`, calls `process.exit(1)`\n\n## Output Messages\n\nSuccess path logs:\n- `\"Created configuration at ${configPath}\"`\n- Multi-line guidance referencing `exclude.patterns`, `ai.concurrency` (range: 1-20), `ai.timeoutMs` (default: 300,000ms = 5 minutes), `ai.backend` (values: claude/gemini/opencode/auto)\n- `\"See README.md for full configuration reference.\"`\n\nExisting config path logs:\n- `\"Config already exists at ${configPath}\"`\n- `\"Edit the file to customize exclusions and options.\"`\n\n## File System Operations\n\nConstructs config path via `path.join(resolvedRoot, CONFIG_DIR, CONFIG_FILE)` where:\n- `CONFIG_DIR` = `'.agents-reverse-engineer'`\n- `CONFIG_FILE` = `'config.yaml'`\n\nExample resolved path: `${resolvedRoot}/.agents-reverse-engineer/config.yaml`\n### specify.ts\n**Purpose:** CLI entry point for synthesizing project specifications from AGENTS.md documentation via AI backend, with single/mult...\n\n**CLI entry point for synthesizing project specifications from AGENTS.md documentation via AI backend, with single/multi-file output modes, dry-run preview, and auto-generation fallback.**\n\n## Exported Functions\n\n**`specifyCommand(targetPath: string, options: SpecifyOptions): Promise<void>`** — orchestrates specification generation: loads config via `loadConfig()`, collects AGENTS.md files via `collectAgentsDocs()`, auto-invokes `generateCommand()` if no docs exist, resolves AI backend via `resolveBackend()`, constructs prompt via `buildSpecPrompt()`, calls `AIService.call()` with extended timeout (minimum 600,000ms), writes output via `writeSpec()`, logs telemetry summary with token counts and duration.\n\n**`SpecifyOptions`** — interface with fields: `output?: string` (custom path, defaults to `specs/SPEC.md`), `force?: boolean` (overwrite existing specs), `dryRun?: boolean` (preview without AI calls), `multiFile?: boolean` (split output by directory), `debug?: boolean` (verbose logging), `trace?: boolean` (enable NDJSON trace events).\n\n## Workflow Phases\n\n**Phase 1: Collection and Validation** — resolves absolute path via `path.resolve(targetPath)`, derives `outputPath` from `options.output` or defaults to `path.join(absolutePath, 'specs', 'SPEC.md')`, calls `loadConfig(absolutePath, { debug })`, invokes `collectAgentsDocs(absolutePath)` returning array of `{ path, content }` tuples, auto-generates via `generateCommand(targetPath, { debug, trace })` if `docs.length === 0` and not in dry-run mode, exits with code 1 if no docs found after generation.\n\n**Phase 2: Dry-Run Preview** — computes `totalChars` via `docs.reduce((sum, d) => sum + d.content.length, 0)`, estimates tokens as `Math.ceil(totalChars / 4) / 1000`, prints summary table showing `docs.length`, estimated tokens, `outputPath`, and mode (`single-file` or `multi-file`), warns if no AGENTS.md files exist or if `estimatedTokensK > 150`, returns early without AI calls.\n\n**Phase 3: Backend Resolution and AI Call** — creates registry via `createBackendRegistry()`, resolves backend via `resolveBackend(registry, config.ai.backend)`, catches `AIServiceError` with code `'CLI_NOT_FOUND'` and prints installation instructions via `getInstallInstructions(registry)` before exiting with code 2, instantiates `AIService` with backend and options `{ timeoutMs: Math.max(config.ai.timeoutMs, 600_000), maxRetries, model, telemetry }`, enables debug mode via `aiService.setDebug(true)` if `options.debug` is true, constructs prompt via `buildSpecPrompt(docs)` returning `{ system, user }`, creates `ProgressLog` via `ProgressLog.create(absolutePath)` for tail monitoring, invokes `aiService.call({ prompt: prompt.user, systemPrompt: prompt.system, taskLabel: 'specify' })`.\n\n**Phase 4: Output Writing** — calls `writeSpec(response.text, { outputPath, force, multiFile })` returning `writtenFiles[]` array, catches `SpecExistsError` and logs to progress before exiting with code 1, prints green success message listing each written file, finalizes telemetry via `aiService.finalize(absolutePath)` returning `{ summary }`, logs summary line with format `Tokens: ${totalInputTokens} in / ${totalOutputTokens} out | Duration: ${totalDurationMs/1000}s | Output: ${outputPath}`, writes summary to progress log and finalizes via `progressLog.finalize()`.\n\n## Dependencies\n\n**Core imports:** `path.resolve()`, `path.join()` for path resolution; `picocolors` as `pc` for terminal color formatting (`pc.bold()`, `pc.green()`, `pc.red()`, `pc.yellow()`, `pc.cyan()`, `pc.dim()`).\n\n**Internal modules:** `loadConfig` from `config/loader.js` for YAML config loading; `collectAgentsDocs` from `generation/collector.js` for recursive AGENTS.md traversal; `buildSpecPrompt`, `writeSpec`, `SpecExistsError` from `specify/index.js` for prompt construction and file writing; `AIService`, `AIServiceError`, `createBackendRegistry`, `resolveBackend`, `getInstallInstructions` from `ai/index.js` for backend orchestration; `ProgressLog` from `orchestration/index.js` for streaming log output; `generateCommand` from `./generate.js` for auto-generation fallback.\n\n## Error Handling\n\n**CLI not found:** catches `AIServiceError` with `code === 'CLI_NOT_FOUND'` during backend resolution, prints error message and installation instructions via `getInstallInstructions(registry)`, exits with code 2.\n\n**Spec file exists:** catches `SpecExistsError` during `writeSpec()`, logs to progress via `progressLog.write()`, finalizes log, prints red error message, exits with code 1.\n\n**No docs after generation:** validates `docs.length === 0` after auto-generation attempt, prints red error message `'No AGENTS.md files found after generation. Cannot proceed.'`, exits with code 1.\n\n## Telemetry Integration\n\n**Progress log format:** writes header line `=== ARE Specify (${ISO timestamp}) ===`, project path, AGENTS.md count, generation status, written file paths, token/duration summary line, finalizes via `progressLog.finalize()` for flush.\n\n**Token estimation:** computes `estimatedTokensK = Math.ceil(totalChars / 4) / 1000` using 1 token ≈ 4 characters heuristic, warns if exceeds 150K tokens.\n\n**Telemetry finalization:** invokes `aiService.finalize(absolutePath)` returning `summary` with fields `totalInputTokens`, `totalOutputTokens`, `totalDurationMs`, formats summary line with duration in seconds (`(totalDurationMs / 1000).toFixed(1)`).\n\n## Timeout Configuration\n\n**Extended timeout:** overrides config timeout with minimum 600,000ms (10 minutes) via `Math.max(config.ai.timeoutMs, 600_000)` to accommodate large specification synthesis tasks.\n### update.ts\n**Purpose:** update.ts implements the incremental documentation update CLI command, detecting changed files via SHA-256 hash compa...\n\n**update.ts implements the incremental documentation update CLI command, detecting changed files via SHA-256 hash comparison, spawning concurrent AI analysis via CommandRunner, regenerating AGENTS.md for affected directories, and writing telemetry/trace logs with exit code 0/1/2 for success/partial/total failure.**\n\n## Exported Function\n\n`updateCommand(targetPath: string, options: UpdateCommandOptions): Promise<void>` — Main entry point for the `are update` CLI command. Orchestrates seven-step workflow: (1) loads config via `loadConfig()`, (2) creates `UpdateOrchestrator` via `createUpdateOrchestrator()`, (3) generates `UpdatePlan` via `orchestrator.preparePlan()` with `includeUncommitted`/`dryRun` flags, (4) resolves AI backend via `resolveBackend()` and instantiates `AIService`, (5) executes Phase 1 file analysis via `runner.executeUpdate()` with configured concurrency, (6) executes Phase 2 AGENTS.md regeneration for `plan.affectedDirs` sequentially (concurrency=1), (7) finalizes telemetry via `aiService.finalize()` and records run state via `orchestrator.recordRun()`. Exits with code 0 (all success), 1 (partial failure with `summary.filesFailed > 0`), or 2 (total failure or CLI not found).\n\n## Exported Interface\n\n`UpdateCommandOptions` — Configuration for `updateCommand()` with fields:\n- `uncommitted?: boolean` — Include staged + working directory changes in change detection\n- `dryRun?: boolean` — Display plan without writing files\n- `concurrency?: number` — Override config worker pool size\n- `failFast?: boolean` — Abort on first file analysis error\n- `debug?: boolean` — Enable verbose subprocess logging with heap/RSS metrics\n- `trace?: boolean` — Emit NDJSON trace events to `.agents-reverse-engineer/traces/`\n\n## AI Service Integration\n\nBackend resolution follows pattern: `createBackendRegistry()` → `resolveBackend(registry, config.ai.backend)` → catch `AIServiceError` with `code === 'CLI_NOT_FOUND'` → print `getInstallInstructions(registry)` → `process.exit(2)`. Creates `AIService` instance with timeout/retry/model from config, enables debug mode via `aiService.setDebug(true)` if `options.debug`, configures subprocess log directory via `aiService.setSubprocessLogDir(logDir)` if `options.trace` (path pattern: `.agents-reverse-engineer/subprocess-logs/<ISO-timestamp>/`).\n\n## Update Plan Display\n\n`formatPlan(plan: UpdatePlan): string` — Renders plan summary with sections: baseline commit hash (7-char abbreviated), file counts (`analyzeCount`, `skipCount`, `cleanupCount`), file list with status markers (`+` for added via `pc.green()`, `R` for renamed via `pc.blue()`, `M` for modified via `pc.yellow()`, `=` for unchanged via `pc.dim()`), cleanup actions via `formatCleanup()`, affected directories for AGENTS.md regeneration. Returns early with message \"No changes detected since last run.\" if all counts are zero.\n\n`formatCleanup(plan: UpdatePlan): string[]` — Formats `plan.cleanup.deletedSumFiles` and `plan.cleanup.deletedAgentsMd` arrays with `pc.red('-')` markers under `pc.yellow()` headers \"Cleanup (deleted .sum files):\" and \"Cleanup (deleted AGENTS.md from empty dirs):\".\n\n## Phase Execution\n\n**Phase 1 (File Analysis):** Invokes `runner.executeUpdate(plan.filesToAnalyze, absolutePath, config)` with `CommandRunner` configured with `concurrency` from options/config. Returns `summary` object with `filesProcessed`/`filesFailed` counts. Writes progress to `ProgressLog` instance created via `ProgressLog.create(absolutePath)` with initial metadata: `=== ARE Update (<ISO-timestamp>) ===`, `Project: <absolutePath>`, `Files to analyze: <count> | Directories: <count>`.\n\n**Phase 2 (Directory Regeneration):** Iterates `plan.affectedDirs` sequentially (no worker pool). For each directory: (1) reads existing `AGENTS.md` via `readFile()` and checks for `GENERATED_MARKER`, stores as `existingAgentsMd` context, (2) builds prompt via `buildDirectoryPrompt(dirPath, absolutePath, debug, knownDirs, undefined, existingAgentsMd)` where `knownDirs = new Set(plan.affectedDirs)`, (3) calls `aiService.call()` with prompt, (4) writes output via `writeAgentsMd()`, (5) emits trace events `task:start`/`task:done` with `phase: 'update-phase-dir-regen'`. Emits phase-level events `phase:start`/`phase:end` with `phase: 'update-phase-dir-regen'`, `taskCount`, `tasksCompleted`, `tasksFailed`. Updates `ProgressReporter` via `onDirectoryStart()`/`onDirectoryDone()` with token counts and duration. Catches errors and logs via `pc.yellow('WARN')` without aborting loop (continues to next directory).\n\n## Trace Integration\n\nCreates `TraceWriter` via `createTraceWriter(absolutePath, options.trace ?? false)` before config loading (used in `loadConfig()` and `createUpdateOrchestrator()` options). Passes `tracer` to `CommandRunner` constructor for worker/task event emission. Phase 2 manually emits events with `type: 'phase:start'/'phase:end'/'task:start'/'task:done'`, `phase: 'update-phase-dir-regen'`. Finalizes trace via `tracer.finalize()` and cleans old traces via `cleanupOldTraces(absolutePath)` if `options.trace`. Logs trace file path to stderr via `pc.dim('[trace] Writing to <path>')`.\n\n## Progress Logging\n\nCreates `ProgressLog` instance with header lines: `=== ARE Update (<ISO-timestamp>) ===`, `Project: <absolutePath>`, `Files to analyze: <count> | Directories: <count>`, empty line. Passes `progressLog` to `CommandRunner` and `ProgressReporter` for real-time streaming output. Finalizes via `progressLog.finalize()` after Phase 2 completion. Designed for tail -f monitoring pattern.\n\n## First Run Handling\n\nChecks `plan.isFirstRun` flag after `orchestrator.preparePlan()`. Prints `pc.yellow()` message: \"First run detected. Use 'are generate' for initial documentation.\" followed by \"Hint: Run 'are generate' first to create initial documentation. Then run 'are update' after making changes.\" Returns early without backend resolution or execution phases.\n\n## No-Change Handling\n\nAfter first run check, verifies `plan.filesToAnalyze.length === 0 && plan.cleanup.deletedSumFiles.length === 0 && plan.cleanup.deletedAgentsMd.length === 0`. Prints `pc.green('All files are up to date.')` and returns early.\n\n## Exit Code Strategy\n\nAfter telemetry finalization and run state recording:\n- `process.exit(2)` if `summary.filesProcessed === 0 && summary.filesFailed > 0` (total failure)\n- `process.exit(1)` if `summary.filesFailed > 0` (partial failure)\n- Implicit exit code 0 if all files succeeded or no files to process\n\nAlso uses `process.exit(2)` during backend resolution if `AIServiceError` with `code === 'CLI_NOT_FOUND'` caught.\n\n## Dry Run Mode\n\nIf `options.dryRun` is true, displays plan via `console.log(formatPlan(plan))` then logs \"Dry run complete. No files written.\" and returns early before backend resolution phase. Does not create AI service or execute Phase 1/Phase 2.\n\n## State Recording\n\nCalls `orchestrator.recordRun(plan.currentCommit, summary.filesProcessed, filesSkipped)` after telemetry finalization where `filesSkipped = plan.filesToSkip.length`. Comment indicates this is \"no-op in frontmatter mode, kept for API compatibility\" (legacy from state file approach before SHA-256 hash migration).\n\n## Dependencies\n\nImports: `path` (Node.js), `readFile` from `node:fs/promises`, `pc` (picocolors), `loadConfig` from `../config/loader.js`, `createLogger` from `../output/logger.js`, `createUpdateOrchestrator`/`UpdatePlan` from `../update/index.js`, `writeAgentsMd`/`GENERATED_MARKER` from `../generation/writers/agents-md.js`, `buildDirectoryPrompt` from `../generation/prompts/index.js`, `AIService`/`AIServiceError`/`createBackendRegistry`/`resolveBackend`/`getInstallInstructions` from `../ai/index.js`, `CommandRunner`/`ProgressReporter`/`ProgressLog`/`createTraceWriter`/`cleanupOldTraces` from `../orchestration/index.js`.\n\n## Import Map (verified — use these exact paths)\n\nclean.ts:\n  ../output/logger.js → createLogger\n  ../generation/writers/agents-md.js → GENERATED_MARKER\n\ndiscover.ts:\n  ../config/loader.js → loadConfig\n  ../discovery/run.js → discoverFiles\n  ../output/logger.js → createLogger\n  ../generation/orchestrator.js → createOrchestrator\n  ../generation/executor.js → buildExecutionPlan, formatExecutionPlanAsMarkdown\n  ../orchestration/index.js → ProgressLog\n  ../types/index.js → DiscoveryResult (type)\n  ../orchestration/trace.js → ITraceWriter (type)\n\ngenerate.ts:\n  ../config/loader.js → loadConfig\n  ../output/logger.js → createLogger\n  ../discovery/run.js → discoverFiles\n  ../generation/orchestrator.js → createOrchestrator, type GenerationPlan\n  ../generation/executor.js → buildExecutionPlan\n  ../ai/index.js → AIService, AIServiceError, createBackendRegistry, resolveBackend, getInstallInstructions\n  ../orchestration/index.js → CommandRunner, ProgressLog, createTraceWriter, cleanupOldTraces\n\nindex.ts:\n  ../installer/index.js → runInstaller, parseInstallerArgs\n  ../version.js → getVersion\n\ninit.ts:\n  ../config/loader.js → configExists, writeDefaultConfig, CONFIG_DIR, CONFIG_FILE\n  ../output/logger.js → createLogger\n\nspecify.ts:\n  ../config/loader.js → loadConfig\n  ../generation/collector.js → collectAgentsDocs\n  ../specify/index.js → buildSpecPrompt, writeSpec, SpecExistsError\n  ../ai/index.js → AIService, AIServiceError, createBackendRegistry, resolveBackend, getInstallInstructions\n  ../orchestration/index.js → ProgressLog\n\nupdate.ts:\n  ../config/loader.js → loadConfig\n  ../output/logger.js → createLogger\n  ../update/index.js → createUpdateOrchestrator, type UpdatePlan\n  ../generation/writers/agents-md.js → writeAgentsMd, GENERATED_MARKER\n  ../generation/prompts/index.js → buildDirectoryPrompt\n  ../ai/index.js → AIService, AIServiceError, createBackendRegistry, resolveBackend, getInstallInstructions\n  ../orchestration/index.js → CommandRunner, ProgressReporter, ProgressLog, createTraceWriter, cleanupOldTraces\n\n## Project Directory Structure\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>",
      "systemPrompt": "You are generating an AGENTS.md file — a directory-level overview for AI coding assistants.\n\nCRITICAL: Output ONLY the raw markdown content. No code fences, no preamble, no explanations, no conversational text. Your entire response IS the AGENTS.md file content.\n\nOUTPUT FORMAT:\n- First line MUST be exactly: <!-- Generated by agents-reverse-engineer -->\n- Use a # heading with the directory name\n- Write a one-paragraph purpose statement for the directory\nusage\nADAPTIVE SECTIONS:\nAnalyze the directory contents and choose the most relevant sections. Do NOT use a fixed template. Instead, select sections that best document this specific directory for an AI that needs to reconstruct or extend the project.\n\nConsider these section types (choose what applies):\n- **Contents**: Group files by purpose/category under ## headings. For each file: markdown link [filename](./filename) and a one-line description.\n- **Subdirectories**: If subdirectories exist, list them with links [dirname/](./dirname/) and brief summaries.\n- **Architecture / Data Flow**: If files form a pipeline, request/response chain, or layered architecture, document it.\n- **Stack**: If this is a package root (has package.json, Cargo.toml, go.mod, etc.), document the technology stack, key scripts, and entry points.\n- **Structure**: If the directory layout follows a convention (feature-sliced, domain-driven, MVC, etc.), document it.\n- **Patterns**: If files share recurring design patterns (factory, strategy, middleware, barrel re-export), name and document them.\n- **Configuration**: If the directory contains config files, schemas, or environment definitions, document the config surface area.\n- **API Surface**: If the directory exports a public API (barrel index, route definitions, SDK), document the interface contract.\n- **File Relationships**: How files collaborate, depend on each other, or share state.\n- **Behavioral Contracts**: If files contain regex patterns, format specifications, magic constants, or template strings that define observable behavior, collect them in a dedicated section. Preserve verbatim patterns from file summaries — do NOT paraphrase regex into prose. This section is MANDATORY when file summaries contain behavioral artifacts.\n\nChoose any relevant sections or create your own based on the directory contents. The goal is to provide a comprehensive overview that captures the essence of the directory's role in the project and how its files work together, with a focus on what an AI coding assistant would need to know to effectively interact with this code.\n\nSCOPE:\n- AGENTS.md is a NAVIGATIONAL INDEX — help an AI find the right file quickly\n- Focus on: what each file does, how files relate, directory-level patterns\n- Do NOT reproduce full architecture sections — those belong in the root CLAUDE.md\n\nPATH ACCURACY (MANDATORY):\n- When referencing files or modules outside this directory, use ONLY paths from the \"Import Map\" section\n- Do NOT invent, rename, or guess module paths — if a path isn't in the Import Map, don't reference it\n- Use the exact directory names from \"Project Directory Structure\" — do NOT rename directories\n  (e.g., if the directory is called \"cli\", write \"src/cli/\", NOT \"src/commands/\")\n- Cross-module references must use the specifier format from actual import statements\n  (e.g., \"../generation/writers/sum.js\", NOT \"../fs/sum-file.js\")\n- If you are unsure about a path, omit the cross-reference rather than guessing\n\nCONSISTENCY (MANDATORY):\n- Do not contradict yourself within the same document\n- If you describe a technique (e.g., \"regex-based\"), do not call it something else later (e.g., \"AST-based\")\n- When stating version numbers, engines, or config fields, use ONLY values present in the file summaries\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this directory\", \"this module\", \"provides\", \"responsible for\", \"is used to\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Per-file descriptions in Contents sections: 1-2 sentences maximum. Reference key symbols but do not reproduce full summaries.\n- Behavioral contracts (regex patterns, format specs, constants) belong in a separate Behavioral Contracts section, not in per-file descriptions.\n- Subdirectory descriptions: 1-2 sentences maximum. Capture the directory's role, not its full contents.\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- Key exported symbols from file summaries MUST appear in the directory overview\n- Preserve exact casing of identifiers\n\nUSER NOTES:\n- If \"User Notes\" are provided in the prompt, they contain user-defined instructions that will be automatically prepended to your output\n- Do NOT repeat or paraphrase user notes in your generated content — they are included separately\n- You may reference information from user notes for context",
      "response": "<!-- Generated by agents-reverse-engineer -->\n\n# src/cli\n\nCLI command entry points for the agents-reverse-engineer tool, routing parsed arguments to specialized command handlers and orchestrating multi-phase documentation generation workflows.\n\n## Contents\n\n### [clean.ts](./clean.ts)\n`cleanCommand()` deletes generated artifacts (`.sum`, `AGENTS.md` with marker, `CLAUDE.md`, `GENERATION-PLAN.md`) via `fast-glob` discovery, filters `AGENTS.md` by `GENERATED_MARKER` presence, restores `AGENTS.local.md` backups, preserves user-authored files.\n\n### [discover.ts](./discover.ts)\n`discoverCommand()` executes file discovery via `discoverFiles()` filter chain, builds `GenerationPlan` via `createOrchestrator().createPlan()`, transforms via `buildExecutionPlan()` post-order traversal, writes markdown to `GENERATION-PLAN.md`, emits `discovery:start/end` trace events with `filesIncluded/filesExcluded/durationMs` metrics.\n\n### [generate.ts](./generate.ts)\n`generateCommand()` orchestrates three-phase pipeline: resolves AI backend via `resolveBackend()`, instantiates `AIService` with timeout/retry/model config, executes `runner.executeGenerate()` for concurrent file analysis → directory aggregation → root synthesis, writes telemetry/traces, exits with status 0/1/2 for success/partial/total failure.\n\n### [index.ts](./index.ts)\nMain entry point (`#!/usr/bin/env node`) parses `process.argv` via `parseArgs()`, routes to command handlers (`initCommand`, `cleanCommand`, `discoverCommand`, `generateCommand`, `updateCommand`, `specifyCommand`), invokes `runInstaller()` for install/uninstall/interactive modes, prints version via `getVersion()` and usage via `USAGE` constant.\n\n### [init.ts](./init.ts)\n`initCommand()` creates `.agents-reverse-engineer/config.yaml` via `writeDefaultConfig()`, checks existence via `configExists()`, warns on `force: false` when config exists, exits on `EACCES/EPERM` errors, logs guidance referencing `exclude.patterns`, `ai.concurrency`, `ai.backend`.\n\n### [specify.ts](./specify.ts)\n`specifyCommand()` synthesizes project specifications from `AGENTS.md` corpus: collects docs via `collectAgentsDocs()`, auto-invokes `generateCommand()` if no docs exist, resolves backend via `resolveBackend()`, constructs prompt via `buildSpecPrompt()`, calls `AIService.call()` with extended timeout (≥600,000ms), writes output via `writeSpec()`, logs telemetry with token/duration summary.\n\n### [update.ts](./update.ts)\n`updateCommand()` executes incremental update workflow: generates `UpdatePlan` via `createUpdateOrchestrator().preparePlan()` with SHA-256 hash comparison, spawns concurrent AI analysis via `runner.executeUpdate()` for `filesToAnalyze`, regenerates `AGENTS.md` sequentially for `affectedDirs`, finalizes telemetry via `aiService.finalize()`, exits with code 0/1/2 for success/partial/total failure.\n\n## Command Routing Logic\n\n`index.ts` implements dual argument parsing paths: (1) interactive installer mode when `args.length === 0`, (2) explicit command routing when first non-flag arg matches `install|uninstall|init|clean|discover|generate|update|specify`. Installer flags (`--global/-g`, `--local/-l`, `--runtime`, `--force`) trigger `runInstaller()` with `parseInstallerArgs()`. Short flags (`-h`, `-g`, `-l`, `-V`) mapped to long forms (`help`, `global`, `local`, `version`) via inline switch statement. Flags parsed into `Set<string>` for boolean presence checks, `Map<string, string>` for key-value pairs (`--concurrency 3`, `--output ./path`). Positional args extracted after command via filter for non-flag strings (missing leading `--` or `-`).\n\n## Shared Options Pattern\n\n`GenerateOptions` interface defines six fields: `dryRun`, `concurrency`, `failFast`, `debug`, `trace`, reused by `generate.ts`/`update.ts`. `UpdateCommandOptions` extends with `uncommitted` boolean for working tree inclusion. `SpecifyOptions` adds `output` string, `force` boolean, `multiFile` boolean for directory-split output. `CleanOptions` contains only `dryRun`. Option construction in `index.ts` uses `flags.has()` for boolean checks, `values.get()` with `parseInt()` for numeric coercion, `values.get()` for string paths.\n\n## Progress Logging Strategy\n\n`discover.ts`, `generate.ts`, `update.ts`, `specify.ts` create `ProgressLog` instances via `ProgressLog.create(absolutePath)` writing to `.agents-reverse-engineer/progress.log` with ISO 8601 timestamp headers (`=== ARE <Command> (${new Date().toISOString()}) ===`), project path, file counts, phase summaries. `CommandRunner` receives `progressLog` in constructor, streams task pickup/completion events with ETA calculations. Finalized via `progressLog.finalize()` before exit. Designed for `tail -f` monitoring pattern (documented in CLAUDE.md \"Progress log\" section).\n\n## Trace Integration\n\n`generate.ts`, `update.ts`, `discover.ts` support `--trace` flag enabling NDJSON trace emission via `createTraceWriter(absolutePath, options.trace ?? false)` called before config loading. Tracer passed to `loadConfig()`, `createUpdateOrchestrator()`, `CommandRunner`, `discoverFiles()` as `options.tracer`. Events include `phase:start/end`, `worker:start/end`, `task:pickup/done`, `subprocess:spawn/exit`, `retry`, `discovery:start/end`. Finalized via `tracer.finalize()` followed by `cleanupOldTraces(absolutePath)` enforcing 500-file retention limit. Trace paths logged to stderr via `pc.dim('[trace] Writing to <path>')`.\n\n## Backend Resolution Workflow\n\n`generate.ts`, `update.ts`, `specify.ts` resolve AI backend via four-step pattern: (1) create registry via `createBackendRegistry()`, (2) call `resolveBackend(registry, config.ai.backend)` returning first available CLI (Claude/Gemini/OpenCode) or throwing `AIServiceError` with `code: 'CLI_NOT_FOUND'`, (3) catch error and print installation instructions via `getInstallInstructions(registry)` to stderr, (4) call `process.exit(2)` on CLI absence. Instantiate `AIService` with backend and config options `{ timeoutMs, maxRetries, model, telemetry }`, enable debug mode via `aiService.setDebug(true)` if `options.debug`, configure subprocess log directory via `aiService.setSubprocessLogDir(logDir)` if `options.trace`.\n\n## Exit Code Strategy\n\nCommands use three exit codes consistently: **0** for success (all files succeeded or no files to process), **1** for partial failure (`summary.filesFailed > 0 && summary.filesProcessed > 0`), **2** for total failure (`summary.filesProcessed === 0 && summary.filesFailed > 0`) or CLI not found during backend resolution. `update.ts` exits early with implicit code 0 on first run detection (`plan.isFirstRun`) or no changes (`filesToAnalyze.length === 0 && cleanup counts === 0`). `specify.ts` exits with code 1 on `SpecExistsError` during `writeSpec()`, code 2 on `AIServiceError` with `CLI_NOT_FOUND`. `init.ts` exits with code 1 on `EACCES/EPERM` during `writeDefaultConfig()`.\n\n## Dry-Run Mode Handling\n\n`generate.ts` shows plan summary via `formatPlan()` and task breakdown via `buildExecutionPlan()` without backend resolution when `options.dryRun === true`. `update.ts` displays plan via `formatPlan(plan)` including baseline commit, file statuses (`+` added, `R` renamed, `M` modified, `=` unchanged), cleanup actions, affected directories, exits before AI service instantiation. `specify.ts` computes `estimatedTokensK = Math.ceil(totalChars / 4) / 1000`, prints doc count/token estimate/output path/mode table, warns if exceeds 150K tokens. `clean.ts` enumerates artifacts to delete with `pc.dim()` paths, skips `unlink()`/`rename()` calls, prints `Dry run — no files were changed.` in yellow.\n\n## Telemetry Finalization\n\n`generate.ts`, `update.ts`, `specify.ts` finalize telemetry via `aiService.finalize(absolutePath)` returning `summary` with fields `totalInputTokens`, `totalOutputTokens`, `totalDurationMs`, `filesRead[]`. Log summary line formatted as `Tokens: ${totalInputTokens} in / ${totalOutputTokens} out | Duration: ${(totalDurationMs / 1000).toFixed(1)}s | Output: ${outputPath}`. `update.ts` records run state via `orchestrator.recordRun(currentCommit, filesProcessed, filesSkipped)` after finalization (legacy no-op since frontmatter hash migration).\n\n## Auto-Generation Fallback\n\n`specify.ts` checks `docs.length === 0` after `collectAgentsDocs()`, invokes `generateCommand(targetPath, { debug, trace })` when no AGENTS.md files exist and not in dry-run mode, re-collects docs after generation, exits with code 1 if still zero. Prints `No AGENTS.md files found after generation. Cannot proceed.` in red. Prevents specification synthesis with empty corpus.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 2408,
      "cacheReadTokens": 17477,
      "cacheCreationTokens": 19403,
      "latencyMs": 87236,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": []
    },
    {
      "timestamp": "2026-02-09T16:13:39.275Z",
      "prompt": "Generate AGENTS.md for directory: \"src/orchestration\" (orchestration)\n\n## File Summaries (7 files)\n\n### index.ts\n**Purpose:** Barrel export module exposing the complete orchestration API: runPool, ProgressReporter, ProgressLog, PlanTracker, Co...\n\n**Barrel export module exposing the complete orchestration API: runPool, ProgressReporter, ProgressLog, PlanTracker, CommandRunner, ITraceWriter, TraceEvent, TraceEventPayload, createTraceWriter, cleanupOldTraces, and shared types (FileTaskResult, RunSummary, ProgressEvent, CommandRunOptions, PoolOptions, TaskResult).**\n\n## Exported Types\n\n**FileTaskResult** — Outcome of a single file analysis task (success/failure with error details).\n\n**RunSummary** — Aggregated metrics from CommandRunner execution (total files, errors, duration, token counts).\n\n**ProgressEvent** — Event payload emitted during pool execution for progress tracking.\n\n**CommandRunOptions** — Configuration object passed to CommandRunner methods (includes concurrency, timeout, tracer, failFast flags).\n\n**PoolOptions** — Configuration for runPool including concurrency limit and optional tracer.\n\n**TaskResult** — Generic result type from runPool execution (success/failure discriminated union).\n\n**ITraceWriter** — Interface defining emit() method for NDJSON trace event emission.\n\n**TraceEvent** — Union type of all trace event payloads (phase:start/end, worker:start/end, task:pickup/done, subprocess:spawn/exit, retry).\n\n**TraceEventPayload** — Base payload structure for all trace events (seq, ts, pid, elapsedMs).\n\n## Exported Functions\n\n**runPool** — Iterator-based worker pool executing async tasks with shared iterator across N workers, returns aggregated results with error handling.\n\n**createTraceWriter** — Factory function returning TraceWriter instance (writes NDJSON to `.agents-reverse-engineer/traces/`) or NullTraceWriter if tracing disabled.\n\n**cleanupOldTraces** — Deletes old trace files from `.agents-reverse-engineer/traces/` directory, retaining only the most recent keepCount files (default 500).\n\n## Exported Classes\n\n**ProgressReporter** — Streaming progress reporter with ETA calculation via moving average of last 10 task durations, emits ProgressEvent objects, supports spinner UI via ora integration.\n\n**ProgressLog** — File-based progress logger writing human-readable output to `.agents-reverse-engineer/progress.log`, mirrors console output for real-time monitoring.\n\n**PlanTracker** — Serialized writer for `GENERATION-PLAN.md` using promise-chain pattern to prevent corruption from concurrent workers, tracks phase breakdown with file counts.\n\n**CommandRunner** — High-level orchestrator executing three-phase pipeline (file analysis, directory aggregation, root synthesis) with configurable concurrency, timeout, and tracing.\n\n## Module Organization\n\nRe-exports from five submodules:\n- `./types.js` — Shared interfaces for task results and run configuration\n- `./pool.js` — Concurrency pool implementation with shared iterator pattern\n- `./progress.js` — Progress reporting with console and file logging\n- `./plan-tracker.js` — Serialized plan file writer\n- `./trace.js` — NDJSON trace event emission with cleanup utilities\n- `./runner.js` — Three-phase pipeline orchestration\n\n## Usage Pattern\n\nPrimary entry point for orchestration layer. Import CommandRunner for high-level three-phase execution, or runPool for custom concurrent task processing. ProgressReporter provides real-time feedback with ETA calculation. TraceWriter enables detailed execution telemetry when `--trace` flag passed via CommandRunOptions.\n### plan-tracker.ts\n**Purpose:** PlanTracker maintains in-memory markdown state for GENERATION-PLAN.md and serializes concurrent checkbox updates via ...\n\n**PlanTracker maintains in-memory markdown state for GENERATION-PLAN.md and serializes concurrent checkbox updates via promise-chain writes during three-phase generation.**\n\n## Exported Interface\n\n**PlanTracker** class with constructor signature `constructor(projectRoot: string, initialMarkdown: string)` stores plan path as `path.join(projectRoot, CONFIG_DIR, 'GENERATION-PLAN.md')` and initializes `content: string` field with provided markdown. Imports `CONFIG_DIR` from `../config/loader.js` (constant value `.agents-reverse-engineer`).\n\n**initialize()** async method creates parent directory via `mkdir(path.dirname(this.planPath), { recursive: true })` and writes `this.content` to disk with `writeFile(this.planPath, this.content, 'utf8')`. Swallows exceptions (non-critical operation—generation continues without tracking).\n\n**markDone(itemPath: string)** synchronous method replaces checkbox via exact string match: `- [ ] \\`${itemPath}\\`` → `- [x] \\`${itemPath}\\``. Returns early if no replacement occurred (no match). Chains `writeFile(this.planPath, this.content, 'utf8')` onto `this.writeQueue` promise field, catching errors silently (non-critical).\n\n**flush()** async method awaits `this.writeQueue` to ensure all pending writes complete before exiting.\n\n## Serialization Pattern\n\n**writeQueue** field (type `Promise<void>`, initialized as `Promise.resolve()`) forms promise chain preventing concurrent `markDone()` calls from corrupting disk file. Each `markDone()` invocation updates in-memory `content`, then chains: `this.writeQueue = this.writeQueue.then(() => writeFile(...)).catch(() => {})`. Subsequent calls wait for previous write to finish before starting next write.\n\n## Path Format Requirements\n\nCallers must pass `itemPath` matching exact markdown format used in GENERATION-PLAN.md:\n- File task: `src/cli/init.ts` (relative source path)\n- Directory task: `src/cli/AGENTS.md` (caller appends `/AGENTS.md` suffix)\n- Root document task: `CLAUDE.md`, `GEMINI.md`, `OPENCODE.md` (root-level filenames)\n\nCheckbox pattern uses backtick-wrapped paths: `` `${itemPath}` ``.\n\n## Integration Context\n\nCreated once per `executeGenerate()` invocation in `src/generation/executor.ts`. Workers call `markDone()` from concurrent Phase 1 file analysis pool. Sequential Phase 2 directory aggregation and Phase 3 root synthesis also call `markDone()` but without concurrency concerns. Final `flush()` call before `executeGenerate()` returns ensures all queued writes complete.\n### pool.ts\n**Purpose:** Iterator-based concurrency pool implementing the shared-iterator worker pattern to execute N async tasks concurrently...\n\n**Iterator-based concurrency pool implementing the shared-iterator worker pattern to execute N async tasks concurrently without idling workers between batches.**\n\n## Exported Functions\n\n`runPool<T>(tasks: Array<() => Promise<T>>, options: PoolOptions, onComplete?: (result: TaskResult<T>) => void): Promise<TaskResult<T>[]>` executes an array of async task factories through a concurrency-limited pool where all workers share a single `entries()` iterator. Each worker pulls the next task immediately after completing the previous one, ensuring all N worker slots stay busy until the iterator exhausts. Returns an array of `TaskResult<T>` indexed by original task position (may be sparse if `failFast` aborts early).\n\n## Core Types\n\n`PoolOptions` configures pool behavior with required `concurrency: number` (maximum concurrent workers), optional `failFast?: boolean` (stop pulling new tasks on first error), optional `tracer?: ITraceWriter` (emit trace events for debugging), optional `phaseLabel?: string` (phase identifier for trace events like `'phase-1-files'`), and optional `taskLabels?: string[]` (human-readable labels for each task by index, used in trace events).\n\n`TaskResult<T>` represents a single task execution outcome with `index: number` (zero-based position in original array), `success: boolean` (completion status), optional `value?: T` (resolved value when `success: true`), and optional `error?: Error` (error when `success: false`).\n\n## Worker Pattern Implementation\n\n`runPool()` creates a shared `iterator` via `tasks.entries()` which yields `[index, taskFn]` tuples. Spawns `effectiveConcurrency = Math.min(options.concurrency, tasks.length)` workers, each invoking the internal `worker(iterator, workerId)` async function. Each worker iterates over the shared iterator via `for (const [index, task] of iterator)`, executing `await task()`, storing results in `results[index]`, invoking `onComplete?.(result)` callback, and checking `aborted` flag before pulling next task. Uses `Promise.allSettled(workers)` to wait for all workers to finish.\n\n## Abort Mechanism\n\nThe shared mutable `aborted` boolean flag stops task pickup when `options.failFast` is true and any worker encounters an error. Workers check `if (aborted) break` before pulling the next task from the iterator, but do not interrupt already-running tasks.\n\n## Trace Event Emission\n\nEmits `worker:start` with `workerId`, `phase` when worker begins. Emits `task:pickup` with `workerId`, `taskIndex`, `taskLabel`, `activeTasks` when worker pulls a task. Emits `task:done` with `workerId`, `taskIndex`, `taskLabel`, `durationMs`, `success`, optional `error`, `activeTasks` when task settles. Emits `worker:end` with `workerId`, `phase`, `tasksExecuted` when worker exhausts iterator or aborts. The `activeTasks` counter increments before task execution and decrements after task settles, providing snapshot of concurrent task count.\n\n## Error Handling\n\nCatches task execution errors via try-catch, converts non-Error exceptions to Error via `err instanceof Error ? err : new Error(String(err))`, stores `TaskResult<T>` with `success: false` and `error` field. When `failFast: false` (default), worker continues pulling tasks after error. When `failFast: true`, worker sets `aborted = true` and breaks from iterator loop.\n\n## Dependency on External Modules\n\nImports `ITraceWriter` from `'./trace.js'` (module-relative path with `.js` extension for ES module resolution). No runtime dependencies on external npm packages (zero-dependency concurrency limiter).\n\n## Anti-Pattern Mitigation\n\nAvoids batch-based concurrency where `Promise.all()` on N-sized chunks idles workers while waiting for the slowest task in each batch. The shared iterator ensures new tasks start immediately as workers become available, maximizing throughput.\n### progress.ts\n**Purpose:** ProgressReporter streams colored build-log progress events with ETA calculation via moving average of completion time...\n\n**ProgressReporter streams colored build-log progress events with ETA calculation via moving average of completion times, optionally mirroring ANSI-stripped output to `.agents-reverse-engineer/progress.log` through ProgressLog for `tail -f` monitoring in buffered environments.**\n\n## Exported Classes\n\n**ProgressLog**: Promise-chain serialized file writer for plain-text progress logging.\n- `static create(projectRoot: string): ProgressLog` — creates instance with path `<projectRoot>/.agents-reverse-engineer/progress.log`\n- `write(line: string): void` — appends line to log file, creating parent directory and opening handle on first call in truncate mode ('w'), silently swallows write errors\n- `async finalize(): Promise<void>` — flushes pending writes and closes file handle\n- Uses pattern `writeQueue: Promise<void>` with chained `.then()` for serialization (same as TraceWriter)\n- Private fields: `filePath: string`, `fd: FileHandle | null`, `writeQueue: Promise<void>`\n\n**ProgressReporter**: Streaming progress reporter with ETA calculation for file/directory/root tasks.\n- `constructor(totalFiles: number, totalDirectories: number = 0, progressLog?: ProgressLog)` — initializes with task counts and optional log mirror\n- `onFileStart(filePath: string): void` — logs `[X/Y] ANALYZING path` with cyan color\n- `onFileDone(filePath: string, durationMs: number, tokensIn: number, tokensOut: number, model: string, cacheReadTokens = 0, cacheCreationTokens = 0): void` — logs `[X/Y] DONE path Xs in/out tok model ~ETA` with green color, records completion time for ETA\n- `onFileError(filePath: string, error: string): void` — logs `[X/Y] FAIL path error` with red color\n- `onDirectoryStart(dirPath: string): void` — logs `[dir X/Y] ANALYZING dirPath/AGENTS.md` with cyan color\n- `onDirectoryDone(dirPath: string, durationMs: number, tokensIn: number, tokensOut: number, model: string, cacheReadTokens = 0, cacheCreationTokens = 0): void` — logs `[dir X/Y] DONE dirPath/AGENTS.md Xs in/out tok model ~ETA` with blue color, records directory completion time\n- `onRootDone(docPath: string): void` — logs `[root] DONE docPath` with blue color\n- `printSummary(summary: RunSummary): void` — prints final summary with ARE version, files processed/failed/skipped, total calls, tokens (input + cache read + cache creation / output), cache statistics, files read (total and unique), elapsed time, errors, retries\n\n## ETA Calculation Algorithm\n\n- **File ETA**: `formatETA()` computes moving average of last `windowSize` (10) completion times from `completionTimes` array, multiplies by remaining file count (`totalFiles - completed - failed`), formats as `~Ns remaining` or `~Mm Ss remaining`, returns empty string if fewer than 2 completions\n- **Directory ETA**: `formatDirectoryETA()` uses same algorithm with `dirCompletionTimes` array and `totalDirectories - dirCompleted` remaining count\n- Sliding window maintenance: `completionTimes.push(durationMs); if (completionTimes.length > windowSize) completionTimes.shift()`\n\n## State Tracking\n\nProgressReporter maintains counters:\n- `totalFiles: number`, `totalDirectories: number` — immutable task counts\n- `started: number`, `completed: number`, `failed: number` — file task progress\n- `dirStarted: number`, `dirCompleted: number` — directory task progress\n- `completionTimes: number[]`, `dirCompletionTimes: number[]` — sliding windows for ETA (max length 10)\n- `startTime: number` — timestamp from `Date.now()` at construction for total elapsed calculation\n- `progressLog: ProgressLog | null` — optional file mirror\n\n## Output Format\n\nAll console output uses `console.log()` for atomic writes. Colored via `picocolors`:\n- `pc.dim()` — counters, timestamps, token counts, model labels, ETA\n- `pc.cyan('ANALYZING')` — file/directory start events\n- `pc.green('DONE')` — file completion\n- `pc.blue('DONE')` — directory/root completion\n- `pc.red('FAIL')` — file errors\n- `pc.bold()` — summary header\n- `pc.yellow()` — skipped files count\n\n## ANSI Stripping\n\n`stripAnsi(str: string): string` removes ANSI escape codes via regex `/\\x1b\\[[0-9;]*m/g` for plain-text log output.\n\n## Token Accounting\n\nTotal input tokens calculated as `tokensIn + cacheReadTokens + cacheCreationTokens` for display in completion messages and summary. Output tokens tracked separately.\n\n## Integration Points\n\n- Imported by `src/orchestration/runner.ts` and `src/generation/executor.ts` for progress reporting during pool execution\n- Consumes `RunSummary` type from `src/orchestration/types.ts` for summary formatting\n- Mirrors output to `PROGRESS_LOG_FILENAME` constant (`'progress.log'`) within `.agents-reverse-engineer/` directory\n### runner.ts\n**Purpose:** CommandRunner orchestrates AI-driven three-phase documentation generation (file analysis, directory AGENTS.md aggrega...\n\n**CommandRunner orchestrates AI-driven three-phase documentation generation (file analysis, directory AGENTS.md aggregation, root document synthesis) with concurrency pool execution, quality validation, telemetry integration, and trace emission.**\n\n## Exported Classes\n\n**CommandRunner** — Main orchestration class accepting `AIService` instance and `CommandRunOptions`, exposes `executeGenerate(plan: ExecutionPlan): Promise<RunSummary>` for full three-phase generation and `executeUpdate(filesToAnalyze: FileChange[], projectRoot: string, config: Config): Promise<RunSummary>` for incremental file re-analysis.\n\nConstructor accepts `aiService: AIService` and `options: CommandRunOptions`, optionally wires `options.tracer` into AI service via `setTracer()` for subprocess/retry event emission.\n\n## Three-Phase Execution Pipeline\n\n**Phase 1: File Analysis (executeGenerate lines 84-259)** — Pre-phase caches existing `.sum` files via `readSumFile()` at concurrency=20 for stale-doc detection. Maps file tasks to async functions reading source via `readFile()`, computing `contentHash` via `computeContentHashFromString()`, calling `aiService.call()` with `buildFilePrompt()` output, stripping preamble via `stripPreamble()`, extracting purpose via `extractPurpose()`, constructing `SumFileContent` with `generatedAt`/`contentHash`/`summary`/`metadata.purpose`, writing via `writeSumFile()`. Executes via `runPool(fileTasks, { concurrency, failFast, tracer, phaseLabel: 'phase-1-files', taskLabels })` with progress callback invoking `reporter.onFileDone()` and `planTracker.markDone()`. Caches `sourceContent` in `Map<string, string>` for quality checks.\n\n**Post-Phase 1 Quality (lines 261-362)** — Groups processed files by directory via `path.dirname()` into `Map<string, string[]>`. Runs throttled (concurrency=10) validation per directory group: reads cached source content, compares against old `.sum` via `checkCodeVsDoc(sourceContent, oldSum, filePath)` appending `(stale documentation)` label, reads fresh `.sum` via `readSumFile()` and runs second `checkCodeVsDoc()` pass, aggregates `filesForCodeVsCode` array and runs `checkCodeVsCode()` for duplicate symbol detection. Flattens results, builds `InconsistencyReport` via `buildInconsistencyReport()`, prints via `formatReportForCli()`, clears `sourceContentCache`.\n\n**Phase 2: Directory AGENTS.md (lines 364-445)** — Groups `plan.directoryTasks` by `metadata.depth` into `Map<number, ExecutionTask[]>`, sorts depth levels descending (deepest-first post-order traversal). For each depth level: builds `phaseLabel` as `phase-2-dirs-depth-${depth}`, computes `dirConcurrency = Math.min(options.concurrency, dirsAtDepth.length)`, emits `phase:start` trace event, maps directory tasks to async functions calling `buildDirectoryPrompt(dirTask.absolutePath, projectRoot, debug, knownDirs, projectStructure)` with `knownDirs` set for filtering, calls `aiService.call()` with prompt, writes via `writeAgentsMd(dirTask.absolutePath, projectRoot, dirResponse.text)`, updates reporter and plan tracker. Executes via `runPool()` with `phaseLabel`/`taskLabels`, emits `phase:end` with `tasksCompleted`/`tasksFailed` counts.\n\n**Post-Phase 2 Quality (lines 447-467)** — Iterates `plan.directoryTasks`, reads `AGENTS.md` via `readFile(path.join(dirTask.absolutePath, 'AGENTS.md'))`, runs `checkPhantomPaths(agentsMdPath, content, projectRoot)`, aggregates issues, builds `phantomReport` via `buildInconsistencyReport()`, prints via `formatReportForCli()`. Non-throwing with catch-all error handler.\n\n**Phase 3: Root Documents (lines 469-532)** — Sequential execution (concurrency=1) over `plan.rootTasks`. Emits `task:start` trace event, calls `buildRootPrompt(projectRoot, debug)` with all AGENTS.md injected, calls `aiService.call({ prompt, systemPrompt, maxTurns: 1 })`, strips conversational preamble via `content.indexOf('# ')` slice if leading content lacks `#` or `<!--`, writes via `writeFile(rootTask.outputPath, content)`, emits `task:done` with `workerId: 0`/`success`/`error`, updates reporter and plan tracker. Flushes `planTracker` via `await planTracker.flush()` after loop.\n\n## Incremental Update Execution\n\n**executeUpdate(filesToAnalyze, projectRoot, config)** — Phase 1 only implementation for changed file re-analysis. Attempts to load `GENERATION-PLAN.md` from `.agents-reverse-engineer/` via `path.join(projectRoot, CONFIG_DIR, 'GENERATION-PLAN.md')` for `projectPlan` context (falls through on error). Maps `filesToAnalyze` to async tasks reading source via `readFile()`, reading existing `.sum` via `readSumFile()`, building prompt via `buildFilePrompt({ filePath, content, projectPlan, existingSum })`, calling AI service, writing `.sum` with hash. Caches source in `updateSourceCache`. Runs post-analysis quality checks (lines 616-717) identical to generate flow but without stale-doc comparison. Builds `RunSummary` with `updateInconsistenciesCodeVsDoc`/`updateInconsistenciesCodeVsCode` from `report.summary`.\n\n## Helper Functions\n\n**stripPreamble(responseText: string): string** — Pattern 1: detects `\\n---\\n` separator within first 500 chars, returns content after. Pattern 2: regex `/^[\\s\\S]{0,500}?(\\*\\*[A-Z])/` matches bold uppercase start, strips preceding content if <300 chars and no `##` markdown headers. Returns original text if no patterns match.\n\n**extractPurpose(responseText: string): string** — Splits on newlines, skips empty/`#`/`---` lines, filters lines starting with `PREAMBLE_PREFIXES` array (`['now i', 'perfect', 'based on', 'let me', 'here is', 'i\\'ll', 'i will', 'great', 'okay', 'sure', 'certainly', 'alright']`) via `lower.startsWith()` checks, strips bold markdown via `.replace(/^\\*\\*(.+)\\*\\*$/, '$1')`, truncates to 120 chars with `...` suffix. Returns empty string if no valid line found.\n\n## Integration Points\n\n**Dependencies:** `AIService` from `../ai/index.js`, `ExecutionPlan`/`ExecutionTask` from `../generation/executor.js`, `writeSumFile`/`readSumFile` from `../generation/writers/sum.js`, `writeAgentsMd` from `../generation/writers/agents-md.js`, `buildFilePrompt`/`buildDirectoryPrompt`/`buildRootPrompt` from `../generation/prompts/index.js`, `runPool` from `./pool.js`, `PlanTracker` from `./plan-tracker.js`, `ProgressReporter` from `./progress.js`, `ITraceWriter` from `./trace.js`, `checkCodeVsDoc`/`checkCodeVsCode`/`checkPhantomPaths`/`buildInconsistencyReport`/`formatReportForCli` from `../quality/index.js`, `computeContentHashFromString` from `../change-detection/index.js`, `getVersion` from `../version.js`.\n\n**Configuration:** `CommandRunOptions` with `concurrency` (worker pool size), `failFast` (abort on first error), `debug` (verbose prompts), `tracer` (ITraceWriter instance), `progressLog` (Writable stream for mirrored output). `Config` type threaded through `executeUpdate` for prompt building.\n\n**Telemetry:** Calls `aiService.addFilesReadToLastEntry([{ path, sizeBytes }])` after each AI call to track file metadata. Calls `aiService.getSummary()` for aggregated token/cost/duration counts. Emits trace events via `tracer?.emit()`: `phase:start/end` with `taskCount`/`concurrency`/`durationMs`/`tasksCompleted`/`tasksFailed`, `task:start` with `taskLabel`/`phase`, `task:done` with `workerId`/`taskIndex`/`durationMs`/`success`/`error`/`activeTasks`.\n\n## Error Handling\n\n**Quality Validation Non-Throwing:** Both post-Phase 1 inconsistency detection (lines 261-362 generate, 616-717 update) and post-Phase 2 phantom path validation (lines 447-467) wrapped in try-catch blocks logging `[quality] Inconsistency detection failed: ${err.message}` or `[quality] Phantom path validation failed:` without propagating errors.\n\n**Pool Error Handling:** Delegates to `runPool()` with `failFast` option. Progress callback distinguishes `result.success` vs `result.error`, invokes `reporter.onFileDone()` for success and `reporter.onFileError(taskPath, errorMsg)` for failure, increments `filesProcessed`/`filesFailed` counters.\n\n**Root Task Errors:** Phase 3 sequential loop wraps each task in try-catch, increments `rootTasksFailed`, emits `task:done` with `success: false`/`error`, re-throws to maintain existing error handling.\n\n## RunSummary Construction\n\n**Fields:** `version` (from `getVersion()`), `filesProcessed`/`filesFailed`/`filesSkipped`, `totalCalls`/`totalInputTokens`/`totalOutputTokens`/`totalCacheReadTokens`/`totalCacheCreationTokens` (from `aiService.getSummary()`), `totalDurationMs` (`Date.now() - runStart`), `errorCount`, `retryCount: 0` (hardcoded), `totalFilesRead`/`uniqueFilesRead`, `inconsistenciesCodeVsDoc`/`inconsistenciesCodeVsCode`/`phantomPaths`, `inconsistencyReport`. Printed via `reporter.printSummary(summary)`.\n### trace.ts\n**Purpose:** trace.ts provides append-only NDJSON tracing system for debugging task/subprocess lifecycle with promise-chain serial...\n\n**trace.ts provides append-only NDJSON tracing system for debugging task/subprocess lifecycle with promise-chain serialization to handle concurrent pool worker writes safely.**\n\n## Public Interface\n\n### Factory Function\n- `createTraceWriter(projectRoot: string, enabled: boolean): ITraceWriter` — returns `NullTraceWriter` when `enabled` is false (zero overhead), otherwise returns `TraceWriter` appending to `.agents-reverse-engineer/traces/trace-{timestamp}.ndjson`\n\n### Cleanup Function\n- `cleanupOldTraces(projectRoot: string, keepCount: number = 500): Promise<number>` — removes old trace files keeping only most recent ones, returns deletion count, mirrors pattern in `src/ai/telemetry/cleanup.ts`\n\n### Core Interface\n```typescript\ninterface ITraceWriter {\n  emit(event: TraceEventPayload): void;\n  finalize(): Promise<void>;\n  readonly filePath: string;\n}\n```\n\n### Event Type Union\n`TraceEvent` discriminated union includes 14 event types: `PhaseStartEvent`, `PhaseEndEvent`, `WorkerStartEvent`, `WorkerEndEvent`, `TaskPickupEvent`, `TaskDoneEvent`, `TaskStartEvent`, `SubprocessSpawnEvent`, `SubprocessExitEvent`, `RetryEvent`, `DiscoveryStartEvent`, `DiscoveryEndEvent`, `FilterAppliedEvent`, `PlanCreatedEvent`, `ConfigLoadedEvent`\n\n### Event Payload Type\n`TraceEventPayload = DistributiveOmit<TraceEvent, BaseKeys>` — user-supplied event data without auto-populated fields (`seq`, `ts`, `pid`, `elapsedMs`)\n\n## Event Schema\n\n### Common Base Fields (Auto-Populated)\nEvery `TraceEvent` extends `TraceEventBase`:\n- `seq: number` — monotonically increasing sequence number per-run\n- `ts: string` — ISO 8601 timestamp at event creation\n- `pid: number` — `process.pid` of Node.js parent process\n- `elapsedMs: number` — high-resolution elapsed time since run start (milliseconds, fractional)\n\n### Phase Events\n- `PhaseStartEvent` (`type: 'phase:start'`) — includes `phase: string`, `taskCount: number`, `concurrency: number`\n- `PhaseEndEvent` (`type: 'phase:end'`) — includes `phase: string`, `durationMs: number`, `tasksCompleted: number`, `tasksFailed: number`\n\n### Worker Events\n- `WorkerStartEvent` (`type: 'worker:start'`) — includes `workerId: number`, `phase: string`\n- `WorkerEndEvent` (`type: 'worker:end'`) — includes `workerId: number`, `phase: string`, `tasksExecuted: number`\n\n### Task Events\n- `TaskPickupEvent` (`type: 'task:pickup'`) — includes `workerId: number`, `taskIndex: number`, `taskLabel: string`, `activeTasks: number`\n- `TaskDoneEvent` (`type: 'task:done'`) — includes `workerId: number`, `taskIndex: number`, `taskLabel: string`, `durationMs: number`, `success: boolean`, `error?: string`, `activeTasks: number`\n- `TaskStartEvent` (`type: 'task:start'`) — includes `taskLabel: string`, `phase: string`\n\n### Subprocess Events\n- `SubprocessSpawnEvent` (`type: 'subprocess:spawn'`) — includes `childPid: number`, `command: string`, `taskLabel: string`\n- `SubprocessExitEvent` (`type: 'subprocess:exit'`) — includes `childPid: number`, `command: string`, `taskLabel: string`, `exitCode: number`, `signal: string | null`, `durationMs: number`, `timedOut: boolean`\n\n### Retry Event\n- `RetryEvent` (`type: 'retry'`) — includes `attempt: number`, `taskLabel: string`, `errorCode: string`\n\n### Discovery Events\n- `DiscoveryStartEvent` (`type: 'discovery:start'`) — includes `targetPath: string`\n- `DiscoveryEndEvent` (`type: 'discovery:end'`) — includes `filesIncluded: number`, `filesExcluded: number`, `durationMs: number`\n\n### Filter Event\n- `FilterAppliedEvent` (`type: 'filter:applied'`) — includes `filterName: string`, `filesMatched: number`, `filesRejected: number`\n\n### Plan Event\n- `PlanCreatedEvent` (`type: 'plan:created'`) — includes `planType: 'generate' | 'update'`, `fileCount: number`, `taskCount: number`\n\n### Config Event\n- `ConfigLoadedEvent` (`type: 'config:loaded'`) — includes `configPath: string`, `model: string`, `concurrency: number`\n\n## Implementation Classes\n\n### NullTraceWriter\nNo-op implementation returned when `--trace` flag not set. All methods empty (zero overhead at call sites). `filePath` returns empty string. `emit()` and `finalize()` perform no operations.\n\n### TraceWriter\nReal NDJSON writer with promise-chain serialization. Constructor stores `filePath: string`. Maintains state: `seq: number` (incremented per event), `nodePid = process.pid`, `startHr = process.hrtime.bigint()` (for `elapsedMs` calculation), `writeQueue: Promise<void>` (serialization chain), `fd: FileHandle | null` (lazy-opened file handle).\n\n`emit(partial: TraceEventPayload)` populates base fields (`seq`, `ts`, `pid`, `elapsedMs`), serializes to JSON line, enqueues write via `writeQueue = writeQueue.then(async () => {...})` pattern. Lazy-opens file handle via `open(filePath, 'a')` after ensuring directory exists with `mkdir(path.dirname(filePath), { recursive: true })`. Catches write errors silently (trace loss acceptable).\n\n`finalize()` awaits `writeQueue` completion, closes file handle via `fd.close()`, nulls `fd` reference.\n\n## Serialization Pattern\n\nPromise-chain pattern identical to `PlanTracker` from `src/orchestration/plan-tracker.ts`. Each `emit()` call chains onto previous write: `this.writeQueue = this.writeQueue.then(async () => { await this.fd.write(line); })`. Guarantees NDJSON line order matches emission order despite concurrent pool workers.\n\n## Storage Location\n\nConstant `TRACES_DIR = '.agents-reverse-engineer/traces'` defines trace directory. Filename format: `trace-{timestamp}.ndjson` where timestamp uses ISO 8601 with colons/periods replaced by hyphens: `new Date().toISOString().replace(/[:.]/g, '-')`.\n\n## Type Utilities\n\n`DistributiveOmit<T, K extends PropertyKey> = T extends unknown ? Omit<T, K> : never` — correctly distributes `Omit` across union members (standard `Omit<Union, Keys>` fails for discriminated unions per TypeScript semantics). Used to define `TraceEventPayload` by stripping auto-populated base fields from `TraceEvent` union.\n\n## Integration Points\n\nThreaded via `CommandRunOptions.tracer: ITraceWriter` parameter passed to pool (`src/orchestration/pool.ts`), AIService (`src/ai/service.ts`), and runner (`src/orchestration/runner.ts`). Enabled by `--trace` CLI flag in command entry points (`src/cli/generate.ts`, `src/cli/update.ts`).\n### types.ts\n**Purpose:** Shared type definitions for orchestration module: FileTaskResult carries per-file AI call metrics (tokensIn/tokensOut...\n\n**Shared type definitions for orchestration module: FileTaskResult carries per-file AI call metrics (tokensIn/tokensOut/cacheReadTokens/cacheCreationTokens/durationMs/model), RunSummary aggregates cross-command totals with quality metrics (inconsistenciesCodeVsDoc/inconsistenciesCodeVsCode/phantomPaths), ProgressEvent discriminates five event types (start/done/error/dir-done/root-done) with type-specific optional fields, CommandRunOptions threads concurrency/failFast/debug/dryRun/tracer/progressLog through pool/runner/AIService.**\n\n## Exported Types\n\n### FileTaskResult\nPer-file AI analysis outcome returned by command runner workers:\n```typescript\ninterface FileTaskResult {\n  path: string;              // Relative source file path\n  success: boolean;          // Whether AI call succeeded\n  tokensIn: number;          // Input tokens (non-cached)\n  tokensOut: number;         // Output tokens generated\n  cacheReadTokens: number;   // Cached input tokens\n  cacheCreationTokens: number; // Cache creation tokens\n  durationMs: number;        // Wall-clock duration\n  model: string;             // Model identifier used\n  error?: string;            // Error message on failure\n}\n```\n\n### RunSummary\nAggregated metrics spanning entire generate/update command execution:\n```typescript\ninterface RunSummary {\n  version: string;                     // ARE version\n  filesProcessed: number;              // Successful file count\n  filesFailed: number;                 // Failed file count\n  filesSkipped: number;                // Skipped file count (dry-run)\n  totalCalls: number;                  // AI call count\n  totalInputTokens: number;            // Sum of tokensIn\n  totalOutputTokens: number;           // Sum of tokensOut\n  totalCacheReadTokens: number;        // Sum of cacheReadTokens\n  totalCacheCreationTokens: number;    // Sum of cacheCreationTokens\n  totalDurationMs: number;             // Total wall-clock time\n  errorCount: number;                  // Error count\n  retryCount: number;                  // Retry attempt count\n  totalFilesRead: number;              // Total file reads across calls\n  uniqueFilesRead: number;             // Deduplicated file reads\n  inconsistenciesCodeVsDoc?: number;   // Code-vs-doc issue count\n  inconsistenciesCodeVsCode?: number;  // Code-vs-code issue count\n  phantomPaths?: number;               // Phantom path reference count\n  inconsistencyReport?: InconsistencyReport; // Full quality report\n}\n```\n\n### ProgressEvent\nDiscriminated union emitted by runner to progress reporter with five event types:\n```typescript\ninterface ProgressEvent {\n  type: 'start' | 'done' | 'error' | 'dir-done' | 'root-done';\n  filePath: string;   // File or directory path\n  index: number;      // Zero-based task index in phase\n  total: number;      // Total phase task count\n  durationMs?: number; // Wall-clock duration ('done' events)\n  tokensIn?: number;   // Input tokens ('done' events)\n  tokensOut?: number;  // Output tokens ('done' events)\n  model?: string;      // Model identifier ('done' events)\n  error?: string;      // Error message ('error' events)\n}\n```\n\n**Event type field mappings:**\n- `start`: filePath, index, total\n- `done`: filePath, index, total, durationMs, tokensIn, tokensOut, model\n- `error`: filePath, index, total, error\n- `dir-done`: filePath (directory path)\n- `root-done`: filePath (root document path)\n\n### CommandRunOptions\nConfiguration threading object passed from CLI through runner to pool and AIService:\n```typescript\ninterface CommandRunOptions {\n  concurrency: number;       // Max concurrent AI calls\n  failFast?: boolean;        // Stop on first error\n  debug?: boolean;           // Show exact prompts sent\n  dryRun?: boolean;          // Preview without execution\n  tracer?: ITraceWriter;     // NDJSON trace emitter (NullTraceWriter when --trace off)\n  progressLog?: ProgressLog; // File-based progress mirror (tail -f monitoring)\n}\n```\n\n## Cross-Module Dependencies\n\nImports `InconsistencyReport` from `src/quality/index.ts` (embedded in RunSummary.inconsistencyReport), `ProgressLog` from `src/orchestration/progress.ts` (threaded via CommandRunOptions.progressLog), `ITraceWriter` from `src/orchestration/trace.ts` (threaded via CommandRunOptions.tracer).\n\n## Integration with Orchestration Flow\n\nFileTaskResult flows from `src/orchestration/runner.ts` worker callbacks → aggregated into RunSummary by runner.run() → consumed by telemetry logger (`src/ai/telemetry/logger.ts`). ProgressEvent emitted by runner via `progressReporter.emit()` → rendered by `src/orchestration/progress.ts` with ETA calculation. CommandRunOptions populated in CLI entry points (`src/cli/generate.ts`, `src/cli/update.ts`) from config defaults + CLI flag overrides → threaded through executor → pool → AIService.call().\n\n## Import Map (verified — use these exact paths)\n\nplan-tracker.ts:\n  ../config/loader.js → CONFIG_DIR\n\nrunner.ts:\n  ../ai/index.js → AIService (type)\n  ../ai/types.js → AIResponse (type)\n  ../generation/executor.js → ExecutionPlan, ExecutionTask (type)\n  ../generation/writers/sum.js → writeSumFile, readSumFile\n  ../generation/writers/sum.js → SumFileContent (type)\n  ../generation/writers/agents-md.js → writeAgentsMd\n  ../change-detection/index.js → computeContentHashFromString\n  ../change-detection/types.js → FileChange (type)\n  ../generation/prompts/index.js → buildFilePrompt, buildDirectoryPrompt, buildRootPrompt\n  ../config/schema.js → Config (type)\n  ../config/loader.js → CONFIG_DIR\n  ../quality/index.js → checkCodeVsDoc, checkCodeVsCode, checkPhantomPaths, buildInconsistencyReport, formatReportForCli\n  ../quality/index.js → Inconsistency (type)\n  ../generation/executor.js → formatExecutionPlanAsMarkdown\n  ../version.js → getVersion\n\ntypes.ts:\n  ../quality/index.js → InconsistencyReport (type)\n\n## Project Directory Structure\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>",
      "systemPrompt": "You are generating an AGENTS.md file — a directory-level overview for AI coding assistants.\n\nCRITICAL: Output ONLY the raw markdown content. No code fences, no preamble, no explanations, no conversational text. Your entire response IS the AGENTS.md file content.\n\nOUTPUT FORMAT:\n- First line MUST be exactly: <!-- Generated by agents-reverse-engineer -->\n- Use a # heading with the directory name\n- Write a one-paragraph purpose statement for the directory\nusage\nADAPTIVE SECTIONS:\nAnalyze the directory contents and choose the most relevant sections. Do NOT use a fixed template. Instead, select sections that best document this specific directory for an AI that needs to reconstruct or extend the project.\n\nConsider these section types (choose what applies):\n- **Contents**: Group files by purpose/category under ## headings. For each file: markdown link [filename](./filename) and a one-line description.\n- **Subdirectories**: If subdirectories exist, list them with links [dirname/](./dirname/) and brief summaries.\n- **Architecture / Data Flow**: If files form a pipeline, request/response chain, or layered architecture, document it.\n- **Stack**: If this is a package root (has package.json, Cargo.toml, go.mod, etc.), document the technology stack, key scripts, and entry points.\n- **Structure**: If the directory layout follows a convention (feature-sliced, domain-driven, MVC, etc.), document it.\n- **Patterns**: If files share recurring design patterns (factory, strategy, middleware, barrel re-export), name and document them.\n- **Configuration**: If the directory contains config files, schemas, or environment definitions, document the config surface area.\n- **API Surface**: If the directory exports a public API (barrel index, route definitions, SDK), document the interface contract.\n- **File Relationships**: How files collaborate, depend on each other, or share state.\n- **Behavioral Contracts**: If files contain regex patterns, format specifications, magic constants, or template strings that define observable behavior, collect them in a dedicated section. Preserve verbatim patterns from file summaries — do NOT paraphrase regex into prose. This section is MANDATORY when file summaries contain behavioral artifacts.\n\nChoose any relevant sections or create your own based on the directory contents. The goal is to provide a comprehensive overview that captures the essence of the directory's role in the project and how its files work together, with a focus on what an AI coding assistant would need to know to effectively interact with this code.\n\nSCOPE:\n- AGENTS.md is a NAVIGATIONAL INDEX — help an AI find the right file quickly\n- Focus on: what each file does, how files relate, directory-level patterns\n- Do NOT reproduce full architecture sections — those belong in the root CLAUDE.md\n\nPATH ACCURACY (MANDATORY):\n- When referencing files or modules outside this directory, use ONLY paths from the \"Import Map\" section\n- Do NOT invent, rename, or guess module paths — if a path isn't in the Import Map, don't reference it\n- Use the exact directory names from \"Project Directory Structure\" — do NOT rename directories\n  (e.g., if the directory is called \"cli\", write \"src/cli/\", NOT \"src/commands/\")\n- Cross-module references must use the specifier format from actual import statements\n  (e.g., \"../generation/writers/sum.js\", NOT \"../fs/sum-file.js\")\n- If you are unsure about a path, omit the cross-reference rather than guessing\n\nCONSISTENCY (MANDATORY):\n- Do not contradict yourself within the same document\n- If you describe a technique (e.g., \"regex-based\"), do not call it something else later (e.g., \"AST-based\")\n- When stating version numbers, engines, or config fields, use ONLY values present in the file summaries\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this directory\", \"this module\", \"provides\", \"responsible for\", \"is used to\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Per-file descriptions in Contents sections: 1-2 sentences maximum. Reference key symbols but do not reproduce full summaries.\n- Behavioral contracts (regex patterns, format specs, constants) belong in a separate Behavioral Contracts section, not in per-file descriptions.\n- Subdirectory descriptions: 1-2 sentences maximum. Capture the directory's role, not its full contents.\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- Key exported symbols from file summaries MUST appear in the directory overview\n- Preserve exact casing of identifiers\n\nUSER NOTES:\n- If \"User Notes\" are provided in the prompt, they contain user-defined instructions that will be automatically prepended to your output\n- Do NOT repeat or paraphrase user notes in your generated content — they are included separately\n- You may reference information from user notes for context",
      "response": "<!-- Generated by agents-reverse-engineer -->\n\n# src/orchestration\n\n**Iterator-based concurrency pool orchestrating three-phase AI-driven documentation generation (file analysis → directory aggregation → root synthesis) with streaming progress, promise-chain serialized writes for GENERATION-PLAN.md updates and NDJSON trace emission, ETA calculation via moving average, and quality validation integration.**\n\n## Contents\n\n### Core Orchestration\n\n**[runner.ts](./runner.ts)** — `CommandRunner` executes three-phase pipeline: concurrent file `.sum` generation via `runPool()` with stale-doc detection, post-order directory `AGENTS.md` traversal grouped by depth, sequential root document synthesis (`CLAUDE.md`/`GEMINI.md`/`OPENCODE.md`), embeds quality validators (`checkCodeVsDoc`/`checkCodeVsCode`/`checkPhantomPaths`) post-phase with throttled directory grouping, builds `RunSummary` with token/cost/duration/inconsistency aggregates.\n\n**[pool.ts](./pool.ts)** — `runPool<T>()` implements shared-iterator worker pattern: all N workers pull from single `tasks.entries()` iterator preventing idle time between batches, emits `worker:start/end` and `task:pickup/done` trace events with `activeTasks` counter, supports `failFast` abort via mutable `aborted` flag checked before each task pickup, returns sparse `TaskResult<T>[]` array indexed by original task position.\n\n**[progress.ts](./progress.ts)** — `ProgressReporter` streams colored console output (`pc.cyan('ANALYZING')`, `pc.green('DONE')`, `pc.red('FAIL')`) with ETA calculated via sliding window (last 10 completion times) for files and directories separately, `ProgressLog` mirrors ANSI-stripped output to `.agents-reverse-engineer/progress.log` via promise-chain serialized writes for `tail -f` monitoring in buffered environments.\n\n**[plan-tracker.ts](./plan-tracker.ts)** — `PlanTracker` serializes concurrent checkbox updates (`- [ ] → - [x]`) for `GENERATION-PLAN.md` via promise-chain pattern (`writeQueue`), maintains in-memory markdown state, requires exact backtick-wrapped path matching (`` `src/cli/init.ts` ``), swallows write errors (non-critical operation).\n\n**[trace.ts](./trace.ts)** — `createTraceWriter()` factory returns `NullTraceWriter` (zero overhead) or `TraceWriter` (appends to `.agents-reverse-engineer/traces/trace-{timestamp}.ndjson`), auto-populates `seq`/`ts`/`pid`/`elapsedMs` base fields, emits 14 event types (`phase:start/end`, `worker:start/end`, `task:pickup/done`, `subprocess:spawn/exit`, `retry`, `discovery:start/end`, `filter:applied`, `plan:created`, `config:loaded`), `cleanupOldTraces()` enforces 500-file retention, uses `DistributiveOmit<TraceEvent, BaseKeys>` for discriminated union stripping.\n\n### Type Definitions\n\n**[types.ts](./types.ts)** — `FileTaskResult` carries per-task metrics (`tokensIn`/`tokensOut`/`cacheReadTokens`/`cacheCreationTokens`/`durationMs`/`model`), `RunSummary` aggregates command-level totals with quality counts (`inconsistenciesCodeVsDoc`/`inconsistenciesCodeVsCode`/`phantomPaths`), `ProgressEvent` discriminates five event types (`start`/`done`/`error`/`dir-done`/`root-done`) with type-specific optional fields, `CommandRunOptions` threads `concurrency`/`failFast`/`debug`/`dryRun`/`tracer`/`progressLog` from CLI through runner to pool and AIService.\n\n**[index.ts](./index.ts)** — Barrel export exposing `runPool`, `ProgressReporter`, `ProgressLog`, `PlanTracker`, `CommandRunner`, `ITraceWriter`, `TraceEvent`, `TraceEventPayload`, `createTraceWriter`, `cleanupOldTraces`, and shared types (`FileTaskResult`, `RunSummary`, `ProgressEvent`, `CommandRunOptions`, `PoolOptions`, `TaskResult`).\n\n## Concurrency Architecture\n\n### Shared-Iterator Pattern\n\n`runPool()` avoids batch-based idling by sharing single `tasks.entries()` iterator across N workers. Each worker pulls next `[index, taskFn]` tuple immediately after completing previous task, ensuring all slots stay busy until iterator exhausts. `Promise.allSettled(workers)` waits for all workers to finish or abort.\n\n### Fail-Fast Abort\n\nMutable `aborted` boolean flag set by any worker encountering error when `failFast: true`. Workers check `if (aborted) break` before pulling next task but do not interrupt running tasks. Results array remains sparse (undefined entries at aborted indices).\n\n### Serialization Guarantees\n\n`PlanTracker` and `TraceWriter` use promise-chain pattern (`writeQueue: Promise<void>`) to serialize concurrent writes. Each `markDone()` or `emit()` call chains onto previous write: `this.writeQueue = this.writeQueue.then(() => fs.writeFile(...))`. Guarantees NDJSON line order matches emission order despite pool concurrency.\n\n## Three-Phase Execution Flow\n\n### Phase 1: File Analysis (Concurrent)\n\n`CommandRunner.executeGenerate()` pre-caches existing `.sum` files at concurrency=20 for stale-doc detection. Maps file tasks to async functions reading source via `fs.readFile()`, computing `contentHash` via `computeContentHashFromString()`, calling `AIService.call()` with `buildFilePrompt()`, stripping preamble, extracting purpose (first non-preamble line truncated to 120 chars), writing `SumFileContent` with YAML frontmatter (`generated_at`, `content_hash`, `purpose`). Executes via `runPool(fileTasks, { concurrency, failFast, tracer, phaseLabel: 'phase-1-files' })`. Post-phase: groups files by directory, runs throttled (concurrency=10) `checkCodeVsDoc()` comparing old vs. new `.sum` against source content, aggregates `filesForCodeVsCode` array and runs `checkCodeVsCode()` for duplicate symbol detection.\n\n### Phase 2: Directory Aggregation (Post-Order Traversal)\n\nGroups `plan.directoryTasks` by `metadata.depth` into `Map<number, ExecutionTask[]>`, sorts depth levels descending (deepest-first). For each depth level: computes `dirConcurrency = Math.min(concurrency, dirsAtDepth.length)`, emits `phase:start`, maps directory tasks to async functions calling `buildDirectoryPrompt()` with `knownDirs` set and project structure context, calls AI service, writes via `writeAgentsMd()` (prepends existing `AGENTS.local.md` content if present), updates reporter and plan tracker. Executes via `runPool()` with depth-specific `phaseLabel`. Post-phase: runs `checkPhantomPaths()` on each generated `AGENTS.md` (three regex patterns: markdown links, backtick-quoted paths, prose-embedded paths), aggregates issues into `phantomReport`.\n\n### Phase 3: Root Synthesis (Sequential)\n\nSequential execution (concurrency=1) over `plan.rootTasks` (`CLAUDE.md`, `GEMINI.md`, `OPENCODE.md`). Emits `task:start`, calls `buildRootPrompt()` with all `AGENTS.md` injected and root `package.json` metadata, calls `AIService.call({ prompt, systemPrompt, maxTurns: 1 })`, strips conversational preamble (two patterns: `\\n---\\n` separator or bold uppercase start), writes via `fs.writeFile()`, emits `task:done`, updates reporter and plan tracker. Flushes `planTracker.flush()` after loop.\n\n## Progress Reporting & ETA\n\n### Moving Average ETA\n\n`ProgressReporter` maintains `completionTimes: number[]` (files) and `dirCompletionTimes: number[]` (directories) sliding windows (max 10 entries). On task completion: pushes `durationMs`, shifts oldest if window exceeds size. `formatETA()` computes moving average `(sum / count) * remaining`, formats as `~Ns` or `~Mm Ss`, returns empty string if fewer than 2 completions.\n\n### Dual Output Streams\n\nConsole output uses `picocolors` for colored ANSI codes (`pc.cyan('ANALYZING')`, `pc.green('DONE')`, `pc.blue('DONE')` for directories/roots, `pc.red('FAIL')`). `ProgressLog` strips ANSI via regex `/\\x1b\\[[0-9;]*m/g` and appends to `.agents-reverse-engineer/progress.log` via promise-chain serialization. File handle opened lazily in truncate mode ('w') on first write, closed via `finalize()`.\n\n### Token Accounting Display\n\nTotal input tokens displayed as `tokensIn + cacheReadTokens + cacheCreationTokens`. Completion messages show format: `[X/Y] DONE path Xs in/out tok model ~ETA`. Summary prints: files processed/failed/skipped, total calls, tokens (input + cache read + cache creation / output), cache statistics (% read tokens, % creation tokens), files read (total and unique), elapsed time, errors, retries.\n\n## Trace Event System\n\n### Event Types & Schema\n\nDiscriminated union `TraceEvent` with 14 types. All events extend `TraceEventBase` with auto-populated fields: `seq` (monotonic counter), `ts` (ISO 8601), `pid` (`process.pid`), `elapsedMs` (high-resolution delta from `process.hrtime.bigint()` start).\n\n**Pool events:** `worker:start` (workerId, phase), `worker:end` (workerId, phase, tasksExecuted), `task:pickup` (workerId, taskIndex, taskLabel, activeTasks), `task:done` (workerId, taskIndex, taskLabel, durationMs, success, error?, activeTasks).\n\n**Subprocess events:** `subprocess:spawn` (childPid, command, taskLabel), `subprocess:exit` (childPid, command, taskLabel, exitCode, signal, durationMs, timedOut).\n\n**Phase events:** `phase:start` (phase, taskCount, concurrency), `phase:end` (phase, durationMs, tasksCompleted, tasksFailed).\n\n**Other events:** `retry` (attempt, taskLabel, errorCode), `discovery:start/end`, `filter:applied`, `plan:created`, `config:loaded`.\n\n### Emission & Serialization\n\nUsers call `tracer.emit(payload)` with `TraceEventPayload = DistributiveOmit<TraceEvent, BaseKeys>` (user-supplied data without auto-populated fields). `TraceWriter.emit()` populates base fields, serializes to JSON line, enqueues write via `writeQueue = writeQueue.then(() => fd.write(line))` promise-chain. Lazy-opens file handle via `fs.open(filePath, 'a')` after ensuring directory exists. `finalize()` awaits `writeQueue`, closes handle.\n\n### Cleanup & Retention\n\n`cleanupOldTraces(projectRoot, keepCount = 500)` reads `.agents-reverse-engineer/traces/`, sorts files by creation time descending via `fs.stat()`, deletes excess files, returns deletion count. Auto-invoked by `AIService.finalize()` in telemetry shutdown.\n\n## Quality Validation Integration\n\n### Code-vs-Doc Consistency\n\nPost-Phase 1: groups processed files by `path.dirname()`, runs throttled (concurrency=10) validation per directory. For each file: reads cached `sourceContent` from map, reads old `.sum` via `readSumFile()`, runs `checkCodeVsDoc(sourceContent, oldSum.summary, filePath)` appending `(stale documentation)` label, reads fresh `.sum`, runs second pass. `checkCodeVsDoc()` extracts exported symbols via regex `/^[ \\t]*export\\s+(?:default\\s+)?(?:function|class|const|let|var|type|interface|enum)\\s+(\\w+)/gm`, verifies all exports appear in summary via substring search, reports `CodeDocInconsistency` with `missingFromDoc` arrays.\n\n### Code-vs-Code Duplicate Detection\n\nAggregates exports across per-directory file groups into `Map<symbol, string[]>`. Runs `checkCodeVsCode(filesForCodeVsCode)` detecting symbols appearing in multiple files, reports `CodeCodeInconsistency` with `pattern: 'duplicate-export'`.\n\n### Phantom Path Resolution\n\nPost-Phase 2: for each generated `AGENTS.md`, runs `checkPhantomPaths(agentsMdPath, content, projectRoot)` extracting path-like strings via three regex patterns (markdown links, backtick-quoted paths, prose-embedded paths), resolves against AGENTS.md directory and project root with `.ts`/`.js` fallback, reports `PhantomPathInconsistency` for unresolved references.\n\n### Report Formatting\n\nBuilds `InconsistencyReport` via `buildInconsistencyReport(issues, metadata)` with `metadata` (timestamp, projectRoot, filesChecked, durationMs), `issues[]` (discriminated union), `summary` (counts by type/severity). Prints via `formatReportForCli(report)` showing counts and groupings. Embeds in `RunSummary.inconsistencyReport` field.\n\n## Incremental Update Support\n\n`CommandRunner.executeUpdate()` implements Phase 1 only re-analysis for changed files. Attempts to load `GENERATION-PLAN.md` from `.agents-reverse-engineer/` for `projectPlan` context (falls through on error). Maps `filesToAnalyze` to async tasks reading source, reading existing `.sum`, building prompt via `buildFilePrompt({ filePath, content, projectPlan, existingSum })`, calling AI service, writing `.sum` with hash. Runs identical post-analysis quality checks (code-vs-doc, code-vs-code) without stale-doc comparison. Builds `RunSummary` with `updateInconsistenciesCodeVsDoc`/`updateInconsistenciesCodeVsCode` from report summary.\n\n## Behavioral Contracts\n\n### Preamble Stripping Patterns\n\n`stripPreamble()` implements two regex-based detection patterns:\n1. YAML separator: `/\\n---\\n/` within first 500 chars, returns content after separator\n2. Bold uppercase start: `/^[\\s\\S]{0,500}?(\\*\\*[A-Z])/` matches conversational preamble preceding bold section header, strips if <300 chars and no `##` markdown headers present\n\n### Purpose Extraction Filter\n\n`extractPurpose()` skips lines matching `PREAMBLE_PREFIXES` array via `line.toLowerCase().startsWith()`:\n```javascript\n['now i', 'perfect', 'based on', 'let me', 'here is', \"i'll\", 'i will', 'great', 'okay', 'sure', 'certainly', 'alright']\n```\n\nReturns first non-preamble/non-header/non-separator line with bold markdown stripped (`/^\\*\\*(.+)\\*\\*$/`) and 120-char truncation.\n\n### Checkbox Markdown Format\n\n`PlanTracker.markDone()` requires exact backtick-wrapped path format:\n```markdown\n- [ ] `src/cli/init.ts`     → file task\n- [ ] `src/cli/AGENTS.md`   → directory task (caller appends /AGENTS.md)\n- [ ] `CLAUDE.md`           → root task\n```\n\nString replacement via `` `- [ ] \\`${itemPath}\\`` → `- [x] \\`${itemPath}\\`` ``. Returns early if no match (no checkbox update queued).\n\n### Trace Filename Format\n\n`trace-{timestamp}.ndjson` where timestamp converts ISO 8601 to filesystem-safe format via `new Date().toISOString().replace(/[:.]/g, '-')` (colons/periods → hyphens).\n\n### Export Symbol Extraction Regex\n\nCode-vs-doc validator uses pattern: `/^[ \\t]*export\\s+(?:default\\s+)?(?:function|class|const|let|var|type|interface|enum)\\s+(\\w+)/gm` matching top-level exports with optional `default` keyword, capturing symbol name via `(\\w+)` group.\n\n### Phantom Path Extraction Patterns\n\nThree regex patterns in `checkPhantomPaths()`:\n1. Markdown links: `/\\[(?:[^\\]]*)\\]\\((\\.[^)]+)\\)/g` captures relative paths in link targets\n2. Backtick-quoted paths: `` /`((?:src\\/|\\.\\.?\\/)[^`]+\\.[a-z]{1,4})`/g `` captures backtick-wrapped paths starting with `src/` or `./` or `../` with file extensions\n3. Prose-embedded paths: `/(?:from|in|by|via|see)\\s+`?(src\\/[\\w\\-./]+)`?/gi` captures paths following prepositions in prose text",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 4232,
      "cacheReadTokens": 17477,
      "cacheCreationTokens": 19025,
      "latencyMs": 132273,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": []
    },
    {
      "timestamp": "2026-02-09T16:15:51.557Z",
      "prompt": "Generate AGENTS.md for directory: \"docs\" (docs)\n\n## File Summaries (1 files)\n\n### INPUT.md\n**Purpose:** INPUT.md documents the original design vision and inception rationale for the agents-reverse-engineer tool, defining ...\n\n**INPUT.md documents the original design vision and inception rationale for the agents-reverse-engineer tool, defining the Recursive Language Model (RLM) algorithm for brownfield project documentation generation through post-order tree traversal of file summaries and directory aggregation.**\n\n## Original Vision\n\nCreated to streamline brownfield project documentation by providing AI agents with clear codebase overviews. Designed for compatibility with Claude Code, OpenCode, and other agent tools, complementing SpecKit (https://github.com/github/spec-kit), BMAD (https://github.com/bmad-code-org/BMAD-METHOD), and Get Shit Done (https://github.com/glittercowboy/get-shit-done).\n\n## RLM Algorithm Definition\n\nThe Recursive Language Model workflow executes as:\n1. Build project structure tree via directory traversal\n2. Start execution at first leaf (deepest file node) and build recursively backward toward root\n3. For leaf files: analyze content and generate `{filename}.sum` summary artifact\n4. For directories: once all child leaf summaries exist, analyze directory contents and generate `AGENTS.md` (plus optional `ARCHITECTURE.md`, `STRUCTURE.md`, `STACK.md`, `INTEGRATIONS.md`, `INFRASTRUCTURE.md`, `CONVENTIONS.md`, `TESTING.md`, `PATTERNS.md`, `CONCERNS.md`)\n5. Continue post-order traversal recursively until project root reached\n\n## Command Interface\n\nProposed command patterns:\n- `/are-generate` — Full documentation generation via RLM execution\n- `/are-update` — Incremental regeneration for modified files\n\n## Research Directives\n\nDocument instructed detailed analysis of GSD (Get Shit Done) and BMAD methodologies, specifically:\n- Repository structure patterns (primarily GSD-inspired with BMAD elements)\n- Brownfield project approach strategies\n- Special command implementations and execution details\n- Codebase architecture analysis requirements\n\n## Session Lifecycle Integration\n\nOriginal design specified session-end hook capability to automatically update impacted documentation files when agent sessions terminate, ensuring documentation remains synchronized with codebase changes without manual intervention.\n\n## Project Directory Structure\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>",
      "systemPrompt": "You are generating an AGENTS.md file — a directory-level overview for AI coding assistants.\n\nCRITICAL: Output ONLY the raw markdown content. No code fences, no preamble, no explanations, no conversational text. Your entire response IS the AGENTS.md file content.\n\nOUTPUT FORMAT:\n- First line MUST be exactly: <!-- Generated by agents-reverse-engineer -->\n- Use a # heading with the directory name\n- Write a one-paragraph purpose statement for the directory\nusage\nADAPTIVE SECTIONS:\nAnalyze the directory contents and choose the most relevant sections. Do NOT use a fixed template. Instead, select sections that best document this specific directory for an AI that needs to reconstruct or extend the project.\n\nConsider these section types (choose what applies):\n- **Contents**: Group files by purpose/category under ## headings. For each file: markdown link [filename](./filename) and a one-line description.\n- **Subdirectories**: If subdirectories exist, list them with links [dirname/](./dirname/) and brief summaries.\n- **Architecture / Data Flow**: If files form a pipeline, request/response chain, or layered architecture, document it.\n- **Stack**: If this is a package root (has package.json, Cargo.toml, go.mod, etc.), document the technology stack, key scripts, and entry points.\n- **Structure**: If the directory layout follows a convention (feature-sliced, domain-driven, MVC, etc.), document it.\n- **Patterns**: If files share recurring design patterns (factory, strategy, middleware, barrel re-export), name and document them.\n- **Configuration**: If the directory contains config files, schemas, or environment definitions, document the config surface area.\n- **API Surface**: If the directory exports a public API (barrel index, route definitions, SDK), document the interface contract.\n- **File Relationships**: How files collaborate, depend on each other, or share state.\n- **Behavioral Contracts**: If files contain regex patterns, format specifications, magic constants, or template strings that define observable behavior, collect them in a dedicated section. Preserve verbatim patterns from file summaries — do NOT paraphrase regex into prose. This section is MANDATORY when file summaries contain behavioral artifacts.\n\nChoose any relevant sections or create your own based on the directory contents. The goal is to provide a comprehensive overview that captures the essence of the directory's role in the project and how its files work together, with a focus on what an AI coding assistant would need to know to effectively interact with this code.\n\nSCOPE:\n- AGENTS.md is a NAVIGATIONAL INDEX — help an AI find the right file quickly\n- Focus on: what each file does, how files relate, directory-level patterns\n- Do NOT reproduce full architecture sections — those belong in the root CLAUDE.md\n\nPATH ACCURACY (MANDATORY):\n- When referencing files or modules outside this directory, use ONLY paths from the \"Import Map\" section\n- Do NOT invent, rename, or guess module paths — if a path isn't in the Import Map, don't reference it\n- Use the exact directory names from \"Project Directory Structure\" — do NOT rename directories\n  (e.g., if the directory is called \"cli\", write \"src/cli/\", NOT \"src/commands/\")\n- Cross-module references must use the specifier format from actual import statements\n  (e.g., \"../generation/writers/sum.js\", NOT \"../fs/sum-file.js\")\n- If you are unsure about a path, omit the cross-reference rather than guessing\n\nCONSISTENCY (MANDATORY):\n- Do not contradict yourself within the same document\n- If you describe a technique (e.g., \"regex-based\"), do not call it something else later (e.g., \"AST-based\")\n- When stating version numbers, engines, or config fields, use ONLY values present in the file summaries\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this directory\", \"this module\", \"provides\", \"responsible for\", \"is used to\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Per-file descriptions in Contents sections: 1-2 sentences maximum. Reference key symbols but do not reproduce full summaries.\n- Behavioral contracts (regex patterns, format specs, constants) belong in a separate Behavioral Contracts section, not in per-file descriptions.\n- Subdirectory descriptions: 1-2 sentences maximum. Capture the directory's role, not its full contents.\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- Key exported symbols from file summaries MUST appear in the directory overview\n- Preserve exact casing of identifiers\n\nUSER NOTES:\n- If \"User Notes\" are provided in the prompt, they contain user-defined instructions that will be automatically prepended to your output\n- Do NOT repeat or paraphrase user notes in your generated content — they are included separately\n- You may reference information from user notes for context",
      "response": "<!-- Generated by agents-reverse-engineer -->\n\n# docs\n\nOriginal design specification and research artifacts defining the Recursive Language Model (RLM) algorithm and agents-reverse-engineer tool inception rationale.\n\n## Contents\n\n### [INPUT.md](./INPUT.md)\nInception document defining the RLM algorithm via post-order tree traversal (leaf file analysis → directory aggregation → root synthesis), proposed command interface (`/are-generate`, `/are-update`), session lifecycle integration requirements, and research directives for GSD/BMAD methodology analysis.\n\n## Purpose\n\n`INPUT.md` serves as the foundational design specification established before implementation began. It defines the core RLM workflow: (1) build project tree, (2) traverse post-order starting at deepest leaves, (3) generate `.sum` summaries for files, (4) generate `AGENTS.md` for directories once all children complete, (5) recurse upward to project root. The document specifies target AI platforms (Claude Code, OpenCode, Gemini) and automatic documentation synchronization via session-end hooks.\n\n## Relationship to Implementation\n\nThe three-phase pipeline in `src/generation/orchestrator.ts` implements the RLM algorithm specified here:\n- **Phase 1** (concurrent file analysis) → `.sum` generation for leaf nodes\n- **Phase 2** (post-order directory aggregation) → `AGENTS.md` synthesis with depth-based sorting\n- **Phase 3** (root synthesis) → `CLAUDE.md`/`GEMINI.md`/`OPENCODE.md` platform-specific integration documents\n\nSession lifecycle hooks in `hooks/are-session-end.js` and `hooks/opencode-are-session-end.js` implement the automatic update mechanism proposed in this specification.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 402,
      "cacheReadTokens": 17477,
      "cacheCreationTokens": 9502,
      "latencyMs": 15093,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": []
    },
    {
      "timestamp": "2026-02-09T16:15:51.561Z",
      "prompt": "Generate AGENTS.md for directory: \"scripts\" (scripts)\n\n## File Summaries (1 files)\n\n### build-hooks.js\n**Purpose:** build-hooks.js copies JavaScript hook files from `hooks/` to `hooks/dist/` for npm package distribution.\n\n**build-hooks.js copies JavaScript hook files from `hooks/` to `hooks/dist/` for npm package distribution.**\n\n## Execution Context\n\nRuns as Node.js script via `#!/usr/bin/env node` shebang. Invoked by `npm run build:hooks` and automatically during `npm run prepublishOnly` lifecycle hook (see package.json build pipeline).\n\n## File Operations\n\nUses synchronous fs operations: `copyFileSync()` for file copying, `existsSync()` for dist directory detection, `mkdirSync()` with `{ recursive: true }` for dist creation, `readdirSync()` for source file enumeration. Filters files via `.endsWith('.js') && f !== 'dist'` predicate to exclude non-JavaScript files and the dist directory itself.\n\n## Path Resolution\n\nResolves `__dirname` via `fileURLToPath(import.meta.url)` (ES module compatibility pattern). Constructs `projectRoot` as `join(__dirname, '..')`, `HOOKS_SRC` as `join(projectRoot, 'hooks')`, `HOOKS_DIST` as `join(projectRoot, 'hooks', 'dist')`.\n\n## Build Workflow\n\n1. Creates `hooks/dist/` directory if missing via `mkdirSync(HOOKS_DIST, { recursive: true })`\n2. Discovers `.js` files in `hooks/` via `readdirSync(HOOKS_SRC).filter()`\n3. Copies each file from `join(HOOKS_SRC, file)` to `join(HOOKS_DIST, file)` via `copyFileSync()`\n4. Logs progress: `\"Building hooks...\"` header, per-file `\"Copied: {file} -> hooks/dist/{file}\"` entries, summary `\"Done. {count} hook(s) built.\"`\n\n## Integration with Package Distribution\n\nEnsures session lifecycle hooks (`are-check-update.js`, `are-session-end.js`, `opencode-are-check-update.js`, `opencode-are-session-end.js`) are bundled in npm tarball for installation into IDE config directories (`~/.claude/hooks/`, `~/.config/opencode/plugins/`) via installer module (`src/installer/operations.ts`).\n\n## Project Directory Structure\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>",
      "systemPrompt": "You are generating an AGENTS.md file — a directory-level overview for AI coding assistants.\n\nCRITICAL: Output ONLY the raw markdown content. No code fences, no preamble, no explanations, no conversational text. Your entire response IS the AGENTS.md file content.\n\nOUTPUT FORMAT:\n- First line MUST be exactly: <!-- Generated by agents-reverse-engineer -->\n- Use a # heading with the directory name\n- Write a one-paragraph purpose statement for the directory\nusage\nADAPTIVE SECTIONS:\nAnalyze the directory contents and choose the most relevant sections. Do NOT use a fixed template. Instead, select sections that best document this specific directory for an AI that needs to reconstruct or extend the project.\n\nConsider these section types (choose what applies):\n- **Contents**: Group files by purpose/category under ## headings. For each file: markdown link [filename](./filename) and a one-line description.\n- **Subdirectories**: If subdirectories exist, list them with links [dirname/](./dirname/) and brief summaries.\n- **Architecture / Data Flow**: If files form a pipeline, request/response chain, or layered architecture, document it.\n- **Stack**: If this is a package root (has package.json, Cargo.toml, go.mod, etc.), document the technology stack, key scripts, and entry points.\n- **Structure**: If the directory layout follows a convention (feature-sliced, domain-driven, MVC, etc.), document it.\n- **Patterns**: If files share recurring design patterns (factory, strategy, middleware, barrel re-export), name and document them.\n- **Configuration**: If the directory contains config files, schemas, or environment definitions, document the config surface area.\n- **API Surface**: If the directory exports a public API (barrel index, route definitions, SDK), document the interface contract.\n- **File Relationships**: How files collaborate, depend on each other, or share state.\n- **Behavioral Contracts**: If files contain regex patterns, format specifications, magic constants, or template strings that define observable behavior, collect them in a dedicated section. Preserve verbatim patterns from file summaries — do NOT paraphrase regex into prose. This section is MANDATORY when file summaries contain behavioral artifacts.\n\nChoose any relevant sections or create your own based on the directory contents. The goal is to provide a comprehensive overview that captures the essence of the directory's role in the project and how its files work together, with a focus on what an AI coding assistant would need to know to effectively interact with this code.\n\nSCOPE:\n- AGENTS.md is a NAVIGATIONAL INDEX — help an AI find the right file quickly\n- Focus on: what each file does, how files relate, directory-level patterns\n- Do NOT reproduce full architecture sections — those belong in the root CLAUDE.md\n\nPATH ACCURACY (MANDATORY):\n- When referencing files or modules outside this directory, use ONLY paths from the \"Import Map\" section\n- Do NOT invent, rename, or guess module paths — if a path isn't in the Import Map, don't reference it\n- Use the exact directory names from \"Project Directory Structure\" — do NOT rename directories\n  (e.g., if the directory is called \"cli\", write \"src/cli/\", NOT \"src/commands/\")\n- Cross-module references must use the specifier format from actual import statements\n  (e.g., \"../generation/writers/sum.js\", NOT \"../fs/sum-file.js\")\n- If you are unsure about a path, omit the cross-reference rather than guessing\n\nCONSISTENCY (MANDATORY):\n- Do not contradict yourself within the same document\n- If you describe a technique (e.g., \"regex-based\"), do not call it something else later (e.g., \"AST-based\")\n- When stating version numbers, engines, or config fields, use ONLY values present in the file summaries\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this directory\", \"this module\", \"provides\", \"responsible for\", \"is used to\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Per-file descriptions in Contents sections: 1-2 sentences maximum. Reference key symbols but do not reproduce full summaries.\n- Behavioral contracts (regex patterns, format specs, constants) belong in a separate Behavioral Contracts section, not in per-file descriptions.\n- Subdirectory descriptions: 1-2 sentences maximum. Capture the directory's role, not its full contents.\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- Key exported symbols from file summaries MUST appear in the directory overview\n- Preserve exact casing of identifiers\n\nUSER NOTES:\n- If \"User Notes\" are provided in the prompt, they contain user-defined instructions that will be automatically prepended to your output\n- Do NOT repeat or paraphrase user notes in your generated content — they are included separately\n- You may reference information from user notes for context",
      "response": "<!-- Generated by agents-reverse-engineer -->\n\n# scripts/\n\nBuild automation utilities for npm package distribution. Currently contains a single script that prepares session lifecycle hooks for inclusion in the published npm tarball.\n\n## Contents\n\n**[build-hooks.js](./build-hooks.js)** — Copies `.js` hook files from `hooks/` to `hooks/dist/` via synchronous `copyFileSync()`, filtering non-JS files and the dist directory itself, invoked by `npm run build:hooks` and `prepublishOnly` lifecycle hook.\n\n## Build Pipeline Integration\n\n**Execution trigger:** `build-hooks.js` runs during `npm run prepublishOnly`, which npm automatically invokes before `npm publish`. The package.json `scripts` section chains `prepublishOnly` → `npm run build && npm run build:hooks`, ensuring TypeScript compilation (`tsc`) and hook copying both complete before tarball creation.\n\n**Distribution requirement:** Session lifecycle hooks (`are-check-update.js`, `are-session-end.js`, `opencode-are-check-update.js`, `opencode-are-session-end.js`) must exist in `hooks/dist/` for the npm tarball to include them. The installer module (`src/installer/operations.ts`) copies these files from the installed package into IDE config directories (`~/.claude/hooks/`, `~/.config/opencode/plugins/`).\n\n## File Operations\n\n**Path resolution:** Uses ES module `import.meta.url` → `fileURLToPath()` → `__dirname` pattern to locate project root. Constructs `HOOKS_SRC` as `join(projectRoot, 'hooks')` and `HOOKS_DIST` as `join(projectRoot, 'hooks', 'dist')`.\n\n**Synchronous workflow:**\n1. `existsSync(HOOKS_DIST)` checks for dist directory\n2. `mkdirSync(HOOKS_DIST, { recursive: true })` creates directory if missing\n3. `readdirSync(HOOKS_SRC)` enumerates source files\n4. `.filter(f => f.endsWith('.js') && f !== 'dist')` excludes non-JavaScript files and dist directory\n5. `copyFileSync(join(HOOKS_SRC, file), join(HOOKS_DIST, file))` copies each hook file\n6. Console logs progress: per-file `\"Copied: {file} -> hooks/dist/{file}\"` and summary `\"Done. {count} hook(s) built.\"`\n\n## Behavioral Contracts\n\n**File filter predicate:** `/\\.js$/ && f !== 'dist'` (endsWith check + exclusion)",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 616,
      "cacheReadTokens": 0,
      "cacheCreationTokens": 27029,
      "latencyMs": 17063,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": []
    },
    {
      "timestamp": "2026-02-09T16:15:51.571Z",
      "prompt": "Generate AGENTS.md for directory: \"hooks\" (hooks)\n\n## File Summaries (4 files)\n\n### are-check-update.js\n**Purpose:** are-check-update.js spawns a detached background process that queries npm for the latest `agents-reverse-engineer` ve...\n\n**are-check-update.js spawns a detached background process that queries npm for the latest `agents-reverse-engineer` version, compares it against the local `ARE-VERSION` file (checking project `.claude/ARE-VERSION` before global `~/.claude/ARE-VERSION`), and writes the update availability result to `~/.claude/cache/are-update-check.json`.**\n\n## Entry Point\n\nSessionStart hook script invoked by Claude Code IDE. Runs synchronously but immediately spawns detached subprocess for actual network check to avoid blocking session initialization.\n\n## Version File Resolution Strategy\n\nChecks version files in priority order:\n1. `${cwd}/.claude/ARE-VERSION` (project-local installation)\n2. `${homedir}/.claude/ARE-VERSION` (global installation)\n\nFalls back to `'0.0.0'` if neither exists.\n\n## Background Subprocess Spawning\n\nUses `spawn(process.execPath, ['-e', scriptString], { stdio: 'ignore', detached: true, windowsHide: true })` pattern followed by `child.unref()` to create detached Node.js process that survives parent termination. Script string is inline JavaScript code interpolated with `JSON.stringify()` for paths.\n\n## Network Check Behavior\n\nBackground process executes `npm view agents-reverse-engineer version` via `execSync()` with 10-second timeout. Network failures set `latest: 'unknown'` but do not throw. Comparison uses strict string equality between `installed` and `latest` versions.\n\n## Cache Output Format\n\nWrites JSON object to `~/.claude/cache/are-update-check.json`:\n```json\n{\n  \"update_available\": boolean,\n  \"installed\": string,\n  \"latest\": string | \"unknown\",\n  \"checked\": number  // Unix timestamp (seconds)\n}\n```\n\n## Directory Creation\n\nCalls `mkdirSync(cacheDir, { recursive: true })` to ensure `~/.claude/cache/` exists before spawning subprocess. Cache directory creation is synchronous and blocks hook execution.\n\n## Error Handling\n\nTry-catch blocks in spawned script swallow errors from:\n- Version file reads (defaults to `'0.0.0'`)\n- npm registry queries (sets `latest: null` → `'unknown'`)\n\nNo error propagation to parent process due to `stdio: 'ignore'` and detached execution.\n\n## Platform Compatibility\n\n`windowsHide: true` option prevents console window flash on Windows. Homedir resolution via `os.homedir()` handles cross-platform user directory lookup. `path.join()` ensures correct path separators.\n### are-session-end.js\n**Purpose:** are-session-end.js spawns detached background `npx agents-reverse-engineer@latest update --quiet` subprocess when ses...\n\n**are-session-end.js spawns detached background `npx agents-reverse-engineer@latest update --quiet` subprocess when session ends if uncommitted git changes detected, with disable mechanisms via `ARE_DISABLE_HOOK=1` environment variable or `hook_enabled: false` config substring match.**\n\n## Hook Lifecycle Integration\n\nExecutes during SessionEnd event in Claude Code/Gemini CLI runtime. Designed to terminate quickly without blocking session close via detached subprocess pattern.\n\n## Disable Mechanisms\n\nExits silently (status 0) if `process.env.ARE_DISABLE_HOOK === '1'` before performing any filesystem or git operations.\n\nReads `.agents-reverse-engineer.yaml` via `existsSync()` and `readFileSync(configPath, 'utf-8')`, performs substring search for `'hook_enabled: false'` (no YAML parser) and exits if match found.\n\n## Change Detection Strategy\n\nInvokes `execSync('git status --porcelain', { encoding: 'utf-8' })` to obtain working tree status. Exits silently if output is empty string after `trim()` (no changes since last commit) or if command throws (non-git repo, git unavailable).\n\n## Background Update Spawn\n\nCalls `spawn('npx', ['agents-reverse-engineer@latest', 'update', '--quiet'], { stdio: 'ignore', detached: true })` to execute incremental update workflow. Child process detached from parent via `detached: true` and unreferenced via `child.unref()` to allow parent exit without waiting for completion.\n\nUses `stdio: 'ignore'` to suppress all output streams, preventing stdio inheritance that could block session close.\n\n## Execution Environment\n\nShebang `#!/usr/bin/env node` enables direct execution as script. Imports `execSync`, `spawn` from `'child_process'` module and `existsSync`, `readFileSync` from `'fs'` module using ES module syntax.\n\nNo error handling for spawn failure—detached process errors are silently lost due to `stdio: 'ignore'`.\n### opencode-are-check-update.js\n**Purpose:** opencode-are-check-update.js spawns detached background process checking npm registry for agents-reverse-engineer ver...\n\n**opencode-are-check-update.js spawns detached background process checking npm registry for agents-reverse-engineer version updates on OpenCode session creation, writes comparison result to `~/.config/opencode/cache/are-update-check.json`.**\n\n## Exported Interface\n\n- `AreCheckUpdate()` — Async factory returning OpenCode plugin object with `event['session.created']` handler\n\n## Plugin Lifecycle Hook\n\n`AreCheckUpdate()` returns plugin object with `event: { 'session.created': async () => {...} }` structure. OpenCode invokes handler when new session starts.\n\n## Version File Resolution Strategy\n\nChecks two locations in priority order:\n1. Project-local: `<cwd>/.opencode/ARE-VERSION`\n2. Global: `~/.config/opencode/ARE-VERSION`\n\nReads first existing file to determine installed version, defaults to `'0.0.0'` if neither exists.\n\n## Cache Directory Creation\n\nConstructs cache path: `join(homedir(), '.config', 'opencode', 'cache', 'are-update-check.json')`. Calls `mkdirSync(cacheDir, { recursive: true })` if directory missing via `!existsSync(cacheDir)` check.\n\n## Detached Subprocess Pattern\n\nSpawns background process using `spawn(process.execPath, ['-e', scriptString], { stdio: 'ignore', detached: true, windowsHide: true })`. Calls `child.unref()` to allow parent process exit without waiting. Subprocess runs Node.js inline script via `-e` flag.\n\n## Inline Script Execution\n\nSubprocess script uses synchronous Node.js APIs:\n- `fs.readFileSync(projectVersionFile, 'utf8')` or `fs.readFileSync(globalVersionFile, 'utf8')` for installed version\n- `execSync('npm view agents-reverse-engineer version', { encoding: 'utf8', timeout: 10000, windowsHide: true })` for latest registry version\n- `fs.writeFileSync(cacheFile, JSON.stringify(result))` for cache persistence\n\n## Cache Schema\n\nWrites JSON object with shape:\n```javascript\n{\n  update_available: boolean,  // true when latest !== installed\n  installed: string,          // version from ARE-VERSION file\n  latest: string,             // npm registry version or 'unknown'\n  checked: number             // Unix timestamp (seconds)\n}\n```\n\n## Error Handling\n\nTry-catch blocks around `fs.readFileSync()` (version file read) and `execSync()` (npm query) prevent subprocess crash. Failed version read defaults installed to `'0.0.0'`, failed npm query sets latest to `null` (serialized as `'unknown'`).\n\n## Platform-Specific Configuration\n\nUses OpenCode conventional paths:\n- Config directory: `~/.config/opencode/` (not overridable by environment)\n- Cache subdirectory: `.config/opencode/cache/`\n- Version file: `.config/opencode/ARE-VERSION`\n\nContrasts with Claude (`~/.claude/`) and Gemini (`~/.gemini/`) paths in sibling hook files.\n\n## Integration with ARE Installer\n\nInstaller writes `ARE-VERSION` file during setup (see `src/installer/operations.ts`). Hook reads this sentinel to compare against npm registry. Version file absence triggers `'0.0.0'` default, causing `update_available: true` for any published version.\n### opencode-are-session-end.js\n**Purpose:** OpenCode session-end plugin that spawns background `are update` process when session closes if git working tree has u...\n\n**OpenCode session-end plugin that spawns background `are update` process when session closes if git working tree has uncommitted changes.**\n\n## Exported Symbol\n\n`AreSessionEnd` — Async factory function returning OpenCode plugin object with `event['session.deleted']` handler that executes update workflow in detached background process.\n\n**Signature:**\n```typescript\nexport const AreSessionEnd = async () => Promise<{\n  event: {\n    'session.deleted': () => Promise<void>\n  }\n}>\n```\n\n## Execution Flow\n\n`AreSessionEnd()` returns plugin with `'session.deleted'` event handler. Handler exits early if `process.env.ARE_DISABLE_HOOK === '1'` or if `.agents-reverse-engineer.yaml` exists and contains substring `'hook_enabled: false'` (uses `readFileSync()` + `includes()` without YAML parsing). Executes `execSync('git status --porcelain', { encoding: 'utf-8' })` and exits silently if output is empty or if `execSync()` throws (non-git repo). On non-empty status, spawns detached child process via `spawn('npx', ['agents-reverse-engineer@latest', 'update', '--quiet'], { stdio: 'ignore', detached: true })` followed by `child.unref()` to prevent blocking session close.\n\n## Behavioral Contracts\n\n**Git status command:** `'git status --porcelain'` with `{ encoding: 'utf-8' }` — detects uncommitted changes (modified/staged/untracked files).\n\n**Update invocation:** `['npx', 'agents-reverse-engineer@latest', 'update', '--quiet']` — spawns latest published version with `--quiet` flag to suppress terminal output.\n\n**Disable mechanisms:**\n- Environment variable check: `process.env.ARE_DISABLE_HOOK === '1'`\n- Config substring check: `config.includes('hook_enabled: false')` against `.agents-reverse-engineer.yaml` file contents\n\n**Config file path:** `'.agents-reverse-engineer.yaml'` (project root, checked via `existsSync(configPath)`)\n\n## Integration Point\n\nOpenCode plugin system loads this module and invokes `AreSessionEnd()` factory. Returned plugin object registers `'session.deleted'` lifecycle hook. Companion to `opencode-are-check-update.js` (SessionStart equivalent).\n\n## Process Management Pattern\n\nUses detached spawn with stdio ignore and unref to avoid blocking parent process termination:\n```javascript\nspawn(cmd, args, { stdio: 'ignore', detached: true }).unref()\n```\n\nNo timeout enforcement or SIGTERM/SIGKILL escalation (unlike `src/ai/subprocess.ts` resource-constrained subprocess execution). Update process runs in background indefinitely until natural completion.\n\n## Project Directory Structure\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>",
      "systemPrompt": "You are generating an AGENTS.md file — a directory-level overview for AI coding assistants.\n\nCRITICAL: Output ONLY the raw markdown content. No code fences, no preamble, no explanations, no conversational text. Your entire response IS the AGENTS.md file content.\n\nOUTPUT FORMAT:\n- First line MUST be exactly: <!-- Generated by agents-reverse-engineer -->\n- Use a # heading with the directory name\n- Write a one-paragraph purpose statement for the directory\nusage\nADAPTIVE SECTIONS:\nAnalyze the directory contents and choose the most relevant sections. Do NOT use a fixed template. Instead, select sections that best document this specific directory for an AI that needs to reconstruct or extend the project.\n\nConsider these section types (choose what applies):\n- **Contents**: Group files by purpose/category under ## headings. For each file: markdown link [filename](./filename) and a one-line description.\n- **Subdirectories**: If subdirectories exist, list them with links [dirname/](./dirname/) and brief summaries.\n- **Architecture / Data Flow**: If files form a pipeline, request/response chain, or layered architecture, document it.\n- **Stack**: If this is a package root (has package.json, Cargo.toml, go.mod, etc.), document the technology stack, key scripts, and entry points.\n- **Structure**: If the directory layout follows a convention (feature-sliced, domain-driven, MVC, etc.), document it.\n- **Patterns**: If files share recurring design patterns (factory, strategy, middleware, barrel re-export), name and document them.\n- **Configuration**: If the directory contains config files, schemas, or environment definitions, document the config surface area.\n- **API Surface**: If the directory exports a public API (barrel index, route definitions, SDK), document the interface contract.\n- **File Relationships**: How files collaborate, depend on each other, or share state.\n- **Behavioral Contracts**: If files contain regex patterns, format specifications, magic constants, or template strings that define observable behavior, collect them in a dedicated section. Preserve verbatim patterns from file summaries — do NOT paraphrase regex into prose. This section is MANDATORY when file summaries contain behavioral artifacts.\n\nChoose any relevant sections or create your own based on the directory contents. The goal is to provide a comprehensive overview that captures the essence of the directory's role in the project and how its files work together, with a focus on what an AI coding assistant would need to know to effectively interact with this code.\n\nSCOPE:\n- AGENTS.md is a NAVIGATIONAL INDEX — help an AI find the right file quickly\n- Focus on: what each file does, how files relate, directory-level patterns\n- Do NOT reproduce full architecture sections — those belong in the root CLAUDE.md\n\nPATH ACCURACY (MANDATORY):\n- When referencing files or modules outside this directory, use ONLY paths from the \"Import Map\" section\n- Do NOT invent, rename, or guess module paths — if a path isn't in the Import Map, don't reference it\n- Use the exact directory names from \"Project Directory Structure\" — do NOT rename directories\n  (e.g., if the directory is called \"cli\", write \"src/cli/\", NOT \"src/commands/\")\n- Cross-module references must use the specifier format from actual import statements\n  (e.g., \"../generation/writers/sum.js\", NOT \"../fs/sum-file.js\")\n- If you are unsure about a path, omit the cross-reference rather than guessing\n\nCONSISTENCY (MANDATORY):\n- Do not contradict yourself within the same document\n- If you describe a technique (e.g., \"regex-based\"), do not call it something else later (e.g., \"AST-based\")\n- When stating version numbers, engines, or config fields, use ONLY values present in the file summaries\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this directory\", \"this module\", \"provides\", \"responsible for\", \"is used to\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Per-file descriptions in Contents sections: 1-2 sentences maximum. Reference key symbols but do not reproduce full summaries.\n- Behavioral contracts (regex patterns, format specs, constants) belong in a separate Behavioral Contracts section, not in per-file descriptions.\n- Subdirectory descriptions: 1-2 sentences maximum. Capture the directory's role, not its full contents.\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- Key exported symbols from file summaries MUST appear in the directory overview\n- Preserve exact casing of identifiers\n\nUSER NOTES:\n- If \"User Notes\" are provided in the prompt, they contain user-defined instructions that will be automatically prepended to your output\n- Do NOT repeat or paraphrase user notes in your generated content — they are included separately\n- You may reference information from user notes for context",
      "response": "<!-- Generated by agents-reverse-engineer -->\n\n# hooks/\n\n**Session lifecycle hooks for Claude Code, Gemini CLI, and OpenCode runtimes that execute version checking on session start and incremental documentation updates on session end via detached background processes.**\n\n## Contents\n\n### Session Start Hooks\n\n**[are-check-update.js](./are-check-update.js)** — Claude/Gemini SessionStart hook spawning detached `npm view agents-reverse-engineer version` subprocess, comparing against local `ARE-VERSION` file (project `.claude/ARE-VERSION` before global `~/.claude/ARE-VERSION`), writing comparison result to `~/.claude/cache/are-update-check.json` with schema `{ update_available, installed, latest, checked }`.\n\n**[opencode-are-check-update.js](./opencode-are-check-update.js)** — OpenCode plugin exporting `AreCheckUpdate()` async factory returning `event['session.created']` handler that spawns detached npm version check subprocess, writes cache to `~/.config/opencode/cache/are-update-check.json`.\n\n### Session End Hooks\n\n**[are-session-end.js](./are-session-end.js)** — Claude/Gemini SessionEnd hook executing `git status --porcelain` change detection, spawning detached `npx agents-reverse-engineer@latest update --quiet` subprocess if uncommitted changes detected, exits silently if `ARE_DISABLE_HOOK=1` or config contains `hook_enabled: false` substring.\n\n**[opencode-are-session-end.js](./opencode-are-session-end.js)** — OpenCode plugin exporting `AreSessionEnd()` async factory returning `event['session.deleted']` handler that executes identical git status check and detached update spawn as Claude/Gemini variant.\n\n## Architecture\n\n### Detached Subprocess Pattern\n\nAll hooks use identical background process spawning pattern:\n```javascript\nspawn(process.execPath, ['-e', scriptString], { \n  stdio: 'ignore', \n  detached: true, \n  windowsHide: true \n}).unref()\n```\n\nInline script via `-e` flag executes synchronous Node.js APIs (`execSync`, `readFileSync`, `writeFileSync`) in isolated process that survives parent termination. `stdio: 'ignore'` prevents output blocking session lifecycle, `unref()` allows parent exit without waiting for completion.\n\n### Version File Resolution\n\nCheck update hooks read sentinel `ARE-VERSION` file in priority order:\n1. Project-local: `<cwd>/.{claude,opencode}/ARE-VERSION`\n2. Global: `~/{.claude,.config/opencode}/ARE-VERSION`\n\nFalls back to `'0.0.0'` if neither exists. Installer writes this file during setup (`src/installer/operations.ts`).\n\n### Cache Schema\n\nVersion check subprocess writes JSON to platform-specific cache:\n- Claude/Gemini: `~/.claude/cache/are-update-check.json`\n- OpenCode: `~/.config/opencode/cache/are-update-check.json`\n\n**Format:**\n```json\n{\n  \"update_available\": boolean,\n  \"installed\": \"semver-string\",\n  \"latest\": \"semver-string\" | \"unknown\",\n  \"checked\": 1234567890\n}\n```\n\n### Disable Mechanisms\n\nSession-end hooks exit silently when:\n- `process.env.ARE_DISABLE_HOOK === '1'`\n- `.agents-reverse-engineer.yaml` exists and contains substring `'hook_enabled: false'` (no YAML parser)\n\n### Change Detection\n\nSession-end hooks execute `execSync('git status --porcelain', { encoding: 'utf-8' })` and exit silently if output is empty (no uncommitted changes) or if command throws (non-git repo).\n\n## File Relationships\n\n**Hook installation:** `src/installer/operations.ts` copies hooks to platform-specific directories and writes `ARE-VERSION` sentinel file.\n\n**Build process:** `scripts/build-hooks.js` copies hook files to `hooks/dist/` directory for npm tarball inclusion before publish.\n\n**Session-end update invocation:** Spawns `npx agents-reverse-engineer@latest update --quiet` which executes `src/cli/update.ts` orchestrator with hash-based change detection and incremental regeneration.\n\n**Platform path resolution:** `src/installer/paths.ts` provides platform-specific config directory lookup respecting environment overrides (`CLAUDE_CONFIG_DIR`, `OPENCODE_CONFIG_DIR`, `GEMINI_CONFIG_DIR`).\n\n## Behavioral Contracts\n\n**Git status command:** `'git status --porcelain'` with `{ encoding: 'utf-8' }` — detects uncommitted changes (modified/staged/untracked files).\n\n**npm version query:** `'npm view agents-reverse-engineer version'` with `{ encoding: 'utf8', timeout: 10000, windowsHide: true }` — queries registry for latest published version.\n\n**Update invocation:** `['npx', 'agents-reverse-engineer@latest', 'update', '--quiet']` — spawns latest published version with `--quiet` flag suppressing terminal output.\n\n**Config disable check pattern:** `readFileSync('.agents-reverse-engineer.yaml', 'utf-8').includes('hook_enabled: false')` — substring match without YAML parsing for fast disable detection.\n\n**Cache directory creation:** `mkdirSync(cacheDir, { recursive: true })` — ensures `~/{.claude,.config/opencode}/cache/` exists before subprocess spawn.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 1325,
      "cacheReadTokens": 17477,
      "cacheCreationTokens": 11715,
      "latencyMs": 33730,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": []
    },
    {
      "timestamp": "2026-02-09T16:15:51.564Z",
      "prompt": "Generate AGENTS.md for directory: \"src\" (src)\n\n## File Summaries (1 files)\n\n### version.ts\n**Purpose:** getVersion() reads package.json from the filesystem to return the current package version string, with fallback to 'u...\n\n**getVersion() reads package.json from the filesystem to return the current package version string, with fallback to 'unknown' on error.**\n\n## Exported Functions\n\n- **getVersion(): string** — Synchronously reads `package.json` from the parent directory of the compiled module, parses JSON, extracts `version` field, returns 'unknown' on any error (file not found, JSON parse failure, missing version field)\n\n## Implementation Details\n\nUses `import.meta.url` → `fileURLToPath()` → `dirname()` to resolve current module directory, then `join(__dirname, '..', 'package.json')` to traverse one level up (from `dist/` to project root in compiled output). Reads file synchronously via `readFileSync(packagePath, 'utf-8')`, parses with `JSON.parse()`, accesses `packageJson.version` with `|| 'unknown'` fallback for nullish values. Wraps entire operation in try-catch returning 'unknown' on exception.\n\n## Usage Context\n\nCalled by CLI entry point (`src/cli/index.ts`) to display version in help text and version flag output. Called by session lifecycle hooks (`hooks/are-check-update.js`) to compare local version against npm registry during update checks. Read at runtime from compiled `dist/version.js` after build process (`npm run build` via tsc).\n\n## Path Resolution\n\nCompiled module location: `dist/version.js` (ES2022 target, NodeNext module resolution per tsconfig.json). Package.json location: project root `package.json`. Relative traversal: `dist/` → project root via `..` ensures correct resolution regardless of execution context (global install, npx, local node_modules).\n\n## Project Directory Structure\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\n\n## Subdirectories\n\n### ai/\n<!-- Generated by agents-reverse-engineer -->\n\n# src/ai\n\nBackend-agnostic AI service orchestration layer: subprocess-spawning adapters for Claude Code/Gemini/OpenCode CLIs, exponential backoff retry logic with rate limit detection, timeout enforcement via SIGTERM/SIGKILL escalation, NDJSON telemetry logging with token cost tracking, and trace event emission for concurrent pool workflows.\n\n## Contents\n\n**[index.ts](./index.ts)** — Barrel export consolidating AIService, BackendRegistry, createBackendRegistry, resolveBackend, detectBackend, getInstallInstructions, withRetry, runSubprocess, isCommandOnPath with type re-exports (AIBackend, AIResponse, AICallOptions, SubprocessResult, RetryOptions, TelemetryEntry, RunLog, FileRead, AIServiceError).\n\n**[registry.ts](./registry.ts)** — BackendRegistry stores AIBackend implementations in insertion-order Map with `register(backend)`, `get(name)`, `getAll()` methods; createBackendRegistry() pre-populates ClaudeBackend/GeminiBackend/OpenCodeBackend; resolveBackend(registry, 'auto'|name) performs auto-detection or explicit lookup throwing CLI_NOT_FOUND with install instructions.\n\n**[retry.ts](./retry.ts)** — withRetry() executes async function with exponential backoff (delay = min(baseDelayMs × multiplier^attempt, maxDelayMs) + jitter[0..500ms]) calling isRetryable(error) predicate before each retry, invoking onRetry(attempt, error) callback for telemetry.\n\n**[service.ts](./service.ts)** — AIService orchestrates call(options) via runSubprocess() wrapped in withRetry(), detects rate limits via stderr pattern matching (['rate limit', '429', 'too many requests', 'overloaded']), emits subprocess:spawn/exit/retry trace events, accumulates TelemetryEntry records via internal TelemetryLogger, writes RunLog to `.agents-reverse-engineer/logs/run-<timestamp>.json` on finalize(), enforces cleanup via cleanupOldLogs(keepRuns).\n\n**[subprocess.ts](./subprocess.ts)** — runSubprocess() spawns child process via execFile() with stdin piping, timeout enforcement (SIGTERM at timeoutMs, SIGKILL after 5s grace), process group killing (`kill(-pid)`) for tree termination, concurrent subprocess tracking via Map<pid, {command, spawnedAt}>, returns SubprocessResult with stdout/stderr/exitCode/signal/durationMs/timedOut/childPid.\n\n**[types.ts](./types.ts)** — Defines AIBackend interface (isAvailable/buildArgs/parseResponse/getInstallInstructions), AICallOptions (prompt/systemPrompt/model/timeoutMs/maxTurns/taskLabel), AIResponse (text/model/inputTokens/outputTokens/cacheReadTokens/cacheCreationTokens/durationMs/exitCode/raw), SubprocessResult, RetryOptions, TelemetryEntry, RunLog with summary aggregation, FileRead, AIServiceError with discriminated codes ('CLI_NOT_FOUND'|'TIMEOUT'|'PARSE_ERROR'|'SUBPROCESS_ERROR'|'RATE_LIMIT').\n\n## Subdirectories\n\n**[backends/](./backends/)** — ClaudeBackend with Zod-validated JSON parsing extracting usage.input_tokens/output_tokens/cache_read_input_tokens/cache_creation_input_tokens, GeminiBackend/OpenCodeBackend stubs throwing SUBPROCESS_ERROR, isCommandOnPath() cross-platform PATH detection with Windows PATHEXT extension iteration.\n\n**[telemetry/](./telemetry/)** — TelemetryLogger accumulates per-call entries computing aggregate summary (totalInputTokens/totalCacheReadTokens/errorCount/uniqueFilesRead), writeRunLog() serializes to `.agents-reverse-engineer/logs/run-<timestamp>.json` with filename sanitization (`/[:.]/g → '-'`), cleanupOldLogs() enforces retention via lexicographic sort on ISO 8601 filenames.\n\n## Architecture\n\n### Three-Layer Design\n\n**Backend Adapter Layer** (backends/): AIBackend implementations translate AICallOptions into CLI-specific argv arrays via buildArgs(), parse stdout JSON into normalized AIResponse via parseResponse(), detect availability via isCommandOnPath() checking PATH directories with Windows PATHEXT handling.\n\n**Subprocess Execution Layer** (subprocess.ts): runSubprocess() spawns execFile() with 10MB maxBuffer, writes input to stdin via Buffer.byteLength() computed payload, enforces timeout via SIGTERM then SIGKILL escalation with unref()'d timer, kills process groups via negative PID (`process.kill(-child.pid, 'SIGKILL')`), tracks active subprocesses in Map for concurrency monitoring.\n\n**Service Orchestration Layer** (service.ts): AIService wraps runSubprocess() calls in withRetry() with isRateLimitStderr() predicate detecting ['rate limit', '429', 'too many requests', 'overloaded'] patterns, emits subprocess:spawn/exit trace events via ITraceWriter, accumulates TelemetryEntry[] via TelemetryLogger, finalizes RunLog with summary (totalInputTokens, totalCacheReadTokens, errorCount, uniqueFilesRead).\n\n### Retry Strategy\n\nwithRetry() executes fn() up to maxRetries+1 times with exponential backoff: `min(baseDelayMs * multiplier^attempt, maxDelayMs) + random(0..500ms)`. Checks isRetryable(error) before each sleep—permanently fails on auth errors or non-retryable signals. Invokes onRetry(attempt, error) before delay for trace emission. AIService.call() only retries RATE_LIMIT errors (code === 'RATE_LIMIT'), treats TIMEOUT as permanent failure to prevent resource exhaustion.\n\n### Timeout Enforcement\n\nrunSubprocess() sends SIGTERM at timeoutMs via execFile killSignal option. Sets unref()'d SIGKILL timer at `timeoutMs + 5000ms`. Clears timer in callback if process exits before escalation. Process group killing (`kill(-pid)`) terminates entire subprocess tree. Falls back to single-process kill if group signal fails.\n\n### Telemetry Pipeline\n\nAIService.call() records TelemetryEntry after each subprocess completion with timestamp/prompt/systemPrompt/response/model/inputTokens/outputTokens/cacheReadTokens/cacheCreationTokens/latencyMs/exitCode/error/retryCount/thinking/filesRead. TelemetryLogger.addEntry() appends to in-memory entries[]. AIService.addFilesReadToLastEntry(filesRead) mutates most recent entry to attach FileRead[] metadata (path + sizeBytes). AIService.finalize() calls TelemetryLogger.toRunLog() computing summary, writes to `.agents-reverse-engineer/logs/run-<timestamp>.json` via writeRunLog(), enforces retention via cleanupOldLogs(keepRuns).\n\n### Resource Management\n\nSubprocess limits injected by AIService via environment variables (set in src/ai/service.ts, executed in subprocess.ts):\n- `NODE_OPTIONS='--max-old-space-size=512'` — limits heap to 512MB per subprocess\n- `UV_THREADPOOL_SIZE='4'` — constrains libuv thread pool to 4 threads\n- `CLAUDE_CODE_DISABLE_BACKGROUND_TASKS='1'` — prevents background task spawning\n- CLI args: `--disallowedTools Task` — blocks subagent spawning\n\nactiveSubprocesses Map tracks concurrent processes keyed by PID with {command, spawnedAt}. getActiveSubprocessCount() returns Map size. getActiveSubprocesses() computes runningMs as `Date.now() - spawnedAt`.\n\n### Debug Logging\n\nAIService.setDebug(true) enables stderr output before/after subprocess with heapUsed/rss metrics via formatBytes(). AIService.setSubprocessLogDir(dir) writes per-subprocess `.log` files with metadata header (task/pid/command/exit/signal/duration/timed_out) followed by stdout/stderr sections. Serialized via logWriteQueue promise chain preventing concurrent mkdir races. Failures silently swallowed (non-critical).\n\n## Integration Points\n\n**Phase 1 Orchestration**: `src/generation/executor.ts` calls AIService.call() for each .sum file generation passing prompt from `src/generation/prompts/builder.ts`, attaches filesRead metadata via addFilesReadToLastEntry() after file analysis.\n\n**Worker Pool**: `src/orchestration/pool.ts` shares single AIService instance across N workers (default 2 for WSL, 5 elsewhere), monitors concurrency via getActiveSubprocessCount(), emits trace events via AIService.setTracer(tracer).\n\n**Trace Emission**: AIService subprocess:spawn/exit events serialized to `.agents-reverse-engineer/traces/trace-<timestamp>.ndjson` via `src/orchestration/trace.ts` TraceWriter with promise-chain ordering guarantees.\n\n**Registry Auto-Detection**: `src/cli/generate.ts` calls createBackendRegistry() then resolveBackend(registry, config.ai.backend) for backend selection, throws CLI_NOT_FOUND with install instructions when backend unavailable.\n\n**Incremental Updates**: `src/update/orchestrator.ts` reuses same AIService instance for modified file regeneration, shares telemetry accumulation across discovery + analysis phases.\n\n## Behavioral Contracts\n\n### Rate Limit Detection Patterns\n\n```javascript\n['rate limit', '429', 'too many requests', 'overloaded'].some(p => \n  stderr.toLowerCase().includes(p)\n)\n```\n\n### Timeout Detection\n\n```javascript\nresult.timedOut === true  // set when error.killed === true in execFile callback\n```\n\n### Exit Code Extraction\n\n```javascript\nerror === null ? 0\n: typeof error.code === 'number' ? error.code\n: child.exitCode !== null ? child.exitCode\n: 1\n```\n\n### Process Group Killing\n\n```javascript\nprocess.kill(-child.pid, 'SIGKILL')  // negative PID targets process group\n```\n\n### Filename Sanitization\n\n```javascript\nrunLog.startTime.replace(/[:.]/g, '-')  // ISO 8601 → filesystem-safe\n// Example: 2026-02-07T12:00:00.000Z → run-2026-02-07T12-00-00-000Z.json\n```\n\n### Log File Filtering\n\n```javascript\nname.startsWith('run-') && name.endsWith('.json')\n```\n### change-detection/\n<!-- Generated by agents-reverse-engineer -->\n\n# src/change-detection\n\nGit-based change detection and SHA-256 content hashing for incremental documentation updates. Parses `git diff --name-status -M` with rename detection, merges uncommitted working tree changes, and computes file content hashes for `.sum` frontmatter verification.\n\n## Contents\n\n### Core Implementation\n\n**[detector.ts](./detector.ts)** — Implements `isGitRepo()`, `getCurrentCommit()`, `getChangedFiles()` with git diff parsing, `computeContentHash()` and `computeContentHashFromString()` for SHA-256 hex digest generation. Parses status codes (`'A'`/`'M'`/`'D'`/`'R'` prefix) from `git diff --name-status -M` (50% similarity threshold), merges `status.modified[]`, `status.deleted[]`, `status.not_added[]`, `status.staged[]` when `includeUncommitted` is true. Returns `ChangeDetectionResult` with `currentCommit`, `baseCommit`, `changes[]`, `includesUncommitted`.\n\n**[types.ts](./types.ts)** — Defines `ChangeType` union (`'added' | 'modified' | 'deleted' | 'renamed'`), `FileChange` interface with `path`, `status`, optional `oldPath` (renames), optional `contentHash`, `ChangeDetectionResult` interface with `currentCommit`, `baseCommit`, `changes[]`, `includesUncommitted`, and `ChangeDetectionOptions` interface with `includeUncommitted` flag.\n\n**[index.ts](./index.ts)** — Barrel export re-exporting `isGitRepo()`, `getCurrentCommit()`, `getChangedFiles()`, `computeContentHash()`, `computeContentHashFromString()` from `detector.ts` and all types from `types.ts`.\n\n## Integration Points\n\n- **src/update/orchestrator.ts**: Calls `getChangedFiles()` with `includeUncommitted` flag, compares `FileChange.contentHash` against `.sum` YAML frontmatter `content_hash` field, builds `filesToAnalyze` and `filesToSkip` sets, uses `ChangeType === 'deleted' | 'renamed'` for orphan cleanup targeting.\n- **src/generation/writers/sum.ts**: Calls `computeContentHash()` before writing `.sum` files to populate YAML frontmatter `content_hash` field.\n- **src/update/orphan-cleaner.ts**: Uses `FileChange.oldPath` (renames) and `FileChange.path` (deletes) to identify stale `.sum` files requiring removal.\n\n## Behavioral Contracts\n\n**Git diff parsing:** Splits `git diff --name-status -M` output on newlines, splits lines on tab separator. Status codes map to `ChangeType`:\n- `'A'` → `status: 'added'`\n- `'M'` → `status: 'modified'`\n- `'D'` → `status: 'deleted'`\n- `startsWith('R')` → `status: 'renamed'` with `oldPath = parts[1]`, `path = parts[parts.length - 1]`\n\n**SHA-256 hashing:** Uses `crypto.createHash('sha256').update(content).digest('hex')` producing lowercase hex string (64 characters). Matches YAML frontmatter pattern `content_hash: [a-f0-9]{64}`.\n\n**Uncommitted merge:** When `includeUncommitted` is true, deduplicates via `changes.some(c => c.path === file)` before pushing `status.modified[]`, `status.deleted[]`, `status.not_added[]`, `status.staged[]` entries.\n### cli/\n<!-- Generated by agents-reverse-engineer -->\n\n# src/cli\n\nCLI command entry points for the agents-reverse-engineer tool, routing parsed arguments to specialized command handlers and orchestrating multi-phase documentation generation workflows.\n\n## Contents\n\n### [clean.ts](./clean.ts)\n`cleanCommand()` deletes generated artifacts (`.sum`, `AGENTS.md` with marker, `CLAUDE.md`, `GENERATION-PLAN.md`) via `fast-glob` discovery, filters `AGENTS.md` by `GENERATED_MARKER` presence, restores `AGENTS.local.md` backups, preserves user-authored files.\n\n### [discover.ts](./discover.ts)\n`discoverCommand()` executes file discovery via `discoverFiles()` filter chain, builds `GenerationPlan` via `createOrchestrator().createPlan()`, transforms via `buildExecutionPlan()` post-order traversal, writes markdown to `GENERATION-PLAN.md`, emits `discovery:start/end` trace events with `filesIncluded/filesExcluded/durationMs` metrics.\n\n### [generate.ts](./generate.ts)\n`generateCommand()` orchestrates three-phase pipeline: resolves AI backend via `resolveBackend()`, instantiates `AIService` with timeout/retry/model config, executes `runner.executeGenerate()` for concurrent file analysis → directory aggregation → root synthesis, writes telemetry/traces, exits with status 0/1/2 for success/partial/total failure.\n\n### [index.ts](./index.ts)\nMain entry point (`#!/usr/bin/env node`) parses `process.argv` via `parseArgs()`, routes to command handlers (`initCommand`, `cleanCommand`, `discoverCommand`, `generateCommand`, `updateCommand`, `specifyCommand`), invokes `runInstaller()` for install/uninstall/interactive modes, prints version via `getVersion()` and usage via `USAGE` constant.\n\n### [init.ts](./init.ts)\n`initCommand()` creates `.agents-reverse-engineer/config.yaml` via `writeDefaultConfig()`, checks existence via `configExists()`, warns on `force: false` when config exists, exits on `EACCES/EPERM` errors, logs guidance referencing `exclude.patterns`, `ai.concurrency`, `ai.backend`.\n\n### [specify.ts](./specify.ts)\n`specifyCommand()` synthesizes project specifications from `AGENTS.md` corpus: collects docs via `collectAgentsDocs()`, auto-invokes `generateCommand()` if no docs exist, resolves backend via `resolveBackend()`, constructs prompt via `buildSpecPrompt()`, calls `AIService.call()` with extended timeout (≥600,000ms), writes output via `writeSpec()`, logs telemetry with token/duration summary.\n\n### [update.ts](./update.ts)\n`updateCommand()` executes incremental update workflow: generates `UpdatePlan` via `createUpdateOrchestrator().preparePlan()` with SHA-256 hash comparison, spawns concurrent AI analysis via `runner.executeUpdate()` for `filesToAnalyze`, regenerates `AGENTS.md` sequentially for `affectedDirs`, finalizes telemetry via `aiService.finalize()`, exits with code 0/1/2 for success/partial/total failure.\n\n## Command Routing Logic\n\n`index.ts` implements dual argument parsing paths: (1) interactive installer mode when `args.length === 0`, (2) explicit command routing when first non-flag arg matches `install|uninstall|init|clean|discover|generate|update|specify`. Installer flags (`--global/-g`, `--local/-l`, `--runtime`, `--force`) trigger `runInstaller()` with `parseInstallerArgs()`. Short flags (`-h`, `-g`, `-l`, `-V`) mapped to long forms (`help`, `global`, `local`, `version`) via inline switch statement. Flags parsed into `Set<string>` for boolean presence checks, `Map<string, string>` for key-value pairs (`--concurrency 3`, `--output ./path`). Positional args extracted after command via filter for non-flag strings (missing leading `--` or `-`).\n\n## Shared Options Pattern\n\n`GenerateOptions` interface defines six fields: `dryRun`, `concurrency`, `failFast`, `debug`, `trace`, reused by `generate.ts`/`update.ts`. `UpdateCommandOptions` extends with `uncommitted` boolean for working tree inclusion. `SpecifyOptions` adds `output` string, `force` boolean, `multiFile` boolean for directory-split output. `CleanOptions` contains only `dryRun`. Option construction in `index.ts` uses `flags.has()` for boolean checks, `values.get()` with `parseInt()` for numeric coercion, `values.get()` for string paths.\n\n## Progress Logging Strategy\n\n`discover.ts`, `generate.ts`, `update.ts`, `specify.ts` create `ProgressLog` instances via `ProgressLog.create(absolutePath)` writing to `.agents-reverse-engineer/progress.log` with ISO 8601 timestamp headers (`=== ARE <Command> (${new Date().toISOString()}) ===`), project path, file counts, phase summaries. `CommandRunner` receives `progressLog` in constructor, streams task pickup/completion events with ETA calculations. Finalized via `progressLog.finalize()` before exit. Designed for `tail -f` monitoring pattern (documented in CLAUDE.md \"Progress log\" section).\n\n## Trace Integration\n\n`generate.ts`, `update.ts`, `discover.ts` support `--trace` flag enabling NDJSON trace emission via `createTraceWriter(absolutePath, options.trace ?? false)` called before config loading. Tracer passed to `loadConfig()`, `createUpdateOrchestrator()`, `CommandRunner`, `discoverFiles()` as `options.tracer`. Events include `phase:start/end`, `worker:start/end`, `task:pickup/done`, `subprocess:spawn/exit`, `retry`, `discovery:start/end`. Finalized via `tracer.finalize()` followed by `cleanupOldTraces(absolutePath)` enforcing 500-file retention limit. Trace paths logged to stderr via `pc.dim('[trace] Writing to <path>')`.\n\n## Backend Resolution Workflow\n\n`generate.ts`, `update.ts`, `specify.ts` resolve AI backend via four-step pattern: (1) create registry via `createBackendRegistry()`, (2) call `resolveBackend(registry, config.ai.backend)` returning first available CLI (Claude/Gemini/OpenCode) or throwing `AIServiceError` with `code: 'CLI_NOT_FOUND'`, (3) catch error and print installation instructions via `getInstallInstructions(registry)` to stderr, (4) call `process.exit(2)` on CLI absence. Instantiate `AIService` with backend and config options `{ timeoutMs, maxRetries, model, telemetry }`, enable debug mode via `aiService.setDebug(true)` if `options.debug`, configure subprocess log directory via `aiService.setSubprocessLogDir(logDir)` if `options.trace`.\n\n## Exit Code Strategy\n\nCommands use three exit codes consistently: **0** for success (all files succeeded or no files to process), **1** for partial failure (`summary.filesFailed > 0 && summary.filesProcessed > 0`), **2** for total failure (`summary.filesProcessed === 0 && summary.filesFailed > 0`) or CLI not found during backend resolution. `update.ts` exits early with implicit code 0 on first run detection (`plan.isFirstRun`) or no changes (`filesToAnalyze.length === 0 && cleanup counts === 0`). `specify.ts` exits with code 1 on `SpecExistsError` during `writeSpec()`, code 2 on `AIServiceError` with `CLI_NOT_FOUND`. `init.ts` exits with code 1 on `EACCES/EPERM` during `writeDefaultConfig()`.\n\n## Dry-Run Mode Handling\n\n`generate.ts` shows plan summary via `formatPlan()` and task breakdown via `buildExecutionPlan()` without backend resolution when `options.dryRun === true`. `update.ts` displays plan via `formatPlan(plan)` including baseline commit, file statuses (`+` added, `R` renamed, `M` modified, `=` unchanged), cleanup actions, affected directories, exits before AI service instantiation. `specify.ts` computes `estimatedTokensK = Math.ceil(totalChars / 4) / 1000`, prints doc count/token estimate/output path/mode table, warns if exceeds 150K tokens. `clean.ts` enumerates artifacts to delete with `pc.dim()` paths, skips `unlink()`/`rename()` calls, prints `Dry run — no files were changed.` in yellow.\n\n## Telemetry Finalization\n\n`generate.ts`, `update.ts`, `specify.ts` finalize telemetry via `aiService.finalize(absolutePath)` returning `summary` with fields `totalInputTokens`, `totalOutputTokens`, `totalDurationMs`, `filesRead[]`. Log summary line formatted as `Tokens: ${totalInputTokens} in / ${totalOutputTokens} out | Duration: ${(totalDurationMs / 1000).toFixed(1)}s | Output: ${outputPath}`. `update.ts` records run state via `orchestrator.recordRun(currentCommit, filesProcessed, filesSkipped)` after finalization (legacy no-op since frontmatter hash migration).\n\n## Auto-Generation Fallback\n\n`specify.ts` checks `docs.length === 0` after `collectAgentsDocs()`, invokes `generateCommand(targetPath, { debug, trace })` when no AGENTS.md files exist and not in dry-run mode, re-collects docs after generation, exits with code 1 if still zero. Prints `No AGENTS.md files found after generation. Cannot proceed.` in red. Prevents specification synthesis with empty corpus.\n### config/\n<!-- Generated by agents-reverse-engineer -->\n\n# src/config\n\nConfiguration management layer providing YAML-based project configuration with Zod schema validation, adaptive concurrency computation based on system resources, and default exclusion rules for file discovery.\n\n## Contents\n\n### Core Modules\n\n**[schema.ts](./schema.ts)** — Exports `ConfigSchema` (root Zod schema accepting empty object), `ExcludeSchema` (patterns/vendorDirs/binaryExtensions arrays), `OptionsSchema` (followSymlinks/maxFileSize), `OutputSchema` (colors boolean), `AISchema` (backend enum/model/timeoutMs/maxRetries/concurrency/telemetry with dynamic default via `getDefaultConcurrency()` invocation), and inferred TypeScript types `Config`, `ExcludeConfig`, `OptionsConfig`, `OutputConfig`, `AIConfig`.\n\n**[loader.ts](./loader.ts)** — Exports `loadConfig(root, options)` reading `.agents-reverse-engineer/config.yaml` via YAML parsing and `ConfigSchema.parse()` validation (returns defaults on ENOENT, emits `config:loaded` trace event, throws `ConfigError` on ZodError/parse failures), `configExists(root)` checking file presence via `fs.access()`, `writeDefaultConfig(root)` generating commented YAML template with inline defaults, `ConfigError` class extending Error with `filePath` and `cause` properties, and constants `CONFIG_DIR` ('.agents-reverse-engineer'), `CONFIG_FILE` ('config.yaml').\n\n**[defaults.ts](./defaults.ts)** — Exports `getDefaultConcurrency()` computing worker pool size via formula `clamp(cores * 5, 2, min(memCap, 20))` where `memCap = floor((totalMemGB * 0.5) / 0.512)` caps memory usage at 50% of RAM divided by 512MB subprocess heap budget, constants `DEFAULT_VENDOR_DIRS` (18 directories: node_modules/.git/dist/build/__pycache__/.next/venv/.venv/target/.cargo/.gradle/.agents-reverse-engineer/.agents/.planning/.claude/.opencode/.gemini/vendor), `DEFAULT_EXCLUDE_PATTERNS` (26 globs: AI docs/lock files/dotfiles), `DEFAULT_BINARY_EXTENSIONS` (26 extensions: images/archives/executables/media/documents/fonts/compiled), `DEFAULT_MAX_FILE_SIZE` (1MB threshold), and `DEFAULT_CONFIG` object spreading all default arrays.\n\n## Configuration Schema Hierarchy\n\nConfigSchema composes four nested schemas via Zod object merging:\n\n1. **exclude** (ExcludeSchema) — File/directory filtering rules\n   - `patterns: string[]` — Gitignore-style globs (default: 26 patterns excluding AGENTS.md/CLAUDE.md/lock files/*.sum/*.log)\n   - `vendorDirs: string[]` — Third-party directories to skip (default: 18 directories)\n   - `binaryExtensions: string[]` — Non-text file extensions (default: 26 extensions)\n\n2. **options** (OptionsSchema) — Discovery traversal behavior\n   - `followSymlinks: boolean` — Enable symlink following (default: false)\n   - `maxFileSize: number` — Binary detection threshold in bytes (default: 1048576 = 1MB)\n\n3. **output** (OutputSchema) — CLI rendering preferences\n   - `colors: boolean` — Enable ANSI color codes (default: true)\n\n4. **ai** (AISchema) — AI service orchestration parameters\n   - `backend: 'claude' | 'gemini' | 'opencode' | 'auto'` — CLI backend selection (default: 'auto')\n   - `model: string` — Backend-specific model identifier (default: 'sonnet')\n   - `timeoutMs: number` — Subprocess timeout milliseconds (default: 300000 = 5 minutes)\n   - `maxRetries: number` — Exponential backoff retry attempts (default: 3, min: 0)\n   - `concurrency: number` — Worker pool size (default: dynamic via `getDefaultConcurrency()`, min: 1, max: 20)\n   - `telemetry.keepRuns: number` — Run log retention count (default: 50, min: 0)\n\n## Adaptive Concurrency Algorithm\n\n`getDefaultConcurrency()` implements memory-aware concurrency calculation:\n\n1. **CPU-based baseline**: `cores * 5` where `cores = os.availableParallelism() || os.cpus().length`\n2. **Memory capacity ceiling**: `memCap = floor((totalMemGB * 0.5) / 0.512)` allocating 50% of system RAM with 512MB per subprocess\n3. **Clamping**: `max(2, min(baseline, memCap, 20))` enforcing [2, 20] interval\n4. **Edge case handling**: Returns Infinity when `totalMemGB <= 1` to bypass memory constraint on low-RAM systems\n\nConstants:\n- `CONCURRENCY_MULTIPLIER = 5` (CPU core scaling factor)\n- `MIN_CONCURRENCY = 2` (floor matching WSL default)\n- `MAX_CONCURRENCY = 20` (ceiling matching Zod `.max(20)` constraint)\n- `SUBPROCESS_HEAP_GB = 0.512` (512MB heap budget from `NODE_OPTIONS='--max-old-space-size=512'` in subprocess.ts)\n- `MEMORY_FRACTION = 0.5` (allocate 50% of total RAM to pool)\n\n## Configuration Loading Workflow\n\n`loadConfig(root, options)` executes sequence:\n\n1. **Path resolution**: Constructs `${root}/.agents-reverse-engineer/config.yaml`\n2. **File read**: `readFile(configPath, 'utf-8')` with ENOENT catch returning `ConfigSchema.parse({})`\n3. **YAML parsing**: `parse(rawYaml)` with Error catch wrapped in ConfigError\n4. **Schema validation**: `ConfigSchema.parse(raw)` with ZodError catch formatted as `${path}: ${message}`\n5. **Trace emission**: `tracer?.emit({ type: 'config:loaded', configPath, model, concurrency })`\n6. **Debug output**: `console.error(pc.dim(JSON.stringify(config, null, 2)))` when `options.debug` enabled\n\nError paths throw ConfigError with:\n- `message`: Formatted validation errors or YAML parse message\n- `filePath`: Absolute config file path\n- `cause`: Underlying ZodError or Error instance\n\n## Default Configuration Template\n\n`writeDefaultConfig(root)` generates YAML with four comment-delimited sections:\n\n```yaml\n# FILE & DIRECTORY EXCLUSIONS\nexclude:\n  patterns: [...26 quoted globs...]\n  vendorDirs: [...18 directory names...]\n  binaryExtensions: [...26 extensions...]\n\n# DISCOVERY OPTIONS\noptions:\n  followSymlinks: false\n  maxFileSize: 1048576\n\n# OUTPUT FORMATTING\noutput:\n  colors: true\n\n# AI SERVICE CONFIGURATION\nai:\n  backend: auto\n  model: sonnet\n  timeoutMs: 300000\n  maxRetries: 3\n  # concurrency: <current machine default>\n  telemetry:\n    keepRuns: 50\n```\n\n`yamlScalar(value)` helper quotes strings containing YAML metacharacters (`[*{}\\[\\]?,:#&!|>'\"%@` ]`) via regex `/[*{}\\[\\]?,:#&!|>'\"%@`]/` test, escapes backslashes (`\\\\` → `\\\\\\\\`) and quotes (`\"` → `\\\\\"`) before wrapping in double quotes.\n\n## Integration with Codebase\n\n**Consumed by**:\n- `src/cli/init.ts` — Calls `writeDefaultConfig()` for initialization command\n- `src/cli/generate.ts`, `src/cli/update.ts`, `src/cli/discover.ts` — Call `loadConfig()` to retrieve runtime configuration\n- `src/orchestration/pool.ts` — Uses `config.ai.concurrency` for worker count\n- `src/ai/service.ts` — Uses `config.ai.backend`, `config.ai.model`, `config.ai.timeoutMs`, `config.ai.maxRetries`\n- `src/discovery/walker.ts` — Uses `config.exclude.*` and `config.options.*` for file filtering\n- `src/output/logger.ts` — Uses `config.output.colors` for picocolors enablement\n- `src/ai/telemetry/cleanup.ts` — Uses `config.ai.telemetry.keepRuns` for log retention\n\n**Provides types to**:\n- All CLI entry points requiring type-safe configuration access\n- Orchestration modules needing concurrency/timeout parameters\n- Discovery filters requiring exclusion rule arrays\n\n## Default Exclusion Patterns\n\n**DEFAULT_EXCLUDE_PATTERNS** (26 globs):\n- AI docs: `AGENTS.md`, `CLAUDE.md`, `OPENCODE.md`, `GEMINI.md` (root + recursive `**/*`)\n- Lock files: `*.lock`, `package-lock.json`, `yarn.lock`, `pnpm-lock.yaml`, `bun.lock`, `bun.lockb`, `Gemfile.lock`, `Cargo.lock`, `poetry.lock`, `composer.lock`, `go.sum`\n- Dotfiles: `.gitignore`, `.gitattributes`, `.gitkeep`, `.env` (root + recursive `**/.env*`)\n- Artifacts: `*.log`, `*.sum` (root + recursive), `**/SKILL.md`\n\n**DEFAULT_VENDOR_DIRS** (18 directories):\n- Package managers: `node_modules`, `vendor`\n- Build outputs: `dist`, `build`, `target`, `.next`, `__pycache__`\n- Version control: `.git`\n- Python venvs: `venv`, `.venv`\n- Cargo/Gradle caches: `.cargo`, `.gradle`\n- AI assistant directories: `.agents-reverse-engineer`, `.agents`, `.planning`, `.claude`, `.opencode`, `.gemini`\n\n**DEFAULT_BINARY_EXTENSIONS** (26 extensions):\n- Images: `.png`, `.jpg`, `.jpeg`, `.gif`, `.bmp`, `.ico`, `.webp`\n- Archives: `.zip`, `.tar`, `.gz`, `.rar`, `.7z`\n- Executables: `.exe`, `.dll`, `.so`, `.dylib`\n- Media: `.mp3`, `.mp4`, `.wav`\n- Documents: `.pdf`\n- Fonts: `.woff`, `.woff2`, `.ttf`, `.eot`\n- Compiled: `.class`, `.pyc`\n### discovery/\n<!-- Generated by agents-reverse-engineer -->\n\n# src/discovery\n\nFile discovery pipeline orchestrating four-filter chain execution (gitignore → vendor → binary → custom) after fast-glob directory traversal, returning attributed inclusion/exclusion results.\n\n## Contents\n\n**[run.ts](./run.ts)** — Exports `discoverFiles()` facade creating filter chain (GitignoreFilter → VendorFilter → BinaryFilter → CustomPatternFilter) via factory functions, invoking `walkDirectory()` with symlink control, applying filters via `applyFilters()` with trace/debug propagation, returning `FilterResult` with per-file attribution.\n\n**[types.ts](./types.ts)** — Defines `FileFilter` interface (`name: string`, `shouldExclude(path, stats?): boolean | Promise<boolean>`), `ExcludedFile` record (`path`, `reason`, `filter`), `FilterResult` aggregate (`included: string[]`, `excluded: ExcludedFile[]`), `WalkerOptions` config (`cwd`, `followSymlinks?`, `dot?`).\n\n**[walker.ts](./walker.ts)** — Implements `walkDirectory(options: WalkerOptions): Promise<string[]>` via `fg.glob('**/*', {absolute: true, onlyFiles: true, suppressErrors: true, ignore: ['**/.git/**']})` with cwd-relative traversal, dotfile inclusion (default true), symlink control (default false).\n\n## Subdirectories\n\n**[filters/](./filters/)** — Four filter implementations: `binary.ts` (extension Set + size threshold + content analysis), `gitignore.ts` (`.gitignore` parser via `ignore` library), `vendor.ts` (Set lookup for single-segment + substring for path patterns), `custom.ts` (user-defined glob patterns). Orchestrator `index.ts` exports `applyFilters()` with 30-worker concurrency, short-circuit evaluation, per-filter statistics (`matched`/`rejected`), `filter:applied` trace events.\n\n## Architecture\n\n### Discovery Pipeline Flow\n\n1. **Configuration** — Caller (CLI commands) loads `DiscoveryConfig` subset from full config schema via `loadConfig()`\n2. **Filter Creation** — `discoverFiles()` constructs four filters in deterministic order: `createGitignoreFilter(root)`, `createVendorFilter(config.exclude.vendorDirs)`, `createBinaryFilter({maxFileSize, additionalExtensions: config.exclude.binaryExtensions})`, `createCustomFilter(config.exclude.patterns, root)`\n3. **Directory Walking** — Delegates to `walkDirectory({cwd: root, followSymlinks: config.options.followSymlinks})` returning absolute paths via fast-glob\n4. **Filter Application** — Invokes `applyFilters(files, filters, {tracer, debug})` from `filters/index.ts` executing short-circuit filter chain across 30 concurrent workers\n5. **Result Aggregation** — Returns `FilterResult` with `included` files passing all filters, `excluded` array containing `ExcludedFile` records with `filter`/`reason` attribution\n\n### Path Normalization Contract\n\nFilters receive absolute paths from walker, convert to relative via `path.relative(normalizedRoot, absolutePath)` for pattern matching (gitignore/custom patterns). Paths outside root (relative path starts with `..`) bypass filtering. Walker emits absolute paths via `fast-glob` option `{absolute: true}`.\n\n### Filter Interface Polymorphism\n\n`FileFilter.shouldExclude()` supports both sync/async implementations via union return type `boolean | Promise<boolean>`. Binary filter uses async `fs.stat()` + `isbinaryfile.isBinaryFile()`, others use sync string matching. Filter chain executor awaits all promises via `Promise.resolve()` wrapper.\n\n### Configuration Structural Typing\n\n`DiscoveryConfig` interface in `run.ts` defines minimal subset of full `Config` schema from `src/config/schema.ts` containing only `exclude.*`, `options.maxFileSize`, `options.followSymlinks` fields. Enables unit testing with minimal mock objects, prevents tight coupling to entire config schema.\n\n## Integration Points\n\n**Callers**: `src/cli/discover.ts`, `src/cli/generate.ts`, `src/cli/update.ts` invoke `discoverFiles()` with config from `loadConfig()`.\n\n**Consumers**: `FilterResult.included` feeds Phase 1 file analysis worker pool in `src/generation/executor.ts`, `FilterResult.excluded` populates `GENERATION-PLAN.md` statistics via `PlanTracker.writeGenerationPlan()`.\n\n**Telemetry**: `ITraceWriter` from `src/orchestration/trace.ts` emits `filter:applied` events with per-filter match/rejection counts when `--trace` flag enabled.\n### generation/\n<!-- Generated by agents-reverse-engineer -->\n\n# generation\n\n**Phase orchestration, file/directory task preparation, complexity analysis, prompt construction, `.sum` + `AGENTS.md` I/O, and root document collection for the three-phase AI-driven documentation pipeline.**\n\n## Contents\n\n### [collector.ts](./collector.ts)\n`collectAgentsDocs()` recursively walks project tree via `readdir()` with `withFileTypes: true`, accumulates all `AGENTS.md` files into `Array<{ relativePath, content }>`, skips 13 directories (`node_modules`, `.git`, `.agents-reverse-engineer`, `vendor`, `dist`, `build`, `__pycache__`, `.next`, `venv`, `.venv`, `target`, `.cargo`, `.gradle`), returns lexicographically sorted array via `localeCompare()`.\n\n### [complexity.ts](./complexity.ts)\n`analyzeComplexity()` computes `ComplexityMetrics` from file array: `fileCount`, `directoryDepth` via `path.relative().split(path.sep).length - 1`, `directories` set via `path.dirname()` parent chain walk until root `.`, `files` array passthrough.\n\n### [executor.ts](./executor.ts)\n`buildExecutionPlan()` transforms `GenerationPlan` into `ExecutionPlan` with three task categories: `fileTasks` (format `file:<path>`, outputs `.sum`, zero dependencies), `directoryTasks` (format `dir:<path>`, outputs `AGENTS.md`, depends on child file IDs, sorted by `getDirectoryDepth()` descending for post-order traversal), `rootTasks` (format `root:CLAUDE.md`, depends on all directory IDs). `isDirectoryComplete()` verifies all `.sum` files exist via `sumFileExists()`. `formatExecutionPlanAsMarkdown()` generates `GENERATION-PLAN.md`.\n\n### [orchestrator.ts](./orchestrator.ts)\n`GenerationOrchestrator.createPlan()` executes seven-step pipeline: reads file content via `readFile()`, calls `analyzeComplexity()`, invokes `buildProjectStructure()` for compact tree, calls `buildFilePrompt()` per file with `projectPlan` injection, groups files by `path.dirname()` for directory task creation, concatenates `fileTasks + dirTasks`, clears `PreparedFile.content` fields to free memory, emits `phase:start`/`plan:created`/`phase:end` trace events.\n\n### [types.ts](./types.ts)\nDefines `AnalysisResult` (fields: `summary`, `metadata: SummaryMetadata`), `SummaryMetadata` (fields: `purpose`, `criticalTodos?`, `relatedFiles?`), `SummaryOptions` (fields: `targetLength: 'short'|'standard'|'detailed'`, `includeCodeSnippets: boolean`).\n\n## Subdirectories\n\n### [prompts/](./prompts/)\n`buildFilePrompt()` substitutes `{{FILE_PATH}}`, `{{CONTENT}}`, `{{LANG}}`, selects `FILE_UPDATE_SYSTEM_PROMPT` when `existingSum` present. `buildDirectoryPrompt()` reads `.sum` via `readSumFile()`, collects child `AGENTS.md`, extracts imports via `extractDirectoryImports()`, detects 9 manifest types (`package.json`, `Cargo.toml`, `go.mod`, `pyproject.toml`, `pom.xml`, `build.gradle`, `Gemfile`, `composer.json`, `CMakeLists.txt`/`Makefile`). `buildRootPrompt()` calls `collectAgentsDocs()`, parses root `package.json`, embeds synthesis constraints. Templates enforce density rules, anchor term preservation, behavioral contract extraction (verbatim regex patterns, format strings, magic constants).\n\n### [writers/](./writers/)\n`writeSumFile()` serializes YAML frontmatter (`generated_at`, `content_hash`, `purpose`, `critical_todos`, `related_files`) with adaptive array formatting (inline `[a,b,c]` for ≤3 items <40 chars, multi-line `- item` otherwise). `readSumFile()` parses via dual-pattern regex. `writeAgentsMd()` implements four-step user content preservation: detect ownership via `GENERATED_MARKER` absence, rename to `AGENTS.local.md`, prepend above LLM output with `---` separator. `getSumPath()` returns `${sourcePath}.sum`.\n\n## Architecture\n\n### Three-Phase Pipeline\n\n**Phase 1 (File Analysis):** `GenerationOrchestrator.createFileTasks()` invokes `buildFilePrompt()` per file, embeds content + project structure, populates `AnalysisTask[]` with `type: 'file'`, `systemPrompt`, `userPrompt`. Workers execute concurrently via `src/orchestration/pool.ts`, call `AIService.call()` → `runSubprocess()` → `execFile()` spawning AI CLI subprocesses with resource limits (`NODE_OPTIONS='--max-old-space-size=512'`, `UV_THREADPOOL_SIZE='4'`, `CLAUDE_CODE_DISABLE_BACKGROUND_TASKS='1'`, `--disallowedTools Task`). Results written via `writeSumFile()` to `.sum` paths from `getSumPath()`.\n\n**Phase 2 (Directory Aggregation):** `GenerationOrchestrator.createDirectoryTasks()` groups files via `Map<dirname, PreparedFile[]>`, creates `AnalysisTask[]` with `type: 'directory'`, `directoryInfo: { sumFiles, fileCount }`. Executor sorts tasks by `getDirectoryDepth()` descending (deepest first), waits for child `.sum` files via `isDirectoryComplete()`, constructs prompts via `buildDirectoryPrompt()` consuming `.sum` frontmatter + child `AGENTS.md` + import maps from `extractDirectoryImports()`. Writes via `writeAgentsMd()` preserving user content from `AGENTS.local.md`.\n\n**Phase 3 (Root Synthesis):** Sequential execution (concurrency=1) generates `CLAUDE.md`, `GEMINI.md`, `OPENCODE.md` via `buildRootPrompt()` aggregating all `AGENTS.md` via `collectAgentsDocs()`, parsing root `package.json`, enforcing synthesis-only constraints (no invention). Strips conversational preamble via pattern matching before writing output.\n\n### Dependency Graph\n\n`buildExecutionPlan()` constructs DAG with three layers:\n1. File tasks: zero dependencies, parallel-eligible\n2. Directory tasks: depend on child file IDs (`files.map(f => 'file:${f}')`), sorted by depth descending\n3. Root tasks: depend on all directory IDs\n\n`isDirectoryComplete()` predicate blocks directory processing until all child `.sum` files exist, ensuring post-order traversal guarantees data availability for parent synthesis.\n\n### Memory Management\n\n`createPlan()` clears `PreparedFile.content` fields via cast `(file as { content: string }).content = ''` after embedding content into file task prompts. Comment: \"The runner re-reads files from disk\" during execution phase. Reduces peak memory from O(total codebase size) to O(largest file × concurrency).\n\n## File Relationships\n\n`orchestrator.ts` calls `buildFilePrompt()` from `./prompts/index.js`, `analyzeComplexity()` from `./complexity.js`, `buildProjectStructure()` internally. `executor.ts` imports `sumFileExists()` from `./writers/sum.ts` for completion checks. `collector.ts` supplies `collectAgentsDocs()` to `./prompts/builder.ts` for root prompt construction. `writers/sum.ts` writes files consumed by `writers/agents-md.ts` prompt builder, both consumed by `executor.ts` via `./writers/index.js` barrel export.\n\n## Integration Points\n\nConsumed by `src/orchestration/runner.ts` which invokes `GenerationOrchestrator.createPlan()` → `buildExecutionPlan()` → three-phase execution via worker pools. Prompts passed to `AIService.call()` in `src/ai/service.ts`. Results written to filesystem via `writeSumFile()` + `writeAgentsMd()`. Trace events emitted to `src/orchestration/trace.ts`. Complexity metrics logged via `src/output/logger.ts`. Discovery input from `src/discovery/run.ts` as `DiscoveryResult`. Configuration from `src/config/loader.ts` as `Config`.\n\n## Behavioral Contracts\n\n**GENERATED_MARKER** (writers/agents-md.ts):\n```javascript\n'<!-- Generated by agents-reverse-engineer -->'\n```\n\n**Frontmatter extraction** (writers/sum.ts):\n```javascript\n/^---\\n([\\s\\S]*?)\\n---\\n/\n```\n\n**YAML field patterns**:\n```javascript\n/generated_at:\\s*(.+)/\n/content_hash:\\s*(.+)/\n/purpose:\\s*(.+)/\n/key:\\s*\\[([^\\]]*)\\]/  // Inline arrays\n/key:\\s*\\n((?:\\s+-\\s+.+\\n?)+)/m  // Multi-line arrays\n```\n\n**Array formatting heuristic** (writers/sum.ts):\n- Inline if `length <= 3` and all items `< 40` chars\n- Multi-line otherwise\n\n**Directory depth calculation** (executor.ts):\n```javascript\ndir.split(path.sep).length  // Returns 0 for '.', 1 for 'src', 2 for 'src/cli'\n```\n\n**Post-order sort comparator** (executor.ts):\n```javascript\n(a, b) => getDirectoryDepth(dirB) - getDirectoryDepth(dirA)  // Descending\n```\n\n**Task ID formats** (executor.ts):\n- File: `file:<relativePath>`\n- Directory: `dir:<relativePath>`\n- Root: `root:<filename>`\n\n**Skip directories** (collector.ts):\n```javascript\nSet(['node_modules', '.git', '.agents-reverse-engineer', 'vendor', 'dist', 'build', '__pycache__', '.next', 'venv', '.venv', 'target', '.cargo', '.gradle'])\n```\n\n**Manifest detection** (prompts/builder.ts):\n```javascript\n['package.json', 'Cargo.toml', 'go.mod', 'pyproject.toml', 'pom.xml', 'build.gradle', 'Gemfile', 'composer.json', 'CMakeLists.txt', 'Makefile']\n```\n\n**Source file filter for imports** (prompts/builder.ts):\n```javascript\n/\\.(ts|tsx|js|jsx|py|go|rs|java|kt)$/\n```\n\n**Language extension map** (prompts/builder.ts):\n22 extensions: `.ts` → `'typescript'`, `.py` → `'python'`, `.rs` → `'rust'`, etc., defaults to `'text'`.\n### imports/\n<!-- Generated by agents-reverse-engineer -->\n\n# src/imports\n\n**Static import analysis module extracting TypeScript/JavaScript import statements via regex parsing to construct dependency graphs and import maps for AI-driven documentation prompts.**\n\n## Contents\n\n### Core Extraction\n\n**[extractor.ts](./extractor.ts)** — Regex-based import parser extracting ES module syntax into `ImportEntry[]` arrays. `extractImports()` applies `IMPORT_REGEX` (`/^import\\s+(type\\s+)?(?:\\{([^}]*)\\}|(\\*\\s+as\\s+\\w+)|(\\w+))\\s+from\\s+['\"]([^'\"]+)['\"]/gm`) to capture five groups: (1) `type` keyword, (2) named symbols, (3) namespace imports, (4) default imports, (5) module specifier. `extractDirectoryImports()` scans first 100 lines of each file to optimize import-region parsing, filters bare specifiers (`'react'`, `node:*`), partitions relative imports into `internal` (`./`) and `external` (`../`). `formatImportMap()` serializes `FileImports[]` into LLM prompt text blocks with `(type)` suffix for type-only imports.\n\n**[types.ts](./types.ts)** — Type definitions for import extraction results. `ImportEntry` models single import statement with `specifier`, `symbols[]`, `typeOnly` discriminator. `FileImports` aggregates directory-level imports via `externalImports[]` (cross-directory dependencies) and `internalImports[]` (same-directory coupling). External/internal partitioning enables `AGENTS.md` prompts to emphasize architectural boundaries.\n\n**[index.ts](./index.ts)** — Barrel export exposing `extractImports()`, `extractDirectoryImports()`, `formatImportMap()`, `ImportEntry`, `FileImports` for integration with Phase 2 directory aggregation pipeline (`src/generation/prompts/builder.ts`).\n\n## Integration Points\n\n**Generation Phase 2 (Directory Aggregation):**\n- `src/generation/prompts/builder.ts` calls `extractDirectoryImports()` during `AGENTS.md` synthesis\n- Import maps injected into prompts via `formatImportMap()` output in `src/generation/prompts/templates.ts`\n- External import paths verified against filesystem constraints to prevent phantom references\n\n**Discovery Pipeline:**\n- `src/generation/orchestrator.ts` passes `discoveredFiles[]` list enabling directory-level filtering without redundant scans\n- Skips binary files, vendor directories, custom exclude patterns from upstream discovery filters\n\n**Data Flow:**\n- Runner invokes `extractDirectoryImports(dirPath, fileNames)` → reads first 100 lines → regex extraction → internal/external partitioning → `FileImports[]` return\n- Prompt builder calls `formatImportMap(fileImports)` → text serialization → template injection\n\n## Behavioral Contracts\n\n**IMPORT_REGEX Pattern:**\n```regex\n/^import\\s+(type\\s+)?(?:\\{([^}]*)\\}|(\\*\\s+as\\s+\\w+)|(\\w+))\\s+from\\s+['\"]([^'\"]+)['\"]/gm\n```\n- Group 1: `type` keyword (`import type { Foo }`)\n- Group 2: Named symbols within braces (`{ Foo, Bar as Baz }`)\n- Group 3: Namespace imports (`* as name`)\n- Group 4: Default imports (bare identifier after `import`)\n- Group 5: Module specifier (quoted string after `from`)\n- Anchored `^` for line start, `gm` flags for global multiline\n- Does NOT capture dynamic imports (`import('...')`) or side-effect imports (`import './styles.css'`)\n\n**Format Template (formatImportMap output):**\n```\nrunner.ts:\n  ../ai/index.js → AIService\n  ../generation/executor.js → ExecutionPlan, ExecutionTask\n\npool.ts:\n  ./trace.js → ITraceWriter (type)\n```\n\n**File Read Optimization:**\n- Reads only first 100 lines via `content.split('\\n').slice(0, 100).join('\\n')` before regex scanning\n- Assumption: ES module imports typically concentrated in file header region\n- Trade-off: Skips late-file dynamic import detection for 95%+ file I/O reduction\n\n**Import Classification:**\n- Bare specifiers (`'react'`, `'lodash'`) → filtered out (external packages)\n- `node:` prefixed (`'node:fs'`) → filtered out (built-in modules)\n- Relative starting `./` → `internalImports[]` (same-directory coupling)\n- Relative starting `../` → `externalImports[]` (cross-directory dependencies)\n### installer/\n<!-- Generated by agents-reverse-engineer -->\n\n# src/installer\n\nnpx installation orchestrator for deploying ARE command templates and session hooks into AI coding assistant runtime directories (`~/.claude`, `~/.config/opencode`, `~/.gemini`), with interactive prompts for missing parameters, parallel multi-runtime support, hook registration in settings.json, and permission preauthorization for Claude Code.\n\n## Contents\n\n### Core Installation Logic\n\n**[operations.ts](./operations.ts)** — Writes command templates from `src/integration/templates.ts` to runtime directories, copies bundled hook files from `hooks/dist/`, registers hooks in `settings.json` via `registerClaudeHooks()`/`registerGeminiHooks()`, writes `ARE-VERSION` file, adds Bash permission patterns to Claude Code allow list. Exports `installFiles()` fan-out across runtimes, `verifyInstallation()` checking file existence via `existsSync()`, `getPackageVersion()` reading from package.json, `formatInstallResult()` for human-readable output.\n\n**[uninstall.ts](./uninstall.ts)** — Removes command templates, hooks, hook registrations from `settings.json`, Bash permission entries from allow list, and `ARE-VERSION` file. Calls `cleanupAreSkillDirs()` deleting empty `are-*` skill directories for Claude, `cleanupLegacyGeminiFiles()` removing pre-TOML `*.md` files, `cleanupEmptyDirs()` recursively deleting empty parent directories stopping at runtime roots. Exports `uninstallFiles()`, `deleteConfigFolder()` for `.agents-reverse-engineer` removal.\n\n**[paths.ts](./paths.ts)** — Resolves runtime installation paths with environment variable overrides (`CLAUDE_CONFIG_DIR`, `OPENCODE_CONFIG_DIR`, `GEMINI_CONFIG_DIR`). Exports `getRuntimePaths()` returning `RuntimePaths` objects with `global`/`local`/`settingsFile` fields, `resolveInstallPath()` joining runtime paths with project root, `isRuntimeInstalledLocally()`/`isRuntimeInstalledGlobally()` existence checks via `stat()`.\n\n### Workflow Orchestration\n\n**[index.ts](./index.ts)** — Parses CLI args via `parseInstallerArgs()` supporting `-g`/`--global`, `-l`/`--local`, `--runtime`, `--force`, `-u`/`--uninstall`, `-q`/`--quiet`, dispatches to `runInstall()`/`runUninstall()` after resolving runtime and location via `determineRuntimes()`/`determineLocation()` with interactive prompt fallback. Calls `verifyInstallation()` checking created file existence, formats results via `displayInstallResults()`/`displayUninstallResults()` showing file counts, hook registration status, and `showNextSteps()` with ARE command list. Exits with code 1 in non-interactive mode when required flags missing.\n\n**[prompts.ts](./prompts.ts)** — Arrow-key selection in TTY mode via `arrowKeySelect()` with `readline.emitKeypressEvents()`, numbered fallback via `numberedSelect()` for non-TTY. Exports `selectRuntime()` prompting for `'claude'`/`'opencode'`/`'gemini'`/`'all'`, `selectLocation()` for `'global'`/`'local'`, `confirmAction()` for yes/no prompts, `isInteractive()` checking `process.stdin.isTTY`. Enforces raw mode cleanup via `cleanupRawMode()` in try/finally blocks and process exit handlers (`'exit'`, `'SIGINT'`) preventing terminal state corruption.\n\n### Display Formatting\n\n**[banner.ts](./banner.ts)** — Styled terminal output with ASCII art \"ARE\" logo in green via `pc.green()`, version footer via `getVersion()`, usage instructions with `pc.bold()` section headers covering CLI options and example invocations. Exports `displayBanner()`, `showHelp()`, `showNextSteps()` listing ARE commands (`/are-help`, `/are-init`, `/are-discover`, `/are-generate`, `/are-update`, `/are-specify`, `/are-clean`) in cyan, message helpers `showSuccess()`/`showError()`/`showWarning()`/`showInfo()` with prefixed symbols.\n\n### Type Definitions\n\n**[types.ts](./types.ts)** — Defines `Runtime = 'claude' | 'opencode' | 'gemini' | 'all'`, `Location = 'global' | 'local'`, `InstallerArgs` interface with boolean flags and optional runtime field, `InstallerResult` interface tracking `filesCreated[]`, `filesSkipped[]`, `errors[]`, `hookRegistered`, `versionWritten`, `RuntimePaths` interface with `global`/`local`/`settingsFile` path strings.\n\n## Installation Flow\n\n1. **Argument Parsing** — `parseInstallerArgs()` extracts flags, validates `--runtime` against `['claude', 'opencode', 'gemini', 'all']`\n2. **Interactive Prompts** — `determineLocation()` calls `selectLocation()` if both/neither `-g`/`-l` set, `determineRuntimes()` calls `selectRuntime()` if `--runtime` missing\n3. **Path Resolution** — `resolveInstallPath()` combines `getRuntimePaths(runtime)` with project root for local installs, uses global path for `-g`\n4. **File Operations** — `installFilesForRuntime()` calls `getTemplatesForRuntime()` from `src/integration/templates.ts`, writes templates via `ensureDir()` → `writeFileSync()`, copies hooks from `hooks/dist/` via `readBundledHook()` → `writeFileSync()`\n5. **Hook Registration** — `registerHooks()` dispatches to `registerClaudeHooks()` or `registerGeminiHooks()`, merges hooks into `settings.json` under `hooks.SessionStart`/`hooks.SessionEnd`, writes ARE Bash permission patterns via `registerPermissions()` for Claude only\n6. **Verification** — `verifyInstallation()` checks all file paths via `existsSync()`, reports `missing[]` array for absent files\n7. **Display Results** — `displayInstallResults()` accumulates `totalCreated`/`totalSkipped`/`hooksRegistered` counters, calls `showNextSteps()` with ARE command list unless `--quiet` flag set\n\n## Uninstallation Flow\n\n1. **File Deletion** — `uninstallFilesForRuntime()` reads templates for path extraction, deletes command files and hooks via `unlinkSync()`, removes `ARE-VERSION` file\n2. **Hook Cleanup** — `unregisterHooks()` filters `settings.json` event arrays removing hooks matching `getHookPatterns()` for current (`node ${runtimeDir}/hooks/${filename}`) and legacy (`node hooks/${filename}`) formats, deletes empty event arrays and empty `hooks` object\n3. **Permission Cleanup** — `unregisterPermissions()` filters `permissions.allow` removing all `ARE_PERMISSIONS` entries, cleans up empty `permissions.allow`/`permissions` structures\n4. **Directory Cleanup** — `cleanupAreSkillDirs()` removes empty `are-*` skill subdirectories for Claude, `cleanupLegacyGeminiFiles()` deletes pre-TOML `*.md` files and nested `are/*.toml` files, `cleanupEmptyDirs()` recursively deletes empty parents stopping at runtime roots (`.claude`, `.opencode`, `.gemini`, `.config`)\n5. **Config Removal** — `deleteConfigFolder()` removes `.agents-reverse-engineer` directory for local installs via `rmSync({ recursive: true, force: true })`\n\n## Runtime-Specific Behaviors\n\n**Claude Code:**\n- Installs to `~/.claude` (global) or `.claude` (local), overridable via `CLAUDE_CONFIG_DIR`\n- Registers hooks in nested format: `settings.hooks[event] = [{ hooks: [{ type: 'command', command: string }] }]`\n- Adds five Bash permission patterns to `settings.permissions.allow`: `Bash(npx agents-reverse-engineer@latest init*)`, `discover*`, `generate*`, `update*`, `clean*`\n- Cleans up skill directories matching `are-*` prefix during uninstall\n\n**OpenCode:**\n- Installs to `~/.config/opencode` (global) or `.opencode` (local), respects `OPENCODE_CONFIG_DIR` → `XDG_CONFIG_HOME/opencode` → default fallback chain\n- Copies plugins to `plugins/` directory (auto-loaded by OpenCode runtime)\n- No settings.json registration (plugin system auto-discovers)\n- Supports plugin filenames: `are-check-update.js`, `are-session-end.js`\n\n**Gemini:**\n- Installs to `~/.gemini` (global) or `.gemini` (local), overridable via `GEMINI_CONFIG_DIR`\n- Registers hooks in flat format: `settings.hooks[event] = [{ name: string, type: 'command', command: string }]`\n- Cleans up legacy files during uninstall: `are-*.md` (pre-TOML commands), `are/*.toml` (pre-flat structure)\n\n## Behavioral Contracts\n\n**Hook Command Pattern:**\n```\nCurrent: node ${runtimeDir}/hooks/${filename}\nLegacy:  node hooks/${filename}\n```\n\n**Permission Pattern:**\n```\nBash(npx agents-reverse-engineer@latest <command>*)\n```\n\n**Settings JSON Indentation:**\n```javascript\nJSON.stringify(settings, null, 2)  // 2-space indent\n```\n\n**Runtime Root Basenames (recursion stop):**\n```\n.claude, .opencode, .gemini, .config\n```\n\n**Hook File Sources:**\n```\nhooks/dist/are-check-update.js\nhooks/dist/are-session-end.js\nhooks/dist/opencode-are-check-update.js\nhooks/dist/opencode-are-session-end.js\n```\n\n**Version File Location:**\n```\n${basePath}/ARE-VERSION\n```\n### integration/\n<!-- Generated by agents-reverse-engineer -->\n\n# src/integration/\n\n**Platform-specific AI assistant integration layer performing environment detection, template generation, and file installation for Claude Code, OpenCode, Gemini CLI, and Aider via filesystem marker scanning and bundled hook deployment.**\n\n## Contents\n\n### Environment Detection\n\n**[detect.ts](./detect.ts)** — Exports `detectEnvironments()` and `hasEnvironment()` for AI runtime discovery via filesystem markers. `detectEnvironments()` scans `projectRoot` for Claude (`.claude/` or `CLAUDE.md`), OpenCode (`.opencode/`), Aider (`.aider.conf.yml` or `.aider/`) using synchronous `existsSync()` checks. Returns `DetectedEnvironment[]` with `type`, `configDir`, `detected` fields. `hasEnvironment()` provides predicate wrapper filtering by `EnvironmentType`.\n\n**[types.ts](./types.ts)** — Defines `EnvironmentType` literal union `'claude' | 'opencode' | 'aider' | 'gemini'`, `DetectedEnvironment` with `type`/`configDir`/`detected` fields, `IntegrationTemplate` with `filename`/`path`/`content` fields, `IntegrationResult` with `environment`/`filesCreated`/`filesSkipped` arrays.\n\n### Template Generation\n\n**[templates.ts](./templates.ts)** — Exports `getClaudeTemplates()`, `getOpenCodeTemplates()`, `getGeminiTemplates()` producing `IntegrationTemplate[]` arrays with platform-specific command files. Defines `COMMANDS` object with six templates (`generate`, `update`, `init`, `discover`, `clean`, `specify`, `help`) containing placeholder substitution for `COMMAND_PREFIX` (`/are-`), `VERSION_FILE_PATH` (`.claude/ARE-VERSION` | `.opencode/ARE-VERSION` | `.gemini/ARE-VERSION`), `$ARGUMENTS`. `PLATFORM_CONFIGS` maps `Platform` to `PlatformConfig` specifying `pathPrefix` (`.claude/skills/` | `.opencode/commands/` | `.gemini/commands/`), `filenameSeparator`, `usesName` (Claude-only YAML frontmatter), `extraFrontmatter` (OpenCode `agent: build`). `buildTemplate()` constructs paths, `buildFrontmatter()` injects YAML, `buildGeminiToml()` generates TOML format. Command content embeds background execution pattern with `run_in_background: true`, `TaskOutput` polling, `.agents-reverse-engineer/progress.log` monitoring via `Read` tool with `offset` parameter. `discover`/`clean` templates include strict rule blocks forbidding flag injection beyond `$ARGUMENTS`.\n\n### File Installation\n\n**[generate.ts](./generate.ts)** — Exports `generateIntegrationFiles(projectRoot, options?)` orchestrating setup via `detectEnvironments()` → `getTemplatesForEnvironment()` → `writeFileSync()` chain. Maps `EnvironmentType` to config directories (`claude` → `.claude`, `opencode` → `.opencode`, `aider` → `.aider`, `gemini` → `.gemini`). Creates parent directories via `ensureDir()` using `mkdirSync({ recursive: true })`. Skips existing files unless `options.force` overrides, populates `IntegrationResult.filesSkipped[]`. For `claude` environment specifically, copies bundled hook `are-session-end.js` from `hooks/dist/` to `.claude/hooks/are-session-end.js` via `readBundledHook()` → `writeFileSync()`. `getBundledHookPath()` resolves hook paths from `dist/integration/` up two levels to project root then into `hooks/dist/{hookName}`. Throws `Error` if bundled hook missing. Returns `IntegrationResult[]` with `filesCreated`/`filesSkipped` per environment. Respects `options.dryRun` for preview mode. Defines `GenerateOptions` interface with `dryRun?`, `force?`, `environment?` fields.\n\n## Integration Points\n\nConsumed by `src/installer/operations.ts` during npx-based installation workflow. `detectEnvironments()` validates target runtime presence before hook registration. Template generators provide command file content for `src/installer/operations.ts` writes. Generated files reference `VERSION_FILE_PATH` for session hooks' version checking logic.\n\n## File Path Patterns\n\n**Claude Code:**\n- Commands: `.claude/skills/are-{command}/SKILL.md`\n- Hooks: `.claude/hooks/are-session-end.js`\n- Version cache: `.claude/ARE-VERSION`\n\n**OpenCode:**\n- Commands: `.opencode/commands/are-{command}.md`\n- Version cache: `.opencode/ARE-VERSION`\n\n**Gemini CLI:**\n- Commands: `.gemini/commands/are-{command}.toml`\n- Version cache: `.gemini/ARE-VERSION`\n\n**Aider:**\n- Markers: `.aider.conf.yml`, `.aider/`\n- No template generation (detection only)\n\n## Hook Bundling Convention\n\nHooks copied from `hooks/dist/{hookName}` (post-build artifacts from `scripts/build-hooks.js`). Claude `are-session-end.js` hook spawns detached `npx agents-reverse-engineer@latest update --quiet` process on `git status --porcelain` detecting changes. Disable via `ARE_DISABLE_HOOK=1` environment variable or `hook_enabled: false` config flag.\n### orchestration/\n<!-- Generated by agents-reverse-engineer -->\n\n# src/orchestration\n\n**Iterator-based concurrency pool orchestrating three-phase AI-driven documentation generation (file analysis → directory aggregation → root synthesis) with streaming progress, promise-chain serialized writes for GENERATION-PLAN.md updates and NDJSON trace emission, ETA calculation via moving average, and quality validation integration.**\n\n## Contents\n\n### Core Orchestration\n\n**[runner.ts](./runner.ts)** — `CommandRunner` executes three-phase pipeline: concurrent file `.sum` generation via `runPool()` with stale-doc detection, post-order directory `AGENTS.md` traversal grouped by depth, sequential root document synthesis (`CLAUDE.md`/`GEMINI.md`/`OPENCODE.md`), embeds quality validators (`checkCodeVsDoc`/`checkCodeVsCode`/`checkPhantomPaths`) post-phase with throttled directory grouping, builds `RunSummary` with token/cost/duration/inconsistency aggregates.\n\n**[pool.ts](./pool.ts)** — `runPool<T>()` implements shared-iterator worker pattern: all N workers pull from single `tasks.entries()` iterator preventing idle time between batches, emits `worker:start/end` and `task:pickup/done` trace events with `activeTasks` counter, supports `failFast` abort via mutable `aborted` flag checked before each task pickup, returns sparse `TaskResult<T>[]` array indexed by original task position.\n\n**[progress.ts](./progress.ts)** — `ProgressReporter` streams colored console output (`pc.cyan('ANALYZING')`, `pc.green('DONE')`, `pc.red('FAIL')`) with ETA calculated via sliding window (last 10 completion times) for files and directories separately, `ProgressLog` mirrors ANSI-stripped output to `.agents-reverse-engineer/progress.log` via promise-chain serialized writes for `tail -f` monitoring in buffered environments.\n\n**[plan-tracker.ts](./plan-tracker.ts)** — `PlanTracker` serializes concurrent checkbox updates (`- [ ] → - [x]`) for `GENERATION-PLAN.md` via promise-chain pattern (`writeQueue`), maintains in-memory markdown state, requires exact backtick-wrapped path matching (`` `src/cli/init.ts` ``), swallows write errors (non-critical operation).\n\n**[trace.ts](./trace.ts)** — `createTraceWriter()` factory returns `NullTraceWriter` (zero overhead) or `TraceWriter` (appends to `.agents-reverse-engineer/traces/trace-{timestamp}.ndjson`), auto-populates `seq`/`ts`/`pid`/`elapsedMs` base fields, emits 14 event types (`phase:start/end`, `worker:start/end`, `task:pickup/done`, `subprocess:spawn/exit`, `retry`, `discovery:start/end`, `filter:applied`, `plan:created`, `config:loaded`), `cleanupOldTraces()` enforces 500-file retention, uses `DistributiveOmit<TraceEvent, BaseKeys>` for discriminated union stripping.\n\n### Type Definitions\n\n**[types.ts](./types.ts)** — `FileTaskResult` carries per-task metrics (`tokensIn`/`tokensOut`/`cacheReadTokens`/`cacheCreationTokens`/`durationMs`/`model`), `RunSummary` aggregates command-level totals with quality counts (`inconsistenciesCodeVsDoc`/`inconsistenciesCodeVsCode`/`phantomPaths`), `ProgressEvent` discriminates five event types (`start`/`done`/`error`/`dir-done`/`root-done`) with type-specific optional fields, `CommandRunOptions` threads `concurrency`/`failFast`/`debug`/`dryRun`/`tracer`/`progressLog` from CLI through runner to pool and AIService.\n\n**[index.ts](./index.ts)** — Barrel export exposing `runPool`, `ProgressReporter`, `ProgressLog`, `PlanTracker`, `CommandRunner`, `ITraceWriter`, `TraceEvent`, `TraceEventPayload`, `createTraceWriter`, `cleanupOldTraces`, and shared types (`FileTaskResult`, `RunSummary`, `ProgressEvent`, `CommandRunOptions`, `PoolOptions`, `TaskResult`).\n\n## Concurrency Architecture\n\n### Shared-Iterator Pattern\n\n`runPool()` avoids batch-based idling by sharing single `tasks.entries()` iterator across N workers. Each worker pulls next `[index, taskFn]` tuple immediately after completing previous task, ensuring all slots stay busy until iterator exhausts. `Promise.allSettled(workers)` waits for all workers to finish or abort.\n\n### Fail-Fast Abort\n\nMutable `aborted` boolean flag set by any worker encountering error when `failFast: true`. Workers check `if (aborted) break` before pulling next task but do not interrupt running tasks. Results array remains sparse (undefined entries at aborted indices).\n\n### Serialization Guarantees\n\n`PlanTracker` and `TraceWriter` use promise-chain pattern (`writeQueue: Promise<void>`) to serialize concurrent writes. Each `markDone()` or `emit()` call chains onto previous write: `this.writeQueue = this.writeQueue.then(() => fs.writeFile(...))`. Guarantees NDJSON line order matches emission order despite pool concurrency.\n\n## Three-Phase Execution Flow\n\n### Phase 1: File Analysis (Concurrent)\n\n`CommandRunner.executeGenerate()` pre-caches existing `.sum` files at concurrency=20 for stale-doc detection. Maps file tasks to async functions reading source via `fs.readFile()`, computing `contentHash` via `computeContentHashFromString()`, calling `AIService.call()` with `buildFilePrompt()`, stripping preamble, extracting purpose (first non-preamble line truncated to 120 chars), writing `SumFileContent` with YAML frontmatter (`generated_at`, `content_hash`, `purpose`). Executes via `runPool(fileTasks, { concurrency, failFast, tracer, phaseLabel: 'phase-1-files' })`. Post-phase: groups files by directory, runs throttled (concurrency=10) `checkCodeVsDoc()` comparing old vs. new `.sum` against source content, aggregates `filesForCodeVsCode` array and runs `checkCodeVsCode()` for duplicate symbol detection.\n\n### Phase 2: Directory Aggregation (Post-Order Traversal)\n\nGroups `plan.directoryTasks` by `metadata.depth` into `Map<number, ExecutionTask[]>`, sorts depth levels descending (deepest-first). For each depth level: computes `dirConcurrency = Math.min(concurrency, dirsAtDepth.length)`, emits `phase:start`, maps directory tasks to async functions calling `buildDirectoryPrompt()` with `knownDirs` set and project structure context, calls AI service, writes via `writeAgentsMd()` (prepends existing `AGENTS.local.md` content if present), updates reporter and plan tracker. Executes via `runPool()` with depth-specific `phaseLabel`. Post-phase: runs `checkPhantomPaths()` on each generated `AGENTS.md` (three regex patterns: markdown links, backtick-quoted paths, prose-embedded paths), aggregates issues into `phantomReport`.\n\n### Phase 3: Root Synthesis (Sequential)\n\nSequential execution (concurrency=1) over `plan.rootTasks` (`CLAUDE.md`, `GEMINI.md`, `OPENCODE.md`). Emits `task:start`, calls `buildRootPrompt()` with all `AGENTS.md` injected and root `package.json` metadata, calls `AIService.call({ prompt, systemPrompt, maxTurns: 1 })`, strips conversational preamble (two patterns: `\\n---\\n` separator or bold uppercase start), writes via `fs.writeFile()`, emits `task:done`, updates reporter and plan tracker. Flushes `planTracker.flush()` after loop.\n\n## Progress Reporting & ETA\n\n### Moving Average ETA\n\n`ProgressReporter` maintains `completionTimes: number[]` (files) and `dirCompletionTimes: number[]` (directories) sliding windows (max 10 entries). On task completion: pushes `durationMs`, shifts oldest if window exceeds size. `formatETA()` computes moving average `(sum / count) * remaining`, formats as `~Ns` or `~Mm Ss`, returns empty string if fewer than 2 completions.\n\n### Dual Output Streams\n\nConsole output uses `picocolors` for colored ANSI codes (`pc.cyan('ANALYZING')`, `pc.green('DONE')`, `pc.blue('DONE')` for directories/roots, `pc.red('FAIL')`). `ProgressLog` strips ANSI via regex `/\\x1b\\[[0-9;]*m/g` and appends to `.agents-reverse-engineer/progress.log` via promise-chain serialization. File handle opened lazily in truncate mode ('w') on first write, closed via `finalize()`.\n\n### Token Accounting Display\n\nTotal input tokens displayed as `tokensIn + cacheReadTokens + cacheCreationTokens`. Completion messages show format: `[X/Y] DONE path Xs in/out tok model ~ETA`. Summary prints: files processed/failed/skipped, total calls, tokens (input + cache read + cache creation / output), cache statistics (% read tokens, % creation tokens), files read (total and unique), elapsed time, errors, retries.\n\n## Trace Event System\n\n### Event Types & Schema\n\nDiscriminated union `TraceEvent` with 14 types. All events extend `TraceEventBase` with auto-populated fields: `seq` (monotonic counter), `ts` (ISO 8601), `pid` (`process.pid`), `elapsedMs` (high-resolution delta from `process.hrtime.bigint()` start).\n\n**Pool events:** `worker:start` (workerId, phase), `worker:end` (workerId, phase, tasksExecuted), `task:pickup` (workerId, taskIndex, taskLabel, activeTasks), `task:done` (workerId, taskIndex, taskLabel, durationMs, success, error?, activeTasks).\n\n**Subprocess events:** `subprocess:spawn` (childPid, command, taskLabel), `subprocess:exit` (childPid, command, taskLabel, exitCode, signal, durationMs, timedOut).\n\n**Phase events:** `phase:start` (phase, taskCount, concurrency), `phase:end` (phase, durationMs, tasksCompleted, tasksFailed).\n\n**Other events:** `retry` (attempt, taskLabel, errorCode), `discovery:start/end`, `filter:applied`, `plan:created`, `config:loaded`.\n\n### Emission & Serialization\n\nUsers call `tracer.emit(payload)` with `TraceEventPayload = DistributiveOmit<TraceEvent, BaseKeys>` (user-supplied data without auto-populated fields). `TraceWriter.emit()` populates base fields, serializes to JSON line, enqueues write via `writeQueue = writeQueue.then(() => fd.write(line))` promise-chain. Lazy-opens file handle via `fs.open(filePath, 'a')` after ensuring directory exists. `finalize()` awaits `writeQueue`, closes handle.\n\n### Cleanup & Retention\n\n`cleanupOldTraces(projectRoot, keepCount = 500)` reads `.agents-reverse-engineer/traces/`, sorts files by creation time descending via `fs.stat()`, deletes excess files, returns deletion count. Auto-invoked by `AIService.finalize()` in telemetry shutdown.\n\n## Quality Validation Integration\n\n### Code-vs-Doc Consistency\n\nPost-Phase 1: groups processed files by `path.dirname()`, runs throttled (concurrency=10) validation per directory. For each file: reads cached `sourceContent` from map, reads old `.sum` via `readSumFile()`, runs `checkCodeVsDoc(sourceContent, oldSum.summary, filePath)` appending `(stale documentation)` label, reads fresh `.sum`, runs second pass. `checkCodeVsDoc()` extracts exported symbols via regex `/^[ \\t]*export\\s+(?:default\\s+)?(?:function|class|const|let|var|type|interface|enum)\\s+(\\w+)/gm`, verifies all exports appear in summary via substring search, reports `CodeDocInconsistency` with `missingFromDoc` arrays.\n\n### Code-vs-Code Duplicate Detection\n\nAggregates exports across per-directory file groups into `Map<symbol, string[]>`. Runs `checkCodeVsCode(filesForCodeVsCode)` detecting symbols appearing in multiple files, reports `CodeCodeInconsistency` with `pattern: 'duplicate-export'`.\n\n### Phantom Path Resolution\n\nPost-Phase 2: for each generated `AGENTS.md`, runs `checkPhantomPaths(agentsMdPath, content, projectRoot)` extracting path-like strings via three regex patterns (markdown links, backtick-quoted paths, prose-embedded paths), resolves against AGENTS.md directory and project root with `.ts`/`.js` fallback, reports `PhantomPathInconsistency` for unresolved references.\n\n### Report Formatting\n\nBuilds `InconsistencyReport` via `buildInconsistencyReport(issues, metadata)` with `metadata` (timestamp, projectRoot, filesChecked, durationMs), `issues[]` (discriminated union), `summary` (counts by type/severity). Prints via `formatReportForCli(report)` showing counts and groupings. Embeds in `RunSummary.inconsistencyReport` field.\n\n## Incremental Update Support\n\n`CommandRunner.executeUpdate()` implements Phase 1 only re-analysis for changed files. Attempts to load `GENERATION-PLAN.md` from `.agents-reverse-engineer/` for `projectPlan` context (falls through on error). Maps `filesToAnalyze` to async tasks reading source, reading existing `.sum`, building prompt via `buildFilePrompt({ filePath, content, projectPlan, existingSum })`, calling AI service, writing `.sum` with hash. Runs identical post-analysis quality checks (code-vs-doc, code-vs-code) without stale-doc comparison. Builds `RunSummary` with `updateInconsistenciesCodeVsDoc`/`updateInconsistenciesCodeVsCode` from report summary.\n\n## Behavioral Contracts\n\n### Preamble Stripping Patterns\n\n`stripPreamble()` implements two regex-based detection patterns:\n1. YAML separator: `/\\n---\\n/` within first 500 chars, returns content after separator\n2. Bold uppercase start: `/^[\\s\\S]{0,500}?(\\*\\*[A-Z])/` matches conversational preamble preceding bold section header, strips if <300 chars and no `##` markdown headers present\n\n### Purpose Extraction Filter\n\n`extractPurpose()` skips lines matching `PREAMBLE_PREFIXES` array via `line.toLowerCase().startsWith()`:\n```javascript\n['now i', 'perfect', 'based on', 'let me', 'here is', \"i'll\", 'i will', 'great', 'okay', 'sure', 'certainly', 'alright']\n```\n\nReturns first non-preamble/non-header/non-separator line with bold markdown stripped (`/^\\*\\*(.+)\\*\\*$/`) and 120-char truncation.\n\n### Checkbox Markdown Format\n\n`PlanTracker.markDone()` requires exact backtick-wrapped path format:\n```markdown\n- [ ] `src/cli/init.ts`     → file task\n- [ ] `src/cli/AGENTS.md`   → directory task (caller appends /AGENTS.md)\n- [ ] `CLAUDE.md`           → root task\n```\n\nString replacement via `` `- [ ] \\`${itemPath}\\`` → `- [x] \\`${itemPath}\\`` ``. Returns early if no match (no checkbox update queued).\n\n### Trace Filename Format\n\n`trace-{timestamp}.ndjson` where timestamp converts ISO 8601 to filesystem-safe format via `new Date().toISOString().replace(/[:.]/g, '-')` (colons/periods → hyphens).\n\n### Export Symbol Extraction Regex\n\nCode-vs-doc validator uses pattern: `/^[ \\t]*export\\s+(?:default\\s+)?(?:function|class|const|let|var|type|interface|enum)\\s+(\\w+)/gm` matching top-level exports with optional `default` keyword, capturing symbol name via `(\\w+)` group.\n\n### Phantom Path Extraction Patterns\n\nThree regex patterns in `checkPhantomPaths()`:\n1. Markdown links: `/\\[(?:[^\\]]*)\\]\\((\\.[^)]+)\\)/g` captures relative paths in link targets\n2. Backtick-quoted paths: `` /`((?:src\\/|\\.\\.?\\/)[^`]+\\.[a-z]{1,4})`/g `` captures backtick-wrapped paths starting with `src/` or `./` or `../` with file extensions\n3. Prose-embedded paths: `/(?:from|in|by|via|see)\\s+`?(src\\/[\\w\\-./]+)`?/gi` captures paths following prepositions in prose text\n### output/\n<!-- Generated by agents-reverse-engineer -->\n\n# src/output/\n\nTerminal output formatting layer providing dependency-injectable logger abstraction with conditional color support via picocolors.\n\n## Contents\n\n### [logger.ts](./logger.ts)\nExports `Logger` interface (six methods: `info()`, `file()`, `excluded()`, `summary()`, `warn()`, `error()`), `createLogger(options)` factory with picocolors-based ANSI formatting, and `createSilentLogger()` no-op factory for test/programmatic contexts.\n\n## Architecture\n\n**Dependency Injection Pattern**: Consumers receive `Logger` interface rather than direct `console` access, enabling output capture in tests and silence in programmatic workflows.\n\n**Conditional Color Support**: `createLogger(options)` binds `ColorFunctions` to picocolors `pc` when `options.colors` is true, otherwise to `noColor` identity functions. Single conditional at factory construction avoids per-call branching.\n\n**Output Routing**: `info()/file()/excluded()/summary()` write to `console.log()`, `warn()` to `console.warn()`, `error()` to `console.error()` for stream separation in shell redirects.\n\n## Behavioral Contracts\n\n**File Discovery Formatting**:\n- Included file: `` `  +` `` (green) + `path`\n- Excluded file: `` `  -` `` (dim) + `path` + ` (${reason}: ${filter})` (dim)\n- Summary: `Discovered ${included} files (${excluded} excluded)` (bold count, dim excluded)\n\n**Warning/Error Prefixes**:\n- `warn()`: `\"Warning: \"` (yellow) + message → stderr\n- `error()`: `\"Error: \"` (red) + message → stderr\n\n## Usage Context\n\nThreaded through `CommandRunOptions` → `DiscoveryOptions` → `run()` in `src/discovery/run.ts`. CLI commands instantiate via `createLogger({ colors: config.output.colors })`, test suites use `createSilentLogger()`.\n### quality/\n<!-- Generated by agents-reverse-engineer -->\n\n# src/quality\n\nCode-documentation consistency validation subsystem executing three detection strategies: regex-based export extraction with substring matching (`code-vs-doc`), cross-file duplicate symbol aggregation (`code-vs-code`), and regex-based path reference resolution with filesystem verification (`phantom-paths`).\n\n## Contents\n\n**[index.ts](./index.ts)** — Barrel module re-exporting `checkCodeVsDoc()`, `checkCodeVsCode()`, `checkPhantomPaths()` validators, `buildInconsistencyReport()`, `formatReportForCli()` reporters, disabled `validateFindability()` density validator, and discriminated union types (`CodeDocInconsistency`, `CodeCodeInconsistency`, `PhantomPathInconsistency`, `Inconsistency`, `InconsistencyReport`).\n\n**[types.ts](./types.ts)** — Defines `InconsistencySeverity` (`'info' | 'warning' | 'error'`), `CodeDocInconsistency` (exported symbols missing from `.sum` with `missingFromDoc[]`), `CodeCodeInconsistency` (duplicate exports across files with `pattern: 'duplicate-export'`), `PhantomPathInconsistency` (unresolved paths in `AGENTS.md` with `referencedPath`, `resolvedTo`, `context`), `Inconsistency` discriminated union, `InconsistencyReport` container with metadata (`timestamp`, `projectRoot`, `filesChecked`, `durationMs`) and summary counts.\n\n## Subdirectories\n\n**[inconsistency/](./inconsistency/)** — code-vs-doc validator extracting exports via `/^[ \\t]*export\\s+(?:default\\s+)?(?:function|class|const|let|var|type|interface|enum)\\s+(\\w+)/gm` with substring verification against `.sum` content; code-vs-code detector aggregating exports into `Map<symbol, paths[]>` to identify duplicates; reporter building aggregated `InconsistencyReport` with formatted CLI output.\n\n**[phantom-paths/](./phantom-paths/)** — Path reference validator extracting strings via three regex patterns (markdown links, backtick paths, prose references), resolving against `AGENTS.md` directory and project root with `.ts`/`.js` fallback, returning `PhantomPathInconsistency[]` for unresolved references.\n\n**[density/](./density/)** — Disabled `validateFindability()` stub previously verifying exported symbols from `.sum` files appeared in parent `AGENTS.md`; implementation gutted after `SumFileContent.metadata.publicInterface` removal.\n\n## Validation Workflow\n\n**Code-vs-Doc Consistency:**  \n`extractExports(filePath)` scans source via regex capturing export identifiers → `checkCodeVsDoc(filePath, sumContent)` performs substring search against `.sum` summary text → returns `CodeDocInconsistency` with `missingFromDoc[]` when drift detected or `null` when consistent.\n\n**Code-vs-Code Duplicate Detection:**  \n`checkCodeVsCode(files[])` aggregates `extractExports()` results into `Map<symbol, string[]>` → filters entries with `paths.length > 1` → returns `CodeCodeInconsistency[]` with `pattern: 'duplicate-export'` and `severity: 'warning'`.\n\n**Phantom Path Resolution:**  \n`checkPhantomPaths(agentsMdPath, content, projectRoot)` applies three regex patterns to extract path references → attempts resolution via `existsSync()` against `AGENTS.md` directory and project root with extension substitution → returns `PhantomPathInconsistency[]` for unresolved paths with 120-char context excerpt.\n\n**Report Aggregation:**  \n`buildInconsistencyReport(issues[], metadata)` merges inconsistency arrays → computes summary counts by `type` (`code-vs-doc`, `code-vs-code`, `phantom-path`) and `severity` (`error`, `warning`, `info`) → returns `InconsistencyReport` with timestamp metadata. `formatReportForCli(report)` transforms into plain-text output with severity tags (`[ERROR]`, `[WARN]`, `[INFO]`) and type-specific field rendering for stderr/`progress.log`.\n\n## Behavioral Contracts\n\n**Export Extraction Regex (inconsistency/):**  \n`/^[ \\t]*export\\s+(?:default\\s+)?(?:function|class|const|let|var|type|interface|enum)\\s+(\\w+)/gm`\n\n**Path Extraction Patterns (phantom-paths/validator.ts):**  \n```javascript\n/\\[(?:[^\\]]*)\\]\\((\\.[^)]+)\\)/g              // Markdown: [text](./path)\n/`((?:src\\/|\\.\\.?\\/)[^`]+\\.[a-z]{1,4})`/g  // Backtick: `src/foo.ts`\n/(?:from|in|by|via|see)\\s+`?(src\\/[\\w\\-./]+)`?/gi // Prose: \"from src/foo/\"\n```\n\n**Severity Tags (inconsistency/reporter.ts):**  \n`severity: 'error'` → `[ERROR]`, `severity: 'warning'` → `[WARN]`, `severity: 'info'` → `[INFO]`\n\n**Type Discriminator Field:**  \n`type: 'code-vs-doc' | 'code-vs-code' | 'phantom-path'` enables exhaustive switch narrowing.\n\n## Known Limitations\n\n**Regex-Based Export Extraction:**  \nMisses destructured (`export { foo }`), namespace (`export *`), dynamic exports, multiline declarations. No AST analysis—purely heuristic pattern matching.\n\n**Substring Verification:**  \nFalse negatives when export names appear in comments/prose rather than documented API surface. No structured field verification after `publicInterface` schema removal.\n\n**Intentional Duplication:**  \nCode-vs-code cannot distinguish factory patterns, parallel implementations, intentional re-exports. Caller must scope input to per-directory groups to minimize false positives.\n\n**Extension Fallback Logic:**  \nPath resolution attempts `.js` → `.ts` substitution but doesn't handle `.jsx`/`.tsx`, `.cjs`/`.mjs`, or index file resolution (`./foo` → `./foo/index.ts`).\n### specify/\n<!-- Generated by agents-reverse-engineer -->\n\n# specify/\n\nSpecification synthesis from AGENTS.md corpus via AI-driven prompt construction (`buildSpecPrompt`) and filesystem output (`writeSpec`) with multi-file splitting and overwrite protection.\n\n## Contents\n\n### Files\n\n**[index.ts](./index.ts)** — Barrel module re-exporting `buildSpecPrompt`, `writeSpec`, `SpecPrompt`, `WriteSpecOptions`, `SpecExistsError` for `/are-specify` command integration.\n\n**[prompts.ts](./prompts.ts)** — `buildSpecPrompt(docs: AgentsDocs): SpecPrompt` constructs system/user prompts enforcing nine-section structure (Overview, Architecture, API Surface, Data Structures, Configuration, Dependencies, Behavioral Contracts, Test Contracts, Build Plan) with verbatim regex/format/constant preservation. `SPEC_SYSTEM_PROMPT` prohibits folder-mirroring, mandates concern-based organization, targets AI agents with actionable instructions.\n\n**[writer.ts](./writer.ts)** — `writeSpec(content: string, options: WriteSpecOptions): Promise<string[]>` writes single-file or multi-file specifications via heading-based splitting. `splitByHeadings(content)` partitions on `/^# /m` regex, `slugify()` sanitizes heading text to filenames (e.g., `\"Project Overview\"` → `\"project-overview.md\"`). Throws `SpecExistsError` with `paths: string[]` when targets exist and `force: false`.\n\n## Data Flow\n\n1. **Orchestration** (`src/cli/specify.ts`) invokes `collectAgentsDocs()` to gather all `AGENTS.md` files recursively\n2. **Prompt Construction** (`prompts.ts`) injects corpus into `SPEC_SYSTEM_PROMPT` template with section delimiters `### ${relativePath}`, appends nine-section requirements\n3. **AI Synthesis** (`AIService.call()` from `src/ai/service.ts`) processes prompt pair, returns specification markdown\n4. **Output** (`writer.ts`) writes single `specs/SPEC.md` or splits into `specs/<slugified-heading>.md` per top-level heading\n5. **Error Handling** catches `SpecExistsError` and displays `--force` hint\n\n## Behavioral Contracts\n\n### Prompt Constraints (SPEC_SYSTEM_PROMPT)\n\n**Organization rules:**\n- Regex `/^# (Project Overview|Architecture|Public API Surface|Data Structures & State|Configuration|Dependencies|Behavioral Contracts|Test Contracts|Build Plan)$/m` defines mandatory section headings\n- Prohibition pattern: \"Do NOT prescribe exact filenames or file paths\" / \"Do NOT mirror the project's folder structure\" / \"Do NOT use directory names as section headings\"\n- Behavioral Contracts split: \"Runtime Behavior (error handling, concurrency, lifecycle)\" + \"Implementation Contracts (verbatim regex patterns, format specs, magic constants)\"\n\n**Output format enforcement:**\n- Terminal instruction: `\"OUTPUT: Raw markdown. No preamble. No meta-commentary. No 'Here is...' or 'I've generated...' prefix.\"`\n- User prompt: `\"Output ONLY the markdown content. No preamble.\"`\n\n### File System Patterns (writer.ts)\n\n**Heading splitting regex:** `/^(?=# )/m` (positive lookahead for top-level headings)  \n**Heading extraction:** `/^# (.+)/` (capture heading text)  \n**Slugification chain:**\n1. `.toLowerCase()`\n2. `/\\s+/g` → `'-'` (whitespace to hyphens)\n3. `/[^a-z0-9-]/g` → `''` (strip non-alphanumeric except hyphens)\n4. `/-+/g` → `'-'` (collapse consecutive hyphens)\n5. `/^-|-$/g` → `''` (trim leading/trailing hyphens)\n\n**Preamble filename:** `00-preamble.md` (content before first `# ` heading)\n\n## Integration Points\n\n**Consumed by:** `src/cli/specify.ts` (command orchestrator)  \n**Consumes:**\n- `collectAgentsDocs()` from `../generation/collector.js` → `AgentsDocs` (array of `{relativePath, content}`)\n- `AIService.call()` from `../ai/service.js` (prompt pair execution)\n- `mkdir()`, `writeFile()`, `access()` from `node:fs/promises` (file operations)\n- `path.dirname()`, `path.join()` from `node:path` (path resolution)\n\n**Error propagation:** `SpecExistsError` thrown when `force: false` and target paths exist, caught by CLI layer for `--force` hint display\n### types/\n<!-- Generated by agents-reverse-engineer -->\n\n# src/types\n\nShared TypeScript interface definitions for file discovery results, exclusion tracking, and statistics aggregation across ARE modules.\n\n## Contents\n\n### [index.ts](./index.ts)\nExports `ExcludedFile` (path + reason), `DiscoveryResult` (files + excluded arrays), `DiscoveryStats` (totalFiles, includedFiles, excludedFiles, exclusionReasons histogram).\n\n## Usage Pattern\n\n`src/discovery/walker.ts` returns `DiscoveryResult` from traversal functions. `src/cli/discover.ts` consumes `DiscoveryResult` to format console output and write `GENERATION-PLAN.md`. `src/orchestration/runner.ts` logs `DiscoveryStats` to telemetry during discovery phase.\n\n## Data Model\n\n### ExcludedFile\n- `path: string` — Absolute/relative file path\n- `reason: string` — Exclusion rationale (\"gitignore pattern\", \"binary file\", \"vendor directory\")\n\n### DiscoveryResult\n- `files: string[]` — Selected for analysis\n- `excluded: ExcludedFile[]` — Rejected with reasons\n\n### DiscoveryStats\n- `totalFiles: number`, `includedFiles: number`, `excludedFiles: number` — Counts\n- `exclusionReasons: Record<string, number>` — Reason → occurrence count map\n### update/\n<!-- Generated by agents-reverse-engineer -->\n\n# src/update/\n\nIncremental documentation synchronization via frontmatter SHA-256 hash comparison, orphaned artifact cleanup, and targeted directory regeneration.\n\n## Contents\n\n### Core Orchestration\n\n**[index.ts](./index.ts)** — Barrel export aggregating `UpdateOrchestrator` class, `createUpdateOrchestrator()` factory, cleanup functions (`cleanupOrphans`, `cleanupEmptyDirectoryDocs`, `getAffectedDirectories`), and type interfaces (`UpdateOptions`, `UpdateResult`, `UpdateProgress`, `CleanupResult`).\n\n**[orchestrator.ts](./orchestrator.ts)** — `UpdateOrchestrator` class executing hash-based change detection via `preparePlan()`: discovers files through `runDiscovery()`, reads `.sum` YAML frontmatter via `readSumFile()` to extract `contentHash`, computes current SHA-256 via `computeContentHash()`, produces `UpdatePlan` with `filesToAnalyze[]` (hash mismatch/missing), `filesToSkip[]` (hash match), `cleanup: CleanupResult` (orphaned `.sum` files), `affectedDirs[]` (depth-sorted parent directories). Emits trace events (`phase:start`, `plan:created`, `phase:end`) via `ITraceWriter`. No-op methods (`close()`, `recordFileAnalyzed()`, `removeFileState()`, `recordRun()`, `getLastRun()`) preserved for API compatibility after SQLite state manager removal.\n\n**[orphan-cleaner.ts](./orphan-cleaner.ts)** — `cleanupOrphans()` deletes `.sum` files for `FileChange` entries with `status === 'deleted'` (uses `change.path`) or `status === 'renamed'` (uses `change.oldPath`), returns `CleanupResult` with `deletedSumFiles[]` and `deletedAgentsMd[]`. `cleanupEmptyDirectoryDocs()` removes `AGENTS.md` from directories containing only hidden files (`.` prefix), `.sum` files, or `GENERATED_FILES` set members. `getAffectedDirectories()` walks parent directories via `path.dirname()` for non-deleted `FileChange` entries, excludes `status === 'deleted'`, always includes `'.'` root.\n\n**[types.ts](./types.ts)** — `UpdateOptions` interface with `includeUncommitted?: boolean` (merges `git status --porcelain` into change set) and `dryRun?: boolean` (preview-only). `UpdateResult` interface with `analyzedFiles[]`, `skippedFiles[]`, `cleanup`, `regeneratedDirs[]`, `baseCommit`, `currentCommit`, `dryRun` fields. `UpdateProgress` callback interface with `onFileStart()`, `onFileDone()`, `onCleanup()`, `onDirRegenerate()` hooks. `CleanupResult` interface with `deletedSumFiles[]`, `deletedAgentsMd[]` arrays. Re-exports `FileChange` from `../change-detection/types.js`.\n\n## Update Algorithm\n\n**Hash-Based Change Detection** (orchestrator.ts `preparePlan()` method):\n1. `checkPrerequisites()` verifies git repository via `isGitRepo()`\n2. `getCurrentCommit()` retrieves HEAD commit SHA\n3. `discoverFiles()` wraps `runDiscovery()` from `../discovery/run.js`, converts absolute paths to relative via `path.relative()`\n4. For each file: `getSumPath()` locates `.sum` file, `readSumFile()` extracts `contentHash` from YAML frontmatter, `computeContentHash()` computes current SHA-256\n5. Hash mismatch or missing `.sum` → `FileChange` with `status: 'added'|'modified'` added to `filesToAnalyze[]`\n6. Hash match → path added to `filesToSkip[]`\n7. `cleanupOrphans()` deletes `.sum` files for deleted/renamed sources\n8. `getAffectedDirectories()` computes unique parent directory set from `filesToAnalyze[]`\n9. `affectedDirs[]` sorted by depth descending via `split(path.sep).length` (deepest-first for bottom-up regeneration)\n\n**Orphan Cleanup** (orphan-cleaner.ts):\n- Deleted files: Appends `.sum` to `change.path`, calls `deleteIfExists()`\n- Renamed files: Appends `.sum` to `change.oldPath`, calls `deleteIfExists()`\n- Empty directories: `cleanupEmptyDirectoryDocs()` removes `AGENTS.md` if directory contains no source files (excludes hidden files, `.sum` files, `AGENTS.md`, `CLAUDE.md`)\n- `deleteIfExists()` internal helper: `stat()` existence check, `unlink()` delete (skipped if `dryRun === true`), returns boolean\n\n## Integration Points\n\n**Upstream Dependencies:**\n- `../change-detection/index.js` — `isGitRepo()`, `getCurrentCommit()`, `computeContentHash()`, `FileChange` type\n- `../generation/writers/sum.js` — `readSumFile()` (parses YAML frontmatter), `getSumPath()` (resolves `.sum` path)\n- `../discovery/run.js` — `discoverFiles()` wrapping `runDiscovery()` for file scanning\n- `../config/schema.js` — `Config` type with exclude patterns and concurrency settings\n- `../orchestration/trace.js` — `ITraceWriter` interface for NDJSON trace emission\n\n**Downstream Consumers:**\n- `../cli/update.ts` — CLI entry point invoking `createUpdateOrchestrator()` with parsed options\n- `hooks/are-session-end.js` — Session hook spawning `npx agents-reverse-engineer@latest update --quiet` on uncommitted changes\n\n## API Compatibility Pattern\n\norchestrator.ts no-op methods preserve interface contract for consumers expecting SQLite-based state management (removed in frontmatter migration):\n- `close()` — No database resources to clean\n- `recordFileAnalyzed()` — Hash now stored in `.sum` YAML frontmatter\n- `removeFileState()` — Cleanup handled by `cleanupOrphans()`\n- `recordRun()` — Returns `0` (no run history tracking)\n- `getLastRun()` — Returns `undefined` (no historical run data)\n\n## Trace Events\n\norchestrator.ts emits via `ITraceWriter`:\n- `{ type: 'phase:start', phase: 'update-plan-creation' }` — Start of `preparePlan()`\n- `{ type: 'plan:created', planType: 'update', fileCount, taskCount }` — Plan finalized\n- `{ type: 'phase:end', phase: 'update-plan-creation', durationMs, tasksCompleted: 1, tasksFailed: 0 }` — Plan complete\n\n## Debug Logging\n\norchestrator.ts stderr output (when `debug: true`):\n- `'[debug] Creating update plan with change detection...'` — Start signal\n- `'[debug] Git commit: <hash>'` — First 7 characters of commit SHA\n- `'[debug] Discovering files...'` — Before discovery phase\n- `'[debug] Change detection: <changed> changed, <unchanged> unchanged, <orphaned> orphaned'` — After hash comparison\n- `'[debug] Affected directories: <count>'` — Directory regeneration scope",
      "systemPrompt": "You are generating an AGENTS.md file — a directory-level overview for AI coding assistants.\n\nCRITICAL: Output ONLY the raw markdown content. No code fences, no preamble, no explanations, no conversational text. Your entire response IS the AGENTS.md file content.\n\nOUTPUT FORMAT:\n- First line MUST be exactly: <!-- Generated by agents-reverse-engineer -->\n- Use a # heading with the directory name\n- Write a one-paragraph purpose statement for the directory\nusage\nADAPTIVE SECTIONS:\nAnalyze the directory contents and choose the most relevant sections. Do NOT use a fixed template. Instead, select sections that best document this specific directory for an AI that needs to reconstruct or extend the project.\n\nConsider these section types (choose what applies):\n- **Contents**: Group files by purpose/category under ## headings. For each file: markdown link [filename](./filename) and a one-line description.\n- **Subdirectories**: If subdirectories exist, list them with links [dirname/](./dirname/) and brief summaries.\n- **Architecture / Data Flow**: If files form a pipeline, request/response chain, or layered architecture, document it.\n- **Stack**: If this is a package root (has package.json, Cargo.toml, go.mod, etc.), document the technology stack, key scripts, and entry points.\n- **Structure**: If the directory layout follows a convention (feature-sliced, domain-driven, MVC, etc.), document it.\n- **Patterns**: If files share recurring design patterns (factory, strategy, middleware, barrel re-export), name and document them.\n- **Configuration**: If the directory contains config files, schemas, or environment definitions, document the config surface area.\n- **API Surface**: If the directory exports a public API (barrel index, route definitions, SDK), document the interface contract.\n- **File Relationships**: How files collaborate, depend on each other, or share state.\n- **Behavioral Contracts**: If files contain regex patterns, format specifications, magic constants, or template strings that define observable behavior, collect them in a dedicated section. Preserve verbatim patterns from file summaries — do NOT paraphrase regex into prose. This section is MANDATORY when file summaries contain behavioral artifacts.\n\nChoose any relevant sections or create your own based on the directory contents. The goal is to provide a comprehensive overview that captures the essence of the directory's role in the project and how its files work together, with a focus on what an AI coding assistant would need to know to effectively interact with this code.\n\nSCOPE:\n- AGENTS.md is a NAVIGATIONAL INDEX — help an AI find the right file quickly\n- Focus on: what each file does, how files relate, directory-level patterns\n- Do NOT reproduce full architecture sections — those belong in the root CLAUDE.md\n\nPATH ACCURACY (MANDATORY):\n- When referencing files or modules outside this directory, use ONLY paths from the \"Import Map\" section\n- Do NOT invent, rename, or guess module paths — if a path isn't in the Import Map, don't reference it\n- Use the exact directory names from \"Project Directory Structure\" — do NOT rename directories\n  (e.g., if the directory is called \"cli\", write \"src/cli/\", NOT \"src/commands/\")\n- Cross-module references must use the specifier format from actual import statements\n  (e.g., \"../generation/writers/sum.js\", NOT \"../fs/sum-file.js\")\n- If you are unsure about a path, omit the cross-reference rather than guessing\n\nCONSISTENCY (MANDATORY):\n- Do not contradict yourself within the same document\n- If you describe a technique (e.g., \"regex-based\"), do not call it something else later (e.g., \"AST-based\")\n- When stating version numbers, engines, or config fields, use ONLY values present in the file summaries\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this directory\", \"this module\", \"provides\", \"responsible for\", \"is used to\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Per-file descriptions in Contents sections: 1-2 sentences maximum. Reference key symbols but do not reproduce full summaries.\n- Behavioral contracts (regex patterns, format specs, constants) belong in a separate Behavioral Contracts section, not in per-file descriptions.\n- Subdirectory descriptions: 1-2 sentences maximum. Capture the directory's role, not its full contents.\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- Key exported symbols from file summaries MUST appear in the directory overview\n- Preserve exact casing of identifiers\n\nUSER NOTES:\n- If \"User Notes\" are provided in the prompt, they contain user-defined instructions that will be automatically prepended to your output\n- Do NOT repeat or paraphrase user notes in your generated content — they are included separately\n- You may reference information from user notes for context",
      "response": "<!-- Generated by agents-reverse-engineer -->\n\n# src\n\n**TypeScript source tree containing CLI entry points, three-phase AI orchestration pipeline, file discovery filters, configuration management, subprocess-spawning AI service layer, quality validators, and IDE integration installers for agents-reverse-engineer brownfield documentation generator.**\n\n## Contents\n\n**[version.ts](./version.ts)** — `getVersion()` reads `package.json` from compiled module parent directory (dist/ → project root) via `fileURLToPath(import.meta.url)` → `dirname()` → `join(__dirname, '..', 'package.json')`, parses JSON extracting `version` field, returns `'unknown'` on error (file not found, JSON parse failure, missing field). Called by CLI help text, version flag output, session hooks for npm registry comparison.\n\n## Subdirectories\n\n**[ai/](./ai/)** — Backend-agnostic AI service orchestration: subprocess-spawning adapters (ClaudeBackend with Zod JSON parsing, GeminiBackend/OpenCodeBackend stubs), exponential backoff retry with rate limit detection (['rate limit', '429', 'too many requests', 'overloaded']), timeout enforcement (SIGTERM at timeoutMs, SIGKILL after 5s, process group killing `kill(-pid)`), NDJSON telemetry logging to `.agents-reverse-engineer/logs/run-<timestamp>.json` with token/cost tracking, trace emission (subprocess:spawn/exit/retry).\n\n**[change-detection/](./change-detection/)** — Git-based change detection parsing `git diff --name-status -M` with rename detection, merges uncommitted working tree changes via `status.modified[]`/`deleted[]`/`not_added[]`/`staged[]`, SHA-256 content hashing for `.sum` frontmatter verification, returns `ChangeDetectionResult` with `FileChange[]` array (path/status/oldPath/contentHash), status codes map to `ChangeType` ('added'/'modified'/'deleted'/'renamed').\n\n**[cli/](./cli/)** — Command entry points routing parsed `process.argv` to handlers: `initCommand()` writes config YAML, `cleanCommand()` deletes artifacts with `GENERATED_MARKER` filtering, `discoverCommand()` builds `GENERATION-PLAN.md`, `generateCommand()` orchestrates three-phase pipeline with backend resolution, `updateCommand()` executes incremental hash-based regeneration, `specifyCommand()` synthesizes project specifications from AGENTS.md corpus. Main index parses flags via `parseArgs()` with short flag expansion (`-h` → `help`, `-g` → `global`), dual routing for installer mode vs. command dispatch, exit codes (0=success, 1=partial failure, 2=total failure/CLI not found).\n\n**[config/](./config/)** — Configuration management: `loadConfig()` reads `.agents-reverse-engineer/config.yaml` via YAML parsing and `ConfigSchema.parse()` Zod validation (returns defaults on ENOENT, throws `ConfigError` on ZodError), `writeDefaultConfig()` generates commented template with inline defaults, `getDefaultConcurrency()` computes adaptive worker pool size via `clamp(cores * 5, 2, min(memCap, 20))` where `memCap = floor((totalMemGB * 0.5) / 0.512)`, default exclusions (26 patterns, 18 vendor dirs, 26 binary extensions).\n\n**[discovery/](./discovery/)** — File discovery pipeline: `discoverFiles()` creates four-filter chain (GitignoreFilter → VendorFilter → BinaryFilter → CustomPatternFilter), `walkDirectory()` via `fast-glob` with `{absolute: true, onlyFiles: true, ignore: ['**/.git/**']}`, `applyFilters()` with 30-worker concurrency and short-circuit evaluation, returns `FilterResult` with `included[]`/`excluded[]` attribution. Filters normalize paths via `path.relative(normalizedRoot, absolutePath)` for pattern matching.\n\n**[generation/](./generation/)** — Three-phase orchestration: `GenerationOrchestrator.createPlan()` executes file discovery → `analyzeComplexity()` → `buildFilePrompt()` per file with `projectPlan` injection → groups by `path.dirname()` for directory tasks → `buildExecutionPlan()` post-order traversal (sorted by depth descending) → memory management clears `PreparedFile.content` fields after embedding. `buildDirectoryPrompt()` reads `.sum` frontmatter, child `AGENTS.md`, import maps via `extractDirectoryImports()`, detects 9 manifest types. `buildRootPrompt()` calls `collectAgentsDocs()`, parses root `package.json`, enforces synthesis-only constraints. Writers serialize YAML frontmatter (adaptive array formatting), preserve user content via `AGENTS.local.md` rename pattern.\n\n**[imports/](./imports/)** — Static import analysis: `extractImports()` applies `IMPORT_REGEX` (`/^import\\s+(type\\s+)?(?:\\{([^}]*)\\}|(\\*\\s+as\\s+\\w+)|(\\w+))\\s+from\\s+['\"]([^'\"]+)['\"]/gm`) extracting ES module syntax into `ImportEntry[]`, `extractDirectoryImports()` scans first 100 lines per file for optimization, partitions relative imports into `internal` (`./`) and `external` (`../`), filters bare specifiers (`'react'`, `node:*`), `formatImportMap()` serializes into prompt text blocks with `(type)` suffix.\n\n**[installer/](./installer/)** — npx installation orchestrator: `runInstall()` parses CLI args (`-g`/`-l`/`--runtime`/`--force`), interactive prompts via `selectRuntime()`/`selectLocation()` with arrow-key selection in TTY mode, `installFilesForRuntime()` writes command templates from `src/integration/templates.ts` and copies bundled hooks from `hooks/dist/`, registers hooks in `settings.json` via `registerClaudeHooks()`/`registerGeminiHooks()`, writes `ARE-VERSION` file, adds Bash permission patterns to Claude Code allow list. Uninstaller removes files, hook registrations, permissions, cleans empty skill directories via `cleanupAreSkillDirs()`/`cleanupLegacyGeminiFiles()`/`cleanupEmptyDirs()`.\n\n**[integration/](./integration/)** — Platform-specific AI assistant integration: `detectEnvironments()` scans for Claude (`.claude/` or `CLAUDE.md`), OpenCode (`.opencode/`), Aider (`.aider.conf.yml` or `.aider/`) via `existsSync()`, `getTemplatesForRuntime()` generates command files with placeholder substitution for `COMMAND_PREFIX` (`/are-`), `VERSION_FILE_PATH`, `$ARGUMENTS`, embeds background execution pattern with `run_in_background: true`, `TaskOutput` polling, `.agents-reverse-engineer/progress.log` monitoring. `generateIntegrationFiles()` writes templates via `ensureDir()` → `writeFileSync()` chain, copies bundled hooks for Claude.\n\n**[orchestration/](./orchestration/)** — Iterator-based worker pool orchestrating three-phase execution: `runPool()` shares single `tasks.entries()` iterator across N workers preventing idle time, emits `worker:start/end` and `task:pickup/done` trace events, supports `failFast` abort via mutable flag. `CommandRunner.executeGenerate()` runs concurrent file `.sum` generation with stale-doc detection, post-order directory `AGENTS.md` traversal grouped by depth, sequential root document synthesis (`CLAUDE.md`/`GEMINI.md`/`OPENCODE.md`), embeds quality validators post-phase (code-vs-doc, code-vs-code, phantom-paths). `ProgressReporter` streams colored console output with ETA via moving average (last 10 completion times), `ProgressLog` mirrors to `.agents-reverse-engineer/progress.log` via promise-chain serialized writes. `PlanTracker` serializes concurrent checkbox updates (`- [ ] → - [x]`) for `GENERATION-PLAN.md` via promise-chain pattern. `TraceWriter` appends to `.agents-reverse-engineer/traces/trace-<timestamp>.ndjson` with auto-populated `seq`/`ts`/`pid`/`elapsedMs` base fields, `cleanupOldTraces()` enforces 500-file retention.\n\n**[output/](./output/)** — Terminal output formatting: `createLogger()` factory with picocolors-based ANSI formatting when `options.colors` is true, `createSilentLogger()` no-op for tests, `Logger` interface (six methods: `info()`, `file()`, `excluded()`, `summary()`, `warn()`, `error()`), output routing (log/warn/error → stdout/stderr).\n\n**[quality/](./quality/)** — Code-documentation consistency validation: `checkCodeVsDoc()` extracts exports via `/^[ \\t]*export\\s+(?:default\\s+)?(?:function|class|const|let|var|type|interface|enum)\\s+(\\w+)/gm` with substring verification against `.sum` content, `checkCodeVsCode()` aggregates exports into `Map<symbol, paths[]>` for duplicate detection, `checkPhantomPaths()` extracts path references via three regex patterns (markdown links, backtick paths, prose references) resolving against `AGENTS.md` directory and project root with `.ts`/`.js` fallback. `buildInconsistencyReport()` merges issues with summary counts by type (`code-vs-doc`/`code-vs-code`/`phantom-path`) and severity (`error`/`warning`/`info`).\n\n**[specify/](./specify/)** — Specification synthesis: `buildSpecPrompt()` constructs system/user prompts enforcing nine-section structure (Overview, Architecture, API Surface, Data Structures, Configuration, Dependencies, Behavioral Contracts, Test Contracts, Build Plan) with verbatim regex/format/constant preservation, `writeSpec()` writes single-file or multi-file specifications via heading-based splitting (`splitByHeadings()` partitions on `/^# /m`, `slugify()` sanitizes heading text), throws `SpecExistsError` when targets exist and `force: false`.\n\n**[types/](./types/)** — Shared interfaces for discovery results (`ExcludedFile`, `DiscoveryResult`, `DiscoveryStats` with `exclusionReasons` histogram), consumed by `src/discovery/walker.ts`, `src/cli/discover.ts`, `src/orchestration/runner.ts`.\n\n**[update/](./update/)** — Incremental documentation synchronization: `UpdateOrchestrator.preparePlan()` executes hash-based change detection via `readSumFile()` YAML frontmatter extraction, `computeContentHash()` SHA-256 comparison, produces `UpdatePlan` with `filesToAnalyze[]` (hash mismatch/missing), `filesToSkip[]` (hash match), `cleanup: CleanupResult` (orphaned `.sum` files), `affectedDirs[]` (depth-sorted parent directories). `cleanupOrphans()` deletes `.sum` files for `FileChange` entries with `status === 'deleted'` (uses `change.path`) or `status === 'renamed'` (uses `change.oldPath`). `cleanupEmptyDirectoryDocs()` removes `AGENTS.md` from directories containing only hidden files, `.sum` files, or generated artifacts. `getAffectedDirectories()` walks parent directories via `path.dirname()` excluding deletes.\n\n## Three-Phase Pipeline Architecture\n\n**Phase 1 (File Analysis)** — Concurrent pool execution (default 2 workers for WSL, 5 elsewhere): `GenerationOrchestrator.createFileTasks()` invokes `buildFilePrompt()` per file embedding content + project structure, workers execute via `runPool()` calling `AIService.call()` → `runSubprocess()` → `execFile()` spawning AI CLI subprocesses with resource limits (`NODE_OPTIONS='--max-old-space-size=512'`, `UV_THREADPOOL_SIZE='4'`, `CLAUDE_CODE_DISABLE_BACKGROUND_TASKS='1'`, `--disallowedTools Task`). Results written via `writeSumFile()` to `.sum` paths with YAML frontmatter (`generated_at`, `content_hash`, `purpose`, `critical_todos`, `related_files`). Post-phase quality validation: throttled (concurrency=10) `checkCodeVsDoc()` comparing old vs. new `.sum` against source content, `checkCodeVsCode()` for duplicate symbol detection.\n\n**Phase 2 (Directory Aggregation)** — Post-order traversal: `GenerationOrchestrator.createDirectoryTasks()` groups files via `Map<dirname, PreparedFile[]>`, executor sorts tasks by `getDirectoryDepth()` descending (deepest first), waits for child `.sum` files via `isDirectoryComplete()`, constructs prompts via `buildDirectoryPrompt()` consuming `.sum` frontmatter + child `AGENTS.md` + import maps from `extractDirectoryImports()`. Writes via `writeAgentsMd()` preserving user content from `AGENTS.local.md`. Post-phase: runs `checkPhantomPaths()` on each generated `AGENTS.md` (three regex patterns for path extraction), aggregates issues into `phantomReport`.\n\n**Phase 3 (Root Synthesis)** — Sequential execution (concurrency=1) generates `CLAUDE.md`, `GEMINI.md`, `OPENCODE.md` via `buildRootPrompt()` aggregating all `AGENTS.md` via `collectAgentsDocs()`, parsing root `package.json`, enforcing synthesis-only constraints (no invention). Strips conversational preamble via pattern matching before writing output.\n\n## Subprocess Resource Management\n\nAI CLI spawning mitigations for Claude #5771 (200 NodeJS instances): `NODE_OPTIONS='--max-old-space-size=512'` limits heap to 512MB per subprocess, `UV_THREADPOOL_SIZE='4'` constrains libuv thread pool to 4 threads, `CLAUDE_CODE_DISABLE_BACKGROUND_TASKS='1'` prevents background task spawning, `--disallowedTools Task` blocks subagents. Process group killing: `kill(-pid)` terminates entire subprocess tree. Timeout enforcement: SIGTERM sent at `timeoutMs`, SIGKILL escalation after 5s grace period. Default concurrency reduced from 5 → 2 for resource-constrained environments.\n\n## Incremental Update Strategy\n\n`UpdateOrchestrator.preparePlan()` workflow: (1) read `content_hash` from `.sum` YAML frontmatter via `readSumFile()`, (2) compute current SHA-256 via `computeContentHash()`, (3) hash mismatch → add to `filesToAnalyze` as `FileChange` with `status: 'modified'`/`'added'`, (4) hash match → add to `filesToSkip`, (5) detect orphans: `.sum` files for deleted source files or renamed oldPaths, (6) call `cleanupOrphans()` to delete stale `.sum` files, (7) call `cleanupEmptyDirectoryDocs()` to remove `AGENTS.md` from directories with no remaining sources, (8) compute `affectedDirs` via `getAffectedDirectories()` walking parent directories of changed files, (9) regenerate `.sum` for `filesToAnalyze` via Phase 1 pool execution, (10) regenerate `AGENTS.md` for `affectedDirs` sequentially (no Phase 2 post-order traversal required). Git integration: supports committed changes (`git diff <baseCommit>..HEAD`) and uncommitted (`git status --porcelain` merge via `--uncommitted` flag), rename detection via `git diff -M` (50% similarity threshold), fallback to SHA-256 hashing for non-git workflows.\n\n## Behavioral Contracts\n\n**GENERATED_MARKER:** `<!-- Generated by agents-reverse-engineer -->`\n\n**Frontmatter extraction regex:** `/^---\\n([\\s\\S]*?)\\n---\\n/`\n\n**YAML field patterns:**\n```javascript\n/generated_at:\\s*(.+)/\n/content_hash:\\s*(.+)/\n/purpose:\\s*(.+)/\n/key:\\s*\\[([^\\]]*)\\]/  // Inline arrays\n/key:\\s*\\n((?:\\s+-\\s+.+\\n?)+)/m  // Multi-line arrays\n```\n\n**Array formatting heuristic:** Inline if `length <= 3` and all items `< 40` chars, multi-line otherwise.\n\n**Export extraction regex:** `/^[ \\t]*export\\s+(?:default\\s+)?(?:function|class|const|let|var|type|interface|enum)\\s+(\\w+)/gm`\n\n**Path extraction patterns (phantom-paths):**\n```javascript\n/\\[(?:[^\\]]*)\\]\\((\\.[^)]+)\\)/g              // Markdown: [text](./path)\n/`((?:src\\/|\\.\\.?\\/)[^`]+\\.[a-z]{1,4})`/g  // Backtick: `src/foo.ts`\n/(?:from|in|by|via|see)\\s+`?(src\\/[\\w\\-./]+)`?/gi // Prose: \"from src/foo/\"\n```\n\n**Rate limit detection patterns:** `['rate limit', '429', 'too many requests', 'overloaded'].some(p => stderr.toLowerCase().includes(p))`\n\n**Preamble stripping patterns:**\n1. YAML separator: `/\\n---\\n/` within first 500 chars\n2. Bold uppercase start: `/^[\\s\\S]{0,500}?(\\*\\*[A-Z])/` matches conversational preamble, strips if <300 chars and no `##` headers\n\n**Skip directories:** `Set(['node_modules', '.git', '.agents-reverse-engineer', 'vendor', 'dist', 'build', '__pycache__', '.next', 'venv', '.venv', 'target', '.cargo', '.gradle'])`\n\n**Manifest detection:** `['package.json', 'Cargo.toml', 'go.mod', 'pyproject.toml', 'pom.xml', 'build.gradle', 'Gemfile', 'composer.json', 'CMakeLists.txt', 'Makefile']`",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 4499,
      "cacheReadTokens": 17477,
      "cacheCreationTokens": 36337,
      "latencyMs": 123792,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": []
    },
    {
      "timestamp": "2026-02-09T16:17:55.364Z",
      "prompt": "Generate AGENTS.md for directory: \".\" (agents-reverse-engineer)\n\n## File Summaries (5 files)\n\n### LANGUAGES-MANIFEST.md\n**Purpose:** LANGUAGES-MANIFEST.md documents package manager manifest files across 26 programming languages, mapping each ecosyste...\n\n**LANGUAGES-MANIFEST.md documents package manager manifest files across 26 programming languages, mapping each ecosystem's dependency declaration files to their respective package managers and indicating which formats ARE's directory aggregation phase currently supports.**\n\n## Supported Manifest Detection\n\nPhase 2 directory aggregation (`src/generation/prompts/builder.ts` via `detectManifest()`) recognizes 9 manifest types for inclusion in AGENTS.md prompts:\n\n- `package.json` (JavaScript/TypeScript: npm/yarn/pnpm)\n- `Cargo.toml` (Rust: cargo)\n- `go.mod` (Go: go modules)\n- `pyproject.toml` (Python: poetry)\n- `pom.xml` (Java: Maven)\n- `build.gradle` (Java/Kotlin: Gradle)\n- `Gemfile` (Ruby: bundler)\n- `composer.json` (PHP: Composer)\n- `CMakeLists.txt` and `Makefile` (C/C++: CMake/make)\n\nSupported languages marked with ✓ in table: JavaScript/TypeScript, Python, Go, Rust.\n\n## Markdown Table Structure\n\nColumn schema:\n- **Language**: Ecosystem name or language family (e.g., \"C#/.NET\", \"Dart/Flutter\")\n- **Manifest File(s)**: Comma-separated list of dependency declaration files, including alternatives (e.g., \"requirements.txt\", \"pyproject.toml\", \"setup.py\", \"Pipfile\" for Python)\n- **Package Manager**: Corresponding dependency resolution tools (e.g., \"pip, poetry, pipenv\")\n- **Supported**: ✓ indicates ARE's `detectManifest()` recognizes this format; empty cells indicate detection not implemented\n\n## Coverage Gaps\n\nUnsupported ecosystems with manifest entries but no detection logic:\n- Ruby (`Gemfile` listed but not in detection array despite being included in supported list)\n- Java/Kotlin Gradle variants (`build.gradle.kts`)\n- C#/.NET NuGet formats (`*.csproj`, `packages.config`, `*.fsproj`)\n- Swift (`Package.swift`)\n- Functional languages: Elixir (`mix.exs`), Erlang (`rebar.config`), Scala (`build.sbt`), Clojure (`deps.edn`, `project.clj`), Haskell (`package.yaml`, `*.cabal`, `stack.yaml`), OCaml (`dune-project`, `*.opam`)\n- Scientific/scripting: Dart (`pubspec.yaml`), Lua (`*.rockspec`), R (`DESCRIPTION`), Julia (`Project.toml`)\n- Systems languages: Zig (`build.zig.zon`), Nim (`*.nimble`)\n- C/C++ alternatives: Conan (`conanfile.txt`), vcpkg (`vcpkg.json`)\n\n## Integration Point\n\nReferenced by `buildDirectoryPrompt()` in `src/generation/prompts/builder.ts` when constructing Phase 2 aggregation prompts. Manifest content appended to prompt context via `fs.readFileSync(manifestPath, 'utf-8')` when `detectManifest(directoryPath)` returns non-null filename.\n\n## Extension Strategy\n\nAdding new manifest support requires:\n1. Append filename to `COMMON_MANIFESTS` array in `src/generation/prompts/builder.ts`\n2. Update Supported column (✓) in this reference table\n3. Verify `fs.existsSync(path.join(directoryPath, manifestFilename))` detection logic handles glob patterns (e.g., `*.csproj`) if wildcard matching needed\n### LICENSE\n**Purpose:** LICENSE grants MIT License permissions for agents-reverse-engineer software under Copyright (c) 2026 GeoloeG-IsT.\n\n**LICENSE grants MIT License permissions for agents-reverse-engineer software under Copyright (c) 2026 GeoloeG-IsT.**\n\n## License Type and Grant\n\nMIT License permits unrestricted use, modification, distribution, sublicensing, and sale of the Software and associated documentation files without royalties or fees, subject to preservation of copyright notice and permission notice in all copies or substantial portions.\n\n## Copyright Holder\n\nCopyright (c) 2026 GeoloeG-IsT owns all intellectual property rights to the agents-reverse-engineer codebase.\n\n## Warranty Disclaimer\n\nSOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT disclaims all warranties and representations.\n\n## Liability Limitation\n\nAUTHORS OR COPYRIGHT HOLDERS SHALL NOT BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE limits legal exposure for maintainers.\n\n## Required Attribution\n\n\"The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software\" mandates preservation of attribution in derivative works and distributions.\n\n## Integration Impact\n\nCI/CD workflow (`.github/workflows/publish.yml`) publishes npm package with `--provenance --access public` flags, making MIT License terms apply to all distributed artifacts. Package.json `\"license\": \"MIT\"` field declares license for npm registry. README.md references LICENSE file as source of legal terms for users and contributors.\n### README.md\n**Purpose:** README.md serves as the project marketing page, installation guide, and CLI reference for agents-reverse-engineer, do...\n\n**README.md serves as the project marketing page, installation guide, and CLI reference for agents-reverse-engineer, documenting npx-based interactive installation, IDE command workflows (`/are-generate`, `/are-update`), three-phase generation pipeline (file analysis → directory docs → root docs), and configuration schema for `.agents-reverse-engineer/config.yaml`.**\n\n## Installation Patterns\n\n**Interactive installer invocation:** `npx agents-reverse-engineer@latest` launches prompt-based flow selecting runtime (`claude`, `opencode`, `gemini`, `all`) and location scope (global `-g` targeting `~/.claude/`, local `-l` targeting `./.claude/`).\n\n**Non-interactive installation flags:** `--runtime <rt>` plus `-g`/`-l` bypass prompts for CI/CD scenarios. Example: `npx agents-reverse-engineer@latest --runtime claude -g` installs to `~/.claude/commands/` and `~/.claude/hooks/`.\n\n**Uninstall command:** `npx agents-reverse-engineer@latest uninstall` removes command files (`/are-*`), session hooks (Claude/Gemini only), ARE permission entries from `settings.json`, and `.agents-reverse-engineer/` folder (local installs only).\n\n**Version check:** `npx agents-reverse-engineer@latest --version` displays installed version.\n\n## CLI Commands\n\n**Entry point:** `are` binary (alias: `agents-reverse-engineer`) invokes `src/cli/index.ts`.\n\n**Command surface:**\n- `are install` — Interactive installer with runtime/location prompts\n- `are install --runtime <rt> -g/-l` — Non-interactive installation\n- `are install -u` — Uninstall (removes files/hooks/permissions)\n- `are init` — Creates `.agents-reverse-engineer/config.yaml` with defaults\n- `are discover` — Scans codebase, lists discoverable files\n- `are discover --plan` — Writes `GENERATION-PLAN.md` with post-order directory traversal\n- `are discover --show-excluded` — Displays excluded files with exclusion reasons\n- `are generate` — Executes three-phase pipeline: `.sum` files → `AGENTS.md` per directory → root docs (`CLAUDE.md`/`GEMINI.md`/`OPENCODE.md`)\n- `are update` — Regenerates only files with SHA-256 content hash mismatches\n- `are specify` — Synthesizes all `AGENTS.md` into `specs/SPEC.md` (use `--multi-file` for split output, `--dry-run` for preview)\n- `are clean` — Deletes `.sum`, `AGENTS.md` (generated only), `CLAUDE.md`, `GEMINI.md`, `OPENCODE.md`, `GENERATION-PLAN.md`\n\n## IDE Command Integration\n\n**Claude Code commands:** Installed to `~/.claude/skills/are-<command>/SKILL.md` with frontmatter `name: /are-<command>`.\n\n**OpenCode commands:** Installed to `~/.config/opencode/commands/are-<command>.md` with frontmatter `agent: build`.\n\n**Gemini CLI commands:** Installed to `~/.gemini/commands/are-<command>.toml` with TOML format (`description`, `prompt` triple-quoted strings).\n\n**Supported commands:**\n- `/are-init` — Initialize configuration and commands\n- `/are-discover` — Rediscover files and regenerate plan\n- `/are-generate` — Full documentation generation (long-running: remove stale `progress.log`, use `run_in_background: true`, poll via `tail -5` every 10-15s, check `TaskOutput` with `block: false`)\n- `/are-update` — Incremental update for changed files\n- `/are-specify` — Generate project specification from aggregated docs\n- `/are-clean` — Remove all generated artifacts\n\n## Generated Documentation Artifacts\n\n**`.sum` files:** Per-file summaries with YAML frontmatter containing `file_type`, `generated_at` timestamp. Body includes purpose statement, public interface (function/class signatures), dependencies (package imports + local file paths), implementation notes. Example: `authenticate(token: string): User`, `generateToken(user: User): string`.\n\n**`AGENTS.md`:** Per-directory overviews with directory role description, files grouped by purpose (Types, Services, Utils), subdirectory brief descriptions. User-authored `AGENTS.md` renamed to `AGENTS.local.md` and prepended to generated content.\n\n**Root documents:**\n- `CLAUDE.md` — Project entry point for Claude Code (auto-loaded via platform convention)\n- `GEMINI.md` — Project entry point for Gemini CLI\n- `OPENCODE.md` — Project entry point for OpenCode\n- Root `AGENTS.md` — Universal format directory overview\n\n**`GENERATION-PLAN.md`:** Post-order directory traversal plan listing all files to analyze, grouped by phase (file analysis, directory aggregation, root synthesis).\n\n**`specs/SPEC.md`:** Synthesized project specification from all `AGENTS.md` files (generated via `are specify`).\n\n## Configuration Schema\n\n**Config file location:** `.agents-reverse-engineer/config.yaml`\n\n**Section: `exclude`**\n- `patterns: string[]` — Gitignore-style globs (e.g., `[\"*.log\", \"temp/**\"]`)\n- `vendorDirs: string[]` — Third-party directories to skip (default: `node_modules`, `dist`, `.git`, etc.)\n- `binaryExtensions: string[]` — Non-text file extensions (default: `.png`, `.jpg`, `.pdf`, etc.)\n\n**Section: `options`**\n- `followSymlinks: boolean` — Follow symbolic links during traversal (default: `false`)\n- `maxFileSize: number` — Binary detection threshold in bytes (default: `1048576` = 1MB)\n\n**Section: `output`**\n- `colors: boolean` — Enable ANSI color codes in terminal (default: `true`)\n- `verbose: boolean` — Show each file as processed (default: `true`)\n\n**Section: `ai`**\n- `backend: 'claude' | 'gemini' | 'opencode' | 'auto'` — AI service selection (default: `auto` detects first available)\n- `model: string | null` — Model identifier override (backend-specific, default: `sonnet`)\n- `timeoutMs: number` — Subprocess timeout in milliseconds (default: `300000` = 5 minutes)\n- `maxRetries: number` — Exponential backoff retry attempts (default: `3`)\n- `concurrency: number` — Worker pool size 1-10 (default: `5`, reduced to `2` for WSL environments due to resource constraints)\n\n**Subsection: `ai.telemetry`**\n- `keepRuns: number` — Retention limit for historical run logs (default: `50`)\n- `costThresholdUsd: number | null` — Warning threshold for cumulative costs in USD (optional)\n\n**Subsection: `ai.pricing` (per-backend overrides)**\n- `inputCostPerMTok: number` — USD per 1 million input tokens\n- `outputCostPerMTok: number` — USD per 1 million output tokens\n\n**Example pricing entry:**\n```yaml\npricing:\n  claude-opus-4:\n    inputCostPerMTok: 15.0\n    outputCostPerMTok: 75.0\n```\n\n## Three-Phase Generation Pipeline\n\n**Phase 1: File Analysis**\n- Concurrent execution via iterator-based worker pool (`src/orchestration/pool.ts`)\n- Generates `.sum` file for each source file via AI subprocess\n- Respects `.gitignore`, excludes vendor directories and binary files\n- SHA-256 content hashing in YAML frontmatter for incremental updates\n\n**Phase 2: Directory Aggregation**\n- Post-order traversal (deepest directories first)\n- Waits for all child `.sum` files via `isDirectoryComplete()` predicate\n- Generates `AGENTS.md` per directory consuming child summaries and subdirectory `AGENTS.md` files\n- Import maps via `extractDirectoryImports()` with path verification\n- Manifest detection (9 types: `package.json`, `Cargo.toml`, `go.mod`, `pyproject.toml`, `pom.xml`, `build.gradle`, `Gemfile`, `composer.json`, `CMakeLists.txt`/`Makefile`)\n\n**Phase 3: Root Document Synthesis**\n- Sequential execution (concurrency=1)\n- Generates `CLAUDE.md`, `GEMINI.md`, `OPENCODE.md`\n- Consumes all `AGENTS.md` files via `collectAgentsDocs()` recursive traversal\n- Parses root `package.json` for project metadata\n- Enforces synthesis-only constraint (no invention of features not in source documents)\n\n## Incremental Update Workflow\n\n**Hash comparison strategy:**\n1. Read `content_hash` from `.sum` YAML frontmatter via `readSumFile()`\n2. Compute current SHA-256 hash via `computeContentHash()`\n3. Mismatch → add to `filesToAnalyze` with `status: 'modified'` or `'added'`\n4. Match → add to `filesToSkip`\n5. Detect orphans: `.sum` files for deleted sources or renamed oldPaths\n6. Call `cleanupOrphans()` to delete stale summaries\n7. Call `cleanupEmptyDirectoryDocs()` to remove `AGENTS.md` from directories with no remaining sources\n8. Compute `affectedDirs` via `getAffectedDirectories()` by walking parent directories\n9. Regenerate `.sum` for changed files via Phase 1 pool\n10. Regenerate `AGENTS.md` for affected directories sequentially\n\n**Git integration:** Supports `git diff <baseCommit>..HEAD` for committed changes, `git status --porcelain` merge via `--uncommitted` flag, rename detection via `git diff -M` (50% similarity threshold).\n\n## Subprocess Resource Management\n\n**Problem:** Claude CLI spawns excessive Node.js threads (GitHub issue #5771: 200 instances reported). With concurrency=5, each subprocess spawns internal threads causing RAM exhaustion in WSL environments.\n\n**Mitigations applied:**\n- `NODE_OPTIONS='--max-old-space-size=512'` limits heap to 512MB per subprocess\n- `UV_THREADPOOL_SIZE='4'` constrains libuv thread pool to 4 threads\n- `CLAUDE_CODE_DISABLE_BACKGROUND_TASKS='1'` prevents background task spawning\n- `--disallowedTools Task` prevents subprocess from spawning subagents\n- Process group killing: `kill(-pid)` terminates entire subprocess tree\n- SIGTERM sent at `timeoutMs`, SIGKILL escalation after 5s grace period\n- Default concurrency reduced from 5 → 2 for resource-constrained environments\n\n## Session Hooks\n\n**Claude/Gemini lifecycle:**\n- `are-check-update.js` (SessionStart) — Spawns detached process querying `npm view agents-reverse-engineer version`, compares against `~/.claude/ARE-VERSION`, caches result to `~/.claude/cache/are-update-check.json`\n- `are-session-end.js` (SessionEnd) — Runs `git status --porcelain`, spawns `npx agents-reverse-engineer@latest update --quiet` as detached background process if changes detected\n\n**OpenCode plugins:**\n- `opencode-are-check-update.js` — Exports `AreCheckUpdate()` async factory returning plugin with `event['session.created']` handler\n- `opencode-are-session-end.js` — Exports `AreSessionEnd()` async factory returning plugin with `event['session.deleted']` handler\n\n**Disable mechanisms:**\n- Environment variable: `ARE_DISABLE_HOOK=1`\n- Config flag: `hook_enabled: false` in `.agents-reverse-engineer.yaml` (substring search, no YAML parser)\n\n## Requirements\n\n**Runtime:** Node.js ≥18.0.0\n\n**AI Coding Assistant (at least one):**\n- Claude Code — Full support with session hooks, auto-loaded `CLAUDE.md`\n- Gemini CLI — Full support with session hooks, manual `GEMINI.md` loading\n- OpenCode — `AGENTS.md` format supported, manual `OPENCODE.md` loading\n- Any assistant supporting `AGENTS.md` format (universal compatibility)\n\n## Marketing Positioning\n\n**Value proposition:** Eliminates repetitive context explanation in AI assistant sessions by generating persistent, assistant-readable documentation (`.sum` files, `AGENTS.md`, root docs).\n\n**Target audience:** Developers using AI coding assistants (Claude Code, OpenCode, Gemini CLI) who want codebase understanding from first message without manual documentation overhead.\n\n**Pain points addressed:**\n- \"AI assistant doesn't know my codebase structure\"\n- \"Every session starts fresh\"\n- \"Explaining same architecture repeatedly\"\n\n**User testimonials (quoted):**\n- \"Finally, my AI assistant actually understands my codebase structure.\"\n- \"No more explaining the same architecture in every conversation.\"\n\n## License\n\nMIT\n### package.json\n**Purpose:** package.json defines npm package metadata for agents-reverse-engineer 0.6.5, a CLI tool that reverse-engineers codeba...\n\n**package.json defines npm package metadata for agents-reverse-engineer 0.6.5, a CLI tool that reverse-engineers codebase documentation for AI agents via ES module-based TypeScript compilation and dual binary entry points.**\n\n## Binary Entry Points\n\n`bin` declares two executable names (`agents-reverse-engineer`, `are`) both resolving to `dist/cli/index.js` after TypeScript compilation.\n\n## Build Scripts\n\n`build` invokes `tsc` to compile TypeScript sources from `src/` to `dist/`.\n\n`build:hooks` executes `node scripts/build-hooks.js` to copy hook files from `hooks/` to `hooks/dist/` for npm tarball inclusion.\n\n`prepack` removes documentation artifacts `LICENSE.sum` and `README.md.sum` before packaging to prevent pollution of published package.\n\n`prepublishOnly` chains `npm run build && npm run build:hooks` ensuring TypeScript compilation and hook preparation execute before `npm publish`.\n\n`dev` launches `tsx watch src/cli/index.ts` for hot-reloading development mode.\n\n## Runtime Dependencies\n\n`fast-glob` (^3.3.3) provides file discovery with glob pattern matching for codebase traversal.\n\n`ignore` (^7.0.3) parses `.gitignore` files to filter discovered files.\n\n`isbinaryfile` (^5.0.4) detects binary files via extension and content analysis to exclude from text-based processing.\n\n`ora` (^8.1.1) renders terminal spinners for progress indication during long-running operations.\n\n`picocolors` (^1.1.1) adds ANSI color codes to CLI output for formatted logging.\n\n`simple-git` (^3.27.0) executes git commands for change detection via `git diff`, `git status`, SHA-256 fallback.\n\n`yaml` (^2.7.0) parses `.agents-reverse-engineer/config.yaml` configuration files.\n\n`zod` (^3.24.1) validates configuration schema against defined TypeScript types in `src/config/schema.ts`.\n\n## Development Dependencies\n\n`@types/node` (^22.10.7) provides Node.js API type definitions for TypeScript compilation.\n\n`tsx` (^4.19.2) executes TypeScript files directly without precompilation for dev script.\n\n`typescript` (^5.7.3) compiles TypeScript sources to JavaScript in `dist/` output directory.\n\n## Engine Requirements\n\n`node` field specifies `>=18.0.0` as minimum required Node.js version for ES module support.\n\n## Package Distribution\n\n`type: \"module\"` declares package uses ES modules instead of CommonJS.\n\n`main` points to `dist/cli/index.js` as default entry when required as module.\n\n`files` array whitelists `dist`, `hooks/dist`, `README.md`, `LICENSE` for inclusion in published tarball (excludes `src/`, `scripts/`, raw `hooks/` before build).\n\n## Repository Metadata\n\n`repository.url` links to `git+https://github.com/GeoloeG-IsT/agents-reverse-engineer.git` for source browsing.\n\n`bugs.url` directs issue reports to `https://github.com/GeoloeG-IsT/agents-reverse-engineer/issues`.\n\n`homepage` references `https://github.com/GeoloeG-IsT/agents-reverse-engineer#readme` for documentation.\n\n`author` identifies package maintainer as \"GeoloeG-IsT\".\n\n`license` declares \"MIT\" license for open-source distribution.\n\n`keywords` array includes `[\"documentation\", \"codebase\", \"ai\", \"agents\", \"reverse-engineering\"]` for npm registry search indexing.\n### tsconfig.json\n**Purpose:** tsconfig.json configures TypeScript compilation to ES2022 with NodeNext module resolution, emitting source maps and d...\n\n**tsconfig.json configures TypeScript compilation to ES2022 with NodeNext module resolution, emitting source maps and declarations from `src/` to `dist/` with strict type checking.**\n\n## Compiler Target and Module System\n\n`target: \"ES2022\"` emits JavaScript compatible with ES2022 runtime features (top-level await, class fields, private methods). `module: \"NodeNext\"` and `moduleResolution: \"NodeNext\"` enforce Node.js ESM conventions with explicit `.js` extensions in import statements and `package.json` `type: \"module\"` semantics. `lib: [\"ES2022\"]` provides ES2022 standard library type definitions.\n\n## Output Configuration\n\n`outDir: \"dist\"` emits compiled JavaScript to `dist/` directory. `rootDir: \"src\"` preserves source tree structure during compilation. `declaration: true` generates `.d.ts` type definition files for package consumers. `declarationMap: true` creates `.d.ts.map` files mapping declarations back to source for IDE navigation. `sourceMap: true` emits `.js.map` files for runtime debugging.\n\n## Type Checking Strictness\n\n`strict: true` enables all strict type-checking options: `strictNullChecks`, `strictFunctionTypes`, `strictBindCallApply`, `strictPropertyInitialization`, `noImplicitAny`, `noImplicitThis`, `alwaysStrict`. `forceConsistentCasingInFileNames: true` prevents case-insensitive import mismatches across filesystems.\n\n## Module Interoperability\n\n`esModuleInterop: true` enables synthetic default imports from CommonJS modules via `import x from 'cjs'` instead of `import * as x from 'cjs'`. `resolveJsonModule: true` allows importing JSON files with type inference (used for `package.json` metadata parsing in `src/generation/prompts/builder.ts`). `isolatedModules: true` ensures each file can be transpiled independently (required for TypeScript-on-demand bundlers like tsx).\n\n## Compilation Scope\n\n`include: [\"src/**/*\"]` compiles all files under `src/` directory. `exclude: [\"node_modules\", \"dist\"]` prevents recursive compilation of dependencies and output artifacts. `skipLibCheck: true` skips type checking of `.d.ts` files in `node_modules/` to improve compilation performance.\n\n## Build Integration\n\nReferenced by `npm run build` script invoking `tsc` compiler. Emitted `dist/` output structure mirrors `src/` tree: `src/cli/index.ts` → `dist/cli/index.js` + `dist/cli/index.d.ts` + `dist/cli/index.js.map`. Binary entry points in `package.json` (`bin.are`, `bin.agents-reverse-engineer`) resolve to `dist/cli/index.js`. Pre-publish hook (`prepublishOnly`) executes `npm run build` ensuring fresh compilation before npm tarball creation.\n\n## Project Directory Structure\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\n\n## Subdirectories\n\n### docs/\n<!-- Generated by agents-reverse-engineer -->\n\n# docs\n\nOriginal design specification and research artifacts defining the Recursive Language Model (RLM) algorithm and agents-reverse-engineer tool inception rationale.\n\n## Contents\n\n### [INPUT.md](./INPUT.md)\nInception document defining the RLM algorithm via post-order tree traversal (leaf file analysis → directory aggregation → root synthesis), proposed command interface (`/are-generate`, `/are-update`), session lifecycle integration requirements, and research directives for GSD/BMAD methodology analysis.\n\n## Purpose\n\n`INPUT.md` serves as the foundational design specification established before implementation began. It defines the core RLM workflow: (1) build project tree, (2) traverse post-order starting at deepest leaves, (3) generate `.sum` summaries for files, (4) generate `AGENTS.md` for directories once all children complete, (5) recurse upward to project root. The document specifies target AI platforms (Claude Code, OpenCode, Gemini) and automatic documentation synchronization via session-end hooks.\n\n## Relationship to Implementation\n\nThe three-phase pipeline in `src/generation/orchestrator.ts` implements the RLM algorithm specified here:\n- **Phase 1** (concurrent file analysis) → `.sum` generation for leaf nodes\n- **Phase 2** (post-order directory aggregation) → `AGENTS.md` synthesis with depth-based sorting\n- **Phase 3** (root synthesis) → `CLAUDE.md`/`GEMINI.md`/`OPENCODE.md` platform-specific integration documents\n\nSession lifecycle hooks in `hooks/are-session-end.js` and `hooks/opencode-are-session-end.js` implement the automatic update mechanism proposed in this specification.\n### hooks/\n<!-- Generated by agents-reverse-engineer -->\n\n# hooks/\n\n**Session lifecycle hooks for Claude Code, Gemini CLI, and OpenCode runtimes that execute version checking on session start and incremental documentation updates on session end via detached background processes.**\n\n## Contents\n\n### Session Start Hooks\n\n**[are-check-update.js](./are-check-update.js)** — Claude/Gemini SessionStart hook spawning detached `npm view agents-reverse-engineer version` subprocess, comparing against local `ARE-VERSION` file (project `.claude/ARE-VERSION` before global `~/.claude/ARE-VERSION`), writing comparison result to `~/.claude/cache/are-update-check.json` with schema `{ update_available, installed, latest, checked }`.\n\n**[opencode-are-check-update.js](./opencode-are-check-update.js)** — OpenCode plugin exporting `AreCheckUpdate()` async factory returning `event['session.created']` handler that spawns detached npm version check subprocess, writes cache to `~/.config/opencode/cache/are-update-check.json`.\n\n### Session End Hooks\n\n**[are-session-end.js](./are-session-end.js)** — Claude/Gemini SessionEnd hook executing `git status --porcelain` change detection, spawning detached `npx agents-reverse-engineer@latest update --quiet` subprocess if uncommitted changes detected, exits silently if `ARE_DISABLE_HOOK=1` or config contains `hook_enabled: false` substring.\n\n**[opencode-are-session-end.js](./opencode-are-session-end.js)** — OpenCode plugin exporting `AreSessionEnd()` async factory returning `event['session.deleted']` handler that executes identical git status check and detached update spawn as Claude/Gemini variant.\n\n## Architecture\n\n### Detached Subprocess Pattern\n\nAll hooks use identical background process spawning pattern:\n```javascript\nspawn(process.execPath, ['-e', scriptString], { \n  stdio: 'ignore', \n  detached: true, \n  windowsHide: true \n}).unref()\n```\n\nInline script via `-e` flag executes synchronous Node.js APIs (`execSync`, `readFileSync`, `writeFileSync`) in isolated process that survives parent termination. `stdio: 'ignore'` prevents output blocking session lifecycle, `unref()` allows parent exit without waiting for completion.\n\n### Version File Resolution\n\nCheck update hooks read sentinel `ARE-VERSION` file in priority order:\n1. Project-local: `<cwd>/.{claude,opencode}/ARE-VERSION`\n2. Global: `~/{.claude,.config/opencode}/ARE-VERSION`\n\nFalls back to `'0.0.0'` if neither exists. Installer writes this file during setup (`src/installer/operations.ts`).\n\n### Cache Schema\n\nVersion check subprocess writes JSON to platform-specific cache:\n- Claude/Gemini: `~/.claude/cache/are-update-check.json`\n- OpenCode: `~/.config/opencode/cache/are-update-check.json`\n\n**Format:**\n```json\n{\n  \"update_available\": boolean,\n  \"installed\": \"semver-string\",\n  \"latest\": \"semver-string\" | \"unknown\",\n  \"checked\": 1234567890\n}\n```\n\n### Disable Mechanisms\n\nSession-end hooks exit silently when:\n- `process.env.ARE_DISABLE_HOOK === '1'`\n- `.agents-reverse-engineer.yaml` exists and contains substring `'hook_enabled: false'` (no YAML parser)\n\n### Change Detection\n\nSession-end hooks execute `execSync('git status --porcelain', { encoding: 'utf-8' })` and exit silently if output is empty (no uncommitted changes) or if command throws (non-git repo).\n\n## File Relationships\n\n**Hook installation:** `src/installer/operations.ts` copies hooks to platform-specific directories and writes `ARE-VERSION` sentinel file.\n\n**Build process:** `scripts/build-hooks.js` copies hook files to `hooks/dist/` directory for npm tarball inclusion before publish.\n\n**Session-end update invocation:** Spawns `npx agents-reverse-engineer@latest update --quiet` which executes `src/cli/update.ts` orchestrator with hash-based change detection and incremental regeneration.\n\n**Platform path resolution:** `src/installer/paths.ts` provides platform-specific config directory lookup respecting environment overrides (`CLAUDE_CONFIG_DIR`, `OPENCODE_CONFIG_DIR`, `GEMINI_CONFIG_DIR`).\n\n## Behavioral Contracts\n\n**Git status command:** `'git status --porcelain'` with `{ encoding: 'utf-8' }` — detects uncommitted changes (modified/staged/untracked files).\n\n**npm version query:** `'npm view agents-reverse-engineer version'` with `{ encoding: 'utf8', timeout: 10000, windowsHide: true }` — queries registry for latest published version.\n\n**Update invocation:** `['npx', 'agents-reverse-engineer@latest', 'update', '--quiet']` — spawns latest published version with `--quiet` flag suppressing terminal output.\n\n**Config disable check pattern:** `readFileSync('.agents-reverse-engineer.yaml', 'utf-8').includes('hook_enabled: false')` — substring match without YAML parsing for fast disable detection.\n\n**Cache directory creation:** `mkdirSync(cacheDir, { recursive: true })` — ensures `~/{.claude,.config/opencode}/cache/` exists before subprocess spawn.\n### scripts/\n<!-- Generated by agents-reverse-engineer -->\n\n# scripts/\n\nBuild automation utilities for npm package distribution. Currently contains a single script that prepares session lifecycle hooks for inclusion in the published npm tarball.\n\n## Contents\n\n**[build-hooks.js](./build-hooks.js)** — Copies `.js` hook files from `hooks/` to `hooks/dist/` via synchronous `copyFileSync()`, filtering non-JS files and the dist directory itself, invoked by `npm run build:hooks` and `prepublishOnly` lifecycle hook.\n\n## Build Pipeline Integration\n\n**Execution trigger:** `build-hooks.js` runs during `npm run prepublishOnly`, which npm automatically invokes before `npm publish`. The package.json `scripts` section chains `prepublishOnly` → `npm run build && npm run build:hooks`, ensuring TypeScript compilation (`tsc`) and hook copying both complete before tarball creation.\n\n**Distribution requirement:** Session lifecycle hooks (`are-check-update.js`, `are-session-end.js`, `opencode-are-check-update.js`, `opencode-are-session-end.js`) must exist in `hooks/dist/` for the npm tarball to include them. The installer module (`src/installer/operations.ts`) copies these files from the installed package into IDE config directories (`~/.claude/hooks/`, `~/.config/opencode/plugins/`).\n\n## File Operations\n\n**Path resolution:** Uses ES module `import.meta.url` → `fileURLToPath()` → `__dirname` pattern to locate project root. Constructs `HOOKS_SRC` as `join(projectRoot, 'hooks')` and `HOOKS_DIST` as `join(projectRoot, 'hooks', 'dist')`.\n\n**Synchronous workflow:**\n1. `existsSync(HOOKS_DIST)` checks for dist directory\n2. `mkdirSync(HOOKS_DIST, { recursive: true })` creates directory if missing\n3. `readdirSync(HOOKS_SRC)` enumerates source files\n4. `.filter(f => f.endsWith('.js') && f !== 'dist')` excludes non-JavaScript files and dist directory\n5. `copyFileSync(join(HOOKS_SRC, file), join(HOOKS_DIST, file))` copies each hook file\n6. Console logs progress: per-file `\"Copied: {file} -> hooks/dist/{file}\"` and summary `\"Done. {count} hook(s) built.\"`\n\n## Behavioral Contracts\n\n**File filter predicate:** `/\\.js$/ && f !== 'dist'` (endsWith check + exclusion)\n### src/\n<!-- Generated by agents-reverse-engineer -->\n\n# src\n\n**TypeScript source tree containing CLI entry points, three-phase AI orchestration pipeline, file discovery filters, configuration management, subprocess-spawning AI service layer, quality validators, and IDE integration installers for agents-reverse-engineer brownfield documentation generator.**\n\n## Contents\n\n**[version.ts](./version.ts)** — `getVersion()` reads `package.json` from compiled module parent directory (dist/ → project root) via `fileURLToPath(import.meta.url)` → `dirname()` → `join(__dirname, '..', 'package.json')`, parses JSON extracting `version` field, returns `'unknown'` on error (file not found, JSON parse failure, missing field). Called by CLI help text, version flag output, session hooks for npm registry comparison.\n\n## Subdirectories\n\n**[ai/](./ai/)** — Backend-agnostic AI service orchestration: subprocess-spawning adapters (ClaudeBackend with Zod JSON parsing, GeminiBackend/OpenCodeBackend stubs), exponential backoff retry with rate limit detection (['rate limit', '429', 'too many requests', 'overloaded']), timeout enforcement (SIGTERM at timeoutMs, SIGKILL after 5s, process group killing `kill(-pid)`), NDJSON telemetry logging to `.agents-reverse-engineer/logs/run-<timestamp>.json` with token/cost tracking, trace emission (subprocess:spawn/exit/retry).\n\n**[change-detection/](./change-detection/)** — Git-based change detection parsing `git diff --name-status -M` with rename detection, merges uncommitted working tree changes via `status.modified[]`/`deleted[]`/`not_added[]`/`staged[]`, SHA-256 content hashing for `.sum` frontmatter verification, returns `ChangeDetectionResult` with `FileChange[]` array (path/status/oldPath/contentHash), status codes map to `ChangeType` ('added'/'modified'/'deleted'/'renamed').\n\n**[cli/](./cli/)** — Command entry points routing parsed `process.argv` to handlers: `initCommand()` writes config YAML, `cleanCommand()` deletes artifacts with `GENERATED_MARKER` filtering, `discoverCommand()` builds `GENERATION-PLAN.md`, `generateCommand()` orchestrates three-phase pipeline with backend resolution, `updateCommand()` executes incremental hash-based regeneration, `specifyCommand()` synthesizes project specifications from AGENTS.md corpus. Main index parses flags via `parseArgs()` with short flag expansion (`-h` → `help`, `-g` → `global`), dual routing for installer mode vs. command dispatch, exit codes (0=success, 1=partial failure, 2=total failure/CLI not found).\n\n**[config/](./config/)** — Configuration management: `loadConfig()` reads `.agents-reverse-engineer/config.yaml` via YAML parsing and `ConfigSchema.parse()` Zod validation (returns defaults on ENOENT, throws `ConfigError` on ZodError), `writeDefaultConfig()` generates commented template with inline defaults, `getDefaultConcurrency()` computes adaptive worker pool size via `clamp(cores * 5, 2, min(memCap, 20))` where `memCap = floor((totalMemGB * 0.5) / 0.512)`, default exclusions (26 patterns, 18 vendor dirs, 26 binary extensions).\n\n**[discovery/](./discovery/)** — File discovery pipeline: `discoverFiles()` creates four-filter chain (GitignoreFilter → VendorFilter → BinaryFilter → CustomPatternFilter), `walkDirectory()` via `fast-glob` with `{absolute: true, onlyFiles: true, ignore: ['**/.git/**']}`, `applyFilters()` with 30-worker concurrency and short-circuit evaluation, returns `FilterResult` with `included[]`/`excluded[]` attribution. Filters normalize paths via `path.relative(normalizedRoot, absolutePath)` for pattern matching.\n\n**[generation/](./generation/)** — Three-phase orchestration: `GenerationOrchestrator.createPlan()` executes file discovery → `analyzeComplexity()` → `buildFilePrompt()` per file with `projectPlan` injection → groups by `path.dirname()` for directory tasks → `buildExecutionPlan()` post-order traversal (sorted by depth descending) → memory management clears `PreparedFile.content` fields after embedding. `buildDirectoryPrompt()` reads `.sum` frontmatter, child `AGENTS.md`, import maps via `extractDirectoryImports()`, detects 9 manifest types. `buildRootPrompt()` calls `collectAgentsDocs()`, parses root `package.json`, enforces synthesis-only constraints. Writers serialize YAML frontmatter (adaptive array formatting), preserve user content via `AGENTS.local.md` rename pattern.\n\n**[imports/](./imports/)** — Static import analysis: `extractImports()` applies `IMPORT_REGEX` (`/^import\\s+(type\\s+)?(?:\\{([^}]*)\\}|(\\*\\s+as\\s+\\w+)|(\\w+))\\s+from\\s+['\"]([^'\"]+)['\"]/gm`) extracting ES module syntax into `ImportEntry[]`, `extractDirectoryImports()` scans first 100 lines per file for optimization, partitions relative imports into `internal` (`./`) and `external` (`../`), filters bare specifiers (`'react'`, `node:*`), `formatImportMap()` serializes into prompt text blocks with `(type)` suffix.\n\n**[installer/](./installer/)** — npx installation orchestrator: `runInstall()` parses CLI args (`-g`/`-l`/`--runtime`/`--force`), interactive prompts via `selectRuntime()`/`selectLocation()` with arrow-key selection in TTY mode, `installFilesForRuntime()` writes command templates from `src/integration/templates.ts` and copies bundled hooks from `hooks/dist/`, registers hooks in `settings.json` via `registerClaudeHooks()`/`registerGeminiHooks()`, writes `ARE-VERSION` file, adds Bash permission patterns to Claude Code allow list. Uninstaller removes files, hook registrations, permissions, cleans empty skill directories via `cleanupAreSkillDirs()`/`cleanupLegacyGeminiFiles()`/`cleanupEmptyDirs()`.\n\n**[integration/](./integration/)** — Platform-specific AI assistant integration: `detectEnvironments()` scans for Claude (`.claude/` or `CLAUDE.md`), OpenCode (`.opencode/`), Aider (`.aider.conf.yml` or `.aider/`) via `existsSync()`, `getTemplatesForRuntime()` generates command files with placeholder substitution for `COMMAND_PREFIX` (`/are-`), `VERSION_FILE_PATH`, `$ARGUMENTS`, embeds background execution pattern with `run_in_background: true`, `TaskOutput` polling, `.agents-reverse-engineer/progress.log` monitoring. `generateIntegrationFiles()` writes templates via `ensureDir()` → `writeFileSync()` chain, copies bundled hooks for Claude.\n\n**[orchestration/](./orchestration/)** — Iterator-based worker pool orchestrating three-phase execution: `runPool()` shares single `tasks.entries()` iterator across N workers preventing idle time, emits `worker:start/end` and `task:pickup/done` trace events, supports `failFast` abort via mutable flag. `CommandRunner.executeGenerate()` runs concurrent file `.sum` generation with stale-doc detection, post-order directory `AGENTS.md` traversal grouped by depth, sequential root document synthesis (`CLAUDE.md`/`GEMINI.md`/`OPENCODE.md`), embeds quality validators post-phase (code-vs-doc, code-vs-code, phantom-paths). `ProgressReporter` streams colored console output with ETA via moving average (last 10 completion times), `ProgressLog` mirrors to `.agents-reverse-engineer/progress.log` via promise-chain serialized writes. `PlanTracker` serializes concurrent checkbox updates (`- [ ] → - [x]`) for `GENERATION-PLAN.md` via promise-chain pattern. `TraceWriter` appends to `.agents-reverse-engineer/traces/trace-<timestamp>.ndjson` with auto-populated `seq`/`ts`/`pid`/`elapsedMs` base fields, `cleanupOldTraces()` enforces 500-file retention.\n\n**[output/](./output/)** — Terminal output formatting: `createLogger()` factory with picocolors-based ANSI formatting when `options.colors` is true, `createSilentLogger()` no-op for tests, `Logger` interface (six methods: `info()`, `file()`, `excluded()`, `summary()`, `warn()`, `error()`), output routing (log/warn/error → stdout/stderr).\n\n**[quality/](./quality/)** — Code-documentation consistency validation: `checkCodeVsDoc()` extracts exports via `/^[ \\t]*export\\s+(?:default\\s+)?(?:function|class|const|let|var|type|interface|enum)\\s+(\\w+)/gm` with substring verification against `.sum` content, `checkCodeVsCode()` aggregates exports into `Map<symbol, paths[]>` for duplicate detection, `checkPhantomPaths()` extracts path references via three regex patterns (markdown links, backtick paths, prose references) resolving against `AGENTS.md` directory and project root with `.ts`/`.js` fallback. `buildInconsistencyReport()` merges issues with summary counts by type (`code-vs-doc`/`code-vs-code`/`phantom-path`) and severity (`error`/`warning`/`info`).\n\n**[specify/](./specify/)** — Specification synthesis: `buildSpecPrompt()` constructs system/user prompts enforcing nine-section structure (Overview, Architecture, API Surface, Data Structures, Configuration, Dependencies, Behavioral Contracts, Test Contracts, Build Plan) with verbatim regex/format/constant preservation, `writeSpec()` writes single-file or multi-file specifications via heading-based splitting (`splitByHeadings()` partitions on `/^# /m`, `slugify()` sanitizes heading text), throws `SpecExistsError` when targets exist and `force: false`.\n\n**[types/](./types/)** — Shared interfaces for discovery results (`ExcludedFile`, `DiscoveryResult`, `DiscoveryStats` with `exclusionReasons` histogram), consumed by `src/discovery/walker.ts`, `src/cli/discover.ts`, `src/orchestration/runner.ts`.\n\n**[update/](./update/)** — Incremental documentation synchronization: `UpdateOrchestrator.preparePlan()` executes hash-based change detection via `readSumFile()` YAML frontmatter extraction, `computeContentHash()` SHA-256 comparison, produces `UpdatePlan` with `filesToAnalyze[]` (hash mismatch/missing), `filesToSkip[]` (hash match), `cleanup: CleanupResult` (orphaned `.sum` files), `affectedDirs[]` (depth-sorted parent directories). `cleanupOrphans()` deletes `.sum` files for `FileChange` entries with `status === 'deleted'` (uses `change.path`) or `status === 'renamed'` (uses `change.oldPath`). `cleanupEmptyDirectoryDocs()` removes `AGENTS.md` from directories containing only hidden files, `.sum` files, or generated artifacts. `getAffectedDirectories()` walks parent directories via `path.dirname()` excluding deletes.\n\n## Three-Phase Pipeline Architecture\n\n**Phase 1 (File Analysis)** — Concurrent pool execution (default 2 workers for WSL, 5 elsewhere): `GenerationOrchestrator.createFileTasks()` invokes `buildFilePrompt()` per file embedding content + project structure, workers execute via `runPool()` calling `AIService.call()` → `runSubprocess()` → `execFile()` spawning AI CLI subprocesses with resource limits (`NODE_OPTIONS='--max-old-space-size=512'`, `UV_THREADPOOL_SIZE='4'`, `CLAUDE_CODE_DISABLE_BACKGROUND_TASKS='1'`, `--disallowedTools Task`). Results written via `writeSumFile()` to `.sum` paths with YAML frontmatter (`generated_at`, `content_hash`, `purpose`, `critical_todos`, `related_files`). Post-phase quality validation: throttled (concurrency=10) `checkCodeVsDoc()` comparing old vs. new `.sum` against source content, `checkCodeVsCode()` for duplicate symbol detection.\n\n**Phase 2 (Directory Aggregation)** — Post-order traversal: `GenerationOrchestrator.createDirectoryTasks()` groups files via `Map<dirname, PreparedFile[]>`, executor sorts tasks by `getDirectoryDepth()` descending (deepest first), waits for child `.sum` files via `isDirectoryComplete()`, constructs prompts via `buildDirectoryPrompt()` consuming `.sum` frontmatter + child `AGENTS.md` + import maps from `extractDirectoryImports()`. Writes via `writeAgentsMd()` preserving user content from `AGENTS.local.md`. Post-phase: runs `checkPhantomPaths()` on each generated `AGENTS.md` (three regex patterns for path extraction), aggregates issues into `phantomReport`.\n\n**Phase 3 (Root Synthesis)** — Sequential execution (concurrency=1) generates `CLAUDE.md`, `GEMINI.md`, `OPENCODE.md` via `buildRootPrompt()` aggregating all `AGENTS.md` via `collectAgentsDocs()`, parsing root `package.json`, enforcing synthesis-only constraints (no invention). Strips conversational preamble via pattern matching before writing output.\n\n## Subprocess Resource Management\n\nAI CLI spawning mitigations for Claude #5771 (200 NodeJS instances): `NODE_OPTIONS='--max-old-space-size=512'` limits heap to 512MB per subprocess, `UV_THREADPOOL_SIZE='4'` constrains libuv thread pool to 4 threads, `CLAUDE_CODE_DISABLE_BACKGROUND_TASKS='1'` prevents background task spawning, `--disallowedTools Task` blocks subagents. Process group killing: `kill(-pid)` terminates entire subprocess tree. Timeout enforcement: SIGTERM sent at `timeoutMs`, SIGKILL escalation after 5s grace period. Default concurrency reduced from 5 → 2 for resource-constrained environments.\n\n## Incremental Update Strategy\n\n`UpdateOrchestrator.preparePlan()` workflow: (1) read `content_hash` from `.sum` YAML frontmatter via `readSumFile()`, (2) compute current SHA-256 via `computeContentHash()`, (3) hash mismatch → add to `filesToAnalyze` as `FileChange` with `status: 'modified'`/`'added'`, (4) hash match → add to `filesToSkip`, (5) detect orphans: `.sum` files for deleted source files or renamed oldPaths, (6) call `cleanupOrphans()` to delete stale `.sum` files, (7) call `cleanupEmptyDirectoryDocs()` to remove `AGENTS.md` from directories with no remaining sources, (8) compute `affectedDirs` via `getAffectedDirectories()` walking parent directories of changed files, (9) regenerate `.sum` for `filesToAnalyze` via Phase 1 pool execution, (10) regenerate `AGENTS.md` for `affectedDirs` sequentially (no Phase 2 post-order traversal required). Git integration: supports committed changes (`git diff <baseCommit>..HEAD`) and uncommitted (`git status --porcelain` merge via `--uncommitted` flag), rename detection via `git diff -M` (50% similarity threshold), fallback to SHA-256 hashing for non-git workflows.\n\n## Behavioral Contracts\n\n**GENERATED_MARKER:** `<!-- Generated by agents-reverse-engineer -->`\n\n**Frontmatter extraction regex:** `/^---\\n([\\s\\S]*?)\\n---\\n/`\n\n**YAML field patterns:**\n```javascript\n/generated_at:\\s*(.+)/\n/content_hash:\\s*(.+)/\n/purpose:\\s*(.+)/\n/key:\\s*\\[([^\\]]*)\\]/  // Inline arrays\n/key:\\s*\\n((?:\\s+-\\s+.+\\n?)+)/m  // Multi-line arrays\n```\n\n**Array formatting heuristic:** Inline if `length <= 3` and all items `< 40` chars, multi-line otherwise.\n\n**Export extraction regex:** `/^[ \\t]*export\\s+(?:default\\s+)?(?:function|class|const|let|var|type|interface|enum)\\s+(\\w+)/gm`\n\n**Path extraction patterns (phantom-paths):**\n```javascript\n/\\[(?:[^\\]]*)\\]\\((\\.[^)]+)\\)/g              // Markdown: [text](./path)\n/`((?:src\\/|\\.\\.?\\/)[^`]+\\.[a-z]{1,4})`/g  // Backtick: `src/foo.ts`\n/(?:from|in|by|via|see)\\s+`?(src\\/[\\w\\-./]+)`?/gi // Prose: \"from src/foo/\"\n```\n\n**Rate limit detection patterns:** `['rate limit', '429', 'too many requests', 'overloaded'].some(p => stderr.toLowerCase().includes(p))`\n\n**Preamble stripping patterns:**\n1. YAML separator: `/\\n---\\n/` within first 500 chars\n2. Bold uppercase start: `/^[\\s\\S]{0,500}?(\\*\\*[A-Z])/` matches conversational preamble, strips if <300 chars and no `##` headers\n\n**Skip directories:** `Set(['node_modules', '.git', '.agents-reverse-engineer', 'vendor', 'dist', 'build', '__pycache__', '.next', 'venv', '.venv', 'target', '.cargo', '.gradle'])`\n\n**Manifest detection:** `['package.json', 'Cargo.toml', 'go.mod', 'pyproject.toml', 'pom.xml', 'build.gradle', 'Gemfile', 'composer.json', 'CMakeLists.txt', 'Makefile']`\n\n## Directory Hints\n\nContains manifest file(s): package.json — likely a package or project root.",
      "systemPrompt": "You are generating an AGENTS.md file — a directory-level overview for AI coding assistants.\n\nCRITICAL: Output ONLY the raw markdown content. No code fences, no preamble, no explanations, no conversational text. Your entire response IS the AGENTS.md file content.\n\nOUTPUT FORMAT:\n- First line MUST be exactly: <!-- Generated by agents-reverse-engineer -->\n- Use a # heading with the directory name\n- Write a one-paragraph purpose statement for the directory\nusage\nADAPTIVE SECTIONS:\nAnalyze the directory contents and choose the most relevant sections. Do NOT use a fixed template. Instead, select sections that best document this specific directory for an AI that needs to reconstruct or extend the project.\n\nConsider these section types (choose what applies):\n- **Contents**: Group files by purpose/category under ## headings. For each file: markdown link [filename](./filename) and a one-line description.\n- **Subdirectories**: If subdirectories exist, list them with links [dirname/](./dirname/) and brief summaries.\n- **Architecture / Data Flow**: If files form a pipeline, request/response chain, or layered architecture, document it.\n- **Stack**: If this is a package root (has package.json, Cargo.toml, go.mod, etc.), document the technology stack, key scripts, and entry points.\n- **Structure**: If the directory layout follows a convention (feature-sliced, domain-driven, MVC, etc.), document it.\n- **Patterns**: If files share recurring design patterns (factory, strategy, middleware, barrel re-export), name and document them.\n- **Configuration**: If the directory contains config files, schemas, or environment definitions, document the config surface area.\n- **API Surface**: If the directory exports a public API (barrel index, route definitions, SDK), document the interface contract.\n- **File Relationships**: How files collaborate, depend on each other, or share state.\n- **Behavioral Contracts**: If files contain regex patterns, format specifications, magic constants, or template strings that define observable behavior, collect them in a dedicated section. Preserve verbatim patterns from file summaries — do NOT paraphrase regex into prose. This section is MANDATORY when file summaries contain behavioral artifacts.\n\nChoose any relevant sections or create your own based on the directory contents. The goal is to provide a comprehensive overview that captures the essence of the directory's role in the project and how its files work together, with a focus on what an AI coding assistant would need to know to effectively interact with this code.\n\nSCOPE:\n- AGENTS.md is a NAVIGATIONAL INDEX — help an AI find the right file quickly\n- Focus on: what each file does, how files relate, directory-level patterns\n- Do NOT reproduce full architecture sections — those belong in the root CLAUDE.md\n\nPATH ACCURACY (MANDATORY):\n- When referencing files or modules outside this directory, use ONLY paths from the \"Import Map\" section\n- Do NOT invent, rename, or guess module paths — if a path isn't in the Import Map, don't reference it\n- Use the exact directory names from \"Project Directory Structure\" — do NOT rename directories\n  (e.g., if the directory is called \"cli\", write \"src/cli/\", NOT \"src/commands/\")\n- Cross-module references must use the specifier format from actual import statements\n  (e.g., \"../generation/writers/sum.js\", NOT \"../fs/sum-file.js\")\n- If you are unsure about a path, omit the cross-reference rather than guessing\n\nCONSISTENCY (MANDATORY):\n- Do not contradict yourself within the same document\n- If you describe a technique (e.g., \"regex-based\"), do not call it something else later (e.g., \"AST-based\")\n- When stating version numbers, engines, or config fields, use ONLY values present in the file summaries\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this directory\", \"this module\", \"provides\", \"responsible for\", \"is used to\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Per-file descriptions in Contents sections: 1-2 sentences maximum. Reference key symbols but do not reproduce full summaries.\n- Behavioral contracts (regex patterns, format specs, constants) belong in a separate Behavioral Contracts section, not in per-file descriptions.\n- Subdirectory descriptions: 1-2 sentences maximum. Capture the directory's role, not its full contents.\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- Key exported symbols from file summaries MUST appear in the directory overview\n- Preserve exact casing of identifiers\n\nUSER NOTES:\n- If \"User Notes\" are provided in the prompt, they contain user-defined instructions that will be automatically prepended to your output\n- Do NOT repeat or paraphrase user notes in your generated content — they are included separately\n- You may reference information from user notes for context",
      "response": "<!-- Generated by agents-reverse-engineer -->\n\n# agents-reverse-engineer\n\nAI-driven codebase documentation generator executing Recursive Language Model (RLM) algorithm: concurrent per-file `.sum` analysis via subprocess pools, post-order directory `AGENTS.md` aggregation, and platform-specific root synthesis (`CLAUDE.md`, `GEMINI.md`, `OPENCODE.md`) with gitignore-aware discovery, SHA-256 incremental updates, and session lifecycle hooks.\n\n## Stack\n\n**Runtime:** Node.js ≥18.0.0, ES modules (`\"type\": \"module\"`)  \n**Language:** TypeScript 5.7.3 (ES2022 target, NodeNext module resolution, strict mode)  \n**Entry Points:** `are` and `agents-reverse-engineer` binaries → `dist/cli/index.js`  \n**Build:** `tsc` emits `src/` → `dist/` with source maps/declarations, `scripts/build-hooks.js` copies `hooks/` → `hooks/dist/`  \n**Package:** [package.json](./package.json) v0.6.5, MIT license, npm tarball includes `dist/`, `hooks/dist/`, `README.md`, `LICENSE`  \n**Config:** TypeScript via [tsconfig.json](./tsconfig.json), npm scripts for `build`/`dev`/`prepublishOnly`\n\n**Dependencies:**\n- `fast-glob` ^3.3.3 (file discovery)\n- `ignore` ^7.0.3 (gitignore parsing)\n- `isbinaryfile` ^5.0.4 (binary detection)\n- `simple-git` ^3.27.0 (change detection)\n- `yaml` ^2.7.0 (config parsing)\n- `zod` ^3.24.1 (schema validation)\n- `ora` ^8.1.1 (spinners)\n- `picocolors` ^1.1.1 (terminal colors)\n\n## Contents\n\n### Root Documentation\n\n**[README.md](./README.md)** — User-facing marketing page and CLI reference documenting npx-based interactive installation (`--runtime <rt>` + `-g`/`-l` flags), command surface (`init`, `discover`, `generate`, `update`, `clean`, `specify`), IDE integration patterns (`/are-generate` long-running with `run_in_background: true` + `TaskOutput` polling), three-phase pipeline (concurrent file analysis → post-order directory aggregation → sequential root synthesis), incremental update workflow (SHA-256 hash comparison via `.sum` frontmatter, orphan cleanup via `cleanupOrphans()`/`cleanupEmptyDirectoryDocs()`), subprocess resource management (heap/thread limits, process group killing, SIGTERM/SIGKILL timeout), configuration schema (`.agents-reverse-engineer/config.yaml` with `exclude`/`options`/`output`/`ai` sections), session hooks (Claude/Gemini/OpenCode lifecycle event handlers with detached subprocess spawning).\n\n**[LICENSE](./LICENSE)** — MIT License copyright 2026 GeoloeG-IsT grants unrestricted use/modification/distribution/sublicensing with warranty disclaimer, liability limitation, mandatory attribution preservation in derivative works.\n\n**[LANGUAGES-MANIFEST.md](./LANGUAGES-MANIFEST.md)** — Reference table mapping 26 programming language ecosystems to package manager manifest files, documenting which formats `src/generation/prompts/builder.ts` detects via `detectManifest()` for Phase 2 directory aggregation prompt inclusion (9 supported: `package.json`, `Cargo.toml`, `go.mod`, `pyproject.toml`, `pom.xml`, `build.gradle`, `Gemfile`, `composer.json`, `CMakeLists.txt`/`Makefile`), identifies coverage gaps for Ruby/Gradle/.NET/Swift/functional/scientific ecosystems.\n\n### Build & Distribution\n\n**[package.json](./package.json)** — npm package metadata declaring dual binaries (`are`, `agents-reverse-engineer`), ES module type, build scripts (`tsc` + `build-hooks.js`), runtime dependencies (fast-glob, ignore, isbinaryfile, simple-git, yaml, zod, ora, picocolors), Node.js ≥18.0.0 engine requirement, npm tarball whitelist (`dist`, `hooks/dist`, `README.md`, `LICENSE`).\n\n**[tsconfig.json](./tsconfig.json)** — TypeScript compiler configuration emitting ES2022 JavaScript from `src/` → `dist/` with NodeNext module resolution, strict type checking, source maps, declaration files, JSON module resolution (enables `package.json` imports in `src/generation/prompts/builder.ts`).\n\n**[.github/workflows/publish.yml](./.github/workflows/publish.yml)** — CI/CD workflow triggering on `release[published]` events, executing `npm ci` → `npm run build` → `npm publish --provenance --access public` with Sigstore-signed provenance attestation linking published artifact to source commit SHA.\n\n**[scripts/](./scripts/)** — Build automation: [build-hooks.js](./scripts/build-hooks.js) copies `.js` files from `hooks/` → `hooks/dist/` via synchronous `copyFileSync()`, invoked by `prepublishOnly` lifecycle hook before npm tarball creation.\n\n### Documentation Artifacts\n\n**[docs/](./docs/)** — [INPUT.md](./docs/INPUT.md) inception document defining RLM algorithm (post-order tree traversal: leaf `.sum` → directory `AGENTS.md` → root synthesis), proposed command interface (`/are-generate`, `/are-update`), session lifecycle integration requirements, research directives for GSD/BMAD methodology analysis.\n\n### Session Lifecycle Hooks\n\n**[hooks/](./hooks/)** — Detached subprocess-spawning session event handlers:\n- [are-check-update.js](./hooks/are-check-update.js) — Claude/Gemini SessionStart: `npm view agents-reverse-engineer version` comparison against `ARE-VERSION` sentinel file (project `.claude/ARE-VERSION` before global), writes cache to `~/.claude/cache/are-update-check.json`\n- [are-session-end.js](./hooks/are-session-end.js) — Claude/Gemini SessionEnd: `git status --porcelain` change detection → `npx agents-reverse-engineer@latest update --quiet` detached spawn if uncommitted changes present, exits silently if `ARE_DISABLE_HOOK=1` or config contains `hook_enabled: false`\n- [opencode-are-check-update.js](./hooks/opencode-are-check-update.js) — OpenCode plugin factory exporting `AreCheckUpdate()` → `event['session.created']` handler with identical npm version check logic\n- [opencode-are-session-end.js](./hooks/opencode-are-session-end.js) — OpenCode plugin factory exporting `AreSessionEnd()` → `event['session.deleted']` handler with identical git status check + update spawn\n\n### TypeScript Source Tree\n\n**[src/](./src/)** — Core implementation modules:\n\n**[src/version.ts](./src/version.ts)** — `getVersion()` reads `package.json` from compiled module parent directory via `fileURLToPath(import.meta.url)` → `dirname()` → `join(__dirname, '..', 'package.json')`, returns `version` field or `'unknown'` on error, called by CLI help text and session hooks.\n\n**[src/cli/](./src/cli/)** — Command entry points: [index.ts](./src/cli/index.ts) parses `process.argv` via `parseArgs()` with short flag expansion, dual routing (installer vs. command dispatch), [generate.ts](./src/cli/generate.ts) orchestrates three-phase pipeline with backend resolution, [update.ts](./src/cli/update.ts) executes hash-based incremental regeneration, [clean.ts](./src/cli/clean.ts) deletes artifacts with `GENERATED_MARKER` filtering, [discover.ts](./src/cli/discover.ts) builds `GENERATION-PLAN.md`, [init.ts](./src/cli/init.ts) writes config YAML, [specify.ts](./src/cli/specify.ts) synthesizes project specs from `AGENTS.md` corpus.\n\n**[src/ai/](./src/ai/)** — Backend-agnostic AI service layer: [subprocess.ts](./src/ai/subprocess.ts) spawns `execFile()` child processes with resource limits (`NODE_OPTIONS='--max-old-space-size=512'`, `UV_THREADPOOL_SIZE='4'`, `CLAUDE_CODE_DISABLE_BACKGROUND_TASKS='1'`, `--disallowedTools Task`), process group killing `kill(-pid)`, SIGTERM/SIGKILL timeout escalation; [retry.ts](./src/ai/retry.ts) implements exponential backoff detecting rate limits via stderr patterns; [service.ts](./src/ai/service.ts) orchestrates `call()` → `retry()` → `runSubprocess()` → backend adapter; [telemetry/](./src/ai/telemetry/) writes NDJSON logs to `.agents-reverse-engineer/logs/run-<timestamp>.json` with token/cost tracking, enforces 50-run retention; [backends/](./src/ai/backends/) implements ClaudeBackend (Zod JSON parsing), GeminiBackend/OpenCodeBackend stubs throwing `SUBPROCESS_ERROR`.\n\n**[src/discovery/](./src/discovery/)** — File discovery pipeline: [walker.ts](./src/discovery/walker.ts) chains four filters (GitignoreFilter → VendorFilter → BinaryFilter → CustomPatternFilter) via `applyFilters()` with 30-worker concurrency, [run.ts](./src/discovery/run.ts) executes `walkDirectory()` via `fast-glob` with `{absolute: true, onlyFiles: true, ignore: ['**/.git/**']}`, returns `DiscoveryResult` with `included[]`/`excluded[]` attribution and `stats.exclusionReasons` histogram.\n\n**[src/generation/](./src/generation/)** — Three-phase orchestration: [orchestrator.ts](./src/generation/orchestrator.ts) `GenerationOrchestrator.createPlan()` executes file discovery → `analyzeComplexity()` → `buildFilePrompt()` per file → post-order directory traversal (depth-sorted) → root synthesis; [prompts/builder.ts](./src/generation/prompts/builder.ts) constructs prompts consuming `.sum` frontmatter, child `AGENTS.md`, import maps via `extractDirectoryImports()`, detects 9 manifest types; [writers/](./src/generation/writers/) serializes YAML frontmatter with adaptive array formatting, preserves user content via `AGENTS.local.md` rename pattern.\n\n**[src/orchestration/](./src/orchestration/)** — Worker pool and progress tracking: [pool.ts](./src/orchestration/pool.ts) iterator-based pool shares single `tasks.entries()` across N workers, emits `worker:start/end` and `task:pickup/done` trace events; [runner.ts](./src/orchestration/runner.ts) `CommandRunner.executeGenerate()` runs concurrent Phase 1, post-order Phase 2, sequential Phase 3 with quality validators; [progress.ts](./src/orchestration/progress.ts) streams colored console output with ETA via moving average (last 10 completion times), mirrors to `.agents-reverse-engineer/progress.log` via promise-chain serialized writes; [trace.ts](./src/orchestration/trace.ts) appends NDJSON to `.agents-reverse-engineer/traces/trace-<timestamp>.ndjson` with auto-populated `seq`/`ts`/`pid`/`elapsedMs` base fields, enforces 500-file retention; [plan-tracker.ts](./src/orchestration/plan-tracker.ts) serializes concurrent checkbox updates (`- [ ] → - [x]`) for `GENERATION-PLAN.md`.\n\n**[src/change-detection/](./src/change-detection/)** — Git-based incremental updates: [detector.ts](./src/change-detection/detector.ts) parses `git diff --name-status -M` with rename detection, merges uncommitted working tree changes via `status.modified[]`/`deleted[]`, computes SHA-256 content hashes, returns `ChangeDetectionResult` with `FileChange[]` array mapping `ChangeType` status codes ('added'/'modified'/'deleted'/'renamed').\n\n**[src/update/](./src/update/)** — Incremental documentation synchronization: [orchestrator.ts](./src/update/orchestrator.ts) `UpdateOrchestrator.preparePlan()` executes hash-based change detection via `readSumFile()` YAML frontmatter extraction → `computeContentHash()` SHA-256 comparison → produces `UpdatePlan` with `filesToAnalyze[]`/`filesToSkip[]`/`cleanup`/`affectedDirs[]` (depth-sorted); [orphan-cleaner.ts](./src/update/orphan-cleaner.ts) deletes `.sum` files for `FileChange` entries with `status === 'deleted'` or `status === 'renamed'`, removes `AGENTS.md` from directories containing only hidden files/`.sum` files/generated artifacts.\n\n**[src/quality/](./src/quality/)** — Code-documentation consistency validation: [inconsistency/code-vs-doc.ts](./src/quality/inconsistency/code-vs-doc.ts) extracts exports via `/^[ \\t]*export\\s+(?:default\\s+)?(?:function|class|const|let|var|type|interface|enum)\\s+(\\w+)/gm` with substring verification against `.sum` content; [inconsistency/code-vs-code.ts](./src/quality/inconsistency/code-vs-code.ts) aggregates exports into `Map<symbol, paths[]>` for duplicate detection; [phantom-paths/validator.ts](./src/quality/phantom-paths/validator.ts) extracts path references via three regex patterns (markdown links, backtick paths, prose references) resolving against `AGENTS.md` directory and project root with `.ts`/`.js` fallback; [inconsistency/reporter.ts](./src/quality/inconsistency/reporter.ts) merges issues with summary counts by type/severity.\n\n**[src/config/](./src/config/)** — Configuration management: [loader.ts](./src/config/loader.ts) `loadConfig()` reads `.agents-reverse-engineer/config.yaml` via YAML parsing and `ConfigSchema.parse()` Zod validation (returns defaults on ENOENT, throws `ConfigError` on ZodError), `writeDefaultConfig()` generates commented template with inline defaults, `getDefaultConcurrency()` computes adaptive worker pool size via `clamp(cores * 5, 2, min(memCap, 20))` where `memCap = floor((totalMemGB * 0.5) / 0.512)`.\n\n**[src/imports/](./src/imports/)** — Static import analysis: [extractor.ts](./src/imports/extractor.ts) `extractImports()` applies `IMPORT_REGEX` (`/^import\\s+(type\\s+)?(?:\\{([^}]*)\\}|(\\*\\s+as\\s+\\w+)|(\\w+))\\s+from\\s+['\"]([^'\"]+)['\"]/gm`) extracting ES module syntax into `ImportEntry[]`, `extractDirectoryImports()` scans first 100 lines per file, partitions relative imports into `internal`/`external`, filters bare specifiers.\n\n**[src/installer/](./src/installer/)** — npx installation orchestrator: [index.ts](./src/installer/index.ts) `runInstall()` parses CLI args (`-g`/`-l`/`--runtime`/`--force`), interactive prompts via [prompts.ts](./src/installer/prompts.ts) `selectRuntime()`/`selectLocation()`, [operations.ts](./src/installer/operations.ts) `installFilesForRuntime()` writes command templates from `src/integration/templates.ts` and copies bundled hooks from `hooks/dist/`, registers hooks in `settings.json` via `registerClaudeHooks()`/`registerGeminiHooks()`, writes `ARE-VERSION` file, adds Bash permission patterns to Claude Code allow list; [uninstall.ts](./src/installer/uninstall.ts) removes files, hook registrations, permissions, cleans empty skill directories.\n\n**[src/integration/](./src/integration/)** — Platform-specific AI assistant integration: [detect.ts](./src/integration/detect.ts) `detectEnvironments()` scans for Claude (`.claude/` or `CLAUDE.md`), OpenCode (`.opencode/`), Aider (`.aider.conf.yml` or `.aider/`) via `existsSync()`; [templates.ts](./src/integration/templates.ts) `getTemplatesForRuntime()` generates command files with placeholder substitution for `COMMAND_PREFIX`, `VERSION_FILE_PATH`, `$ARGUMENTS`, embeds background execution pattern with `run_in_background: true` + `TaskOutput` polling + `.agents-reverse-engineer/progress.log` monitoring; [generate.ts](./src/integration/generate.ts) `generateIntegrationFiles()` writes templates via `ensureDir()` → `writeFileSync()` chain, copies bundled hooks for Claude.\n\n**[src/specify/](./src/specify/)** — Specification synthesis: [prompts.ts](./src/specify/prompts.ts) `buildSpecPrompt()` constructs system/user prompts enforcing nine-section structure (Overview, Architecture, API Surface, Data Structures, Configuration, Dependencies, Behavioral Contracts, Test Contracts, Build Plan) with verbatim regex/format/constant preservation; [writer.ts](./src/specify/writer.ts) `writeSpec()` writes single-file or multi-file specifications via heading-based splitting (`splitByHeadings()` partitions on `/^# /m`, `slugify()` sanitizes heading text), throws `SpecExistsError` when targets exist and `force: false`.\n\n**[src/output/](./src/output/)** — Terminal output formatting: [logger.ts](./src/output/logger.ts) `createLogger()` factory with picocolors-based ANSI formatting when `options.colors` is true, `createSilentLogger()` no-op for tests, `Logger` interface with six methods (`info()`, `file()`, `excluded()`, `summary()`, `warn()`, `error()`), output routing (log/warn/error → stdout/stderr).\n\n**[src/types/](./src/types/)** — Shared interfaces: [index.ts](./src/types/index.ts) defines `ExcludedFile`, `DiscoveryResult`, `DiscoveryStats` with `exclusionReasons` histogram, consumed by `src/discovery/walker.ts`, `src/cli/discover.ts`, `src/orchestration/runner.ts`.\n\n## Architecture\n\n### Three-Phase RLM Pipeline\n\n**Phase 1: File Analysis** — Iterator-based worker pool (`src/orchestration/pool.ts`) shares single `tasks.entries()` iterator across N workers (default 2 for WSL, 5 elsewhere) invoking `AIService.call()` → `runSubprocess()` → `execFile()` spawning AI CLI subprocesses with resource limits (heap 512MB, thread pool 4, background tasks disabled, Task tool disallowed). Process group killing (`kill(-pid)`) terminates subprocess trees on timeout (SIGTERM at `timeoutMs`, SIGKILL after 5s). Exponential backoff retry on rate limits (stderr patterns: \"rate limit\", \"429\", \"too many requests\", \"overloaded\"). Writes `.sum` files with YAML frontmatter (`generated_at`, `content_hash`, `purpose`, `critical_todos`, `related_files`) + markdown summary body.\n\n**Phase 2: Directory Aggregation** — Post-order traversal sorts directories by depth descending (`path.relative().split(path.sep).length`), waits for all child `.sum` files via `isDirectoryComplete()` predicate. `buildDirectoryPrompt()` aggregates child `.sum` content via `readSumFile()`, subdirectory `AGENTS.md` files via recursive traversal, import maps via `extractDirectoryImports()` with verified path constraints, manifest detection (9 types: `package.json`, `Cargo.toml`, `go.mod`, `pyproject.toml`, `pom.xml`, `build.gradle`, `Gemfile`, `composer.json`, `CMakeLists.txt`/`Makefile`). User-authored `AGENTS.md` files renamed to `AGENTS.local.md` and prepended above generated content. Writes `AGENTS.md` with `<!-- Generated by agents-reverse-engineer -->` marker.\n\n**Phase 3: Root Document Synthesis** — Sequential execution (concurrency=1) generates `CLAUDE.md`, `GEMINI.md`, `OPENCODE.md`. Prompts consume all `AGENTS.md` files via `collectAgentsDocs()` recursive tree traversal, parse root `package.json` for project metadata, enforce synthesis-only constraints (no invention of features/hooks/patterns not in source documents). Strips conversational preamble via pattern matching before writing output.\n\n### Incremental Update Strategy\n\n`UpdateOrchestrator.preparePlan()` workflow: (1) read `content_hash` from `.sum` YAML frontmatter via `readSumFile()`, (2) compute current SHA-256 via `computeContentHash()`, (3) hash mismatch → add to `filesToAnalyze` as `FileChange` with `status: 'modified'`/`'added'`, (4) hash match → add to `filesToSkip`, (5) detect orphans: `.sum` files for deleted source files or renamed oldPaths, (6) call `cleanupOrphans()` to delete stale `.sum` files, (7) call `cleanupEmptyDirectoryDocs()` to remove `AGENTS.md` from directories with no remaining sources, (8) compute `affectedDirs` via `getAffectedDirectories()` by walking parent directories of changed files, (9) regenerate `.sum` for `filesToAnalyze` via Phase 1 pool execution, (10) regenerate `AGENTS.md` for `affectedDirs` sequentially (no Phase 2 post-order traversal required). Git integration: supports committed changes (`git diff <baseCommit>..HEAD`) and uncommitted (`git status --porcelain` merge via `--uncommitted` flag), rename detection via `git diff -M` (50% similarity threshold), fallback to SHA-256 hashing for non-git workflows.\n\n### Subprocess Resource Management\n\nMitigations for Claude CLI excessive thread spawning (GitHub issue #5771: 200 NodeJS instances reported): `NODE_OPTIONS='--max-old-space-size=512'` limits heap to 512MB per subprocess, `UV_THREADPOOL_SIZE='4'` constrains libuv thread pool to 4 threads, `CLAUDE_CODE_DISABLE_BACKGROUND_TASKS='1'` prevents background task spawning, `--disallowedTools Task` blocks subagents. Process group killing: `kill(-pid)` terminates entire subprocess tree. Timeout enforcement: SIGTERM sent at `timeoutMs`, SIGKILL escalation after 5s grace period. Default concurrency reduced from 5 → 2 for resource-constrained environments.\n\n### Telemetry & Tracing\n\n**Run logs** (`.agents-reverse-engineer/logs/run-<timestamp>.json`) aggregate per-call token counts, costs, durations, errors, track `filesRead[]` metadata with `path`/`sizeBytes`/`linesRead`, compute summary (`totalInputTokens`, `totalCacheReadTokens`, `errorCount`, `uniqueFilesRead`), enforce retention via `cleanupOldLogs(keepCount)` after each run.\n\n**Trace events** (`.agents-reverse-engineer/traces/trace-<timestamp>.ndjson`) enabled via `--trace` flag emit `phase:start/end`, `worker:start/end`, `task:pickup/done`, `subprocess:spawn/exit`, `retry` events with auto-populated fields (`seq`, `ts`, `pid`, `elapsedMs`), promise-chain serialization ensures NDJSON line order matches emission order despite concurrent workers, retention keeps 500 most recent traces via `cleanupOldTraces(keepCount)`.\n\n**Progress log** (`.agents-reverse-engineer/progress.log`) human-readable streaming output mirroring console, ETA calculation via moving average of last 10 task durations, quality metrics (code-vs-doc/code-vs-code inconsistencies, phantom path counts), real-time monitoring via `tail -f .agents-reverse-engineer/progress.log`.\n\n### Quality Validation\n\n**Code-vs-Doc Consistency** — Extracts exported symbols via regex `/^[ \\t]*export\\s+(?:default\\s+)?(?:function|class|const|let|var|type|interface|enum)\\s+(\\w+)/gm`, verifies all exports appear in `.sum` summary text via substring search, reports `CodeDocInconsistency` with `missingFromDoc` arrays.\n\n**Code-vs-Code Duplicate Detection** — Aggregates exports across per-directory file groups into `Map<symbol, string[]>`, reports `CodeCodeInconsistency` for symbols appearing in multiple files (pattern: `'duplicate-export'`).\n\n**Phantom Path Resolution** — Extracts path-like strings from `AGENTS.md` via three regex patterns (markdown link targets `/\\[(?:[^\\]]*)\\]\\((\\.[^)]+)\\)/g`, backtick-quoted paths `` /`((?:src\\/|\\.\\.?\\/)[^`]+\\.[a-z]{1,4})`/g ``, prose-embedded paths `/(?:from|in|by|via|see)\\s+`?(src\\/[\\w\\-./]+)`?/gi`), resolves against `AGENTS.md` directory and project root with `.ts`/`.js` fallback, reports `PhantomPathInconsistency` for unresolved references.\n\n## Behavioral Contracts\n\n**GENERATED_MARKER:** `<!-- Generated by agents-reverse-engineer -->`\n\n**Frontmatter extraction regex:** `/^---\\n([\\s\\S]*?)\\n---\\n/`\n\n**YAML field patterns:**\n```javascript\n/generated_at:\\s*(.+)/\n/content_hash:\\s*(.+)/\n/purpose:\\s*(.+)/\n/key:\\s*\\[([^\\]]*)\\]/  // Inline arrays\n/key:\\s*\\n((?:\\s+-\\s+.+\\n?)+)/m  // Multi-line arrays\n```\n\n**Array formatting heuristic:** Inline if `length <= 3` and all items `< 40` chars, multi-line otherwise.\n\n**Export extraction regex:** `/^[ \\t]*export\\s+(?:default\\s+)?(?:function|class|const|let|var|type|interface|enum)\\s+(\\w+)/gm`\n\n**Path extraction patterns (phantom-paths):**\n```javascript\n/\\[(?:[^\\]]*)\\]\\((\\.[^)]+)\\)/g              // Markdown: [text](./path)\n/`((?:src\\/|\\.\\.?\\/)[^`]+\\.[a-z]{1,4})`/g  // Backtick: `src/foo.ts`\n/(?:from|in|by|via|see)\\s+`?(src\\/[\\w\\-./]+)`?/gi // Prose: \"from src/foo/\"\n```\n\n**Rate limit detection patterns:** `['rate limit', '429', 'too many requests', 'overloaded'].some(p => stderr.toLowerCase().includes(p))`\n\n**Preamble stripping patterns:**\n1. YAML separator: `/\\n---\\n/` within first 500 chars\n2. Bold uppercase start: `/^[\\s\\S]{0,500}?(\\*\\*[A-Z])/` matches conversational preamble, strips if <300 chars and no `##` headers\n\n**Skip directories:** `Set(['node_modules', '.git', '.agents-reverse-engineer', 'vendor', 'dist', 'build', '__pycache__', '.next', 'venv', '.venv', 'target', '.cargo', '.gradle'])`\n\n**Manifest detection:** `['package.json', 'Cargo.toml', 'go.mod', 'pyproject.toml', 'pom.xml', 'build.gradle', 'Gemfile', 'composer.json', 'CMakeLists.txt', 'Makefile']`\n\n**Git commands:**\n- Change detection: `git diff --name-status -M <baseCommit>..HEAD` (rename detection 50% threshold)\n- Uncommitted changes: `git status --porcelain` merged with `status.modified[]`/`deleted[]`/`not_added[]`/`staged[]`\n- SHA-256 content hash fallback for non-git workflows\n\n**Detached subprocess spawn pattern:**\n```javascript\nspawn(process.execPath, ['-e', scriptString], { \n  stdio: 'ignore', \n  detached: true, \n  windowsHide: true \n}).unref()\n```\n\n**Hook disable mechanisms:**\n- Environment variable: `ARE_DISABLE_HOOK=1`\n- Config substring: `readFileSync('.agents-reverse-engineer.yaml', 'utf-8').includes('hook_enabled: false')`",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 7195,
      "cacheReadTokens": 16334,
      "cacheCreationTokens": 23157,
      "latencyMs": 159990,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": []
    },
    {
      "timestamp": "2026-02-09T16:20:35.404Z",
      "prompt": "Generate CLAUDE.md for the project root.\n\nSynthesize the following AGENTS.md files into a single comprehensive project overview document.\nUse ONLY the information provided below. Do NOT invent features, hooks, patterns, or APIs that are not explicitly mentioned in the AGENTS.md content.\n\n## AGENTS.md Files (29 directories)\n\n### .github/workflows/AGENTS.md\n\n<!-- Generated by agents-reverse-engineer -->\n\n# .github/workflows\n\nGitHub Actions CI/CD automation for npm package publication with Sigstore-signed provenance attestation.\n\n## Contents\n\n### [publish.yml](./publish.yml)\nWorkflow automating npm publication on `release[published]` events or manual `workflow_dispatch`. Executes `actions/checkout@v4`, `actions/setup-node@v4` (Node.js 20, registry `https://registry.npmjs.org`), `npm ci`, `npm run build` (invokes `prepublishOnly`: TypeScript compilation + `build:hooks` script from `scripts/build-hooks.js`), and `npm publish --provenance --access public` with `NODE_AUTH_TOKEN` from `secrets.NPM_TOKEN`. Grants `id-token: write` permission enabling Sigstore keyless signing to generate SLSA attestation linking published tarball to source commit SHA.\n\n## CI/CD Pipeline Architecture\n\n**Trigger Strategy:**  \nExecutes on GitHub release publication or manual dispatch, ensuring controlled release cadence.\n\n**Build Steps:**  \nSequential execution: repository checkout at release commit SHA → Node.js 20 installation with npm registry configuration → clean dependency install via `npm ci` → TypeScript compilation and hook file preparation via `npm run build` → authenticated publication with cryptographic attestation.\n\n**Security Model:**  \nOIDC token-based authentication (`id-token: write`) enables Sigstore keyless signing without long-lived credentials. Generated provenance attestation creates verifiable supply chain metadata recording workflow execution context (commit SHA, repository coordinates, workflow file path) consumable via `npm audit signatures` or `sigstore-js` tooling.\n\n## Integration Points\n\n**Build System:**  \nDepends on `prepublishOnly` script in root `package.json` executing TypeScript compiler (`tsc`) and hook file copier (`scripts/build-hooks.js`). Outputs `dist/` directory tree and `hooks/dist/` session lifecycle hook bundles included in npm tarball.\n\n**Publication Authentication:**  \nRequires `NPM_TOKEN` repository secret storing npm automation token with publish permissions for package scope. Token injected as `NODE_AUTH_TOKEN` environment variable during `npm publish` execution.\n\n**Provenance Attestation:**  \n`--provenance` flag triggers GitHub Actions attestation generation via `@actions/attest-build-provenance` internal API, uploading signed SLSA v0.2 attestation to npm registry metadata storage. Consumers verify authenticity by inspecting `attestations` field in registry metadata or using `npm audit signatures` command.\n### AGENTS.md\n\n<!-- Generated by agents-reverse-engineer -->\n\n# agents-reverse-engineer\n\nAI-driven codebase documentation generator executing Recursive Language Model (RLM) algorithm: concurrent per-file `.sum` analysis via subprocess pools, post-order directory `AGENTS.md` aggregation, and platform-specific root synthesis (`CLAUDE.md`, `GEMINI.md`, `OPENCODE.md`) with gitignore-aware discovery, SHA-256 incremental updates, and session lifecycle hooks.\n\n## Stack\n\n**Runtime:** Node.js ≥18.0.0, ES modules (`\"type\": \"module\"`)  \n**Language:** TypeScript 5.7.3 (ES2022 target, NodeNext module resolution, strict mode)  \n**Entry Points:** `are` and `agents-reverse-engineer` binaries → `dist/cli/index.js`  \n**Build:** `tsc` emits `src/` → `dist/` with source maps/declarations, `scripts/build-hooks.js` copies `hooks/` → `hooks/dist/`  \n**Package:** [package.json](./package.json) v0.6.5, MIT license, npm tarball includes `dist/`, `hooks/dist/`, `README.md`, `LICENSE`  \n**Config:** TypeScript via [tsconfig.json](./tsconfig.json), npm scripts for `build`/`dev`/`prepublishOnly`\n\n**Dependencies:**\n- `fast-glob` ^3.3.3 (file discovery)\n- `ignore` ^7.0.3 (gitignore parsing)\n- `isbinaryfile` ^5.0.4 (binary detection)\n- `simple-git` ^3.27.0 (change detection)\n- `yaml` ^2.7.0 (config parsing)\n- `zod` ^3.24.1 (schema validation)\n- `ora` ^8.1.1 (spinners)\n- `picocolors` ^1.1.1 (terminal colors)\n\n## Contents\n\n### Root Documentation\n\n**[README.md](./README.md)** — User-facing marketing page and CLI reference documenting npx-based interactive installation (`--runtime <rt>` + `-g`/`-l` flags), command surface (`init`, `discover`, `generate`, `update`, `clean`, `specify`), IDE integration patterns (`/are-generate` long-running with `run_in_background: true` + `TaskOutput` polling), three-phase pipeline (concurrent file analysis → post-order directory aggregation → sequential root synthesis), incremental update workflow (SHA-256 hash comparison via `.sum` frontmatter, orphan cleanup via `cleanupOrphans()`/`cleanupEmptyDirectoryDocs()`), subprocess resource management (heap/thread limits, process group killing, SIGTERM/SIGKILL timeout), configuration schema (`.agents-reverse-engineer/config.yaml` with `exclude`/`options`/`output`/`ai` sections), session hooks (Claude/Gemini/OpenCode lifecycle event handlers with detached subprocess spawning).\n\n**[LICENSE](./LICENSE)** — MIT License copyright 2026 GeoloeG-IsT grants unrestricted use/modification/distribution/sublicensing with warranty disclaimer, liability limitation, mandatory attribution preservation in derivative works.\n\n**[LANGUAGES-MANIFEST.md](./LANGUAGES-MANIFEST.md)** — Reference table mapping 26 programming language ecosystems to package manager manifest files, documenting which formats `src/generation/prompts/builder.ts` detects via `detectManifest()` for Phase 2 directory aggregation prompt inclusion (9 supported: `package.json`, `Cargo.toml`, `go.mod`, `pyproject.toml`, `pom.xml`, `build.gradle`, `Gemfile`, `composer.json`, `CMakeLists.txt`/`Makefile`), identifies coverage gaps for Ruby/Gradle/.NET/Swift/functional/scientific ecosystems.\n\n### Build & Distribution\n\n**[package.json](./package.json)** — npm package metadata declaring dual binaries (`are`, `agents-reverse-engineer`), ES module type, build scripts (`tsc` + `build-hooks.js`), runtime dependencies (fast-glob, ignore, isbinaryfile, simple-git, yaml, zod, ora, picocolors), Node.js ≥18.0.0 engine requirement, npm tarball whitelist (`dist`, `hooks/dist`, `README.md`, `LICENSE`).\n\n**[tsconfig.json](./tsconfig.json)** — TypeScript compiler configuration emitting ES2022 JavaScript from `src/` → `dist/` with NodeNext module resolution, strict type checking, source maps, declaration files, JSON module resolution (enables `package.json` imports in `src/generation/prompts/builder.ts`).\n\n**[.github/workflows/publish.yml](./.github/workflows/publish.yml)** — CI/CD workflow triggering on `release[published]` events, executing `npm ci` → `npm run build` → `npm publish --provenance --access public` with Sigstore-signed provenance attestation linking published artifact to source commit SHA.\n\n**[scripts/](./scripts/)** — Build automation: [build-hooks.js](./scripts/build-hooks.js) copies `.js` files from `hooks/` → `hooks/dist/` via synchronous `copyFileSync()`, invoked by `prepublishOnly` lifecycle hook before npm tarball creation.\n\n### Documentation Artifacts\n\n**[docs/](./docs/)** — [INPUT.md](./docs/INPUT.md) inception document defining RLM algorithm (post-order tree traversal: leaf `.sum` → directory `AGENTS.md` → root synthesis), proposed command interface (`/are-generate`, `/are-update`), session lifecycle integration requirements, research directives for GSD/BMAD methodology analysis.\n\n### Session Lifecycle Hooks\n\n**[hooks/](./hooks/)** — Detached subprocess-spawning session event handlers:\n- [are-check-update.js](./hooks/are-check-update.js) — Claude/Gemini SessionStart: `npm view agents-reverse-engineer version` comparison against `ARE-VERSION` sentinel file (project `.claude/ARE-VERSION` before global), writes cache to `~/.claude/cache/are-update-check.json`\n- [are-session-end.js](./hooks/are-session-end.js) — Claude/Gemini SessionEnd: `git status --porcelain` change detection → `npx agents-reverse-engineer@latest update --quiet` detached spawn if uncommitted changes present, exits silently if `ARE_DISABLE_HOOK=1` or config contains `hook_enabled: false`\n- [opencode-are-check-update.js](./hooks/opencode-are-check-update.js) — OpenCode plugin factory exporting `AreCheckUpdate()` → `event['session.created']` handler with identical npm version check logic\n- [opencode-are-session-end.js](./hooks/opencode-are-session-end.js) — OpenCode plugin factory exporting `AreSessionEnd()` → `event['session.deleted']` handler with identical git status check + update spawn\n\n### TypeScript Source Tree\n\n**[src/](./src/)** — Core implementation modules:\n\n**[src/version.ts](./src/version.ts)** — `getVersion()` reads `package.json` from compiled module parent directory via `fileURLToPath(import.meta.url)` → `dirname()` → `join(__dirname, '..', 'package.json')`, returns `version` field or `'unknown'` on error, called by CLI help text and session hooks.\n\n**[src/cli/](./src/cli/)** — Command entry points: [index.ts](./src/cli/index.ts) parses `process.argv` via `parseArgs()` with short flag expansion, dual routing (installer vs. command dispatch), [generate.ts](./src/cli/generate.ts) orchestrates three-phase pipeline with backend resolution, [update.ts](./src/cli/update.ts) executes hash-based incremental regeneration, [clean.ts](./src/cli/clean.ts) deletes artifacts with `GENERATED_MARKER` filtering, [discover.ts](./src/cli/discover.ts) builds `GENERATION-PLAN.md`, [init.ts](./src/cli/init.ts) writes config YAML, [specify.ts](./src/cli/specify.ts) synthesizes project specs from `AGENTS.md` corpus.\n\n**[src/ai/](./src/ai/)** — Backend-agnostic AI service layer: [subprocess.ts](./src/ai/subprocess.ts) spawns `execFile()` child processes with resource limits (`NODE_OPTIONS='--max-old-space-size=512'`, `UV_THREADPOOL_SIZE='4'`, `CLAUDE_CODE_DISABLE_BACKGROUND_TASKS='1'`, `--disallowedTools Task`), process group killing `kill(-pid)`, SIGTERM/SIGKILL timeout escalation; [retry.ts](./src/ai/retry.ts) implements exponential backoff detecting rate limits via stderr patterns; [service.ts](./src/ai/service.ts) orchestrates `call()` → `retry()` → `runSubprocess()` → backend adapter; [telemetry/](./src/ai/telemetry/) writes NDJSON logs to `.agents-reverse-engineer/logs/run-<timestamp>.json` with token/cost tracking, enforces 50-run retention; [backends/](./src/ai/backends/) implements ClaudeBackend (Zod JSON parsing), GeminiBackend/OpenCodeBackend stubs throwing `SUBPROCESS_ERROR`.\n\n**[src/discovery/](./src/discovery/)** — File discovery pipeline: [walker.ts](./src/discovery/walker.ts) chains four filters (GitignoreFilter → VendorFilter → BinaryFilter → CustomPatternFilter) via `applyFilters()` with 30-worker concurrency, [run.ts](./src/discovery/run.ts) executes `walkDirectory()` via `fast-glob` with `{absolute: true, onlyFiles: true, ignore: ['**/.git/**']}`, returns `DiscoveryResult` with `included[]`/`excluded[]` attribution and `stats.exclusionReasons` histogram.\n\n**[src/generation/](./src/generation/)** — Three-phase orchestration: [orchestrator.ts](./src/generation/orchestrator.ts) `GenerationOrchestrator.createPlan()` executes file discovery → `analyzeComplexity()` → `buildFilePrompt()` per file → post-order directory traversal (depth-sorted) → root synthesis; [prompts/builder.ts](./src/generation/prompts/builder.ts) constructs prompts consuming `.sum` frontmatter, child `AGENTS.md`, import maps via `extractDirectoryImports()`, detects 9 manifest types; [writers/](./src/generation/writers/) serializes YAML frontmatter with adaptive array formatting, preserves user content via `AGENTS.local.md` rename pattern.\n\n**[src/orchestration/](./src/orchestration/)** — Worker pool and progress tracking: [pool.ts](./src/orchestration/pool.ts) iterator-based pool shares single `tasks.entries()` across N workers, emits `worker:start/end` and `task:pickup/done` trace events; [runner.ts](./src/orchestration/runner.ts) `CommandRunner.executeGenerate()` runs concurrent Phase 1, post-order Phase 2, sequential Phase 3 with quality validators; [progress.ts](./src/orchestration/progress.ts) streams colored console output with ETA via moving average (last 10 completion times), mirrors to `.agents-reverse-engineer/progress.log` via promise-chain serialized writes; [trace.ts](./src/orchestration/trace.ts) appends NDJSON to `.agents-reverse-engineer/traces/trace-<timestamp>.ndjson` with auto-populated `seq`/`ts`/`pid`/`elapsedMs` base fields, enforces 500-file retention; [plan-tracker.ts](./src/orchestration/plan-tracker.ts) serializes concurrent checkbox updates (`- [ ] → - [x]`) for `GENERATION-PLAN.md`.\n\n**[src/change-detection/](./src/change-detection/)** — Git-based incremental updates: [detector.ts](./src/change-detection/detector.ts) parses `git diff --name-status -M` with rename detection, merges uncommitted working tree changes via `status.modified[]`/`deleted[]`, computes SHA-256 content hashes, returns `ChangeDetectionResult` with `FileChange[]` array mapping `ChangeType` status codes ('added'/'modified'/'deleted'/'renamed').\n\n**[src/update/](./src/update/)** — Incremental documentation synchronization: [orchestrator.ts](./src/update/orchestrator.ts) `UpdateOrchestrator.preparePlan()` executes hash-based change detection via `readSumFile()` YAML frontmatter extraction → `computeContentHash()` SHA-256 comparison → produces `UpdatePlan` with `filesToAnalyze[]`/`filesToSkip[]`/`cleanup`/`affectedDirs[]` (depth-sorted); [orphan-cleaner.ts](./src/update/orphan-cleaner.ts) deletes `.sum` files for `FileChange` entries with `status === 'deleted'` or `status === 'renamed'`, removes `AGENTS.md` from directories containing only hidden files/`.sum` files/generated artifacts.\n\n**[src/quality/](./src/quality/)** — Code-documentation consistency validation: [inconsistency/code-vs-doc.ts](./src/quality/inconsistency/code-vs-doc.ts) extracts exports via `/^[ \\t]*export\\s+(?:default\\s+)?(?:function|class|const|let|var|type|interface|enum)\\s+(\\w+)/gm` with substring verification against `.sum` content; [inconsistency/code-vs-code.ts](./src/quality/inconsistency/code-vs-code.ts) aggregates exports into `Map<symbol, paths[]>` for duplicate detection; [phantom-paths/validator.ts](./src/quality/phantom-paths/validator.ts) extracts path references via three regex patterns (markdown links, backtick paths, prose references) resolving against `AGENTS.md` directory and project root with `.ts`/`.js` fallback; [inconsistency/reporter.ts](./src/quality/inconsistency/reporter.ts) merges issues with summary counts by type/severity.\n\n**[src/config/](./src/config/)** — Configuration management: [loader.ts](./src/config/loader.ts) `loadConfig()` reads `.agents-reverse-engineer/config.yaml` via YAML parsing and `ConfigSchema.parse()` Zod validation (returns defaults on ENOENT, throws `ConfigError` on ZodError), `writeDefaultConfig()` generates commented template with inline defaults, `getDefaultConcurrency()` computes adaptive worker pool size via `clamp(cores * 5, 2, min(memCap, 20))` where `memCap = floor((totalMemGB * 0.5) / 0.512)`.\n\n**[src/imports/](./src/imports/)** — Static import analysis: [extractor.ts](./src/imports/extractor.ts) `extractImports()` applies `IMPORT_REGEX` (`/^import\\s+(type\\s+)?(?:\\{([^}]*)\\}|(\\*\\s+as\\s+\\w+)|(\\w+))\\s+from\\s+['\"]([^'\"]+)['\"]/gm`) extracting ES module syntax into `ImportEntry[]`, `extractDirectoryImports()` scans first 100 lines per file, partitions relative imports into `internal`/`external`, filters bare specifiers.\n\n**[src/installer/](./src/installer/)** — npx installation orchestrator: [index.ts](./src/installer/index.ts) `runInstall()` parses CLI args (`-g`/`-l`/`--runtime`/`--force`), interactive prompts via [prompts.ts](./src/installer/prompts.ts) `selectRuntime()`/`selectLocation()`, [operations.ts](./src/installer/operations.ts) `installFilesForRuntime()` writes command templates from `src/integration/templates.ts` and copies bundled hooks from `hooks/dist/`, registers hooks in `settings.json` via `registerClaudeHooks()`/`registerGeminiHooks()`, writes `ARE-VERSION` file, adds Bash permission patterns to Claude Code allow list; [uninstall.ts](./src/installer/uninstall.ts) removes files, hook registrations, permissions, cleans empty skill directories.\n\n**[src/integration/](./src/integration/)** — Platform-specific AI assistant integration: [detect.ts](./src/integration/detect.ts) `detectEnvironments()` scans for Claude (`.claude/` or `CLAUDE.md`), OpenCode (`.opencode/`), Aider (`.aider.conf.yml` or `.aider/`) via `existsSync()`; [templates.ts](./src/integration/templates.ts) `getTemplatesForRuntime()` generates command files with placeholder substitution for `COMMAND_PREFIX`, `VERSION_FILE_PATH`, `$ARGUMENTS`, embeds background execution pattern with `run_in_background: true` + `TaskOutput` polling + `.agents-reverse-engineer/progress.log` monitoring; [generate.ts](./src/integration/generate.ts) `generateIntegrationFiles()` writes templates via `ensureDir()` → `writeFileSync()` chain, copies bundled hooks for Claude.\n\n**[src/specify/](./src/specify/)** — Specification synthesis: [prompts.ts](./src/specify/prompts.ts) `buildSpecPrompt()` constructs system/user prompts enforcing nine-section structure (Overview, Architecture, API Surface, Data Structures, Configuration, Dependencies, Behavioral Contracts, Test Contracts, Build Plan) with verbatim regex/format/constant preservation; [writer.ts](./src/specify/writer.ts) `writeSpec()` writes single-file or multi-file specifications via heading-based splitting (`splitByHeadings()` partitions on `/^# /m`, `slugify()` sanitizes heading text), throws `SpecExistsError` when targets exist and `force: false`.\n\n**[src/output/](./src/output/)** — Terminal output formatting: [logger.ts](./src/output/logger.ts) `createLogger()` factory with picocolors-based ANSI formatting when `options.colors` is true, `createSilentLogger()` no-op for tests, `Logger` interface with six methods (`info()`, `file()`, `excluded()`, `summary()`, `warn()`, `error()`), output routing (log/warn/error → stdout/stderr).\n\n**[src/types/](./src/types/)** — Shared interfaces: [index.ts](./src/types/index.ts) defines `ExcludedFile`, `DiscoveryResult`, `DiscoveryStats` with `exclusionReasons` histogram, consumed by `src/discovery/walker.ts`, `src/cli/discover.ts`, `src/orchestration/runner.ts`.\n\n## Architecture\n\n### Three-Phase RLM Pipeline\n\n**Phase 1: File Analysis** — Iterator-based worker pool (`src/orchestration/pool.ts`) shares single `tasks.entries()` iterator across N workers (default 2 for WSL, 5 elsewhere) invoking `AIService.call()` → `runSubprocess()` → `execFile()` spawning AI CLI subprocesses with resource limits (heap 512MB, thread pool 4, background tasks disabled, Task tool disallowed). Process group killing (`kill(-pid)`) terminates subprocess trees on timeout (SIGTERM at `timeoutMs`, SIGKILL after 5s). Exponential backoff retry on rate limits (stderr patterns: \"rate limit\", \"429\", \"too many requests\", \"overloaded\"). Writes `.sum` files with YAML frontmatter (`generated_at`, `content_hash`, `purpose`, `critical_todos`, `related_files`) + markdown summary body.\n\n**Phase 2: Directory Aggregation** — Post-order traversal sorts directories by depth descending (`path.relative().split(path.sep).length`), waits for all child `.sum` files via `isDirectoryComplete()` predicate. `buildDirectoryPrompt()` aggregates child `.sum` content via `readSumFile()`, subdirectory `AGENTS.md` files via recursive traversal, import maps via `extractDirectoryImports()` with verified path constraints, manifest detection (9 types: `package.json`, `Cargo.toml`, `go.mod`, `pyproject.toml`, `pom.xml`, `build.gradle`, `Gemfile`, `composer.json`, `CMakeLists.txt`/`Makefile`). User-authored `AGENTS.md` files renamed to `AGENTS.local.md` and prepended above generated content. Writes `AGENTS.md` with `<!-- Generated by agents-reverse-engineer -->` marker.\n\n**Phase 3: Root Document Synthesis** — Sequential execution (concurrency=1) generates `CLAUDE.md`, `GEMINI.md`, `OPENCODE.md`. Prompts consume all `AGENTS.md` files via `collectAgentsDocs()` recursive tree traversal, parse root `package.json` for project metadata, enforce synthesis-only constraints (no invention of features/hooks/patterns not in source documents). Strips conversational preamble via pattern matching before writing output.\n\n### Incremental Update Strategy\n\n`UpdateOrchestrator.preparePlan()` workflow: (1) read `content_hash` from `.sum` YAML frontmatter via `readSumFile()`, (2) compute current SHA-256 via `computeContentHash()`, (3) hash mismatch → add to `filesToAnalyze` as `FileChange` with `status: 'modified'`/`'added'`, (4) hash match → add to `filesToSkip`, (5) detect orphans: `.sum` files for deleted source files or renamed oldPaths, (6) call `cleanupOrphans()` to delete stale `.sum` files, (7) call `cleanupEmptyDirectoryDocs()` to remove `AGENTS.md` from directories with no remaining sources, (8) compute `affectedDirs` via `getAffectedDirectories()` by walking parent directories of changed files, (9) regenerate `.sum` for `filesToAnalyze` via Phase 1 pool execution, (10) regenerate `AGENTS.md` for `affectedDirs` sequentially (no Phase 2 post-order traversal required). Git integration: supports committed changes (`git diff <baseCommit>..HEAD`) and uncommitted (`git status --porcelain` merge via `--uncommitted` flag), rename detection via `git diff -M` (50% similarity threshold), fallback to SHA-256 hashing for non-git workflows.\n\n### Subprocess Resource Management\n\nMitigations for Claude CLI excessive thread spawning (GitHub issue #5771: 200 NodeJS instances reported): `NODE_OPTIONS='--max-old-space-size=512'` limits heap to 512MB per subprocess, `UV_THREADPOOL_SIZE='4'` constrains libuv thread pool to 4 threads, `CLAUDE_CODE_DISABLE_BACKGROUND_TASKS='1'` prevents background task spawning, `--disallowedTools Task` blocks subagents. Process group killing: `kill(-pid)` terminates entire subprocess tree. Timeout enforcement: SIGTERM sent at `timeoutMs`, SIGKILL escalation after 5s grace period. Default concurrency reduced from 5 → 2 for resource-constrained environments.\n\n### Telemetry & Tracing\n\n**Run logs** (`.agents-reverse-engineer/logs/run-<timestamp>.json`) aggregate per-call token counts, costs, durations, errors, track `filesRead[]` metadata with `path`/`sizeBytes`/`linesRead`, compute summary (`totalInputTokens`, `totalCacheReadTokens`, `errorCount`, `uniqueFilesRead`), enforce retention via `cleanupOldLogs(keepCount)` after each run.\n\n**Trace events** (`.agents-reverse-engineer/traces/trace-<timestamp>.ndjson`) enabled via `--trace` flag emit `phase:start/end`, `worker:start/end`, `task:pickup/done`, `subprocess:spawn/exit`, `retry` events with auto-populated fields (`seq`, `ts`, `pid`, `elapsedMs`), promise-chain serialization ensures NDJSON line order matches emission order despite concurrent workers, retention keeps 500 most recent traces via `cleanupOldTraces(keepCount)`.\n\n**Progress log** (`.agents-reverse-engineer/progress.log`) human-readable streaming output mirroring console, ETA calculation via moving average of last 10 task durations, quality metrics (code-vs-doc/code-vs-code inconsistencies, phantom path counts), real-time monitoring via `tail -f .agents-reverse-engineer/progress.log`.\n\n### Quality Validation\n\n**Code-vs-Doc Consistency** — Extracts exported symbols via regex `/^[ \\t]*export\\s+(?:default\\s+)?(?:function|class|const|let|var|type|interface|enum)\\s+(\\w+)/gm`, verifies all exports appear in `.sum` summary text via substring search, reports `CodeDocInconsistency` with `missingFromDoc` arrays.\n\n**Code-vs-Code Duplicate Detection** — Aggregates exports across per-directory file groups into `Map<symbol, string[]>`, reports `CodeCodeInconsistency` for symbols appearing in multiple files (pattern: `'duplicate-export'`).\n\n**Phantom Path Resolution** — Extracts path-like strings from `AGENTS.md` via three regex patterns (markdown link targets `/\\[(?:[^\\]]*)\\]\\((\\.[^)]+)\\)/g`, backtick-quoted paths `` /`((?:src\\/|\\.\\.?\\/)[^`]+\\.[a-z]{1,4})`/g ``, prose-embedded paths `/(?:from|in|by|via|see)\\s+`?(src\\/[\\w\\-./]+)`?/gi`), resolves against `AGENTS.md` directory and project root with `.ts`/`.js` fallback, reports `PhantomPathInconsistency` for unresolved references.\n\n## Behavioral Contracts\n\n**GENERATED_MARKER:** `<!-- Generated by agents-reverse-engineer -->`\n\n**Frontmatter extraction regex:** `/^---\\n([\\s\\S]*?)\\n---\\n/`\n\n**YAML field patterns:**\n```javascript\n/generated_at:\\s*(.+)/\n/content_hash:\\s*(.+)/\n/purpose:\\s*(.+)/\n/key:\\s*\\[([^\\]]*)\\]/  // Inline arrays\n/key:\\s*\\n((?:\\s+-\\s+.+\\n?)+)/m  // Multi-line arrays\n```\n\n**Array formatting heuristic:** Inline if `length <= 3` and all items `< 40` chars, multi-line otherwise.\n\n**Export extraction regex:** `/^[ \\t]*export\\s+(?:default\\s+)?(?:function|class|const|let|var|type|interface|enum)\\s+(\\w+)/gm`\n\n**Path extraction patterns (phantom-paths):**\n```javascript\n/\\[(?:[^\\]]*)\\]\\((\\.[^)]+)\\)/g              // Markdown: [text](./path)\n/`((?:src\\/|\\.\\.?\\/)[^`]+\\.[a-z]{1,4})`/g  // Backtick: `src/foo.ts`\n/(?:from|in|by|via|see)\\s+`?(src\\/[\\w\\-./]+)`?/gi // Prose: \"from src/foo/\"\n```\n\n**Rate limit detection patterns:** `['rate limit', '429', 'too many requests', 'overloaded'].some(p => stderr.toLowerCase().includes(p))`\n\n**Preamble stripping patterns:**\n1. YAML separator: `/\\n---\\n/` within first 500 chars\n2. Bold uppercase start: `/^[\\s\\S]{0,500}?(\\*\\*[A-Z])/` matches conversational preamble, strips if <300 chars and no `##` headers\n\n**Skip directories:** `Set(['node_modules', '.git', '.agents-reverse-engineer', 'vendor', 'dist', 'build', '__pycache__', '.next', 'venv', '.venv', 'target', '.cargo', '.gradle'])`\n\n**Manifest detection:** `['package.json', 'Cargo.toml', 'go.mod', 'pyproject.toml', 'pom.xml', 'build.gradle', 'Gemfile', 'composer.json', 'CMakeLists.txt', 'Makefile']`\n\n**Git commands:**\n- Change detection: `git diff --name-status -M <baseCommit>..HEAD` (rename detection 50% threshold)\n- Uncommitted changes: `git status --porcelain` merged with `status.modified[]`/`deleted[]`/`not_added[]`/`staged[]`\n- SHA-256 content hash fallback for non-git workflows\n\n**Detached subprocess spawn pattern:**\n```javascript\nspawn(process.execPath, ['-e', scriptString], { \n  stdio: 'ignore', \n  detached: true, \n  windowsHide: true \n}).unref()\n```\n\n**Hook disable mechanisms:**\n- Environment variable: `ARE_DISABLE_HOOK=1`\n- Config substring: `readFileSync('.agents-reverse-engineer.yaml', 'utf-8').includes('hook_enabled: false')`\n### docs/AGENTS.md\n\n<!-- Generated by agents-reverse-engineer -->\n\n# docs\n\nOriginal design specification and research artifacts defining the Recursive Language Model (RLM) algorithm and agents-reverse-engineer tool inception rationale.\n\n## Contents\n\n### [INPUT.md](./INPUT.md)\nInception document defining the RLM algorithm via post-order tree traversal (leaf file analysis → directory aggregation → root synthesis), proposed command interface (`/are-generate`, `/are-update`), session lifecycle integration requirements, and research directives for GSD/BMAD methodology analysis.\n\n## Purpose\n\n`INPUT.md` serves as the foundational design specification established before implementation began. It defines the core RLM workflow: (1) build project tree, (2) traverse post-order starting at deepest leaves, (3) generate `.sum` summaries for files, (4) generate `AGENTS.md` for directories once all children complete, (5) recurse upward to project root. The document specifies target AI platforms (Claude Code, OpenCode, Gemini) and automatic documentation synchronization via session-end hooks.\n\n## Relationship to Implementation\n\nThe three-phase pipeline in `src/generation/orchestrator.ts` implements the RLM algorithm specified here:\n- **Phase 1** (concurrent file analysis) → `.sum` generation for leaf nodes\n- **Phase 2** (post-order directory aggregation) → `AGENTS.md` synthesis with depth-based sorting\n- **Phase 3** (root synthesis) → `CLAUDE.md`/`GEMINI.md`/`OPENCODE.md` platform-specific integration documents\n\nSession lifecycle hooks in `hooks/are-session-end.js` and `hooks/opencode-are-session-end.js` implement the automatic update mechanism proposed in this specification.\n### hooks/AGENTS.md\n\n<!-- Generated by agents-reverse-engineer -->\n\n# hooks/\n\n**Session lifecycle hooks for Claude Code, Gemini CLI, and OpenCode runtimes that execute version checking on session start and incremental documentation updates on session end via detached background processes.**\n\n## Contents\n\n### Session Start Hooks\n\n**[are-check-update.js](./are-check-update.js)** — Claude/Gemini SessionStart hook spawning detached `npm view agents-reverse-engineer version` subprocess, comparing against local `ARE-VERSION` file (project `.claude/ARE-VERSION` before global `~/.claude/ARE-VERSION`), writing comparison result to `~/.claude/cache/are-update-check.json` with schema `{ update_available, installed, latest, checked }`.\n\n**[opencode-are-check-update.js](./opencode-are-check-update.js)** — OpenCode plugin exporting `AreCheckUpdate()` async factory returning `event['session.created']` handler that spawns detached npm version check subprocess, writes cache to `~/.config/opencode/cache/are-update-check.json`.\n\n### Session End Hooks\n\n**[are-session-end.js](./are-session-end.js)** — Claude/Gemini SessionEnd hook executing `git status --porcelain` change detection, spawning detached `npx agents-reverse-engineer@latest update --quiet` subprocess if uncommitted changes detected, exits silently if `ARE_DISABLE_HOOK=1` or config contains `hook_enabled: false` substring.\n\n**[opencode-are-session-end.js](./opencode-are-session-end.js)** — OpenCode plugin exporting `AreSessionEnd()` async factory returning `event['session.deleted']` handler that executes identical git status check and detached update spawn as Claude/Gemini variant.\n\n## Architecture\n\n### Detached Subprocess Pattern\n\nAll hooks use identical background process spawning pattern:\n```javascript\nspawn(process.execPath, ['-e', scriptString], { \n  stdio: 'ignore', \n  detached: true, \n  windowsHide: true \n}).unref()\n```\n\nInline script via `-e` flag executes synchronous Node.js APIs (`execSync`, `readFileSync`, `writeFileSync`) in isolated process that survives parent termination. `stdio: 'ignore'` prevents output blocking session lifecycle, `unref()` allows parent exit without waiting for completion.\n\n### Version File Resolution\n\nCheck update hooks read sentinel `ARE-VERSION` file in priority order:\n1. Project-local: `<cwd>/.{claude,opencode}/ARE-VERSION`\n2. Global: `~/{.claude,.config/opencode}/ARE-VERSION`\n\nFalls back to `'0.0.0'` if neither exists. Installer writes this file during setup (`src/installer/operations.ts`).\n\n### Cache Schema\n\nVersion check subprocess writes JSON to platform-specific cache:\n- Claude/Gemini: `~/.claude/cache/are-update-check.json`\n- OpenCode: `~/.config/opencode/cache/are-update-check.json`\n\n**Format:**\n```json\n{\n  \"update_available\": boolean,\n  \"installed\": \"semver-string\",\n  \"latest\": \"semver-string\" | \"unknown\",\n  \"checked\": 1234567890\n}\n```\n\n### Disable Mechanisms\n\nSession-end hooks exit silently when:\n- `process.env.ARE_DISABLE_HOOK === '1'`\n- `.agents-reverse-engineer.yaml` exists and contains substring `'hook_enabled: false'` (no YAML parser)\n\n### Change Detection\n\nSession-end hooks execute `execSync('git status --porcelain', { encoding: 'utf-8' })` and exit silently if output is empty (no uncommitted changes) or if command throws (non-git repo).\n\n## File Relationships\n\n**Hook installation:** `src/installer/operations.ts` copies hooks to platform-specific directories and writes `ARE-VERSION` sentinel file.\n\n**Build process:** `scripts/build-hooks.js` copies hook files to `hooks/dist/` directory for npm tarball inclusion before publish.\n\n**Session-end update invocation:** Spawns `npx agents-reverse-engineer@latest update --quiet` which executes `src/cli/update.ts` orchestrator with hash-based change detection and incremental regeneration.\n\n**Platform path resolution:** `src/installer/paths.ts` provides platform-specific config directory lookup respecting environment overrides (`CLAUDE_CONFIG_DIR`, `OPENCODE_CONFIG_DIR`, `GEMINI_CONFIG_DIR`).\n\n## Behavioral Contracts\n\n**Git status command:** `'git status --porcelain'` with `{ encoding: 'utf-8' }` — detects uncommitted changes (modified/staged/untracked files).\n\n**npm version query:** `'npm view agents-reverse-engineer version'` with `{ encoding: 'utf8', timeout: 10000, windowsHide: true }` — queries registry for latest published version.\n\n**Update invocation:** `['npx', 'agents-reverse-engineer@latest', 'update', '--quiet']` — spawns latest published version with `--quiet` flag suppressing terminal output.\n\n**Config disable check pattern:** `readFileSync('.agents-reverse-engineer.yaml', 'utf-8').includes('hook_enabled: false')` — substring match without YAML parsing for fast disable detection.\n\n**Cache directory creation:** `mkdirSync(cacheDir, { recursive: true })` — ensures `~/{.claude,.config/opencode}/cache/` exists before subprocess spawn.\n### scripts/AGENTS.md\n\n<!-- Generated by agents-reverse-engineer -->\n\n# scripts/\n\nBuild automation utilities for npm package distribution. Currently contains a single script that prepares session lifecycle hooks for inclusion in the published npm tarball.\n\n## Contents\n\n**[build-hooks.js](./build-hooks.js)** — Copies `.js` hook files from `hooks/` to `hooks/dist/` via synchronous `copyFileSync()`, filtering non-JS files and the dist directory itself, invoked by `npm run build:hooks` and `prepublishOnly` lifecycle hook.\n\n## Build Pipeline Integration\n\n**Execution trigger:** `build-hooks.js` runs during `npm run prepublishOnly`, which npm automatically invokes before `npm publish`. The package.json `scripts` section chains `prepublishOnly` → `npm run build && npm run build:hooks`, ensuring TypeScript compilation (`tsc`) and hook copying both complete before tarball creation.\n\n**Distribution requirement:** Session lifecycle hooks (`are-check-update.js`, `are-session-end.js`, `opencode-are-check-update.js`, `opencode-are-session-end.js`) must exist in `hooks/dist/` for the npm tarball to include them. The installer module (`src/installer/operations.ts`) copies these files from the installed package into IDE config directories (`~/.claude/hooks/`, `~/.config/opencode/plugins/`).\n\n## File Operations\n\n**Path resolution:** Uses ES module `import.meta.url` → `fileURLToPath()` → `__dirname` pattern to locate project root. Constructs `HOOKS_SRC` as `join(projectRoot, 'hooks')` and `HOOKS_DIST` as `join(projectRoot, 'hooks', 'dist')`.\n\n**Synchronous workflow:**\n1. `existsSync(HOOKS_DIST)` checks for dist directory\n2. `mkdirSync(HOOKS_DIST, { recursive: true })` creates directory if missing\n3. `readdirSync(HOOKS_SRC)` enumerates source files\n4. `.filter(f => f.endsWith('.js') && f !== 'dist')` excludes non-JavaScript files and dist directory\n5. `copyFileSync(join(HOOKS_SRC, file), join(HOOKS_DIST, file))` copies each hook file\n6. Console logs progress: per-file `\"Copied: {file} -> hooks/dist/{file}\"` and summary `\"Done. {count} hook(s) built.\"`\n\n## Behavioral Contracts\n\n**File filter predicate:** `/\\.js$/ && f !== 'dist'` (endsWith check + exclusion)\n### src/AGENTS.md\n\n<!-- Generated by agents-reverse-engineer -->\n\n# src\n\n**TypeScript source tree containing CLI entry points, three-phase AI orchestration pipeline, file discovery filters, configuration management, subprocess-spawning AI service layer, quality validators, and IDE integration installers for agents-reverse-engineer brownfield documentation generator.**\n\n## Contents\n\n**[version.ts](./version.ts)** — `getVersion()` reads `package.json` from compiled module parent directory (dist/ → project root) via `fileURLToPath(import.meta.url)` → `dirname()` → `join(__dirname, '..', 'package.json')`, parses JSON extracting `version` field, returns `'unknown'` on error (file not found, JSON parse failure, missing field). Called by CLI help text, version flag output, session hooks for npm registry comparison.\n\n## Subdirectories\n\n**[ai/](./ai/)** — Backend-agnostic AI service orchestration: subprocess-spawning adapters (ClaudeBackend with Zod JSON parsing, GeminiBackend/OpenCodeBackend stubs), exponential backoff retry with rate limit detection (['rate limit', '429', 'too many requests', 'overloaded']), timeout enforcement (SIGTERM at timeoutMs, SIGKILL after 5s, process group killing `kill(-pid)`), NDJSON telemetry logging to `.agents-reverse-engineer/logs/run-<timestamp>.json` with token/cost tracking, trace emission (subprocess:spawn/exit/retry).\n\n**[change-detection/](./change-detection/)** — Git-based change detection parsing `git diff --name-status -M` with rename detection, merges uncommitted working tree changes via `status.modified[]`/`deleted[]`/`not_added[]`/`staged[]`, SHA-256 content hashing for `.sum` frontmatter verification, returns `ChangeDetectionResult` with `FileChange[]` array (path/status/oldPath/contentHash), status codes map to `ChangeType` ('added'/'modified'/'deleted'/'renamed').\n\n**[cli/](./cli/)** — Command entry points routing parsed `process.argv` to handlers: `initCommand()` writes config YAML, `cleanCommand()` deletes artifacts with `GENERATED_MARKER` filtering, `discoverCommand()` builds `GENERATION-PLAN.md`, `generateCommand()` orchestrates three-phase pipeline with backend resolution, `updateCommand()` executes incremental hash-based regeneration, `specifyCommand()` synthesizes project specifications from AGENTS.md corpus. Main index parses flags via `parseArgs()` with short flag expansion (`-h` → `help`, `-g` → `global`), dual routing for installer mode vs. command dispatch, exit codes (0=success, 1=partial failure, 2=total failure/CLI not found).\n\n**[config/](./config/)** — Configuration management: `loadConfig()` reads `.agents-reverse-engineer/config.yaml` via YAML parsing and `ConfigSchema.parse()` Zod validation (returns defaults on ENOENT, throws `ConfigError` on ZodError), `writeDefaultConfig()` generates commented template with inline defaults, `getDefaultConcurrency()` computes adaptive worker pool size via `clamp(cores * 5, 2, min(memCap, 20))` where `memCap = floor((totalMemGB * 0.5) / 0.512)`, default exclusions (26 patterns, 18 vendor dirs, 26 binary extensions).\n\n**[discovery/](./discovery/)** — File discovery pipeline: `discoverFiles()` creates four-filter chain (GitignoreFilter → VendorFilter → BinaryFilter → CustomPatternFilter), `walkDirectory()` via `fast-glob` with `{absolute: true, onlyFiles: true, ignore: ['**/.git/**']}`, `applyFilters()` with 30-worker concurrency and short-circuit evaluation, returns `FilterResult` with `included[]`/`excluded[]` attribution. Filters normalize paths via `path.relative(normalizedRoot, absolutePath)` for pattern matching.\n\n**[generation/](./generation/)** — Three-phase orchestration: `GenerationOrchestrator.createPlan()` executes file discovery → `analyzeComplexity()` → `buildFilePrompt()` per file with `projectPlan` injection → groups by `path.dirname()` for directory tasks → `buildExecutionPlan()` post-order traversal (sorted by depth descending) → memory management clears `PreparedFile.content` fields after embedding. `buildDirectoryPrompt()` reads `.sum` frontmatter, child `AGENTS.md`, import maps via `extractDirectoryImports()`, detects 9 manifest types. `buildRootPrompt()` calls `collectAgentsDocs()`, parses root `package.json`, enforces synthesis-only constraints. Writers serialize YAML frontmatter (adaptive array formatting), preserve user content via `AGENTS.local.md` rename pattern.\n\n**[imports/](./imports/)** — Static import analysis: `extractImports()` applies `IMPORT_REGEX` (`/^import\\s+(type\\s+)?(?:\\{([^}]*)\\}|(\\*\\s+as\\s+\\w+)|(\\w+))\\s+from\\s+['\"]([^'\"]+)['\"]/gm`) extracting ES module syntax into `ImportEntry[]`, `extractDirectoryImports()` scans first 100 lines per file for optimization, partitions relative imports into `internal` (`./`) and `external` (`../`), filters bare specifiers (`'react'`, `node:*`), `formatImportMap()` serializes into prompt text blocks with `(type)` suffix.\n\n**[installer/](./installer/)** — npx installation orchestrator: `runInstall()` parses CLI args (`-g`/`-l`/`--runtime`/`--force`), interactive prompts via `selectRuntime()`/`selectLocation()` with arrow-key selection in TTY mode, `installFilesForRuntime()` writes command templates from `src/integration/templates.ts` and copies bundled hooks from `hooks/dist/`, registers hooks in `settings.json` via `registerClaudeHooks()`/`registerGeminiHooks()`, writes `ARE-VERSION` file, adds Bash permission patterns to Claude Code allow list. Uninstaller removes files, hook registrations, permissions, cleans empty skill directories via `cleanupAreSkillDirs()`/`cleanupLegacyGeminiFiles()`/`cleanupEmptyDirs()`.\n\n**[integration/](./integration/)** — Platform-specific AI assistant integration: `detectEnvironments()` scans for Claude (`.claude/` or `CLAUDE.md`), OpenCode (`.opencode/`), Aider (`.aider.conf.yml` or `.aider/`) via `existsSync()`, `getTemplatesForRuntime()` generates command files with placeholder substitution for `COMMAND_PREFIX` (`/are-`), `VERSION_FILE_PATH`, `$ARGUMENTS`, embeds background execution pattern with `run_in_background: true`, `TaskOutput` polling, `.agents-reverse-engineer/progress.log` monitoring. `generateIntegrationFiles()` writes templates via `ensureDir()` → `writeFileSync()` chain, copies bundled hooks for Claude.\n\n**[orchestration/](./orchestration/)** — Iterator-based worker pool orchestrating three-phase execution: `runPool()` shares single `tasks.entries()` iterator across N workers preventing idle time, emits `worker:start/end` and `task:pickup/done` trace events, supports `failFast` abort via mutable flag. `CommandRunner.executeGenerate()` runs concurrent file `.sum` generation with stale-doc detection, post-order directory `AGENTS.md` traversal grouped by depth, sequential root document synthesis (`CLAUDE.md`/`GEMINI.md`/`OPENCODE.md`), embeds quality validators post-phase (code-vs-doc, code-vs-code, phantom-paths). `ProgressReporter` streams colored console output with ETA via moving average (last 10 completion times), `ProgressLog` mirrors to `.agents-reverse-engineer/progress.log` via promise-chain serialized writes. `PlanTracker` serializes concurrent checkbox updates (`- [ ] → - [x]`) for `GENERATION-PLAN.md` via promise-chain pattern. `TraceWriter` appends to `.agents-reverse-engineer/traces/trace-<timestamp>.ndjson` with auto-populated `seq`/`ts`/`pid`/`elapsedMs` base fields, `cleanupOldTraces()` enforces 500-file retention.\n\n**[output/](./output/)** — Terminal output formatting: `createLogger()` factory with picocolors-based ANSI formatting when `options.colors` is true, `createSilentLogger()` no-op for tests, `Logger` interface (six methods: `info()`, `file()`, `excluded()`, `summary()`, `warn()`, `error()`), output routing (log/warn/error → stdout/stderr).\n\n**[quality/](./quality/)** — Code-documentation consistency validation: `checkCodeVsDoc()` extracts exports via `/^[ \\t]*export\\s+(?:default\\s+)?(?:function|class|const|let|var|type|interface|enum)\\s+(\\w+)/gm` with substring verification against `.sum` content, `checkCodeVsCode()` aggregates exports into `Map<symbol, paths[]>` for duplicate detection, `checkPhantomPaths()` extracts path references via three regex patterns (markdown links, backtick paths, prose references) resolving against `AGENTS.md` directory and project root with `.ts`/`.js` fallback. `buildInconsistencyReport()` merges issues with summary counts by type (`code-vs-doc`/`code-vs-code`/`phantom-path`) and severity (`error`/`warning`/`info`).\n\n**[specify/](./specify/)** — Specification synthesis: `buildSpecPrompt()` constructs system/user prompts enforcing nine-section structure (Overview, Architecture, API Surface, Data Structures, Configuration, Dependencies, Behavioral Contracts, Test Contracts, Build Plan) with verbatim regex/format/constant preservation, `writeSpec()` writes single-file or multi-file specifications via heading-based splitting (`splitByHeadings()` partitions on `/^# /m`, `slugify()` sanitizes heading text), throws `SpecExistsError` when targets exist and `force: false`.\n\n**[types/](./types/)** — Shared interfaces for discovery results (`ExcludedFile`, `DiscoveryResult`, `DiscoveryStats` with `exclusionReasons` histogram), consumed by `src/discovery/walker.ts`, `src/cli/discover.ts`, `src/orchestration/runner.ts`.\n\n**[update/](./update/)** — Incremental documentation synchronization: `UpdateOrchestrator.preparePlan()` executes hash-based change detection via `readSumFile()` YAML frontmatter extraction, `computeContentHash()` SHA-256 comparison, produces `UpdatePlan` with `filesToAnalyze[]` (hash mismatch/missing), `filesToSkip[]` (hash match), `cleanup: CleanupResult` (orphaned `.sum` files), `affectedDirs[]` (depth-sorted parent directories). `cleanupOrphans()` deletes `.sum` files for `FileChange` entries with `status === 'deleted'` (uses `change.path`) or `status === 'renamed'` (uses `change.oldPath`). `cleanupEmptyDirectoryDocs()` removes `AGENTS.md` from directories containing only hidden files, `.sum` files, or generated artifacts. `getAffectedDirectories()` walks parent directories via `path.dirname()` excluding deletes.\n\n## Three-Phase Pipeline Architecture\n\n**Phase 1 (File Analysis)** — Concurrent pool execution (default 2 workers for WSL, 5 elsewhere): `GenerationOrchestrator.createFileTasks()` invokes `buildFilePrompt()` per file embedding content + project structure, workers execute via `runPool()` calling `AIService.call()` → `runSubprocess()` → `execFile()` spawning AI CLI subprocesses with resource limits (`NODE_OPTIONS='--max-old-space-size=512'`, `UV_THREADPOOL_SIZE='4'`, `CLAUDE_CODE_DISABLE_BACKGROUND_TASKS='1'`, `--disallowedTools Task`). Results written via `writeSumFile()` to `.sum` paths with YAML frontmatter (`generated_at`, `content_hash`, `purpose`, `critical_todos`, `related_files`). Post-phase quality validation: throttled (concurrency=10) `checkCodeVsDoc()` comparing old vs. new `.sum` against source content, `checkCodeVsCode()` for duplicate symbol detection.\n\n**Phase 2 (Directory Aggregation)** — Post-order traversal: `GenerationOrchestrator.createDirectoryTasks()` groups files via `Map<dirname, PreparedFile[]>`, executor sorts tasks by `getDirectoryDepth()` descending (deepest first), waits for child `.sum` files via `isDirectoryComplete()`, constructs prompts via `buildDirectoryPrompt()` consuming `.sum` frontmatter + child `AGENTS.md` + import maps from `extractDirectoryImports()`. Writes via `writeAgentsMd()` preserving user content from `AGENTS.local.md`. Post-phase: runs `checkPhantomPaths()` on each generated `AGENTS.md` (three regex patterns for path extraction), aggregates issues into `phantomReport`.\n\n**Phase 3 (Root Synthesis)** — Sequential execution (concurrency=1) generates `CLAUDE.md`, `GEMINI.md`, `OPENCODE.md` via `buildRootPrompt()` aggregating all `AGENTS.md` via `collectAgentsDocs()`, parsing root `package.json`, enforcing synthesis-only constraints (no invention). Strips conversational preamble via pattern matching before writing output.\n\n## Subprocess Resource Management\n\nAI CLI spawning mitigations for Claude #5771 (200 NodeJS instances): `NODE_OPTIONS='--max-old-space-size=512'` limits heap to 512MB per subprocess, `UV_THREADPOOL_SIZE='4'` constrains libuv thread pool to 4 threads, `CLAUDE_CODE_DISABLE_BACKGROUND_TASKS='1'` prevents background task spawning, `--disallowedTools Task` blocks subagents. Process group killing: `kill(-pid)` terminates entire subprocess tree. Timeout enforcement: SIGTERM sent at `timeoutMs`, SIGKILL escalation after 5s grace period. Default concurrency reduced from 5 → 2 for resource-constrained environments.\n\n## Incremental Update Strategy\n\n`UpdateOrchestrator.preparePlan()` workflow: (1) read `content_hash` from `.sum` YAML frontmatter via `readSumFile()`, (2) compute current SHA-256 via `computeContentHash()`, (3) hash mismatch → add to `filesToAnalyze` as `FileChange` with `status: 'modified'`/`'added'`, (4) hash match → add to `filesToSkip`, (5) detect orphans: `.sum` files for deleted source files or renamed oldPaths, (6) call `cleanupOrphans()` to delete stale `.sum` files, (7) call `cleanupEmptyDirectoryDocs()` to remove `AGENTS.md` from directories with no remaining sources, (8) compute `affectedDirs` via `getAffectedDirectories()` walking parent directories of changed files, (9) regenerate `.sum` for `filesToAnalyze` via Phase 1 pool execution, (10) regenerate `AGENTS.md` for `affectedDirs` sequentially (no Phase 2 post-order traversal required). Git integration: supports committed changes (`git diff <baseCommit>..HEAD`) and uncommitted (`git status --porcelain` merge via `--uncommitted` flag), rename detection via `git diff -M` (50% similarity threshold), fallback to SHA-256 hashing for non-git workflows.\n\n## Behavioral Contracts\n\n**GENERATED_MARKER:** `<!-- Generated by agents-reverse-engineer -->`\n\n**Frontmatter extraction regex:** `/^---\\n([\\s\\S]*?)\\n---\\n/`\n\n**YAML field patterns:**\n```javascript\n/generated_at:\\s*(.+)/\n/content_hash:\\s*(.+)/\n/purpose:\\s*(.+)/\n/key:\\s*\\[([^\\]]*)\\]/  // Inline arrays\n/key:\\s*\\n((?:\\s+-\\s+.+\\n?)+)/m  // Multi-line arrays\n```\n\n**Array formatting heuristic:** Inline if `length <= 3` and all items `< 40` chars, multi-line otherwise.\n\n**Export extraction regex:** `/^[ \\t]*export\\s+(?:default\\s+)?(?:function|class|const|let|var|type|interface|enum)\\s+(\\w+)/gm`\n\n**Path extraction patterns (phantom-paths):**\n```javascript\n/\\[(?:[^\\]]*)\\]\\((\\.[^)]+)\\)/g              // Markdown: [text](./path)\n/`((?:src\\/|\\.\\.?\\/)[^`]+\\.[a-z]{1,4})`/g  // Backtick: `src/foo.ts`\n/(?:from|in|by|via|see)\\s+`?(src\\/[\\w\\-./]+)`?/gi // Prose: \"from src/foo/\"\n```\n\n**Rate limit detection patterns:** `['rate limit', '429', 'too many requests', 'overloaded'].some(p => stderr.toLowerCase().includes(p))`\n\n**Preamble stripping patterns:**\n1. YAML separator: `/\\n---\\n/` within first 500 chars\n2. Bold uppercase start: `/^[\\s\\S]{0,500}?(\\*\\*[A-Z])/` matches conversational preamble, strips if <300 chars and no `##` headers\n\n**Skip directories:** `Set(['node_modules', '.git', '.agents-reverse-engineer', 'vendor', 'dist', 'build', '__pycache__', '.next', 'venv', '.venv', 'target', '.cargo', '.gradle'])`\n\n**Manifest detection:** `['package.json', 'Cargo.toml', 'go.mod', 'pyproject.toml', 'pom.xml', 'build.gradle', 'Gemfile', 'composer.json', 'CMakeLists.txt', 'Makefile']`\n### src/ai/AGENTS.md\n\n<!-- Generated by agents-reverse-engineer -->\n\n# src/ai\n\nBackend-agnostic AI service orchestration layer: subprocess-spawning adapters for Claude Code/Gemini/OpenCode CLIs, exponential backoff retry logic with rate limit detection, timeout enforcement via SIGTERM/SIGKILL escalation, NDJSON telemetry logging with token cost tracking, and trace event emission for concurrent pool workflows.\n\n## Contents\n\n**[index.ts](./index.ts)** — Barrel export consolidating AIService, BackendRegistry, createBackendRegistry, resolveBackend, detectBackend, getInstallInstructions, withRetry, runSubprocess, isCommandOnPath with type re-exports (AIBackend, AIResponse, AICallOptions, SubprocessResult, RetryOptions, TelemetryEntry, RunLog, FileRead, AIServiceError).\n\n**[registry.ts](./registry.ts)** — BackendRegistry stores AIBackend implementations in insertion-order Map with `register(backend)`, `get(name)`, `getAll()` methods; createBackendRegistry() pre-populates ClaudeBackend/GeminiBackend/OpenCodeBackend; resolveBackend(registry, 'auto'|name) performs auto-detection or explicit lookup throwing CLI_NOT_FOUND with install instructions.\n\n**[retry.ts](./retry.ts)** — withRetry() executes async function with exponential backoff (delay = min(baseDelayMs × multiplier^attempt, maxDelayMs) + jitter[0..500ms]) calling isRetryable(error) predicate before each retry, invoking onRetry(attempt, error) callback for telemetry.\n\n**[service.ts](./service.ts)** — AIService orchestrates call(options) via runSubprocess() wrapped in withRetry(), detects rate limits via stderr pattern matching (['rate limit', '429', 'too many requests', 'overloaded']), emits subprocess:spawn/exit/retry trace events, accumulates TelemetryEntry records via internal TelemetryLogger, writes RunLog to `.agents-reverse-engineer/logs/run-<timestamp>.json` on finalize(), enforces cleanup via cleanupOldLogs(keepRuns).\n\n**[subprocess.ts](./subprocess.ts)** — runSubprocess() spawns child process via execFile() with stdin piping, timeout enforcement (SIGTERM at timeoutMs, SIGKILL after 5s grace), process group killing (`kill(-pid)`) for tree termination, concurrent subprocess tracking via Map<pid, {command, spawnedAt}>, returns SubprocessResult with stdout/stderr/exitCode/signal/durationMs/timedOut/childPid.\n\n**[types.ts](./types.ts)** — Defines AIBackend interface (isAvailable/buildArgs/parseResponse/getInstallInstructions), AICallOptions (prompt/systemPrompt/model/timeoutMs/maxTurns/taskLabel), AIResponse (text/model/inputTokens/outputTokens/cacheReadTokens/cacheCreationTokens/durationMs/exitCode/raw), SubprocessResult, RetryOptions, TelemetryEntry, RunLog with summary aggregation, FileRead, AIServiceError with discriminated codes ('CLI_NOT_FOUND'|'TIMEOUT'|'PARSE_ERROR'|'SUBPROCESS_ERROR'|'RATE_LIMIT').\n\n## Subdirectories\n\n**[backends/](./backends/)** — ClaudeBackend with Zod-validated JSON parsing extracting usage.input_tokens/output_tokens/cache_read_input_tokens/cache_creation_input_tokens, GeminiBackend/OpenCodeBackend stubs throwing SUBPROCESS_ERROR, isCommandOnPath() cross-platform PATH detection with Windows PATHEXT extension iteration.\n\n**[telemetry/](./telemetry/)** — TelemetryLogger accumulates per-call entries computing aggregate summary (totalInputTokens/totalCacheReadTokens/errorCount/uniqueFilesRead), writeRunLog() serializes to `.agents-reverse-engineer/logs/run-<timestamp>.json` with filename sanitization (`/[:.]/g → '-'`), cleanupOldLogs() enforces retention via lexicographic sort on ISO 8601 filenames.\n\n## Architecture\n\n### Three-Layer Design\n\n**Backend Adapter Layer** (backends/): AIBackend implementations translate AICallOptions into CLI-specific argv arrays via buildArgs(), parse stdout JSON into normalized AIResponse via parseResponse(), detect availability via isCommandOnPath() checking PATH directories with Windows PATHEXT handling.\n\n**Subprocess Execution Layer** (subprocess.ts): runSubprocess() spawns execFile() with 10MB maxBuffer, writes input to stdin via Buffer.byteLength() computed payload, enforces timeout via SIGTERM then SIGKILL escalation with unref()'d timer, kills process groups via negative PID (`process.kill(-child.pid, 'SIGKILL')`), tracks active subprocesses in Map for concurrency monitoring.\n\n**Service Orchestration Layer** (service.ts): AIService wraps runSubprocess() calls in withRetry() with isRateLimitStderr() predicate detecting ['rate limit', '429', 'too many requests', 'overloaded'] patterns, emits subprocess:spawn/exit trace events via ITraceWriter, accumulates TelemetryEntry[] via TelemetryLogger, finalizes RunLog with summary (totalInputTokens, totalCacheReadTokens, errorCount, uniqueFilesRead).\n\n### Retry Strategy\n\nwithRetry() executes fn() up to maxRetries+1 times with exponential backoff: `min(baseDelayMs * multiplier^attempt, maxDelayMs) + random(0..500ms)`. Checks isRetryable(error) before each sleep—permanently fails on auth errors or non-retryable signals. Invokes onRetry(attempt, error) before delay for trace emission. AIService.call() only retries RATE_LIMIT errors (code === 'RATE_LIMIT'), treats TIMEOUT as permanent failure to prevent resource exhaustion.\n\n### Timeout Enforcement\n\nrunSubprocess() sends SIGTERM at timeoutMs via execFile killSignal option. Sets unref()'d SIGKILL timer at `timeoutMs + 5000ms`. Clears timer in callback if process exits before escalation. Process group killing (`kill(-pid)`) terminates entire subprocess tree. Falls back to single-process kill if group signal fails.\n\n### Telemetry Pipeline\n\nAIService.call() records TelemetryEntry after each subprocess completion with timestamp/prompt/systemPrompt/response/model/inputTokens/outputTokens/cacheReadTokens/cacheCreationTokens/latencyMs/exitCode/error/retryCount/thinking/filesRead. TelemetryLogger.addEntry() appends to in-memory entries[]. AIService.addFilesReadToLastEntry(filesRead) mutates most recent entry to attach FileRead[] metadata (path + sizeBytes). AIService.finalize() calls TelemetryLogger.toRunLog() computing summary, writes to `.agents-reverse-engineer/logs/run-<timestamp>.json` via writeRunLog(), enforces retention via cleanupOldLogs(keepRuns).\n\n### Resource Management\n\nSubprocess limits injected by AIService via environment variables (set in src/ai/service.ts, executed in subprocess.ts):\n- `NODE_OPTIONS='--max-old-space-size=512'` — limits heap to 512MB per subprocess\n- `UV_THREADPOOL_SIZE='4'` — constrains libuv thread pool to 4 threads\n- `CLAUDE_CODE_DISABLE_BACKGROUND_TASKS='1'` — prevents background task spawning\n- CLI args: `--disallowedTools Task` — blocks subagent spawning\n\nactiveSubprocesses Map tracks concurrent processes keyed by PID with {command, spawnedAt}. getActiveSubprocessCount() returns Map size. getActiveSubprocesses() computes runningMs as `Date.now() - spawnedAt`.\n\n### Debug Logging\n\nAIService.setDebug(true) enables stderr output before/after subprocess with heapUsed/rss metrics via formatBytes(). AIService.setSubprocessLogDir(dir) writes per-subprocess `.log` files with metadata header (task/pid/command/exit/signal/duration/timed_out) followed by stdout/stderr sections. Serialized via logWriteQueue promise chain preventing concurrent mkdir races. Failures silently swallowed (non-critical).\n\n## Integration Points\n\n**Phase 1 Orchestration**: `src/generation/executor.ts` calls AIService.call() for each .sum file generation passing prompt from `src/generation/prompts/builder.ts`, attaches filesRead metadata via addFilesReadToLastEntry() after file analysis.\n\n**Worker Pool**: `src/orchestration/pool.ts` shares single AIService instance across N workers (default 2 for WSL, 5 elsewhere), monitors concurrency via getActiveSubprocessCount(), emits trace events via AIService.setTracer(tracer).\n\n**Trace Emission**: AIService subprocess:spawn/exit events serialized to `.agents-reverse-engineer/traces/trace-<timestamp>.ndjson` via `src/orchestration/trace.ts` TraceWriter with promise-chain ordering guarantees.\n\n**Registry Auto-Detection**: `src/cli/generate.ts` calls createBackendRegistry() then resolveBackend(registry, config.ai.backend) for backend selection, throws CLI_NOT_FOUND with install instructions when backend unavailable.\n\n**Incremental Updates**: `src/update/orchestrator.ts` reuses same AIService instance for modified file regeneration, shares telemetry accumulation across discovery + analysis phases.\n\n## Behavioral Contracts\n\n### Rate Limit Detection Patterns\n\n```javascript\n['rate limit', '429', 'too many requests', 'overloaded'].some(p => \n  stderr.toLowerCase().includes(p)\n)\n```\n\n### Timeout Detection\n\n```javascript\nresult.timedOut === true  // set when error.killed === true in execFile callback\n```\n\n### Exit Code Extraction\n\n```javascript\nerror === null ? 0\n: typeof error.code === 'number' ? error.code\n: child.exitCode !== null ? child.exitCode\n: 1\n```\n\n### Process Group Killing\n\n```javascript\nprocess.kill(-child.pid, 'SIGKILL')  // negative PID targets process group\n```\n\n### Filename Sanitization\n\n```javascript\nrunLog.startTime.replace(/[:.]/g, '-')  // ISO 8601 → filesystem-safe\n// Example: 2026-02-07T12:00:00.000Z → run-2026-02-07T12-00-00-000Z.json\n```\n\n### Log File Filtering\n\n```javascript\nname.startsWith('run-') && name.endsWith('.json')\n```\n### src/ai/backends/AGENTS.md\n\n<!-- Generated by agents-reverse-engineer -->\n\n# src/ai/backends\n\n**Backend adapter implementations for Claude Code, Gemini CLI, and OpenCode providing AIBackend interface compliance with CLI argument construction, JSON response parsing, PATH availability detection, and installation instructions.**\n\n## Contents\n\n### Backend Implementations\n\n**[claude.ts](./claude.ts)** — ClaudeBackend adapter with Zod-validated JSON parsing extracting `text`/`model`/`inputTokens`/`outputTokens`/`cacheReadTokens`/`cacheCreationTokens` from Claude CLI v2.1.31 output, `buildArgs()` constructing `['-p', '--output-format', 'json', '--no-session-persistence', '--permission-mode', 'bypassPermissions']` base arguments with conditional `--model`/`--system-prompt`/`--max-turns` appends, `isCommandOnPath()` cross-platform PATH detection handling Windows `PATHEXT` extension iteration.\n\n**[gemini.ts](./gemini.ts)** — GeminiBackend stub returning `['-p', '--output-format', 'json']` from `buildArgs()` but throwing `SUBPROCESS_ERROR` in `parseResponse()` until Gemini JSON output format stabilizes per RESEARCH.md Open Question 2.\n\n**[opencode.ts](./opencode.ts)** — OpenCodeBackend stub returning `['run', '--format', 'json']` from `buildArgs()` but throwing `SUBPROCESS_ERROR` in `parseResponse()` until JSONL parsing implementation completes per RESEARCH.md Open Question 3.\n\n## AIBackend Interface Contract\n\nAll backends implement five required methods:\n\n- **`isAvailable(): Promise<boolean>`** — Delegates to `isCommandOnPath(this.cliCommand)` checking PATH directories via `fs.stat().isFile()` with Windows PATHEXT handling\n- **`buildArgs(options: AICallOptions): string[]`** — Constructs CLI argument arrays (prompt content delivered via stdin by `runSubprocess()` caller, not CLI arg)\n- **`parseResponse(stdout: string, durationMs: number, exitCode: number): AIResponse`** — Normalizes CLI output to `{ text, model, inputTokens, outputTokens, cacheReadTokens, cacheCreationTokens, durationMs, exitCode, raw }` structure\n- **`getInstallInstructions(): string`** — Returns multiline installation guidance with npm/curl commands and documentation URLs\n- **Properties:** `name: string` (backend identifier), `cliCommand: string` (executable name)\n\n## Claude Response Schema\n\n`ClaudeResponseSchema` validates JSON structure:\n\n```typescript\n{\n  type: 'result',\n  subtype: 'success'|'error',\n  is_error: boolean,\n  duration_ms: number,\n  duration_api_ms: number,\n  num_turns: number,\n  result: string,\n  session_id: string,\n  total_cost_usd: number,\n  usage: {\n    input_tokens: number,\n    cache_creation_input_tokens: number,\n    cache_read_input_tokens: number,\n    output_tokens: number\n  },\n  modelUsage: Record<string, {\n    inputTokens: number,\n    outputTokens: number,\n    cacheReadInputTokens: number,\n    cacheCreationInputTokens: number,\n    costUSD: number\n  }>\n}\n```\n\n## Integration with Registry\n\nBackends registered in `src/ai/registry.ts` `AIBackendRegistry.backends` array enabling `detectAvailableBackend()` auto-detection via parallel `isAvailable()` checks. Registry returns first available backend when `config.ai.backend === 'auto'`.\n\n## PATH Detection Strategy\n\n`isCommandOnPath()` implements cross-platform availability checking:\n\n1. Reads `process.env.PATH`, strips double quotes, splits by `path.delimiter` (`:` Unix, `;` Windows)\n2. Reads `process.env.PATHEXT` on Windows (e.g., `.COM;.EXE;.BAT;.CMD`), defaults to `['']` on Unix\n3. Iterates PATH directories × PATHEXT extensions constructing candidate paths\n4. Calls `fs.stat(candidatePath).isFile()` returning `true` on first match (avoids Unix execute bit assumptions)\n5. Returns `false` if no candidates exist as files\n\n## Error Handling\n\nClaudeBackend throws `AIServiceError` with codes:\n\n- **`PARSE_ERROR`** — When `stdout.indexOf('{')` returns `-1` (includes first 200 chars of raw output)\n- **`PARSE_ERROR`** — When Zod validation fails (includes validation error message)\n\nStub backends throw:\n\n- **`SUBPROCESS_ERROR`** — GeminiBackend/OpenCodeBackend until parsing implementation completes\n\n## Behavioral Contracts\n\n### ClaudeBackend CLI Arguments\n\nBase arguments (always included):\n```\n-p\n--output-format json\n--no-session-persistence\n--permission-mode bypassPermissions\n```\n\nConditional arguments:\n```\n--model <value>               # if options.model present\n--system-prompt <value>       # if options.systemPrompt present\n--max-turns <value>           # if options.maxTurns !== undefined\n```\n\n### JSON Parsing Defensive Pattern\n\n```javascript\nconst jsonStart = stdout.indexOf('{')\nif (jsonStart === -1) {\n  throw new AIServiceError('PARSE_ERROR', `No JSON in output: ${stdout.slice(0, 200)}`)\n}\nconst parsed = ClaudeResponseSchema.parse(JSON.parse(stdout.slice(jsonStart)))\n```\n\n### Windows PATHEXT Extensions\n\n```\n.COM;.EXE;.BAT;.CMD;.VBS;.VBE;.JS;.JSE;.WSF;.WSH;.MSC\n```\n### src/ai/telemetry/AGENTS.md\n\n<!-- Generated by agents-reverse-engineer -->\n\n# src/ai/telemetry\n\nPersistent telemetry accumulation and retention management: in-memory RunLog assembly via TelemetryLogger, JSON serialization with sanitized ISO 8601 filenames, and log rotation enforcing configurable retention limits.\n\n## Contents\n\n**[cleanup.ts](./cleanup.ts)** — `cleanupOldLogs(projectRoot, keepCount)` deletes old run log files from `.agents-reverse-engineer/logs/`, retaining N most recent via lexicographic sort on ISO 8601 filenames, handles ENOENT gracefully.\n\n**[logger.ts](./logger.ts)** — `TelemetryLogger` class accumulates per-call `TelemetryEntry` objects in memory, computes aggregate summary statistics (total tokens/costs/errors, unique files read), exports `toRunLog()` for serialization.\n\n**[run-log.ts](./run-log.ts)** — `writeRunLog(projectRoot, runLog)` writes RunLog JSON to `.agents-reverse-engineer/logs/run-<timestamp>.json` with directory creation and filename sanitization (`/[:.]/g` → `-`).\n\n## Architecture\n\n**Single-Run Lifecycle:**\n1. CLI invokes `new TelemetryLogger(runId)` with ISO timestamp, captures `startTime`\n2. AIService calls `addEntry(entry)` after each subprocess completion, `setFilesReadOnLastEntry(filesRead)` after file metadata attachment\n3. CLI invokes `toRunLog()` to freeze in-memory state into RunLog structure with computed summary\n4. `writeRunLog()` persists JSON to disk with 2-space indentation\n5. `cleanupOldLogs(projectRoot, config.ai.telemetry.keepRuns)` removes oldest files beyond retention limit\n\n**Cross-Run Retention:**\n- Log filenames encode sortable timestamps via `run-2026-02-07T12-00-00-000Z.json` pattern\n- Lexicographic sort produces correct chronological ordering without timestamp parsing\n- Retention enforced via `entries.slice(keepCount)` after newest-first reversal\n\n## Data Flow\n\n```\nAIService subprocess\n  ↓ (per-call metrics)\nTelemetryLogger.addEntry()\n  ↓ (in-memory accumulation)\nTelemetryLogger.toRunLog()\n  ↓ (RunLog structure with summary)\nwriteRunLog()\n  ↓ (filesystem persistence)\n.agents-reverse-engineer/logs/run-<timestamp>.json\n  ↓ (retention enforcement)\ncleanupOldLogs() removes oldest files\n```\n\n## RunLog Summary Computation\n\n`TelemetryLogger.getSummary()` iterates all `entries[]` to compute:\n- `totalInputTokens`, `totalOutputTokens`, `totalCacheReadTokens`, `totalCacheCreationTokens` (sum of per-call counts)\n- `totalDurationMs` (sum of `latencyMs` values)\n- `errorCount` (count where `error !== undefined`)\n- `totalFilesRead` (sum of `filesRead.length` across entries)\n- `uniqueFilesRead` (cardinality of `Set<string>` from all `file.path` values)\n\nRecomputes on every invocation without caching (safe because typically called once at run end).\n\n## Filesystem Safety\n\n**Filename Sanitization:**\n- ISO 8601 contains `:` (Windows path separator) and `.` (shell metacharacter)\n- `runLog.startTime.replace(/[:.]/g, '-')` produces cross-platform-safe filenames\n- Example: `2026-02-07T12:00:00.000Z` → `run-2026-02-07T12-00-00-000Z.json`\n\n**Directory Creation:**\n- `fs.mkdir()` with `recursive: true` creates `.agents-reverse-engineer/logs/` if absent\n- `cleanupOldLogs()` catches ENOENT from `fs.readdir()` and returns `0` when logs directory missing\n\n## Integration Points\n\n- **TelemetryLogger** threaded via `CommandRunOptions.telemetryLogger` → AIService constructor → per-call hooks\n- **writeRunLog** called by command runners after run completion when `config.ai.telemetry.enabled === true`\n- **cleanupOldLogs** invoked immediately after `writeRunLog()` with `config.ai.telemetry.keepRuns` (default 50)\n- Complements `src/orchestration/trace.ts` which retains 500 NDJSON trace files via similar lexicographic sort + slice pattern\n\n## Type Dependencies\n\nAll modules import from `../types.js`:\n- `TelemetryEntry`: per-call metrics with `inputTokens`, `outputTokens`, `cacheReadTokens`, `cacheCreationTokens`, `latencyMs`, `error?: string`, `filesRead: FileRead[]`\n- `RunLog`: container with `runId`, `startTime`, `endTime`, `entries: TelemetryEntry[]`, `summary` (aggregate statistics)\n- `FileRead`: file metadata with `path`, `sizeBytes`, `linesRead`\n\n## Behavioral Contracts\n\n**Filename Pattern:**\n```regex\n^run-\\d{4}-\\d{2}-\\d{2}T\\d{2}-\\d{2}-\\d{2}-\\d{3}Z\\.json$\n```\nMatches sanitized ISO 8601 timestamps where `:` and `.` replaced with `-`.\n\n**Log File Filtering:**\n```typescript\nname.startsWith('run-') && name.endsWith('.json')\n```\nIsolates run log files from other potential artifacts in `.agents-reverse-engineer/logs/`.\n\n**Timestamp Sanitization Transform:**\n```javascript\nrunLog.startTime.replace(/[:.]/g, '-')\n```\nConverts ISO 8601 string to filesystem-safe filename component.\n### src/change-detection/AGENTS.md\n\n<!-- Generated by agents-reverse-engineer -->\n\n# src/change-detection\n\nGit-based change detection and SHA-256 content hashing for incremental documentation updates. Parses `git diff --name-status -M` with rename detection, merges uncommitted working tree changes, and computes file content hashes for `.sum` frontmatter verification.\n\n## Contents\n\n### Core Implementation\n\n**[detector.ts](./detector.ts)** — Implements `isGitRepo()`, `getCurrentCommit()`, `getChangedFiles()` with git diff parsing, `computeContentHash()` and `computeContentHashFromString()` for SHA-256 hex digest generation. Parses status codes (`'A'`/`'M'`/`'D'`/`'R'` prefix) from `git diff --name-status -M` (50% similarity threshold), merges `status.modified[]`, `status.deleted[]`, `status.not_added[]`, `status.staged[]` when `includeUncommitted` is true. Returns `ChangeDetectionResult` with `currentCommit`, `baseCommit`, `changes[]`, `includesUncommitted`.\n\n**[types.ts](./types.ts)** — Defines `ChangeType` union (`'added' | 'modified' | 'deleted' | 'renamed'`), `FileChange` interface with `path`, `status`, optional `oldPath` (renames), optional `contentHash`, `ChangeDetectionResult` interface with `currentCommit`, `baseCommit`, `changes[]`, `includesUncommitted`, and `ChangeDetectionOptions` interface with `includeUncommitted` flag.\n\n**[index.ts](./index.ts)** — Barrel export re-exporting `isGitRepo()`, `getCurrentCommit()`, `getChangedFiles()`, `computeContentHash()`, `computeContentHashFromString()` from `detector.ts` and all types from `types.ts`.\n\n## Integration Points\n\n- **src/update/orchestrator.ts**: Calls `getChangedFiles()` with `includeUncommitted` flag, compares `FileChange.contentHash` against `.sum` YAML frontmatter `content_hash` field, builds `filesToAnalyze` and `filesToSkip` sets, uses `ChangeType === 'deleted' | 'renamed'` for orphan cleanup targeting.\n- **src/generation/writers/sum.ts**: Calls `computeContentHash()` before writing `.sum` files to populate YAML frontmatter `content_hash` field.\n- **src/update/orphan-cleaner.ts**: Uses `FileChange.oldPath` (renames) and `FileChange.path` (deletes) to identify stale `.sum` files requiring removal.\n\n## Behavioral Contracts\n\n**Git diff parsing:** Splits `git diff --name-status -M` output on newlines, splits lines on tab separator. Status codes map to `ChangeType`:\n- `'A'` → `status: 'added'`\n- `'M'` → `status: 'modified'`\n- `'D'` → `status: 'deleted'`\n- `startsWith('R')` → `status: 'renamed'` with `oldPath = parts[1]`, `path = parts[parts.length - 1]`\n\n**SHA-256 hashing:** Uses `crypto.createHash('sha256').update(content).digest('hex')` producing lowercase hex string (64 characters). Matches YAML frontmatter pattern `content_hash: [a-f0-9]{64}`.\n\n**Uncommitted merge:** When `includeUncommitted` is true, deduplicates via `changes.some(c => c.path === file)` before pushing `status.modified[]`, `status.deleted[]`, `status.not_added[]`, `status.staged[]` entries.\n### src/cli/AGENTS.md\n\n<!-- Generated by agents-reverse-engineer -->\n\n# src/cli\n\nCLI command entry points for the agents-reverse-engineer tool, routing parsed arguments to specialized command handlers and orchestrating multi-phase documentation generation workflows.\n\n## Contents\n\n### [clean.ts](./clean.ts)\n`cleanCommand()` deletes generated artifacts (`.sum`, `AGENTS.md` with marker, `CLAUDE.md`, `GENERATION-PLAN.md`) via `fast-glob` discovery, filters `AGENTS.md` by `GENERATED_MARKER` presence, restores `AGENTS.local.md` backups, preserves user-authored files.\n\n### [discover.ts](./discover.ts)\n`discoverCommand()` executes file discovery via `discoverFiles()` filter chain, builds `GenerationPlan` via `createOrchestrator().createPlan()`, transforms via `buildExecutionPlan()` post-order traversal, writes markdown to `GENERATION-PLAN.md`, emits `discovery:start/end` trace events with `filesIncluded/filesExcluded/durationMs` metrics.\n\n### [generate.ts](./generate.ts)\n`generateCommand()` orchestrates three-phase pipeline: resolves AI backend via `resolveBackend()`, instantiates `AIService` with timeout/retry/model config, executes `runner.executeGenerate()` for concurrent file analysis → directory aggregation → root synthesis, writes telemetry/traces, exits with status 0/1/2 for success/partial/total failure.\n\n### [index.ts](./index.ts)\nMain entry point (`#!/usr/bin/env node`) parses `process.argv` via `parseArgs()`, routes to command handlers (`initCommand`, `cleanCommand`, `discoverCommand`, `generateCommand`, `updateCommand`, `specifyCommand`), invokes `runInstaller()` for install/uninstall/interactive modes, prints version via `getVersion()` and usage via `USAGE` constant.\n\n### [init.ts](./init.ts)\n`initCommand()` creates `.agents-reverse-engineer/config.yaml` via `writeDefaultConfig()`, checks existence via `configExists()`, warns on `force: false` when config exists, exits on `EACCES/EPERM` errors, logs guidance referencing `exclude.patterns`, `ai.concurrency`, `ai.backend`.\n\n### [specify.ts](./specify.ts)\n`specifyCommand()` synthesizes project specifications from `AGENTS.md` corpus: collects docs via `collectAgentsDocs()`, auto-invokes `generateCommand()` if no docs exist, resolves backend via `resolveBackend()`, constructs prompt via `buildSpecPrompt()`, calls `AIService.call()` with extended timeout (≥600,000ms), writes output via `writeSpec()`, logs telemetry with token/duration summary.\n\n### [update.ts](./update.ts)\n`updateCommand()` executes incremental update workflow: generates `UpdatePlan` via `createUpdateOrchestrator().preparePlan()` with SHA-256 hash comparison, spawns concurrent AI analysis via `runner.executeUpdate()` for `filesToAnalyze`, regenerates `AGENTS.md` sequentially for `affectedDirs`, finalizes telemetry via `aiService.finalize()`, exits with code 0/1/2 for success/partial/total failure.\n\n## Command Routing Logic\n\n`index.ts` implements dual argument parsing paths: (1) interactive installer mode when `args.length === 0`, (2) explicit command routing when first non-flag arg matches `install|uninstall|init|clean|discover|generate|update|specify`. Installer flags (`--global/-g`, `--local/-l`, `--runtime`, `--force`) trigger `runInstaller()` with `parseInstallerArgs()`. Short flags (`-h`, `-g`, `-l`, `-V`) mapped to long forms (`help`, `global`, `local`, `version`) via inline switch statement. Flags parsed into `Set<string>` for boolean presence checks, `Map<string, string>` for key-value pairs (`--concurrency 3`, `--output ./path`). Positional args extracted after command via filter for non-flag strings (missing leading `--` or `-`).\n\n## Shared Options Pattern\n\n`GenerateOptions` interface defines six fields: `dryRun`, `concurrency`, `failFast`, `debug`, `trace`, reused by `generate.ts`/`update.ts`. `UpdateCommandOptions` extends with `uncommitted` boolean for working tree inclusion. `SpecifyOptions` adds `output` string, `force` boolean, `multiFile` boolean for directory-split output. `CleanOptions` contains only `dryRun`. Option construction in `index.ts` uses `flags.has()` for boolean checks, `values.get()` with `parseInt()` for numeric coercion, `values.get()` for string paths.\n\n## Progress Logging Strategy\n\n`discover.ts`, `generate.ts`, `update.ts`, `specify.ts` create `ProgressLog` instances via `ProgressLog.create(absolutePath)` writing to `.agents-reverse-engineer/progress.log` with ISO 8601 timestamp headers (`=== ARE <Command> (${new Date().toISOString()}) ===`), project path, file counts, phase summaries. `CommandRunner` receives `progressLog` in constructor, streams task pickup/completion events with ETA calculations. Finalized via `progressLog.finalize()` before exit. Designed for `tail -f` monitoring pattern (documented in CLAUDE.md \"Progress log\" section).\n\n## Trace Integration\n\n`generate.ts`, `update.ts`, `discover.ts` support `--trace` flag enabling NDJSON trace emission via `createTraceWriter(absolutePath, options.trace ?? false)` called before config loading. Tracer passed to `loadConfig()`, `createUpdateOrchestrator()`, `CommandRunner`, `discoverFiles()` as `options.tracer`. Events include `phase:start/end`, `worker:start/end`, `task:pickup/done`, `subprocess:spawn/exit`, `retry`, `discovery:start/end`. Finalized via `tracer.finalize()` followed by `cleanupOldTraces(absolutePath)` enforcing 500-file retention limit. Trace paths logged to stderr via `pc.dim('[trace] Writing to <path>')`.\n\n## Backend Resolution Workflow\n\n`generate.ts`, `update.ts`, `specify.ts` resolve AI backend via four-step pattern: (1) create registry via `createBackendRegistry()`, (2) call `resolveBackend(registry, config.ai.backend)` returning first available CLI (Claude/Gemini/OpenCode) or throwing `AIServiceError` with `code: 'CLI_NOT_FOUND'`, (3) catch error and print installation instructions via `getInstallInstructions(registry)` to stderr, (4) call `process.exit(2)` on CLI absence. Instantiate `AIService` with backend and config options `{ timeoutMs, maxRetries, model, telemetry }`, enable debug mode via `aiService.setDebug(true)` if `options.debug`, configure subprocess log directory via `aiService.setSubprocessLogDir(logDir)` if `options.trace`.\n\n## Exit Code Strategy\n\nCommands use three exit codes consistently: **0** for success (all files succeeded or no files to process), **1** for partial failure (`summary.filesFailed > 0 && summary.filesProcessed > 0`), **2** for total failure (`summary.filesProcessed === 0 && summary.filesFailed > 0`) or CLI not found during backend resolution. `update.ts` exits early with implicit code 0 on first run detection (`plan.isFirstRun`) or no changes (`filesToAnalyze.length === 0 && cleanup counts === 0`). `specify.ts` exits with code 1 on `SpecExistsError` during `writeSpec()`, code 2 on `AIServiceError` with `CLI_NOT_FOUND`. `init.ts` exits with code 1 on `EACCES/EPERM` during `writeDefaultConfig()`.\n\n## Dry-Run Mode Handling\n\n`generate.ts` shows plan summary via `formatPlan()` and task breakdown via `buildExecutionPlan()` without backend resolution when `options.dryRun === true`. `update.ts` displays plan via `formatPlan(plan)` including baseline commit, file statuses (`+` added, `R` renamed, `M` modified, `=` unchanged), cleanup actions, affected directories, exits before AI service instantiation. `specify.ts` computes `estimatedTokensK = Math.ceil(totalChars / 4) / 1000`, prints doc count/token estimate/output path/mode table, warns if exceeds 150K tokens. `clean.ts` enumerates artifacts to delete with `pc.dim()` paths, skips `unlink()`/`rename()` calls, prints `Dry run — no files were changed.` in yellow.\n\n## Telemetry Finalization\n\n`generate.ts`, `update.ts`, `specify.ts` finalize telemetry via `aiService.finalize(absolutePath)` returning `summary` with fields `totalInputTokens`, `totalOutputTokens`, `totalDurationMs`, `filesRead[]`. Log summary line formatted as `Tokens: ${totalInputTokens} in / ${totalOutputTokens} out | Duration: ${(totalDurationMs / 1000).toFixed(1)}s | Output: ${outputPath}`. `update.ts` records run state via `orchestrator.recordRun(currentCommit, filesProcessed, filesSkipped)` after finalization (legacy no-op since frontmatter hash migration).\n\n## Auto-Generation Fallback\n\n`specify.ts` checks `docs.length === 0` after `collectAgentsDocs()`, invokes `generateCommand(targetPath, { debug, trace })` when no AGENTS.md files exist and not in dry-run mode, re-collects docs after generation, exits with code 1 if still zero. Prints `No AGENTS.md files found after generation. Cannot proceed.` in red. Prevents specification synthesis with empty corpus.\n### src/config/AGENTS.md\n\n<!-- Generated by agents-reverse-engineer -->\n\n# src/config\n\nConfiguration management layer providing YAML-based project configuration with Zod schema validation, adaptive concurrency computation based on system resources, and default exclusion rules for file discovery.\n\n## Contents\n\n### Core Modules\n\n**[schema.ts](./schema.ts)** — Exports `ConfigSchema` (root Zod schema accepting empty object), `ExcludeSchema` (patterns/vendorDirs/binaryExtensions arrays), `OptionsSchema` (followSymlinks/maxFileSize), `OutputSchema` (colors boolean), `AISchema` (backend enum/model/timeoutMs/maxRetries/concurrency/telemetry with dynamic default via `getDefaultConcurrency()` invocation), and inferred TypeScript types `Config`, `ExcludeConfig`, `OptionsConfig`, `OutputConfig`, `AIConfig`.\n\n**[loader.ts](./loader.ts)** — Exports `loadConfig(root, options)` reading `.agents-reverse-engineer/config.yaml` via YAML parsing and `ConfigSchema.parse()` validation (returns defaults on ENOENT, emits `config:loaded` trace event, throws `ConfigError` on ZodError/parse failures), `configExists(root)` checking file presence via `fs.access()`, `writeDefaultConfig(root)` generating commented YAML template with inline defaults, `ConfigError` class extending Error with `filePath` and `cause` properties, and constants `CONFIG_DIR` ('.agents-reverse-engineer'), `CONFIG_FILE` ('config.yaml').\n\n**[defaults.ts](./defaults.ts)** — Exports `getDefaultConcurrency()` computing worker pool size via formula `clamp(cores * 5, 2, min(memCap, 20))` where `memCap = floor((totalMemGB * 0.5) / 0.512)` caps memory usage at 50% of RAM divided by 512MB subprocess heap budget, constants `DEFAULT_VENDOR_DIRS` (18 directories: node_modules/.git/dist/build/__pycache__/.next/venv/.venv/target/.cargo/.gradle/.agents-reverse-engineer/.agents/.planning/.claude/.opencode/.gemini/vendor), `DEFAULT_EXCLUDE_PATTERNS` (26 globs: AI docs/lock files/dotfiles), `DEFAULT_BINARY_EXTENSIONS` (26 extensions: images/archives/executables/media/documents/fonts/compiled), `DEFAULT_MAX_FILE_SIZE` (1MB threshold), and `DEFAULT_CONFIG` object spreading all default arrays.\n\n## Configuration Schema Hierarchy\n\nConfigSchema composes four nested schemas via Zod object merging:\n\n1. **exclude** (ExcludeSchema) — File/directory filtering rules\n   - `patterns: string[]` — Gitignore-style globs (default: 26 patterns excluding AGENTS.md/CLAUDE.md/lock files/*.sum/*.log)\n   - `vendorDirs: string[]` — Third-party directories to skip (default: 18 directories)\n   - `binaryExtensions: string[]` — Non-text file extensions (default: 26 extensions)\n\n2. **options** (OptionsSchema) — Discovery traversal behavior\n   - `followSymlinks: boolean` — Enable symlink following (default: false)\n   - `maxFileSize: number` — Binary detection threshold in bytes (default: 1048576 = 1MB)\n\n3. **output** (OutputSchema) — CLI rendering preferences\n   - `colors: boolean` — Enable ANSI color codes (default: true)\n\n4. **ai** (AISchema) — AI service orchestration parameters\n   - `backend: 'claude' | 'gemini' | 'opencode' | 'auto'` — CLI backend selection (default: 'auto')\n   - `model: string` — Backend-specific model identifier (default: 'sonnet')\n   - `timeoutMs: number` — Subprocess timeout milliseconds (default: 300000 = 5 minutes)\n   - `maxRetries: number` — Exponential backoff retry attempts (default: 3, min: 0)\n   - `concurrency: number` — Worker pool size (default: dynamic via `getDefaultConcurrency()`, min: 1, max: 20)\n   - `telemetry.keepRuns: number` — Run log retention count (default: 50, min: 0)\n\n## Adaptive Concurrency Algorithm\n\n`getDefaultConcurrency()` implements memory-aware concurrency calculation:\n\n1. **CPU-based baseline**: `cores * 5` where `cores = os.availableParallelism() || os.cpus().length`\n2. **Memory capacity ceiling**: `memCap = floor((totalMemGB * 0.5) / 0.512)` allocating 50% of system RAM with 512MB per subprocess\n3. **Clamping**: `max(2, min(baseline, memCap, 20))` enforcing [2, 20] interval\n4. **Edge case handling**: Returns Infinity when `totalMemGB <= 1` to bypass memory constraint on low-RAM systems\n\nConstants:\n- `CONCURRENCY_MULTIPLIER = 5` (CPU core scaling factor)\n- `MIN_CONCURRENCY = 2` (floor matching WSL default)\n- `MAX_CONCURRENCY = 20` (ceiling matching Zod `.max(20)` constraint)\n- `SUBPROCESS_HEAP_GB = 0.512` (512MB heap budget from `NODE_OPTIONS='--max-old-space-size=512'` in subprocess.ts)\n- `MEMORY_FRACTION = 0.5` (allocate 50% of total RAM to pool)\n\n## Configuration Loading Workflow\n\n`loadConfig(root, options)` executes sequence:\n\n1. **Path resolution**: Constructs `${root}/.agents-reverse-engineer/config.yaml`\n2. **File read**: `readFile(configPath, 'utf-8')` with ENOENT catch returning `ConfigSchema.parse({})`\n3. **YAML parsing**: `parse(rawYaml)` with Error catch wrapped in ConfigError\n4. **Schema validation**: `ConfigSchema.parse(raw)` with ZodError catch formatted as `${path}: ${message}`\n5. **Trace emission**: `tracer?.emit({ type: 'config:loaded', configPath, model, concurrency })`\n6. **Debug output**: `console.error(pc.dim(JSON.stringify(config, null, 2)))` when `options.debug` enabled\n\nError paths throw ConfigError with:\n- `message`: Formatted validation errors or YAML parse message\n- `filePath`: Absolute config file path\n- `cause`: Underlying ZodError or Error instance\n\n## Default Configuration Template\n\n`writeDefaultConfig(root)` generates YAML with four comment-delimited sections:\n\n```yaml\n# FILE & DIRECTORY EXCLUSIONS\nexclude:\n  patterns: [...26 quoted globs...]\n  vendorDirs: [...18 directory names...]\n  binaryExtensions: [...26 extensions...]\n\n# DISCOVERY OPTIONS\noptions:\n  followSymlinks: false\n  maxFileSize: 1048576\n\n# OUTPUT FORMATTING\noutput:\n  colors: true\n\n# AI SERVICE CONFIGURATION\nai:\n  backend: auto\n  model: sonnet\n  timeoutMs: 300000\n  maxRetries: 3\n  # concurrency: <current machine default>\n  telemetry:\n    keepRuns: 50\n```\n\n`yamlScalar(value)` helper quotes strings containing YAML metacharacters (`[*{}\\[\\]?,:#&!|>'\"%@` ]`) via regex `/[*{}\\[\\]?,:#&!|>'\"%@`]/` test, escapes backslashes (`\\\\` → `\\\\\\\\`) and quotes (`\"` → `\\\\\"`) before wrapping in double quotes.\n\n## Integration with Codebase\n\n**Consumed by**:\n- `src/cli/init.ts` — Calls `writeDefaultConfig()` for initialization command\n- `src/cli/generate.ts`, `src/cli/update.ts`, `src/cli/discover.ts` — Call `loadConfig()` to retrieve runtime configuration\n- `src/orchestration/pool.ts` — Uses `config.ai.concurrency` for worker count\n- `src/ai/service.ts` — Uses `config.ai.backend`, `config.ai.model`, `config.ai.timeoutMs`, `config.ai.maxRetries`\n- `src/discovery/walker.ts` — Uses `config.exclude.*` and `config.options.*` for file filtering\n- `src/output/logger.ts` — Uses `config.output.colors` for picocolors enablement\n- `src/ai/telemetry/cleanup.ts` — Uses `config.ai.telemetry.keepRuns` for log retention\n\n**Provides types to**:\n- All CLI entry points requiring type-safe configuration access\n- Orchestration modules needing concurrency/timeout parameters\n- Discovery filters requiring exclusion rule arrays\n\n## Default Exclusion Patterns\n\n**DEFAULT_EXCLUDE_PATTERNS** (26 globs):\n- AI docs: `AGENTS.md`, `CLAUDE.md`, `OPENCODE.md`, `GEMINI.md` (root + recursive `**/*`)\n- Lock files: `*.lock`, `package-lock.json`, `yarn.lock`, `pnpm-lock.yaml`, `bun.lock`, `bun.lockb`, `Gemfile.lock`, `Cargo.lock`, `poetry.lock`, `composer.lock`, `go.sum`\n- Dotfiles: `.gitignore`, `.gitattributes`, `.gitkeep`, `.env` (root + recursive `**/.env*`)\n- Artifacts: `*.log`, `*.sum` (root + recursive), `**/SKILL.md`\n\n**DEFAULT_VENDOR_DIRS** (18 directories):\n- Package managers: `node_modules`, `vendor`\n- Build outputs: `dist`, `build`, `target`, `.next`, `__pycache__`\n- Version control: `.git`\n- Python venvs: `venv`, `.venv`\n- Cargo/Gradle caches: `.cargo`, `.gradle`\n- AI assistant directories: `.agents-reverse-engineer`, `.agents`, `.planning`, `.claude`, `.opencode`, `.gemini`\n\n**DEFAULT_BINARY_EXTENSIONS** (26 extensions):\n- Images: `.png`, `.jpg`, `.jpeg`, `.gif`, `.bmp`, `.ico`, `.webp`\n- Archives: `.zip`, `.tar`, `.gz`, `.rar`, `.7z`\n- Executables: `.exe`, `.dll`, `.so`, `.dylib`\n- Media: `.mp3`, `.mp4`, `.wav`\n- Documents: `.pdf`\n- Fonts: `.woff`, `.woff2`, `.ttf`, `.eot`\n- Compiled: `.class`, `.pyc`\n### src/discovery/AGENTS.md\n\n<!-- Generated by agents-reverse-engineer -->\n\n# src/discovery\n\nFile discovery pipeline orchestrating four-filter chain execution (gitignore → vendor → binary → custom) after fast-glob directory traversal, returning attributed inclusion/exclusion results.\n\n## Contents\n\n**[run.ts](./run.ts)** — Exports `discoverFiles()` facade creating filter chain (GitignoreFilter → VendorFilter → BinaryFilter → CustomPatternFilter) via factory functions, invoking `walkDirectory()` with symlink control, applying filters via `applyFilters()` with trace/debug propagation, returning `FilterResult` with per-file attribution.\n\n**[types.ts](./types.ts)** — Defines `FileFilter` interface (`name: string`, `shouldExclude(path, stats?): boolean | Promise<boolean>`), `ExcludedFile` record (`path`, `reason`, `filter`), `FilterResult` aggregate (`included: string[]`, `excluded: ExcludedFile[]`), `WalkerOptions` config (`cwd`, `followSymlinks?`, `dot?`).\n\n**[walker.ts](./walker.ts)** — Implements `walkDirectory(options: WalkerOptions): Promise<string[]>` via `fg.glob('**/*', {absolute: true, onlyFiles: true, suppressErrors: true, ignore: ['**/.git/**']})` with cwd-relative traversal, dotfile inclusion (default true), symlink control (default false).\n\n## Subdirectories\n\n**[filters/](./filters/)** — Four filter implementations: `binary.ts` (extension Set + size threshold + content analysis), `gitignore.ts` (`.gitignore` parser via `ignore` library), `vendor.ts` (Set lookup for single-segment + substring for path patterns), `custom.ts` (user-defined glob patterns). Orchestrator `index.ts` exports `applyFilters()` with 30-worker concurrency, short-circuit evaluation, per-filter statistics (`matched`/`rejected`), `filter:applied` trace events.\n\n## Architecture\n\n### Discovery Pipeline Flow\n\n1. **Configuration** — Caller (CLI commands) loads `DiscoveryConfig` subset from full config schema via `loadConfig()`\n2. **Filter Creation** — `discoverFiles()` constructs four filters in deterministic order: `createGitignoreFilter(root)`, `createVendorFilter(config.exclude.vendorDirs)`, `createBinaryFilter({maxFileSize, additionalExtensions: config.exclude.binaryExtensions})`, `createCustomFilter(config.exclude.patterns, root)`\n3. **Directory Walking** — Delegates to `walkDirectory({cwd: root, followSymlinks: config.options.followSymlinks})` returning absolute paths via fast-glob\n4. **Filter Application** — Invokes `applyFilters(files, filters, {tracer, debug})` from `filters/index.ts` executing short-circuit filter chain across 30 concurrent workers\n5. **Result Aggregation** — Returns `FilterResult` with `included` files passing all filters, `excluded` array containing `ExcludedFile` records with `filter`/`reason` attribution\n\n### Path Normalization Contract\n\nFilters receive absolute paths from walker, convert to relative via `path.relative(normalizedRoot, absolutePath)` for pattern matching (gitignore/custom patterns). Paths outside root (relative path starts with `..`) bypass filtering. Walker emits absolute paths via `fast-glob` option `{absolute: true}`.\n\n### Filter Interface Polymorphism\n\n`FileFilter.shouldExclude()` supports both sync/async implementations via union return type `boolean | Promise<boolean>`. Binary filter uses async `fs.stat()` + `isbinaryfile.isBinaryFile()`, others use sync string matching. Filter chain executor awaits all promises via `Promise.resolve()` wrapper.\n\n### Configuration Structural Typing\n\n`DiscoveryConfig` interface in `run.ts` defines minimal subset of full `Config` schema from `src/config/schema.ts` containing only `exclude.*`, `options.maxFileSize`, `options.followSymlinks` fields. Enables unit testing with minimal mock objects, prevents tight coupling to entire config schema.\n\n## Integration Points\n\n**Callers**: `src/cli/discover.ts`, `src/cli/generate.ts`, `src/cli/update.ts` invoke `discoverFiles()` with config from `loadConfig()`.\n\n**Consumers**: `FilterResult.included` feeds Phase 1 file analysis worker pool in `src/generation/executor.ts`, `FilterResult.excluded` populates `GENERATION-PLAN.md` statistics via `PlanTracker.writeGenerationPlan()`.\n\n**Telemetry**: `ITraceWriter` from `src/orchestration/trace.ts` emits `filter:applied` events with per-filter match/rejection counts when `--trace` flag enabled.\n### src/discovery/filters/AGENTS.md\n\n<!-- Generated by agents-reverse-engineer -->\n\n# src/discovery/filters\n\nComposable file exclusion filters implementing gitignore semantics, binary detection, vendor directory skipping, and custom glob patterns for the file discovery pipeline.\n\n## Contents\n\n### Filter Implementations\n\n**[binary.ts](./binary.ts)** — Three-phase binary file detector: extension fast-path lookup in 82-member `BINARY_EXTENSIONS` set (images/archives/executables/media/fonts/bytecode/databases), size threshold comparison via `fs.stat()` (default 1MB), content analysis fallback via `isbinaryfile.isBinaryFile()`.\n\n**[custom.ts](./custom.ts)** — User-defined gitignore-style pattern matcher via `ignore` library, converts absolute paths to relative via `path.relative()`, bypasses filtering for paths outside root (relative path starts with `..`).\n\n**[gitignore.ts](./gitignore.ts)** — `.gitignore` parser consuming `ignore` library, silently passes all paths when `.gitignore` missing, converts absolute paths to relative for pattern matching.\n\n**[vendor.ts](./vendor.ts)** — Vendor directory excluder with dual matching: single-segment Set lookup (`node_modules`, `.git`, `dist`, `build`, `__pycache__`, `.next`, `venv`, `.venv`, `target`) and path-pattern substring search for multi-segment patterns (`.agents/skills`, `apps/vendor`).\n\n**[index.ts](./index.ts)** — Filter chain orchestrator with `applyFilters()` executing short-circuit evaluation (stops at first exclusion) across 30 concurrent workers sharing iterator, collects per-filter statistics (`matched`/`rejected` counts), emits `filter:applied` trace events, re-exports all filter creators.\n\n## Architecture\n\n### Filter Chain Execution\n\n`applyFilters()` bounds concurrency to 30 workers via `Math.min(CONCURRENCY, files.length)` to prevent file descriptor exhaustion during binary detection I/O. Each worker drains shared `files.entries()` iterator, processes files serially through filter array with short-circuit semantics (breaks on first `shouldExclude()` true return). Results sorted by original index to preserve input order despite concurrent execution.\n\n### Path Normalization Contract\n\nAll filters receive absolute paths, convert to relative via `path.relative(normalizedRoot, absolutePath)` for pattern matching libraries (`ignore` requires relative paths without leading slash). Paths outside root (starting with `..`) bypass filtering by returning `false`. Empty relative paths also bypass exclusion.\n\n### Binary Detection Strategy\n\n`createBinaryFilter()` prioritizes performance via three-tiered approach:\n1. Extension lookup in pre-allocated Set (O(1), no I/O)\n2. Size threshold check via `fs.stat()` (single syscall)\n3. Content analysis via `isbinaryfile` heuristics (reads file header bytes)\n\nReturns `true` on `fs.stat()` failure (file unreadable/missing) to fail-safe exclude corrupt/inaccessible files.\n\n### Vendor Path Pattern Classification\n\n`createVendorFilter()` splits patterns into `singleSegments` Set and `pathPatterns` array based on presence of path separator after normalization (`dir.replace(/[\\\\/]/g, path.sep)`). Single segments use Set membership check, path patterns use substring search. Normalization ensures cross-platform compatibility (Windows backslashes converted to `path.sep`).\n\n## Filter Statistics\n\n`applyFilters()` aggregates statistics in `Map<string, { matched: number; rejected: number }>`:\n- `rejected` increments when filter excludes file\n- `matched` increments for all filters when file passes entire chain (not per individual filter pass)\n\nStatistics emitted as `filter:applied` trace events with `filterName`, `filesMatched`, `filesRejected` fields for downstream effectiveness analysis.\n\n## Behavioral Contracts\n\n### Binary Extension Set (82 entries)\nImages: `.png`, `.jpg`, `.jpeg`, `.gif`, `.bmp`, `.ico`, `.webp`, `.svg`, `.tiff`, `.tif`, `.psd`, `.raw`, `.heif`, `.heic`\nArchives: `.zip`, `.tar`, `.gz`, `.rar`, `.7z`, `.bz2`, `.xz`, `.tgz`\nExecutables: `.exe`, `.dll`, `.so`, `.dylib`, `.bin`, `.msi`, `.app`, `.dmg`\nMedia: `.mp3`, `.mp4`, `.wav`, `.avi`, `.mov`, `.mkv`, `.flac`, `.ogg`, `.webm`, `.m4a`, `.aac`, `.wma`, `.wmv`, `.flv`\nDocuments: `.pdf`, `.doc`, `.docx`, `.xls`, `.xlsx`, `.ppt`, `.pptx`, `.odt`, `.ods`, `.odp`\nFonts: `.woff`, `.woff2`, `.ttf`, `.eot`, `.otf`\nBytecode: `.class`, `.pyc`, `.pyo`, `.o`, `.obj`, `.a`, `.lib`, `.wasm`\nDatabases: `.db`, `.sqlite`, `.sqlite3`, `.mdb`\nMisc: `.ico`, `.icns`, `.cur`, `.deb`, `.rpm`, `.jar`, `.war`, `.ear`\n\n### Default Vendor Directories (10 entries)\n`node_modules`, `vendor`, `.git`, `dist`, `build`, `__pycache__`, `.next`, `venv`, `.venv`, `target`\n\n### Default Max File Size\n`1024 * 1024` (1MB)\n\n### Filter Chain Concurrency\n30 workers (constant `CONCURRENCY` in `index.ts`)\n\n## Integration Points\n\nConsumed by `src/discovery/walker.ts` as composable filter chain during Phase 1 file analysis. Configured via `config.exclude` fields in `.agents-reverse-engineer/config.yaml`:\n- `patterns` → `createCustomFilter()`\n- `vendorDirs` → `createVendorFilter()`\n- `binaryExtensions` → `createBinaryFilter()` (merged with `BINARY_EXTENSIONS`)\n- `.maxFileSize` → `createBinaryFilter()`\n\nImplements `FileFilter` interface from `../types.js` with `name: string` and `shouldExclude(absolutePath: string): Promise<boolean>` signature.\n### src/generation/AGENTS.md\n\n<!-- Generated by agents-reverse-engineer -->\n\n# generation\n\n**Phase orchestration, file/directory task preparation, complexity analysis, prompt construction, `.sum` + `AGENTS.md` I/O, and root document collection for the three-phase AI-driven documentation pipeline.**\n\n## Contents\n\n### [collector.ts](./collector.ts)\n`collectAgentsDocs()` recursively walks project tree via `readdir()` with `withFileTypes: true`, accumulates all `AGENTS.md` files into `Array<{ relativePath, content }>`, skips 13 directories (`node_modules`, `.git`, `.agents-reverse-engineer`, `vendor`, `dist`, `build`, `__pycache__`, `.next`, `venv`, `.venv`, `target`, `.cargo`, `.gradle`), returns lexicographically sorted array via `localeCompare()`.\n\n### [complexity.ts](./complexity.ts)\n`analyzeComplexity()` computes `ComplexityMetrics` from file array: `fileCount`, `directoryDepth` via `path.relative().split(path.sep).length - 1`, `directories` set via `path.dirname()` parent chain walk until root `.`, `files` array passthrough.\n\n### [executor.ts](./executor.ts)\n`buildExecutionPlan()` transforms `GenerationPlan` into `ExecutionPlan` with three task categories: `fileTasks` (format `file:<path>`, outputs `.sum`, zero dependencies), `directoryTasks` (format `dir:<path>`, outputs `AGENTS.md`, depends on child file IDs, sorted by `getDirectoryDepth()` descending for post-order traversal), `rootTasks` (format `root:CLAUDE.md`, depends on all directory IDs). `isDirectoryComplete()` verifies all `.sum` files exist via `sumFileExists()`. `formatExecutionPlanAsMarkdown()` generates `GENERATION-PLAN.md`.\n\n### [orchestrator.ts](./orchestrator.ts)\n`GenerationOrchestrator.createPlan()` executes seven-step pipeline: reads file content via `readFile()`, calls `analyzeComplexity()`, invokes `buildProjectStructure()` for compact tree, calls `buildFilePrompt()` per file with `projectPlan` injection, groups files by `path.dirname()` for directory task creation, concatenates `fileTasks + dirTasks`, clears `PreparedFile.content` fields to free memory, emits `phase:start`/`plan:created`/`phase:end` trace events.\n\n### [types.ts](./types.ts)\nDefines `AnalysisResult` (fields: `summary`, `metadata: SummaryMetadata`), `SummaryMetadata` (fields: `purpose`, `criticalTodos?`, `relatedFiles?`), `SummaryOptions` (fields: `targetLength: 'short'|'standard'|'detailed'`, `includeCodeSnippets: boolean`).\n\n## Subdirectories\n\n### [prompts/](./prompts/)\n`buildFilePrompt()` substitutes `{{FILE_PATH}}`, `{{CONTENT}}`, `{{LANG}}`, selects `FILE_UPDATE_SYSTEM_PROMPT` when `existingSum` present. `buildDirectoryPrompt()` reads `.sum` via `readSumFile()`, collects child `AGENTS.md`, extracts imports via `extractDirectoryImports()`, detects 9 manifest types (`package.json`, `Cargo.toml`, `go.mod`, `pyproject.toml`, `pom.xml`, `build.gradle`, `Gemfile`, `composer.json`, `CMakeLists.txt`/`Makefile`). `buildRootPrompt()` calls `collectAgentsDocs()`, parses root `package.json`, embeds synthesis constraints. Templates enforce density rules, anchor term preservation, behavioral contract extraction (verbatim regex patterns, format strings, magic constants).\n\n### [writers/](./writers/)\n`writeSumFile()` serializes YAML frontmatter (`generated_at`, `content_hash`, `purpose`, `critical_todos`, `related_files`) with adaptive array formatting (inline `[a,b,c]` for ≤3 items <40 chars, multi-line `- item` otherwise). `readSumFile()` parses via dual-pattern regex. `writeAgentsMd()` implements four-step user content preservation: detect ownership via `GENERATED_MARKER` absence, rename to `AGENTS.local.md`, prepend above LLM output with `---` separator. `getSumPath()` returns `${sourcePath}.sum`.\n\n## Architecture\n\n### Three-Phase Pipeline\n\n**Phase 1 (File Analysis):** `GenerationOrchestrator.createFileTasks()` invokes `buildFilePrompt()` per file, embeds content + project structure, populates `AnalysisTask[]` with `type: 'file'`, `systemPrompt`, `userPrompt`. Workers execute concurrently via `src/orchestration/pool.ts`, call `AIService.call()` → `runSubprocess()` → `execFile()` spawning AI CLI subprocesses with resource limits (`NODE_OPTIONS='--max-old-space-size=512'`, `UV_THREADPOOL_SIZE='4'`, `CLAUDE_CODE_DISABLE_BACKGROUND_TASKS='1'`, `--disallowedTools Task`). Results written via `writeSumFile()` to `.sum` paths from `getSumPath()`.\n\n**Phase 2 (Directory Aggregation):** `GenerationOrchestrator.createDirectoryTasks()` groups files via `Map<dirname, PreparedFile[]>`, creates `AnalysisTask[]` with `type: 'directory'`, `directoryInfo: { sumFiles, fileCount }`. Executor sorts tasks by `getDirectoryDepth()` descending (deepest first), waits for child `.sum` files via `isDirectoryComplete()`, constructs prompts via `buildDirectoryPrompt()` consuming `.sum` frontmatter + child `AGENTS.md` + import maps from `extractDirectoryImports()`. Writes via `writeAgentsMd()` preserving user content from `AGENTS.local.md`.\n\n**Phase 3 (Root Synthesis):** Sequential execution (concurrency=1) generates `CLAUDE.md`, `GEMINI.md`, `OPENCODE.md` via `buildRootPrompt()` aggregating all `AGENTS.md` via `collectAgentsDocs()`, parsing root `package.json`, enforcing synthesis-only constraints (no invention). Strips conversational preamble via pattern matching before writing output.\n\n### Dependency Graph\n\n`buildExecutionPlan()` constructs DAG with three layers:\n1. File tasks: zero dependencies, parallel-eligible\n2. Directory tasks: depend on child file IDs (`files.map(f => 'file:${f}')`), sorted by depth descending\n3. Root tasks: depend on all directory IDs\n\n`isDirectoryComplete()` predicate blocks directory processing until all child `.sum` files exist, ensuring post-order traversal guarantees data availability for parent synthesis.\n\n### Memory Management\n\n`createPlan()` clears `PreparedFile.content` fields via cast `(file as { content: string }).content = ''` after embedding content into file task prompts. Comment: \"The runner re-reads files from disk\" during execution phase. Reduces peak memory from O(total codebase size) to O(largest file × concurrency).\n\n## File Relationships\n\n`orchestrator.ts` calls `buildFilePrompt()` from `./prompts/index.js`, `analyzeComplexity()` from `./complexity.js`, `buildProjectStructure()` internally. `executor.ts` imports `sumFileExists()` from `./writers/sum.ts` for completion checks. `collector.ts` supplies `collectAgentsDocs()` to `./prompts/builder.ts` for root prompt construction. `writers/sum.ts` writes files consumed by `writers/agents-md.ts` prompt builder, both consumed by `executor.ts` via `./writers/index.js` barrel export.\n\n## Integration Points\n\nConsumed by `src/orchestration/runner.ts` which invokes `GenerationOrchestrator.createPlan()` → `buildExecutionPlan()` → three-phase execution via worker pools. Prompts passed to `AIService.call()` in `src/ai/service.ts`. Results written to filesystem via `writeSumFile()` + `writeAgentsMd()`. Trace events emitted to `src/orchestration/trace.ts`. Complexity metrics logged via `src/output/logger.ts`. Discovery input from `src/discovery/run.ts` as `DiscoveryResult`. Configuration from `src/config/loader.ts` as `Config`.\n\n## Behavioral Contracts\n\n**GENERATED_MARKER** (writers/agents-md.ts):\n```javascript\n'<!-- Generated by agents-reverse-engineer -->'\n```\n\n**Frontmatter extraction** (writers/sum.ts):\n```javascript\n/^---\\n([\\s\\S]*?)\\n---\\n/\n```\n\n**YAML field patterns**:\n```javascript\n/generated_at:\\s*(.+)/\n/content_hash:\\s*(.+)/\n/purpose:\\s*(.+)/\n/key:\\s*\\[([^\\]]*)\\]/  // Inline arrays\n/key:\\s*\\n((?:\\s+-\\s+.+\\n?)+)/m  // Multi-line arrays\n```\n\n**Array formatting heuristic** (writers/sum.ts):\n- Inline if `length <= 3` and all items `< 40` chars\n- Multi-line otherwise\n\n**Directory depth calculation** (executor.ts):\n```javascript\ndir.split(path.sep).length  // Returns 0 for '.', 1 for 'src', 2 for 'src/cli'\n```\n\n**Post-order sort comparator** (executor.ts):\n```javascript\n(a, b) => getDirectoryDepth(dirB) - getDirectoryDepth(dirA)  // Descending\n```\n\n**Task ID formats** (executor.ts):\n- File: `file:<relativePath>`\n- Directory: `dir:<relativePath>`\n- Root: `root:<filename>`\n\n**Skip directories** (collector.ts):\n```javascript\nSet(['node_modules', '.git', '.agents-reverse-engineer', 'vendor', 'dist', 'build', '__pycache__', '.next', 'venv', '.venv', 'target', '.cargo', '.gradle'])\n```\n\n**Manifest detection** (prompts/builder.ts):\n```javascript\n['package.json', 'Cargo.toml', 'go.mod', 'pyproject.toml', 'pom.xml', 'build.gradle', 'Gemfile', 'composer.json', 'CMakeLists.txt', 'Makefile']\n```\n\n**Source file filter for imports** (prompts/builder.ts):\n```javascript\n/\\.(ts|tsx|js|jsx|py|go|rs|java|kt)$/\n```\n\n**Language extension map** (prompts/builder.ts):\n22 extensions: `.ts` → `'typescript'`, `.py` → `'python'`, `.rs` → `'rust'`, etc., defaults to `'text'`.\n### src/generation/prompts/AGENTS.md\n\n<!-- Generated by agents-reverse-engineer -->\n\n# generation/prompts\n\n**This directory exports `buildFilePrompt()`, `buildDirectoryPrompt()`, `buildRootPrompt()` for constructing system+user prompt pairs fed to AI CLI subprocesses during three-phase documentation generation: per-file `.sum` analysis, directory-level `AGENTS.md` aggregation, and root integration document synthesis (`CLAUDE.md`, `GEMINI.md`, `OPENCODE.md`).**\n\n## Contents\n\n### [builder.ts](./builder.ts)\nImplements `buildFilePrompt()` (substitutes `{{FILE_PATH}}`, `{{CONTENT}}`, `{{LANG}}`, `{{PROJECT_PLAN_SECTION}}` placeholders, appends contextFiles, selects `FILE_UPDATE_SYSTEM_PROMPT` when `context.existingSum` present), `buildDirectoryPrompt()` (reads `.sum` files via `readSumFile()`, collects child `AGENTS.md`, extracts imports via `extractDirectoryImports()`, detects 9 manifest types: `package.json`, `Cargo.toml`, `go.mod`, `pyproject.toml`, `pom.xml`, `build.gradle`, `Gemfile`, `composer.json`, `CMakeLists.txt`, `Makefile`), `buildRootPrompt()` (calls `collectAgentsDocs()`, parses root `package.json`, embeds synthesis constraints), and `detectLanguage()` (maps 22 file extensions to syntax identifiers).\n\n### [index.ts](./index.ts)\nBarrel export re-exporting `PromptContext`, `SUMMARY_GUIDELINES`, `buildFilePrompt()`, `buildDirectoryPrompt()`, `buildRootPrompt()`, `detectLanguage()` from `types.ts` and `builder.ts` to form public API consumed by `src/generation/executor.ts`.\n\n### [templates.ts](./templates.ts)\nDefines `FILE_SYSTEM_PROMPT` (density rules, anchor term preservation, behavioral contract extraction), `FILE_USER_PROMPT` (placeholders: `{{FILE_PATH}}`, `{{CONTENT}}`), `DIRECTORY_SYSTEM_PROMPT` (path accuracy constraints, adaptive section selection), `FILE_UPDATE_SYSTEM_PROMPT` (incremental preservation directives), `DIRECTORY_UPDATE_SYSTEM_PROMPT` (structure preservation), `ROOT_SYSTEM_PROMPT` (synthesis constraints forbidding invention).\n\n### [types.ts](./types.ts)\nExports `PromptContext` (fields: `filePath`, `content`, `contextFiles`, `projectPlan`, `existingSum`) consumed by prompt builders and `SUMMARY_GUIDELINES` (targetLength: 300-500 words, include array mandating behavioral contracts: verbatim regex patterns, format strings, magic constants, sentinel values, environment variables).\n\n## Architecture\n\n### Three-Phase Prompt Construction\n\n**Phase 1: File Analysis**  \n`buildFilePrompt()` receives `PromptContext`, detects language via extension lookup (`detectLanguage()`), substitutes placeholders in `FILE_USER_PROMPT` template, appends related files section when `contextFiles` populated, selects `FILE_UPDATE_SYSTEM_PROMPT` for incremental updates (when `existingSum` present) or `FILE_SYSTEM_PROMPT` for fresh analysis.\n\n**Phase 2: Directory Aggregation**  \n`buildDirectoryPrompt()` reads directory via `readdir()`, filters against `knownDirs` set (skips non-source directories), reads `.sum` files in parallel via `Promise.all(readSumFile(getSumPath(entryPath)))`, collects child `AGENTS.md`, extracts imports from source files matching `/\\.(ts|tsx|js|jsx|py|go|rs|java|kt)$/` via `extractDirectoryImports()`, detects manifest files, checks for user documentation (AGENTS.local.md or non-generated AGENTS.md lacking `GENERATED_MARKER`), assembles sections: file summaries, import map via `formatImportMap()`, project structure, subdirectories, directory hints, user notes, existing AGENTS.md (update mode), returns `DIRECTORY_UPDATE_SYSTEM_PROMPT` when `existingAgentsMd` supplied.\n\n**Phase 3: Root Synthesis**  \n`buildRootPrompt()` calls `collectAgentsDocs()` to recursively gather all `AGENTS.md` files, reads root `package.json` and extracts metadata (name, version, description, packageManager, scripts), embeds all AGENTS.md content as `### ${relativePath}` sections, appends package metadata, includes synthesis constraint: \"Use ONLY the information provided below. Do NOT invent features, hooks, patterns, or APIs that are not explicitly mentioned in the AGENTS.md content.\"\n\n### Template Substitution Pattern\n\nAll prompts follow structure: system prompt (behavioral constraints) + user prompt (context injection). Builder functions perform string interpolation with runtime data:\n- `buildFilePrompt()` injects file path, source code, language identifier, project plan section\n- `buildDirectoryPrompt()` injects `.sum` summaries, import maps, child AGENTS.md, manifest hints, user notes\n- `buildRootPrompt()` injects all AGENTS.md content, package metadata, output requirements checklist\n\n## Behavioral Contracts\n\n### Density Rules (FILE_SYSTEM_PROMPT)\n`\"Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\"`. Banned filler phrases: `\"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"`.\n\n### Anchor Term Preservation (FILE_SYSTEM_PROMPT)\n`\"All exported function/class/type/const names MUST appear in the summary exactly as written in source\"`, `\"Preserve exact casing of identifiers\"`, `\"Missing any exported identifier is a failure\"`.\n\n### Output Format Requirements (FILE_SYSTEM_PROMPT, DIRECTORY_SYSTEM_PROMPT)\n`\"Start your response DIRECTLY with the purpose statement\"` for file prompts. `\"Output ONLY the raw markdown content. No code fences, no preamble\"` for directory prompts. `\"First line MUST be exactly: <!-- Generated by agents-reverse-engineer -->\"` for AGENTS.md.\n\n### Behavioral Contract Extraction (FILE_SYSTEM_PROMPT)\nRequired captures: `\"Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\"`, `\"Format strings, output templates, serialization structures — show exact format\"`, `\"Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\"`, `\"Prompt text or template strings that control AI/LLM behavior\"`, `\"Error message patterns and error code strings\"`, `\"Environment variable names and their expected values\"`, `\"File format specifications (YAML frontmatter schemas, NDJSON line formats)\"`.\n\n### Path Accuracy Constraints (DIRECTORY_SYSTEM_PROMPT)\n`\"When referencing files or modules outside this directory, use ONLY paths from the 'Import Map' section\"`, `\"Do NOT invent, rename, or guess module paths\"`, `\"Use the exact directory names from 'Project Directory Structure' — do NOT rename directories\"`, `\"Cross-module references must use the specifier format from actual import statements\"`, `\"If you are unsure about a path, omit the cross-reference rather than guessing\"`.\n\n### Incremental Update Preservation (FILE_UPDATE_SYSTEM_PROMPT, DIRECTORY_UPDATE_SYSTEM_PROMPT)\n`\"Preserve the structure, section headings, and phrasing of the existing summary wherever the underlying code is unchanged\"`, `\"Only modify content that is directly affected by the code changes\"`, `\"Regex patterns, format strings, magic constants, and template content from the existing summary MUST be preserved verbatim unless the source code changed them\"`, `\"Never summarize or paraphrase regex patterns — always show the exact pattern in backticks\"`.\n\n### Root Synthesis Constraints (ROOT_SYSTEM_PROMPT)\n`\"Synthesize ONLY from the AGENTS.md content provided in the user prompt\"`, `\"Do NOT invent, extrapolate, or hallucinate features, hooks, APIs, patterns, or dependencies not explicitly mentioned\"`, `\"Every claim must be traceable to a specific AGENTS.md file provided\"`.\n\n### Source File Extension Filter (builder.ts)\n`/\\.(ts|tsx|js|jsx|py|go|rs|java|kt)$/` — only files matching these extensions processed by `extractDirectoryImports()` for import map generation.\n\n### Manifest Detection Pattern (builder.ts)\n`['package.json', 'Cargo.toml', 'go.mod', 'pyproject.toml', 'pom.xml', 'build.gradle', 'Gemfile', 'composer.json', 'CMakeLists.txt', 'Makefile']` — presence triggers \"likely a package or project root\" hint in directory prompts.\n\n### Language Extension Map (builder.ts detectLanguage())\n22 extensions: `.ts` → `'typescript'`, `.tsx` → `'tsx'`, `.js` → `'javascript'`, `.jsx` → `'jsx'`, `.py` → `'python'`, `.rb` → `'ruby'`, `.go` → `'go'`, `.rs` → `'rust'`, `.java` → `'java'`, `.kt` → `'kotlin'`, `.swift` → `'swift'`, `.cs` → `'csharp'`, `.php` → `'php'`, `.vue` → `'vue'`, `.svelte` → `'svelte'`, `.json` → `'json'`, `.yaml`/`.yml` → `'yaml'`, `.md` → `'markdown'`, `.css` → `'css'`, `.scss` → `'scss'`, `.html` → `'html'`. Defaults to `'text'`.\n\n## File Relationships\n\n`builder.ts` calls `readSumFile()` + `getSumPath()` from `../writers/sum.js` to retrieve `.sum` frontmatter+content, references `GENERATED_MARKER` from `../writers/agents-md.js` to detect user-authored vs. generated AGENTS.md, invokes `extractDirectoryImports()` + `formatImportMap()` from `../../imports/index.js` for verified import maps, calls `collectAgentsDocs()` from `../collector.js` to aggregate AGENTS.md recursively. `templates.ts` provides raw prompt text consumed by `builder.ts` via import. `types.ts` defines `PromptContext` structure threaded through `builder.ts` functions. `index.ts` re-exports public API consumed by `src/generation/executor.ts` during three-phase pipeline execution.\n\n## Integration Points\n\nConsumed by `src/generation/executor.ts` which invokes prompt builders during phase execution: `buildFilePrompt()` called for each file in Phase 1 concurrent pool, `buildDirectoryPrompt()` called for each directory in Phase 2 post-order traversal, `buildRootPrompt()` called once per root document in Phase 3 sequential execution. Prompts passed as `{ system, user }` pairs to `AIService.call()` in `src/ai/service.ts` which spawns CLI subprocesses (`claude`, `gemini`, `opencode`) with resource limits (`NODE_OPTIONS='--max-old-space-size=512'`, `UV_THREADPOOL_SIZE='4'`, `CLAUDE_CODE_DISABLE_BACKGROUND_TASKS='1'`, `--disallowedTools Task`).\n### src/generation/writers/AGENTS.md\n\n<!-- Generated by agents-reverse-engineer -->\n\n# src/generation/writers\n\n`.sum` file I/O with YAML frontmatter serialization, `AGENTS.md` lifecycle management with user content preservation, and path resolution utilities for Phase 1/Phase 2 writer operations.\n\n## Contents\n\n**[sum.ts](./sum.ts)** — Serializes `SumFileContent` to `.sum` files with YAML frontmatter (`generated_at`, `content_hash`, `purpose`, `critical_todos`, `related_files`), parses frontmatter via regex extraction of inline/multi-line arrays, computes `.sum` paths via `getSumPath()`, validates existence via `sumFileExists()`.\n\n**[agents-md.ts](./agents-md.ts)** — Writes directory-level `AGENTS.md` with `GENERATED_MARKER` ownership detection, preserves user-authored content by renaming to `AGENTS.local.md` and prepending above LLM output, strips marker prefix from incoming content, constructs final structure with optional user content block and `---` separator.\n\n**[index.ts](./index.ts)** — Re-exports `writeSumFile`, `readSumFile`, `getSumPath`, `sumFileExists`, `SumFileContent` from `sum.ts` and `writeAgentsMd` from `agents-md.ts` as public API boundary.\n\n## YAML Frontmatter Format\n\nsum.ts serializes frontmatter with adaptive array formatting: inline `[a, b, c]` for ≤3 items under 40 chars each, multi-line `- item` otherwise. Parses via dual-pattern regex supporting both syntaxes. Mandatory fields: `generated_at` (ISO 8601), `content_hash` (SHA-256 hex), `purpose` (single-line string). Optional arrays: `critical_todos`, `related_files`.\n\n## User Content Preservation\n\nagents-md.ts implements idempotent preservation via four-step workflow:\n\n1. Read existing `AGENTS.md`, detect user authorship via `GENERATED_MARKER` absence, rename to `AGENTS.local.md` if user-authored\n2. Attempt read of already-renamed `AGENTS.local.md` from previous run if step 1 found no user content\n3. Strip `GENERATED_MARKER` prefix from incoming LLM content via `slice()` + `/^\\n+/` normalization\n4. Construct final content: `GENERATED_MARKER` → optional user block with `<!-- User-defined AGENTS.md preserved as AGENTS.local.md -->` header and `---` separator → LLM content, write via `writeFile()` with `mkdir({recursive: true})`\n\nPreserved content prepended above generated content ensures AI assistants encounter project-specific context before auto-generated summaries.\n\n## Path Resolution\n\n`getSumPath(sourcePath)` returns `${sourcePath}.sum` for given source file. No directory transformation — `.sum` files colocate with source files throughout tree. `sumFileExists()` delegates to `readSumFile()` non-null check, avoiding redundant `access()` syscalls.\n\n## Integration with Pipeline Phases\n\n**Phase 1 (File Analysis):** `src/generation/executor.ts` workers call `writeSumFile()` after AI subprocess returns `SumFileContent`, writes to path from `getSumPath(sourceFile.path)`.\n\n**Phase 2 (Directory Aggregation):** `src/generation/orchestrator.ts` calls `readSumFile()` via `collectAgentsDocs()` to load child `.sum` files for prompt construction, calls `writeAgentsMd()` with LLM response from `AIService.call()`.\n\n**Incremental Updates:** `src/update/orphan-cleaner.ts` calls `readSumFile()` to extract `content_hash` from YAML frontmatter for SHA-256 comparison, `cleanupEmptyDirectoryDocs()` uses `isGeneratedAgentsMd()` to determine deletion eligibility.\n\n**Cleanup:** `src/cli/clean.ts` calls `isGeneratedAgentsMd()` to filter deletion targets, skips user-authored files, restores `AGENTS.local.md` → `AGENTS.md` after removing generated version.\n\n## Behavioral Contracts\n\n**GENERATED_MARKER constant:**\n```javascript\n'<!-- Generated by agents-reverse-engineer -->'\n```\n\n**Frontmatter extraction regex (parseSumFile):**\n```javascript\n/^---\\n([\\s\\S]*?)\\n---\\n/\n```\n\n**Field patterns:**\n```javascript\n/generated_at:\\s*(.+)/\n/content_hash:\\s*(.+)/\n/purpose:\\s*(.+)/\n```\n\n**Array parsing patterns (parseYamlArray):**\n```javascript\n/key:\\s*\\[([^\\]]*)\\]/  // Inline: key: [a, b, c]\n/key:\\s*\\n((?:\\s+-\\s+.+\\n?)+)/m  // Multi-line: key:\\n  - item\n```\n\n**Quote stripping (inline arrays):**\n```javascript\n.replace(/^[\"']|[\"']$/g, '')\n```\n\n**Multi-line item prefix:**\n```javascript\n/^\\s*-\\s*/  // Strips leading '  - '\n```\n\n**Array format heuristic (formatYamlArray):**\n- Inline if `values.length <= 3` and all items `< 40` chars\n- Multi-line otherwise\n- Empty: `key: []`\n\n**Newline normalization (writeAgentsMd):**\n```javascript\ncontent.slice(GENERATED_MARKER.length).replace(/^\\n+/, '')\n```\n### src/imports/AGENTS.md\n\n<!-- Generated by agents-reverse-engineer -->\n\n# src/imports\n\n**Static import analysis module extracting TypeScript/JavaScript import statements via regex parsing to construct dependency graphs and import maps for AI-driven documentation prompts.**\n\n## Contents\n\n### Core Extraction\n\n**[extractor.ts](./extractor.ts)** — Regex-based import parser extracting ES module syntax into `ImportEntry[]` arrays. `extractImports()` applies `IMPORT_REGEX` (`/^import\\s+(type\\s+)?(?:\\{([^}]*)\\}|(\\*\\s+as\\s+\\w+)|(\\w+))\\s+from\\s+['\"]([^'\"]+)['\"]/gm`) to capture five groups: (1) `type` keyword, (2) named symbols, (3) namespace imports, (4) default imports, (5) module specifier. `extractDirectoryImports()` scans first 100 lines of each file to optimize import-region parsing, filters bare specifiers (`'react'`, `node:*`), partitions relative imports into `internal` (`./`) and `external` (`../`). `formatImportMap()` serializes `FileImports[]` into LLM prompt text blocks with `(type)` suffix for type-only imports.\n\n**[types.ts](./types.ts)** — Type definitions for import extraction results. `ImportEntry` models single import statement with `specifier`, `symbols[]`, `typeOnly` discriminator. `FileImports` aggregates directory-level imports via `externalImports[]` (cross-directory dependencies) and `internalImports[]` (same-directory coupling). External/internal partitioning enables `AGENTS.md` prompts to emphasize architectural boundaries.\n\n**[index.ts](./index.ts)** — Barrel export exposing `extractImports()`, `extractDirectoryImports()`, `formatImportMap()`, `ImportEntry`, `FileImports` for integration with Phase 2 directory aggregation pipeline (`src/generation/prompts/builder.ts`).\n\n## Integration Points\n\n**Generation Phase 2 (Directory Aggregation):**\n- `src/generation/prompts/builder.ts` calls `extractDirectoryImports()` during `AGENTS.md` synthesis\n- Import maps injected into prompts via `formatImportMap()` output in `src/generation/prompts/templates.ts`\n- External import paths verified against filesystem constraints to prevent phantom references\n\n**Discovery Pipeline:**\n- `src/generation/orchestrator.ts` passes `discoveredFiles[]` list enabling directory-level filtering without redundant scans\n- Skips binary files, vendor directories, custom exclude patterns from upstream discovery filters\n\n**Data Flow:**\n- Runner invokes `extractDirectoryImports(dirPath, fileNames)` → reads first 100 lines → regex extraction → internal/external partitioning → `FileImports[]` return\n- Prompt builder calls `formatImportMap(fileImports)` → text serialization → template injection\n\n## Behavioral Contracts\n\n**IMPORT_REGEX Pattern:**\n```regex\n/^import\\s+(type\\s+)?(?:\\{([^}]*)\\}|(\\*\\s+as\\s+\\w+)|(\\w+))\\s+from\\s+['\"]([^'\"]+)['\"]/gm\n```\n- Group 1: `type` keyword (`import type { Foo }`)\n- Group 2: Named symbols within braces (`{ Foo, Bar as Baz }`)\n- Group 3: Namespace imports (`* as name`)\n- Group 4: Default imports (bare identifier after `import`)\n- Group 5: Module specifier (quoted string after `from`)\n- Anchored `^` for line start, `gm` flags for global multiline\n- Does NOT capture dynamic imports (`import('...')`) or side-effect imports (`import './styles.css'`)\n\n**Format Template (formatImportMap output):**\n```\nrunner.ts:\n  ../ai/index.js → AIService\n  ../generation/executor.js → ExecutionPlan, ExecutionTask\n\npool.ts:\n  ./trace.js → ITraceWriter (type)\n```\n\n**File Read Optimization:**\n- Reads only first 100 lines via `content.split('\\n').slice(0, 100).join('\\n')` before regex scanning\n- Assumption: ES module imports typically concentrated in file header region\n- Trade-off: Skips late-file dynamic import detection for 95%+ file I/O reduction\n\n**Import Classification:**\n- Bare specifiers (`'react'`, `'lodash'`) → filtered out (external packages)\n- `node:` prefixed (`'node:fs'`) → filtered out (built-in modules)\n- Relative starting `./` → `internalImports[]` (same-directory coupling)\n- Relative starting `../` → `externalImports[]` (cross-directory dependencies)\n### src/installer/AGENTS.md\n\n<!-- Generated by agents-reverse-engineer -->\n\n# src/installer\n\nnpx installation orchestrator for deploying ARE command templates and session hooks into AI coding assistant runtime directories (`~/.claude`, `~/.config/opencode`, `~/.gemini`), with interactive prompts for missing parameters, parallel multi-runtime support, hook registration in settings.json, and permission preauthorization for Claude Code.\n\n## Contents\n\n### Core Installation Logic\n\n**[operations.ts](./operations.ts)** — Writes command templates from `src/integration/templates.ts` to runtime directories, copies bundled hook files from `hooks/dist/`, registers hooks in `settings.json` via `registerClaudeHooks()`/`registerGeminiHooks()`, writes `ARE-VERSION` file, adds Bash permission patterns to Claude Code allow list. Exports `installFiles()` fan-out across runtimes, `verifyInstallation()` checking file existence via `existsSync()`, `getPackageVersion()` reading from package.json, `formatInstallResult()` for human-readable output.\n\n**[uninstall.ts](./uninstall.ts)** — Removes command templates, hooks, hook registrations from `settings.json`, Bash permission entries from allow list, and `ARE-VERSION` file. Calls `cleanupAreSkillDirs()` deleting empty `are-*` skill directories for Claude, `cleanupLegacyGeminiFiles()` removing pre-TOML `*.md` files, `cleanupEmptyDirs()` recursively deleting empty parent directories stopping at runtime roots. Exports `uninstallFiles()`, `deleteConfigFolder()` for `.agents-reverse-engineer` removal.\n\n**[paths.ts](./paths.ts)** — Resolves runtime installation paths with environment variable overrides (`CLAUDE_CONFIG_DIR`, `OPENCODE_CONFIG_DIR`, `GEMINI_CONFIG_DIR`). Exports `getRuntimePaths()` returning `RuntimePaths` objects with `global`/`local`/`settingsFile` fields, `resolveInstallPath()` joining runtime paths with project root, `isRuntimeInstalledLocally()`/`isRuntimeInstalledGlobally()` existence checks via `stat()`.\n\n### Workflow Orchestration\n\n**[index.ts](./index.ts)** — Parses CLI args via `parseInstallerArgs()` supporting `-g`/`--global`, `-l`/`--local`, `--runtime`, `--force`, `-u`/`--uninstall`, `-q`/`--quiet`, dispatches to `runInstall()`/`runUninstall()` after resolving runtime and location via `determineRuntimes()`/`determineLocation()` with interactive prompt fallback. Calls `verifyInstallation()` checking created file existence, formats results via `displayInstallResults()`/`displayUninstallResults()` showing file counts, hook registration status, and `showNextSteps()` with ARE command list. Exits with code 1 in non-interactive mode when required flags missing.\n\n**[prompts.ts](./prompts.ts)** — Arrow-key selection in TTY mode via `arrowKeySelect()` with `readline.emitKeypressEvents()`, numbered fallback via `numberedSelect()` for non-TTY. Exports `selectRuntime()` prompting for `'claude'`/`'opencode'`/`'gemini'`/`'all'`, `selectLocation()` for `'global'`/`'local'`, `confirmAction()` for yes/no prompts, `isInteractive()` checking `process.stdin.isTTY`. Enforces raw mode cleanup via `cleanupRawMode()` in try/finally blocks and process exit handlers (`'exit'`, `'SIGINT'`) preventing terminal state corruption.\n\n### Display Formatting\n\n**[banner.ts](./banner.ts)** — Styled terminal output with ASCII art \"ARE\" logo in green via `pc.green()`, version footer via `getVersion()`, usage instructions with `pc.bold()` section headers covering CLI options and example invocations. Exports `displayBanner()`, `showHelp()`, `showNextSteps()` listing ARE commands (`/are-help`, `/are-init`, `/are-discover`, `/are-generate`, `/are-update`, `/are-specify`, `/are-clean`) in cyan, message helpers `showSuccess()`/`showError()`/`showWarning()`/`showInfo()` with prefixed symbols.\n\n### Type Definitions\n\n**[types.ts](./types.ts)** — Defines `Runtime = 'claude' | 'opencode' | 'gemini' | 'all'`, `Location = 'global' | 'local'`, `InstallerArgs` interface with boolean flags and optional runtime field, `InstallerResult` interface tracking `filesCreated[]`, `filesSkipped[]`, `errors[]`, `hookRegistered`, `versionWritten`, `RuntimePaths` interface with `global`/`local`/`settingsFile` path strings.\n\n## Installation Flow\n\n1. **Argument Parsing** — `parseInstallerArgs()` extracts flags, validates `--runtime` against `['claude', 'opencode', 'gemini', 'all']`\n2. **Interactive Prompts** — `determineLocation()` calls `selectLocation()` if both/neither `-g`/`-l` set, `determineRuntimes()` calls `selectRuntime()` if `--runtime` missing\n3. **Path Resolution** — `resolveInstallPath()` combines `getRuntimePaths(runtime)` with project root for local installs, uses global path for `-g`\n4. **File Operations** — `installFilesForRuntime()` calls `getTemplatesForRuntime()` from `src/integration/templates.ts`, writes templates via `ensureDir()` → `writeFileSync()`, copies hooks from `hooks/dist/` via `readBundledHook()` → `writeFileSync()`\n5. **Hook Registration** — `registerHooks()` dispatches to `registerClaudeHooks()` or `registerGeminiHooks()`, merges hooks into `settings.json` under `hooks.SessionStart`/`hooks.SessionEnd`, writes ARE Bash permission patterns via `registerPermissions()` for Claude only\n6. **Verification** — `verifyInstallation()` checks all file paths via `existsSync()`, reports `missing[]` array for absent files\n7. **Display Results** — `displayInstallResults()` accumulates `totalCreated`/`totalSkipped`/`hooksRegistered` counters, calls `showNextSteps()` with ARE command list unless `--quiet` flag set\n\n## Uninstallation Flow\n\n1. **File Deletion** — `uninstallFilesForRuntime()` reads templates for path extraction, deletes command files and hooks via `unlinkSync()`, removes `ARE-VERSION` file\n2. **Hook Cleanup** — `unregisterHooks()` filters `settings.json` event arrays removing hooks matching `getHookPatterns()` for current (`node ${runtimeDir}/hooks/${filename}`) and legacy (`node hooks/${filename}`) formats, deletes empty event arrays and empty `hooks` object\n3. **Permission Cleanup** — `unregisterPermissions()` filters `permissions.allow` removing all `ARE_PERMISSIONS` entries, cleans up empty `permissions.allow`/`permissions` structures\n4. **Directory Cleanup** — `cleanupAreSkillDirs()` removes empty `are-*` skill subdirectories for Claude, `cleanupLegacyGeminiFiles()` deletes pre-TOML `*.md` files and nested `are/*.toml` files, `cleanupEmptyDirs()` recursively deletes empty parents stopping at runtime roots (`.claude`, `.opencode`, `.gemini`, `.config`)\n5. **Config Removal** — `deleteConfigFolder()` removes `.agents-reverse-engineer` directory for local installs via `rmSync({ recursive: true, force: true })`\n\n## Runtime-Specific Behaviors\n\n**Claude Code:**\n- Installs to `~/.claude` (global) or `.claude` (local), overridable via `CLAUDE_CONFIG_DIR`\n- Registers hooks in nested format: `settings.hooks[event] = [{ hooks: [{ type: 'command', command: string }] }]`\n- Adds five Bash permission patterns to `settings.permissions.allow`: `Bash(npx agents-reverse-engineer@latest init*)`, `discover*`, `generate*`, `update*`, `clean*`\n- Cleans up skill directories matching `are-*` prefix during uninstall\n\n**OpenCode:**\n- Installs to `~/.config/opencode` (global) or `.opencode` (local), respects `OPENCODE_CONFIG_DIR` → `XDG_CONFIG_HOME/opencode` → default fallback chain\n- Copies plugins to `plugins/` directory (auto-loaded by OpenCode runtime)\n- No settings.json registration (plugin system auto-discovers)\n- Supports plugin filenames: `are-check-update.js`, `are-session-end.js`\n\n**Gemini:**\n- Installs to `~/.gemini` (global) or `.gemini` (local), overridable via `GEMINI_CONFIG_DIR`\n- Registers hooks in flat format: `settings.hooks[event] = [{ name: string, type: 'command', command: string }]`\n- Cleans up legacy files during uninstall: `are-*.md` (pre-TOML commands), `are/*.toml` (pre-flat structure)\n\n## Behavioral Contracts\n\n**Hook Command Pattern:**\n```\nCurrent: node ${runtimeDir}/hooks/${filename}\nLegacy:  node hooks/${filename}\n```\n\n**Permission Pattern:**\n```\nBash(npx agents-reverse-engineer@latest <command>*)\n```\n\n**Settings JSON Indentation:**\n```javascript\nJSON.stringify(settings, null, 2)  // 2-space indent\n```\n\n**Runtime Root Basenames (recursion stop):**\n```\n.claude, .opencode, .gemini, .config\n```\n\n**Hook File Sources:**\n```\nhooks/dist/are-check-update.js\nhooks/dist/are-session-end.js\nhooks/dist/opencode-are-check-update.js\nhooks/dist/opencode-are-session-end.js\n```\n\n**Version File Location:**\n```\n${basePath}/ARE-VERSION\n```\n### src/integration/AGENTS.md\n\n<!-- Generated by agents-reverse-engineer -->\n\n# src/integration/\n\n**Platform-specific AI assistant integration layer performing environment detection, template generation, and file installation for Claude Code, OpenCode, Gemini CLI, and Aider via filesystem marker scanning and bundled hook deployment.**\n\n## Contents\n\n### Environment Detection\n\n**[detect.ts](./detect.ts)** — Exports `detectEnvironments()` and `hasEnvironment()` for AI runtime discovery via filesystem markers. `detectEnvironments()` scans `projectRoot` for Claude (`.claude/` or `CLAUDE.md`), OpenCode (`.opencode/`), Aider (`.aider.conf.yml` or `.aider/`) using synchronous `existsSync()` checks. Returns `DetectedEnvironment[]` with `type`, `configDir`, `detected` fields. `hasEnvironment()` provides predicate wrapper filtering by `EnvironmentType`.\n\n**[types.ts](./types.ts)** — Defines `EnvironmentType` literal union `'claude' | 'opencode' | 'aider' | 'gemini'`, `DetectedEnvironment` with `type`/`configDir`/`detected` fields, `IntegrationTemplate` with `filename`/`path`/`content` fields, `IntegrationResult` with `environment`/`filesCreated`/`filesSkipped` arrays.\n\n### Template Generation\n\n**[templates.ts](./templates.ts)** — Exports `getClaudeTemplates()`, `getOpenCodeTemplates()`, `getGeminiTemplates()` producing `IntegrationTemplate[]` arrays with platform-specific command files. Defines `COMMANDS` object with six templates (`generate`, `update`, `init`, `discover`, `clean`, `specify`, `help`) containing placeholder substitution for `COMMAND_PREFIX` (`/are-`), `VERSION_FILE_PATH` (`.claude/ARE-VERSION` | `.opencode/ARE-VERSION` | `.gemini/ARE-VERSION`), `$ARGUMENTS`. `PLATFORM_CONFIGS` maps `Platform` to `PlatformConfig` specifying `pathPrefix` (`.claude/skills/` | `.opencode/commands/` | `.gemini/commands/`), `filenameSeparator`, `usesName` (Claude-only YAML frontmatter), `extraFrontmatter` (OpenCode `agent: build`). `buildTemplate()` constructs paths, `buildFrontmatter()` injects YAML, `buildGeminiToml()` generates TOML format. Command content embeds background execution pattern with `run_in_background: true`, `TaskOutput` polling, `.agents-reverse-engineer/progress.log` monitoring via `Read` tool with `offset` parameter. `discover`/`clean` templates include strict rule blocks forbidding flag injection beyond `$ARGUMENTS`.\n\n### File Installation\n\n**[generate.ts](./generate.ts)** — Exports `generateIntegrationFiles(projectRoot, options?)` orchestrating setup via `detectEnvironments()` → `getTemplatesForEnvironment()` → `writeFileSync()` chain. Maps `EnvironmentType` to config directories (`claude` → `.claude`, `opencode` → `.opencode`, `aider` → `.aider`, `gemini` → `.gemini`). Creates parent directories via `ensureDir()` using `mkdirSync({ recursive: true })`. Skips existing files unless `options.force` overrides, populates `IntegrationResult.filesSkipped[]`. For `claude` environment specifically, copies bundled hook `are-session-end.js` from `hooks/dist/` to `.claude/hooks/are-session-end.js` via `readBundledHook()` → `writeFileSync()`. `getBundledHookPath()` resolves hook paths from `dist/integration/` up two levels to project root then into `hooks/dist/{hookName}`. Throws `Error` if bundled hook missing. Returns `IntegrationResult[]` with `filesCreated`/`filesSkipped` per environment. Respects `options.dryRun` for preview mode. Defines `GenerateOptions` interface with `dryRun?`, `force?`, `environment?` fields.\n\n## Integration Points\n\nConsumed by `src/installer/operations.ts` during npx-based installation workflow. `detectEnvironments()` validates target runtime presence before hook registration. Template generators provide command file content for `src/installer/operations.ts` writes. Generated files reference `VERSION_FILE_PATH` for session hooks' version checking logic.\n\n## File Path Patterns\n\n**Claude Code:**\n- Commands: `.claude/skills/are-{command}/SKILL.md`\n- Hooks: `.claude/hooks/are-session-end.js`\n- Version cache: `.claude/ARE-VERSION`\n\n**OpenCode:**\n- Commands: `.opencode/commands/are-{command}.md`\n- Version cache: `.opencode/ARE-VERSION`\n\n**Gemini CLI:**\n- Commands: `.gemini/commands/are-{command}.toml`\n- Version cache: `.gemini/ARE-VERSION`\n\n**Aider:**\n- Markers: `.aider.conf.yml`, `.aider/`\n- No template generation (detection only)\n\n## Hook Bundling Convention\n\nHooks copied from `hooks/dist/{hookName}` (post-build artifacts from `scripts/build-hooks.js`). Claude `are-session-end.js` hook spawns detached `npx agents-reverse-engineer@latest update --quiet` process on `git status --porcelain` detecting changes. Disable via `ARE_DISABLE_HOOK=1` environment variable or `hook_enabled: false` config flag.\n### src/orchestration/AGENTS.md\n\n<!-- Generated by agents-reverse-engineer -->\n\n# src/orchestration\n\n**Iterator-based concurrency pool orchestrating three-phase AI-driven documentation generation (file analysis → directory aggregation → root synthesis) with streaming progress, promise-chain serialized writes for GENERATION-PLAN.md updates and NDJSON trace emission, ETA calculation via moving average, and quality validation integration.**\n\n## Contents\n\n### Core Orchestration\n\n**[runner.ts](./runner.ts)** — `CommandRunner` executes three-phase pipeline: concurrent file `.sum` generation via `runPool()` with stale-doc detection, post-order directory `AGENTS.md` traversal grouped by depth, sequential root document synthesis (`CLAUDE.md`/`GEMINI.md`/`OPENCODE.md`), embeds quality validators (`checkCodeVsDoc`/`checkCodeVsCode`/`checkPhantomPaths`) post-phase with throttled directory grouping, builds `RunSummary` with token/cost/duration/inconsistency aggregates.\n\n**[pool.ts](./pool.ts)** — `runPool<T>()` implements shared-iterator worker pattern: all N workers pull from single `tasks.entries()` iterator preventing idle time between batches, emits `worker:start/end` and `task:pickup/done` trace events with `activeTasks` counter, supports `failFast` abort via mutable `aborted` flag checked before each task pickup, returns sparse `TaskResult<T>[]` array indexed by original task position.\n\n**[progress.ts](./progress.ts)** — `ProgressReporter` streams colored console output (`pc.cyan('ANALYZING')`, `pc.green('DONE')`, `pc.red('FAIL')`) with ETA calculated via sliding window (last 10 completion times) for files and directories separately, `ProgressLog` mirrors ANSI-stripped output to `.agents-reverse-engineer/progress.log` via promise-chain serialized writes for `tail -f` monitoring in buffered environments.\n\n**[plan-tracker.ts](./plan-tracker.ts)** — `PlanTracker` serializes concurrent checkbox updates (`- [ ] → - [x]`) for `GENERATION-PLAN.md` via promise-chain pattern (`writeQueue`), maintains in-memory markdown state, requires exact backtick-wrapped path matching (`` `src/cli/init.ts` ``), swallows write errors (non-critical operation).\n\n**[trace.ts](./trace.ts)** — `createTraceWriter()` factory returns `NullTraceWriter` (zero overhead) or `TraceWriter` (appends to `.agents-reverse-engineer/traces/trace-{timestamp}.ndjson`), auto-populates `seq`/`ts`/`pid`/`elapsedMs` base fields, emits 14 event types (`phase:start/end`, `worker:start/end`, `task:pickup/done`, `subprocess:spawn/exit`, `retry`, `discovery:start/end`, `filter:applied`, `plan:created`, `config:loaded`), `cleanupOldTraces()` enforces 500-file retention, uses `DistributiveOmit<TraceEvent, BaseKeys>` for discriminated union stripping.\n\n### Type Definitions\n\n**[types.ts](./types.ts)** — `FileTaskResult` carries per-task metrics (`tokensIn`/`tokensOut`/`cacheReadTokens`/`cacheCreationTokens`/`durationMs`/`model`), `RunSummary` aggregates command-level totals with quality counts (`inconsistenciesCodeVsDoc`/`inconsistenciesCodeVsCode`/`phantomPaths`), `ProgressEvent` discriminates five event types (`start`/`done`/`error`/`dir-done`/`root-done`) with type-specific optional fields, `CommandRunOptions` threads `concurrency`/`failFast`/`debug`/`dryRun`/`tracer`/`progressLog` from CLI through runner to pool and AIService.\n\n**[index.ts](./index.ts)** — Barrel export exposing `runPool`, `ProgressReporter`, `ProgressLog`, `PlanTracker`, `CommandRunner`, `ITraceWriter`, `TraceEvent`, `TraceEventPayload`, `createTraceWriter`, `cleanupOldTraces`, and shared types (`FileTaskResult`, `RunSummary`, `ProgressEvent`, `CommandRunOptions`, `PoolOptions`, `TaskResult`).\n\n## Concurrency Architecture\n\n### Shared-Iterator Pattern\n\n`runPool()` avoids batch-based idling by sharing single `tasks.entries()` iterator across N workers. Each worker pulls next `[index, taskFn]` tuple immediately after completing previous task, ensuring all slots stay busy until iterator exhausts. `Promise.allSettled(workers)` waits for all workers to finish or abort.\n\n### Fail-Fast Abort\n\nMutable `aborted` boolean flag set by any worker encountering error when `failFast: true`. Workers check `if (aborted) break` before pulling next task but do not interrupt running tasks. Results array remains sparse (undefined entries at aborted indices).\n\n### Serialization Guarantees\n\n`PlanTracker` and `TraceWriter` use promise-chain pattern (`writeQueue: Promise<void>`) to serialize concurrent writes. Each `markDone()` or `emit()` call chains onto previous write: `this.writeQueue = this.writeQueue.then(() => fs.writeFile(...))`. Guarantees NDJSON line order matches emission order despite pool concurrency.\n\n## Three-Phase Execution Flow\n\n### Phase 1: File Analysis (Concurrent)\n\n`CommandRunner.executeGenerate()` pre-caches existing `.sum` files at concurrency=20 for stale-doc detection. Maps file tasks to async functions reading source via `fs.readFile()`, computing `contentHash` via `computeContentHashFromString()`, calling `AIService.call()` with `buildFilePrompt()`, stripping preamble, extracting purpose (first non-preamble line truncated to 120 chars), writing `SumFileContent` with YAML frontmatter (`generated_at`, `content_hash`, `purpose`). Executes via `runPool(fileTasks, { concurrency, failFast, tracer, phaseLabel: 'phase-1-files' })`. Post-phase: groups files by directory, runs throttled (concurrency=10) `checkCodeVsDoc()` comparing old vs. new `.sum` against source content, aggregates `filesForCodeVsCode` array and runs `checkCodeVsCode()` for duplicate symbol detection.\n\n### Phase 2: Directory Aggregation (Post-Order Traversal)\n\nGroups `plan.directoryTasks` by `metadata.depth` into `Map<number, ExecutionTask[]>`, sorts depth levels descending (deepest-first). For each depth level: computes `dirConcurrency = Math.min(concurrency, dirsAtDepth.length)`, emits `phase:start`, maps directory tasks to async functions calling `buildDirectoryPrompt()` with `knownDirs` set and project structure context, calls AI service, writes via `writeAgentsMd()` (prepends existing `AGENTS.local.md` content if present), updates reporter and plan tracker. Executes via `runPool()` with depth-specific `phaseLabel`. Post-phase: runs `checkPhantomPaths()` on each generated `AGENTS.md` (three regex patterns: markdown links, backtick-quoted paths, prose-embedded paths), aggregates issues into `phantomReport`.\n\n### Phase 3: Root Synthesis (Sequential)\n\nSequential execution (concurrency=1) over `plan.rootTasks` (`CLAUDE.md`, `GEMINI.md`, `OPENCODE.md`). Emits `task:start`, calls `buildRootPrompt()` with all `AGENTS.md` injected and root `package.json` metadata, calls `AIService.call({ prompt, systemPrompt, maxTurns: 1 })`, strips conversational preamble (two patterns: `\\n---\\n` separator or bold uppercase start), writes via `fs.writeFile()`, emits `task:done`, updates reporter and plan tracker. Flushes `planTracker.flush()` after loop.\n\n## Progress Reporting & ETA\n\n### Moving Average ETA\n\n`ProgressReporter` maintains `completionTimes: number[]` (files) and `dirCompletionTimes: number[]` (directories) sliding windows (max 10 entries). On task completion: pushes `durationMs`, shifts oldest if window exceeds size. `formatETA()` computes moving average `(sum / count) * remaining`, formats as `~Ns` or `~Mm Ss`, returns empty string if fewer than 2 completions.\n\n### Dual Output Streams\n\nConsole output uses `picocolors` for colored ANSI codes (`pc.cyan('ANALYZING')`, `pc.green('DONE')`, `pc.blue('DONE')` for directories/roots, `pc.red('FAIL')`). `ProgressLog` strips ANSI via regex `/\\x1b\\[[0-9;]*m/g` and appends to `.agents-reverse-engineer/progress.log` via promise-chain serialization. File handle opened lazily in truncate mode ('w') on first write, closed via `finalize()`.\n\n### Token Accounting Display\n\nTotal input tokens displayed as `tokensIn + cacheReadTokens + cacheCreationTokens`. Completion messages show format: `[X/Y] DONE path Xs in/out tok model ~ETA`. Summary prints: files processed/failed/skipped, total calls, tokens (input + cache read + cache creation / output), cache statistics (% read tokens, % creation tokens), files read (total and unique), elapsed time, errors, retries.\n\n## Trace Event System\n\n### Event Types & Schema\n\nDiscriminated union `TraceEvent` with 14 types. All events extend `TraceEventBase` with auto-populated fields: `seq` (monotonic counter), `ts` (ISO 8601), `pid` (`process.pid`), `elapsedMs` (high-resolution delta from `process.hrtime.bigint()` start).\n\n**Pool events:** `worker:start` (workerId, phase), `worker:end` (workerId, phase, tasksExecuted), `task:pickup` (workerId, taskIndex, taskLabel, activeTasks), `task:done` (workerId, taskIndex, taskLabel, durationMs, success, error?, activeTasks).\n\n**Subprocess events:** `subprocess:spawn` (childPid, command, taskLabel), `subprocess:exit` (childPid, command, taskLabel, exitCode, signal, durationMs, timedOut).\n\n**Phase events:** `phase:start` (phase, taskCount, concurrency), `phase:end` (phase, durationMs, tasksCompleted, tasksFailed).\n\n**Other events:** `retry` (attempt, taskLabel, errorCode), `discovery:start/end`, `filter:applied`, `plan:created`, `config:loaded`.\n\n### Emission & Serialization\n\nUsers call `tracer.emit(payload)` with `TraceEventPayload = DistributiveOmit<TraceEvent, BaseKeys>` (user-supplied data without auto-populated fields). `TraceWriter.emit()` populates base fields, serializes to JSON line, enqueues write via `writeQueue = writeQueue.then(() => fd.write(line))` promise-chain. Lazy-opens file handle via `fs.open(filePath, 'a')` after ensuring directory exists. `finalize()` awaits `writeQueue`, closes handle.\n\n### Cleanup & Retention\n\n`cleanupOldTraces(projectRoot, keepCount = 500)` reads `.agents-reverse-engineer/traces/`, sorts files by creation time descending via `fs.stat()`, deletes excess files, returns deletion count. Auto-invoked by `AIService.finalize()` in telemetry shutdown.\n\n## Quality Validation Integration\n\n### Code-vs-Doc Consistency\n\nPost-Phase 1: groups processed files by `path.dirname()`, runs throttled (concurrency=10) validation per directory. For each file: reads cached `sourceContent` from map, reads old `.sum` via `readSumFile()`, runs `checkCodeVsDoc(sourceContent, oldSum.summary, filePath)` appending `(stale documentation)` label, reads fresh `.sum`, runs second pass. `checkCodeVsDoc()` extracts exported symbols via regex `/^[ \\t]*export\\s+(?:default\\s+)?(?:function|class|const|let|var|type|interface|enum)\\s+(\\w+)/gm`, verifies all exports appear in summary via substring search, reports `CodeDocInconsistency` with `missingFromDoc` arrays.\n\n### Code-vs-Code Duplicate Detection\n\nAggregates exports across per-directory file groups into `Map<symbol, string[]>`. Runs `checkCodeVsCode(filesForCodeVsCode)` detecting symbols appearing in multiple files, reports `CodeCodeInconsistency` with `pattern: 'duplicate-export'`.\n\n### Phantom Path Resolution\n\nPost-Phase 2: for each generated `AGENTS.md`, runs `checkPhantomPaths(agentsMdPath, content, projectRoot)` extracting path-like strings via three regex patterns (markdown links, backtick-quoted paths, prose-embedded paths), resolves against AGENTS.md directory and project root with `.ts`/`.js` fallback, reports `PhantomPathInconsistency` for unresolved references.\n\n### Report Formatting\n\nBuilds `InconsistencyReport` via `buildInconsistencyReport(issues, metadata)` with `metadata` (timestamp, projectRoot, filesChecked, durationMs), `issues[]` (discriminated union), `summary` (counts by type/severity). Prints via `formatReportForCli(report)` showing counts and groupings. Embeds in `RunSummary.inconsistencyReport` field.\n\n## Incremental Update Support\n\n`CommandRunner.executeUpdate()` implements Phase 1 only re-analysis for changed files. Attempts to load `GENERATION-PLAN.md` from `.agents-reverse-engineer/` for `projectPlan` context (falls through on error). Maps `filesToAnalyze` to async tasks reading source, reading existing `.sum`, building prompt via `buildFilePrompt({ filePath, content, projectPlan, existingSum })`, calling AI service, writing `.sum` with hash. Runs identical post-analysis quality checks (code-vs-doc, code-vs-code) without stale-doc comparison. Builds `RunSummary` with `updateInconsistenciesCodeVsDoc`/`updateInconsistenciesCodeVsCode` from report summary.\n\n## Behavioral Contracts\n\n### Preamble Stripping Patterns\n\n`stripPreamble()` implements two regex-based detection patterns:\n1. YAML separator: `/\\n---\\n/` within first 500 chars, returns content after separator\n2. Bold uppercase start: `/^[\\s\\S]{0,500}?(\\*\\*[A-Z])/` matches conversational preamble preceding bold section header, strips if <300 chars and no `##` markdown headers present\n\n### Purpose Extraction Filter\n\n`extractPurpose()` skips lines matching `PREAMBLE_PREFIXES` array via `line.toLowerCase().startsWith()`:\n```javascript\n['now i', 'perfect', 'based on', 'let me', 'here is', \"i'll\", 'i will', 'great', 'okay', 'sure', 'certainly', 'alright']\n```\n\nReturns first non-preamble/non-header/non-separator line with bold markdown stripped (`/^\\*\\*(.+)\\*\\*$/`) and 120-char truncation.\n\n### Checkbox Markdown Format\n\n`PlanTracker.markDone()` requires exact backtick-wrapped path format:\n```markdown\n- [ ] `src/cli/init.ts`     → file task\n- [ ] `src/cli/AGENTS.md`   → directory task (caller appends /AGENTS.md)\n- [ ] `CLAUDE.md`           → root task\n```\n\nString replacement via `` `- [ ] \\`${itemPath}\\`` → `- [x] \\`${itemPath}\\`` ``. Returns early if no match (no checkbox update queued).\n\n### Trace Filename Format\n\n`trace-{timestamp}.ndjson` where timestamp converts ISO 8601 to filesystem-safe format via `new Date().toISOString().replace(/[:.]/g, '-')` (colons/periods → hyphens).\n\n### Export Symbol Extraction Regex\n\nCode-vs-doc validator uses pattern: `/^[ \\t]*export\\s+(?:default\\s+)?(?:function|class|const|let|var|type|interface|enum)\\s+(\\w+)/gm` matching top-level exports with optional `default` keyword, capturing symbol name via `(\\w+)` group.\n\n### Phantom Path Extraction Patterns\n\nThree regex patterns in `checkPhantomPaths()`:\n1. Markdown links: `/\\[(?:[^\\]]*)\\]\\((\\.[^)]+)\\)/g` captures relative paths in link targets\n2. Backtick-quoted paths: `` /`((?:src\\/|\\.\\.?\\/)[^`]+\\.[a-z]{1,4})`/g `` captures backtick-wrapped paths starting with `src/` or `./` or `../` with file extensions\n3. Prose-embedded paths: `/(?:from|in|by|via|see)\\s+`?(src\\/[\\w\\-./]+)`?/gi` captures paths following prepositions in prose text\n### src/output/AGENTS.md\n\n<!-- Generated by agents-reverse-engineer -->\n\n# src/output/\n\nTerminal output formatting layer providing dependency-injectable logger abstraction with conditional color support via picocolors.\n\n## Contents\n\n### [logger.ts](./logger.ts)\nExports `Logger` interface (six methods: `info()`, `file()`, `excluded()`, `summary()`, `warn()`, `error()`), `createLogger(options)` factory with picocolors-based ANSI formatting, and `createSilentLogger()` no-op factory for test/programmatic contexts.\n\n## Architecture\n\n**Dependency Injection Pattern**: Consumers receive `Logger` interface rather than direct `console` access, enabling output capture in tests and silence in programmatic workflows.\n\n**Conditional Color Support**: `createLogger(options)` binds `ColorFunctions` to picocolors `pc` when `options.colors` is true, otherwise to `noColor` identity functions. Single conditional at factory construction avoids per-call branching.\n\n**Output Routing**: `info()/file()/excluded()/summary()` write to `console.log()`, `warn()` to `console.warn()`, `error()` to `console.error()` for stream separation in shell redirects.\n\n## Behavioral Contracts\n\n**File Discovery Formatting**:\n- Included file: `` `  +` `` (green) + `path`\n- Excluded file: `` `  -` `` (dim) + `path` + ` (${reason}: ${filter})` (dim)\n- Summary: `Discovered ${included} files (${excluded} excluded)` (bold count, dim excluded)\n\n**Warning/Error Prefixes**:\n- `warn()`: `\"Warning: \"` (yellow) + message → stderr\n- `error()`: `\"Error: \"` (red) + message → stderr\n\n## Usage Context\n\nThreaded through `CommandRunOptions` → `DiscoveryOptions` → `run()` in `src/discovery/run.ts`. CLI commands instantiate via `createLogger({ colors: config.output.colors })`, test suites use `createSilentLogger()`.\n### src/quality/AGENTS.md\n\n<!-- Generated by agents-reverse-engineer -->\n\n# src/quality\n\nCode-documentation consistency validation subsystem executing three detection strategies: regex-based export extraction with substring matching (`code-vs-doc`), cross-file duplicate symbol aggregation (`code-vs-code`), and regex-based path reference resolution with filesystem verification (`phantom-paths`).\n\n## Contents\n\n**[index.ts](./index.ts)** — Barrel module re-exporting `checkCodeVsDoc()`, `checkCodeVsCode()`, `checkPhantomPaths()` validators, `buildInconsistencyReport()`, `formatReportForCli()` reporters, disabled `validateFindability()` density validator, and discriminated union types (`CodeDocInconsistency`, `CodeCodeInconsistency`, `PhantomPathInconsistency`, `Inconsistency`, `InconsistencyReport`).\n\n**[types.ts](./types.ts)** — Defines `InconsistencySeverity` (`'info' | 'warning' | 'error'`), `CodeDocInconsistency` (exported symbols missing from `.sum` with `missingFromDoc[]`), `CodeCodeInconsistency` (duplicate exports across files with `pattern: 'duplicate-export'`), `PhantomPathInconsistency` (unresolved paths in `AGENTS.md` with `referencedPath`, `resolvedTo`, `context`), `Inconsistency` discriminated union, `InconsistencyReport` container with metadata (`timestamp`, `projectRoot`, `filesChecked`, `durationMs`) and summary counts.\n\n## Subdirectories\n\n**[inconsistency/](./inconsistency/)** — code-vs-doc validator extracting exports via `/^[ \\t]*export\\s+(?:default\\s+)?(?:function|class|const|let|var|type|interface|enum)\\s+(\\w+)/gm` with substring verification against `.sum` content; code-vs-code detector aggregating exports into `Map<symbol, paths[]>` to identify duplicates; reporter building aggregated `InconsistencyReport` with formatted CLI output.\n\n**[phantom-paths/](./phantom-paths/)** — Path reference validator extracting strings via three regex patterns (markdown links, backtick paths, prose references), resolving against `AGENTS.md` directory and project root with `.ts`/`.js` fallback, returning `PhantomPathInconsistency[]` for unresolved references.\n\n**[density/](./density/)** — Disabled `validateFindability()` stub previously verifying exported symbols from `.sum` files appeared in parent `AGENTS.md`; implementation gutted after `SumFileContent.metadata.publicInterface` removal.\n\n## Validation Workflow\n\n**Code-vs-Doc Consistency:**  \n`extractExports(filePath)` scans source via regex capturing export identifiers → `checkCodeVsDoc(filePath, sumContent)` performs substring search against `.sum` summary text → returns `CodeDocInconsistency` with `missingFromDoc[]` when drift detected or `null` when consistent.\n\n**Code-vs-Code Duplicate Detection:**  \n`checkCodeVsCode(files[])` aggregates `extractExports()` results into `Map<symbol, string[]>` → filters entries with `paths.length > 1` → returns `CodeCodeInconsistency[]` with `pattern: 'duplicate-export'` and `severity: 'warning'`.\n\n**Phantom Path Resolution:**  \n`checkPhantomPaths(agentsMdPath, content, projectRoot)` applies three regex patterns to extract path references → attempts resolution via `existsSync()` against `AGENTS.md` directory and project root with extension substitution → returns `PhantomPathInconsistency[]` for unresolved paths with 120-char context excerpt.\n\n**Report Aggregation:**  \n`buildInconsistencyReport(issues[], metadata)` merges inconsistency arrays → computes summary counts by `type` (`code-vs-doc`, `code-vs-code`, `phantom-path`) and `severity` (`error`, `warning`, `info`) → returns `InconsistencyReport` with timestamp metadata. `formatReportForCli(report)` transforms into plain-text output with severity tags (`[ERROR]`, `[WARN]`, `[INFO]`) and type-specific field rendering for stderr/`progress.log`.\n\n## Behavioral Contracts\n\n**Export Extraction Regex (inconsistency/):**  \n`/^[ \\t]*export\\s+(?:default\\s+)?(?:function|class|const|let|var|type|interface|enum)\\s+(\\w+)/gm`\n\n**Path Extraction Patterns (phantom-paths/validator.ts):**  \n```javascript\n/\\[(?:[^\\]]*)\\]\\((\\.[^)]+)\\)/g              // Markdown: [text](./path)\n/`((?:src\\/|\\.\\.?\\/)[^`]+\\.[a-z]{1,4})`/g  // Backtick: `src/foo.ts`\n/(?:from|in|by|via|see)\\s+`?(src\\/[\\w\\-./]+)`?/gi // Prose: \"from src/foo/\"\n```\n\n**Severity Tags (inconsistency/reporter.ts):**  \n`severity: 'error'` → `[ERROR]`, `severity: 'warning'` → `[WARN]`, `severity: 'info'` → `[INFO]`\n\n**Type Discriminator Field:**  \n`type: 'code-vs-doc' | 'code-vs-code' | 'phantom-path'` enables exhaustive switch narrowing.\n\n## Known Limitations\n\n**Regex-Based Export Extraction:**  \nMisses destructured (`export { foo }`), namespace (`export *`), dynamic exports, multiline declarations. No AST analysis—purely heuristic pattern matching.\n\n**Substring Verification:**  \nFalse negatives when export names appear in comments/prose rather than documented API surface. No structured field verification after `publicInterface` schema removal.\n\n**Intentional Duplication:**  \nCode-vs-code cannot distinguish factory patterns, parallel implementations, intentional re-exports. Caller must scope input to per-directory groups to minimize false positives.\n\n**Extension Fallback Logic:**  \nPath resolution attempts `.js` → `.ts` substitution but doesn't handle `.jsx`/`.tsx`, `.cjs`/`.mjs`, or index file resolution (`./foo` → `./foo/index.ts`).\n### src/quality/density/AGENTS.md\n\n<!-- Generated by agents-reverse-engineer -->\n\n# src/quality/density\n\nDisabled findability validation stub that previously verified exported symbols from .sum files appeared in parent AGENTS.md content, retained for future structured metadata extraction support.\n\n## Contents\n\n**[validator.ts](./validator.ts)** — Exports `validateFindability()` returning empty array; previously extracted symbol names from `SumFileContent.metadata.publicInterface` and performed substring search in AGENTS.md content to compute per-file `FindabilityResult` with score calculation.\n\n## Exported Interface\n\n**FindabilityResult** — Validation result containing `filePath`, `symbolsTested[]`, `symbolsFound[]`, `symbolsMissing[]`, and `score` (0.0–1.0 ratio of found to tested symbols).\n\n**validateFindability()** — `(agentsMdContent: string, sumFiles: Map<string, SumFileContent>) => FindabilityResult[]` — Returns empty array; signature preserved but implementation gutted after `SumFileContent.metadata.publicInterface` removal.\n\n## Disabled Feature Context\n\nvalidator.ts implements LLM-free validation using string-based symbol matching with no AI subprocess calls. Disabled in `../index.ts` quality validation pipeline when `publicInterface` field removed from `SumFileContent` schema in `../../generation/writers/sum.ts`. Previously detected when directory-level AGENTS.md aggregation failed to preserve critical symbol names from child .sum file summaries.\n\n## Restoration Path\n\nRe-implementation requires:\n1. Adding structured export extraction to .sum file generation (Phase 1 of three-phase pipeline in `../../generation/orchestrator.ts`)\n2. Parsing YAML frontmatter in .sum files via `readSumFile()` from `../../generation/writers/sum.ts`\n3. Implementing symbol presence checks against AGENTS.md content\n4. Re-enabling in `../index.ts` quality validator orchestrator\n\n## Type Dependencies\n\nImports `SumFileContent` from `../../generation/writers/sum.ts` — Interface for parsed .sum file containing YAML frontmatter (`generated_at`, `content_hash`, `purpose`, `critical_todos[]`, `related_files[]`) and markdown summary content.\n### src/quality/inconsistency/AGENTS.md\n\n<!-- Generated by agents-reverse-engineer -->\n\n# src/quality/inconsistency\n\nValidates code-documentation consistency by extracting exports from source files, verifying their presence in `.sum` documentation, detecting duplicate exports across file groups, and aggregating validation results into structured reports with CLI-formatted output.\n\n## Contents\n\n### Validators\n\n**[code-vs-doc.ts](./code-vs-doc.ts)** — `extractExports()` extracts exported identifiers from TypeScript/JavaScript source via regex pattern `/^[ \\t]*export\\s+(?:default\\s+)?(?:function|class|const|let|var|type|interface|enum)\\s+(\\w+)/gm`. `checkCodeVsDoc()` verifies extracted exports appear in `.sum` file `summary` text via substring search, returning `CodeDocInconsistency` with `missingFromDoc[]` array when drift detected, or `null` when consistent.\n\n**[code-vs-code.ts](./code-vs-code.ts)** — `checkCodeVsCode()` detects duplicate exports across file groups by aggregating `extractExports()` results into `Map<symbol, paths[]>`, returning `CodeCodeInconsistency[]` array for symbols exported from multiple files with `pattern: 'duplicate-export'` and `severity: 'warning'`.\n\n### Report Generation\n\n**[reporter.ts](./reporter.ts)** — `buildInconsistencyReport()` aggregates `Inconsistency[]` into `InconsistencyReport` with summary counts by type (`code-vs-doc`, `code-vs-code`, `phantom-path`) and severity (`error`, `warning`, `info`). `formatReportForCli()` transforms report into plain-text output with severity tags `[ERROR]`/`[WARN]`/`[INFO]` and type-specific field rendering for stderr and `progress.log` output.\n\n## Validation Algorithm\n\n**Phase 1: Export Extraction**  \nRegex-based identifier capture from source declarations (`export function foo`, `export const BAR`, `export default class App`). Matches capture group 1 containing symbol name, filtering lines matching `/^[ \\t]*export\\s+(?:default\\s+)?(?:function|class|const|let|var|type|interface|enum)\\s+(\\w+)/gm`.\n\n**Phase 2: Documentation Verification**  \nSubstring search against `.sum` file `summary` text. Export name must appear anywhere in summary content to pass validation. Missing symbols populate `CodeDocInconsistency.details.missingFromDoc[]` array.\n\n**Phase 3: Cross-File Duplicate Detection**  \nAggregates exports into `Map<symbol, string[]>` where keys are export names and values are file paths. Filters map entries where `paths.length > 1`, constructing `CodeCodeInconsistency` records with formatted description `\"Symbol \\\"${name}\\\" exported from ${paths.length} files\"`.\n\n**Phase 4: Report Aggregation**  \nMerges inconsistency arrays from code-vs-doc, code-vs-code, and phantom-path validators into single `InconsistencyReport` with summary counts. Populates metadata (timestamp, projectRoot, filesChecked, durationMs) and discriminates union via `issue.type` for CLI rendering.\n\n## Behavioral Contracts\n\n**Regex Export Pattern (code-vs-doc.ts, code-vs-code.ts):**  \n`/^[ \\t]*export\\s+(?:default\\s+)?(?:function|class|const|let|var|type|interface|enum)\\s+(\\w+)/gm`\n\n**CLI Report Severity Tags (reporter.ts):**  \n- `severity: 'error'` → `[ERROR]`  \n- `severity: 'warning'` → `[WARN]`  \n- `severity: 'info'` → `[INFO]`\n\n**Type-Specific Field Rendering (reporter.ts):**  \n- `code-vs-doc`: `File: ${filePath}`  \n- `code-vs-code`: `Files: ${files.join(', ')}`  \n- `phantom-path`: `Doc: ${agentsMdPath}`, `Path: ${referencedPath}`\n\n## Integration with Quality Pipeline\n\nCalled by `src/quality/index.ts` orchestrator after per-file validation. `checkCodeVsDoc()` invoked for each source file with parsed `.sum` content. `checkCodeVsCode()` receives scoped per-directory file groups to avoid false positives across unrelated modules. Results merged with phantom-path validation output, passed to `buildInconsistencyReport()` with run metadata (filesChecked, durationMs), then formatted via `formatReportForCli()` for stderr and `progress.log` output.\n\n## Known Limitations\n\n**Regex-Based Export Extraction:**  \nMisses destructured exports (`export { foo, bar }`), namespace exports (`export * from './module'`), dynamic exports, multiline declarations. No AST analysis—purely heuristic pattern matching.\n\n**Substring Verification:**  \nProduces false negatives when export names appear in comments, prose descriptions, or unrelated context rather than as documented API surface. No structured field verification after `publicInterface` schema removal.\n\n**Intentional Duplication:**  \nCode-vs-code validator cannot distinguish factory patterns, parallel implementations, or other intentional duplication without semantic analysis. Caller must scope input to per-directory groups to minimize false positives.\n### src/quality/phantom-paths/AGENTS.md\n\n<!-- Generated by agents-reverse-engineer -->\n\n# src/quality/phantom-paths\n\nDetects unresolvable path references in `AGENTS.md` files by extracting path-like strings via regex patterns, resolving them against both the `AGENTS.md` directory and project root with `.ts`/`.js` fallback logic, and returning `PhantomPathInconsistency` objects for non-existent paths.\n\n## Contents\n\n### [index.ts](./index.ts)\nBarrel re-export consolidating `checkPhantomPaths` validator for single import point.\n\n### [validator.ts](./validator.ts)\nCore validation logic extracting paths via `PATH_PATTERNS`, resolving with extension fallback, reporting `PhantomPathInconsistency` for unresolved references.\n\n## Exported Interface\n\n- **`checkPhantomPaths(agentsMdPath, content, projectRoot)`** — Validates path references in `AGENTS.md` content, returns `PhantomPathInconsistency[]` with `type: 'phantom-path'`, `severity: 'warning'`, `referencedPath`, `resolvedTo`, and 120-char `context` excerpt\n\n## Path Extraction Patterns\n\nThree regex patterns (`PATH_PATTERNS`) capture references:\n\n```javascript\n/\\[(?:[^\\]]*)\\]\\((\\.[^)]+)\\)/g              // Markdown links: [text](./path)\n/`((?:src\\/|\\.\\.?\\/)[^`]+\\.[a-z]{1,4})`/g  // Backtick paths: `src/foo/bar.ts`\n/(?:from|in|by|via|see)\\s+`?(src\\/[\\w\\-./]+)`?/gi // Prose paths: \"from src/foo/\"\n```\n\n`SKIP_PATTERNS` excludes `node_modules`, `.git/`, URLs (`https?:`), template syntax (`{{`, `${`), glob wildcards (`*`), brace expansions (`{a,b,c}`).\n\n## Resolution Strategy\n\nAttempts resolution in order: (1) relative to `agentsMdDir` via `path.dirname(agentsMdPath)`, (2) relative to `projectRoot`, (3) `.js` → `.ts` substitution for TypeScript import convention. Uses `existsSync()` to validate at least one candidate exists. Deduplicates via `seen` Set tracking `rawPath` strings.\n\n## Integration Context\n\nPart of quality validation subsystem (`src/quality/`) alongside code-vs-doc consistency (`../inconsistency/code-vs-doc.js`) and code-vs-code duplicate detection (`../inconsistency/code-vs-code.js`). Invoked during post-generation validation phase via `src/quality/index.ts` to identify broken path references that may mislead AI coding assistants.\n### src/specify/AGENTS.md\n\n<!-- Generated by agents-reverse-engineer -->\n\n# specify/\n\nSpecification synthesis from AGENTS.md corpus via AI-driven prompt construction (`buildSpecPrompt`) and filesystem output (`writeSpec`) with multi-file splitting and overwrite protection.\n\n## Contents\n\n### Files\n\n**[index.ts](./index.ts)** — Barrel module re-exporting `buildSpecPrompt`, `writeSpec`, `SpecPrompt`, `WriteSpecOptions`, `SpecExistsError` for `/are-specify` command integration.\n\n**[prompts.ts](./prompts.ts)** — `buildSpecPrompt(docs: AgentsDocs): SpecPrompt` constructs system/user prompts enforcing nine-section structure (Overview, Architecture, API Surface, Data Structures, Configuration, Dependencies, Behavioral Contracts, Test Contracts, Build Plan) with verbatim regex/format/constant preservation. `SPEC_SYSTEM_PROMPT` prohibits folder-mirroring, mandates concern-based organization, targets AI agents with actionable instructions.\n\n**[writer.ts](./writer.ts)** — `writeSpec(content: string, options: WriteSpecOptions): Promise<string[]>` writes single-file or multi-file specifications via heading-based splitting. `splitByHeadings(content)` partitions on `/^# /m` regex, `slugify()` sanitizes heading text to filenames (e.g., `\"Project Overview\"` → `\"project-overview.md\"`). Throws `SpecExistsError` with `paths: string[]` when targets exist and `force: false`.\n\n## Data Flow\n\n1. **Orchestration** (`src/cli/specify.ts`) invokes `collectAgentsDocs()` to gather all `AGENTS.md` files recursively\n2. **Prompt Construction** (`prompts.ts`) injects corpus into `SPEC_SYSTEM_PROMPT` template with section delimiters `### ${relativePath}`, appends nine-section requirements\n3. **AI Synthesis** (`AIService.call()` from `src/ai/service.ts`) processes prompt pair, returns specification markdown\n4. **Output** (`writer.ts`) writes single `specs/SPEC.md` or splits into `specs/<slugified-heading>.md` per top-level heading\n5. **Error Handling** catches `SpecExistsError` and displays `--force` hint\n\n## Behavioral Contracts\n\n### Prompt Constraints (SPEC_SYSTEM_PROMPT)\n\n**Organization rules:**\n- Regex `/^# (Project Overview|Architecture|Public API Surface|Data Structures & State|Configuration|Dependencies|Behavioral Contracts|Test Contracts|Build Plan)$/m` defines mandatory section headings\n- Prohibition pattern: \"Do NOT prescribe exact filenames or file paths\" / \"Do NOT mirror the project's folder structure\" / \"Do NOT use directory names as section headings\"\n- Behavioral Contracts split: \"Runtime Behavior (error handling, concurrency, lifecycle)\" + \"Implementation Contracts (verbatim regex patterns, format specs, magic constants)\"\n\n**Output format enforcement:**\n- Terminal instruction: `\"OUTPUT: Raw markdown. No preamble. No meta-commentary. No 'Here is...' or 'I've generated...' prefix.\"`\n- User prompt: `\"Output ONLY the markdown content. No preamble.\"`\n\n### File System Patterns (writer.ts)\n\n**Heading splitting regex:** `/^(?=# )/m` (positive lookahead for top-level headings)  \n**Heading extraction:** `/^# (.+)/` (capture heading text)  \n**Slugification chain:**\n1. `.toLowerCase()`\n2. `/\\s+/g` → `'-'` (whitespace to hyphens)\n3. `/[^a-z0-9-]/g` → `''` (strip non-alphanumeric except hyphens)\n4. `/-+/g` → `'-'` (collapse consecutive hyphens)\n5. `/^-|-$/g` → `''` (trim leading/trailing hyphens)\n\n**Preamble filename:** `00-preamble.md` (content before first `# ` heading)\n\n## Integration Points\n\n**Consumed by:** `src/cli/specify.ts` (command orchestrator)  \n**Consumes:**\n- `collectAgentsDocs()` from `../generation/collector.js` → `AgentsDocs` (array of `{relativePath, content}`)\n- `AIService.call()` from `../ai/service.js` (prompt pair execution)\n- `mkdir()`, `writeFile()`, `access()` from `node:fs/promises` (file operations)\n- `path.dirname()`, `path.join()` from `node:path` (path resolution)\n\n**Error propagation:** `SpecExistsError` thrown when `force: false` and target paths exist, caught by CLI layer for `--force` hint display\n### src/types/AGENTS.md\n\n<!-- Generated by agents-reverse-engineer -->\n\n# src/types\n\nShared TypeScript interface definitions for file discovery results, exclusion tracking, and statistics aggregation across ARE modules.\n\n## Contents\n\n### [index.ts](./index.ts)\nExports `ExcludedFile` (path + reason), `DiscoveryResult` (files + excluded arrays), `DiscoveryStats` (totalFiles, includedFiles, excludedFiles, exclusionReasons histogram).\n\n## Usage Pattern\n\n`src/discovery/walker.ts` returns `DiscoveryResult` from traversal functions. `src/cli/discover.ts` consumes `DiscoveryResult` to format console output and write `GENERATION-PLAN.md`. `src/orchestration/runner.ts` logs `DiscoveryStats` to telemetry during discovery phase.\n\n## Data Model\n\n### ExcludedFile\n- `path: string` — Absolute/relative file path\n- `reason: string` — Exclusion rationale (\"gitignore pattern\", \"binary file\", \"vendor directory\")\n\n### DiscoveryResult\n- `files: string[]` — Selected for analysis\n- `excluded: ExcludedFile[]` — Rejected with reasons\n\n### DiscoveryStats\n- `totalFiles: number`, `includedFiles: number`, `excludedFiles: number` — Counts\n- `exclusionReasons: Record<string, number>` — Reason → occurrence count map\n### src/update/AGENTS.md\n\n<!-- Generated by agents-reverse-engineer -->\n\n# src/update/\n\nIncremental documentation synchronization via frontmatter SHA-256 hash comparison, orphaned artifact cleanup, and targeted directory regeneration.\n\n## Contents\n\n### Core Orchestration\n\n**[index.ts](./index.ts)** — Barrel export aggregating `UpdateOrchestrator` class, `createUpdateOrchestrator()` factory, cleanup functions (`cleanupOrphans`, `cleanupEmptyDirectoryDocs`, `getAffectedDirectories`), and type interfaces (`UpdateOptions`, `UpdateResult`, `UpdateProgress`, `CleanupResult`).\n\n**[orchestrator.ts](./orchestrator.ts)** — `UpdateOrchestrator` class executing hash-based change detection via `preparePlan()`: discovers files through `runDiscovery()`, reads `.sum` YAML frontmatter via `readSumFile()` to extract `contentHash`, computes current SHA-256 via `computeContentHash()`, produces `UpdatePlan` with `filesToAnalyze[]` (hash mismatch/missing), `filesToSkip[]` (hash match), `cleanup: CleanupResult` (orphaned `.sum` files), `affectedDirs[]` (depth-sorted parent directories). Emits trace events (`phase:start`, `plan:created`, `phase:end`) via `ITraceWriter`. No-op methods (`close()`, `recordFileAnalyzed()`, `removeFileState()`, `recordRun()`, `getLastRun()`) preserved for API compatibility after SQLite state manager removal.\n\n**[orphan-cleaner.ts](./orphan-cleaner.ts)** — `cleanupOrphans()` deletes `.sum` files for `FileChange` entries with `status === 'deleted'` (uses `change.path`) or `status === 'renamed'` (uses `change.oldPath`), returns `CleanupResult` with `deletedSumFiles[]` and `deletedAgentsMd[]`. `cleanupEmptyDirectoryDocs()` removes `AGENTS.md` from directories containing only hidden files (`.` prefix), `.sum` files, or `GENERATED_FILES` set members. `getAffectedDirectories()` walks parent directories via `path.dirname()` for non-deleted `FileChange` entries, excludes `status === 'deleted'`, always includes `'.'` root.\n\n**[types.ts](./types.ts)** — `UpdateOptions` interface with `includeUncommitted?: boolean` (merges `git status --porcelain` into change set) and `dryRun?: boolean` (preview-only). `UpdateResult` interface with `analyzedFiles[]`, `skippedFiles[]`, `cleanup`, `regeneratedDirs[]`, `baseCommit`, `currentCommit`, `dryRun` fields. `UpdateProgress` callback interface with `onFileStart()`, `onFileDone()`, `onCleanup()`, `onDirRegenerate()` hooks. `CleanupResult` interface with `deletedSumFiles[]`, `deletedAgentsMd[]` arrays. Re-exports `FileChange` from `../change-detection/types.js`.\n\n## Update Algorithm\n\n**Hash-Based Change Detection** (orchestrator.ts `preparePlan()` method):\n1. `checkPrerequisites()` verifies git repository via `isGitRepo()`\n2. `getCurrentCommit()` retrieves HEAD commit SHA\n3. `discoverFiles()` wraps `runDiscovery()` from `../discovery/run.js`, converts absolute paths to relative via `path.relative()`\n4. For each file: `getSumPath()` locates `.sum` file, `readSumFile()` extracts `contentHash` from YAML frontmatter, `computeContentHash()` computes current SHA-256\n5. Hash mismatch or missing `.sum` → `FileChange` with `status: 'added'|'modified'` added to `filesToAnalyze[]`\n6. Hash match → path added to `filesToSkip[]`\n7. `cleanupOrphans()` deletes `.sum` files for deleted/renamed sources\n8. `getAffectedDirectories()` computes unique parent directory set from `filesToAnalyze[]`\n9. `affectedDirs[]` sorted by depth descending via `split(path.sep).length` (deepest-first for bottom-up regeneration)\n\n**Orphan Cleanup** (orphan-cleaner.ts):\n- Deleted files: Appends `.sum` to `change.path`, calls `deleteIfExists()`\n- Renamed files: Appends `.sum` to `change.oldPath`, calls `deleteIfExists()`\n- Empty directories: `cleanupEmptyDirectoryDocs()` removes `AGENTS.md` if directory contains no source files (excludes hidden files, `.sum` files, `AGENTS.md`, `CLAUDE.md`)\n- `deleteIfExists()` internal helper: `stat()` existence check, `unlink()` delete (skipped if `dryRun === true`), returns boolean\n\n## Integration Points\n\n**Upstream Dependencies:**\n- `../change-detection/index.js` — `isGitRepo()`, `getCurrentCommit()`, `computeContentHash()`, `FileChange` type\n- `../generation/writers/sum.js` — `readSumFile()` (parses YAML frontmatter), `getSumPath()` (resolves `.sum` path)\n- `../discovery/run.js` — `discoverFiles()` wrapping `runDiscovery()` for file scanning\n- `../config/schema.js` — `Config` type with exclude patterns and concurrency settings\n- `../orchestration/trace.js` — `ITraceWriter` interface for NDJSON trace emission\n\n**Downstream Consumers:**\n- `../cli/update.ts` — CLI entry point invoking `createUpdateOrchestrator()` with parsed options\n- `hooks/are-session-end.js` — Session hook spawning `npx agents-reverse-engineer@latest update --quiet` on uncommitted changes\n\n## API Compatibility Pattern\n\norchestrator.ts no-op methods preserve interface contract for consumers expecting SQLite-based state management (removed in frontmatter migration):\n- `close()` — No database resources to clean\n- `recordFileAnalyzed()` — Hash now stored in `.sum` YAML frontmatter\n- `removeFileState()` — Cleanup handled by `cleanupOrphans()`\n- `recordRun()` — Returns `0` (no run history tracking)\n- `getLastRun()` — Returns `undefined` (no historical run data)\n\n## Trace Events\n\norchestrator.ts emits via `ITraceWriter`:\n- `{ type: 'phase:start', phase: 'update-plan-creation' }` — Start of `preparePlan()`\n- `{ type: 'plan:created', planType: 'update', fileCount, taskCount }` — Plan finalized\n- `{ type: 'phase:end', phase: 'update-plan-creation', durationMs, tasksCompleted: 1, tasksFailed: 0 }` — Plan complete\n\n## Debug Logging\n\norchestrator.ts stderr output (when `debug: true`):\n- `'[debug] Creating update plan with change detection...'` — Start signal\n- `'[debug] Git commit: <hash>'` — First 7 characters of commit SHA\n- `'[debug] Discovering files...'` — Before discovery phase\n- `'[debug] Change detection: <changed> changed, <unchanged> unchanged, <orphaned> orphaned'` — After hash comparison\n- `'[debug] Affected directories: <count>'` — Directory regeneration scope\n\n## Package Metadata (package.json)\n\n- **Name**: agents-reverse-engineer\n- **Version**: 0.6.5\n- **Description**: CLI tool for reverse-engineering codebase documentation for AI agents\n- **Scripts**:\n  - `build`: `tsc`\n  - `build:hooks`: `node scripts/build-hooks.js`\n  - `prepack`: `rm -f LICENSE.sum README.md.sum`\n  - `prepublishOnly`: `npm run build && npm run build:hooks`\n  - `dev`: `tsx watch src/cli/index.ts`\n\n## Output Requirements\n\nThe document MUST include:\n- Project purpose and description\n- Architecture overview with directory structure\n- Key directories table\n- Getting started (install, build, run commands)\n- Key technologies and dependencies\n\nThis document is the COMPREHENSIVE reference for the entire project.\nIt should contain architecture, configuration, build instructions, and project-wide patterns.\nIndividual file details belong in directory-level AGENTS.md files — reference them, don't duplicate them.\n\nOutput ONLY the markdown content. No preamble.",
      "systemPrompt": "You generate markdown documentation files. Output ONLY the raw markdown content.\nDo NOT include any conversational text, preamble, or meta-commentary.\nDo NOT say \"Here is...\" or \"I've generated...\" — just output the document itself.\nThe output will be written directly to a file.\n\nCRITICAL CONSTRAINT:\n- Synthesize ONLY from the AGENTS.md content provided in the user prompt\n- Do NOT invent, extrapolate, or hallucinate features, hooks, APIs, patterns, or dependencies not explicitly mentioned\n- If information is missing, omit that section rather than guessing\n- Every claim must be traceable to a specific AGENTS.md file provided",
      "response": "# agents-reverse-engineer\n\n**AI-driven codebase documentation generator executing Recursive Language Model (RLM) algorithm: concurrent per-file `.sum` analysis via subprocess pools, post-order directory `AGENTS.md` aggregation, and platform-specific root synthesis (`CLAUDE.md`, `GEMINI.md`, `OPENCODE.md`) with gitignore-aware discovery, SHA-256 incremental updates, and session lifecycle hooks.**\n\n## Project Overview\n\nagents-reverse-engineer (ARE) automates brownfield documentation for AI coding assistants by executing a three-phase pipeline that discovers source files, analyzes them concurrently via AI CLI subprocesses (Claude Code, Gemini, OpenCode), generates file summaries with YAML frontmatter containing SHA-256 content hashes, synthesizes directory-level documentation from bottom-up via post-order traversal, and produces root integration documents tailored to each AI platform.\n\n**Version**: 0.6.5  \n**License**: MIT (GeoloeG-IsT, 2026)  \n**Runtime**: Node.js ≥18.0.0 (ES modules)\n\n### Core Capabilities\n\n- **Parallel file analysis** with configurable concurrency pools (default 2 workers for WSL, 5 elsewhere)\n- **Incremental updates** via SHA-256 content hash comparison (skip unchanged files)\n- **Multi-platform AI backend support** (Claude Code, Gemini CLI, OpenCode) with automatic detection\n- **Gitignore-aware file discovery** with binary detection and vendor directory exclusion\n- **Quality validation** detecting code-documentation inconsistencies and phantom path references\n- **Session lifecycle hooks** for automatic documentation refresh on IDE session end\n- **NDJSON telemetry logging** with token cost tracking and run retention management\n\n## Architecture\n\n### Three-Phase Generation Pipeline\n\n**Phase 1: Concurrent File Analysis**\n\nIterator-based worker pool (`src/orchestration/pool.ts`) shares single task iterator across N workers to prevent over-allocation. Each worker invokes `AIService.call()` → `runSubprocess()` → `execFile()` spawning AI CLI subprocesses with resource limits:\n\n- `NODE_OPTIONS='--max-old-space-size=512'` limits heap to 512MB per subprocess\n- `UV_THREADPOOL_SIZE='4'` constrains libuv thread pool to 4 threads\n- `CLAUDE_CODE_DISABLE_BACKGROUND_TASKS='1'` prevents background task spawning\n- `--disallowedTools Task` blocks subagents\n\nProcess group killing (`kill(-pid)`) terminates subprocess trees on timeout. SIGTERM sent at `timeoutMs`, SIGKILL escalation after 5s grace period. Exponential backoff retry on rate limits (stderr patterns: \"rate limit\", \"429\", \"too many requests\", \"overloaded\").\n\nWrites `.sum` files with YAML frontmatter:\n\n```yaml\n---\ngenerated_at: 2026-02-09T12:34:56.789Z\ncontent_hash: a3f5d8e9... (SHA-256 hex)\npurpose: One-line purpose statement\ncritical_todos:\n  - Security issue\nrelated_files: [path1, path2]\n---\n\nMarkdown summary content...\n```\n\n**Phase 2: Post-Order Directory Aggregation**\n\nSorts directories by depth descending via `path.relative().split(path.sep).length` (deepest first). Waits for all child `.sum` files to exist via `isDirectoryComplete()` predicate before processing directory.\n\nPrompts include:\n\n- Aggregated child `.sum` content via `readSumFile()`\n- Subdirectory `AGENTS.md` files via recursive traversal\n- Import maps via `extractDirectoryImports()` with verified path constraints\n- Manifest detection (9 types: package.json, Cargo.toml, go.mod, pyproject.toml, pom.xml, build.gradle, Gemfile, composer.json, CMakeLists.txt/Makefile)\n\nUser-authored `AGENTS.md` files renamed to `AGENTS.local.md` and prepended above generated content. Writes `AGENTS.md` with `<!-- Generated by agents-reverse-engineer -->` marker.\n\n**Phase 3: Root Document Synthesis**\n\nSequential execution (concurrency=1) generates `CLAUDE.md`, `GEMINI.md`, `OPENCODE.md`. Prompts consume all `AGENTS.md` files via `collectAgentsDocs()` recursive tree traversal, parse root `package.json` for project metadata, enforce synthesis-only constraints (no invention of features/hooks/patterns not in source documents). Strips conversational preamble via pattern matching before writing output.\n\n### Directory Structure\n\n```\n.\n├── .github/workflows/    # CI/CD: npm publish workflow with provenance attestation\n├── docs/                 # Original vision document (INPUT.md) defining RLM algorithm\n├── hooks/                # Session lifecycle hooks for Claude/Gemini/OpenCode\n│   ├── are-check-update.js              # SessionStart: npm version check\n│   ├── are-session-end.js               # SessionEnd: auto-update on uncommitted changes\n│   ├── opencode-are-check-update.js     # OpenCode plugin: version check\n│   └── opencode-are-session-end.js      # OpenCode plugin: session end update\n├── scripts/              # Build automation: hook file copying (build-hooks.js)\n└── src/                  # TypeScript source tree\n    ├── ai/               # AI service orchestration with backend abstraction\n    ├── change-detection/ # Git-based delta computation and SHA-256 hashing\n    ├── cli/              # Command entry points (init, discover, generate, update, clean, specify)\n    ├── config/           # YAML config loading with Zod validation\n    ├── discovery/        # File walking with composable filters (gitignore, binary, vendor, custom)\n    ├── generation/       # Three-phase pipeline orchestration and prompt engineering\n    ├── imports/          # Static import analysis for dependency graphs\n    ├── installer/        # npx-based command/hook installation for IDE runtimes\n    ├── integration/      # Platform-specific template generation (Claude/OpenCode/Gemini)\n    ├── orchestration/    # Worker pool, progress reporting, trace emission, plan tracking\n    ├── output/           # Terminal logging with picocolors formatting\n    ├── quality/          # Code-doc consistency validation and phantom path detection\n    ├── specify/          # Project specification synthesis from AGENTS.md corpus\n    ├── types/            # Shared interfaces for discovery results\n    └── update/           # Incremental update workflow with orphan cleanup\n```\n\n## Key Directories\n\n| Directory | Purpose |\n|-----------|---------|\n| **src/ai/** | Backend-agnostic AI service layer with subprocess management, retry logic, telemetry logging, and trace emission. Contains adapters for Claude Code, Gemini CLI, OpenCode. |\n| **src/generation/** | Orchestrates three-phase pipeline: file analysis prompt construction with import maps, directory aggregation with user content preservation, root document synthesis with stack detection. |\n| **src/orchestration/** | Iterator-based worker pool with shared task queue, streaming progress reporter with ETA calculation, promise-chain serialization for concurrent writes (GENERATION-PLAN.md, progress.log, trace NDJSON). |\n| **src/discovery/** | Directory walker with filter chain: gitignore parsing via `ignore` library, binary detection (extension + content analysis), vendor directory exclusion (node_modules/.git/dist), custom glob patterns. |\n| **src/change-detection/** | Git diff parsing (`git diff --name-status -M`) with rename detection and SHA-256 content hashing for non-git workflows. Detects added/modified/deleted/renamed files, optionally merges uncommitted changes. |\n| **src/quality/** | Post-generation validators: regex-based export extraction vs. substring search in summaries (code-vs-doc), duplicate symbol detection (code-vs-code), path resolution via `existsSync()` (phantom-paths). |\n| **src/config/** | Zod schema validation for `.agents-reverse-engineer/config.yaml` with defaults for vendor directories (18 entries), exclude patterns (gitignore-style globs), binary extensions (26 types), AI backend config (concurrency/timeout/model/pricing). |\n| **src/installer/** | npx install orchestrator supporting global (`~/.claude`) and local (`.claude`) installation modes with interactive prompts, platform-specific path resolution (environment overrides: `CLAUDE_CONFIG_DIR`, `OPENCODE_CONFIG_DIR`, `GEMINI_CONFIG_DIR`), hook registration, permission setup. |\n| **hooks/** | Detached background processes for version checking (compares `npm view agents-reverse-engineer version` against `~/.claude/ARE-VERSION`) and session-end updates (spawns `npx agents-reverse-engineer@latest update --quiet` when `git status --porcelain` detects changes). |\n\n## Getting Started\n\n### Installation\n\n**Global install:**\n```bash\nnpm install -g agents-reverse-engineer\n```\n\n**Project-local install:**\n```bash\nnpm install --save-dev agents-reverse-engineer\n```\n\n**npx usage (no install required):**\n```bash\nnpx agents-reverse-engineer init\n```\n\n### Prerequisites\n\n- Node.js ≥18.0.0\n- At least one AI CLI installed:\n  - Claude Code: `npm install -g @anthropic-ai/claude-code`\n  - Gemini CLI: Installation instructions at https://gemini.google.com/cli\n  - OpenCode: Installation instructions at https://opencode.dev\n\n### Basic Workflow\n\n**1. Initialize configuration:**\n```bash\nare init\n# Creates .agents-reverse-engineer/config.yaml with defaults\n```\n\n**2. Discover files (optional preview):**\n```bash\nare discover\n# Scans files, writes GENERATION-PLAN.md with phase breakdown\n```\n\n**3. Generate documentation:**\n```bash\nare generate\n# Three-phase execution: .sum files → AGENTS.md → CLAUDE.md\n# Progress logged to .agents-reverse-engineer/progress.log\n# Monitor with: tail -f .agents-reverse-engineer/progress.log\n```\n\n**4. Incremental updates:**\n```bash\nare update\n# Hash-based change detection, regenerates only modified files\n# Use --uncommitted flag to include working tree changes\n```\n\n**5. Clean artifacts:**\n```bash\nare clean\n# Removes .sum, AGENTS.md (generated only), CLAUDE.md, GENERATION-PLAN.md\n# Restores AGENTS.local.md → AGENTS.md\n```\n\n**6. Generate project spec:**\n```bash\nare specify\n# Synthesizes all AGENTS.md into specs/SPEC.md\n# Use --multi-file for split specs (specs/<dirname>.md)\n```\n\n### CLI Options\n\n**Global flags:**\n- `--debug` — Enable verbose subprocess logging with heap/RSS metrics\n- `--trace` — Emit NDJSON trace events to `.agents-reverse-engineer/traces/`\n- `--concurrency N` — Override worker pool size (1-10, default from config)\n- `--fail-fast` — Abort on first task failure\n- `--dry-run` — Preview operations without writing files\n\n**Generate/Update:**\n- `--uncommitted` — Include working tree changes (update only)\n- `--force` — Overwrite existing files (specify only)\n\n**Trace output:**\n```bash\nare generate --trace\n# Writes trace-<timestamp>.ndjson with events:\n# phase:start/end, worker:start/end, task:pickup/done, subprocess:spawn/exit, retry\n```\n\n### IDE Integration\n\n**Install commands/hooks for Claude Code:**\n```bash\nnpx agents-reverse-engineer --runtime claude -g\n# Installs to ~/.claude/commands/ and ~/.claude/hooks/\n# Registers SessionStart hook: are-check-update.js\n# Registers SessionEnd hook: are-session-end.js\n```\n\n**Install for OpenCode:**\n```bash\nnpx agents-reverse-engineer --runtime opencode -g\n# Installs to ~/.config/opencode/commands/ and ~/.config/opencode/plugins/\n```\n\n**Install for Gemini:**\n```bash\nnpx agents-reverse-engineer --runtime gemini -g\n# Installs to ~/.gemini/commands/ with TOML format\n```\n\n**Available IDE commands:**\n- `/are-init` — Create configuration\n- `/are-discover` — Preview generation plan\n- `/are-generate` — Full documentation generation\n- `/are-update` — Incremental update\n- `/are-clean` — Remove generated artifacts\n- `/are-specify` — Synthesize project specification\n- `/are-help` — Show command list\n\n## Key Technologies\n\n**Runtime & Language:**\n- Node.js ≥18.0.0 (ES modules)\n- TypeScript 5.7.3 (ES2022 target, NodeNext module resolution, strict mode)\n\n**Core Dependencies:**\n- `fast-glob` — File discovery with glob patterns\n- `ignore` — Gitignore parsing\n- `isbinaryfile` — Binary file detection\n- `simple-git` — Change detection via git diff\n- `yaml` — Config parsing\n- `zod` — Schema validation\n- `ora` — Spinner UI\n- `picocolors` — Terminal colors\n\n**Build & Distribution:**\n- TypeScript compiler (`tsc`) emits to `dist/`\n- Binary entry points: `are` and `agents-reverse-engineer` → `dist/cli/index.js`\n- Pre-publish hooks: `build` + `build:hooks` (copies hooks/ to hooks/dist/)\n\n**AI Backends:**\n- `@anthropic-ai/claude-code` (Claude Code CLI adapter)\n- Gemini CLI (stub implementation pending JSON output stability)\n- OpenCode CLI (stub implementation pending JSONL parsing)\n\n## Configuration\n\n**Config file:** `.agents-reverse-engineer/config.yaml`\n\n**Schema sections:**\n\n```yaml\nexclude:\n  patterns:              # Gitignore-style globs (e.g., \"*.log\", \"**/*.test.ts\")\n  vendorDirs:           # Third-party directories to skip (default: node_modules, .git, dist, etc.)\n  binaryExtensions:     # Non-text file extensions (default: .png, .jpg, .zip, .pdf, etc.)\n\noptions:\n  followSymlinks: false # Follow symbolic links during discovery\n  maxFileSize: 1048576  # Binary detection threshold (1MB default)\n\noutput:\n  colors: true          # Enable ANSI color codes in CLI output\n\nai:\n  backend: 'auto'       # 'claude' | 'gemini' | 'opencode' | 'auto' (detect first available)\n  model: null           # Override backend default model\n  timeoutMs: 120000     # Subprocess timeout (2 minutes)\n  maxRetries: 3         # Exponential backoff retry attempts\n  concurrency: 2        # Worker pool size (1-10, default 2 for WSL, 5 elsewhere)\n  \n  telemetry:\n    enabled: true       # Write run logs to .agents-reverse-engineer/logs/\n    keepRuns: 50        # Retention limit for historical logs\n    costThresholdUsd: 10  # Warning threshold for cumulative costs\n  \n  pricing:              # Per-backend token cost configuration (input/output/cacheRead/cacheWrite)\n    claude: { inputCostPer1kTokens: 0.003, outputCostPer1kTokens: 0.015, ... }\n```\n\n**Environment overrides:**\n- `CLAUDE_CONFIG_DIR` — Override `~/.claude` path\n- `OPENCODE_CONFIG_DIR` — Override `~/.config/opencode` path\n- `GEMINI_CONFIG_DIR` — Override `~/.gemini` path\n- `ARE_DISABLE_HOOK` — Disable session-end auto-update (set to `1`)\n\n## Telemetry & Tracing\n\n**Run logs:** `.agents-reverse-engineer/logs/run-<timestamp>.json`\n- Aggregates per-call token counts, costs, durations, errors\n- Tracks `filesRead[]` metadata with `path`, `sizeBytes`, `linesRead`\n- Computes summary: `totalInputTokens`, `totalCacheReadTokens`, `errorCount`, `uniqueFilesRead`\n- Enforces retention via `cleanupOldLogs(keepCount)` after each run\n\n**Trace events:** `.agents-reverse-engineer/traces/trace-<timestamp>.ndjson`\n- Enabled via `--trace` flag\n- Events: `phase:start/end`, `worker:start/end`, `task:pickup/done`, `subprocess:spawn/exit`, `retry`\n- Auto-populated fields: `seq` (monotonic), `ts` (ISO 8601), `pid`, `elapsedMs` (high-resolution delta)\n- Promise-chain serialization ensures NDJSON line order matches emission order despite concurrent workers\n- Retention: keeps 500 most recent traces via `cleanupOldTraces(keepCount)`\n\n**Progress log:** `.agents-reverse-engineer/progress.log`\n- Human-readable streaming output mirroring console\n- ETA calculation via moving average of last 10 task durations\n- Quality metrics: code-vs-doc/code-vs-code inconsistencies, phantom path counts\n- Real-time monitoring: `tail -f .agents-reverse-engineer/progress.log`\n\n## Quality Validation\n\n**Code-vs-Doc Consistency:**\n- Extracts exported symbols via regex: `/^[ \\t]*export\\s+(?:default\\s+)?(?:function|class|const|let|var|type|interface|enum)\\s+(\\w+)/gm`\n- Verifies all exports appear in `.sum` summary text via substring search\n- Reports `CodeDocInconsistency` with `missingFromDoc` arrays\n\n**Code-vs-Code Duplicate Detection:**\n- Aggregates exports across per-directory file groups into `Map<symbol, string[]>`\n- Reports `CodeCodeInconsistency` for symbols appearing in multiple files (pattern: `'duplicate-export'`)\n\n**Phantom Path Resolution:**\n- Extracts path-like strings from `AGENTS.md` via three regex patterns:\n  - Markdown link targets: `/\\[(?:[^\\]]*)\\]\\((\\.[^)]+)\\)/g`\n  - Backtick-quoted paths: `` /`((?:src\\/|\\.\\.?\\/)[^`]+\\.[a-z]{1,4})`/g ``\n  - Prose-embedded paths: `/(?:from|in|by|via|see)\\s+`?(src\\/[\\w\\-./]+)`?/gi`\n- Resolves against AGENTS.md directory and project root with `.ts`/`.js` fallback\n- Reports `PhantomPathInconsistency` for unresolved references\n\n**Report format:** `InconsistencyReport` with `metadata` (timestamp, projectRoot, filesChecked, durationMs), `issues[]` (discriminated union), `summary` (counts by type/severity).\n\n## Incremental Update Strategy\n\n**Workflow:**\n1. Read `content_hash` from each `.sum` file's YAML frontmatter via `readSumFile()`\n2. Compute current file content hash via `computeContentHash()` (SHA-256)\n3. Hash mismatch → add to `filesToAnalyze` as `FileChange` with `status: 'modified'` or `'added'`\n4. Hash match → add to `filesToSkip`\n5. Detect orphans: `.sum` files for deleted source files or renamed oldPaths\n6. Call `cleanupOrphans()` to delete stale `.sum` files\n7. Call `cleanupEmptyDirectoryDocs()` to remove `AGENTS.md` from directories with no remaining sources\n8. Compute `affectedDirs` via `getAffectedDirectories()` by walking parent directories of changed files\n9. Regenerate `.sum` for `filesToAnalyze` via Phase 1 pool execution\n10. Regenerate `AGENTS.md` for `affectedDirs` sequentially (no Phase 2 post-order traversal required)\n\n**Git integration:**\n- Supports both committed changes (`git diff <baseCommit>..HEAD`) and uncommitted (`git status --porcelain` merge via `--uncommitted` flag)\n- Rename detection via `git diff -M` (50% similarity threshold)\n- Fallback to SHA-256 hashing for non-git workflows\n\n## Subprocess Resource Management\n\n**Problem:** Claude CLI spawns excessive Node.js threads (GitHub #5771: 200 instances reported). With concurrency=5, each subprocess spawns internal threads causing RAM exhaustion in WSL environments.\n\n**Mitigations applied in `src/ai/subprocess.ts`:**\n- `NODE_OPTIONS='--max-old-space-size=512'` limits heap to 512MB per subprocess\n- `UV_THREADPOOL_SIZE='4'` constrains libuv thread pool to 4 threads\n- `CLAUDE_CODE_DISABLE_BACKGROUND_TASKS='1'` prevents background task spawning\n- `--disallowedTools Task` prevents subprocess from spawning subagents\n- Process group killing: `kill(-pid)` terminates entire subprocess tree\n- Default concurrency reduced from 5 → 2 for resource-constrained environments\n\n**Timeout enforcement:**\n- SIGTERM sent at `timeoutMs`\n- SIGKILL escalation after 5s grace period\n- Unref'd timeout handle allows Node.js exit without cleanup blocking\n\n## Build & Development\n\n**Install dependencies:**\n```bash\nnpm install\n```\n\n**Development mode (hot reload):**\n```bash\nnpm run dev\n# Equivalent to: tsx watch src/cli/index.ts\n```\n\n**Build TypeScript:**\n```bash\nnpm run build\n# Compiles src/ → dist/ via tsc\n```\n\n**Prepare for publish:**\n```bash\nnpm run prepublishOnly\n# Executes: npm run build && npm run build:hooks\n# Copies hooks/ → hooks/dist/ for npm tarball inclusion\n```\n\n**Run from source:**\n```bash\nnode dist/cli/index.js generate\n# Or via npm link:\nnpm link\nare generate\n```\n\n## Session Hooks\n\n**Claude/Gemini session lifecycle:**\n- `are-check-update.js` (SessionStart): Spawns detached process querying `npm view agents-reverse-engineer version`, compares against `~/.claude/ARE-VERSION`, caches result to `~/.claude/cache/are-update-check.json`\n- `are-session-end.js` (SessionEnd): Runs `git status --porcelain`, spawns `npx agents-reverse-engineer@latest update --quiet` as detached background process if changes detected\n\n**OpenCode plugin system:**\n- `opencode-are-check-update.js`: Exports `AreCheckUpdate()` async factory returning plugin with `event['session.created']` handler\n- `opencode-are-session-end.js`: Exports `AreSessionEnd()` async factory returning plugin with `event['session.deleted']` handler\n\n**Detached spawn pattern:**\n```javascript\nspawn(process.execPath, ['-e', scriptString], {\n  stdio: 'ignore',\n  detached: true,\n  windowsHide: true\n}).unref()\n```\n\n**Disable mechanisms:**\n- Environment variable: `ARE_DISABLE_HOOK=1`\n- Config flag: `hook_enabled: false` in `.agents-reverse-engineer.yaml` (substring search, no YAML parser)\n\n## Integration with AI Assistants\n\n**Platform-specific command formats:**\n\n**Claude Code:**\n- Location: `.claude/skills/are-generate/SKILL.md`\n- Frontmatter: `name: /are-generate`\n- Long-running pattern: Remove stale `progress.log`, run with `run_in_background: true`, poll via `tail -5` every 10-15s, check `TaskOutput` with `block: false`, summarize on completion\n\n**OpenCode:**\n- Location: `.opencode/commands/are-generate.md`\n- Frontmatter: `agent: build`\n- Identical progress monitoring pattern\n\n**Gemini:**\n- Location: `.gemini/commands/are-generate.toml`\n- Format: `description = \"...\"` and `prompt = \"\"\"...\"\"\"` triple-quoted multiline\n\n**Aider:**\n- Detection only via `.aider.conf.yml` or `.aider/` directory\n- No template generation (manual integration required)\n\n## CI/CD\n\n**GitHub Actions workflow:** `.github/workflows/publish.yml`\n- Triggers on `release[published]` events or manual `workflow_dispatch`\n- Executes `ubuntu-latest` job with `id-token: write` permission enabling Sigstore-signed provenance\n- Runs `actions/checkout@v4`, `actions/setup-node@v4` with registry-url `https://registry.npmjs.org`\n- Executes `npm ci`, `npm run build` (invokes `tsc` + `build:hooks` from `prepublishOnly`)\n- Publishes with `npm publish --provenance --access public` using `NPM_TOKEN` secret\n- Cryptographic attestation links published artifact to source commit SHA\n\n## Roadmap & Known Limitations\n\n**Stub Backends:**\n- Gemini CLI adapter throws `SUBPROCESS_ERROR` until JSON output format stabilizes\n- OpenCode CLI adapter throws `SUBPROCESS_ERROR` until JSONL parsing implemented\n\n**Quality Validator Limitations:**\n- Regex-based export extraction misses complex patterns (destructured, namespace, dynamic exports)\n- Substring matching in code-vs-doc yields false negatives for prose mentions unrelated to API surface\n- Code-vs-code operates on symbol names only without AST analysis to distinguish intentional duplication\n\n**Disabled Features:**\n- Density validator (`validateFindability`) disabled after structured `publicInterface` removal from `SumFileContent` schema",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 6235,
      "cacheReadTokens": 16334,
      "cacheCreationTokens": 61410,
      "latencyMs": 154241,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": []
    }
  ],
  "summary": {
    "totalCalls": 126,
    "totalInputTokens": 384,
    "totalOutputTokens": 151030,
    "totalCacheReadTokens": 1990113,
    "totalCacheCreationTokens": 1677416,
    "totalDurationMs": 5092597,
    "errorCount": 0,
    "totalFilesRead": 96,
    "uniqueFilesRead": 96
  }
}