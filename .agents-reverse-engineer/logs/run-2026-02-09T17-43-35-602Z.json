{
  "runId": "2026-02-09T17:43:35.602Z",
  "startTime": "2026-02-09T17:43:35.602Z",
  "endTime": "2026-02-09T17:56:21.916Z",
  "entries": [
    {
      "timestamp": "2026-02-09T17:43:35.648Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/discovery/filters/custom.ts\n\n```typescript\n/**\n * Custom pattern filter for file discovery.\n *\n * Allows users to specify additional exclusion patterns in their configuration.\n * Uses the `ignore` library for gitignore-style pattern matching.\n */\n\nimport ignore, { type Ignore } from 'ignore';\nimport path from 'node:path';\nimport type { FileFilter } from '../types.js';\n\n/**\n * Creates a custom filter that excludes files matching user-provided patterns.\n *\n * Patterns use gitignore syntax and are checked against relative paths from\n * the root directory. If no patterns are provided, the filter passes all files.\n *\n * @param patterns - Array of gitignore-style patterns to exclude\n * @param root - Root directory for converting absolute paths to relative\n * @returns A FileFilter that checks paths against custom patterns\n *\n * @example\n * ```typescript\n * const filter = createCustomFilter(['*.log', 'tmp/**', 'secret.txt'], '/project');\n * filter.shouldExclude('/project/debug.log'); // true\n * filter.shouldExclude('/project/src/app.ts'); // false\n * ```\n */\nexport function createCustomFilter(patterns: string[], root: string): FileFilter {\n  const ig: Ignore = ignore();\n  const normalizedRoot = path.resolve(root);\n\n  // Add all patterns to the ignore instance\n  if (patterns.length > 0) {\n    ig.add(patterns);\n  }\n\n  return {\n    name: 'custom',\n\n    shouldExclude(absolutePath: string): boolean {\n      // If no patterns, nothing to exclude\n      if (patterns.length === 0) {\n        return false;\n      }\n\n      // Convert to relative path (ignore library requires relative paths)\n      const relativePath = path.relative(normalizedRoot, absolutePath);\n\n      // If path is outside root (starts with ..) or is empty, don't exclude\n      if (!relativePath || relativePath.startsWith('..')) {\n        return false;\n      }\n\n      return ig.ignores(relativePath);\n    },\n  };\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**createCustomFilter() constructs a FileFilter that excludes files matching user-provided gitignore-style patterns via the `ignore` library with relative path normalization.**\n\n## Exported Interface\n\n`createCustomFilter(patterns: string[], root: string): FileFilter` — Returns a FileFilter named `'custom'` that checks absolute paths against gitignore-style exclusion patterns. Empty pattern array results in pass-through filter (excludes nothing).\n\n## Pattern Matching Strategy\n\nUses `ignore` library instance (`Ignore` type) initialized via `ignore()` factory. Patterns added via `ig.add(patterns)` bulk operation. Matching performed via `ig.ignores(relativePath)` against paths converted from absolute to relative using `path.relative(normalizedRoot, absolutePath)`.\n\n## Path Normalization\n\n`normalizedRoot` computed via `path.resolve(root)` to canonicalize user-provided root. `shouldExclude()` converts absolute paths to relative before matching. Paths outside root (starting with `..`) or empty relative paths return `false` (not excluded) as guard against incorrect pattern application.\n\n## Integration Point\n\nReturns `FileFilter` interface from `../types.js` with `name: 'custom'` discriminator and `shouldExclude(absolutePath: string): boolean` predicate. Composable with other filters in `src/discovery/filters/index.ts` filter chain.\n\n## Short-Circuit Behavior\n\n`shouldExclude()` returns `false` early when `patterns.length === 0` (no patterns configured). Second early return when relative path is empty or starts with `'..'` prevents matching against files outside project root.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 376,
      "cacheReadTokens": 0,
      "cacheCreationTokens": 26967,
      "latencyMs": 20011,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/discovery/filters/custom.ts",
          "sizeBytes": 1847
        }
      ]
    },
    {
      "timestamp": "2026-02-09T17:43:35.643Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/ai/telemetry/run-log.ts\n\n```typescript\n/**\n * Run log writer for AI telemetry.\n *\n * Writes a completed {@link RunLog} to disk as pretty-printed JSON.\n * Each run produces a single file in `.agents-reverse-engineer/logs/`\n * with a filename derived from the run's start timestamp.\n *\n * @module\n */\n\nimport * as fs from 'node:fs/promises';\nimport * as path from 'node:path';\nimport type { RunLog } from '../types.js';\n\n/** Directory name for telemetry log files (relative to project root) */\nconst LOGS_DIR = '.agents-reverse-engineer/logs';\n\n/**\n * Write a completed run log to disk as pretty-printed JSON.\n *\n * Creates the logs directory if it does not exist. The filename is derived\n * from the run's `startTime` field with `:` and `.` replaced by `-` so\n * that it forms a valid filename on all platforms.\n *\n * @param projectRoot - Absolute path to the project root directory\n * @param runLog - The completed run log to write\n * @returns Absolute path to the written file\n *\n * @example\n * ```typescript\n * const logPath = await writeRunLog('/home/user/project', runLog);\n * // logPath: /home/user/project/.agents-reverse-engineer/logs/run-2026-02-07T12-00-00-000Z.json\n * ```\n */\nexport async function writeRunLog(projectRoot: string, runLog: RunLog): Promise<string> {\n  const logsDir = path.join(projectRoot, LOGS_DIR);\n\n  // Create the logs directory if it does not exist\n  await fs.mkdir(logsDir, { recursive: true });\n\n  // Build filename: replace : and . with - so ISO timestamps become valid filenames\n  const safeTimestamp = runLog.startTime.replace(/[:.]/g, '-');\n  const filename = `run-${safeTimestamp}.json`;\n  const filePath = path.join(logsDir, filename);\n\n  // Write pretty-printed JSON\n  await fs.writeFile(filePath, JSON.stringify(runLog, null, 2), 'utf-8');\n\n  return filePath;\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**run-log.ts persists completed AI telemetry RunLog objects as pretty-printed JSON files in `.agents-reverse-engineer/logs/` with ISO-8601-derived filenames.**\n\n## Exported Functions\n\n### writeRunLog\n\n```typescript\nasync function writeRunLog(projectRoot: string, runLog: RunLog): Promise<string>\n```\n\nWrites a completed `RunLog` to disk as pretty-printed JSON (2-space indentation). Creates `<projectRoot>/.agents-reverse-engineer/logs/` directory via `fs.mkdir(logsDir, { recursive: true })` if missing. Derives filename from `runLog.startTime` by replacing `:` and `.` characters with `-` to ensure cross-platform filesystem compatibility, producing pattern `run-2026-02-07T12-00-00-000Z.json`. Returns absolute path to written file.\n\n## Constants\n\n`LOGS_DIR` — String constant `'.agents-reverse-engineer/logs'` defining relative path from project root to telemetry log storage directory.\n\n## Dependencies\n\n- `node:fs/promises` — Async file operations (`mkdir`, `writeFile`)\n- `node:path` — Path manipulation (`join`)\n- `../types.js` — Imports `RunLog` type interface\n\n## Integration Points\n\nConsumed by telemetry logger (`src/ai/telemetry/logger.ts`) during AIService call completion to persist aggregated token counts, costs, durations, and file metadata. Output files feed into retention management via cleanup utilities (`src/ai/telemetry/cleanup.ts`) enforcing `keepRuns` limits from configuration schema.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 375,
      "cacheReadTokens": 0,
      "cacheCreationTokens": 27012,
      "latencyMs": 20144,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/ai/telemetry/run-log.ts",
          "sizeBytes": 1766
        }
      ]
    },
    {
      "timestamp": "2026-02-09T17:43:35.638Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/ai/telemetry/cleanup.ts\n\n```typescript\n/**\n * Telemetry log cleanup utility.\n *\n * Removes old run log files from `.agents-reverse-engineer/logs/`,\n * keeping only the N most recent. Files are sorted lexicographically\n * by name, which works correctly because filenames contain ISO timestamps.\n *\n * @module\n */\n\nimport * as fs from 'node:fs/promises';\nimport * as path from 'node:path';\n\n/** Directory name for telemetry log files (relative to project root) */\nconst LOGS_DIR = '.agents-reverse-engineer/logs';\n\n/**\n * Remove old telemetry log files, keeping only the most recent ones.\n *\n * Reads the logs directory, filters for files matching `run-*.json`,\n * sorts newest-first (lexicographic sort on ISO timestamp filenames),\n * and deletes everything beyond `keepCount`.\n *\n * If the logs directory does not exist, returns 0 without error.\n *\n * @param projectRoot - Absolute path to the project root directory\n * @param keepCount - Number of most recent log files to retain\n * @returns Number of files deleted\n *\n * @example\n * ```typescript\n * const deleted = await cleanupOldLogs('/home/user/project', 10);\n * console.log(`Cleaned up ${deleted} old log files`);\n * ```\n */\nexport async function cleanupOldLogs(projectRoot: string, keepCount: number): Promise<number> {\n  const logsDir = path.join(projectRoot, LOGS_DIR);\n\n  let entries: string[];\n  try {\n    const allEntries = await fs.readdir(logsDir);\n    entries = allEntries.filter(\n      (name) => name.startsWith('run-') && name.endsWith('.json'),\n    );\n  } catch (error) {\n    // Directory doesn't exist -- nothing to clean up\n    if ((error as NodeJS.ErrnoException).code === 'ENOENT') {\n      return 0;\n    }\n    throw error;\n  }\n\n  // Sort lexicographically (newest first) and find files to delete\n  entries.sort();\n  entries.reverse();\n\n  const toDelete = entries.slice(keepCount);\n\n  for (const filename of toDelete) {\n    await fs.unlink(path.join(logsDir, filename));\n  }\n\n  return toDelete.length;\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**cleanupOldLogs removes expired telemetry run logs from `.agents-reverse-engineer/logs/` by lexicographic timestamp sorting.**\n\n## Exported Function\n\n`cleanupOldLogs(projectRoot: string, keepCount: number): Promise<number>` deletes telemetry log files exceeding retention limit. Returns count of deleted files. Scans `LOGS_DIR` (`'.agents-reverse-engineer/logs'`) for files matching pattern `run-*.json`, sorts lexicographically in reverse order (newest first via ISO 8601 timestamp filenames), slices array at `keepCount` index to identify deletion candidates, unlinks each file via `fs.unlink()`, returns `toDelete.length`. Returns `0` when logs directory does not exist (`ENOENT` error code). Throws on other filesystem errors.\n\n## Algorithm\n\nReads directory entries with `fs.readdir(logsDir)`, filters for files where `name.startsWith('run-')` and `name.endsWith('.json')`, applies `entries.sort()` followed by `entries.reverse()` for descending lexicographic order, extracts slice `entries.slice(keepCount)` as deletion targets, iterates over `toDelete` array executing `fs.unlink(path.join(logsDir, filename))` for each entry.\n\n## Integration Point\n\nCalled by `TelemetryLogger` after writing run logs to enforce retention policy specified in `config.ai.telemetry.keepRuns` (default 50). Prevents unbounded log accumulation during repeated `are generate` and `are update` operations.\n\n## Directory Constant\n\n`LOGS_DIR` constant defines relative path `'.agents-reverse-engineer/logs'` joined with `projectRoot` via `path.join()` to compute absolute directory path.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 413,
      "cacheReadTokens": 0,
      "cacheCreationTokens": 27054,
      "latencyMs": 20664,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/ai/telemetry/cleanup.ts",
          "sizeBytes": 1937
        }
      ]
    },
    {
      "timestamp": "2026-02-09T17:43:35.636Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/ai/backends/opencode.ts\n\n```typescript\n/**\n * OpenCode CLI backend stub.\n *\n * Implements the {@link AIBackend} interface for the OpenCode CLI (`opencode`).\n * This is a stub that demonstrates the extension pattern -- `parseResponse`\n * throws \"not implemented\". Full implementation deferred to a future phase\n * once the OpenCode JSONL output parsing is built (see RESEARCH.md Open Question 3).\n *\n * @module\n */\n\nimport type { AIBackend, AICallOptions, AIResponse } from '../types.js';\nimport { AIServiceError } from '../types.js';\nimport { isCommandOnPath } from './claude.js';\n\n/**\n * OpenCode CLI backend stub.\n *\n * Detects CLI availability and builds argument arrays, but throws when\n * `parseResponse` is called since the OpenCode adapter is not yet implemented.\n *\n * @example\n * ```typescript\n * const backend = new OpenCodeBackend();\n * console.log(await backend.isAvailable()); // true if `opencode` is on PATH\n * backend.parseResponse('{}', 0, 0);        // throws AIServiceError\n * ```\n */\nexport class OpenCodeBackend implements AIBackend {\n  readonly name = 'opencode';\n  readonly cliCommand = 'opencode';\n\n  /**\n   * Check if the `opencode` CLI is available on PATH.\n   */\n  async isAvailable(): Promise<boolean> {\n    return isCommandOnPath(this.cliCommand);\n  }\n\n  /**\n   * Build CLI arguments for an OpenCode invocation.\n   *\n   * Based on documented OpenCode CLI flags from RESEARCH.md.\n   * The prompt goes to stdin via the subprocess wrapper.\n   *\n   * @param _options - Call options (unused in stub)\n   * @returns Argument array for the OpenCode CLI\n   */\n  buildArgs(_options: AICallOptions): string[] {\n    return ['run', '--format', 'json'];\n  }\n\n  /**\n   * Parse OpenCode CLI output into a normalized {@link AIResponse}.\n   *\n   * @throws {AIServiceError} Always -- OpenCode backend is not yet implemented\n   */\n  parseResponse(_stdout: string, _durationMs: number, _exitCode: number): AIResponse {\n    throw new AIServiceError(\n      'SUBPROCESS_ERROR',\n      'OpenCode backend is not yet implemented. Use Claude backend.',\n    );\n  }\n\n  /**\n   * Get user-facing install instructions for the OpenCode CLI.\n   */\n  getInstallInstructions(): string {\n    return [\n      'OpenCode (experimental):',\n      '  curl -fsSL https://opencode.ai/install | bash',\n      '  https://opencode.ai',\n    ].join('\\n');\n  }\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**OpenCodeBackend implements AIBackend interface as a stub demonstrating extension pattern for OpenCode CLI integration, throwing \"not implemented\" errors until JSONL output parsing is built.**\n\n## Exported Class\n\n`OpenCodeBackend` implements `AIBackend` interface with four readonly/method members: `name: 'opencode'`, `cliCommand: 'opencode'`, `isAvailable(): Promise<boolean>`, `buildArgs(_options: AICallOptions): string[]`, `parseResponse(_stdout: string, _durationMs: number, _exitCode: number): AIResponse`, `getInstallInstructions(): string`.\n\n## Implementation Status\n\n`parseResponse()` throws `AIServiceError` with code `'SUBPROCESS_ERROR'` and message `'OpenCode backend is not yet implemented. Use Claude backend.'` blocking all AI calls until JSONL parsing implemented (deferred per RESEARCH.md Open Question 3).\n\n## CLI Interaction\n\n`isAvailable()` delegates to `isCommandOnPath(this.cliCommand)` imported from `claude.ts` module to detect `opencode` binary on PATH.\n\n`buildArgs()` returns `['run', '--format', 'json']` based on documented OpenCode CLI flags from RESEARCH.md, expects prompt via stdin through subprocess wrapper.\n\n`getInstallInstructions()` returns three-line string with curl install command `'curl -fsSL https://opencode.ai/install | bash'` and URL `'https://opencode.ai'`.\n\n## Integration Points\n\nImports `AIBackend`, `AICallOptions`, `AIResponse` from `../types.js` and `AIServiceError` from same module for error throwing. Imports `isCommandOnPath` utility from `./claude.ts` for CLI detection. Follows same interface contract as `ClaudeBackend` and `GeminiBackend` siblings, enabling registration in `src/ai/registry.ts` backend map.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 444,
      "cacheReadTokens": 0,
      "cacheCreationTokens": 27137,
      "latencyMs": 20864,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/ai/backends/opencode.ts",
          "sizeBytes": 2298
        }
      ]
    },
    {
      "timestamp": "2026-02-09T17:43:35.650Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/discovery/filters/gitignore.ts\n\n```typescript\n/**\n * Gitignore pattern filter for file discovery.\n *\n * Uses the `ignore` library to parse and match .gitignore patterns.\n * This filter loads the root .gitignore file and checks paths against\n * the patterns to determine exclusion.\n */\n\nimport ignore, { type Ignore } from 'ignore';\nimport fs from 'node:fs/promises';\nimport path from 'node:path';\nimport type { FileFilter } from '../types.js';\n\n/**\n * Creates a gitignore filter that excludes files matching .gitignore patterns.\n *\n * @param root - The root directory containing the .gitignore file\n * @returns A FileFilter that checks paths against gitignore patterns\n *\n * @example\n * ```typescript\n * const filter = await createGitignoreFilter('/path/to/project');\n * if (filter.shouldExclude('/path/to/project/dist/bundle.js')) {\n *   console.log('File is gitignored');\n * }\n * ```\n */\nexport async function createGitignoreFilter(root: string): Promise<FileFilter> {\n  const ig: Ignore = ignore();\n  const normalizedRoot = path.resolve(root);\n\n  // Load .gitignore from root if it exists\n  const gitignorePath = path.join(normalizedRoot, '.gitignore');\n  try {\n    const content = await fs.readFile(gitignorePath, 'utf-8');\n    ig.add(content);\n  } catch {\n    // No .gitignore file, filter will pass everything through\n  }\n\n  return {\n    name: 'gitignore',\n\n    shouldExclude(absolutePath: string): boolean {\n      // Convert to relative path (ignore library requires relative paths)\n      const relativePath = path.relative(normalizedRoot, absolutePath);\n\n      // If path is outside root (starts with ..) or is empty, don't exclude\n      if (!relativePath || relativePath.startsWith('..')) {\n        return false;\n      }\n\n      // CRITICAL: The ignore library treats paths differently based on trailing slash\n      // We check the path as-is (for files) - directory handling would need trailing slash\n      // Since our walker returns files only, we don't append slash here\n      return ig.ignores(relativePath);\n    },\n  };\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**createGitignoreFilter() returns a FileFilter that excludes files matching .gitignore patterns using the ignore library with relative path normalization.**\n\n## Exported Function\n\n**createGitignoreFilter(root: string): Promise<FileFilter>**\n- Resolves root directory via path.resolve()\n- Attempts to read .gitignore from `path.join(normalizedRoot, '.gitignore')` using fs.readFile() with utf-8 encoding\n- Creates Ignore instance via ignore() library\n- Adds .gitignore content to Ignore instance via ig.add(content)\n- Silently continues on read failure (missing .gitignore passes all paths)\n- Returns FileFilter object with name property `'gitignore'` and shouldExclude() method\n\n## FileFilter Implementation\n\n**shouldExclude(absolutePath: string): boolean**\n- Converts absolutePath to relativePath via path.relative(normalizedRoot, absolutePath)\n- Returns false if relativePath is empty or starts with `'..'` (paths outside root)\n- Delegates to ig.ignores(relativePath) for pattern matching\n- Passes relative paths without trailing slash (file-only mode, not directory mode)\n\n## Dependencies\n\n- `ignore` library (Ignore type) for .gitignore pattern parsing and matching\n- `node:fs/promises` for async file reading (fs.readFile)\n- `node:path` for path normalization (path.resolve, path.join, path.relative)\n- `../types.js` for FileFilter interface\n\n## Critical Behavior\n\nThe ignore library treats paths differently based on trailing slashes. This implementation passes relativePath without appending slash because walker returns files only, not directories. Directory exclusion would require `relativePath + '/'` pattern.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 405,
      "cacheReadTokens": 0,
      "cacheCreationTokens": 27031,
      "latencyMs": 20893,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/discovery/filters/gitignore.ts",
          "sizeBytes": 1990
        }
      ]
    },
    {
      "timestamp": "2026-02-09T17:43:35.659Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/discovery/filters/vendor.ts\n\n```typescript\n/**\n * Vendor directory filter for file discovery.\n *\n * Excludes files within common vendor/dependency directories that typically\n * contain third-party code not relevant for documentation purposes.\n */\n\nimport path from 'node:path';\nimport type { FileFilter } from '../types.js';\n\n/**\n * Default vendor directories to exclude.\n * These are common directories containing third-party code, build output,\n * or generated files that should not be analyzed.\n */\nexport const DEFAULT_VENDOR_DIRS = [\n  'node_modules',\n  'vendor',\n  '.git',\n  'dist',\n  'build',\n  '__pycache__',\n  '.next',\n  'venv',\n  '.venv',\n  'target',\n] as const;\n\n/**\n * Creates a vendor filter that excludes files within specified directories.\n *\n * Supports two patterns:\n * - Single directory names (e.g., 'node_modules') - matches anywhere in path\n * - Path patterns (e.g., 'apps/vendor' or '.agents/skills') - matches path containing this sequence\n *\n * @param vendorDirs - Array of directory names or path patterns to exclude.\n * @returns A FileFilter that checks if a path is within a vendor directory\n *\n * @example\n * ```typescript\n * const filter = createVendorFilter(['node_modules', '.agents/skills']);\n * filter.shouldExclude('/project/node_modules/lodash/index.js'); // true\n * filter.shouldExclude('/project/apps/foo/.agents/skills/bar.md'); // true\n * filter.shouldExclude('/project/src/utils.js'); // false\n * ```\n */\nexport function createVendorFilter(vendorDirs: string[]): FileFilter {\n  // Separate single segments from path patterns\n  const singleSegments = new Set<string>();\n  const pathPatterns: string[] = [];\n\n  for (const dir of vendorDirs) {\n    // Normalize path separators to current OS\n    const normalized = dir.replace(/[\\\\/]/g, path.sep);\n    if (normalized.includes(path.sep)) {\n      pathPatterns.push(normalized);\n    } else {\n      singleSegments.add(dir);\n    }\n  }\n\n  return {\n    name: 'vendor',\n\n    shouldExclude(absolutePath: string): boolean {\n      // Check single segment matches\n      const segments = absolutePath.split(path.sep);\n      for (const segment of segments) {\n        if (singleSegments.has(segment)) {\n          return true;\n        }\n      }\n\n      // Check path pattern matches\n      for (const pattern of pathPatterns) {\n        if (absolutePath.includes(pattern)) {\n          return true;\n        }\n      }\n\n      return false;\n    },\n  };\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**createVendorFilter() constructs a FileFilter that excludes files within third-party dependency directories using both single-segment matching and path pattern matching.**\n\n## Exports\n\n- **`DEFAULT_VENDOR_DIRS`**: `readonly string[]` — Constant array of 10 common vendor directory names: `'node_modules'`, `'vendor'`, `'.git'`, `'dist'`, `'build'`, `'__pycache__'`, `'.next'`, `'venv'`, `'.venv'`, `'target'`.\n\n- **`createVendorFilter(vendorDirs: string[]): FileFilter`** — Factory function that returns a `FileFilter` object with `name: 'vendor'` and `shouldExclude()` method.\n\n## Matching Strategy\n\ncreateVendorFilter() implements two-tier pattern matching:\n\n1. **Single-segment matching**: Directory names without path separators (e.g., `'node_modules'`) are stored in a `Set<string>` and matched against any segment in the absolute path via `absolutePath.split(path.sep)`.\n\n2. **Path pattern matching**: Patterns containing path separators (e.g., `'apps/vendor'`, `'.agents/skills'`) are normalized to the OS-specific separator via `dir.replace(/[\\\\/]/g, path.sep)` and matched via `absolutePath.includes(pattern)`.\n\n## FileFilter Interface\n\nThe returned object conforms to `FileFilter` from `../types.js`:\n\n```typescript\n{\n  name: string;\n  shouldExclude(absolutePath: string): boolean;\n}\n```\n\nshouldExclude() returns `true` if the absolute path contains any configured vendor directory as a path segment or substring pattern, `false` otherwise.\n\n## Integration Points\n\n- Consumed by filter chain in `src/discovery/filters/index.ts` as part of composable file exclusion logic.\n- DEFAULT_VENDOR_DIRS referenced in `src/config/defaults.ts` for default configuration schema.\n- Used during file discovery phase (`src/discovery/walker.ts`) to skip third-party code directories before binary detection and gitignore filtering.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 501,
      "cacheReadTokens": 0,
      "cacheCreationTokens": 27160,
      "latencyMs": 21509,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/discovery/filters/vendor.ts",
          "sizeBytes": 2375
        }
      ]
    },
    {
      "timestamp": "2026-02-09T17:43:35.712Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/generation/prompts/types.ts\n\n```typescript\n/**\n * Context provided when building a prompt.\n */\nexport interface PromptContext {\n  /** Absolute path to the file */\n  filePath: string;\n  /** File content to analyze */\n  content: string;\n  /** Related files for additional context */\n  contextFiles?: Array<{\n    path: string;\n    content: string;\n  }>;\n  /** Project structure listing for bird's-eye context */\n  projectPlan?: string;\n  /** Existing .sum summary text for incremental updates */\n  existingSum?: string;\n}\n\n/**\n * Guidelines for summary generation (from CONTEXT.md).\n */\nexport const SUMMARY_GUIDELINES = {\n  /** Target word count range */\n  targetLength: { min: 300, max: 500 },\n  /** What to include */\n  include: [\n    'Purpose and responsibility',\n    'Public interface (exports, key functions)',\n    'Key patterns and notable algorithms',\n    'Dependencies with usage context',\n    'Key function signatures as code snippets',\n    'Tightly coupled sibling files',\n    'Behavioral contracts: verbatim regex patterns, format strings, magic constants, sentinel values, output templates, environment variables',\n    'Annex references: for files with large string constants (prompt templates, config arrays, IDE templates), list each constant name with a one-line description in an ## Annex References section',\n  ],\n  /** What to exclude */\n  exclude: [\n    'Control flow minutiae (loop structures, variable naming, temporary state)',\n    'Generic TODOs/FIXMEs (keep only security/breaking)',\n    'Broad architectural relationships (handled by AGENTS.md)',\n  ],\n} as const;\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**Defines TypeScript interfaces and constants for prompt construction context in the three-phase documentation generation pipeline.**\n\n## Exported Types\n\n**`PromptContext`** — Interface providing file analysis context to prompt builders during Phase 1 (file analysis) and Phase 2 (directory aggregation):\n- `filePath: string` — Absolute path to file being analyzed\n- `content: string` — Raw file content to analyze\n- `contextFiles?: Array<{path: string; content: string}>` — Optional related files for dependency context\n- `projectPlan?: string` — Optional GENERATION-PLAN.md content providing bird's-eye project structure\n- `existingSum?: string` — Optional existing `.sum` summary text for incremental update workflows (enables delta-aware regeneration)\n\n## Documentation Guidelines\n\n**`SUMMARY_GUIDELINES`** — Immutable configuration object (as const) defining documentation generation constraints consumed by `src/generation/prompts/builder.ts` and `src/generation/prompts/templates.ts`:\n\n**`targetLength`** — Word count boundaries: `{min: 300, max: 500}`\n\n**`include`** — Array of 8 mandatory content categories:\n1. Purpose and responsibility\n2. Public interface (exports, key functions)\n3. Key patterns and notable algorithms\n4. Dependencies with usage context\n5. Key function signatures as code snippets\n6. Tightly coupled sibling files\n7. Behavioral contracts: verbatim regex patterns, format strings, magic constants, sentinel values, output templates, environment variables\n8. Annex references: for files with large string constants (prompt templates, config arrays, IDE templates), list each constant name with a one-line description in an ## Annex References section\n\n**`exclude`** — Array of 3 prohibited content categories:\n1. Control flow minutiae (loop structures, variable naming, temporary state)\n2. Generic TODOs/FIXMEs (keep only security/breaking)\n3. Broad architectural relationships (handled by AGENTS.md)\n\n## Integration Points\n\nConsumed by `PromptBuilder` class in `src/generation/prompts/builder.ts` for constructing AI prompts with standardized documentation quality constraints. The `existingSum` field enables incremental update workflows orchestrated by `src/update/orchestrator.ts` where hash-mismatched files receive their previous summary as context for delta-aware regeneration.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 546,
      "cacheReadTokens": 0,
      "cacheCreationTokens": 26864,
      "latencyMs": 21584,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/generation/prompts/types.ts",
          "sizeBytes": 1544
        }
      ]
    },
    {
      "timestamp": "2026-02-09T17:43:35.671Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/generation/prompts/index.ts\n\n```typescript\nexport type {\n  PromptContext,\n} from './types.js';\nexport { SUMMARY_GUIDELINES } from './types.js';\nexport {\n  buildFilePrompt,\n  buildDirectoryPrompt,\n  buildRootPrompt,\n  detectLanguage,\n} from './builder.js';\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**Barrel module exporting prompt construction API for three-phase documentation generation: file analysis, directory aggregation, and root synthesis.**\n\n## Exported Types\n\n- `PromptContext` — Context object passed to prompt builders containing file metadata, import maps, child summaries, and configuration\n- `SUMMARY_GUIDELINES` — Constant string containing behavioral contracts for AI-generated summaries (density rules, anchor term preservation, reproduction-critical content handling)\n\n## Exported Functions\n\n- `buildFilePrompt(context: PromptContext): string` — Constructs Phase 1 prompt for `.sum` file generation from source code with import maps and language detection\n- `buildDirectoryPrompt(context: PromptContext): string` — Constructs Phase 2 prompt for `AGENTS.md` synthesis from child `.sum` files, subdirectory docs, and manifest detection\n- `buildRootPrompt(context: PromptContext): string` — Constructs Phase 3 prompt for root integration documents (`CLAUDE.md`, `GEMINI.md`, `OPENCODE.md`) from aggregated `AGENTS.md` corpus\n- `detectLanguage(filePath: string): string` — Maps file extensions to language names for prompt context (e.g., `.ts` → `\"TypeScript\"`, `.py` → `\"Python\"`)\n\n## Module Organization\n\nRe-exports from `./types.js` provide shared interfaces and constants, while `./builder.js` contains concrete prompt construction logic. Separates prompt template strings (in `./templates.js`) from assembly logic.\n\n## Integration Points\n\nUsed by `src/generation/executor.ts` Phase 1/2/3 runners to construct AI service prompts. `PromptContext` populated by orchestrator with discovery results, import extractor output, and child document content via `src/generation/collector.ts`.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 421,
      "cacheReadTokens": 0,
      "cacheCreationTokens": 26537,
      "latencyMs": 22645,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/generation/prompts/index.ts",
          "sizeBytes": 213
        }
      ]
    },
    {
      "timestamp": "2026-02-09T17:43:35.634Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/ai/backends/gemini.ts\n\n```typescript\n/**\n * Gemini CLI backend stub.\n *\n * Implements the {@link AIBackend} interface for the Gemini CLI (`gemini`).\n * This is a stub that demonstrates the extension pattern -- `parseResponse`\n * throws \"not implemented\". Full implementation deferred to a future phase\n * once the Gemini CLI JSON output is stable (see RESEARCH.md Open Question 2).\n *\n * @module\n */\n\nimport type { AIBackend, AICallOptions, AIResponse } from '../types.js';\nimport { AIServiceError } from '../types.js';\nimport { isCommandOnPath } from './claude.js';\n\n/**\n * Gemini CLI backend stub.\n *\n * Detects CLI availability and builds argument arrays, but throws when\n * `parseResponse` is called since the Gemini adapter is not yet implemented.\n *\n * @example\n * ```typescript\n * const backend = new GeminiBackend();\n * console.log(await backend.isAvailable()); // true if `gemini` is on PATH\n * backend.parseResponse('{}', 0, 0);        // throws AIServiceError\n * ```\n */\nexport class GeminiBackend implements AIBackend {\n  readonly name = 'gemini';\n  readonly cliCommand = 'gemini';\n\n  /**\n   * Check if the `gemini` CLI is available on PATH.\n   */\n  async isAvailable(): Promise<boolean> {\n    return isCommandOnPath(this.cliCommand);\n  }\n\n  /**\n   * Build CLI arguments for a Gemini invocation.\n   *\n   * Based on documented Gemini CLI flags from RESEARCH.md.\n   * The prompt goes to stdin via the subprocess wrapper.\n   *\n   * @param _options - Call options (unused in stub)\n   * @returns Argument array for the Gemini CLI\n   */\n  buildArgs(_options: AICallOptions): string[] {\n    return ['-p', '--output-format', 'json'];\n  }\n\n  /**\n   * Parse Gemini CLI output into a normalized {@link AIResponse}.\n   *\n   * @throws {AIServiceError} Always -- Gemini backend is not yet implemented\n   */\n  parseResponse(_stdout: string, _durationMs: number, _exitCode: number): AIResponse {\n    throw new AIServiceError(\n      'SUBPROCESS_ERROR',\n      'Gemini backend is not yet implemented. Use Claude backend.',\n    );\n  }\n\n  /**\n   * Get user-facing install instructions for the Gemini CLI.\n   */\n  getInstallInstructions(): string {\n    return [\n      'Gemini CLI (experimental):',\n      '  npm install -g @anthropic-ai/gemini-cli',\n      '  https://github.com/google-gemini/gemini-cli',\n    ].join('\\n');\n  }\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**GeminiBackend implements AIBackend interface as a stub adapter for the Gemini CLI, providing command detection and argument construction but throwing AIServiceError on response parsing until JSON output format stabilizes.**\n\n## Implementation Status\n\nGeminiBackend is a non-functional stub demonstrating the extension pattern for future Gemini CLI integration. The `parseResponse()` method always throws `AIServiceError` with code `'SUBPROCESS_ERROR'` and message `'Gemini backend is not yet implemented. Use Claude backend.'` This deferred implementation awaits stable JSON output from the Gemini CLI (referenced as \"RESEARCH.md Open Question 2\" in module documentation).\n\n## Exported Class\n\n`export class GeminiBackend implements AIBackend` provides:\n\n- `readonly name = 'gemini'` — Backend identifier string used by AIBackendRegistry\n- `readonly cliCommand = 'gemini'` — Executable name for PATH resolution\n\n## Public Methods\n\n`async isAvailable(): Promise<boolean>` delegates to `isCommandOnPath(this.cliCommand)` imported from `claude.ts` to detect `gemini` binary on system PATH.\n\n`buildArgs(_options: AICallOptions): string[]` constructs CLI argument array `['-p', '--output-format', 'json']` following documented Gemini CLI flags. The prompt is injected via stdin by the subprocess wrapper in `src/ai/subprocess.ts`. The `_options` parameter is unused in this stub implementation.\n\n`parseResponse(_stdout: string, _durationMs: number, _exitCode: number): AIResponse` unconditionally throws `AIServiceError` with `'SUBPROCESS_ERROR'` code, blocking actual response parsing until full implementation.\n\n`getInstallInstructions(): string` returns multiline installation guide string with NPM install command `npm install -g @anthropic-ai/gemini-cli` and GitHub repository URL `https://github.com/google-gemini/gemini-cli`.\n\n## Dependencies\n\nImports `AIBackend`, `AICallOptions`, `AIResponse`, and `AIServiceError` from `../types.js`. Imports `isCommandOnPath` utility from `./claude.js` for command detection logic.\n\n## Integration Points\n\nRegistered with AIBackendRegistry in `src/ai/registry.ts` alongside ClaudeBackend and OpenCodeBackend. The `isAvailable()` check determines backend auto-detection fallback order when `ai.backend: 'auto'` configured in `.agents-reverse-engineer/config.yaml`. Despite registration, any attempt to use this backend for actual AI calls will fail at response parsing phase.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 598,
      "cacheReadTokens": 0,
      "cacheCreationTokens": 27162,
      "latencyMs": 22736,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/ai/backends/gemini.ts",
          "sizeBytes": 2281
        }
      ]
    },
    {
      "timestamp": "2026-02-09T17:43:35.641Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/ai/telemetry/logger.ts\n\n```typescript\n/**\n * In-memory telemetry logger for AI service calls.\n *\n * Accumulates {@link TelemetryEntry} instances during a run and computes\n * aggregate summaries. The logger is created once per CLI invocation and\n * finalized when the run completes.\n *\n * @module\n */\n\nimport type { TelemetryEntry, RunLog, FileRead } from '../types.js';\n\n/**\n * Accumulates per-call telemetry entries in memory and produces a\n * complete {@link RunLog} when the run finishes.\n *\n * @example\n * ```typescript\n * const logger = new TelemetryLogger('2026-02-07T12:00:00.000Z');\n * logger.addEntry(entry);\n * const summary = logger.getSummary();\n * const runLog = logger.toRunLog();\n * ```\n */\nexport class TelemetryLogger {\n  /** Unique identifier for this run (ISO timestamp-based) */\n  readonly runId: string;\n\n  /** ISO 8601 timestamp when the run started */\n  readonly startTime: string;\n\n  /** Accumulated telemetry entries */\n  private readonly entries: TelemetryEntry[] = [];\n\n  /**\n   * Create a new telemetry logger for a run.\n   *\n   * @param runId - Unique run identifier (typically an ISO timestamp)\n   */\n  constructor(runId: string) {\n    this.runId = runId;\n    this.startTime = new Date().toISOString();\n  }\n\n  /**\n   * Record a telemetry entry for a completed AI call.\n   *\n   * @param entry - The telemetry entry to record\n   */\n  addEntry(entry: TelemetryEntry): void {\n    this.entries.push(entry);\n  }\n\n  /**\n   * Get all recorded entries as a read-only array.\n   *\n   * @returns Immutable view of the accumulated entries\n   */\n  getEntries(): readonly TelemetryEntry[] {\n    return this.entries;\n  }\n\n  /**\n   * Update the most recent entry's filesRead array.\n   *\n   * Called by the AI service after the command runner attaches file\n   * metadata to the last call.\n   *\n   * @param filesRead - Array of file-read records to attach\n   */\n  setFilesReadOnLastEntry(filesRead: FileRead[]): void {\n    if (this.entries.length === 0) return;\n    this.entries[this.entries.length - 1]!.filesRead = filesRead;\n  }\n\n  /**\n   * Compute aggregate summary statistics from all recorded entries.\n   *\n   * Totals are computed on every call (not cached) so the summary\n   * always reflects the current state of the entries array.\n   *\n   * @returns Summary with totals for calls, tokens, duration, and errors\n   */\n  getSummary(): RunLog['summary'] {\n    let totalInputTokens = 0;\n    let totalOutputTokens = 0;\n    let totalCacheReadTokens = 0;\n    let totalCacheCreationTokens = 0;\n    let totalDurationMs = 0;\n    let errorCount = 0;\n    let totalFilesRead = 0;\n    const uniqueFilePaths = new Set<string>();\n\n    for (const entry of this.entries) {\n      totalInputTokens += entry.inputTokens;\n      totalOutputTokens += entry.outputTokens;\n      totalCacheReadTokens += entry.cacheReadTokens;\n      totalCacheCreationTokens += entry.cacheCreationTokens;\n      totalDurationMs += entry.latencyMs;\n      if (entry.error !== undefined) {\n        errorCount++;\n      }\n      totalFilesRead += entry.filesRead.length;\n      for (const file of entry.filesRead) {\n        uniqueFilePaths.add(file.path);\n      }\n    }\n\n    return {\n      totalCalls: this.entries.length,\n      totalInputTokens,\n      totalOutputTokens,\n      totalCacheReadTokens,\n      totalCacheCreationTokens,\n      totalDurationMs,\n      errorCount,\n      totalFilesRead,\n      uniqueFilesRead: uniqueFilePaths.size,\n    };\n  }\n\n  /**\n   * Assemble the complete {@link RunLog} for this run.\n   *\n   * Sets `endTime` to the current time, includes all entries, and\n   * computes the summary. Call this once when the run is finished.\n   *\n   * @returns Complete run log ready for serialization\n   */\n  toRunLog(): RunLog {\n    return {\n      runId: this.runId,\n      startTime: this.startTime,\n      endTime: new Date().toISOString(),\n      entries: [...this.entries],\n      summary: this.getSummary(),\n    };\n  }\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**TelemetryLogger accumulates per-call AI service metrics in memory during a CLI run and computes aggregate statistics for serialization into RunLog objects.**\n\n## Exported Class\n\n`TelemetryLogger` — In-memory accumulator for `TelemetryEntry` instances created during a single CLI invocation. Tracks runId (unique identifier, typically ISO timestamp), startTime (ISO 8601 creation timestamp), and private entries array. Constructor accepts `runId: string`.\n\n## Public Methods\n\n`addEntry(entry: TelemetryEntry): void` — Appends a telemetry entry to the internal entries array after an AI service call completes.\n\n`getEntries(): readonly TelemetryEntry[]` — Returns immutable view of accumulated entries array.\n\n`setFilesReadOnLastEntry(filesRead: FileRead[]): void` — Mutates the most recent entry's `filesRead` field with file metadata. Returns early if entries array is empty. Called by AIService after command runner attaches file read records to the last call.\n\n`getSummary(): RunLog['summary']` — Computes aggregate statistics from all recorded entries on every invocation (not cached). Iterates entries array to sum `totalInputTokens`, `totalOutputTokens`, `totalCacheReadTokens`, `totalCacheCreationTokens`, `totalDurationMs`, counts entries with `error !== undefined` for `errorCount`, sums `filesRead.length` for `totalFilesRead`, deduplicates file paths into `Set<string>` for `uniqueFilesRead`. Returns object with `totalCalls: this.entries.length` plus computed totals.\n\n`toRunLog(): RunLog` — Assembles complete `RunLog` object for serialization. Sets `endTime` to `new Date().toISOString()`, spreads entries array into new array, calls `getSummary()` for summary field. Intended for single invocation when run finishes.\n\n## Integration Points\n\nConsumes `TelemetryEntry`, `RunLog`, `FileRead` types from `../types.js`. Created once per CLI invocation by telemetry system. Lifecycle: instantiate with runId → accumulate entries via `addEntry()` → optionally update last entry with `setFilesReadOnLastEntry()` → finalize via `toRunLog()` when run completes. Used by `src/ai/telemetry/run-log.ts` for writing NDJSON run logs to `.agents-reverse-engineer/logs/`.\n\n## Data Structures\n\nMaintains single-level flat array of `TelemetryEntry` objects. No hierarchical grouping or phase-aware partitioning. Summary computation uses transient `Set<string>` for path deduplication without persisting the set.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 631,
      "cacheReadTokens": 0,
      "cacheCreationTokens": 27666,
      "latencyMs": 23348,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/ai/telemetry/logger.ts",
          "sizeBytes": 3866
        }
      ]
    },
    {
      "timestamp": "2026-02-09T17:43:35.655Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/discovery/filters/index.ts\n\n```typescript\n/**\n * Filter chain orchestration for file discovery.\n *\n * This module exports all filter creators and provides the applyFilters\n * function that runs files through the filter chain, recording which\n * filter excluded each file.\n */\n\nimport pc from 'picocolors';\nimport type { FileFilter, FilterResult, ExcludedFile } from '../types.js';\nimport type { ITraceWriter } from '../../orchestration/trace.js';\n\n// Re-export all filter creators\nexport { createGitignoreFilter } from './gitignore.js';\nexport { createVendorFilter, DEFAULT_VENDOR_DIRS } from './vendor.js';\nexport { createBinaryFilter, BINARY_EXTENSIONS, type BinaryFilterOptions } from './binary.js';\nexport { createCustomFilter } from './custom.js';\n\n/**\n * Applies a chain of filters to a list of files.\n *\n * Each file is run through filters in order until one excludes it\n * (short-circuit evaluation). Files are processed with bounded concurrency\n * to avoid opening too many file handles simultaneously (important for\n * binary content detection which performs file I/O).\n *\n * @param files - Array of absolute file paths to filter\n * @param filters - Array of filters to apply in order\n * @param options - Optional tracing and debug options\n * @returns Promise resolving to FilterResult with included and excluded lists\n *\n * @example\n * ```typescript\n * const filters = [\n *   createVendorFilter(['node_modules']),\n *   createBinaryFilter({}),\n * ];\n * const result = await applyFilters(['/a/b.js', '/a/node_modules/c.js'], filters);\n * // result.included: ['/a/b.js']\n * // result.excluded: [{ path: '/a/node_modules/c.js', filter: 'vendor', reason: '...' }]\n * ```\n */\nexport async function applyFilters(\n  files: string[],\n  filters: FileFilter[],\n  options?: { tracer?: ITraceWriter; debug?: boolean }\n): Promise<FilterResult> {\n  const included: string[] = [];\n  const excluded: ExcludedFile[] = [];\n\n  // Track exclusions per filter for trace events\n  const filterStats = new Map<string, { matched: number; rejected: number }>();\n  for (const filter of filters) {\n    filterStats.set(filter.name, { matched: 0, rejected: 0 });\n  }\n\n  // Process files with bounded concurrency to avoid exhausting file descriptors.\n  // Binary filter calls isBinaryFile() which does file I/O.\n  const CONCURRENCY = 30;\n  const iterator = files.entries();\n\n  async function worker(\n    iter: IterableIterator<[number, string]>,\n  ): Promise<Array<{ index: number; file: string; excluded?: ExcludedFile }>> {\n    const results: Array<{ index: number; file: string; excluded?: ExcludedFile }> = [];\n    for (const [index, file] of iter) {\n      let wasExcluded = false;\n\n      // Run through filters in order, stop at first exclusion\n      for (const filter of filters) {\n        const shouldExclude = await filter.shouldExclude(file);\n\n        if (shouldExclude) {\n          results.push({\n            index,\n            file,\n            excluded: {\n              path: file,\n              reason: `Excluded by ${filter.name} filter`,\n              filter: filter.name,\n            },\n          });\n          wasExcluded = true;\n          break; // Short-circuit: stop checking other filters\n        }\n      }\n\n      if (!wasExcluded) {\n        results.push({ index, file });\n      }\n    }\n    return results;\n  }\n\n  const effectiveConcurrency = Math.min(CONCURRENCY, files.length);\n  const workers = Array.from({ length: effectiveConcurrency }, () =>\n    worker(iterator),\n  );\n\n  const allResults = (await Promise.all(workers)).flat();\n\n  // Sort by original index to preserve order\n  allResults.sort((a, b) => a.index - b.index);\n\n  // Collect results and update filter stats\n  for (const result of allResults) {\n    if (result.excluded) {\n      excluded.push(result.excluded);\n      const stats = filterStats.get(result.excluded.filter);\n      if (stats) {\n        stats.rejected++;\n      }\n    } else {\n      included.push(result.file);\n      // All filters \"matched\" (passed through) this file\n      for (const filter of filters) {\n        const stats = filterStats.get(filter.name);\n        if (stats) {\n          stats.matched++;\n        }\n      }\n    }\n  }\n\n  // Emit trace events for each filter\n  for (const filter of filters) {\n    const stats = filterStats.get(filter.name);\n    if (stats) {\n      options?.tracer?.emit({\n        type: 'filter:applied',\n        filterName: filter.name,\n        filesMatched: stats.matched,\n        filesRejected: stats.rejected,\n      });\n\n      // Debug output\n      if (options?.debug && stats.rejected > 0) {\n        console.error(pc.dim(`[debug] Filter [${filter.name}]: ${stats.rejected} files rejected`));\n      }\n    }\n  }\n\n  return { included, excluded };\n}\n\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**src/discovery/filters/index.ts orchestrates filter chain execution for file discovery, re-exports filter creators, and implements bounded-concurrency file filtering with per-filter exclusion tracking and trace emission.**\n\n## Exported Functions\n\n**applyFilters(files: string[], filters: FileFilter[], options?: { tracer?: ITraceWriter; debug?: boolean }): Promise<FilterResult>** runs files through filter chain with short-circuit evaluation (first exclusion stops further checks), processes files with concurrency limit of 30 to prevent file descriptor exhaustion during binary detection I/O, returns `FilterResult` containing `included: string[]` and `excluded: ExcludedFile[]` arrays sorted by original file order.\n\n## Re-Exported Filter Creators\n\nExports `createGitignoreFilter` from `./gitignore.js`, `createVendorFilter` and `DEFAULT_VENDOR_DIRS` from `./vendor.js`, `createBinaryFilter`, `BINARY_EXTENSIONS`, and `BinaryFilterOptions` type from `./binary.js`, `createCustomFilter` from `./custom.js`.\n\n## Filter Chain Execution Strategy\n\n**applyFilters** spawns `CONCURRENCY` (30) worker promises sharing single `files.entries()` iterator to distribute work. Each worker calls `filter.shouldExclude(file)` for each filter in sequence until first exclusion or all filters pass. Collects results as `Array<{ index: number; file: string; excluded?: ExcludedFile }>`, sorts by original `index` to preserve input order, then segregates into `included` and `excluded` arrays.\n\n## Per-Filter Statistics Tracking\n\nMaintains `filterStats: Map<string, { matched: number; rejected: number }>` initialized for each filter before processing. Increments `rejected` counter when filter excludes file. Increments `matched` counter for all filters when file passes through entire chain. Emits `filter:applied` trace event for each filter with `filterName`, `filesMatched`, `filesRejected` fields. Writes debug output via `console.error(pc.dim(...))` for filters with `rejected > 0` when `options.debug` enabled.\n\n## Bounded Concurrency Pattern\n\nSets `effectiveConcurrency = Math.min(CONCURRENCY, files.length)` to avoid spawning excess workers. Uses iterator-based work stealing pattern where workers pull `[index, file]` pairs from shared iterator via `for (const [index, file] of iter)` loop. Worker function signature: `async function worker(iter: IterableIterator<[number, string]>): Promise<Array<{ index: number; file: string; excluded?: ExcludedFile }>>`.\n\n## Integration with Trace System\n\nAccepts optional `options.tracer: ITraceWriter` parameter threaded from CLI via `CommandRunOptions`. Emits `filter:applied` events after all workers complete, providing per-filter exclusion metrics for telemetry analysis. Debug flag `options.debug` enables console logging of rejection counts using `picocolors` dim formatting.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 706,
      "cacheReadTokens": 0,
      "cacheCreationTokens": 27810,
      "latencyMs": 25369,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/discovery/filters/index.ts",
          "sizeBytes": 4669
        }
      ]
    },
    {
      "timestamp": "2026-02-09T17:43:35.630Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/ai/backends/claude.ts\n\n```typescript\n/**\n * Claude CLI backend adapter.\n *\n * Full implementation of the {@link AIBackend} interface for the Claude Code\n * CLI (`claude`). Builds CLI arguments, parses structured JSON responses\n * with Zod validation, detects CLI availability on PATH, and provides\n * install instructions.\n *\n * The prompt is NOT included in the args array -- it goes to stdin via\n * the subprocess wrapper ({@link runSubprocess}).\n *\n * @module\n */\n\nimport * as fs from 'node:fs/promises';\nimport * as path from 'node:path';\nimport { z } from 'zod';\nimport type { AIBackend, AICallOptions, AIResponse } from '../types.js';\nimport { AIServiceError } from '../types.js';\n\n// ---------------------------------------------------------------------------\n// Zod schema for Claude CLI JSON output\n// ---------------------------------------------------------------------------\n\n/**\n * Schema validated against Claude CLI v2.1.31 JSON output.\n *\n * When invoked with `claude -p --output-format json`, the CLI produces a\n * single JSON object on stdout matching this shape. See RESEARCH.md for\n * the live verification details.\n */\nconst ClaudeResponseSchema = z.object({\n  type: z.literal('result'),\n  subtype: z.enum(['success', 'error']),\n  is_error: z.boolean(),\n  duration_ms: z.number(),\n  duration_api_ms: z.number(),\n  num_turns: z.number(),\n  result: z.string(),\n  session_id: z.string(),\n  total_cost_usd: z.number(),\n  usage: z.object({\n    input_tokens: z.number(),\n    cache_creation_input_tokens: z.number(),\n    cache_read_input_tokens: z.number(),\n    output_tokens: z.number(),\n  }),\n  modelUsage: z.record(z.object({\n    inputTokens: z.number(),\n    outputTokens: z.number(),\n    cacheReadInputTokens: z.number(),\n    cacheCreationInputTokens: z.number(),\n    costUSD: z.number(),\n  })),\n});\n\n// ---------------------------------------------------------------------------\n// PATH detection utility\n// ---------------------------------------------------------------------------\n\n/**\n * Check whether a command is available on the system PATH.\n *\n * Splits `process.env.PATH` by the platform delimiter and checks each\n * directory for a file matching the command name. On Windows, also checks\n * each extension from `process.env.PATHEXT` (e.g., `.exe`, `.cmd`, `.bat`).\n *\n * Uses `fs.stat` (not `fs.access` with execute bit) for cross-platform\n * compatibility -- Windows does not have Unix execute permissions.\n *\n * @param command - The bare command name to look for (e.g., \"claude\")\n * @returns `true` if the command exists as a file in any PATH directory\n *\n * @example\n * ```typescript\n * if (await isCommandOnPath('claude')) {\n *   console.log('Claude CLI is available');\n * }\n * ```\n */\nexport async function isCommandOnPath(command: string): Promise<boolean> {\n  const envPath = process.env.PATH ?? '';\n  const pathDirs = envPath\n    .replace(/[\"]+/g, '')\n    .split(path.delimiter)\n    .filter(Boolean);\n\n  // On Windows, PATHEXT lists executable extensions (e.g., \".EXE;.CMD;.BAT\").\n  // On other platforms, PATHEXT is unset; check the bare command name only.\n  const envExt = process.env.PATHEXT ?? '';\n  const extensions = envExt ? envExt.split(';') : [''];\n\n  for (const dir of pathDirs) {\n    for (const ext of extensions) {\n      try {\n        const candidate = path.join(dir, command + ext);\n        const stat = await fs.stat(candidate);\n        if (stat.isFile()) {\n          return true;\n        }\n      } catch {\n        // Not found in this dir/ext combination, continue\n      }\n    }\n  }\n\n  return false;\n}\n\n// ---------------------------------------------------------------------------\n// Claude backend\n// ---------------------------------------------------------------------------\n\n/**\n * Claude Code CLI backend adapter.\n *\n * Implements the {@link AIBackend} interface for the `claude` CLI.\n * This is the primary (and currently only fully implemented) backend.\n *\n * @example\n * ```typescript\n * const backend = new ClaudeBackend();\n * if (await backend.isAvailable()) {\n *   const args = backend.buildArgs({ prompt: 'Summarize this file' });\n *   const result = await runSubprocess('claude', args, {\n *     timeoutMs: 120_000,\n *     input: 'Summarize this file',\n *   });\n *   const response = backend.parseResponse(result.stdout, result.durationMs, result.exitCode);\n * }\n * ```\n */\nexport class ClaudeBackend implements AIBackend {\n  readonly name = 'claude';\n  readonly cliCommand = 'claude';\n\n  /**\n   * Check if the `claude` CLI is available on PATH.\n   */\n  async isAvailable(): Promise<boolean> {\n    return isCommandOnPath(this.cliCommand);\n  }\n\n  /**\n   * Build CLI arguments for a Claude invocation.\n   *\n   * Returns the argument array for `claude -p --output-format json`. The\n   * prompt itself is NOT included -- it goes to stdin via the subprocess\n   * wrapper.\n   *\n   * @param options - Call options (model, systemPrompt, maxTurns)\n   * @returns Argument array suitable for {@link runSubprocess}\n   */\n  buildArgs(options: AICallOptions): string[] {\n    const args: string[] = [\n      '-p',                        // Non-interactive print mode\n      '--output-format', 'json',   // Structured JSON output\n      '--no-session-persistence',  // Don't save session to disk\n      '--permission-mode', 'bypassPermissions',  // Non-interactive: skip permission prompts (PITFALLS.md §8)\n    ];\n\n    if (options.model) {\n      args.push('--model', options.model);\n    }\n\n    if (options.systemPrompt) {\n      args.push('--system-prompt', options.systemPrompt);\n    }\n\n    if (options.maxTurns !== undefined) {\n      args.push('--max-turns', String(options.maxTurns));\n    }\n\n    return args;\n  }\n\n  /**\n   * Parse Claude CLI JSON output into a normalized {@link AIResponse}.\n   *\n   * Handles non-JSON prefix text by finding the first `{` character\n   * (defensive parsing per RESEARCH.md Pitfall 4). Validates the response\n   * against the Zod schema and extracts the model name from `modelUsage`.\n   *\n   * @param stdout - Raw stdout from the Claude CLI process\n   * @param durationMs - Wall-clock duration of the subprocess\n   * @param exitCode - Process exit code\n   * @returns Normalized AI response\n   * @throws {AIServiceError} With code `PARSE_ERROR` if JSON is missing or schema validation fails\n   */\n  parseResponse(stdout: string, durationMs: number, exitCode: number): AIResponse {\n    // Find JSON object in stdout (handle any prefix text like upgrade notices)\n    const jsonStart = stdout.indexOf('{');\n    if (jsonStart === -1) {\n      throw new AIServiceError(\n        'PARSE_ERROR',\n        `No JSON object found in Claude CLI output. Raw output (first 200 chars): ${stdout.slice(0, 200)}`,\n      );\n    }\n\n    let parsed: z.infer<typeof ClaudeResponseSchema>;\n    try {\n      parsed = ClaudeResponseSchema.parse(JSON.parse(stdout.slice(jsonStart)));\n    } catch (error) {\n      const message = error instanceof Error ? error.message : String(error);\n      throw new AIServiceError(\n        'PARSE_ERROR',\n        `Failed to parse Claude CLI JSON response: ${message}`,\n      );\n    }\n\n    // Extract model name from modelUsage keys (first key is the model used)\n    const modelName = Object.keys(parsed.modelUsage)[0] ?? 'unknown';\n\n    return {\n      text: parsed.result,\n      model: modelName,\n      inputTokens: parsed.usage.input_tokens,\n      outputTokens: parsed.usage.output_tokens,\n      cacheReadTokens: parsed.usage.cache_read_input_tokens,\n      cacheCreationTokens: parsed.usage.cache_creation_input_tokens,\n      durationMs,\n      exitCode,\n      raw: parsed,\n    };\n  }\n\n  /**\n   * Get user-facing install instructions for the Claude CLI.\n   */\n  getInstallInstructions(): string {\n    return [\n      'Claude Code (recommended):',\n      '  npm install -g @anthropic-ai/claude-code',\n      '  https://code.claude.com',\n    ].join('\\n');\n  }\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**ClaudeBackend implements AIBackend interface for Claude Code CLI, providing PATH detection via isCommandOnPath(), argument construction with permission bypass flags, JSON response parsing with Zod validation against ClaudeResponseSchema, and installation instructions.**\n\n## Exported Interface\n\n`ClaudeBackend` class implements `AIBackend` interface with:\n- `name: 'claude'` — Backend identifier string\n- `cliCommand: 'claude'` — Executable name for PATH lookup\n- `isAvailable(): Promise<boolean>` — Delegates to `isCommandOnPath('claude')`\n- `buildArgs(options: AICallOptions): string[]` — Constructs CLI arguments array\n- `parseResponse(stdout: string, durationMs: number, exitCode: number): AIResponse` — Parses JSON output into normalized response\n- `getInstallInstructions(): string` — Returns npm install command for `@anthropic-ai/claude-code`\n\n`isCommandOnPath(command: string): Promise<boolean>` exported function splits `process.env.PATH` by platform delimiter, iterates through directories, checks file existence via `fs.stat()` (not `fs.access()` for cross-platform compatibility), handles Windows `PATHEXT` environment variable for extension variants (`.exe`, `.cmd`, `.bat`).\n\n## CLI Arguments Construction\n\n`buildArgs()` returns fixed arguments array:\n- `-p` — Non-interactive print mode\n- `--output-format json` — Structured JSON output\n- `--no-session-persistence` — Prevent session disk writes\n- `--permission-mode bypassPermissions` — Skip interactive permission prompts (PITFALLS.md §8)\n\nConditional arguments from `AICallOptions`:\n- `--model <model>` if `options.model` provided\n- `--system-prompt <systemPrompt>` if `options.systemPrompt` provided\n- `--max-turns <maxTurns>` if `options.maxTurns !== undefined`\n\nPrompt text NOT included in args array — delivered to stdin via `runSubprocess()` wrapper.\n\n## Response Schema Validation\n\n`ClaudeResponseSchema` validated against Claude CLI v2.1.31 JSON output (see RESEARCH.md):\n```typescript\n{\n  type: 'result',\n  subtype: 'success' | 'error',\n  is_error: boolean,\n  duration_ms: number,\n  duration_api_ms: number,\n  num_turns: number,\n  result: string,\n  session_id: string,\n  total_cost_usd: number,\n  usage: {\n    input_tokens: number,\n    cache_creation_input_tokens: number,\n    cache_read_input_tokens: number,\n    output_tokens: number,\n  },\n  modelUsage: Record<string, {\n    inputTokens: number,\n    outputTokens: number,\n    cacheReadInputTokens: number,\n    cacheCreationInputTokens: number,\n    costUSD: number,\n  }>\n}\n```\n\n`parseResponse()` defensively finds first `{` character via `stdout.indexOf('{')` to handle non-JSON prefix text (upgrade notices per RESEARCH.md Pitfall 4). Extracts model name from first key in `modelUsage` object (defaults to `'unknown'`). Throws `AIServiceError` with code `PARSE_ERROR` if JSON missing or Zod validation fails.\n\n## PATH Detection Algorithm\n\n`isCommandOnPath()` algorithm:\n1. Parse `process.env.PATH` with `path.delimiter` (`;` on Windows, `:` on Unix)\n2. Strip quote characters via `.replace(/[\"]+/g, '')`\n3. Parse `process.env.PATHEXT` on Windows for executable extensions (empty array on Unix defaults to `['']`)\n4. Nested loop: for each directory × extension, construct candidate path via `path.join(dir, command + ext)`\n5. Check `(await fs.stat(candidate)).isFile()` — returns `true` on first match, `false` if all candidates fail\n\nUses `fs.stat()` instead of `fs.access(fs.constants.X_OK)` because Windows lacks Unix execute permission bits.\n\n## Dependencies\n\n- `node:fs/promises` — `fs.stat()` for PATH detection\n- `node:path` — `path.join()`, `path.delimiter` for cross-platform path manipulation\n- `zod` — `ClaudeResponseSchema` validation, `z.infer<>` type extraction\n- `../types.js` — `AIBackend`, `AICallOptions`, `AIResponse`, `AIServiceError` imports\n\n## Integration Points\n\n`ClaudeBackend` consumed by `AIService` in `src/ai/service.ts` via backend registry. `runSubprocess()` in `src/ai/subprocess.ts` receives args from `buildArgs()` and stdout for `parseResponse()`. `isCommandOnPath()` utility reusable for other backend availability checks.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 1174,
      "cacheReadTokens": 0,
      "cacheCreationTokens": 28711,
      "latencyMs": 31228,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/ai/backends/claude.ts",
          "sizeBytes": 7804
        }
      ]
    },
    {
      "timestamp": "2026-02-09T17:43:35.646Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/discovery/filters/binary.ts\n\n```typescript\n/**\n * Binary file filter for file discovery.\n *\n * Uses extension-based detection as a fast path, falling back to content\n * analysis via `isbinaryfile` for unknown extensions. Also handles large\n * files by size threshold.\n */\n\nimport { isBinaryFile } from 'isbinaryfile';\nimport fs from 'node:fs/promises';\nimport path from 'node:path';\nimport type { FileFilter } from '../types.js';\n\n/**\n * Set of file extensions known to be binary.\n * These are excluded without content analysis for performance.\n */\nexport const BINARY_EXTENSIONS = new Set([\n  // Images\n  '.png',\n  '.jpg',\n  '.jpeg',\n  '.gif',\n  '.bmp',\n  '.ico',\n  '.webp',\n  '.svg',\n  '.tiff',\n  '.tif',\n  '.psd',\n  '.raw',\n  '.heif',\n  '.heic',\n  // Archives\n  '.zip',\n  '.tar',\n  '.gz',\n  '.rar',\n  '.7z',\n  '.bz2',\n  '.xz',\n  '.tgz',\n  // Executables\n  '.exe',\n  '.dll',\n  '.so',\n  '.dylib',\n  '.bin',\n  '.msi',\n  '.app',\n  '.dmg',\n  // Media\n  '.mp3',\n  '.mp4',\n  '.wav',\n  '.avi',\n  '.mov',\n  '.mkv',\n  '.flac',\n  '.ogg',\n  '.webm',\n  '.m4a',\n  '.aac',\n  '.wma',\n  '.wmv',\n  '.flv',\n  // Documents (binary formats)\n  '.pdf',\n  '.doc',\n  '.docx',\n  '.xls',\n  '.xlsx',\n  '.ppt',\n  '.pptx',\n  '.odt',\n  '.ods',\n  '.odp',\n  // Fonts\n  '.woff',\n  '.woff2',\n  '.ttf',\n  '.eot',\n  '.otf',\n  // Compiled/bytecode\n  '.class',\n  '.pyc',\n  '.pyo',\n  '.o',\n  '.obj',\n  '.a',\n  '.lib',\n  '.wasm',\n  // Database\n  '.db',\n  '.sqlite',\n  '.sqlite3',\n  '.mdb',\n  // Other\n  '.ico',\n  '.icns',\n  '.cur',\n  '.deb',\n  '.rpm',\n  '.jar',\n  '.war',\n  '.ear',\n]);\n\n/**\n * Options for the binary filter.\n */\nexport interface BinaryFilterOptions {\n  /**\n   * Maximum file size in bytes. Files larger than this are excluded.\n   * Default: 1MB (1048576 bytes)\n   */\n  maxFileSize?: number;\n\n  /**\n   * Additional binary extensions to recognize beyond the defaults.\n   */\n  additionalExtensions?: string[];\n}\n\n/** Default maximum file size: 1MB */\nconst DEFAULT_MAX_FILE_SIZE = 1024 * 1024;\n\n/**\n * Creates a binary filter that excludes binary files and files exceeding size limit.\n *\n * The filter uses a two-phase detection approach:\n * 1. Fast path: Check extension against known binary extensions\n * 2. Slow path: For unknown extensions, analyze file content with isbinaryfile\n *\n * @param options - Filter configuration options\n * @returns A FileFilter that identifies binary files\n *\n * @example\n * ```typescript\n * const filter = createBinaryFilter({ maxFileSize: 500000 });\n * await filter.shouldExclude('/path/to/image.png'); // true (extension)\n * await filter.shouldExclude('/path/to/unknown.xyz'); // checks content\n * ```\n */\nexport function createBinaryFilter(options: BinaryFilterOptions = {}): FileFilter {\n  const { maxFileSize = DEFAULT_MAX_FILE_SIZE, additionalExtensions = [] } = options;\n\n  // Create a combined set of all binary extensions\n  const binaryExtensions = new Set(BINARY_EXTENSIONS);\n  for (const ext of additionalExtensions) {\n    // Ensure extension has leading dot\n    binaryExtensions.add(ext.startsWith('.') ? ext : `.${ext}`);\n  }\n\n  return {\n    name: 'binary',\n\n    async shouldExclude(absolutePath: string): Promise<boolean> {\n      // Fast path: check extension\n      const ext = path.extname(absolutePath).toLowerCase();\n      if (binaryExtensions.has(ext)) {\n        return true;\n      }\n\n      // Check file size and content\n      try {\n        const stats = await fs.stat(absolutePath);\n\n        // Exclude files exceeding size limit\n        if (stats.size > maxFileSize) {\n          return true;\n        }\n\n        // Slow path: content analysis for unknown extensions\n        return await isBinaryFile(absolutePath);\n      } catch {\n        // If we can't read it, skip it\n        return true;\n      }\n    },\n  };\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**binary.ts implements two-phase binary file detection for file discovery: extension-based fast path against `BINARY_EXTENSIONS` set followed by content analysis via `isBinaryFile()` for unknown extensions, with configurable file size thresholds.**\n\n## Exported Constants\n\n`BINARY_EXTENSIONS` is a `Set<string>` containing 80+ file extensions known to be binary, organized by category:\n- Images: `.png`, `.jpg`, `.jpeg`, `.gif`, `.bmp`, `.ico`, `.webp`, `.svg`, `.tiff`, `.tif`, `.psd`, `.raw`, `.heif`, `.heic`\n- Archives: `.zip`, `.tar`, `.gz`, `.rar`, `.7z`, `.bz2`, `.xz`, `.tgz`\n- Executables: `.exe`, `.dll`, `.so`, `.dylib`, `.bin`, `.msi`, `.app`, `.dmg`\n- Media: `.mp3`, `.mp4`, `.wav`, `.avi`, `.mov`, `.mkv`, `.flac`, `.ogg`, `.webm`, `.m4a`, `.aac`, `.wma`, `.wmv`, `.flv`\n- Documents: `.pdf`, `.doc`, `.docx`, `.xls`, `.xlsx`, `.ppt`, `.pptx`, `.odt`, `.ods`, `.odp`\n- Fonts: `.woff`, `.woff2`, `.ttf`, `.eot`, `.otf`\n- Compiled/bytecode: `.class`, `.pyc`, `.pyo`, `.o`, `.obj`, `.a`, `.lib`, `.wasm`\n- Database: `.db`, `.sqlite`, `.sqlite3`, `.mdb`\n- Other: `.ico`, `.icns`, `.cur`, `.deb`, `.rpm`, `.jar`, `.war`, `.ear`\n\n`DEFAULT_MAX_FILE_SIZE` constant is `1048576` (1MB in bytes).\n\n## Exported Interfaces\n\n`BinaryFilterOptions` configures `createBinaryFilter()` with optional fields:\n- `maxFileSize?: number` — Maximum file size in bytes (default: `DEFAULT_MAX_FILE_SIZE` = 1MB)\n- `additionalExtensions?: string[]` — Extra binary extensions beyond `BINARY_EXTENSIONS` defaults\n\n## Exported Functions\n\n`createBinaryFilter(options?: BinaryFilterOptions): FileFilter` returns a `FileFilter` implementation with `name: 'binary'` and `shouldExclude()` method. Detection algorithm:\n\n1. **Fast path:** Extract extension via `path.extname(absolutePath).toLowerCase()`, return `true` if present in merged `binaryExtensions` set (combines `BINARY_EXTENSIONS` + `additionalExtensions` with leading-dot normalization)\n2. **Size check:** Call `fs.stat(absolutePath)` and return `true` if `stats.size > maxFileSize`\n3. **Slow path:** For unknown extensions below size threshold, delegate to `isBinaryFile(absolutePath)` for content analysis\n4. **Error handling:** Return `true` for any `fs.stat()` or `isBinaryFile()` errors (skip unreadable files)\n\nThe `additionalExtensions` array undergoes normalization: non-dot-prefixed strings get `.` prepended via ternary `ext.startsWith('.') ? ext : \\`.${ext}\\``.\n\n## Dependencies\n\n- `isbinaryfile` — Content-based binary detection for unknown extensions\n- `node:fs/promises` — Async `fs.stat()` for file size retrieval\n- `node:path` — `path.extname()` for extension extraction\n- `../types.js` — `FileFilter` interface defining `name: string` and `shouldExclude(absolutePath: string): Promise<boolean>`\n\n## Integration Points\n\n`createBinaryFilter()` composes with other `FileFilter` implementations from `src/discovery/filters/` (gitignore, custom, vendor) via filter chains in discovery walker. The `maxFileSize` parameter enforces hard exclusion threshold distinct from config's `options.maxFileSize` used elsewhere for binary detection triggering.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 959,
      "cacheReadTokens": 0,
      "cacheCreationTokens": 27731,
      "latencyMs": 31291,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/discovery/filters/binary.ts",
          "sizeBytes": 3701
        }
      ]
    },
    {
      "timestamp": "2026-02-09T17:43:35.709Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/generation/prompts/templates.ts\n\n```typescript\n/**\n * Prompt constants for file and directory analysis.\n */\n\nexport const FILE_SYSTEM_PROMPT = `You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - \\`FILE_SYSTEM_PROMPT\\` — system prompt for file analysis (250 lines)\n  - \\`DIRECTORY_SYSTEM_PROMPT\\` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation`;\n\nexport const FILE_USER_PROMPT = `Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: {{FILE_PATH}}\n\n\\`\\`\\`typescript\n{{CONTENT}}\n\\`\\`\\`\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.`;\n\n/**\n * System prompt for directory-level AGENTS.md generation.\n * Used by buildDirectoryPrompt() in builder.ts.\n */\nexport const DIRECTORY_SYSTEM_PROMPT = `You are generating an AGENTS.md file — a directory-level overview for AI coding assistants.\n\nCRITICAL: Output ONLY the raw markdown content. No code fences, no preamble, no explanations, no conversational text. Your entire response IS the AGENTS.md file content.\n\nOUTPUT FORMAT:\n- First line MUST be exactly: <!-- Generated by agents-reverse-engineer -->\n- Use a # heading with the directory name\n- Write a one-paragraph purpose statement for the directory\nusage\nADAPTIVE SECTIONS:\nAnalyze the directory contents and choose the most relevant sections. Do NOT use a fixed template. Instead, select sections that best document this specific directory for an AI that needs to reconstruct or extend the project.\n\nConsider these section types (choose what applies):\n- **Contents**: Group files by purpose/category under ## headings. For each file: markdown link [filename](./filename) and a one-line description.\n- **Subdirectories**: If subdirectories exist, list them with links [dirname/](./dirname/) and brief summaries.\n- **Architecture / Data Flow**: If files form a pipeline, request/response chain, or layered architecture, document it.\n- **Stack**: If this is a package root (has package.json, Cargo.toml, go.mod, etc.), document the technology stack, key scripts, and entry points.\n- **Structure**: If the directory layout follows a convention (feature-sliced, domain-driven, MVC, etc.), document it.\n- **Patterns**: If files share recurring design patterns (factory, strategy, middleware, barrel re-export), name and document them.\n- **Configuration**: If the directory contains config files, schemas, or environment definitions, document the config surface area.\n- **API Surface**: If the directory exports a public API (barrel index, route definitions, SDK), document the interface contract.\n- **File Relationships**: How files collaborate, depend on each other, or share state.\n- **Behavioral Contracts**: If files contain regex patterns, format specifications, magic constants, or template strings that define observable behavior, collect them in a dedicated section. Preserve verbatim patterns from file summaries — do NOT paraphrase regex into prose. This section is MANDATORY when file summaries contain behavioral artifacts.\n- **Reproduction-Critical Constants**: If file summaries reference annex files (via ## Annex References sections), list them with links. Example: \"Full prompt template text: [templates.ts.annex.md](./prompts/templates.ts.annex.md)\". Do NOT reproduce annex content in AGENTS.md — just link to it.\n\nChoose any relevant sections or create your own based on the directory contents. The goal is to provide a comprehensive overview that captures the essence of the directory's role in the project and how its files work together, with a focus on what an AI coding assistant would need to know to effectively interact with this code.\n\nSCOPE:\n- AGENTS.md is a NAVIGATIONAL INDEX — help an AI find the right file quickly\n- Focus on: what each file does, how files relate, directory-level patterns\n- Do NOT reproduce full architecture sections — those belong in the root CLAUDE.md\n\nPATH ACCURACY (MANDATORY):\n- When referencing files or modules outside this directory, use ONLY paths from the \"Import Map\" section\n- Do NOT invent, rename, or guess module paths — if a path isn't in the Import Map, don't reference it\n- Use the exact directory names from \"Project Directory Structure\" — do NOT rename directories\n  (e.g., if the directory is called \"cli\", write \"src/cli/\", NOT \"src/commands/\")\n- Cross-module references must use the specifier format from actual import statements\n  (e.g., \"../generation/writers/sum.js\", NOT \"../fs/sum-file.js\")\n- If you are unsure about a path, omit the cross-reference rather than guessing\n\nCONSISTENCY (MANDATORY):\n- Do not contradict yourself within the same document\n- If you describe a technique (e.g., \"regex-based\"), do not call it something else later (e.g., \"AST-based\")\n- When stating version numbers, engines, or config fields, use ONLY values present in the file summaries\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this directory\", \"this module\", \"provides\", \"responsible for\", \"is used to\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Per-file descriptions in Contents sections: 1-2 sentences maximum. Reference key symbols but do not reproduce full summaries.\n- Behavioral contracts (regex patterns, format specs, constants) belong in a separate Behavioral Contracts section, not in per-file descriptions.\n- Annex file references: link to .annex.md files, do not inline their content. One line per annex reference.\n- Subdirectory descriptions: 1-2 sentences maximum. Capture the directory's role, not its full contents.\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- Key exported symbols from file summaries MUST appear in the directory overview\n- Preserve exact casing of identifiers\n\nUSER NOTES:\n- If \"User Notes\" are provided in the prompt, they contain user-defined instructions that will be automatically prepended to your output\n- Do NOT repeat or paraphrase user notes in your generated content — they are included separately\n- You may reference information from user notes for context`;\n\n/**\n * System prompt for incremental file summary updates.\n * Used by buildFilePrompt() when existingSum is provided.\n */\nexport const FILE_UPDATE_SYSTEM_PROMPT = `You are updating an existing file summary for an AI coding assistant. The source code has changed and the summary needs to reflect those changes.\n\nCRITICAL — INCREMENTAL UPDATE RULES:\n- You are given the EXISTING summary and the UPDATED source code\n- Preserve the structure, section headings, and phrasing of the existing summary wherever the underlying code is unchanged\n- Only modify content that is directly affected by the code changes\n- If a section describes code that has not changed, keep it VERBATIM — do not rephrase, reorganize, or \"improve\" stable text\n- Add new sections only if the code changes introduce entirely new concepts\n- Remove sections only if the code they described has been deleted\n- Update signatures, type names, and identifiers to match the current source exactly\n\nBEHAVIORAL CONTRACT PRESERVATION (MANDATORY):\n- Regex patterns, format strings, magic constants, and template content from the existing summary MUST be preserved verbatim unless the source code changed them\n- If source code changes a regex pattern or constant, update the summary to show the NEW value verbatim\n- Never summarize or paraphrase regex patterns — always show the exact pattern in backticks\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers\n- Missing any exported identifier is a failure\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation`;\n\n/**\n * System prompt for incremental directory AGENTS.md updates.\n * Used by buildDirectoryPrompt() when existingAgentsMd is provided.\n */\nexport const DIRECTORY_UPDATE_SYSTEM_PROMPT = `You are updating an existing AGENTS.md file — a directory-level overview for AI coding assistants. Some file summaries or subdirectory documents have changed, and AGENTS.md needs to reflect those changes.\n\nCRITICAL — INCREMENTAL UPDATE RULES:\n- You are given the EXISTING AGENTS.md and the CURRENT file summaries and subdirectory documents\n- Preserve the structure, section headings, and descriptions that are still accurate\n- Only modify entries for files or subdirectories whose summaries have changed\n- Add entries for new files, remove entries for deleted files\n- Do NOT reorganize, rephrase, or restructure sections that are unaffected by changes\n- Keep the same section ordering unless files were added/removed in a way that requires regrouping\n- Behavioral Contracts section: preserve verbatim regex patterns and constants unless source file summaries show they changed\n- Reproduction-Critical Constants: if file summaries reference annex files, preserve the links. Add links for new annexes, remove links for deleted ones.\n\nCRITICAL: Output ONLY the raw markdown content. No code fences, no preamble, no explanations, no conversational text. Your entire response IS the AGENTS.md file content.\n\nOUTPUT FORMAT:\n- First line MUST be exactly: <!-- Generated by agents-reverse-engineer -->\n- Use a # heading with the directory name\n- Preserve the existing purpose statement unless the directory's role has fundamentally changed\n\nSCOPE:\n- AGENTS.md is a NAVIGATIONAL INDEX — help an AI find the right file quickly\n- Focus on: what each file does, how files relate, directory-level patterns\n- Do NOT reproduce full architecture sections — those belong in the root CLAUDE.md\n\nPATH ACCURACY (MANDATORY):\n- When referencing files or modules outside this directory, use ONLY paths from the \"Import Map\" section\n- Do NOT invent, rename, or guess module paths — if a path isn't in the Import Map, don't reference it\n- Use the exact directory names from \"Project Directory Structure\" — do NOT rename directories\n- Cross-module references must use the specifier format from actual import statements\n- If you are unsure about a path, omit the cross-reference rather than guessing\n\nCONSISTENCY (MANDATORY):\n- Do not contradict yourself within the same document\n- If you describe a technique (e.g., \"regex-based\"), do not call it something else later (e.g., \"AST-based\")\n- When stating version numbers, engines, or config fields, use ONLY values present in the file summaries\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this directory\", \"this module\", \"provides\", \"responsible for\", \"is used to\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Per-file descriptions: 1-2 sentences maximum. Reference key symbols but do not reproduce full summaries.\n- Subdirectory descriptions: 1-2 sentences maximum. Capture the directory's role, not its full contents.\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- Key exported symbols from file summaries MUST appear in the directory overview\n- Preserve exact casing of identifiers\n\nUSER NOTES:\n- If \"User Notes\" are provided in the prompt, they contain user-defined instructions that will be automatically prepended to your output\n- Do NOT repeat or paraphrase user notes in your generated content — they are included separately\n- You may reference information from user notes for context`;\n\n/**\n * System prompt for root CLAUDE.md generation.\n * Used by buildRootPrompt() in builder.ts.\n */\nexport const ROOT_SYSTEM_PROMPT = `You generate markdown documentation files. Output ONLY the raw markdown content.\nDo NOT include any conversational text, preamble, or meta-commentary.\nDo NOT say \"Here is...\" or \"I've generated...\" — just output the document itself.\nThe output will be written directly to a file.\n\nCRITICAL CONSTRAINT:\n- Synthesize ONLY from the AGENTS.md content provided in the user prompt\n- Do NOT invent, extrapolate, or hallucinate features, hooks, APIs, patterns, or dependencies not explicitly mentioned\n- If information is missing, omit that section rather than guessing\n- Every claim must be traceable to a specific AGENTS.md file provided`;\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**templates.ts exports six system prompt constants controlling AI-driven documentation generation across three pipeline phases: file analysis, directory aggregation, and root synthesis.**\n\n## Exported Constants\n\n### FILE_SYSTEM_PROMPT\nSystem prompt for Phase 1 file analysis controlling `.sum` generation. Enforces density rules (every sentence must reference specific identifiers), anchor term preservation (exact export name casing), and behavioral contract extraction (verbatim regex patterns, format strings, magic constants, environment variables). Mandates output format starting with bold purpose statement without preamble. Defines REPRODUCTION-CRITICAL CONTENT strategy: files with large string constants should list them in `## Annex References` section rather than inlining content (e.g., `` `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)``), delegating extraction to pipeline automation.\n\n### FILE_USER_PROMPT\nUser prompt template for file analysis with two placeholders: `{{FILE_PATH}}` and `{{CONTENT}}`. Includes embedded `<project-structure>` listing all project files (hardcoded tree from root to `src/update/types.ts`). Instructs AI to lead with bold purpose statement and use `##` headings for organization.\n\n### FILE_UPDATE_SYSTEM_PROMPT\nSystem prompt for incremental file summary updates when `existingSum` is provided to `buildFilePrompt()`. Enforces preservation rules: keep unchanged sections verbatim, modify only content affected by code changes, preserve behavioral contracts (regex/constants) unless source changed them. Prohibits reorganizing or rephrasing stable text. Maintains same density rules and anchor term preservation as `FILE_SYSTEM_PROMPT`.\n\n### DIRECTORY_SYSTEM_PROMPT\nSystem prompt for Phase 2 `AGENTS.md` generation controlling directory-level aggregation. Mandates first line exactly `<!-- Generated by agents-reverse-engineer -->` followed by `#` heading with directory name. Defines adaptive section strategy: choose from Contents, Subdirectories, Architecture, Stack, Structure, Patterns, Configuration, API Surface, File Relationships, Behavioral Contracts, Reproduction-Critical Constants based on directory contents. Enforces PATH ACCURACY rules: use only paths from \"Import Map\" section, exact directory names from \"Project Directory Structure\", no path invention. Prohibits contradictions (e.g., calling technique \"regex-based\" then \"AST-based\"). USER NOTES section describes automatic prepending behavior: user-defined instructions are included separately, do not repeat them.\n\n### DIRECTORY_UPDATE_SYSTEM_PROMPT\nSystem prompt for incremental `AGENTS.md` updates when `existingAgentsMd` is provided to `buildDirectoryPrompt()`. Preserves structure/headings/descriptions for unchanged files, modifies only entries whose summaries changed, adds/removes entries for new/deleted files. Maintains same output format (first line `<!-- Generated by agents-reverse-engineer -->`), PATH ACCURACY rules, CONSISTENCY rules, and density rules as `DIRECTORY_SYSTEM_PROMPT`.\n\n### ROOT_SYSTEM_PROMPT\nSystem prompt for Phase 3 root document synthesis (`CLAUDE.md`, `GEMINI.md`, `OPENCODE.md`). Enforces output-only mode: no conversational text, preamble, or meta-commentary. CRITICAL CONSTRAINT: synthesize only from provided `AGENTS.md` content, do not invent features/hooks/APIs/patterns, omit missing sections rather than guessing, every claim must be traceable to specific `AGENTS.md` file.\n\n## Behavioral Contracts\n\nAll prompts prohibit filler phrases: `\"this file\"`, `\"this module\"`, `\"provides\"`, `\"responsible for\"`, `\"is used to\"`, `\"basically\"`, `\"essentially\"`, `\"provides functionality for\"`. Enforce pattern `\"[ExportName] does X\"` not `\"The ExportName function is responsible for doing X\"`.\n\nDensity rule sentinel: \"Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\".\n\nAnchor term preservation rule: \"All exported function/class/type/const names MUST appear in the summary exactly as written in source\".\n\nOutput format enforcement: \"Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\"\n\n## Integration Points\n\nUsed by `buildFilePrompt()` and `buildDirectoryPrompt()` in `src/generation/prompts/builder.ts` to construct AI prompts for Phase 1 (file analysis) and Phase 2 (directory aggregation). `ROOT_SYSTEM_PROMPT` consumed by `buildRootPrompt()` for Phase 3 synthesis. Update variants (`FILE_UPDATE_SYSTEM_PROMPT`, `DIRECTORY_UPDATE_SYSTEM_PROMPT`) selected conditionally when existing content is provided for incremental regeneration workflows.\n\n## Annex References\n\n- `FILE_SYSTEM_PROMPT` — system prompt for file analysis (129 lines)\n- `FILE_USER_PROMPT` — user prompt template with project structure (104 lines)\n- `FILE_UPDATE_SYSTEM_PROMPT` — incremental file update prompt (47 lines)\n- `DIRECTORY_SYSTEM_PROMPT` — directory aggregation prompt (93 lines)\n- `DIRECTORY_UPDATE_SYSTEM_PROMPT` — incremental directory update prompt (74 lines)\n- `ROOT_SYSTEM_PROMPT` — root synthesis constraint prompt (10 lines)",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 1250,
      "cacheReadTokens": 0,
      "cacheCreationTokens": 31343,
      "latencyMs": 33855,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/generation/prompts/templates.ts",
          "sizeBytes": 17806
        }
      ]
    },
    {
      "timestamp": "2026-02-09T17:43:55.790Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/generation/writers/index.ts\n\n```typescript\nexport {\n  writeSumFile,\n  readSumFile,\n  getSumPath,\n  sumFileExists,\n  type SumFileContent,\n} from './sum.js';\n\nexport {\n  writeAgentsMd,\n} from './agents-md.js';\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**Barrel module re-exporting file and directory documentation writers for `.sum` files and `AGENTS.md` generation.**\n\n## Exported Symbols\n\n### From `./sum.js`\n\n- **`writeSumFile`** — writes `.sum` file with YAML frontmatter containing `generated_at`, `content_hash`, `purpose`, `critical_todos`, `related_files`, followed by markdown summary content\n- **`readSumFile`** — parses `.sum` file YAML frontmatter and markdown body, returns `SumFileContent` structure with metadata and summary text\n- **`getSumPath`** — computes `.sum` file path for given source file by appending `.sum` extension\n- **`sumFileExists`** — checks filesystem for existence of `.sum` file at computed path via `getSumPath()`\n- **`SumFileContent`** (type) — interface defining structure returned by `readSumFile()` with fields: `generatedAt: string`, `contentHash: string`, `purpose: string`, `criticalTodos?: string[]`, `relatedFiles?: string[]`, `summary: string`\n\n### From `./agents-md.js`\n\n- **`writeAgentsMd`** — writes directory-level `AGENTS.md` aggregation file during Phase 2 post-order traversal, preserves user-authored `AGENTS.local.md` content if present, inserts `<!-- Generated by agents-reverse-engineer -->` marker\n\n## Integration Role\n\nActs as single import point for Phase 1 (file analysis → `.sum` via `writeSumFile`) and Phase 2 (directory aggregation → `AGENTS.md` via `writeAgentsMd`) output generation. Consumed by `src/generation/orchestrator.ts` for three-phase pipeline execution and `src/update/orchestrator.ts` for incremental update workflow.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 431,
      "cacheReadTokens": 17367,
      "cacheCreationTokens": 9161,
      "latencyMs": 16826,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/generation/writers/index.ts",
          "sizeBytes": 165
        }
      ]
    },
    {
      "timestamp": "2026-02-09T17:43:56.512Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/quality/density/validator.ts\n\n```typescript\n/**\n * Heuristic findability validation for AGENTS.md content.\n *\n * validateFindability checks that key exported symbols from .sum files\n * appear in the parent AGENTS.md content.\n * No LLM calls -- purely string-based symbol matching.\n *\n * NOTE: Previously relied on metadata.publicInterface which has been removed.\n * This module is retained for future structured extraction support.\n */\n\nimport type { SumFileContent } from '../../generation/writers/sum.js';\n\n/**\n * Result of findability validation for a single .sum file.\n *\n * score ranges from 0 (no symbols found) to 1 (all symbols found).\n */\nexport interface FindabilityResult {\n  /** Path to the .sum file that was checked */\n  filePath: string;\n  /** Symbol names that were tested for presence */\n  symbolsTested: string[];\n  /** Symbol names found in AGENTS.md content */\n  symbolsFound: string[];\n  /** Symbol names missing from AGENTS.md content */\n  symbolsMissing: string[];\n  /** Ratio of found to tested (0-1) */\n  score: number;\n}\n\n/**\n * Check that key symbols from .sum files appear in AGENTS.md content.\n *\n * Currently returns an empty array since structured metadata extraction\n * (publicInterface) has been removed. The function signature is preserved\n * for future re-implementation via post-processing passes.\n *\n * @param _agentsMdContent - Full text content of the AGENTS.md file\n * @param _sumFiles - Map of file path to parsed SumFileContent\n * @returns Empty array (no structured symbols to validate)\n */\nexport function validateFindability(\n  _agentsMdContent: string,\n  _sumFiles: Map<string, SumFileContent>,\n): FindabilityResult[] {\n  return [];\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**validator.ts implements a disabled findability validation system that previously checked whether exported symbols from .sum files appeared in parent AGENTS.md content, now returning empty results after structured metadata removal.**\n\n## Exported Interface\n\n`FindabilityResult` represents validation output for a single .sum file with fields:\n- `filePath: string` — path to the validated .sum file\n- `symbolsTested: string[]` — symbol names checked for presence\n- `symbolsFound: string[]` — symbols located in AGENTS.md content\n- `symbolsMissing: string[]` — symbols absent from AGENTS.md\n- `score: number` — ratio of found to tested symbols (0-1 range)\n\n## Exported Function\n\n`validateFindability(_agentsMdContent: string, _sumFiles: Map<string, SumFileContent>): FindabilityResult[]` accepts AGENTS.md full text and a map of file paths to parsed `SumFileContent` objects, returns an empty array since the `publicInterface` field was removed from the `SumFileContent` schema. Function signature preserved for future re-implementation via post-processing passes that extract structured symbols.\n\n## Implementation Status\n\nModule retained as stub after `metadata.publicInterface` removal from `SumFileContent` schema (referenced in comment). Originally performed heuristic string-based symbol matching without LLM calls to verify that key exports from source files appeared in aggregated directory documentation. The `validateFindability` function signature remains to support future structured extraction support, but currently no validation logic executes.\n\n## Dependencies\n\nImports `SumFileContent` type from `../../generation/writers/sum.js` for the `_sumFiles` parameter type annotation.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 381,
      "cacheReadTokens": 17367,
      "cacheCreationTokens": 9552,
      "latencyMs": 16480,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/quality/density/validator.ts",
          "sizeBytes": 1637
        }
      ]
    },
    {
      "timestamp": "2026-02-09T17:43:58.341Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/quality/phantom-paths/index.ts\n\n```typescript\nexport { checkPhantomPaths } from './validator.js';\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**Barrel module re-exporting `checkPhantomPaths` validator function for phantom path resolution in generated AGENTS.md documentation.**\n\n## Exported Interface\n\n- `checkPhantomPaths` — Validates path references in AGENTS.md files by resolving them against the filesystem and reporting unresolved paths as `PhantomPathInconsistency` issues.\n\n## Module Role\n\nSingle export statement serving as public API surface for the `src/quality/phantom-paths/` subsystem. Implementation resides in `./validator.js` which extracts path-like strings from AGENTS.md via three regex patterns (markdown link targets, backtick-quoted paths, prose-embedded paths), resolves against AGENTS.md directory and project root with `.ts`/`.js` fallback, and reports unresolved references in `InconsistencyReport` format.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 195,
      "cacheReadTokens": 17367,
      "cacheCreationTokens": 9113,
      "latencyMs": 15710,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/quality/phantom-paths/index.ts",
          "sizeBytes": 52
        }
      ]
    },
    {
      "timestamp": "2026-02-09T17:43:59.019Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/.github/workflows/publish.yml\n\n```yaml\nname: Publish to npm\n\non:\n  release:\n    types: [published]\n  workflow_dispatch:\n\njobs:\n  publish:\n    runs-on: ubuntu-latest\n    permissions:\n      contents: read\n      id-token: write\n    steps:\n      - name: Checkout\n        uses: actions/checkout@v4\n\n      - name: Setup Node.js\n        uses: actions/setup-node@v4\n        with:\n          node-version: '20'\n          registry-url: 'https://registry.npmjs.org'\n\n      - name: Install dependencies\n        run: npm ci\n\n      - name: Build\n        run: npm run build\n\n      - name: Publish to npm\n        run: npm publish --provenance --access public\n        env:\n          NODE_AUTH_TOKEN: ${{ secrets.NPM_TOKEN }}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**GitHub Actions workflow automating npm package publication with Sigstore provenance attestation on release events or manual dispatch.**\n\n## Workflow Triggers\n\nActivates on two GitHub event types:\n- `release` with `types: [published]` when GitHub release is published\n- `workflow_dispatch` for manual execution via GitHub UI\n\n## Job Configuration\n\n`publish` job runs on `ubuntu-latest` with two permissions:\n- `contents: read` for repository checkout\n- `id-token: write` enabling OIDC token generation for Sigstore provenance signing\n\n## Build Steps\n\nExecutes five sequential steps:\n\n1. **Checkout** via `actions/checkout@v4` clones repository at triggered commit SHA\n2. **Setup Node.js** via `actions/setup-node@v4` with `node-version: '20'` and `registry-url: 'https://registry.npmjs.org'` configuring npm authentication context\n3. **Install dependencies** via `npm ci` performing clean install from `package-lock.json`\n4. **Build** via `npm run build` invoking `prepublishOnly` script executing `tsc` TypeScript compilation and `build:hooks` hook file copying\n5. **Publish to npm** via `npm publish --provenance --access public` with `NODE_AUTH_TOKEN` from GitHub secret `NPM_TOKEN`\n\n## Provenance Attestation\n\n`--provenance` flag generates cryptographic attestation linking published npm package to source commit SHA via Sigstore transparency log, requires `id-token: write` permission for OIDC token exchange.\n\n## Access Control\n\n`--access public` flag ensures scoped package `agents-reverse-engineer` publishes with public visibility on npm registry.\n\n## Environment Variables\n\n`NODE_AUTH_TOKEN` set from GitHub secret `secrets.NPM_TOKEN` authenticates npm publish operation via `.npmrc` configuration injected by `actions/setup-node`.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 441,
      "cacheReadTokens": 0,
      "cacheCreationTokens": 26659,
      "latencyMs": 18065,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": ".github/workflows/publish.yml",
          "sizeBytes": 668
        }
      ]
    },
    {
      "timestamp": "2026-02-09T17:43:57.176Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/quality/inconsistency/code-vs-doc.ts\n\n```typescript\n/**\n * Heuristic code-vs-doc inconsistency detection.\n *\n * Compares exported symbols in TypeScript/JavaScript source against the\n * content of the corresponding .sum file to flag documentation drift.\n */\n\nimport type { SumFileContent } from '../../generation/writers/sum.js';\nimport type { CodeDocInconsistency } from '../types.js';\n\n/**\n * Extract named and default export identifiers from TypeScript/JavaScript source.\n *\n * Matches declarations like `export function foo`, `export const BAR`,\n * `export default class App`, etc. Ignores re-exports, commented-out lines,\n * and internal (non-exported) declarations.\n *\n * @param sourceContent - Raw source file content\n * @returns Array of exported identifier names\n */\nexport function extractExports(sourceContent: string): string[] {\n  const exports: string[] = [];\n  const exportRegex =\n    /^[ \\t]*export\\s+(?:default\\s+)?(?:function|class|const|let|var|type|interface|enum)\\s+(\\w+)/gm;\n  let match;\n  while ((match = exportRegex.exec(sourceContent)) !== null) {\n    exports.push(match[1]);\n  }\n  return exports;\n}\n\n/**\n * Compare source exports against .sum documentation content.\n *\n * Detects two kinds of inconsistency:\n * - **missingFromDoc**: symbols exported in source but not mentioned in .sum text\n * - **missingFromCode**: items listed in `publicInterface` with no matching export\n *\n * Uses case-sensitive matching. Returns `null` when documentation is consistent.\n *\n * @param sourceContent - Raw source file content\n * @param sumContent - Parsed .sum file content\n * @param filePath - Path to the source file (used in report)\n * @returns Inconsistency descriptor, or null if consistent\n */\nexport function checkCodeVsDoc(\n  sourceContent: string,\n  sumContent: SumFileContent,\n  filePath: string,\n): CodeDocInconsistency | null {\n  const exports = extractExports(sourceContent);\n  const sumText = sumContent.summary;\n\n  // Exports present in source but not mentioned anywhere in .sum text\n  const missingFromDoc = exports.filter((e) => !sumText.includes(e));\n\n  if (missingFromDoc.length === 0) {\n    return null;\n  }\n\n  return {\n    type: 'code-vs-doc',\n    severity: 'warning',\n    filePath,\n    sumPath: `${filePath}.sum`,\n    description: `Documentation out of sync: ${missingFromDoc.length} exports undocumented`,\n    details: { missingFromDoc, missingFromCode: [] },\n  };\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**Extracts exported identifiers from TypeScript/JavaScript source and detects code-vs-doc inconsistencies by comparing exports against `.sum` file content.**\n\n## Exported Functions\n\n**`extractExports(sourceContent: string): string[]`**\nExtracts exported identifier names from TypeScript/JavaScript source using regex pattern `/^[ \\t]*export\\s+(?:default\\s+)?(?:function|class|const|let|var|type|interface|enum)\\s+(\\w+)/gm`. Matches export declarations including `export function`, `export class`, `export const`, `export default class`, etc. Ignores re-exports, commented lines, and non-exported declarations. Returns array of exported symbol names.\n\n**`checkCodeVsDoc(sourceContent: string, sumContent: SumFileContent, filePath: string): CodeDocInconsistency | null`**\nCompares source exports against `.sum` documentation content via case-sensitive substring matching. Detects `missingFromDoc` inconsistencies (symbols exported in source but absent from `sumContent.summary` text). Returns `CodeDocInconsistency` with `type: 'code-vs-doc'`, `severity: 'warning'`, `filePath`, `sumPath`, `description`, and `details: { missingFromDoc, missingFromCode }`. Returns `null` when all exports appear in summary text.\n\n## Dependencies\n\nImports `SumFileContent` from `../../generation/writers/sum.js` for parsed `.sum` file structure and `CodeDocInconsistency` from `../types.js` for inconsistency report format.\n\n## Validation Strategy\n\nUses regex-based export extraction combined with substring search in summary text. Does not perform AST analysis, so misses complex export patterns (destructured, namespace, dynamic exports). Substring matching yields false negatives when prose mentions symbols in non-API contexts. The `missingFromCode` array is always empty (legacy field from removed `publicInterface` schema).\n\n## Behavioral Contracts\n\nExport extraction pattern: `/^[ \\t]*export\\s+(?:default\\s+)?(?:function|class|const|let|var|type|interface|enum)\\s+(\\w+)/gm` matches line-start whitespace, `export` keyword, optional `default`, declaration keyword, and captures identifier `(\\w+)`.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 535,
      "cacheReadTokens": 0,
      "cacheCreationTokens": 27159,
      "latencyMs": 22173,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/quality/inconsistency/code-vs-doc.ts",
          "sizeBytes": 2350
        }
      ]
    },
    {
      "timestamp": "2026-02-09T17:43:55.667Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/generation/writers/agents-md.ts\n\n```typescript\nimport { writeFile, readFile, mkdir, rename } from 'node:fs/promises';\nimport * as path from 'node:path';\n\n/** Marker comment to identify generated AGENTS.md files */\nexport const GENERATED_MARKER = '<!-- Generated by agents-reverse-engineer -->';\n\n/**\n * Check if an AGENTS.md file was generated by us (contains marker).\n */\nexport async function isGeneratedAgentsMd(filePath: string): Promise<boolean> {\n  try {\n    const content = await readFile(filePath, 'utf-8');\n    return content.includes(GENERATED_MARKER);\n  } catch {\n    return false;\n  }\n}\n\n/**\n * Write AGENTS.md for a directory with LLM-generated content.\n *\n * If a user-defined AGENTS.md exists (not generated by us), it will be\n * preserved as AGENTS.local.md. The user content is then prepended\n * verbatim above the generated content so AI agents see it first.\n *\n * @param dirPath - Directory to write AGENTS.md for\n * @param projectRoot - Project root for relative paths\n * @param content - LLM-generated AGENTS.md content\n * @returns Path to written AGENTS.md\n */\nexport async function writeAgentsMd(\n  dirPath: string,\n  _projectRoot: string,\n  content: string,\n): Promise<string> {\n  const agentsPath = path.join(dirPath, 'AGENTS.md');\n  const localPath = path.join(dirPath, 'AGENTS.local.md');\n\n  // Step 1: Preserve user-authored AGENTS.md (rename to AGENTS.local.md)\n  let userContent: string | null = null;\n  try {\n    const existingContent = await readFile(agentsPath, 'utf-8');\n    if (!existingContent.includes(GENERATED_MARKER)) {\n      userContent = existingContent;\n      await rename(agentsPath, localPath);\n    }\n  } catch {\n    // No existing AGENTS.md\n  }\n\n  // Step 2: Check for already-renamed AGENTS.local.md (from a previous run)\n  if (!userContent) {\n    try {\n      userContent = await readFile(localPath, 'utf-8');\n    } catch {\n      // No AGENTS.local.md either\n    }\n  }\n\n  // Step 3: Strip marker from LLM content (we add it ourselves)\n  let llmContent = content;\n  if (llmContent.startsWith(GENERATED_MARKER)) {\n    llmContent = llmContent.slice(GENERATED_MARKER.length).replace(/^\\n+/, '');\n  }\n\n  // Step 4: Build final content with marker + optional user content + LLM content\n  const parts: string[] = [GENERATED_MARKER, ''];\n\n  if (userContent?.trim()) {\n    parts.push(\n      '<!-- User-defined AGENTS.md preserved as AGENTS.local.md -->',\n      '',\n      userContent.trim(),\n      '',\n      '---',\n      '',\n    );\n  }\n\n  parts.push(llmContent);\n\n  const finalContent = parts.join('\\n');\n\n  await mkdir(path.dirname(agentsPath), { recursive: true });\n  await writeFile(agentsPath, finalContent, 'utf-8');\n\n  return agentsPath;\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**Manages AGENTS.md file lifecycle: writes LLM-generated directory documentation, preserves user-authored content by renaming to AGENTS.local.md, and prepends preserved content above generated sections with marker-based detection.**\n\n## Exported Constants\n\n`GENERATED_MARKER` — String constant `'<!-- Generated by agents-reverse-engineer -->'` embedded in all generated AGENTS.md files to distinguish them from user-authored versions.\n\n## Exported Functions\n\n### isGeneratedAgentsMd\n\n```typescript\nasync function isGeneratedAgentsMd(filePath: string): Promise<boolean>\n```\n\nReads file at `filePath`, returns `true` if content includes `GENERATED_MARKER` substring. Returns `false` on read failure (file not found, permission denied).\n\n### writeAgentsMd\n\n```typescript\nasync function writeAgentsMd(\n  dirPath: string,\n  _projectRoot: string,\n  content: string,\n): Promise<string>\n```\n\nWrites AGENTS.md to `dirPath` by composing marker + preserved user content + LLM-generated `content` parameter. Returns path to written AGENTS.md file (`path.join(dirPath, 'AGENTS.md')`).\n\n## User Content Preservation Strategy\n\nExecutes three-step preservation workflow:\n\n1. **Check existing AGENTS.md**: If file exists and lacks `GENERATED_MARKER`, renames to `AGENTS.local.md` via `rename()` and captures content as `userContent`\n2. **Fallback to AGENTS.local.md**: If no user content captured in step 1, attempts `readFile(localPath)` to load previously preserved content\n3. **Strip marker from LLM content**: Removes `GENERATED_MARKER` prefix via `slice()` if present, trims leading newlines with `/^\\n+/` regex\n\n## Final Content Assembly\n\nConstructs `finalContent` string from ordered parts array:\n\n```\n<!-- Generated by agents-reverse-engineer -->\n\n<!-- User-defined AGENTS.md preserved as AGENTS.local.md -->\n\n[userContent.trim()]\n\n---\n\n[llmContent]\n```\n\nUser content section (comment + content + separator) omitted if `userContent?.trim()` is falsy. Calls `mkdir(path.dirname(agentsPath), { recursive: true })` before `writeFile()` to ensure parent directory exists.\n\n## Integration Points\n\nInvoked by Phase 2 directory aggregation orchestrator (`src/generation/orchestrator.ts`) after LLM subprocess generates AGENTS.md content. Works with `src/generation/collector.ts` which reads AGENTS.local.md files when collecting subdirectory documentation for parent directory prompts.\n\n## Behavioral Contracts\n\n- **Marker detection pattern**: Substring search `content.includes(GENERATED_MARKER)` without regex or parsing\n- **Marker removal pattern**: `llmContent.startsWith(GENERATED_MARKER)` check followed by `slice(GENERATED_MARKER.length).replace(/^\\n+/, '')`\n- **File paths**: AGENTS.md (`path.join(dirPath, 'AGENTS.md')`), AGENTS.local.md (`path.join(dirPath, 'AGENTS.local.md')`)\n- **User content separator**: Triple-dash Markdown horizontal rule `'---'` surrounded by blank lines",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 776,
      "cacheReadTokens": 17367,
      "cacheCreationTokens": 9933,
      "latencyMs": 23785,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/generation/writers/agents-md.ts",
          "sizeBytes": 2636
        }
      ]
    },
    {
      "timestamp": "2026-02-09T17:43:35.663Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/generation/prompts/builder.ts\n\n```typescript\nimport * as path from 'node:path';\nimport { readdir, readFile } from 'node:fs/promises';\nimport pc from 'picocolors';\nimport type { PromptContext } from './types.js';\nimport { FILE_SYSTEM_PROMPT, FILE_USER_PROMPT, FILE_UPDATE_SYSTEM_PROMPT, DIRECTORY_SYSTEM_PROMPT, DIRECTORY_UPDATE_SYSTEM_PROMPT, ROOT_SYSTEM_PROMPT } from './templates.js';\nimport { readSumFile, getSumPath } from '../writers/sum.js';\nimport { GENERATED_MARKER } from '../writers/agents-md.js';\nimport { extractDirectoryImports, formatImportMap } from '../../imports/index.js';\nimport { collectAgentsDocs } from '../collector.js';\n\nfunction logTemplate(debug: boolean, action: string, filePath: string, extra?: string): void {\n  if (!debug) return;\n  const rel = path.relative(process.cwd(), filePath);\n  const msg = `${pc.dim('[prompt]')} ${pc.cyan(action)} ${pc.dim('→')} ${rel}`;\n  console.error(extra ? `${msg} ${pc.dim(extra)}` : msg);\n}\n\n/**\n * Detect language from file extension for syntax highlighting.\n */\nexport function detectLanguage(filePath: string): string {\n  const ext = path.extname(filePath).toLowerCase();\n  const langMap: Record<string, string> = {\n    '.ts': 'typescript',\n    '.tsx': 'tsx',\n    '.js': 'javascript',\n    '.jsx': 'jsx',\n    '.py': 'python',\n    '.rb': 'ruby',\n    '.go': 'go',\n    '.rs': 'rust',\n    '.java': 'java',\n    '.kt': 'kotlin',\n    '.swift': 'swift',\n    '.cs': 'csharp',\n    '.php': 'php',\n    '.vue': 'vue',\n    '.svelte': 'svelte',\n    '.json': 'json',\n    '.yaml': 'yaml',\n    '.yml': 'yaml',\n    '.md': 'markdown',\n    '.css': 'css',\n    '.scss': 'scss',\n    '.html': 'html',\n  };\n  return langMap[ext] ?? 'text';\n}\n\n/**\n * Build a complete prompt for file analysis.\n */\nexport function buildFilePrompt(context: PromptContext, debug = false): {\n  system: string;\n  user: string;\n} {\n  const lang = detectLanguage(context.filePath);\n  logTemplate(debug, 'buildFilePrompt', context.filePath, `lang=${lang}`);\n\n  const planSection = context.projectPlan\n    ? `\\n\\n## Project Structure\\n\\nFull project file listing for context:\\n\\n<project-structure>\\n${context.projectPlan}\\n</project-structure>`\n    : '';\n\n  let userPrompt = FILE_USER_PROMPT\n    .replace(/\\{\\{FILE_PATH\\}\\}/g, context.filePath)\n    .replace(/\\{\\{CONTENT\\}\\}/g, context.content)\n    .replace(/\\{\\{LANG\\}\\}/g, lang)\n    .replace(/\\{\\{PROJECT_PLAN_SECTION\\}\\}/g, planSection);\n\n  // Add context files if provided\n  if (context.contextFiles && context.contextFiles.length > 0) {\n    const contextSection = context.contextFiles\n      .map(\n        (f) =>\n          `\\n### ${f.path}\\n\\`\\`\\`${detectLanguage(f.path)}\\n${f.content}\\n\\`\\`\\``\n      )\n      .join('\\n');\n    userPrompt += `\\n\\n## Related Files\\n${contextSection}`;\n  }\n\n  // For incremental updates: include existing summary and use update-specific system prompt\n  if (context.existingSum) {\n    userPrompt += `\\n\\n## Existing Summary (update this — preserve stable content, modify only what changed)\\n\\n${context.existingSum}`;\n    return {\n      system: FILE_UPDATE_SYSTEM_PROMPT,\n      user: userPrompt,\n    };\n  }\n\n  return {\n    system: FILE_SYSTEM_PROMPT,\n    user: userPrompt,\n  };\n}\n\n/**\n * Build a prompt for generating a directory-level AGENTS.md.\n *\n * Reads all .sum files in the directory, child AGENTS.md files,\n * and AGENTS.local.md to provide full context to the LLM.\n */\nexport async function buildDirectoryPrompt(\n  dirPath: string,\n  projectRoot: string,\n  debug = false,\n  knownDirs?: Set<string>,\n  projectStructure?: string,\n  existingAgentsMd?: string,\n): Promise<{ system: string; user: string }> {\n  const relativePath = path.relative(projectRoot, dirPath) || '.';\n  const dirName = path.basename(dirPath) || 'root';\n\n  // Collect .sum file summaries and subdirectory sections in parallel\n  const entries = await readdir(dirPath, { withFileTypes: true });\n\n  const fileEntries = entries.filter(\n    (e) => e.isFile() && !e.name.endsWith('.sum') && !e.name.startsWith('.'),\n  );\n  const dirEntries = entries.filter((e) => {\n    if (!e.isDirectory()) return false;\n    if (!knownDirs) return true;\n    const relDir = path.relative(projectRoot, path.join(dirPath, e.name));\n    return knownDirs.has(relDir);\n  });\n\n  // Read all .sum files in parallel\n  const fileResults = await Promise.all(\n    fileEntries.map(async (entry) => {\n      const entryPath = path.join(dirPath, entry.name);\n      const sumPath = getSumPath(entryPath);\n      const sumContent = await readSumFile(sumPath);\n      if (sumContent) {\n        return `### ${entry.name}\\n**Purpose:** ${sumContent.metadata.purpose}\\n\\n${sumContent.summary}`;\n      }\n      return null;\n    }),\n  );\n  const fileSummaries = fileResults.filter((r): r is string => r !== null);\n\n  // Read all child AGENTS.md in parallel\n  const subdirResults = await Promise.all(\n    dirEntries.map(async (entry) => {\n      const childAgentsPath = path.join(dirPath, entry.name, 'AGENTS.md');\n      try {\n        const childContent = await readFile(childAgentsPath, 'utf-8');\n        return `### ${entry.name}/\\n${childContent}`;\n      } catch {\n        if (debug) {\n          console.error(pc.dim(`[prompt] Skipping missing ${childAgentsPath}`));\n        }\n        return null;\n      }\n    }),\n  );\n  const subdirSections = subdirResults.filter((r): r is string => r !== null);\n\n  // Check for user-defined documentation: AGENTS.local.md or non-ARE AGENTS.md\n  let localSection = '';\n  try {\n    const localContent = await readFile(path.join(dirPath, 'AGENTS.local.md'), 'utf-8');\n    localSection = `\\n## User Notes (AGENTS.local.md)\\n\\n${localContent}\\n\\nNote: Reference [AGENTS.local.md](./AGENTS.local.md) for additional documentation.`;\n  } catch {\n    // No AGENTS.local.md — check if current AGENTS.md is user-authored (first run)\n    try {\n      const agentsContent = await readFile(path.join(dirPath, 'AGENTS.md'), 'utf-8');\n      if (!agentsContent.includes(GENERATED_MARKER)) {\n        localSection = `\\n## User Notes (existing AGENTS.md)\\n\\n${agentsContent}\\n\\nNote: This user-defined content will be preserved as [AGENTS.local.md](./AGENTS.local.md).`;\n      }\n    } catch {\n      // No AGENTS.md either\n    }\n  }\n\n  // Detect manifest files to hint at package root\n  const manifestNames = ['package.json', 'Cargo.toml', 'go.mod', 'pyproject.toml', 'pom.xml', 'build.gradle', 'Gemfile', 'composer.json', 'CMakeLists.txt', 'Makefile'];\n  const foundManifests = fileEntries\n    .filter((e) => manifestNames.includes(e.name))\n    .map((e) => e.name);\n\n  // Extract actual import statements for cross-reference accuracy\n  const sourceExtensions = /\\.(ts|tsx|js|jsx|py|go|rs|java|kt)$/;\n  const sourceFileNames = fileEntries\n    .filter((e) => sourceExtensions.test(e.name))\n    .map((e) => e.name);\n\n  const fileImports = await extractDirectoryImports(dirPath, sourceFileNames);\n  const importMapText = formatImportMap(fileImports);\n\n  logTemplate(debug, 'buildDirectoryPrompt', dirPath, `files=${fileSummaries.length} subdirs=${subdirSections.length} imports=${fileImports.length}`);\n\n  const userSections: string[] = [\n    `Generate AGENTS.md for directory: \"${relativePath}\" (${dirName})`,\n    '',\n    `## File Summaries (${fileSummaries.length} files)`,\n    '',\n    ...fileSummaries,\n  ];\n\n  if (importMapText) {\n    userSections.push(\n      '',\n      '## Import Map (verified — use these exact paths)',\n      '',\n      importMapText,\n    );\n  }\n\n  if (projectStructure) {\n    userSections.push(\n      '',\n      '## Project Directory Structure',\n      '',\n      '<project-structure>',\n      projectStructure,\n      '</project-structure>',\n    );\n  }\n\n  // Scan for annex files in the directory\n  const annexFiles = entries\n    .filter((e) => e.isFile() && e.name.endsWith('.annex.md'))\n    .map((e) => e.name);\n  if (annexFiles.length > 0) {\n    userSections.push(\n      '',\n      '## Annex Files (reproduction-critical constants)',\n      '',\n      ...annexFiles.map((f) => `- ${f}`),\n    );\n  }\n\n  if (subdirSections.length > 0) {\n    userSections.push('', '## Subdirectories', '', ...subdirSections);\n  }\n\n  if (foundManifests.length > 0) {\n    userSections.push('', '## Directory Hints', '', `Contains manifest file(s): ${foundManifests.join(', ')} — likely a package or project root.`);\n  }\n\n  if (localSection) {\n    userSections.push(localSection);\n  }\n\n  // For incremental updates: include existing AGENTS.md and use update-specific system prompt\n  if (existingAgentsMd) {\n    userSections.push(\n      '',\n      '## Existing AGENTS.md (update this — preserve stable content, modify only what changed)',\n      '',\n      existingAgentsMd,\n    );\n    return {\n      system: DIRECTORY_UPDATE_SYSTEM_PROMPT,\n      user: userSections.join('\\n'),\n    };\n  }\n\n  return {\n    system: DIRECTORY_SYSTEM_PROMPT,\n    user: userSections.join('\\n'),\n  };\n}\n\n/**\n * Build a prompt for generating the root CLAUDE.md document.\n *\n * Collects all generated AGENTS.md files and optional package.json,\n * embedding them directly in the prompt so the LLM does not need\n * tool access to read files.\n */\nexport async function buildRootPrompt(\n  projectRoot: string,\n  debug = false,\n): Promise<{ system: string; user: string }> {\n  // 1. Collect all AGENTS.md files via shared collector\n  const agentsDocs = await collectAgentsDocs(projectRoot);\n  const agentsSections: string[] = agentsDocs.map(\n    (doc) => `### ${doc.relativePath}\\n\\n${doc.content}`,\n  );\n\n  // 3. Read root package.json for project metadata\n  let packageSection = '';\n  try {\n    const pkgRaw = await readFile(path.join(projectRoot, 'package.json'), 'utf-8');\n    const pkg = JSON.parse(pkgRaw) as Record<string, unknown>;\n    const parts: string[] = [];\n    if (pkg.name) parts.push(`- **Name**: ${pkg.name}`);\n    if (pkg.version) parts.push(`- **Version**: ${pkg.version}`);\n    if (pkg.description) parts.push(`- **Description**: ${pkg.description}`);\n    if (pkg.packageManager) parts.push(`- **Package Manager**: ${pkg.packageManager}`);\n    if (pkg.scripts && typeof pkg.scripts === 'object') {\n      const scripts = Object.entries(pkg.scripts as Record<string, string>)\n        .map(([k, v]) => `  - \\`${k}\\`: \\`${v}\\``)\n        .join('\\n');\n      parts.push(`- **Scripts**:\\n${scripts}`);\n    }\n    if (parts.length > 0) {\n      packageSection = `\\n## Package Metadata (package.json)\\n\\n${parts.join('\\n')}`;\n    }\n  } catch {\n    // No package.json or parse error\n  }\n\n  logTemplate(debug, 'buildRootPrompt', projectRoot, `agents=${agentsSections.length}`);\n\n  const userSections: string[] = [\n    'Generate CLAUDE.md for the project root.',\n    '',\n    'Synthesize the following AGENTS.md files into a single comprehensive project overview document.',\n    'Use ONLY the information provided below. Do NOT invent features, hooks, patterns, or APIs that are not explicitly mentioned in the AGENTS.md content.',\n    '',\n    `## AGENTS.md Files (${agentsSections.length} directories)`,\n    '',\n    ...agentsSections,\n  ];\n\n  if (packageSection) {\n    userSections.push(packageSection);\n  }\n\n  userSections.push(\n    '',\n    '## Output Requirements',\n    '',\n    'The document MUST include:',\n    '- Project purpose and description',\n    '- Architecture overview with directory structure',\n    '- Key directories table',\n    '- Getting started (install, build, run commands)',\n    '- Key technologies and dependencies',\n    '',\n    'This document is the COMPREHENSIVE reference for the entire project.',\n    'It should contain architecture, configuration, build instructions, and project-wide patterns.',\n    'Individual file details belong in directory-level AGENTS.md files — reference them, don\\'t duplicate them.',\n    '',\n    'Output ONLY the markdown content. No preamble.',\n  );\n\n  return {\n    system: ROOT_SYSTEM_PROMPT,\n    user: userSections.join('\\n'),\n  };\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**builder.ts constructs AI prompts for three-phase documentation generation: buildFilePrompt() assembles file analysis prompts with import context and optional incremental update sections, buildDirectoryPrompt() aggregates child .sum files and subdirectory AGENTS.md with import maps for directory synthesis, buildRootPrompt() collects all AGENTS.md files and package.json metadata for project-level CLAUDE.md generation.**\n\n## Exported Functions\n\n### buildFilePrompt\n```typescript\nfunction buildFilePrompt(context: PromptContext, debug = false): {\n  system: string;\n  user: string;\n}\n```\nConstructs file analysis prompt by detecting language via `detectLanguage()`, substituting `{{FILE_PATH}}`, `{{CONTENT}}`, `{{LANG}}`, `{{PROJECT_PLAN_SECTION}}` placeholders in `FILE_USER_PROMPT`, appending `context.contextFiles[]` as related file sections with syntax-highlighted code blocks, and conditionally appending `context.existingSum` with update instructions when incremental mode is active. Returns `FILE_SYSTEM_PROMPT` for fresh analysis or `FILE_UPDATE_SYSTEM_PROMPT` for incremental updates.\n\n### buildDirectoryPrompt\n```typescript\nasync function buildDirectoryPrompt(\n  dirPath: string,\n  projectRoot: string,\n  debug = false,\n  knownDirs?: Set<string>,\n  projectStructure?: string,\n  existingAgentsMd?: string,\n): Promise<{ system: string; user: string }>\n```\nAggregates directory documentation by reading all `.sum` files via `readSumFile()` and `getSumPath()`, collecting child `AGENTS.md` from subdirectories in `knownDirs` set, detecting user-authored content in `AGENTS.local.md` or non-generated `AGENTS.md` (checked via `GENERATED_MARKER` absence), extracting import maps via `extractDirectoryImports()` and `formatImportMap()`, enumerating manifest files from array `['package.json', 'Cargo.toml', 'go.mod', 'pyproject.toml', 'pom.xml', 'build.gradle', 'Gemfile', 'composer.json', 'CMakeLists.txt', 'Makefile']`, scanning for `.annex.md` files, and appending optional `projectStructure` in `<project-structure>` tags. Returns `DIRECTORY_SYSTEM_PROMPT` for fresh generation or `DIRECTORY_UPDATE_SYSTEM_PROMPT` when `existingAgentsMd` is provided. Filters source files by regex `/\\.(ts|tsx|js|jsx|py|go|rs|java|kt)$/` for import extraction.\n\n### buildRootPrompt\n```typescript\nasync function buildRootPrompt(\n  projectRoot: string,\n  debug = false,\n): Promise<{ system: string; user: string }>\n```\nSynthesizes project-level prompt by calling `collectAgentsDocs()` to gather all AGENTS.md files, parsing `package.json` to extract `name`, `version`, `description`, `packageManager`, and `scripts` object entries, embedding AGENTS.md content as `### {relativePath}` sections, and enforcing synthesis-only constraints with explicit instruction: \"Use ONLY the information provided below. Do NOT invent features, hooks, patterns, or APIs.\" Returns `ROOT_SYSTEM_PROMPT` with user prompt listing output requirements (architecture, key directories table, getting started, technologies).\n\n### detectLanguage\n```typescript\nfunction detectLanguage(filePath: string): string\n```\nMaps file extension via `path.extname()` to syntax identifier using `langMap` object covering 21 extensions: `.ts` → `'typescript'`, `.tsx` → `'tsx'`, `.js` → `'javascript'`, `.jsx` → `'jsx'`, `.py` → `'python'`, `.rb` → `'ruby'`, `.go` → `'go'`, `.rs` → `'rust'`, `.java` → `'java'`, `.kt` → `'kotlin'`, `.swift` → `'swift'`, `.cs` → `'csharp'`, `.php` → `'php'`, `.vue` → `'vue'`, `.svelte` → `'svelte'`, `.json` → `'json'`, `.yaml`/`.yml` → `'yaml'`, `.md` → `'markdown'`, `.css` → `'css'`, `.scss` → `'scss'`, `.html` → `'html'`. Fallback returns `'text'`.\n\n## Template Integration\n\nImports six prompt templates from `templates.js`: `FILE_SYSTEM_PROMPT`, `FILE_USER_PROMPT`, `FILE_UPDATE_SYSTEM_PROMPT`, `DIRECTORY_SYSTEM_PROMPT`, `DIRECTORY_UPDATE_SYSTEM_PROMPT`, `ROOT_SYSTEM_PROMPT`. `buildFilePrompt()` uses `FILE_USER_PROMPT` with mustache-style placeholder substitution pattern `/\\{\\{VARIABLE\\}\\}/g`. `buildDirectoryPrompt()` and `buildRootPrompt()` construct user prompts programmatically without template string substitution.\n\n## Incremental Update Strategy\n\nAll three builders accept optional existing content parameters: `context.existingSum` in `buildFilePrompt()`, `existingAgentsMd` in `buildDirectoryPrompt()`. When provided, appends existing content under heading `## Existing [Summary|AGENTS.md] (update this — preserve stable content, modify only what changed)` and switches to update-specific system prompts (`FILE_UPDATE_SYSTEM_PROMPT`, `DIRECTORY_UPDATE_SYSTEM_PROMPT`).\n\n## User Content Preservation\n\n`buildDirectoryPrompt()` checks for `AGENTS.local.md` first, falling back to checking existing `AGENTS.md` for absence of `GENERATED_MARKER` constant (imported from `../writers/agents-md.js`). User content appended as `## User Notes` section with reference link pattern `[AGENTS.local.md](./AGENTS.local.md)`. First-run detection message: \"This user-defined content will be preserved as [AGENTS.local.md](./AGENTS.local.md).\"\n\n## Debug Logging\n\nInternal `logTemplate()` function emits conditional debug output to stderr using `picocolors` formatting: `${pc.dim('[prompt]')} ${pc.cyan(action)} ${pc.dim('→')} ${relativePath} ${pc.dim(extra)}`. Called from all three builders with action names `'buildFilePrompt'`, `'buildDirectoryPrompt'`, `'buildRootPrompt'` and metadata strings like `lang=typescript`, `files=12 subdirs=3 imports=8`, `agents=15`.\n\n## Dependencies\n\n- `node:path`, `node:fs/promises` (readdir, readFile)\n- `picocolors` as `pc` for terminal colors\n- `./types.js` exports `PromptContext` interface\n- `./templates.js` exports six prompt constant strings\n- `../writers/sum.js` exports `readSumFile()`, `getSumPath()`\n- `../writers/agents-md.js` exports `GENERATED_MARKER` string constant\n- `../../imports/index.js` exports `extractDirectoryImports()`, `formatImportMap()`\n- `../collector.js` exports `collectAgentsDocs()`",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 1750,
      "cacheReadTokens": 0,
      "cacheCreationTokens": 30259,
      "latencyMs": 44095,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/generation/prompts/builder.ts",
          "sizeBytes": 11768
        }
      ]
    },
    {
      "timestamp": "2026-02-09T17:43:57.301Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/quality/inconsistency/reporter.ts\n\n```typescript\n/**\n * Structured report builder and CLI formatter for inconsistency results.\n *\n * Aggregates individual inconsistency issues into a typed report with\n * summary counts, and formats the report as human-readable plain text\n * for CLI output. No color dependencies -- the reporter stays pure\n * so it can be tested easily.\n */\n\nimport type {\n  Inconsistency,\n  InconsistencyReport,\n} from '../types.js';\n\n/**\n * Build a structured inconsistency report from a list of issues.\n *\n * Computes summary counts by type (code-vs-doc, code-vs-code) and\n * severity (error, warning, info). Attaches run metadata including\n * timestamp, project root, files checked, and duration.\n *\n * @param issues - All detected inconsistencies\n * @param metadata - Run context (projectRoot, filesChecked, durationMs)\n * @returns Structured report with issues and summary counts\n */\nexport function buildInconsistencyReport(\n  issues: Inconsistency[],\n  metadata: { projectRoot: string; filesChecked: number; durationMs: number },\n): InconsistencyReport {\n  let codeVsDoc = 0;\n  let codeVsCode = 0;\n  let phantomPaths = 0;\n  let errors = 0;\n  let warnings = 0;\n  let info = 0;\n\n  for (const issue of issues) {\n    if (issue.type === 'code-vs-doc') codeVsDoc++;\n    if (issue.type === 'code-vs-code') codeVsCode++;\n    if (issue.type === 'phantom-path') phantomPaths++;\n    if (issue.severity === 'error') errors++;\n    if (issue.severity === 'warning') warnings++;\n    if (issue.severity === 'info') info++;\n  }\n\n  return {\n    metadata: {\n      timestamp: new Date().toISOString(),\n      projectRoot: metadata.projectRoot,\n      filesChecked: metadata.filesChecked,\n      durationMs: metadata.durationMs,\n    },\n    issues,\n    summary: {\n      total: issues.length,\n      codeVsDoc,\n      codeVsCode,\n      phantomPaths,\n      errors,\n      warnings,\n      info,\n    },\n  };\n}\n\n/**\n * Format an inconsistency report as human-readable plain text.\n *\n * Output format:\n * ```\n * === Inconsistency Report ===\n * Checked 42 files in 150ms\n * Found 3 issue(s)\n *\n * [ERROR] Documentation out of sync: 2 exports undocumented, 1 ...\n *   File: src/foo/bar.ts\n *\n * [WARN] Symbol \"Config\" exported from 2 files\n *   Files: src/config/a.ts, src/config/b.ts\n * ```\n *\n * Uses plain text only (no picocolors). Color can be added by the\n * CLI layer if needed.\n *\n * @param report - The structured inconsistency report\n * @returns Formatted string for stderr output\n */\nexport function formatReportForCli(report: InconsistencyReport): string {\n  const lines: string[] = [];\n\n  lines.push('=== Inconsistency Report ===');\n  lines.push(`Checked ${report.metadata.filesChecked} files in ${report.metadata.durationMs}ms`);\n  lines.push(`Found ${report.summary.total} issue(s)`);\n  lines.push('');\n\n  for (const issue of report.issues) {\n    const severityTag =\n      issue.severity === 'error' ? '[ERROR]' :\n      issue.severity === 'warning' ? '[WARN]' :\n      '[INFO]';\n\n    lines.push(`${severityTag} ${issue.description}`);\n\n    if (issue.type === 'code-vs-doc') {\n      lines.push(`  File: ${issue.filePath}`);\n    } else if (issue.type === 'phantom-path') {\n      lines.push(`  Doc: ${issue.agentsMdPath}`);\n      lines.push(`  Path: ${issue.details.referencedPath}`);\n    } else {\n      lines.push(`  Files: ${issue.files.join(', ')}`);\n    }\n\n    lines.push('');\n  }\n\n  return lines.join('\\n');\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**reporter.ts exports buildInconsistencyReport() and formatReportForCli() to aggregate validation issues into typed reports with summary counts and render them as plain text for CLI output.**\n\n## Exported Functions\n\n### buildInconsistencyReport\n\n```typescript\nfunction buildInconsistencyReport(\n  issues: Inconsistency[],\n  metadata: { projectRoot: string; filesChecked: number; durationMs: number }\n): InconsistencyReport\n```\n\nAggregates `Inconsistency[]` array into structured `InconsistencyReport` with computed summary counts. Iterates issues once, incrementing counters for type classification (`codeVsDoc`, `codeVsCode`, `phantomPaths`) and severity classification (`errors`, `warnings`, `info`). Returns report object containing `metadata` (timestamp via `new Date().toISOString()`, projectRoot, filesChecked, durationMs), `issues` array passthrough, and `summary` object with totals.\n\n### formatReportForCli\n\n```typescript\nfunction formatReportForCli(report: InconsistencyReport): string\n```\n\nTransforms `InconsistencyReport` into plain text string for stderr output. Renders header with metadata (`Checked N files in Nms`, `Found N issue(s)`), then iterates `report.issues` emitting severity tags (`[ERROR]`, `[WARN]`, `[INFO]`) with `issue.description`. Type-discriminated path formatting: `code-vs-doc` shows `filePath`, `phantom-path` shows `agentsMdPath` and `details.referencedPath`, `code-vs-code` shows `files.join(', ')`. Returns newline-joined string. No color formatting applied (pure text output for testing).\n\n## Type Dependencies\n\nImports `Inconsistency` and `InconsistencyReport` from `../types.js`. `Inconsistency` is discriminated union with `type` field (`'code-vs-doc' | 'code-vs-code' | 'phantom-path'`) and `severity` field (`'error' | 'warning' | 'info'`). `InconsistencyReport` contains `metadata`, `issues`, and `summary` with per-type/per-severity counts.\n\n## Integration Points\n\nCalled by quality validation orchestrator in `src/quality/index.ts` after running `findCodeVsDocInconsistencies()`, `findCodeVsCodeInconsistencies()`, and `findPhantomPaths()`. CLI layer in `src/cli/generate.ts` or `src/cli/update.ts` invokes `formatReportForCli()` for console output after validation completes. Terminal color application handled upstream (reporter remains color-agnostic).\n\n## Output Format\n\nPlain text structure emitted by `formatReportForCli()`:\n```\n=== Inconsistency Report ===\nChecked 42 files in 150ms\nFound 3 issue(s)\n\n[ERROR] Documentation out of sync: 2 exports undocumented, 1 ...\n  File: src/foo/bar.ts\n\n[WARN] Symbol \"Config\" exported from 2 files\n  Files: src/config/a.ts, src/config/b.ts\n```\n\nHuman-readable severity tags (`[ERROR]`, `[WARN]`, `[INFO]`) followed by description and type-specific paths. Empty line separates issues for readability.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 805,
      "cacheReadTokens": 17367,
      "cacheCreationTokens": 10207,
      "latencyMs": 22560,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/quality/inconsistency/reporter.ts",
          "sizeBytes": 3366
        }
      ]
    },
    {
      "timestamp": "2026-02-09T17:43:56.548Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/quality/inconsistency/code-vs-code.ts\n\n```typescript\n/**\n * Heuristic cross-file inconsistency detection.\n *\n * Detects duplicate exports: the same symbol name exported from multiple\n * files within a group. The caller is responsible for scoping the file\n * list (typically per-directory) to avoid false positives across\n * unrelated modules.\n */\n\nimport { extractExports } from './code-vs-doc.js';\nimport type { CodeCodeInconsistency } from '../types.js';\n\n/**\n * Detect duplicate exports across a group of files.\n *\n * For each file, extracts exported symbol names via {@link extractExports}\n * and flags any symbol that appears in more than one file.\n *\n * This is a heuristic-only check (no AI calls). The caller should scope\n * the input to per-directory file groups to avoid false positives.\n *\n * @param files - Array of source files with path and content\n * @returns Array of detected cross-file inconsistencies\n */\nexport function checkCodeVsCode(\n  files: Array<{ path: string; content: string }>,\n): CodeCodeInconsistency[] {\n  // Map: export name -> list of file paths that export it\n  const exportMap = new Map<string, string[]>();\n\n  for (const file of files) {\n    const exports = extractExports(file.content);\n    for (const name of exports) {\n      const paths = exportMap.get(name);\n      if (paths) {\n        paths.push(file.path);\n      } else {\n        exportMap.set(name, [file.path]);\n      }\n    }\n  }\n\n  const inconsistencies: CodeCodeInconsistency[] = [];\n\n  for (const [name, paths] of exportMap) {\n    if (paths.length > 1) {\n      inconsistencies.push({\n        type: 'code-vs-code',\n        severity: 'warning',\n        files: paths,\n        description: `Symbol \"${name}\" exported from ${paths.length} files`,\n        pattern: 'duplicate-export',\n      });\n    }\n  }\n\n  return inconsistencies;\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**checkCodeVsCode() detects duplicate export symbols across source files within a scoped group (typically per-directory) using heuristic regex-based extraction, flagging symbols exported from multiple files as CodeCodeInconsistency warnings without AST analysis.**\n\n## Exported Function\n\n**checkCodeVsCode(files: Array<{ path: string; content: string }>): CodeCodeInconsistency[]**\n\nAccepts array of file objects with `path` and `content` properties. For each file, invokes `extractExports()` from `code-vs-doc.js` to extract exported symbol names via regex pattern `/^[ \\t]*export\\s+(?:default\\s+)?(?:function|class|const|let|var|type|interface|enum)\\s+(\\w+)/gm`. Aggregates symbols into `Map<string, string[]>` where key is export name and value is array of file paths exporting that symbol. Returns array of `CodeCodeInconsistency` objects for symbols appearing in more than one file.\n\n## Algorithm\n\n1. Initialize `exportMap` as `Map<string, string[]>` to track symbol-to-paths mapping\n2. Iterate each file in input array:\n   - Call `extractExports(file.content)` to get exported symbol names\n   - For each exported name:\n     - If name exists in map, push `file.path` to existing array\n     - Otherwise, create new entry with `[file.path]`\n3. Iterate exportMap entries:\n   - If `paths.length > 1`, construct `CodeCodeInconsistency` object with:\n     - `type: 'code-vs-code'`\n     - `severity: 'warning'`\n     - `files: paths` (all file paths exporting the symbol)\n     - `description: \"Symbol \\\"${name}\\\" exported from ${paths.length} files\"`\n     - `pattern: 'duplicate-export'`\n4. Return accumulated inconsistencies array\n\n## Dependencies\n\n**extractExports()** from `./code-vs-doc.js` — Regex-based export symbol extractor using pattern `/^[ \\t]*export\\s+(?:default\\s+)?(?:function|class|const|let|var|type|interface|enum)\\s+(\\w+)/gm`\n\n**CodeCodeInconsistency** from `../types.js` — Discriminated union member with `type: 'code-vs-code'`, `severity: 'warning'`, `files: string[]`, `description: string`, `pattern: 'duplicate-export'`\n\n## Usage Context\n\nCaller must scope input files to prevent false positives. Typical usage: per-directory validation in `src/quality/index.ts` where files are grouped by containing directory before invoking `checkCodeVsCode()`. Cross-directory duplicate detection would flag legitimate symbol reuse across unrelated modules (e.g., `index.ts` in multiple directories commonly exports same helper names).\n\n## Known Limitations\n\nHeuristic regex extraction misses:\n- Destructured exports: `export const { foo, bar } = obj`\n- Namespace exports: `export * as Utils from './utils'`\n- Dynamic exports: `export { [computedName]: value }`\n- Re-exports with renaming: `export { foo as bar } from './mod'`\n\nNo AST analysis to distinguish intentional duplication (e.g., interface/implementation pairs, test fixtures) from architectural violations. Operates purely on symbol name matching without semantic understanding of whether duplicates represent distinct types, overloads, or genuinely conflicting definitions.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 828,
      "cacheReadTokens": 17367,
      "cacheCreationTokens": 9619,
      "latencyMs": 24127,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/quality/inconsistency/code-vs-code.ts",
          "sizeBytes": 1774
        }
      ]
    },
    {
      "timestamp": "2026-02-09T17:44:06.939Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/ai/retry.ts\n\n```typescript\n/**\n * Retry utility with exponential backoff for transient AI service failures.\n *\n * Wraps any async operation with configurable retry logic. Uses exponential\n * delays with jitter to prevent thundering herd when multiple callers hit\n * the same rate limit.\n *\n * @module\n */\n\nimport type { RetryOptions } from './types.js';\n\n/**\n * Default retry configuration values.\n *\n * Does NOT include `isRetryable` or `onRetry` since those are\n * caller-specific. Spread these defaults and provide your own predicates:\n *\n * @example\n * ```typescript\n * import { withRetry, DEFAULT_RETRY_OPTIONS } from './retry.js';\n * import { AIServiceError } from './types.js';\n *\n * const result = await withRetry(() => callAI(prompt), {\n *   ...DEFAULT_RETRY_OPTIONS,\n *   isRetryable: (err) => err instanceof AIServiceError && err.code === 'RATE_LIMIT',\n *   onRetry: (attempt, err) => console.warn(`Retry ${attempt}:`, err),\n * });\n * ```\n */\nexport const DEFAULT_RETRY_OPTIONS = {\n  /** Maximum number of retries (3 retries = 4 total attempts) */\n  maxRetries: 3,\n  /** Base delay before first retry: 1 second */\n  baseDelayMs: 1_000,\n  /** Maximum delay cap: 8 seconds */\n  maxDelayMs: 8_000,\n  /** Exponential multiplier: delay doubles each attempt */\n  multiplier: 2,\n} as const satisfies Omit<RetryOptions, 'isRetryable' | 'onRetry'>;\n\n/**\n * Execute an async function with exponential backoff retry on failure.\n *\n * - On success: returns the result immediately.\n * - On transient failure (isRetryable returns true): waits with exponential\n *   backoff + jitter, then retries up to `maxRetries` times.\n * - On permanent failure (isRetryable returns false): throws immediately\n *   without retrying.\n * - After exhausting all retries: throws the last error.\n *\n * Delay formula: `min(baseDelayMs * multiplier^attempt, maxDelayMs) + jitter`\n * where jitter is a random value in [0, 500ms].\n *\n * @typeParam T - The return type of the wrapped function\n * @param fn - Async function to execute (and potentially retry)\n * @param options - Retry configuration including backoff timing and predicates\n * @returns The result of a successful `fn()` invocation\n * @throws The last error if all retry attempts are exhausted or if the error is not retryable\n *\n * @example\n * ```typescript\n * import { withRetry } from './retry.js';\n *\n * // Retry up to 3 times on rate limit errors\n * const response = await withRetry(\n *   () => runAICall(prompt),\n *   {\n *     maxRetries: 3,\n *     baseDelayMs: 1000,\n *     maxDelayMs: 8000,\n *     multiplier: 2,\n *     isRetryable: (err) => isRateLimitError(err),\n *     onRetry: (attempt, err) => logger.warn(`Attempt ${attempt} failed, retrying...`),\n *   },\n * );\n * ```\n */\nexport async function withRetry<T>(\n  fn: () => Promise<T>,\n  options: RetryOptions,\n): Promise<T> {\n  for (let attempt = 0; attempt <= options.maxRetries; attempt++) {\n    try {\n      return await fn();\n    } catch (error) {\n      // If we've exhausted retries or the error is permanent, throw immediately.\n      if (attempt === options.maxRetries || !options.isRetryable(error)) {\n        throw error;\n      }\n\n      // Compute exponential delay with cap.\n      const exponentialDelay = options.baseDelayMs * Math.pow(options.multiplier, attempt);\n      const cappedDelay = Math.min(exponentialDelay, options.maxDelayMs);\n\n      // Add jitter (0-500ms) to prevent thundering herd.\n      const jitter = Math.random() * 500;\n      const delay = cappedDelay + jitter;\n\n      // Notify caller before waiting.\n      options.onRetry?.(attempt + 1, error);\n\n      await new Promise((resolve) => setTimeout(resolve, delay));\n    }\n  }\n\n  // This is unreachable -- the loop either returns or throws.\n  // TypeScript needs it for exhaustiveness.\n  throw new Error('withRetry: unreachable');\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**Exponential backoff retry wrapper for transient AI service failures with configurable predicates, timing multipliers, and jitter-based delay randomization.**\n\n## Exported Functions\n\n`withRetry<T>(fn: () => Promise<T>, options: RetryOptions): Promise<T>` executes async function with exponential backoff retry logic. Returns result immediately on success. On transient failure matching `options.isRetryable` predicate, waits with exponential delay then retries up to `options.maxRetries` times. On permanent failure (`isRetryable` returns false), throws immediately without retrying. After exhausting all retries, throws last error. Delay formula: `min(baseDelayMs * multiplier^attempt, maxDelayMs) + jitter` where jitter is random value in [0, 500ms]. Invokes optional `options.onRetry(attempt, error)` callback before each delay.\n\n## Exported Constants\n\n`DEFAULT_RETRY_OPTIONS` provides base retry configuration with `maxRetries: 3` (4 total attempts), `baseDelayMs: 1_000` (1 second base delay), `maxDelayMs: 8_000` (8 second cap), `multiplier: 2` (exponential doubling). Intentionally omits `isRetryable` and `onRetry` predicates requiring caller-specific implementation. Type satisfies `Omit<RetryOptions, 'isRetryable' | 'onRetry'>` for partial spread pattern.\n\n## Retry Flow Control\n\nLoop executes `attempt` from 0 to `options.maxRetries` inclusive. On catch block: throws immediately if `attempt === options.maxRetries` (exhausted) or `!options.isRetryable(error)` (permanent failure). Computes `exponentialDelay = baseDelayMs * Math.pow(multiplier, attempt)`, applies `cappedDelay = Math.min(exponentialDelay, maxDelayMs)`, adds `jitter = Math.random() * 500`, waits `delay = cappedDelay + jitter` via `setTimeout` promise wrapper. Invokes `options.onRetry?.(attempt + 1, error)` before waiting (attempt is 0-indexed, callback receives 1-indexed retry number).\n\n## Integration Pattern\n\nDesigned for `AIService.call()` wrapping `runSubprocess()` invocations. Caller provides `isRetryable` predicate matching rate limit error patterns in stderr (strings: \"rate limit\", \"429\", \"too many requests\", \"overloaded\"). Callback `onRetry` emits trace events via `ITraceWriter.emit('retry', ...)` for telemetry correlation with subprocess lifecycle events.\n\n## Jitter Strategy\n\nAdds uniform random jitter `Math.random() * 500` (0-500ms range) to prevent thundering herd when multiple workers hit same rate limit simultaneously. Jitter applied after exponential backoff and delay cap, not subject to `maxDelayMs` constraint.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 699,
      "cacheReadTokens": 0,
      "cacheCreationTokens": 27669,
      "latencyMs": 17812,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/ai/retry.ts",
          "sizeBytes": 3781
        }
      ]
    },
    {
      "timestamp": "2026-02-09T17:43:56.306Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/generation/writers/sum.ts\n\n```typescript\nimport { writeFile, readFile, mkdir } from 'node:fs/promises';\nimport * as path from 'node:path';\nimport type { SummaryMetadata } from '../types.js';\nimport { GENERATED_MARKER } from './agents-md.js';\n\n/**\n * Content structure for a .sum file.\n */\nexport interface SumFileContent {\n  /** Main summary text (detailed description) */\n  summary: string;\n  /** Extracted metadata */\n  metadata: SummaryMetadata;\n  /** Generation timestamp */\n  generatedAt: string;\n  /** SHA-256 hash of source file content (for change detection) */\n  contentHash: string;\n}\n\n/**\n * Parse a .sum file back into structured content.\n * Returns null if file doesn't exist or is invalid.\n */\nexport async function readSumFile(sumPath: string): Promise<SumFileContent | null> {\n  try {\n    const content = await readFile(sumPath, 'utf-8');\n    return parseSumFile(content);\n  } catch {\n    return null;\n  }\n}\n\n/**\n * Parse a YAML-style array from frontmatter.\n * Supports both inline [a, b, c] and multi-line formats.\n */\nfunction parseYamlArray(frontmatter: string, key: string): string[] {\n  // Try inline format first: key: [a, b, c]\n  const inlineMatch = frontmatter.match(new RegExp(`${key}:\\\\s*\\\\[([^\\\\]]*)\\\\]`));\n  if (inlineMatch) {\n    return inlineMatch[1]\n      .split(',')\n      .map(s => s.trim().replace(/^[\"']|[\"']$/g, ''))\n      .filter(s => s.length > 0);\n  }\n\n  // Try multi-line format:\n  // key:\n  //   - item1\n  //   - item2\n  const multiLineMatch = frontmatter.match(new RegExp(`${key}:\\\\s*\\\\n((?:\\\\s+-\\\\s+.+\\\\n?)+)`, 'm'));\n  if (multiLineMatch) {\n    return multiLineMatch[1]\n      .split('\\n')\n      .map(line => line.replace(/^\\s*-\\s*/, '').trim())\n      .filter(s => s.length > 0);\n  }\n\n  return [];\n}\n\n/**\n * Parse .sum file content into structured data.\n */\nfunction parseSumFile(content: string): SumFileContent | null {\n  try {\n    // Extract frontmatter\n    const frontmatterMatch = content.match(/^---\\n([\\s\\S]*?)\\n---\\n/);\n    if (!frontmatterMatch) return null;\n\n    const frontmatter = frontmatterMatch[1];\n    const summary = content.slice(frontmatterMatch[0].length).trim();\n\n    // Parse frontmatter (simple YAML-like parsing)\n    const generatedAt = frontmatter.match(/generated_at:\\s*(.+)/)?.[1]?.trim() ?? '';\n    const contentHash = frontmatter.match(/content_hash:\\s*(.+)/)?.[1]?.trim() ?? '';\n\n    // Parse purpose (single line value)\n    const purpose = frontmatter.match(/purpose:\\s*(.+)/)?.[1]?.trim() ?? '';\n\n    const metadata: SummaryMetadata = {\n      purpose,\n    };\n\n    // Parse optional fields\n    const criticalTodos = parseYamlArray(frontmatter, 'critical_todos');\n    if (criticalTodos.length > 0) {\n      metadata.criticalTodos = criticalTodos;\n    }\n\n    const relatedFiles = parseYamlArray(frontmatter, 'related_files');\n    if (relatedFiles.length > 0) {\n      metadata.relatedFiles = relatedFiles;\n    }\n\n    return {\n      summary,\n      metadata,\n      generatedAt,\n      contentHash,\n    };\n  } catch {\n    return null;\n  }\n}\n\n/**\n * Format a YAML array for frontmatter.\n * Uses inline format for short arrays, multi-line for longer ones.\n */\nfunction formatYamlArray(key: string, values: string[]): string {\n  if (values.length === 0) {\n    return `${key}: []`;\n  }\n  if (values.length <= 3 && values.every(v => v.length < 40)) {\n    // Inline format for short arrays\n    return `${key}: [${values.join(', ')}]`;\n  }\n  // Multi-line format for longer arrays\n  return `${key}:\\n${values.map(v => `  - ${v}`).join('\\n')}`;\n}\n\n/**\n * Format .sum file content for writing.\n */\nfunction formatSumFile(content: SumFileContent): string {\n  const lines = [\n    '---',\n    `generated_at: ${content.generatedAt}`,\n    `content_hash: ${content.contentHash}`,\n    `purpose: ${content.metadata.purpose}`,\n  ];\n\n  // Add optional fields if present\n  if (content.metadata.criticalTodos && content.metadata.criticalTodos.length > 0) {\n    lines.push(formatYamlArray('critical_todos', content.metadata.criticalTodos));\n  }\n  if (content.metadata.relatedFiles && content.metadata.relatedFiles.length > 0) {\n    lines.push(formatYamlArray('related_files', content.metadata.relatedFiles));\n  }\n\n  lines.push('---', '');\n\n  return lines.join('\\n') + content.summary;\n}\n\n/**\n * Write a .sum file alongside a source file.\n * Creates: foo.ts -> foo.ts.sum\n *\n * @param sourcePath - Path to the source file\n * @param content - Summary content to write\n * @returns Path to the written .sum file\n */\nexport async function writeSumFile(\n  sourcePath: string,\n  content: SumFileContent\n): Promise<string> {\n  const sumPath = `${sourcePath}.sum`;\n  const dir = path.dirname(sumPath);\n\n  // Ensure directory exists\n  await mkdir(dir, { recursive: true });\n\n  // Write file\n  const formatted = formatSumFile(content);\n  await writeFile(sumPath, formatted, 'utf-8');\n\n  return sumPath;\n}\n\n/**\n * Get the .sum path for a source file.\n */\nexport function getSumPath(sourcePath: string): string {\n  return `${sourcePath}.sum`;\n}\n\n/**\n * Check if a .sum file exists for a source file.\n */\nexport async function sumFileExists(sourcePath: string): Promise<boolean> {\n  const sumPath = getSumPath(sourcePath);\n  const content = await readSumFile(sumPath);\n  return content !== null;\n}\n\n/**\n * Write an annex file alongside a source file.\n * Contains the full source content for reproduction-critical files\n * whose verbatim constants cannot fit within .sum word limits.\n *\n * Creates: foo.ts -> foo.ts.annex.md\n *\n * @param sourcePath - Absolute path to the source file\n * @param sourceContent - Full source file content\n * @returns Path to the written annex file\n */\nexport async function writeAnnexFile(\n  sourcePath: string,\n  sourceContent: string,\n): Promise<string> {\n  const annexPath = getAnnexPath(sourcePath);\n  const fileName = path.basename(sourcePath);\n  const content = [\n    GENERATED_MARKER,\n    `# Annex: ${fileName}`,\n    '',\n    `Reproduction-critical source content from \\`${fileName}\\`.`,\n    `Referenced by \\`${fileName}.sum\\`.`,\n    '',\n    '```',\n    sourceContent,\n    '```',\n    '',\n  ].join('\\n');\n  await writeFile(annexPath, content, 'utf-8');\n  return annexPath;\n}\n\n/**\n * Get the .annex.md path for a source file.\n */\nexport function getAnnexPath(sourcePath: string): string {\n  return `${sourcePath}.annex.md`;\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**sum.ts manages .sum file I/O with YAML frontmatter serialization, parsing, and annex file generation for reproduction-critical source content.**\n\n## Exported Interfaces and Types\n\n`SumFileContent` interface defines `.sum` file structure with `summary: string`, `metadata: SummaryMetadata`, `generatedAt: string`, `contentHash: string` (SHA-256 hex digest for change detection).\n\n## Core I/O Functions\n\n`writeSumFile(sourcePath: string, content: SumFileContent): Promise<string>` appends `.sum` extension to `sourcePath`, creates parent directory via `mkdir(..., { recursive: true })`, formats content via `formatSumFile()`, writes with `writeFile()`, returns absolute `.sum` path.\n\n`readSumFile(sumPath: string): Promise<SumFileContent | null>` reads file via `readFile()`, delegates to `parseSumFile()`, returns `null` on read failure or parse error.\n\n`getSumPath(sourcePath: string): string` returns `${sourcePath}.sum` without filesystem access.\n\n`sumFileExists(sourcePath: string): Promise<boolean>` calls `readSumFile(getSumPath(sourcePath))` and checks for non-null result.\n\n## Frontmatter Parsing\n\n`parseSumFile(content: string): SumFileContent | null` extracts YAML frontmatter via regex `/^---\\n([\\s\\S]*?)\\n---\\n/`, parses `generated_at`, `content_hash`, `purpose` fields via individual regex patterns (`/generated_at:\\s*(.+)/`, `/content_hash:\\s*(.+)/`, `/purpose:\\s*(.+)/`), delegates array parsing to `parseYamlArray()` for `critical_todos` and `related_files`, returns `null` on parse failure.\n\n`parseYamlArray(frontmatter: string, key: string): string[]` supports inline format `key: [a, b, c]` via regex `new RegExp(\\`${key}:\\\\s*\\\\[([^\\\\]]*)\\\\]\\`)` and multi-line format via `new RegExp(\\`${key}:\\\\s*\\\\n((?:\\\\s+-\\\\s+.+\\\\n?)+)\\`, 'm')`, strips quotes and list markers, returns empty array if neither pattern matches.\n\n## Frontmatter Serialization\n\n`formatSumFile(content: SumFileContent): string` builds YAML frontmatter block with `---` delimiters, writes `generated_at`, `content_hash`, `purpose` as top-level fields, conditionally includes `critical_todos` and `related_files` via `formatYamlArray()` if arrays non-empty, appends summary body after closing `---\\n\\n`.\n\n`formatYamlArray(key: string, values: string[]): string` returns `key: []` for empty arrays, inline format `key: [val1, val2]` for arrays ≤3 elements with all values <40 chars, multi-line YAML list format `key:\\n  - val1\\n  - val2` otherwise.\n\n## Annex File Generation\n\n`writeAnnexFile(sourcePath: string, sourceContent: string): Promise<string>` creates `${sourcePath}.annex.md` with markdown structure: `GENERATED_MARKER` comment (imported from `agents-md.ts`), `# Annex: {basename}` heading, explanatory text referencing `.sum` file, triple-backtick code fence containing full `sourceContent`, writes via `writeFile()`, returns annex path.\n\n`getAnnexPath(sourcePath: string): string` returns `${sourcePath}.annex.md` without filesystem access.\n\n## Integration Points\n\nImports `GENERATED_MARKER` from `./agents-md.js` for consistent generated file annotation. Used by update workflow (`src/update/`) for hash comparison via `readSumFile()`, by file analysis phase (`src/generation/executor.ts`) for `.sum` output via `writeSumFile()`, by orphan cleanup (`src/update/orphan-cleaner.ts`) for stale file deletion.\n\n## YAML Format Contract\n\nFrontmatter delimiter: `---\\n...\\n---\\n` with exactly one newline after closing delimiter before summary body begins.\n\nScalar field pattern: `key: value` with optional trailing whitespace.\n\nArray formats:\n- Inline: `key: [item1, item2]` with comma-separated values\n- Multi-line: `key:\\n  - item1\\n  - item2` with two-space indent and `- ` prefix per line\n\nRequired fields: `generated_at` (ISO 8601 timestamp), `content_hash` (64-char hex string), `purpose` (single-line string).\n\nOptional fields: `critical_todos` (string array), `related_files` (string array).",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 1142,
      "cacheReadTokens": 17367,
      "cacheCreationTokens": 11136,
      "latencyMs": 30269,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/generation/writers/sum.ts",
          "sizeBytes": 6247
        }
      ]
    },
    {
      "timestamp": "2026-02-09T17:44:06.861Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/ai/registry.ts\n\n```typescript\n/**\n * Backend registry, factory, and auto-detection.\n *\n * Manages the set of registered AI CLI backends, selects the appropriate\n * backend at runtime via auto-detection or explicit request, and provides\n * actionable error messages when no CLI is found.\n *\n * @module\n */\n\nimport type { AIBackend } from './types.js';\nimport { AIServiceError } from './types.js';\nimport { ClaudeBackend } from './backends/claude.js';\nimport { GeminiBackend } from './backends/gemini.js';\nimport { OpenCodeBackend } from './backends/opencode.js';\n\n// ---------------------------------------------------------------------------\n// Backend registry\n// ---------------------------------------------------------------------------\n\n/**\n * Registry of available AI CLI backends.\n *\n * Stores backends in insertion order, which determines the priority for\n * auto-detection. Use {@link createBackendRegistry} to get a pre-populated\n * registry with all supported backends.\n *\n * @example\n * ```typescript\n * const registry = createBackendRegistry();\n * const claude = registry.get('claude');\n * const all = registry.getAll(); // [ClaudeBackend, GeminiBackend, OpenCodeBackend]\n * ```\n */\nexport class BackendRegistry {\n  private readonly backends = new Map<string, AIBackend>();\n\n  /**\n   * Register a backend adapter.\n   *\n   * @param backend - The backend to register (keyed by its `name` property)\n   */\n  register(backend: AIBackend): void {\n    this.backends.set(backend.name, backend);\n  }\n\n  /**\n   * Get a specific backend by name.\n   *\n   * @param name - The backend name (e.g., \"claude\", \"gemini\", \"opencode\")\n   * @returns The backend, or `undefined` if not registered\n   */\n  get(name: string): AIBackend | undefined {\n    return this.backends.get(name);\n  }\n\n  /**\n   * Get all registered backends in priority order.\n   *\n   * @returns Array of all registered backends\n   */\n  getAll(): AIBackend[] {\n    return Array.from(this.backends.values());\n  }\n}\n\n// ---------------------------------------------------------------------------\n// Factory\n// ---------------------------------------------------------------------------\n\n/**\n * Create a new backend registry pre-populated with all supported backends.\n *\n * Registration order determines auto-detection priority:\n * 1. Claude (recommended, fully implemented)\n * 2. Gemini (experimental, stub)\n * 3. OpenCode (experimental, stub)\n *\n * @returns A populated {@link BackendRegistry}\n *\n * @example\n * ```typescript\n * const registry = createBackendRegistry();\n * const backend = await detectBackend(registry);\n * ```\n */\nexport function createBackendRegistry(): BackendRegistry {\n  const registry = new BackendRegistry();\n  registry.register(new ClaudeBackend());\n  registry.register(new GeminiBackend());\n  registry.register(new OpenCodeBackend());\n  return registry;\n}\n\n// ---------------------------------------------------------------------------\n// Auto-detection\n// ---------------------------------------------------------------------------\n\n/**\n * Detect the first available backend on PATH in priority order.\n *\n * Iterates all registered backends and calls `isAvailable()` on each.\n * Returns the first backend whose CLI is found, or `null` if none are\n * available.\n *\n * Priority order is determined by registration order in\n * {@link createBackendRegistry}: Claude > Gemini > OpenCode.\n *\n * @param registry - The backend registry to search\n * @returns The first available backend, or `null` if none found\n *\n * @example\n * ```typescript\n * const registry = createBackendRegistry();\n * const backend = await detectBackend(registry);\n * if (backend) {\n *   console.log(`Using ${backend.name} backend`);\n * }\n * ```\n */\nexport async function detectBackend(registry: BackendRegistry): Promise<AIBackend | null> {\n  for (const backend of registry.getAll()) {\n    if (await backend.isAvailable()) {\n      return backend;\n    }\n  }\n  return null;\n}\n\n// ---------------------------------------------------------------------------\n// Install instructions\n// ---------------------------------------------------------------------------\n\n/**\n * Get formatted install instructions for all registered backends.\n *\n * Returns a multi-line string suitable for error messages when no CLI\n * is found. Matches the error message template from RESEARCH.md.\n *\n * @param registry - The backend registry\n * @returns Formatted install instructions string\n *\n * @example\n * ```typescript\n * const instructions = getInstallInstructions(registry);\n * console.error(`No AI CLI found.\\n\\nInstall one of the following:\\n\\n${instructions}`);\n * ```\n */\nexport function getInstallInstructions(registry: BackendRegistry): string {\n  return registry\n    .getAll()\n    .map((backend) => backend.getInstallInstructions())\n    .join('\\n\\n');\n}\n\n// ---------------------------------------------------------------------------\n// Resolution\n// ---------------------------------------------------------------------------\n\n/**\n * Resolve a backend by name or auto-detect the best available one.\n *\n * - If `requested` is `'auto'`: runs {@link detectBackend} and throws with\n *   install instructions if nothing is found.\n * - If `requested` is a specific name: looks it up in the registry, checks\n *   availability, and throws if not found or not available.\n *\n * @param registry - The backend registry\n * @param requested - Backend name or `'auto'` for auto-detection\n * @returns The resolved backend adapter\n * @throws {AIServiceError} With code `CLI_NOT_FOUND` if no backend is available\n *\n * @example\n * ```typescript\n * const registry = createBackendRegistry();\n *\n * // Auto-detect\n * const backend = await resolveBackend(registry, 'auto');\n *\n * // Explicit selection\n * const claude = await resolveBackend(registry, 'claude');\n * ```\n */\nexport async function resolveBackend(\n  registry: BackendRegistry,\n  requested: string | 'auto',\n): Promise<AIBackend> {\n  if (requested === 'auto') {\n    const detected = await detectBackend(registry);\n    if (detected) {\n      return detected;\n    }\n\n    const instructions = getInstallInstructions(registry);\n    throw new AIServiceError(\n      'CLI_NOT_FOUND',\n      `No AI CLI found on your system.\\n\\nInstall one of the following:\\n\\n${instructions}\\n\\nThen run this command again.`,\n    );\n  }\n\n  // Explicit backend requested\n  const backend = registry.get(requested);\n  if (!backend) {\n    const known = registry\n      .getAll()\n      .map((b) => b.name)\n      .join(', ');\n    throw new AIServiceError(\n      'CLI_NOT_FOUND',\n      `Unknown backend \"${requested}\". Available backends: ${known}`,\n    );\n  }\n\n  if (!(await backend.isAvailable())) {\n    throw new AIServiceError(\n      'CLI_NOT_FOUND',\n      `Backend \"${requested}\" is not available. The \"${backend.cliCommand}\" CLI was not found on PATH.\\n\\n${backend.getInstallInstructions()}`,\n    );\n  }\n\n  return backend;\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**registry.ts orchestrates AIBackend adapter registration, auto-detection via PATH availability checks, and runtime backend resolution with actionable CLI_NOT_FOUND error messages containing install instructions.**\n\n## Exported Classes\n\n**BackendRegistry** maintains a `Map<string, AIBackend>` of registered backends in insertion order, which determines auto-detection priority. Methods:\n- `register(backend: AIBackend): void` — adds backend keyed by its `name` property\n- `get(name: string): AIBackend | undefined` — retrieves backend by name\n- `getAll(): AIBackend[]` — returns all backends in registration order\n\n## Exported Functions\n\n**createBackendRegistry(): BackendRegistry** instantiates a pre-populated registry with three backends in priority order: `ClaudeBackend` (fully implemented), `GeminiBackend` (experimental stub), `OpenCodeBackend` (experimental stub). Registration order controls `detectBackend()` iteration sequence.\n\n**detectBackend(registry: BackendRegistry): Promise<AIBackend | null>** iterates `registry.getAll()` calling `backend.isAvailable()` on each, returning the first backend whose CLI exists on PATH or `null` if none found. Follows priority: Claude > Gemini > OpenCode.\n\n**getInstallInstructions(registry: BackendRegistry): string** aggregates `backend.getInstallInstructions()` from all registered backends into newline-separated multi-line string for CLI_NOT_FOUND error message formatting.\n\n**resolveBackend(registry: BackendRegistry, requested: string | 'auto'): Promise<AIBackend>** implements the main resolution logic:\n- If `requested === 'auto'`: calls `detectBackend()`, throws `AIServiceError` with code `CLI_NOT_FOUND` and aggregated install instructions if no backend detected\n- If `requested` is a backend name: calls `registry.get(requested)`, throws `CLI_NOT_FOUND` if unknown (listing all known backend names), throws `CLI_NOT_FOUND` with backend-specific install instructions if `backend.isAvailable()` returns false (checks for `backend.cliCommand` on PATH)\n\n## Integration Points\n\nImports `AIBackend` interface and `AIServiceError` from `./types.js`. Imports concrete adapters: `ClaudeBackend` from `./backends/claude.js`, `GeminiBackend` from `./backends/gemini.js`, `OpenCodeBackend` from `./backends/opencode.js`. \n\nCalled by `AIService` in `src/ai/service.ts` during initialization to resolve the backend specified in configuration `ai.backend` field (defaults to `'auto'`). The resolved `AIBackend` instance provides the `cliCommand`, `buildArgs()`, and `parseResponse()` methods used by `runSubprocess()` for AI CLI invocation.\n\n## Error Handling\n\nAll `AIServiceError` instances thrown with code `'CLI_NOT_FOUND'`:\n- Auto-detection failure includes aggregated install instructions for all backends via `getInstallInstructions()`\n- Unknown backend name includes comma-separated list of known backend names from `registry.getAll()`\n- Unavailable backend includes specific `backend.getInstallInstructions()` and mentions the missing `backend.cliCommand`",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 766,
      "cacheReadTokens": 17367,
      "cacheCreationTokens": 10914,
      "latencyMs": 20195,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/ai/registry.ts",
          "sizeBytes": 6848
        }
      ]
    },
    {
      "timestamp": "2026-02-09T17:43:58.373Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/quality/phantom-paths/validator.ts\n\n```typescript\nimport { existsSync } from 'node:fs';\nimport * as path from 'node:path';\nimport type { PhantomPathInconsistency } from '../types.js';\n\n/**\n * Regex patterns matching path-like references in AGENTS.md content.\n *\n * Matches patterns like:\n * - `src/foo/bar.ts` (source file paths in backticks)\n * - `../foo/bar.js` (relative import paths in backticks)\n * - [text](./path) (markdown links)\n */\nconst PATH_PATTERNS: RegExp[] = [\n  // Markdown links: [text](./path) or [text](path)\n  /\\[(?:[^\\]]*)\\]\\((\\.[^)]+)\\)/g,\n  // Backtick-quoted paths: `src/foo/bar.ts` or `../foo/bar.js`\n  /`((?:src\\/|\\.\\.?\\/)[^`]+\\.[a-z]{1,4})`/g,\n  // Prose paths: \"from src/foo/\" or \"in src/foo/bar.ts\"\n  /(?:from|in|by|via|see)\\s+`?(src\\/[\\w\\-./]+)`?/gi,\n];\n\n/**\n * Paths/patterns to skip during validation (not actual file references).\n */\nconst SKIP_PATTERNS: RegExp[] = [\n  /node_modules/,\n  /\\.git\\//,\n  /^https?:/,\n  /\\{\\{/,          // template placeholders\n  /\\$\\{/,          // template literals\n  /\\*/,            // glob patterns\n  /\\{[^}]*,[^}]*\\}/,  // brace expansion: {a,b,c}\n];\n\n/**\n * Check an AGENTS.md file for phantom path references.\n *\n * Extracts all path-like strings from the document, resolves them\n * relative to the AGENTS.md file location, and verifies they exist.\n *\n * @param agentsMdPath - Absolute path to the AGENTS.md file\n * @param content - Content of the AGENTS.md file\n * @param projectRoot - Project root for resolving src/ paths\n * @returns Array of phantom path inconsistencies\n */\nexport function checkPhantomPaths(\n  agentsMdPath: string,\n  content: string,\n  projectRoot: string,\n): PhantomPathInconsistency[] {\n  const issues: PhantomPathInconsistency[] = [];\n  const agentsMdDir = path.dirname(agentsMdPath);\n  const seen = new Set<string>();\n\n  for (const pattern of PATH_PATTERNS) {\n    pattern.lastIndex = 0;\n    let match;\n\n    while ((match = pattern.exec(content)) !== null) {\n      const rawPath = match[1];\n      if (!rawPath || seen.has(rawPath)) continue;\n      if (SKIP_PATTERNS.some((p) => p.test(rawPath))) continue;\n\n      seen.add(rawPath);\n\n      // Try resolving relative to AGENTS.md location\n      const fromAgentsMd = path.resolve(agentsMdDir, rawPath);\n      // Try resolving relative to project root (for src/ paths)\n      const fromRoot = path.resolve(projectRoot, rawPath);\n\n      // Strip .js extension and try .ts (TypeScript import convention)\n      const tryPaths = [fromAgentsMd, fromRoot];\n      if (rawPath.endsWith('.js')) {\n        tryPaths.push(fromAgentsMd.replace(/\\.js$/, '.ts'));\n        tryPaths.push(fromRoot.replace(/\\.js$/, '.ts'));\n      }\n\n      const exists = tryPaths.some((p) => existsSync(p));\n\n      if (!exists) {\n        // Find the line containing this reference for context\n        const lines = content.split('\\n');\n        const contextLine = lines.find((l) => l.includes(rawPath)) ?? '';\n\n        issues.push({\n          type: 'phantom-path',\n          severity: 'warning',\n          agentsMdPath: path.relative(projectRoot, agentsMdPath),\n          description: `Phantom path reference: \"${rawPath}\" does not exist`,\n          details: {\n            referencedPath: rawPath,\n            resolvedTo: path.relative(projectRoot, fromAgentsMd),\n            context: contextLine.trim().slice(0, 120),\n          },\n        });\n      }\n    }\n  }\n\n  return issues;\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**validator.ts detects phantom path references in AGENTS.md files by extracting path-like strings via regex patterns, resolving them against filesystem locations, and reporting unresolved references as PhantomPathInconsistency objects.**\n\n## Exported Function\n\n`checkPhantomPaths(agentsMdPath: string, content: string, projectRoot: string): PhantomPathInconsistency[]` extracts path references from AGENTS.md content using `PATH_PATTERNS`, resolves each path against multiple base directories, verifies existence via `existsSync()`, and returns array of inconsistency objects for non-existent paths.\n\n## Path Extraction Patterns\n\n`PATH_PATTERNS` contains three regex patterns for detecting file references:\n- `/\\[(?:[^\\]]*)\\]\\((\\.[^)]+)\\)/g` matches Markdown link targets like `[text](./path)` or `[text](path)`\n- `` /`((?:src\\/|\\.\\.?\\/)[^`]+\\.[a-z]{1,4})`/g `` matches backtick-quoted paths starting with `src/`, `./`, or `../` followed by 1-4 character file extension\n- `/(?:from|in|by|via|see)\\s+`?(src\\/[\\w\\-./]+)`?/gi` matches prose-embedded paths following contextual keywords with optional backticks\n\n## Path Resolution Strategy\n\nFor each extracted path, `checkPhantomPaths()` attempts resolution in priority order:\n1. `path.resolve(agentsMdDir, rawPath)` resolves relative to AGENTS.md directory\n2. `path.resolve(projectRoot, rawPath)` resolves relative to project root for `src/` paths\n3. If `rawPath.endsWith('.js')`, appends `.replace(/\\.js$/, '.ts')` variants to `tryPaths[]` to handle TypeScript import convention where `.ts` source imports use `.js` extension\n\nCalls `existsSync()` on all candidate paths; if none exist, creates `PhantomPathInconsistency` with `type: 'phantom-path'`, `severity: 'warning'`.\n\n## Exclusion Patterns\n\n`SKIP_PATTERNS` array filters out non-file references before validation:\n- `/node_modules/`, `/\\.git\\//` exclude dependency/VCS paths\n- `/^https?:/` excludes URLs\n- `/\\{\\{/`, `/\\$\\{/` exclude template placeholders and template literals\n- `/\\*/`, `/\\{[^}]*,[^}]*\\}/` exclude glob patterns and brace expansions\n\n## Inconsistency Reporting\n\nEach phantom path generates `PhantomPathInconsistency` object with:\n- `agentsMdPath` normalized via `path.relative(projectRoot, agentsMdPath)`\n- `description` formatted as `\"Phantom path reference: \\\"${rawPath}\\\" does not exist\"`\n- `details.referencedPath` contains original extracted path string\n- `details.resolvedTo` shows attempted resolution via `path.relative(projectRoot, fromAgentsMd)`\n- `details.context` includes containing line via `lines.find((l) => l.includes(rawPath))` truncated to 120 characters\n\n## Deduplication\n\n`seen` Set tracks `rawPath` strings to prevent duplicate validation of same reference appearing multiple times in content. Skips processing when `seen.has(rawPath)` returns true.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 806,
      "cacheReadTokens": 0,
      "cacheCreationTokens": 27584,
      "latencyMs": 28702,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/quality/phantom-paths/validator.ts",
          "sizeBytes": 3338
        }
      ]
    },
    {
      "timestamp": "2026-02-09T17:44:01.036Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/ai/index.ts\n\n```typescript\n/**\n * Public API for the AI service layer.\n *\n * This barrel export is the ONLY import point for the AI service layer.\n * No other module should reach into `src/ai/backends/` or `src/ai/telemetry/`\n * directly.\n *\n * @module\n *\n * @example\n * ```typescript\n * import {\n *   AIService,\n *   createBackendRegistry,\n *   resolveBackend,\n * } from './ai/index.js';\n *\n * const registry = createBackendRegistry();\n * const backend = await resolveBackend(registry, 'auto');\n * const service = new AIService(backend, {\n *   timeoutMs: 120_000,\n *   maxRetries: 3,\n *   telemetry: { keepRuns: 10 },\n * });\n *\n * const response = await service.call({ prompt: 'Hello' });\n * ```\n */\n\n// ---------------------------------------------------------------------------\n// Types and errors\n// ---------------------------------------------------------------------------\n\nexport type {\n  AIBackend,\n  AIResponse,\n  AICallOptions,\n  SubprocessResult,\n  RetryOptions,\n  TelemetryEntry,\n  RunLog,\n  FileRead,\n} from './types.js';\n\nexport { AIServiceError } from './types.js';\n\n// ---------------------------------------------------------------------------\n// Service orchestrator\n// ---------------------------------------------------------------------------\n\nexport { AIService } from './service.js';\nexport type { AIServiceOptions } from './service.js';\n\n// ---------------------------------------------------------------------------\n// Backend registry\n// ---------------------------------------------------------------------------\n\nexport {\n  BackendRegistry,\n  createBackendRegistry,\n  resolveBackend,\n  detectBackend,\n  getInstallInstructions,\n} from './registry.js';\n\n// ---------------------------------------------------------------------------\n// Retry utility\n// ---------------------------------------------------------------------------\n\nexport { withRetry, DEFAULT_RETRY_OPTIONS } from './retry.js';\n\n// ---------------------------------------------------------------------------\n// Subprocess wrapper\n// ---------------------------------------------------------------------------\n\nexport { runSubprocess } from './subprocess.js';\n\n// ---------------------------------------------------------------------------\n// Backend utilities\n// ---------------------------------------------------------------------------\n\nexport { isCommandOnPath } from './backends/claude.js';\n\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**Barrel export aggregating AI service layer public API: exports AIService orchestrator, BackendRegistry with resolveBackend/detectBackend/createBackendRegistry, withRetry retry utility, runSubprocess wrapper, isCommandOnPath utility, and all types from ./types.js (AIBackend, AIResponse, AICallOptions, SubprocessResult, RetryOptions, TelemetryEntry, RunLog, FileRead, AIServiceError) with AIServiceOptions from ./service.js.**\n\n## Exported Symbols\n\n**Types:**\n- `AIBackend` — Backend interface with `name`, `isAvailable()`, `call()` methods\n- `AIResponse` — Response shape with `content: string`, `tokensUsed`, `costUsd` metrics\n- `AICallOptions` — Call parameters with `prompt: string`, `fileContext`, `model`, `taskType`, `filePath`\n- `SubprocessResult` — Subprocess execution result with `stdout`, `stderr`, `exitCode`, `signal`, `error`\n- `RetryOptions` — Retry configuration with `maxAttempts: number`, `baseDelayMs: number`, `maxDelayMs: number`, `shouldRetry: (error) => boolean`\n- `TelemetryEntry` — Per-call telemetry with `timestamp`, `backend`, `model`, `tokensUsed`, `costUsd`, `durationMs`, `error`, `filesRead`\n- `RunLog` — Aggregated run metadata with `runId`, `startTime`, `endTime`, `totalInputTokens`, `totalOutputTokens`, `totalCacheReadTokens`, `totalCacheWriteTokens`, `totalCostUsd`, `totalDurationMs`, `errorCount`, `uniqueFilesRead`, `entries: TelemetryEntry[]`\n- `FileRead` — File read metadata with `path: string`, `sizeBytes: number`, `linesRead: number`\n- `AIServiceOptions` — Service constructor options from `./service.js`\n\n**Error Class:**\n- `AIServiceError` — Custom error class for AI service failures\n\n**Service Orchestrator:**\n- `AIService` — Main service class coordinating backend calls, retry logic, telemetry logging\n\n**Backend Registry:**\n- `BackendRegistry` — Registry class managing available backends\n- `createBackendRegistry()` — Factory creating registry with Claude/Gemini/OpenCode backends registered\n- `resolveBackend(registry: BackendRegistry, backendName: string): Promise<AIBackend>` — Resolves backend by name ('claude' | 'gemini' | 'opencode' | 'auto'), auto-detection tries backends in order until `isAvailable()` succeeds\n- `detectBackend(registry: BackendRegistry): Promise<AIBackend | null>` — Auto-detects first available backend\n- `getInstallInstructions(backendName: string): string` — Returns installation instructions for specified backend\n\n**Retry Utility:**\n- `withRetry<T>(fn: () => Promise<T>, options?: RetryOptions): Promise<T>` — Exponential backoff retry wrapper\n- `DEFAULT_RETRY_OPTIONS` — Default retry configuration constant\n\n**Subprocess Wrapper:**\n- `runSubprocess(options: {...}): Promise<SubprocessResult>` — Spawns subprocess with resource limits, timeout enforcement, process group killing\n\n**Backend Utilities:**\n- `isCommandOnPath(command: string): Promise<boolean>` — Checks if command exists in PATH via `which` (Unix) or `where` (Windows)\n\n## Architecture Role\n\nServes as the single entry point for AI service layer consumption by CLI commands and orchestration modules. Prevents direct imports from `src/ai/backends/` or `src/ai/telemetry/` by re-exporting all necessary symbols. Consumers import from `./ai/index.js` exclusively.\n\n## Module Isolation Boundary\n\nEnforces encapsulation of backend implementations (Claude/Gemini/OpenCode adapters in `./backends/`) and telemetry subsystem (`./telemetry/`) by exposing only high-level abstractions. Internal implementation details remain hidden behind `AIBackend` interface and `AIService` orchestrator.\n\n## Usage Pattern\n\n```typescript\nimport { AIService, createBackendRegistry, resolveBackend } from './ai/index.js';\n\nconst registry = createBackendRegistry();\nconst backend = await resolveBackend(registry, 'auto');\nconst service = new AIService(backend, {\n  timeoutMs: 120_000,\n  maxRetries: 3,\n  telemetry: { keepRuns: 10 },\n});\n\nconst response = await service.call({ prompt: 'Hello' });\n```",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 1095,
      "cacheReadTokens": 0,
      "cacheCreationTokens": 26977,
      "latencyMs": 28039,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/ai/index.ts",
          "sizeBytes": 2362
        }
      ]
    },
    {
      "timestamp": "2026-02-09T17:44:19.351Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/change-detection/types.ts\n\n```typescript\n/**\n * Types for git change detection\n */\n\n/**\n * Type of change detected for a file\n */\nexport type ChangeType = 'added' | 'modified' | 'deleted' | 'renamed';\n\n/**\n * A file change detected from git diff\n */\nexport interface FileChange {\n  /** Relative path to the file (new path for renames) */\n  path: string;\n  /** Type of change */\n  status: ChangeType;\n  /** Original path for renamed files */\n  oldPath?: string;\n}\n\n/**\n * Result of change detection\n */\nexport interface ChangeDetectionResult {\n  /** Current commit hash */\n  currentCommit: string;\n  /** Commit hash we're comparing from */\n  baseCommit: string;\n  /** List of changed files */\n  changes: FileChange[];\n  /** Whether uncommitted changes were included */\n  includesUncommitted: boolean;\n}\n\n/**\n * Options for change detection\n */\nexport interface ChangeDetectionOptions {\n  /** Include uncommitted (staged and working directory) changes */\n  includeUncommitted?: boolean;\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**Defines TypeScript interfaces and types for git-based change detection, representing file changes, change detection results, and configuration options.**\n\n## Exported Types\n\n**ChangeType** — Union type `'added' | 'modified' | 'deleted' | 'renamed'` representing possible git diff statuses for files.\n\n**FileChange** — Interface with `path: string` (relative path, new path for renames), `status: ChangeType`, optional `oldPath?: string` (original path for renamed files). Represents individual file change detected from git diff.\n\n**ChangeDetectionResult** — Interface aggregating `currentCommit: string`, `baseCommit: string`, `changes: FileChange[]`, `includesUncommitted: boolean`. Returned by change detection operations to describe what changed between commits.\n\n**ChangeDetectionOptions** — Configuration interface with optional `includeUncommitted?: boolean` flag controlling whether staged and working directory changes merge into detection results.\n\n## Integration Context\n\nUsed by `src/change-detection/detector.ts` which implements git diff parsing via `simple-git` library. Supports `git diff --name-status -M` with rename detection (50% similarity threshold). The `FileChange.oldPath` field populated when `ChangeType` is `'renamed'`. Consumed by `src/update/orchestrator.ts` for incremental documentation updates via SHA-256 hash comparison against `.sum` frontmatter `content_hash` fields.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 330,
      "cacheReadTokens": 17367,
      "cacheCreationTokens": 9365,
      "latencyMs": 15174,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/change-detection/types.ts",
          "sizeBytes": 946
        }
      ]
    },
    {
      "timestamp": "2026-02-09T17:44:14.053Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/change-detection/detector.ts\n\n```typescript\n/**\n * Git change detection and content hashing\n *\n * Uses simple-git for git operations and Node.js crypto for hashing.\n */\nimport { simpleGit } from 'simple-git';\nimport { createHash } from 'node:crypto';\nimport { readFile } from 'node:fs/promises';\nimport type { FileChange, ChangeDetectionResult, ChangeDetectionOptions } from './types.js';\n\n/**\n * Check if a path is inside a git repository.\n */\nexport async function isGitRepo(projectRoot: string): Promise<boolean> {\n  const git = simpleGit(projectRoot);\n  return git.checkIsRepo();\n}\n\n/**\n * Get the current HEAD commit hash.\n */\nexport async function getCurrentCommit(projectRoot: string): Promise<string> {\n  const git = simpleGit(projectRoot);\n  const hash = await git.revparse(['HEAD']);\n  return hash.trim();\n}\n\n/**\n * Detect files changed since a base commit.\n *\n * Uses git diff with --name-status and -M for rename detection.\n * Optionally includes uncommitted changes (staged + working directory).\n */\nexport async function getChangedFiles(\n  projectRoot: string,\n  baseCommit: string,\n  options: ChangeDetectionOptions = {}\n): Promise<ChangeDetectionResult> {\n  const git = simpleGit(projectRoot);\n  const currentCommit = await getCurrentCommit(projectRoot);\n  const changes: FileChange[] = [];\n\n  // Get committed changes from baseCommit to HEAD\n  const diff = await git.diff([\n    '--name-status',\n    '-M', // Detect renames (50% similarity threshold)\n    baseCommit,\n    'HEAD',\n  ]);\n\n  // Parse diff output\n  // Format: STATUS\\tFILE (or STATUS\\tOLD\\tNEW for renames)\n  const lines = diff.trim().split('\\n').filter(line => line.length > 0);\n\n  for (const line of lines) {\n    const parts = line.split('\\t');\n    if (parts.length < 2) continue;\n\n    const status = parts[0];\n    const filePath = parts[parts.length - 1]; // Last part is always the (new) path\n\n    if (status === 'A') {\n      changes.push({ path: filePath, status: 'added' });\n    } else if (status === 'M') {\n      changes.push({ path: filePath, status: 'modified' });\n    } else if (status === 'D') {\n      changes.push({ path: filePath, status: 'deleted' });\n    } else if (status.startsWith('R')) {\n      // Rename: R100 old new (R followed by similarity percentage)\n      const oldPath = parts[1];\n      changes.push({\n        path: filePath,\n        status: 'renamed',\n        oldPath,\n      });\n    }\n  }\n\n  // Optionally include uncommitted changes\n  if (options.includeUncommitted) {\n    const status = await git.status();\n\n    // Modified but not staged\n    for (const file of status.modified) {\n      if (!changes.some(c => c.path === file)) {\n        changes.push({ path: file, status: 'modified' });\n      }\n    }\n\n    // Staged for deletion\n    for (const file of status.deleted) {\n      if (!changes.some(c => c.path === file)) {\n        changes.push({ path: file, status: 'deleted' });\n      }\n    }\n\n    // Untracked files (new files not yet added)\n    for (const file of status.not_added) {\n      if (!changes.some(c => c.path === file)) {\n        changes.push({ path: file, status: 'added' });\n      }\n    }\n\n    // Staged files (new or modified)\n    for (const file of status.staged) {\n      if (!changes.some(c => c.path === file)) {\n        changes.push({ path: file, status: 'added' });\n      }\n    }\n  }\n\n  return {\n    currentCommit,\n    baseCommit,\n    changes,\n    includesUncommitted: options.includeUncommitted ?? false,\n  };\n}\n\n/**\n * Compute SHA-256 hash of a file's content.\n *\n * @param filePath - Absolute path to the file\n * @returns Hex-encoded SHA-256 hash\n */\nexport async function computeContentHash(filePath: string): Promise<string> {\n  const content = await readFile(filePath);\n  return createHash('sha256').update(content).digest('hex');\n}\n\n/**\n * Compute SHA-256 hash from an already-loaded string.\n *\n * Use this when the file content is already in memory to avoid\n * a redundant disk read.\n *\n * @param content - The file content as a string\n * @returns Hex-encoded SHA-256 hash\n */\nexport function computeContentHashFromString(content: string): string {\n  return createHash('sha256').update(content).digest('hex');\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**detector.ts provides git-based change detection via simple-git and SHA-256 content hashing for incremental documentation updates.**\n\n## Exported Functions\n\n**isGitRepo(projectRoot: string): Promise<boolean>**  \nCalls `simpleGit(projectRoot).checkIsRepo()` to verify git repository presence.\n\n**getCurrentCommit(projectRoot: string): Promise<string>**  \nExecutes `git.revparse(['HEAD'])` returning trimmed HEAD commit hash.\n\n**getChangedFiles(projectRoot: string, baseCommit: string, options?: ChangeDetectionOptions): Promise<ChangeDetectionResult>**  \nDetects file changes between `baseCommit` and `HEAD` via `git diff --name-status -M`, parsing status codes `A` (added), `M` (modified), `D` (deleted), `R` (renamed with 50% similarity threshold). When `options.includeUncommitted` is true, merges uncommitted changes via `git.status()` reading `status.modified`, `status.deleted`, `status.not_added`, `status.staged` arrays. Returns `ChangeDetectionResult` containing `currentCommit`, `baseCommit`, `changes: FileChange[]`, `includesUncommitted: boolean`.\n\n**computeContentHash(filePath: string): Promise<string>**  \nReads file via `readFile()`, computes SHA-256 hash via `createHash('sha256').update(content).digest('hex')`, returns hex-encoded hash string matching `.sum` YAML frontmatter `content_hash` field.\n\n**computeContentHashFromString(content: string): string**  \nSynchronous variant computing SHA-256 from in-memory string content, avoiding redundant disk reads when file content already loaded.\n\n## Change Detection Algorithm\n\nParses `git diff` output with format `STATUS\\tFILE` or `STATUS\\tOLD\\tNEW` for renames. Status codes map to `FileChange.status`: `'A'` → `'added'`, `'M'` → `'modified'`, `'D'` → `'deleted'`, `'R*'` → `'renamed'` with `oldPath` extracted from second tab-delimited field. Rename detection uses `-M` flag with implicit 50% similarity threshold.\n\n## Uncommitted Change Merge Strategy\n\nWhen `includeUncommitted` enabled, calls `git.status()` and deduplicates against committed changes via `changes.some(c => c.path === file)` predicate. Adds `status.modified` as `'modified'`, `status.deleted` as `'deleted'`, `status.not_added` as `'added'`, `status.staged` as `'added'`. Prevents duplicate `FileChange` entries for files appearing in both committed diff and working tree status.\n\n## Integration Points\n\nConsumed by `src/update/orchestrator.ts` to compute `filesToAnalyze` by comparing `computeContentHash()` results against `.sum` frontmatter `content_hash` values. Supports non-git workflows via `computeContentHash()` fallback when `isGitRepo()` returns false.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 738,
      "cacheReadTokens": 0,
      "cacheCreationTokens": 27732,
      "latencyMs": 21678,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/change-detection/detector.ts",
          "sizeBytes": 4102
        }
      ]
    },
    {
      "timestamp": "2026-02-09T17:44:17.087Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/change-detection/index.ts\n\n```typescript\n/**\n * Change detection module\n *\n * Provides git-based change detection for incremental updates.\n * Detects added, modified, deleted, and renamed files.\n */\nexport {\n  isGitRepo,\n  getCurrentCommit,\n  getChangedFiles,\n  computeContentHash,\n  computeContentHashFromString,\n} from './detector.js';\n\nexport type {\n  ChangeType,\n  FileChange,\n  ChangeDetectionResult,\n  ChangeDetectionOptions,\n} from './types.js';\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**Barrel module exporting git-based change detection functions and types for incremental documentation updates with SHA-256 content hashing.**\n\n## Exported Functions\n\nRe-exports five functions from `./detector.js`:\n\n- `isGitRepo(): Promise<boolean>` — Detects whether current directory is inside a git repository\n- `getCurrentCommit(): Promise<string | null>` — Returns current HEAD commit SHA or null if not in git repo\n- `getChangedFiles(options: ChangeDetectionOptions): Promise<ChangeDetectionResult>` — Detects added/modified/deleted/renamed files via git diff with optional uncommitted changes merge\n- `computeContentHash(filePath: string): Promise<string>` — Computes SHA-256 hash of file content for incremental update comparison\n- `computeContentHashFromString(content: string): string` — Computes SHA-256 hash of string content directly\n\n## Exported Types\n\nRe-exports four types from `./types.js`:\n\n- `ChangeType` — Discriminated union literal: `'added' | 'modified' | 'deleted' | 'renamed'`\n- `FileChange` — Object containing `{ path: string, status: ChangeType, oldPath?: string }` where oldPath populated only for `status: 'renamed'`\n- `ChangeDetectionResult` — Object containing `{ changes: FileChange[], baseCommit: string | null }` where baseCommit is source commit SHA\n- `ChangeDetectionOptions` — Configuration object controlling `{ baseCommit?: string, includeUncommitted?: boolean }` for `getChangedFiles()`\n\n## Integration Points\n\nConsumed by `src/update/orchestrator.ts` which calls `getChangedFiles()` to compute `filesToAnalyze`, `filesToSkip`, and orphaned `.sum` files requiring cleanup. The `computeContentHash()` function compares against `content_hash` from YAML frontmatter in existing `.sum` files to determine modification status.\n\n## Module Organization\n\nPure barrel module providing single import path `src/change-detection` for consumers. Implementation logic resides in `detector.ts` (git operations, SHA-256 hashing) and `types.ts` (TypeScript interfaces). No runtime logic executed in this index file.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 527,
      "cacheReadTokens": 17367,
      "cacheCreationTokens": 9226,
      "latencyMs": 19538,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/change-detection/index.ts",
          "sizeBytes": 412
        }
      ]
    },
    {
      "timestamp": "2026-02-09T17:44:12.619Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/ai/subprocess.ts\n\n```typescript\n/**\n * Low-level subprocess wrapper for AI CLI invocations.\n *\n * This is the ONLY place in the codebase that spawns AI CLI processes.\n * Centralizes timeout enforcement, stdin piping, zombie prevention,\n * SIGKILL escalation, and exit code extraction.\n *\n * @module\n */\n\nimport { execFile } from 'node:child_process';\nimport type { ExecFileException } from 'node:child_process';\nimport type { SubprocessResult } from './types.js';\n\n/** Grace period after SIGTERM before escalating to SIGKILL (ms) */\nconst SIGKILL_GRACE_MS = 5_000;\n\n/**\n * Track active subprocesses for debugging concurrency.\n * Maps PID to spawn timestamp and command.\n */\nconst activeSubprocesses = new Map<number, { command: string; spawnedAt: number }>();\n\n/**\n * Get current count of active subprocesses.\n */\nexport function getActiveSubprocessCount(): number {\n  return activeSubprocesses.size;\n}\n\n/**\n * Get details of all active subprocesses.\n */\nexport function getActiveSubprocesses(): Array<{ pid: number; command: string; spawnedAt: number; runningMs: number }> {\n  const now = Date.now();\n  return Array.from(activeSubprocesses.entries()).map(([pid, info]) => ({\n    pid,\n    command: info.command,\n    spawnedAt: info.spawnedAt,\n    runningMs: now - info.spawnedAt,\n  }));\n}\n\n/**\n * Options for subprocess execution.\n */\nexport interface SubprocessOptions {\n  /** Maximum time in milliseconds before the process is killed */\n  timeoutMs: number;\n  /** Optional stdin input to pipe to the process */\n  input?: string;\n  /**\n   * Callback fired synchronously when the child process is spawned.\n   * Use this for trace events that need the actual spawn time.\n   */\n  onSpawn?: (pid: number | undefined) => void;\n}\n\n/**\n * Spawn a CLI subprocess with timeout enforcement and stdin piping.\n *\n * Always resolves -- never rejects. Errors are captured in the returned\n * {@link SubprocessResult} fields (`exitCode`, `timedOut`, `stderr`) so\n * that callers can decide how to handle failures.\n *\n * When the subprocess exceeds its timeout, `execFile` sends SIGTERM.\n * If the process doesn't exit within {@link SIGKILL_GRACE_MS} after\n * SIGTERM, we escalate to SIGKILL to prevent hung processes from\n * lingering indefinitely.\n *\n * @param command - The CLI executable to run (e.g., \"claude\", \"gemini\")\n * @param args - Argument array passed to the executable\n * @param options - Timeout, optional stdin input, and spawn callback\n * @returns Resolved result with stdout, stderr, exit code, timing, and timeout flag\n *\n * @example\n * ```typescript\n * import { runSubprocess } from './subprocess.js';\n *\n * const result = await runSubprocess('claude', ['-p', '--output-format', 'json'], {\n *   timeoutMs: 120_000,\n *   input: 'Summarize this codebase',\n *   onSpawn: (pid) => console.log(`Spawned PID ${pid}`),\n * });\n *\n * if (result.timedOut) {\n *   console.error('CLI timed out after 120s');\n * } else if (result.exitCode !== 0) {\n *   console.error('CLI failed:', result.stderr);\n * } else {\n *   const response = JSON.parse(result.stdout);\n * }\n * ```\n */\nexport function runSubprocess(\n  command: string,\n  args: string[],\n  options: SubprocessOptions,\n): Promise<SubprocessResult> {\n  return new Promise((resolve) => {\n    const startTime = Date.now();\n    let sigkillTimer: ReturnType<typeof setTimeout> | undefined;\n\n    // const activeCount = activeSubprocesses.size;\n    // console.error(`[subprocess] Currently active: ${activeCount} subprocess(es)`);\n    // console.error(`[subprocess] Spawning: ${command} ${args.join(' ')}`);\n\n    const child = execFile(\n      command,\n      args,\n      {\n        timeout: options.timeoutMs,\n        killSignal: 'SIGTERM',\n        maxBuffer: 10 * 1024 * 1024, // 10MB for large AI responses\n        encoding: 'utf-8',\n        env: {\n          ...process.env,\n        },\n      },\n      (error: ExecFileException | null, stdout: string, stderr: string) => {\n        const durationMs = Date.now() - startTime;\n        // console.error(`[subprocess:${child.pid}] Callback fired after ${durationMs}ms`);\n\n        // Clear SIGKILL escalation timer -- process has exited\n        if (sigkillTimer !== undefined) {\n          // console.error(`[subprocess:${child.pid}] Clearing SIGKILL timer`);\n          clearTimeout(sigkillTimer);\n        }\n\n        // Ensure the process is fully terminated (no-op if already dead)\n        // This handles edge cases where child processes might still be running\n        // console.error(`[subprocess:${child.pid}] Attempting explicit SIGKILL cleanup`);\n        try {\n          if (child.pid !== undefined) {\n            // Kill the entire process tree by sending signal to process group\n            // Use negative PID to target the process group\n            try {\n              process.kill(-child.pid, 'SIGKILL');\n              // console.error(`[subprocess:${child.pid}] Sent SIGKILL to process group -${child.pid}`);\n            } catch (pgError) {\n              // Process group kill failed, try single process\n              process.kill(child.pid, 'SIGKILL');\n              // console.error(`[subprocess:${child.pid}] Sent SIGKILL to single process (pg kill failed)`);\n            }\n          }\n        } catch (killError) {\n          // Process is already dead or doesn't exist -- expected in most cases\n          // console.error(`[subprocess:${child.pid}] SIGKILL failed (process already dead): ${killError instanceof Error ? killError.message : String(killError)}`);\n        }\n\n        // Detect timeout: execFile sets `killed = true` when the process\n        // is terminated due to exceeding the timeout option.\n        const timedOut = error !== null && 'killed' in error && error.killed === true;\n\n        // Extract exit code from the error or child process.\n        // execFile puts the exit code in error.code when the process exits\n        // with a non-zero code, but error.code can also be a string like\n        // 'ERR_CHILD_PROCESS_STDIO_MAXBUFFER'. Fall back to child.exitCode,\n        // then default to 1 for unknown failures and 0 for no error.\n        let exitCode: number;\n        if (error === null) {\n          exitCode = 0;\n        } else if (typeof error.code === 'number') {\n          exitCode = error.code;\n        } else if (child.exitCode !== null) {\n          exitCode = child.exitCode;\n        } else {\n          exitCode = 1;\n        }\n\n        // console.error(`[subprocess:${child.pid}] Exited: code=${exitCode}, timedOut=${timedOut}, signal=${error !== null && 'signal' in error ? error.signal : null}`);\n\n        // Remove from active tracking\n        if (child.pid !== undefined) {\n          activeSubprocesses.delete(child.pid);\n          // console.error(`[subprocess:${child.pid}] Removed from active list (remaining: ${activeSubprocesses.size})`);\n        }\n\n        resolve({\n          stdout: stdout ?? '',\n          stderr: stderr ?? '',\n          exitCode,\n          signal: (error !== null && 'signal' in error ? error.signal as string : null) ?? null,\n          durationMs,\n          timedOut,\n          childPid: child.pid,\n        });\n      },\n    );\n\n    // console.error(`[subprocess:${child.pid}] Spawned with PID ${child.pid}`);\n\n    // Track this subprocess\n    if (child.pid !== undefined) {\n      activeSubprocesses.set(child.pid, {\n        command: `${command} ${args.join(' ')}`,\n        spawnedAt: startTime,\n      });\n      // console.error(`[subprocess:${child.pid}] Registered in active list (total: ${activeSubprocesses.size})`);\n    }\n\n    // Notify caller of spawn (for trace events at actual spawn time)\n    options.onSpawn?.(child.pid);\n    // console.error(`[subprocess:${child.pid}] onSpawn callback invoked`);\n\n    // SIGKILL escalation: if the process doesn't exit within\n    // timeout + grace period, force-kill it. This handles cases where\n    // SIGTERM is caught/ignored by the child or its process tree.\n    if (child.pid !== undefined) {\n      const killDelay = options.timeoutMs + SIGKILL_GRACE_MS;\n      // console.error(`[subprocess:${child.pid}] Setting SIGKILL timer for ${killDelay}ms (timeout=${options.timeoutMs}ms + grace=${SIGKILL_GRACE_MS}ms)`);\n\n      sigkillTimer = setTimeout(() => {\n        // console.error(`[subprocess:${child.pid}] SIGKILL timer fired - process exceeded timeout+grace`);\n        try {\n          child.kill('SIGKILL');\n          // console.error(`[subprocess:${child.pid}] Sent escalation SIGKILL`);\n        } catch (killError) {\n          // Process may already be dead -- ignore\n          // console.error(`[subprocess:${child.pid}] Escalation SIGKILL failed: ${killError instanceof Error ? killError.message : String(killError)}`);\n        }\n      }, killDelay);\n\n      // Don't let this timer keep the event loop alive\n      sigkillTimer.unref();\n      // console.error(`[subprocess:${child.pid}] SIGKILL timer unref'd`);\n    }\n\n    // Write prompt to stdin if provided, then close the stream.\n    // IMPORTANT: Always call .end() -- the child process blocks waiting\n    // for EOF on stdin otherwise (see RESEARCH.md Pitfall 1).\n    if (options.input !== undefined && child.stdin !== null) {\n      const inputSize = Buffer.byteLength(options.input, 'utf-8');\n      // console.error(`[subprocess:${child.pid}] Writing ${inputSize} bytes to stdin`);\n      child.stdin.write(options.input);\n      child.stdin.end();\n      // console.error(`[subprocess:${child.pid}] Stdin closed`);\n    } else {\n      // console.error(`[subprocess:${child.pid}] No stdin input provided`);\n    }\n  });\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**subprocess.ts spawns and manages AI CLI child processes with timeout enforcement, SIGKILL escalation, stdin piping, and active subprocess tracking.**\n\n## Exported Functions\n\n**runSubprocess(command: string, args: string[], options: SubprocessOptions): Promise<SubprocessResult>** — Spawns AI CLI subprocess via `execFile()`, pipes optional `input` to stdin, enforces timeout via SIGTERM at `options.timeoutMs`, escalates to SIGKILL after 5s grace period if process doesn't exit, tracks active subprocesses in module-scoped Map, invokes `options.onSpawn(pid)` callback synchronously after spawn, always resolves (never rejects) with `SubprocessResult` containing `stdout`, `stderr`, `exitCode`, `signal`, `durationMs`, `timedOut` boolean, and `childPid`.\n\n**getActiveSubprocessCount(): number** — Returns current count of active subprocesses from module-scoped `activeSubprocesses` Map.\n\n**getActiveSubprocesses(): Array<{ pid: number; command: string; spawnedAt: number; runningMs: number }>** — Returns array of all active subprocess details with computed `runningMs` elapsed time.\n\n## Exported Types\n\n**SubprocessOptions** interface with required `timeoutMs: number`, optional `input?: string` for stdin piping, optional `onSpawn?: (pid: number | undefined) => void` callback fired synchronously when child process spawns.\n\n## Process Lifecycle Management\n\n**Timeout enforcement** sends SIGTERM at `timeoutMs` via `execFile` timeout option with `killSignal: 'SIGTERM'`, then schedules unref'd SIGKILL timer at `timeoutMs + SIGKILL_GRACE_MS` (5000ms constant) to force-kill hung processes that ignore SIGTERM.\n\n**Process group killing** attempts `process.kill(-child.pid, 'SIGKILL')` with negative PID to target entire process tree, falls back to single-process `process.kill(child.pid, 'SIGKILL')` if process group signal fails, wraps in try-catch since process may already be dead.\n\n**Active subprocess tracking** maintains module-scoped `activeSubprocesses` Map storing `{ command: string, spawnedAt: number }` keyed by PID, adds entry after spawn, removes in `execFile` callback, enables concurrency debugging via `getActiveSubprocessCount()` and `getActiveSubprocesses()`.\n\n**Stdin handling** writes `options.input` to `child.stdin` via `.write()` then calls `.end()` to send EOF, preventing child process from blocking indefinitely waiting for stdin closure (see comment: \"RESEARCH.md Pitfall 1\").\n\n## Exit Code Extraction\n\n**Exit code precedence** checks `error === null` (returns 0), then `typeof error.code === 'number'` (returns `error.code`), then `child.exitCode !== null` (returns `child.exitCode`), else defaults to 1. Handles `execFile` placing numeric exit codes in `error.code` while also using string error codes like `'ERR_CHILD_PROCESS_STDIO_MAXBUFFER'`.\n\n**Timeout detection** sets `timedOut = true` when `error !== null && 'killed' in error && error.killed === true`, which `execFile` sets when process exceeds timeout option and is terminated.\n\n## Configuration\n\n**maxBuffer** set to `10 * 1024 * 1024` (10MB) to accommodate large AI responses without triggering `ERR_CHILD_PROCESS_STDIO_MAXBUFFER`.\n\n**SIGKILL_GRACE_MS** constant defines 5000ms grace period between SIGTERM and SIGKILL escalation.\n\n## Integration Points\n\nImported by `src/ai/service.ts` as sole subprocess spawning mechanism for AI CLI backends (Claude Code, Gemini CLI, OpenCode). Returns `SubprocessResult` consumed by `AIService.call()` for retry logic, telemetry logging, and trace emission.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 948,
      "cacheReadTokens": 17367,
      "cacheCreationTokens": 11866,
      "latencyMs": 27837,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/ai/subprocess.ts",
          "sizeBytes": 9456
        }
      ]
    },
    {
      "timestamp": "2026-02-09T17:44:12.994Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/ai/types.ts\n\n```typescript\n/**\n * Shared types for the AI service layer.\n *\n * Defines the contract for backends, responses, subprocess results,\n * retry configuration, and telemetry logging. Every AI service module\n * imports from this file.\n */\n\n// ---------------------------------------------------------------------------\n// Subprocess\n// ---------------------------------------------------------------------------\n\n/**\n * Result returned by the subprocess wrapper after a CLI process completes.\n *\n * Always populated -- even on error or timeout, the fields are filled\n * with whatever information was available.\n */\nexport interface SubprocessResult {\n  /** Standard output captured from the child process */\n  stdout: string;\n  /** Standard error captured from the child process */\n  stderr: string;\n  /** Numeric exit code (0 = success, non-zero = failure) */\n  exitCode: number;\n  /** Signal that terminated the process, or `null` if it exited normally */\n  signal: string | null;\n  /** Wall-clock duration in milliseconds */\n  durationMs: number;\n  /** Whether the process was killed because it exceeded its timeout */\n  timedOut: boolean;\n  /** OS PID of the child process (undefined if spawn failed) */\n  childPid?: number;\n}\n\n// ---------------------------------------------------------------------------\n// AI Call\n// ---------------------------------------------------------------------------\n\n/**\n * Input options for an AI call.\n *\n * Only `prompt` is required; all other fields have backend-specific defaults.\n */\nexport interface AICallOptions {\n  /** The prompt to send to the AI model */\n  prompt: string;\n  /** Optional system prompt to set context/behavior */\n  systemPrompt?: string;\n  /** Model identifier (e.g., \"sonnet\", \"opus\") -- backend interprets this */\n  model?: string;\n  /** Subprocess timeout in milliseconds (overrides config default) */\n  timeoutMs?: number;\n  /** Maximum number of agentic turns (backend-specific) */\n  maxTurns?: number;\n  /** Label for tracing (e.g., file path being processed) */\n  taskLabel?: string;\n}\n\n/**\n * Normalized response from any AI CLI backend.\n *\n * Every backend adapter must parse its CLI's raw output into this shape\n * so that callers never need to know which backend was used.\n */\nexport interface AIResponse {\n  /** The AI model's text response */\n  text: string;\n  /** Model identifier as reported by the backend */\n  model: string;\n  /** Number of input tokens consumed */\n  inputTokens: number;\n  /** Number of output tokens generated */\n  outputTokens: number;\n  /** Number of tokens served from cache reads */\n  cacheReadTokens: number;\n  /** Number of tokens written to cache */\n  cacheCreationTokens: number;\n  /** Wall-clock duration in milliseconds */\n  durationMs: number;\n  /** Process exit code from the CLI */\n  exitCode: number;\n  /** Original CLI JSON output for debugging */\n  raw: unknown;\n}\n\n// ---------------------------------------------------------------------------\n// Backend\n// ---------------------------------------------------------------------------\n\n/**\n * Contract for an AI CLI backend adapter.\n *\n * Each supported CLI (Claude, Gemini, OpenCode) implements this interface.\n * The registry selects the appropriate backend at runtime.\n *\n * @example\n * ```typescript\n * const backend: AIBackend = new ClaudeBackend();\n * if (await backend.isAvailable()) {\n *   const args = backend.buildArgs({ prompt: 'Hello' });\n *   // spawn the process with backend.cliCommand and args\n * }\n * ```\n */\nexport interface AIBackend {\n  /** Human-readable backend name (e.g., \"Claude\", \"Gemini\") */\n  readonly name: string;\n  /** CLI executable name on PATH (e.g., \"claude\", \"gemini\") */\n  readonly cliCommand: string;\n\n  /** Check whether this backend's CLI is available on PATH */\n  isAvailable(): Promise<boolean>;\n\n  /** Build the CLI argument array for a given call */\n  buildArgs(options: AICallOptions): string[];\n\n  /** Parse the CLI's stdout into a normalized {@link AIResponse} */\n  parseResponse(stdout: string, durationMs: number, exitCode: number): AIResponse;\n\n  /** Get user-facing install instructions when the CLI is not found */\n  getInstallInstructions(): string;\n}\n\n// ---------------------------------------------------------------------------\n// Retry\n// ---------------------------------------------------------------------------\n\n/**\n * Configuration for the retry utility.\n *\n * Controls exponential backoff timing and which errors are retryable.\n */\nexport interface RetryOptions {\n  /** Maximum number of retries (e.g., 3 means up to 4 total attempts) */\n  maxRetries: number;\n  /** Base delay in milliseconds before first retry */\n  baseDelayMs: number;\n  /** Maximum delay cap in milliseconds */\n  maxDelayMs: number;\n  /** Exponential multiplier applied to the base delay */\n  multiplier: number;\n  /** Predicate that returns `true` if the error is transient and retryable */\n  isRetryable: (error: unknown) => boolean;\n  /** Optional callback invoked before each retry attempt */\n  onRetry?: (attempt: number, error: unknown) => void;\n}\n\n// ---------------------------------------------------------------------------\n// Telemetry\n// ---------------------------------------------------------------------------\n\n/**\n * Record of a single file read that was sent as context to an AI call.\n */\nexport interface FileRead {\n  /** File path relative to project root */\n  path: string;\n  /** File size in bytes at time of read */\n  sizeBytes: number;\n}\n\n/**\n * Per-call telemetry log entry.\n *\n * Captures everything needed to replay or debug a single AI call\n * without re-running it.\n */\nexport interface TelemetryEntry {\n  /** ISO 8601 timestamp when the call was initiated */\n  timestamp: string;\n  /** The prompt that was sent */\n  prompt: string;\n  /** The system prompt, if one was used */\n  systemPrompt?: string;\n  /** The AI model's text response */\n  response: string;\n  /** Model identifier */\n  model: string;\n  /** Number of input tokens consumed */\n  inputTokens: number;\n  /** Number of output tokens generated */\n  outputTokens: number;\n  /** Number of tokens served from cache reads */\n  cacheReadTokens: number;\n  /** Number of tokens written to cache */\n  cacheCreationTokens: number;\n  /** Wall-clock latency in milliseconds */\n  latencyMs: number;\n  /** Process exit code */\n  exitCode: number;\n  /** Error message, if the call failed */\n  error?: string;\n  /** Number of retries that occurred before this result */\n  retryCount: number;\n  /** AI thinking/reasoning content. \"not supported\" when backend doesn't provide it */\n  thinking: string;\n  /** Files sent as context for this call */\n  filesRead: FileRead[];\n}\n\n/**\n * Per-run log file structure.\n *\n * Aggregates all {@link TelemetryEntry} instances for a single CLI run,\n * plus a computed summary for quick performance review.\n */\nexport interface RunLog {\n  /** Unique run identifier (ISO timestamp-based) */\n  runId: string;\n  /** ISO 8601 timestamp when the run started */\n  startTime: string;\n  /** ISO 8601 timestamp when the run finished */\n  endTime: string;\n  /** All individual call entries */\n  entries: TelemetryEntry[];\n  /** Aggregated summary across all entries */\n  summary: {\n    /** Total number of AI calls made */\n    totalCalls: number;\n    /** Sum of input tokens across all calls */\n    totalInputTokens: number;\n    /** Sum of output tokens across all calls */\n    totalOutputTokens: number;\n    /** Total wall-clock duration in milliseconds */\n    totalDurationMs: number;\n    /** Number of calls that resulted in an error */\n    errorCount: number;\n    /** Sum of cache read tokens across all calls */\n    totalCacheReadTokens: number;\n    /** Sum of cache creation tokens across all calls */\n    totalCacheCreationTokens: number;\n    /** Total file reads across all calls (including duplicates) */\n    totalFilesRead: number;\n    /** Unique files read (deduped by path) */\n    uniqueFilesRead: number;\n  };\n}\n\n// ---------------------------------------------------------------------------\n// Errors\n// ---------------------------------------------------------------------------\n\n/** Error codes for typed error handling in the AI service layer */\nexport type AIServiceErrorCode =\n  | 'CLI_NOT_FOUND'\n  | 'TIMEOUT'\n  | 'PARSE_ERROR'\n  | 'SUBPROCESS_ERROR'\n  | 'RATE_LIMIT';\n\n/**\n * Typed error for AI service failures.\n *\n * Carries a machine-readable {@link AIServiceErrorCode} so callers can\n * branch on the error type without parsing message strings.\n *\n * @example\n * ```typescript\n * try {\n *   await callAI(options);\n * } catch (error) {\n *   if (error instanceof AIServiceError && error.code === 'RATE_LIMIT') {\n *     // handle rate limiting\n *   }\n * }\n * ```\n */\nexport class AIServiceError extends Error {\n  /** Machine-readable error code */\n  readonly code: AIServiceErrorCode;\n\n  constructor(code: AIServiceErrorCode, message: string) {\n    super(message);\n    this.name = 'AIServiceError';\n    this.code = code;\n  }\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**Central type definitions for the AI service layer establishing contracts for backend adapters, subprocess execution, retry logic, and telemetry logging.**\n\n## Exported Types\n\n### Subprocess Execution\n\n**SubprocessResult** — Return value from subprocess wrapper after CLI process completion, always populated regardless of success/failure:\n- `stdout: string` — captured standard output\n- `stderr: string` — captured standard error  \n- `exitCode: number` — numeric exit code (0=success)\n- `signal: string | null` — termination signal or null for normal exit\n- `durationMs: number` — wall-clock duration in milliseconds\n- `timedOut: boolean` — whether process exceeded timeout threshold\n- `childPid?: number` — OS PID (undefined if spawn failed)\n\n### AI Call Interface\n\n**AICallOptions** — Input parameters for AI service calls:\n- `prompt: string` (required) — prompt text sent to model\n- `systemPrompt?: string` — optional system context/behavior instructions\n- `model?: string` — backend-specific model identifier (e.g., \"sonnet\", \"opus\")\n- `timeoutMs?: number` — subprocess timeout override in milliseconds\n- `maxTurns?: number` — maximum agentic turns (backend-dependent)\n- `taskLabel?: string` — tracing label (typically file path being processed)\n\n**AIResponse** — Normalized response shape that all backend adapters must produce:\n- `text: string` — AI model's generated text\n- `model: string` — model identifier as reported by backend\n- `inputTokens: number` — consumed input tokens\n- `outputTokens: number` — generated output tokens\n- `cacheReadTokens: number` — tokens served from cache reads\n- `cacheCreationTokens: number` — tokens written to cache\n- `durationMs: number` — wall-clock latency\n- `exitCode: number` — CLI process exit code\n- `raw: unknown` — original CLI JSON output for debugging\n\n### Backend Adapter Contract\n\n**AIBackend** interface — Contract implemented by each CLI adapter (Claude, Gemini, OpenCode):\n- `readonly name: string` — human-readable backend name\n- `readonly cliCommand: string` — executable name on PATH\n- `isAvailable(): Promise<boolean>` — checks CLI availability\n- `buildArgs(options: AICallOptions): string[]` — constructs CLI argument array\n- `parseResponse(stdout: string, durationMs: number, exitCode: number): AIResponse` — parses CLI stdout into normalized AIResponse\n- `getInstallInstructions(): string` — returns user-facing install guidance when CLI not found\n\n### Retry Configuration\n\n**RetryOptions** — Exponential backoff retry configuration:\n- `maxRetries: number` — maximum retry attempts (e.g., 3 = 4 total attempts)\n- `baseDelayMs: number` — initial delay before first retry\n- `maxDelayMs: number` — delay ceiling cap\n- `multiplier: number` — exponential backoff multiplier\n- `isRetryable: (error: unknown) => boolean` — predicate for transient error detection\n- `onRetry?: (attempt: number, error: unknown) => void` — optional pre-retry callback\n\n### Telemetry Logging\n\n**FileRead** — Record of a single file read operation sent as context:\n- `path: string` — file path relative to project root\n- `sizeBytes: number` — file size in bytes at read time\n\n**TelemetryEntry** — Per-call log entry capturing complete call metadata:\n- `timestamp: string` — ISO 8601 call initiation time\n- `prompt: string` — sent prompt text\n- `systemPrompt?: string` — optional system prompt\n- `response: string` — AI model's text response\n- `model: string` — model identifier\n- `inputTokens: number` — consumed input tokens\n- `outputTokens: number` — generated output tokens\n- `cacheReadTokens: number` — cache read tokens\n- `cacheCreationTokens: number` — cache write tokens\n- `latencyMs: number` — wall-clock latency\n- `exitCode: number` — process exit code\n- `error?: string` — error message if call failed\n- `retryCount: number` — number of retries before result\n- `thinking: string` — AI reasoning content (\"not supported\" when backend doesn't provide)\n- `filesRead: FileRead[]` — context files sent with call\n\n**RunLog** — Per-run log file structure aggregating all TelemetryEntry instances:\n- `runId: string` — unique run identifier (ISO timestamp-based)\n- `startTime: string` — ISO 8601 run start time\n- `endTime: string` — ISO 8601 run end time\n- `entries: TelemetryEntry[]` — all individual call entries\n- `summary` object with aggregated metrics:\n  - `totalCalls: number` — count of AI calls\n  - `totalInputTokens: number` — sum of input tokens\n  - `totalOutputTokens: number` — sum of output tokens\n  - `totalDurationMs: number` — total wall-clock duration\n  - `errorCount: number` — count of failed calls\n  - `totalCacheReadTokens: number` — sum of cache reads\n  - `totalCacheCreationTokens: number` — sum of cache writes\n  - `totalFilesRead: number` — total file reads (including duplicates)\n  - `uniqueFilesRead: number` — deduplicated file read count\n\n### Error Handling\n\n**AIServiceErrorCode** type — Machine-readable error code discriminator:\n- `'CLI_NOT_FOUND'` — CLI executable not found on PATH\n- `'TIMEOUT'` — subprocess exceeded timeout threshold\n- `'PARSE_ERROR'` — failed to parse CLI JSON output\n- `'SUBPROCESS_ERROR'` — generic subprocess execution failure\n- `'RATE_LIMIT'` — backend rate limiting detected\n\n**AIServiceError** class — Typed error extending Error with structured error code:\n- `readonly code: AIServiceErrorCode` — machine-readable error type\n- `constructor(code: AIServiceErrorCode, message: string)` — sets name to 'AIServiceError' and stores code\n\n## Design Patterns\n\n**Backend Adapter Pattern** — AIBackend interface enables polymorphic CLI invocation across Claude/Gemini/OpenCode without callers knowing which backend executes. Registry selects backend at runtime via `isAvailable()` detection.\n\n**Normalized Response Contract** — Every backend's `parseResponse()` must transform CLI-specific output into uniform AIResponse shape, abstracting away stdout parsing differences (Claude JSON vs. Gemini JSONL vs. OpenCode format).\n\n**Typed Error Discrimination** — AIServiceError.code enables structured error handling via `error instanceof AIServiceError && error.code === 'RATE_LIMIT'` pattern instead of string parsing.\n\n## Integration Points\n\n**Consumed by:**\n- `src/ai/service.ts` — AIService class uses AIBackend interface for backend selection and AICallOptions for call construction\n- `src/ai/registry.ts` — Backend detection and selection logic references AIBackend interface\n- `src/ai/retry.ts` — Retry utility consumes RetryOptions for exponential backoff\n- `src/ai/telemetry/logger.ts` — TelemetryLogger writes TelemetryEntry instances to run logs\n- `src/ai/telemetry/run-log.ts` — Constructs RunLog structure from entry accumulation\n- `src/ai/backends/*.ts` — Claude/Gemini/OpenCode adapters implement AIBackend interface\n\n**Referenced by:**\n- `src/orchestration/pool.ts` — Worker pool consumes AIResponse for progress reporting\n- `src/generation/executor.ts` — Phase executor handles AIServiceError codes for failure modes",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 1836,
      "cacheReadTokens": 17367,
      "cacheCreationTokens": 11411,
      "latencyMs": 31262,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/ai/types.ts",
          "sizeBytes": 8930
        }
      ]
    },
    {
      "timestamp": "2026-02-09T17:44:19.454Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/cli/clean.ts\n\n```typescript\n/**\n * `are clean` command - Delete all generated documentation artifacts\n *\n * Removes .sum files, AGENTS.md files, root documents (CLAUDE.md),\n * and the GENERATION-PLAN.md file.\n */\n\nimport path from 'node:path';\nimport { access, readFile, rename, unlink } from 'node:fs/promises';\nimport { constants } from 'node:fs';\nimport fg from 'fast-glob';\nimport pc from 'picocolors';\nimport { createLogger } from '../output/logger.js';\nimport { GENERATED_MARKER } from '../generation/writers/agents-md.js';\n\n/**\n * Options for the clean command.\n */\nexport interface CleanOptions {\n  /**\n   * Show files that would be deleted without deleting them.\n   * @default false\n   */\n  dryRun: boolean;\n}\n\n/**\n * Execute the `are clean` command.\n *\n * Finds and deletes all generated documentation artifacts:\n * - `*.sum` files\n * - `AGENTS.md` files\n * - `CLAUDE.md` at project root\n * - `.agents-reverse-engineer/GENERATION-PLAN.md`\n *\n * @param targetPath - Project root directory (defaults to current working directory)\n * @param options - Command options\n */\nexport async function cleanCommand(\n  targetPath: string,\n  options: CleanOptions\n): Promise<void> {\n  const resolvedPath = path.resolve(targetPath || process.cwd());\n\n  const logger = createLogger({ colors: true });\n\n  // Verify target path exists\n  try {\n    await access(resolvedPath, constants.R_OK);\n  } catch (err) {\n    const error = err as NodeJS.ErrnoException;\n    if (error.code === 'ENOENT') {\n      logger.error(`Directory not found: ${resolvedPath}`);\n      process.exit(1);\n    }\n    if (error.code === 'EACCES' || error.code === 'EPERM') {\n      logger.error(`Permission denied: ${resolvedPath}`);\n      process.exit(1);\n    }\n    throw error;\n  }\n\n  // Find all artifacts\n  const [sumFiles, annexFiles, agentsFiles, localAgentsFiles] = await Promise.all([\n    fg.glob('**/*.sum', {\n      cwd: resolvedPath,\n      absolute: true,\n      onlyFiles: true,\n      dot: true,\n      ignore: ['**/node_modules/**', '**/.git/**'],\n    }),\n    fg.glob('**/*.annex.md', {\n      cwd: resolvedPath,\n      absolute: true,\n      onlyFiles: true,\n      dot: true,\n      ignore: ['**/node_modules/**', '**/.git/**'],\n    }),\n    fg.glob('**/AGENTS.md', {\n      cwd: resolvedPath,\n      absolute: true,\n      onlyFiles: true,\n      dot: true,\n      ignore: ['**/node_modules/**', '**/.git/**'],\n    }),\n    fg.glob('**/AGENTS.local.md', {\n      cwd: resolvedPath,\n      absolute: true,\n      onlyFiles: true,\n      dot: true,\n      ignore: ['**/node_modules/**', '**/.git/**'],\n    }),\n  ]);\n\n  // Filter AGENTS.md to only those generated by ARE (contain the marker).\n  // User-authored AGENTS.md files (e.g. SDK docs) must not be deleted.\n  const generatedAgentsFiles: string[] = [];\n  const skippedAgentsFiles: string[] = [];\n  for (const file of agentsFiles) {\n    try {\n      const content = await readFile(file, 'utf-8');\n      if (content.includes(GENERATED_MARKER)) {\n        generatedAgentsFiles.push(file);\n      } else {\n        skippedAgentsFiles.push(file);\n      }\n    } catch {\n      // Can't read — skip silently\n    }\n  }\n\n  // Check for root docs and plan file\n  const singleFiles: string[] = [];\n  const claudeMd = path.join(resolvedPath, 'CLAUDE.md');\n  const planFile = path.join(resolvedPath, '.agents-reverse-engineer', 'GENERATION-PLAN.md');\n\n  for (const filePath of [claudeMd, planFile]) {\n    try {\n      await access(filePath, constants.F_OK);\n      singleFiles.push(filePath);\n    } catch {\n      // File doesn't exist - skip\n    }\n  }\n\n  const allFiles = [...sumFiles, ...annexFiles, ...generatedAgentsFiles, ...singleFiles];\n\n  if (allFiles.length === 0 && localAgentsFiles.length === 0) {\n    logger.info('No generated artifacts found.');\n    return;\n  }\n\n  // Display found files\n  const relativePath = (absPath: string): string =>\n    path.relative(resolvedPath, absPath);\n\n  if (options.dryRun) {\n    logger.info('Files that would be deleted:');\n  }\n\n  for (const file of allFiles) {\n    logger.info(`  ${relativePath(file)}`);\n  }\n\n  if (skippedAgentsFiles.length > 0) {\n    logger.info('');\n    logger.info('Preserving user-authored AGENTS.md:');\n    for (const file of skippedAgentsFiles) {\n      logger.info(`  ${relativePath(file)}`);\n    }\n  }\n\n  if (localAgentsFiles.length > 0) {\n    logger.info('');\n    logger.info(options.dryRun ? 'Files that would be restored:' : 'Restoring user-defined files:');\n    for (const file of localAgentsFiles) {\n      const target = path.join(path.dirname(file), 'AGENTS.md');\n      logger.info(`  ${relativePath(file)} → ${relativePath(target)}`);\n    }\n  }\n\n  logger.info('');\n  logger.info(\n    `${pc.bold(String(sumFiles.length))} .sum file(s), ` +\n    `${pc.bold(String(annexFiles.length))} .annex.md file(s), ` +\n    `${pc.bold(String(generatedAgentsFiles.length))} AGENTS.md file(s), ` +\n    `${pc.bold(String(singleFiles.length))} root doc(s), ` +\n    `${pc.bold(String(localAgentsFiles.length))} AGENTS.local.md to restore`\n  );\n\n  if (options.dryRun) {\n    logger.info('');\n    logger.info(pc.yellow('Dry run — no files were changed.'));\n    return;\n  }\n\n  // Delete all generated files\n  let deleted = 0;\n  for (const file of allFiles) {\n    try {\n      await unlink(file);\n      deleted++;\n    } catch (err) {\n      logger.error(`Failed to delete ${relativePath(file)}: ${(err as Error).message}`);\n    }\n  }\n\n  // Restore AGENTS.local.md → AGENTS.md (undo the rename from generation)\n  let restored = 0;\n  for (const localFile of localAgentsFiles) {\n    const agentsPath = path.join(path.dirname(localFile), 'AGENTS.md');\n    try {\n      await rename(localFile, agentsPath);\n      restored++;\n    } catch (err) {\n      logger.error(`Failed to restore ${relativePath(localFile)}: ${(err as Error).message}`);\n    }\n  }\n\n  logger.info('');\n  const parts = [`Deleted ${deleted} file(s)`];\n  if (restored > 0) {\n    parts.push(`restored ${restored} AGENTS.local.md file(s)`);\n  }\n  logger.info(pc.green(`${parts.join(', ')}.`));\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**`src/cli/clean.ts` implements the `are clean` command to delete all generated documentation artifacts (.sum files, AGENTS.md, CLAUDE.md, GENERATION-PLAN.md) while preserving user-authored AGENTS.md and restoring AGENTS.local.md files.**\n\n## Exported Interface\n\n### CleanOptions\n```typescript\ninterface CleanOptions {\n  dryRun: boolean;  // Preview deletions without executing\n}\n```\n\n### cleanCommand\n```typescript\nasync function cleanCommand(\n  targetPath: string,\n  options: CleanOptions\n): Promise<void>\n```\n\nOrchestrates deletion workflow: validates target path exists with read permissions, discovers artifacts via `fast-glob`, filters generated vs. user-authored AGENTS.md via `GENERATED_MARKER` substring search, displays deletion preview, deletes files via `unlink()`, restores AGENTS.local.md → AGENTS.md via `rename()`.\n\n## Artifact Discovery Strategy\n\nUses parallel `fg.glob()` calls with shared ignore patterns `['**/node_modules/**', '**/.git/**']` to find:\n- `**/*.sum` — per-file summaries\n- `**/*.annex.md` — reproduction-critical constant annexes\n- `**/AGENTS.md` — directory-level aggregations (requires marker filtering)\n- `**/AGENTS.local.md` — user-authored files renamed during generation\n\nSingle-file existence checks via `access()` for:\n- `CLAUDE.md` at project root\n- `.agents-reverse-engineer/GENERATION-PLAN.md`\n\n## Generated vs. User-Authored AGENTS.md Detection\n\nReads each discovered AGENTS.md file, checks for `GENERATED_MARKER` constant (`<!-- Generated by agents-reverse-engineer -->`) via `content.includes()`. Files without marker added to `skippedAgentsFiles[]` and excluded from deletion. Prevents accidental removal of SDK documentation or manually maintained AGENTS.md files.\n\n## Deletion Workflow\n\n**Dry-run mode (`options.dryRun: true`):**\n- Displays all files that would be deleted/restored\n- Logs preservation of user-authored AGENTS.md\n- Exits without filesystem modifications\n- Emits yellow warning: `'Dry run — no files were changed.'`\n\n**Execution mode:**\n- Iterates `allFiles = [...sumFiles, ...annexFiles, ...generatedAgentsFiles, ...singleFiles]`\n- Deletes each via `unlink()`, increments `deleted` counter, logs errors without aborting\n- Iterates `localAgentsFiles`, restores via `rename(localFile, 'AGENTS.md')`, increments `restored` counter\n- Emits green summary: `'Deleted N file(s), restored M AGENTS.local.md file(s).'`\n\n## Error Handling\n\n**Path validation:**\n- `ENOENT` → logs `'Directory not found'`, exits with code 1\n- `EACCES`/`EPERM` → logs `'Permission denied'`, exits with code 1\n- Other errors rethrown\n\n**File operations:**\n- `unlink()` failures logged as `'Failed to delete <path>: <message>'` without aborting\n- `rename()` failures logged as `'Failed to restore <path>: <message>'` without aborting\n- `readFile()` failures during marker detection silently skip file (no log entry)\n\n## Dependencies\n\n- `fast-glob` — parallel artifact discovery with glob patterns\n- `picocolors` — terminal formatting for dry-run warnings and success messages\n- `src/output/logger.js` → `createLogger()` for structured logging\n- `src/generation/writers/agents-md.js` → `GENERATED_MARKER` constant for detection logic\n\n## Output Format\n\nDisplays relative paths via `path.relative(resolvedPath, absPath)` for readability. Summary line uses `pc.bold()` for file counts:\n```\n<count> .sum file(s), <count> .annex.md file(s), <count> AGENTS.md file(s), <count> root doc(s), <count> AGENTS.local.md to restore\n```",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 983,
      "cacheReadTokens": 17367,
      "cacheCreationTokens": 11081,
      "latencyMs": 25003,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/cli/clean.ts",
          "sizeBytes": 5971
        }
      ]
    },
    {
      "timestamp": "2026-02-09T17:44:24.756Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/cli/init.ts\n\n```typescript\n/**\n * `are init` command - Create default configuration\n *\n * Creates the `.agents-reverse/config.yaml` file with documented defaults.\n * Warns if configuration already exists.\n */\n\nimport path from 'node:path';\nimport { configExists, writeDefaultConfig, CONFIG_DIR, CONFIG_FILE } from '../config/loader.js';\nimport { createLogger } from '../output/logger.js';\n\n/**\n * Execute the `are init` command.\n *\n * Creates a default configuration file at `.agents-reverse/config.yaml`.\n * If the file already exists, logs a warning and returns without modification.\n *\n * @param root - Root directory where config will be created\n *\n * @example\n * ```typescript\n * await initCommand('.');\n * // Creates .agents-reverse/config.yaml in current directory\n * ```\n */\nexport async function initCommand(root: string, options?: { force?: boolean }): Promise<void> {\n  const resolvedRoot = path.resolve(root);\n  const configPath = path.join(resolvedRoot, CONFIG_DIR, CONFIG_FILE);\n  const force = options?.force ?? false;\n\n  const logger = createLogger({ colors: true });\n\n  try {\n    // Check if config already exists\n    if (!force && await configExists(resolvedRoot)) {\n      logger.warn(`Config already exists at ${configPath}`);\n      logger.info('Edit the file to customize exclusions and options.');\n    } else {\n      // Create default config\n      await writeDefaultConfig(resolvedRoot);\n\n      logger.info(`Created configuration at ${configPath}`);\n      logger.info('');\n      logger.info('Edit the file to customize:');\n      logger.info('  - exclude.patterns: Custom glob patterns to exclude');\n      logger.info('  - ai.concurrency: Parallel AI calls (1-20, default: auto)');\n      logger.info('  - ai.timeoutMs: Subprocess timeout (default: 300,000ms = 5 min)');\n      logger.info('  - ai.backend: AI backend (claude/gemini/opencode/auto)');\n      logger.info('');\n      logger.info('See README.md for full configuration reference.');\n    }\n  } catch (err) {\n    const error = err as NodeJS.ErrnoException;\n\n    // Permission error\n    if (error.code === 'EACCES' || error.code === 'EPERM') {\n      logger.error(`Permission denied: Cannot create ${configPath}`);\n      logger.info('Check that you have write permissions to this directory.');\n      process.exit(1);\n    }\n\n    // Other error\n    logger.error(`Failed to create configuration: ${error.message}`);\n    process.exit(1);\n  }\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**`init.ts` implements the `are init` CLI command that creates the `.agents-reverse-engineer/config.yaml` configuration file with documented defaults, performing existence checks to prevent accidental overwrites.**\n\n## Exported Function\n\n### `initCommand(root: string, options?: { force?: boolean }): Promise<void>`\n\nExecutes the `are init` command workflow:\n1. Resolves `root` to absolute path via `path.resolve()`\n2. Constructs `configPath` from `CONFIG_DIR` and `CONFIG_FILE` constants imported from `../config/loader.js`\n3. Checks `options?.force ?? false` to determine overwrite behavior\n4. Calls `configExists(resolvedRoot)` to detect existing configuration\n5. If config exists and `!force`, logs warning via `logger.warn()` with message `Config already exists at ${configPath}`\n6. If config missing or `force` enabled, calls `writeDefaultConfig(resolvedRoot)` to create YAML file\n7. Logs success via `logger.info()` with user guidance listing customizable fields: `exclude.patterns`, `ai.concurrency`, `ai.timeoutMs`, `ai.backend`\n8. Catches errors with errno code inspection: exits with `process.exit(1)` on `EACCES`/`EPERM` permission errors or other failures\n\n## Dependencies\n\n- `path.resolve()` and `path.join()` for path construction\n- `configExists()`, `writeDefaultConfig()`, `CONFIG_DIR`, `CONFIG_FILE` from `../config/loader.js` for configuration file operations\n- `createLogger({ colors: true })` from `../output/logger.js` for terminal output with ANSI formatting\n\n## Error Handling Strategy\n\nInspects `NodeJS.ErrnoException.code` property for errno classification:\n- `EACCES` or `EPERM` → permission denied error with directory write hint\n- Other errors → generic failure message with `error.message`\n- All error paths call `process.exit(1)` for non-zero exit code\n\n## User Guidance Content\n\nSuccess message enumerates four customizable config sections with inline examples:\n- `exclude.patterns: Custom glob patterns to exclude`\n- `ai.concurrency: Parallel AI calls (1-20, default: auto)`\n- `ai.timeoutMs: Subprocess timeout (default: 300,000ms = 5 min)`\n- `ai.backend: AI backend (claude/gemini/opencode/auto)`\n\nFinal instruction references `README.md for full configuration reference`.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 596,
      "cacheReadTokens": 17367,
      "cacheCreationTokens": 9761,
      "latencyMs": 21217,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/cli/init.ts",
          "sizeBytes": 2388
        }
      ]
    },
    {
      "timestamp": "2026-02-09T17:44:36.628Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/discovery/types.ts\n\n```typescript\n/**\n * Discovery types for the agents-reverse file discovery system.\n * \n * This module defines the core interfaces and types used by the directory\n * walker and filter chain for file discovery operations.\n */\n\nimport type { Stats } from 'node:fs';\n\n/**\n * Interface for file filters in the discovery pipeline.\n * \n * Filters are applied to each file discovered by the walker to determine\n * whether it should be included or excluded from analysis. Filters can be\n * synchronous or asynchronous.\n * \n * Examples: GitignoreFilter, BinaryFilter, VendorFilter, CustomPatternFilter\n */\nexport interface FileFilter {\n  /** Name of the filter for logging which filter excluded a file */\n  readonly name: string;\n  \n  /**\n   * Determine whether a file should be excluded from discovery.\n   * \n   * @param path - Absolute path to the file\n   * @param stats - Optional file stats (for size-based filtering, etc.)\n   * @returns true if the file should be excluded, false to include\n   */\n  shouldExclude(path: string, stats?: Stats): Promise<boolean> | boolean;\n}\n\n/**\n * Record of an excluded file with reason and responsible filter.\n */\nexport interface ExcludedFile {\n  /** Absolute path to the excluded file */\n  path: string;\n  /** Human-readable reason for exclusion */\n  reason: string;\n  /** Name of the filter that excluded this file */\n  filter: string;\n}\n\n/**\n * Result of running the discovery and filter chain.\n */\nexport interface FilterResult {\n  /** Files that passed all filters and should be analyzed */\n  included: string[];\n  /** Files that were excluded with reasons */\n  excluded: ExcludedFile[];\n}\n\n/**\n * Options for the directory walker.\n */\nexport interface WalkerOptions {\n  /** Root directory to walk (absolute path) */\n  cwd: string;\n  \n  /** \n   * Whether to follow symbolic links.\n   * Default: false (per CONTEXT.md - skip symlinks by default)\n   */\n  followSymlinks?: boolean;\n  \n  /**\n   * Whether to include dotfiles (files starting with .).\n   * Default: true (include dotfiles for analysis)\n   */\n  dot?: boolean;\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**Defines core TypeScript interfaces for the file discovery pipeline: FileFilter for composable exclusion predicates, FilterResult for discovery outcomes, WalkerOptions for traversal configuration, and ExcludedFile for audit trails.**\n\n## Exported Interfaces\n\n**FileFilter** — Contract for composable file exclusion predicates in the discovery filter chain. Exposes `name: string` for audit logging and `shouldExclude(path: string, stats?: Stats): Promise<boolean> | boolean` for synchronous or asynchronous exclusion decisions. Implemented by GitignoreFilter, BinaryFilter, VendorFilter, CustomPatternFilter.\n\n**ExcludedFile** — Record structure for excluded files with `path: string` (absolute path), `reason: string` (human-readable exclusion explanation), `filter: string` (name of the FileFilter that triggered exclusion).\n\n**FilterResult** — Discriminated discovery outcome containing `included: string[]` (absolute paths passing all filters) and `excluded: ExcludedFile[]` (rejected files with exclusion metadata).\n\n**WalkerOptions** — Configuration for directory traversal with `cwd: string` (absolute root path), `followSymlinks?: boolean` (default false per CONTEXT.md), `dot?: boolean` (include dotfiles, default true).\n\n## Integration Points\n\nConsumed by `src/discovery/walker.ts` for traversal configuration and `src/discovery/filters/` implementations (binary.ts, gitignore.ts, vendor.ts, custom.ts) for FileFilter contract adherence. FilterResult returned by `src/discovery/run.ts` aggregates results across the filter chain. Stats type imported from `node:fs` enables size-based filtering (see BinaryFilter with maxFileSize threshold).",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 398,
      "cacheReadTokens": 17367,
      "cacheCreationTokens": 9648,
      "latencyMs": 15425,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/discovery/types.ts",
          "sizeBytes": 2043
        }
      ]
    },
    {
      "timestamp": "2026-02-09T17:44:35.733Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/discovery/run.ts\n\n```typescript\n/**\n * High-level file discovery pipeline.\n *\n * Combines directory walking and filter application into a single function\n * that all commands can share.\n */\n\nimport { walkDirectory } from './walker.js';\nimport {\n  applyFilters,\n  createGitignoreFilter,\n  createVendorFilter,\n  createBinaryFilter,\n  createCustomFilter,\n} from './filters/index.js';\nimport type { FilterResult } from './types.js';\nimport type { ITraceWriter } from '../orchestration/trace.js';\n\n/**\n * Configuration subset needed for file discovery.\n * Structurally compatible with the full Config type from config/schema.ts.\n */\nexport interface DiscoveryConfig {\n  exclude: {\n    vendorDirs: string[];\n    binaryExtensions: string[];\n    patterns: string[];\n  };\n  options: {\n    maxFileSize: number;\n    followSymlinks: boolean;\n  };\n}\n\n/**\n * Options for the discovery pipeline.\n */\nexport interface DiscoverFilesOptions {\n  tracer?: ITraceWriter;\n  debug?: boolean;\n}\n\n/**\n * Discover files in a directory by walking and applying the standard filter chain.\n *\n * Creates the four standard filters (gitignore, vendor, binary, custom),\n * walks the directory tree, and applies the filters. Returns the full\n * FilterResult so callers can access both included and excluded files\n * with filter attribution.\n *\n * @param root - Absolute path to the directory to scan\n * @param config - Discovery-related configuration (exclude rules and options)\n * @param options - Optional tracing and debug settings\n * @returns FilterResult with included and excluded file lists\n */\nexport async function discoverFiles(\n  root: string,\n  config: DiscoveryConfig,\n  options?: DiscoverFilesOptions,\n): Promise<FilterResult> {\n  // Create filters in standard order\n  const gitignoreFilter = await createGitignoreFilter(root);\n  const vendorFilter = createVendorFilter(config.exclude.vendorDirs);\n  const binaryFilter = createBinaryFilter({\n    maxFileSize: config.options.maxFileSize,\n    additionalExtensions: config.exclude.binaryExtensions,\n  });\n  const customFilter = createCustomFilter(config.exclude.patterns, root);\n  const filters = [gitignoreFilter, vendorFilter, binaryFilter, customFilter];\n\n  // Walk directory\n  const files = await walkDirectory({\n    cwd: root,\n    followSymlinks: config.options.followSymlinks,\n  });\n\n  // Apply filters and return\n  return applyFilters(files, filters, {\n    tracer: options?.tracer,\n    debug: options?.debug,\n  });\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**discoverFiles orchestrates the four-stage filter chain over walkDirectory results to produce FilterResult with included/excluded file lists and filter attribution.**\n\n## Exported Functions\n\n**discoverFiles(root: string, config: DiscoveryConfig, options?: DiscoverFilesOptions): Promise<FilterResult>** — Creates gitignore, vendor, binary, and custom filters in order via createGitignoreFilter, createVendorFilter, createBinaryFilter, createCustomFilter; invokes walkDirectory with cwd=root and followSymlinks flag; applies filters via applyFilters with optional tracer/debug; returns FilterResult containing included/excluded arrays.\n\n## Type Definitions\n\n**DiscoveryConfig** — Structural subset interface with exclude: {vendorDirs: string[], binaryExtensions: string[], patterns: string[]} and options: {maxFileSize: number, followSymlinks: boolean}. Compatible with full Config from src/config/schema.ts for dependency inversion.\n\n**DiscoverFilesOptions** — Optional interface with tracer?: ITraceWriter and debug?: boolean for trace emission and verbose logging control.\n\n## Filter Chain Composition\n\nCreates four filters in fixed order before applying to walked files:\n\n1. **gitignoreFilter** — Async createGitignoreFilter(root) parses .gitignore files\n2. **vendorFilter** — createVendorFilter(config.exclude.vendorDirs) excludes node_modules/.git/dist\n3. **binaryFilter** — createBinaryFilter({maxFileSize, additionalExtensions}) detects non-text files\n4. **customFilter** — createCustomFilter(config.exclude.patterns, root) applies user glob patterns\n\nFilter array passed to applyFilters with tracer/debug options threaded through.\n\n## Integration Points\n\nImported by src/cli/discover.ts, src/cli/generate.ts, src/cli/update.ts as single entry point for file discovery. Decouples command layer from walker/filter implementation details. DiscoveryConfig interface allows commands to pass full Config object without circular dependency on src/config/schema.ts.\n\n## Dependencies\n\n- `./walker.js` — walkDirectory function\n- `./filters/index.js` — Filter factory functions and applyFilters orchestrator\n- `./types.js` — FilterResult type definition\n- `../orchestration/trace.js` — ITraceWriter interface for trace emission",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 552,
      "cacheReadTokens": 0,
      "cacheCreationTokens": 27139,
      "latencyMs": 16998,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/discovery/run.ts",
          "sizeBytes": 2419
        }
      ]
    },
    {
      "timestamp": "2026-02-09T17:44:40.459Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/discovery/walker.ts\n\n```typescript\n/**\n * Directory walker for the agents-reverse file discovery system.\n * \n * Uses fast-glob to traverse directories and return all candidate files.\n * Filters are applied separately via the filter chain (not in this module).\n */\n\nimport fg from 'fast-glob';\nimport type { WalkerOptions } from './types.js';\n\n/**\n * Walk a directory tree and return all files.\n * \n * This walker returns ALL files in the directory tree. Filtering happens\n * separately via the filter chain (gitignore, binary, vendor, custom patterns).\n * \n * @param options - Walker configuration\n * @returns Array of absolute file paths\n * \n * @example\n * ```typescript\n * const files = await walkDirectory({ cwd: '/path/to/repo' });\n * console.log(`Found ${files.length} files`);\n * ```\n */\nexport async function walkDirectory(options: WalkerOptions): Promise<string[]> {\n  return fg.glob('**/*', {\n    cwd: options.cwd,\n    absolute: true,\n    onlyFiles: true,\n    dot: options.dot ?? true,\n    followSymbolicLinks: options.followSymlinks ?? false,\n    suppressErrors: true, // Don't throw on permission errors (per RESEARCH.md)\n    // Always exclude .git internals for performance\n    ignore: ['**/.git/**'],\n  });\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**walkDirectory() wraps fast-glob to return all file paths in a directory tree, delegating filtering to the separate filter chain.**\n\n## Exported Function\n\n```typescript\nasync function walkDirectory(options: WalkerOptions): Promise<string[]>\n```\n\nReturns array of absolute file paths from `fg.glob('**/*', {...})` invocation. Accepts `WalkerOptions` with fields `cwd` (required base directory), `dot` (include dotfiles, defaults true), and `followSymlinks` (defaults false).\n\n## Glob Configuration\n\nPasses these options to `fg.glob()`:\n- `absolute: true` — returns full paths, not relative\n- `onlyFiles: true` — excludes directories from results\n- `dot: options.dot ?? true` — includes dotfiles by default\n- `followSymbolicLinks: options.followSymlinks ?? false` — does not traverse symlinks by default\n- `suppressErrors: true` — silently skips permission denied errors per `RESEARCH.md` directive\n- `ignore: ['**/.git/**']` — hardcoded exclusion for `.git` internals to improve performance\n\n## Filter Chain Decoupling\n\nDoes NOT apply gitignore, binary, vendor, or custom pattern filters. Module comment explicitly states \"Filters are applied separately via the filter chain (not in this module).\" The `src/discovery/filters/` directory contains the actual filter implementations (binary.ts, custom.ts, gitignore.ts, vendor.ts).\n\n## Dependencies\n\nImports `fg` from `fast-glob` (glob engine) and `WalkerOptions` from `./types.js` (type-only import).",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 379,
      "cacheReadTokens": 17367,
      "cacheCreationTokens": 9438,
      "latencyMs": 12443,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/discovery/walker.ts",
          "sizeBytes": 1187
        }
      ]
    },
    {
      "timestamp": "2026-02-09T17:44:34.527Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/config/schema.ts\n\n```typescript\n/**\n * Zod schema for configuration validation\n *\n * This schema defines the structure of `.agents-reverse/config.yaml`\n * and provides sensible defaults for all fields.\n */\n\nimport { z } from 'zod';\nimport {\n  DEFAULT_VENDOR_DIRS,\n  DEFAULT_BINARY_EXTENSIONS,\n  DEFAULT_MAX_FILE_SIZE,\n  DEFAULT_EXCLUDE_PATTERNS,\n  getDefaultConcurrency,\n} from './defaults.js';\n\n/**\n * Schema for exclusion configuration\n */\nconst ExcludeSchema = z.object({\n  /** Custom glob patterns to exclude (e.g., [\"*.log\", \"temp/**\"]) */\n  patterns: z.array(z.string()).default([...DEFAULT_EXCLUDE_PATTERNS]),\n  /** Vendor directories to exclude */\n  vendorDirs: z.array(z.string()).default([...DEFAULT_VENDOR_DIRS]),\n  /** Binary file extensions to exclude */\n  binaryExtensions: z.array(z.string()).default([...DEFAULT_BINARY_EXTENSIONS]),\n}).default({});\n\n/**\n * Schema for options configuration\n */\nconst OptionsSchema = z.object({\n  /** Whether to follow symbolic links during traversal */\n  followSymlinks: z.boolean().default(false),\n  /** Maximum file size in bytes (files larger than this are skipped) */\n  maxFileSize: z.number().positive().default(DEFAULT_MAX_FILE_SIZE),\n}).default({});\n\n/**\n * Schema for output configuration\n */\nconst OutputSchema = z.object({\n  /** Whether to use colors in terminal output */\n  colors: z.boolean().default(true),\n}).default({});\n\n/**\n * Schema for AI service configuration.\n *\n * Controls backend selection, model, timeout, retry behavior, and\n * telemetry log retention. All fields have sensible defaults.\n */\nconst AISchema = z.object({\n  /** AI CLI backend to use ('auto' detects from PATH) */\n  backend: z.enum(['claude', 'gemini', 'opencode', 'auto']).default('auto'),\n  /** Model identifier (backend-specific, e.g., \"sonnet\", \"opus\") */\n  model: z.string().default('sonnet'),\n  /** Default subprocess timeout in milliseconds */\n  timeoutMs: z.number().positive().default(300_000),\n  /** Maximum number of retries for transient errors */\n  maxRetries: z.number().min(0).default(3),\n  /** Default parallelism for concurrent AI calls (1-20). Auto-detected from CPU cores and available memory. */\n  concurrency: z.number().min(1).max(20).default(getDefaultConcurrency),\n  /** Telemetry settings */\n  telemetry: z.object({\n    /** Number of most recent run logs to keep on disk */\n    keepRuns: z.number().min(0).default(50),\n  }).default({}),\n}).default({});\n\n/**\n * Main configuration schema for agents-reverse.\n *\n * All fields have sensible defaults, so an empty object `{}` is valid\n * and will result in a fully populated configuration.\n *\n * @example\n * ```typescript\n * // Parse with defaults\n * const config = ConfigSchema.parse({});\n *\n * // Parse with partial overrides\n * const config = ConfigSchema.parse({\n *   exclude: { patterns: ['*.log'] },\n *   ai: { backend: 'claude', model: 'opus' },\n * });\n * ```\n */\nexport const ConfigSchema = z.object({\n  /** Exclusion rules for files and directories */\n  exclude: ExcludeSchema,\n  /** Discovery options */\n  options: OptionsSchema,\n  /** Output formatting options */\n  output: OutputSchema,\n  /** AI service configuration */\n  ai: AISchema,\n}).default({});\n\n/**\n * Inferred TypeScript type from the schema.\n * Use this type for function parameters and return types.\n */\nexport type Config = z.infer<typeof ConfigSchema>;\n\n/**\n * Type for the exclude section of config\n */\nexport type ExcludeConfig = z.infer<typeof ExcludeSchema>;\n\n/**\n * Type for the options section of config\n */\nexport type OptionsConfig = z.infer<typeof OptionsSchema>;\n\n/**\n * Type for the output section of config\n */\nexport type OutputConfig = z.infer<typeof OutputSchema>;\n\n/**\n * Type for the AI service section of config\n */\nexport type AIConfig = z.infer<typeof AISchema>;\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**ConfigSchema defines Zod validation schema for `.agents-reverse-engineer/config.yaml` with nested object structures for exclusion rules, discovery options, output formatting, and AI service configuration, enforcing type safety and providing default values for all fields.**\n\n## Schema Structure\n\nConfigSchema is the root schema composed of four nested schemas: ExcludeSchema, OptionsSchema, OutputSchema, and AISchema. Each sub-schema has `.default({})` fallback enabling empty object validation.\n\n## Exported Schemas\n\n**ExcludeSchema** validates exclusion configuration with three array fields:\n- `patterns: z.array(z.string()).default([...DEFAULT_EXCLUDE_PATTERNS])` — custom glob patterns for file exclusion\n- `vendorDirs: z.array(z.string()).default([...DEFAULT_VENDOR_DIRS])` — vendor directory names to skip\n- `binaryExtensions: z.array(z.string()).default([...DEFAULT_BINARY_EXTENSIONS])` — binary file extension list\n\n**OptionsSchema** validates discovery behavior with two fields:\n- `followSymlinks: z.boolean().default(false)` — symbolic link traversal toggle\n- `maxFileSize: z.number().positive().default(DEFAULT_MAX_FILE_SIZE)` — file size threshold in bytes for skipping large files\n\n**OutputSchema** validates terminal output formatting:\n- `colors: z.boolean().default(true)` — ANSI color code toggle for CLI output\n\n**AISchema** validates AI service configuration with six fields:\n- `backend: z.enum(['claude', 'gemini', 'opencode', 'auto']).default('auto')` — AI CLI backend selection with auto-detection\n- `model: z.string().default('sonnet')` — backend-specific model identifier\n- `timeoutMs: z.number().positive().default(300_000)` — subprocess timeout (5 minutes default)\n- `maxRetries: z.number().min(0).default(3)` — exponential backoff retry limit\n- `concurrency: z.number().min(1).max(20).default(getDefaultConcurrency)` — worker pool size (1-20 range, auto-detected default)\n- `telemetry: z.object({ keepRuns: z.number().min(0).default(50) }).default({})` — nested schema for run log retention count\n\n**ConfigSchema** combines all sub-schemas:\n```typescript\nz.object({\n  exclude: ExcludeSchema,\n  options: OptionsSchema,\n  output: OutputSchema,\n  ai: AISchema,\n}).default({})\n```\n\n## Exported Types\n\n**Config** = `z.infer<typeof ConfigSchema>` — inferred TypeScript type for full configuration object\n\n**ExcludeConfig** = `z.infer<typeof ExcludeSchema>` — inferred type for `exclude` section\n\n**OptionsConfig** = `z.infer<typeof OptionsSchema>` — inferred type for `options` section\n\n**OutputConfig** = `z.infer<typeof OutputSchema>` — inferred type for `output` section\n\n**AIConfig** = `z.infer<typeof AISchema>` — inferred type for `ai` section\n\n## Dependencies\n\nImports DEFAULT_VENDOR_DIRS, DEFAULT_BINARY_EXTENSIONS, DEFAULT_MAX_FILE_SIZE, DEFAULT_EXCLUDE_PATTERNS, and getDefaultConcurrency from `./defaults.js` for default value arrays and computation.\n\n## Validation Behavior\n\nAll schemas use `.default()` chaining enabling partial configuration parsing: `ConfigSchema.parse({})` returns fully populated Config object with all defaults applied. Spread operator `[...DEFAULT_VENDOR_DIRS]` creates shallow copies preventing array mutation across parse invocations.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 872,
      "cacheReadTokens": 17367,
      "cacheCreationTokens": 10211,
      "latencyMs": 18898,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/config/schema.ts",
          "sizeBytes": 3737
        }
      ]
    },
    {
      "timestamp": "2026-02-09T17:44:20.685Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/cli/index.ts\n\n```typescript\n#!/usr/bin/env node\n/**\n * CLI entry point for agents-reverse\n *\n * Commands:\n *   init              Create default configuration\n *   discover [path]   Discover files to analyze\n *   generate [path]   Generate documentation plan\n *   update [path]     Update docs incrementally\n *   specify [path]    Generate project specification from AGENTS.md docs\n *   clean [path]      Delete all generated artifacts\n */\n\nimport { initCommand } from './init.js';\nimport { discoverCommand } from './discover.js';\nimport { generateCommand, type GenerateOptions } from './generate.js';\nimport { updateCommand, type UpdateCommandOptions } from './update.js';\nimport { cleanCommand, type CleanOptions } from './clean.js';\nimport { specifyCommand, type SpecifyOptions } from './specify.js';\n\nimport { runInstaller, parseInstallerArgs } from '../installer/index.js';\nimport { getVersion } from '../version.js';\n\nconst VERSION = getVersion();\n\nconst USAGE = `\nagents-reverse-engineer - AI-friendly codebase documentation\n\nCommands:\n  install           Install commands and hooks to AI assistant\n  uninstall         Remove installed commands and hooks\n  init              Create default configuration\n  discover [path]   Discover files to analyze (default: current directory)\n  generate [path]   Generate documentation plan (default: current directory)\n  update [path]     Update docs incrementally (default: current directory)\n  specify [path]    Generate project specification from AGENTS.md docs\n  clean [path]      Delete all generated artifacts (.sum, AGENTS.md, etc.)\n\nInstall/Uninstall Options:\n  --runtime <name>  Runtime to target (claude, opencode, gemini, all)\n  -g, --global      Target global config directory\n  -l, --local       Target current project directory\n  --force           Overwrite existing files (init, install, specify)\n\nGeneral Options:\n  --debug           Show AI prompts and backend details\n  --trace           Enable concurrency tracing (.agents-reverse-engineer/traces/)\n  --dry-run         Show plan without writing files (generate, update, specify)\n  --output <path>   Output path for specification (specify only)\n  --multi-file      Split specification into multiple files (specify only)\n  --concurrency <n> Number of concurrent AI calls (default: auto)\n  --fail-fast       Stop on first file analysis failure\n  --uncommitted     Include uncommitted changes (update only)\n  --help, -h        Show this help\n  --version, -V     Show version number\n\nExamples:\n  are install\n  are install --runtime claude -g\n  are uninstall\n  are uninstall --runtime claude -g\n  are init\n  are discover\n  are generate --dry-run\n  are generate --concurrency 3\n  are generate ./my-project --concurrency 3\n  are update\n  are update --uncommitted\n  are specify --dry-run\n  are specify --output ./docs/spec.md --force\n`;\n\n/**\n * Parse command-line arguments.\n *\n * Extracts the command, positional arguments, and flags.\n * Handles global flags (--help, -h) that may appear before the command.\n */\nfunction parseArgs(args: string[]): {\n  command: string | undefined;\n  positional: string[];\n  flags: Set<string>;\n  values: Map<string, string>;\n} {\n  let command: string | undefined;\n  const positional: string[] = [];\n  const flags = new Set<string>();\n  const values = new Map<string, string>();\n\n  for (let i = 0; i < args.length; i++) {\n    const arg = args[i];\n    if (arg.startsWith('--')) {\n      const flagName = arg.slice(2);\n      // Check if next arg is a value (not starting with -)\n      if (i + 1 < args.length && !args[i + 1].startsWith('-')) {\n        values.set(flagName, args[i + 1]);\n        i++; // Skip the value\n      } else {\n        flags.add(flagName);\n      }\n    } else if (arg.startsWith('-')) {\n      // Handle short flags (e.g., -h, -g, -l)\n      for (const char of arg.slice(1)) {\n        switch (char) {\n          case 'h':\n            flags.add('help');\n            break;\n          case 'g':\n            flags.add('global');\n            break;\n          case 'l':\n            flags.add('local');\n            break;\n          case 'V':\n            flags.add('version');\n            break;\n          default:\n            // Unknown short flag - ignore\n            break;\n        }\n      }\n    } else if (!command) {\n      // First non-flag argument is the command\n      command = arg;\n    } else {\n      // Subsequent non-flag arguments are positional\n      positional.push(arg);\n    }\n  }\n\n  return { command, positional, flags, values };\n}\n\n/**\n * Show version and exit.\n */\nfunction showVersion(): void {\n  console.log(`agents-reverse-engineer v${VERSION}`);\n  process.exit(0);\n}\n\n/**\n * Display version banner.\n */\nfunction showVersionBanner(): void {\n  console.log(`agents-reverse-engineer v${VERSION}\\n`);\n}\n\n/**\n * Show usage information and exit.\n */\nfunction showHelp(): void {\n  console.log(USAGE);\n  process.exit(0);\n}\n\n/**\n * Show error for unknown command and exit.\n */\nfunction showUnknownCommand(command: string): void {\n  console.error(`Unknown command: ${command}`);\n  console.error(`Run 'are --help' for usage information.`);\n  process.exit(1);\n}\n\n/**\n * Check if command-line has installer-related flags.\n *\n * Used to detect direct installer invocation without 'install' command.\n */\nfunction hasInstallerFlags(flags: Set<string>, values: Map<string, string>): boolean {\n  return (\n    flags.has('global') ||\n    flags.has('local') ||\n    flags.has('force') ||\n    values.has('runtime')\n  );\n}\n\n/**\n * Main CLI entry point.\n */\nasync function main(): Promise<void> {\n  const args = process.argv.slice(2);\n  const { command, positional, flags, values } = parseArgs(args);\n\n  // Handle version flag\n  if (flags.has('version')) {\n    showVersion();\n  }\n\n  // Handle help flag anywhere (but not if --help is for install command)\n  if (flags.has('help') && !command && !hasInstallerFlags(flags, values)) {\n    showHelp();\n  }\n\n  // No command and no args - launch interactive installer\n  if (args.length === 0) {\n    await runInstaller({\n      global: false,\n      local: false,\n      uninstall: false,\n      force: false,\n      help: false,\n      quiet: false,\n    });\n    return;\n  }\n\n  // Direct installer invocation without 'install' command\n  // Supports: npx agents-reverse-engineer --runtime claude -g\n  if (!command && hasInstallerFlags(flags, values)) {\n    const installerArgs = parseInstallerArgs(args);\n    await runInstaller(installerArgs);\n    return;\n  }\n\n  // Show version banner\n  showVersionBanner();\n\n  // Route to command handlers\n  switch (command) {\n    case 'install': {\n      // Re-parse args for installer-specific flags\n      const installerArgs = parseInstallerArgs(args);\n      await runInstaller(installerArgs);\n      break;\n    }\n\n    case 'uninstall': {\n      // Re-parse args and force uninstall mode\n      const installerArgs = parseInstallerArgs(args);\n      installerArgs.uninstall = true;\n      await runInstaller(installerArgs);\n      break;\n    }\n\n    case 'init': {\n      await initCommand(positional[0] || '.', { force: flags.has('force') });\n      break;\n    }\n\n    case 'clean': {\n      const cleanOpts: CleanOptions = {\n        dryRun: flags.has('dry-run'),\n      };\n      await cleanCommand(positional[0] || '.', cleanOpts);\n      break;\n    }\n\n    case 'discover': {\n      await discoverCommand(positional[0] || '.', {});\n      break;\n    }\n\n    case 'generate': {\n      const options: GenerateOptions = {\n        dryRun: flags.has('dry-run'),\n        concurrency: values.has('concurrency') ? parseInt(values.get('concurrency')!, 10) : undefined,\n        failFast: flags.has('fail-fast'),\n        debug: flags.has('debug'),\n        trace: flags.has('trace'),\n      };\n      await generateCommand(positional[0] || '.', options);\n      break;\n    }\n\n    case 'update': {\n      const options: UpdateCommandOptions = {\n        uncommitted: flags.has('uncommitted'),\n        dryRun: flags.has('dry-run'),\n        concurrency: values.has('concurrency') ? parseInt(values.get('concurrency')!, 10) : undefined,\n        failFast: flags.has('fail-fast'),\n        debug: flags.has('debug'),\n        trace: flags.has('trace'),\n      };\n      await updateCommand(positional[0] || '.', options);\n      break;\n    }\n\n    case 'specify': {\n      const specifyOpts: SpecifyOptions = {\n        output: values.get('output'),\n        force: flags.has('force'),\n        dryRun: flags.has('dry-run'),\n        multiFile: flags.has('multi-file'),\n        debug: flags.has('debug'),\n        trace: flags.has('trace'),\n      };\n      await specifyCommand(positional[0] || '.', specifyOpts);\n      break;\n    }\n\n    default:\n      if (command) {\n        showUnknownCommand(command);\n      }\n      showHelp();\n  }\n}\n\n// Run main and handle any uncaught errors\nmain().catch((err: Error) => {\n  console.error(`Error: ${err.message}`);\n  process.exit(1);\n});\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**CLI entry point orchestrating command routing, argument parsing, and installer invocation for the agents-reverse-engineer tool.**\n\n## Exported Symbols\n\nNo exported functions or types. This module executes immediately via shebang `#!/usr/bin/env node` and calls `main().catch()` at module scope.\n\n## Commands\n\nSupports seven commands routed via switch statement in `main()`:\n\n- `install` → `runInstaller()` with `parseInstallerArgs(args)`\n- `uninstall` → `runInstaller()` with `installerArgs.uninstall = true`\n- `init` → `initCommand(positional[0] || '.', { force })` from `./init.js`\n- `clean` → `cleanCommand(positional[0] || '.', cleanOpts)` from `./clean.js`\n- `discover` → `discoverCommand(positional[0] || '.', {})` from `./discover.js`\n- `generate` → `generateCommand(positional[0] || '.', options)` from `./generate.js`\n- `update` → `updateCommand(positional[0] || '.', options)` from `./update.js`\n- `specify` → `specifyCommand(positional[0] || '.', specifyOpts)` from `./specify.js`\n\n## Argument Parsing\n\n`parseArgs(args: string[])` returns `{ command, positional, flags, values }` by iterating `process.argv.slice(2)`:\n\n- Flags starting with `--` populate `flags: Set<string>` or `values: Map<string, string>` if next arg is non-flag value\n- Short flags (`-h`, `-g`, `-l`, `-V`) expanded via character iteration with switch mapping (`-h` → `help`, `-g` → `global`, etc.)\n- First non-flag arg becomes `command`, subsequent non-flag args populate `positional: string[]`\n\n## Installer Invocation Patterns\n\nThree patterns trigger installer:\n\n1. **Interactive mode:** `args.length === 0` calls `runInstaller()` with all-false defaults\n2. **Direct flags without command:** `!command && hasInstallerFlags(flags, values)` passes `parseInstallerArgs(args)` to `runInstaller()`\n3. **Explicit install/uninstall command:** Passes `parseInstallerArgs(args)` to `runInstaller()`, setting `uninstall: true` for uninstall command\n\n`hasInstallerFlags()` returns true if `flags` contains `global`/`local`/`force` or `values` contains `runtime`.\n\n## Command Options\n\n`GenerateOptions` constructed from:\n- `dryRun: flags.has('dry-run')`\n- `concurrency: parseInt(values.get('concurrency')!, 10)` if present\n- `failFast: flags.has('fail-fast')`\n- `debug: flags.has('debug')`\n- `trace: flags.has('trace')`\n\n`UpdateCommandOptions` adds `uncommitted: flags.has('uncommitted')` to `GenerateOptions` fields.\n\n`SpecifyOptions` constructed from:\n- `output: values.get('output')`\n- `force: flags.has('force')`\n- `dryRun: flags.has('dry-run')`\n- `multiFile: flags.has('multi-file')`\n- `debug: flags.has('debug')`\n- `trace: flags.has('trace')`\n\n`CleanOptions` contains only `dryRun: flags.has('dry-run')`.\n\n## Help and Version\n\n`showHelp()` prints `USAGE` constant (multiline template with command list, option descriptions, examples) and calls `process.exit(0)`.\n\n`showVersion()` prints `agents-reverse-engineer v${VERSION}` via `getVersion()` from `../version.js` and exits.\n\n`showVersionBanner()` prints version without exiting, called before command routing.\n\n`showUnknownCommand(command: string)` prints error and suggests `are --help`, exits with code 1.\n\n## Global Flag Handling\n\nVersion flag (`--version` or `-V`) triggers `showVersion()` before command routing.\n\nHelp flag (`--help` or `-h`) triggers `showHelp()` only if no command present and no installer flags detected (prevents interference with `install --help`).\n\n## Error Handling\n\n`main().catch((err: Error) => { console.error(\\`Error: ${err.message}\\`); process.exit(1); })` catches uncaught errors at top level.\n\n## USAGE Constant\n\nMultiline string documenting:\n- Command signatures with `[path]` optional positional defaults to current directory\n- Install/uninstall options: `--runtime <name>`, `-g`/`--global`, `-l`/`--local`, `--force`\n- General options: `--debug`, `--trace`, `--dry-run`, `--output <path>`, `--multi-file`, `--concurrency <n>`, `--fail-fast`, `--uncommitted`, `--help`, `--version`\n- Examples showing installer, init, discover, generate with concurrency, update with uncommitted, specify with output/force",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 1240,
      "cacheReadTokens": 17367,
      "cacheCreationTokens": 11653,
      "latencyMs": 33112,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/cli/index.ts",
          "sizeBytes": 8809
        }
      ]
    },
    {
      "timestamp": "2026-02-09T17:44:27.080Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/config/defaults.ts\n\n```typescript\n/**\n * Default configuration values for agents-reverse\n */\n\nimport os from 'node:os';\n\n/** Multiplier applied to CPU core count for default concurrency */\nconst CONCURRENCY_MULTIPLIER = 5;\n\n/** Minimum default concurrency */\nconst MIN_CONCURRENCY = 2;\n\n/** Maximum default concurrency (matches schema .max(20)) */\nconst MAX_CONCURRENCY = 20;\n\n/** Heap budget per subprocess in GB (matches NODE_OPTIONS --max-old-space-size=512) */\nconst SUBPROCESS_HEAP_GB = 0.512;\n\n/** Fraction of total system memory to allocate to subprocesses */\nconst MEMORY_FRACTION = 0.5;\n\n/**\n * Compute the default concurrency based on available CPU cores and system memory.\n *\n * Formula: clamp(cores * 5, MIN, min(memCap, MAX))\n * - cores: os.availableParallelism() or os.cpus().length\n * - memCap: floor(totalMemGB * 0.5 / 0.512) — use at most 50% of RAM for subprocesses\n *\n * @returns Default concurrency value (integer between MIN_CONCURRENCY and MAX_CONCURRENCY)\n */\nexport function getDefaultConcurrency(): number {\n  const cores = typeof os.availableParallelism === 'function'\n    ? os.availableParallelism()\n    : (os.cpus().length || MIN_CONCURRENCY);\n\n  const totalMemGB = os.totalmem() / (1024 ** 3);\n  const memCap = totalMemGB > 1\n    ? Math.floor((totalMemGB * MEMORY_FRACTION) / SUBPROCESS_HEAP_GB)\n    : Infinity;\n\n  const computed = cores * CONCURRENCY_MULTIPLIER;\n  return Math.max(MIN_CONCURRENCY, Math.min(computed, memCap, MAX_CONCURRENCY));\n}\n\n/**\n * Default vendor directories to exclude from analysis.\n * These are typically package managers, build outputs, or version control directories.\n */\nexport const DEFAULT_VENDOR_DIRS = [\n  'node_modules',\n  'vendor',\n  '.git',\n  'dist',\n  'build',\n  '__pycache__',\n  '.next',\n  'venv',\n  '.venv',\n  'target',\n  '.cargo',\n  '.gradle',\n  // AI assistant tooling directories\n  '.agents-reverse-engineer',\n  '.agents',\n  '.planning',\n  '.claude',\n  '.opencode',\n  '.gemini',\n] as const;\n\n/**\n * Default file patterns to exclude from analysis.\n * These patterns use gitignore syntax and are matched by the custom filter.\n */\nexport const DEFAULT_EXCLUDE_PATTERNS = [\n  // AI assistant documentation files\n  'AGENTS.md',\n  'CLAUDE.md',\n  'OPENCODE.md',\n  'GEMINI.md',\n  '**/AGENTS.md',\n  '**/CLAUDE.md',\n  '**/OPENCODE.md',\n  '**/GEMINI.md',\n  // Lock files (not useful for documentation, can be very large)\n  '*.lock',\n  'package-lock.json',\n  'yarn.lock',\n  'pnpm-lock.yaml',\n  'bun.lock',\n  'bun.lockb',\n  'Gemfile.lock',\n  'Cargo.lock',\n  'poetry.lock',\n  'composer.lock',\n  'go.sum',\n  // Dotfiles and generated artifacts (path.extname returns '' for dotfiles,\n  // so these must be matched as glob patterns, not binary extensions)\n  '.gitignore',\n  '.gitattributes',\n  '.gitkeep',\n  '.env',\n  '**/.env',\n  '**/.env.*',\n  '*.log',\n  '*.sum',\n  '**/*.sum',\n  '**/SKILL.md',\n] as const;\n\n/**\n * Default binary file extensions to exclude from analysis.\n * These files cannot be meaningfully analyzed as text.\n */\nexport const DEFAULT_BINARY_EXTENSIONS = [\n  // Images\n  '.png',\n  '.jpg',\n  '.jpeg',\n  '.gif',\n  '.bmp',\n  '.ico',\n  '.webp',\n  // Archives\n  '.zip',\n  '.tar',\n  '.gz',\n  '.rar',\n  '.7z',\n  // Executables\n  '.exe',\n  '.dll',\n  '.so',\n  '.dylib',\n  // Media\n  '.mp3',\n  '.mp4',\n  '.wav',\n  // Documents\n  '.pdf',\n  // Fonts\n  '.woff',\n  '.woff2',\n  '.ttf',\n  '.eot',\n  // Compiled\n  '.class',\n  '.pyc',\n] as const;\n\n/**\n * Default maximum file size in bytes (1MB).\n * Files larger than this will be skipped with a warning.\n */\nexport const DEFAULT_MAX_FILE_SIZE = 1024 * 1024;\n\n/**\n * Default configuration object matching the schema structure.\n * This is used when no config file is present or for missing fields.\n */\nexport const DEFAULT_CONFIG = {\n  exclude: {\n    patterns: [...DEFAULT_EXCLUDE_PATTERNS],\n    vendorDirs: [...DEFAULT_VENDOR_DIRS],\n    binaryExtensions: [...DEFAULT_BINARY_EXTENSIONS],\n  },\n  options: {\n    followSymlinks: false,\n    maxFileSize: DEFAULT_MAX_FILE_SIZE,\n  },\n  output: {\n    colors: true,\n  },\n} as const;\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**defaults.ts exports default configuration constants and platform-adaptive concurrency calculation for agents-reverse-engineer initialization.**\n\n## Exported Functions\n\n**getDefaultConcurrency(): number** computes adaptive concurrency limit based on CPU cores and available system memory. Returns integer between `MIN_CONCURRENCY` (2) and `MAX_CONCURRENCY` (20). Formula: `clamp(cores × 5, MIN, min(memCap, MAX))` where `cores` resolves via `os.availableParallelism()` fallback to `os.cpus().length`, and `memCap = floor((totalMemGB × 0.5) / 0.512)` enforces memory constraint preventing subprocess heap exhaustion (allocates 50% of total RAM divided by 512MB per subprocess).\n\n## Exported Constants\n\n**DEFAULT_VENDOR_DIRS**: readonly string array of 18 directory names excluded from file discovery: package managers (`node_modules`, `vendor`), build outputs (`dist`, `build`, `target`, `.next`), version control (`.git`), Python environments (`__pycache__`, `venv`, `.venv`), Rust/Gradle caches (`.cargo`, `.gradle`), AI assistant tooling directories (`.agents-reverse-engineer`, `.agents`, `.planning`, `.claude`, `.opencode`, `.gemini`).\n\n**DEFAULT_EXCLUDE_PATTERNS**: readonly string array of gitignore-style glob patterns for custom filter: AI-generated documentation files (`AGENTS.md`, `CLAUDE.md`, `OPENCODE.md`, `GEMINI.md` with `**/` variants), lock files (`*.lock`, `package-lock.json`, `yarn.lock`, `pnpm-lock.yaml`, `bun.lock`, `bun.lockb`, `Gemfile.lock`, `Cargo.lock`, `poetry.lock`, `composer.lock`, `go.sum`), dotfiles and logs (`.gitignore`, `.gitattributes`, `.gitkeep`, `.env`, `**/.env`, `**/.env.*`, `*.log`), generated summaries (`*.sum`, `**/*.sum`), skill definitions (`**/SKILL.md`).\n\n**DEFAULT_BINARY_EXTENSIONS**: readonly string array of 26 file extensions for binary detection: images (`.png`, `.jpg`, `.jpeg`, `.gif`, `.bmp`, `.ico`, `.webp`), archives (`.zip`, `.tar`, `.gz`, `.rar`, `.7z`), executables (`.exe`, `.dll`, `.so`, `.dylib`), media (`.mp3`, `.mp4`, `.wav`), documents (`.pdf`), fonts (`.woff`, `.woff2`, `.ttf`, `.eot`), compiled bytecode (`.class`, `.pyc`).\n\n**DEFAULT_MAX_FILE_SIZE**: numeric constant `1048576` (1MB in bytes) for binary detection threshold via file size heuristic.\n\n**DEFAULT_CONFIG**: readonly object matching config schema structure with nested `exclude` (patterns/vendorDirs/binaryExtensions arrays), `options` (followSymlinks: false, maxFileSize: 1048576), `output` (colors: true). Used by config loader for missing fields or absent config file fallback.\n\n## Configuration Constants\n\n**CONCURRENCY_MULTIPLIER**: value `5` scales CPU core count in concurrency formula.\n\n**MIN_CONCURRENCY**: value `2` enforces lower bound matching schema validation and WSL resource constraints documented in subprocess management mitigations.\n\n**MAX_CONCURRENCY**: value `20` enforces upper bound matching Zod schema `.max(20)` constraint in `src/config/schema.ts`.\n\n**SUBPROCESS_HEAP_GB**: value `0.512` (512MB) matches `NODE_OPTIONS='--max-old-space-size=512'` environment variable set in `src/ai/subprocess.ts` for heap limit per Claude CLI subprocess.\n\n**MEMORY_FRACTION**: value `0.5` allocates maximum 50% of total system RAM to concurrent subprocess pool, preventing host system memory exhaustion under high concurrency.\n\n## Memory-Aware Concurrency Logic\n\n**getDefaultConcurrency()** implements memory-bounded worker pool sizing preventing RAM exhaustion scenarios where `cores × 5` would spawn too many 512MB subprocesses. Checks `os.totalmem() / (1024³)` for total RAM in GB, computes `memCap = floor((totalMemGB × 0.5) / 0.512)` limiting concurrent subprocesses to fit within 50% memory budget, applies `Math.min(computed, memCap, MAX_CONCURRENCY)` to respect memory constraint alongside schema maximum. Fallback path uses `Infinity` memCap when `totalMemGB ≤ 1` to avoid division-by-zero on resource-constrained systems, deferring to CPU-based formula and MAX_CONCURRENCY ceiling.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 1134,
      "cacheReadTokens": 17367,
      "cacheCreationTokens": 10488,
      "latencyMs": 27434,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/config/defaults.ts",
          "sizeBytes": 3998
        }
      ]
    },
    {
      "timestamp": "2026-02-09T17:44:26.577Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/cli/specify.ts\n\n```typescript\n/**\n * CLI specify command\n *\n * Generates a project specification from AGENTS.md documentation by:\n * 1. Loading configuration\n * 2. Collecting all AGENTS.md files (auto-generating if none exist)\n * 3. Building a synthesis prompt from the collected docs\n * 4. Resolving an AI CLI backend and calling the AI service\n * 5. Writing the specification to disk (single or multi-file)\n *\n * With --dry-run, shows input statistics without making any AI calls.\n */\n\nimport * as path from 'node:path';\nimport pc from 'picocolors';\nimport { loadConfig } from '../config/loader.js';\nimport { collectAgentsDocs, collectAnnexFiles } from '../generation/collector.js';\nimport { buildSpecPrompt, writeSpec, SpecExistsError } from '../specify/index.js';\nimport {\n  AIService,\n  AIServiceError,\n  createBackendRegistry,\n  resolveBackend,\n  getInstallInstructions,\n} from '../ai/index.js';\nimport { ProgressLog } from '../orchestration/index.js';\nimport { generateCommand } from './generate.js';\n\n/**\n * Options for the specify command.\n */\nexport interface SpecifyOptions {\n  /** Custom output path (default: specs/SPEC.md) */\n  output?: string;\n  /** Overwrite existing specs */\n  force?: boolean;\n  /** Show plan without calling AI */\n  dryRun?: boolean;\n  /** Split output into multiple files */\n  multiFile?: boolean;\n  /** Show verbose debug info */\n  debug?: boolean;\n  /** Enable tracing */\n  trace?: boolean;\n}\n\n/**\n * Specify command - collects AGENTS.md documentation, synthesizes it via AI,\n * and writes a comprehensive project specification.\n *\n * @param targetPath - Directory to generate specification for\n * @param options - Command options (output, force, dryRun, multiFile, debug, trace)\n */\nexport async function specifyCommand(\n  targetPath: string,\n  options: SpecifyOptions,\n): Promise<void> {\n  const absolutePath = path.resolve(targetPath);\n  const outputPath = options.output\n    ? path.resolve(options.output)\n    : path.join(absolutePath, 'specs', 'SPEC.md');\n\n  // Load configuration\n  const config = await loadConfig(absolutePath, { debug: options.debug });\n\n  // Collect AGENTS.md files\n  let docs = await collectAgentsDocs(absolutePath);\n\n  // ---------------------------------------------------------------------------\n  // Dry-run mode: show summary without calling AI or generating\n  // ---------------------------------------------------------------------------\n\n  // Collect annex files for reproduction-critical content\n  let annexFiles = await collectAnnexFiles(absolutePath);\n\n  if (options.dryRun) {\n    const totalChars = docs.reduce((sum, d) => sum + d.content.length, 0)\n      + annexFiles.reduce((sum, d) => sum + d.content.length, 0);\n    const estimatedTokensK = Math.ceil(totalChars / 4) / 1000;\n\n    console.log(pc.bold('\\n--- Dry Run Summary ---\\n'));\n    console.log(`  AGENTS.md files:   ${pc.cyan(String(docs.length))}`);\n    console.log(`  Annex files:       ${pc.cyan(String(annexFiles.length))}`);\n    console.log(`  Total input:       ${pc.cyan(`~${estimatedTokensK}K tokens`)}`);\n    console.log(`  Output:            ${pc.cyan(outputPath)}`);\n    console.log(`  Mode:              ${pc.cyan(options.multiFile ? 'multi-file' : 'single-file')}`);\n    console.log('');\n    console.log(pc.dim('No AI calls made (dry run).'));\n\n    if (docs.length === 0) {\n      console.log('');\n      console.log(pc.yellow('Warning: No AGENTS.md files found. Run `are generate` first or omit --dry-run to auto-generate.'));\n    } else if (estimatedTokensK > 150) {\n      console.log('');\n      console.log(pc.yellow('Warning: Input exceeds 150K tokens. Consider using a model with extended context.'));\n    }\n\n    return;\n  }\n\n  // Auto-generate if no AGENTS.md files exist\n  if (docs.length === 0) {\n    console.log(pc.yellow('No AGENTS.md files found. Running generate first...'));\n    await generateCommand(targetPath, {\n      debug: options.debug,\n      trace: options.trace,\n    });\n    docs = await collectAgentsDocs(absolutePath);\n    annexFiles = await collectAnnexFiles(absolutePath);\n    if (docs.length === 0) {\n      console.error(pc.red('Error: No AGENTS.md files found after generation. Cannot proceed.'));\n      process.exit(1);\n    }\n  }\n\n  // ---------------------------------------------------------------------------\n  // Resolve backend and run AI synthesis\n  // ---------------------------------------------------------------------------\n\n  const registry = createBackendRegistry();\n  let backend;\n  try {\n    backend = await resolveBackend(registry, config.ai.backend);\n  } catch (error) {\n    if (error instanceof AIServiceError && error.code === 'CLI_NOT_FOUND') {\n      console.error(pc.red('Error: No AI CLI found.\\n'));\n      console.error(getInstallInstructions(registry));\n      process.exit(2);\n    }\n    throw error;\n  }\n\n  // Debug: log backend info\n  if (options.debug) {\n    console.error(pc.dim(`[debug] Backend: ${backend.name}`));\n    console.error(pc.dim(`[debug] CLI command: ${backend.cliCommand}`));\n    console.error(pc.dim(`[debug] Model: ${config.ai.model}`));\n  }\n\n  // Create AI service with extended timeout (spec generation takes longer)\n  const aiService = new AIService(backend, {\n    timeoutMs: Math.max(config.ai.timeoutMs, 600_000),\n    maxRetries: config.ai.maxRetries,\n    model: config.ai.model,\n    telemetry: { keepRuns: config.ai.telemetry.keepRuns },\n  });\n\n  if (options.debug) {\n    aiService.setDebug(true);\n  }\n\n  // Build prompt from collected docs and annex files\n  const prompt = buildSpecPrompt(docs, annexFiles.length > 0 ? annexFiles : undefined);\n\n  if (options.debug) {\n    console.error(pc.dim(`[debug] System prompt: ${prompt.system.length} chars`));\n    console.error(pc.dim(`[debug] User prompt: ${prompt.user.length} chars`));\n  }\n\n  // Create progress log for tail -f monitoring\n  const progressLog = ProgressLog.create(absolutePath);\n  progressLog.write(`=== ARE Specify (${new Date().toISOString()}) ===`);\n  progressLog.write(`Project: ${absolutePath}`);\n  progressLog.write(`AGENTS.md files: ${docs.length}`);\n  progressLog.write(`Annex files: ${annexFiles.length}`);\n  progressLog.write('');\n\n  console.log(pc.bold('Generating specification...'));\n  console.log(pc.dim('This may take several minutes depending on project size.'));\n  progressLog.write('Generating specification...');\n\n  const response = await aiService.call({\n    prompt: prompt.user,\n    systemPrompt: prompt.system,\n    taskLabel: 'specify',\n  });\n\n  // ---------------------------------------------------------------------------\n  // Write output\n  // ---------------------------------------------------------------------------\n\n  try {\n    const writtenFiles = await writeSpec(response.text, {\n      outputPath,\n      force: options.force ?? false,\n      multiFile: options.multiFile ?? false,\n    });\n\n    console.log('');\n    console.log(pc.green(pc.bold('Specification written successfully:')));\n    for (const file of writtenFiles) {\n      console.log(pc.green(`  ${file}`));\n      progressLog.write(`Written: ${file}`);\n    }\n  } catch (error) {\n    if (error instanceof SpecExistsError) {\n      progressLog.write(`Error: ${error.message}`);\n      await progressLog.finalize();\n      console.error(pc.red(error.message));\n      process.exit(1);\n    }\n    throw error;\n  }\n\n  // ---------------------------------------------------------------------------\n  // Finalize telemetry\n  // ---------------------------------------------------------------------------\n\n  const { summary } = await aiService.finalize(absolutePath);\n\n  const summaryLine = `Tokens: ${summary.totalInputTokens} in / ${summary.totalOutputTokens} out` +\n    ` | Duration: ${(summary.totalDurationMs / 1000).toFixed(1)}s` +\n    ` | Output: ${outputPath}`;\n  console.log('');\n  console.log(pc.dim(summaryLine));\n  progressLog.write(summaryLine);\n  await progressLog.finalize();\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**specify.ts implements the `are specify` CLI command that synthesizes project specifications from AGENTS.md documentation via AI backend calls with auto-generation fallback, dry-run preview, and single/multi-file output modes.**\n\n## Exported Interface\n\n**specifyCommand(targetPath: string, options: SpecifyOptions): Promise<void>** — Main entry point that orchestrates specification generation by loading config, collecting AGENTS.md/annex files, optionally auto-generating missing docs, building synthesis prompt, resolving AI backend, invoking AI service with extended timeout (600s minimum), writing output via `writeSpec()`, and finalizing telemetry.\n\n**SpecifyOptions** — Configuration interface with fields:\n- `output?: string` — Custom output path (default: `specs/SPEC.md`)\n- `force?: boolean` — Overwrite existing specs\n- `dryRun?: boolean` — Show statistics without AI calls\n- `multiFile?: boolean` — Split output into multiple files\n- `debug?: boolean` — Enable verbose logging\n- `trace?: boolean` — Enable trace emission\n\n## Command Workflow\n\n### Phase 1: Collection and Validation\nCalls `collectAgentsDocs(absolutePath)` and `collectAnnexFiles(absolutePath)` to gather input corpus. If `docs.length === 0` and not `dryRun`, invokes `generateCommand(targetPath, {debug, trace})` to auto-generate AGENTS.md files. Exits with code 1 if docs still empty after generation.\n\n### Phase 2: Dry-Run Mode\nWhen `options.dryRun === true`, computes `totalChars` from `docs` and `annexFiles` content lengths, estimates tokens via `Math.ceil(totalChars / 4) / 1000`, prints summary showing AGENTS.md count, annex count, token estimate, output path, and mode (single/multi-file). Warns if no docs found or if `estimatedTokensK > 150`. Returns without AI calls.\n\n### Phase 3: Backend Resolution\nCreates `BackendRegistry` via `createBackendRegistry()`, resolves backend via `resolveBackend(registry, config.ai.backend)`. Catches `AIServiceError` with `code === 'CLI_NOT_FOUND'`, prints `getInstallInstructions(registry)`, exits with code 2. Logs backend name, `cliCommand`, model if `debug === true`.\n\n### Phase 4: AI Synthesis\nInstantiates `AIService` with extended timeout `Math.max(config.ai.timeoutMs, 600_000)` (minimum 10 minutes). Calls `buildSpecPrompt(docs, annexFiles.length > 0 ? annexFiles : undefined)` to construct system/user prompt pair. Creates `ProgressLog` via `ProgressLog.create(absolutePath)`, writes header with ISO timestamp, project path, doc counts. Invokes `aiService.call({prompt: prompt.user, systemPrompt: prompt.system, taskLabel: 'specify'})` and awaits response.\n\n### Phase 5: Output Writing\nCalls `writeSpec(response.text, {outputPath, force, multiFile})` which returns `writtenFiles: string[]`. Catches `SpecExistsError`, logs error message, finalizes progress log, exits with code 1. Prints green success message listing all written paths.\n\n### Phase 6: Telemetry Finalization\nCalls `aiService.finalize(absolutePath)` to flush telemetry, extracts `summary` with `totalInputTokens`, `totalOutputTokens`, `totalDurationMs`. Constructs `summaryLine` string showing token counts, duration in seconds (1 decimal place), and output path. Logs summary and writes to progress log before calling `progressLog.finalize()`.\n\n## Dependencies\n\nImports `loadConfig` from `../config/loader.js`, `collectAgentsDocs` and `collectAnnexFiles` from `../generation/collector.js`, `buildSpecPrompt`, `writeSpec`, and `SpecExistsError` from `../specify/index.js`. Imports `AIService`, `AIServiceError`, `createBackendRegistry`, `resolveBackend`, `getInstallInstructions` from `../ai/index.js`. Imports `ProgressLog` from `../orchestration/index.js`. Imports `generateCommand` from `./generate.js` for auto-generation fallback.\n\n## Error Handling\n\n- **CLI_NOT_FOUND**: Catches `AIServiceError` with code `'CLI_NOT_FOUND'`, prints red error message, calls `getInstallInstructions(registry)`, exits with code 2\n- **SpecExistsError**: Catches error from `writeSpec()` when output exists without `force` flag, logs to progress log, prints red error message, exits with code 1\n- **Empty docs after generation**: Prints red error message \"No AGENTS.md files found after generation. Cannot proceed.\", exits with code 1\n\n## Behavioral Contracts\n\n**Output path resolution**: `options.output` resolves via `path.resolve(options.output)`, defaults to `path.join(absolutePath, 'specs', 'SPEC.md')` if undefined.\n\n**Token estimation**: Computes `estimatedTokensK = Math.ceil(totalChars / 4) / 1000` assuming 4 characters per token.\n\n**High-context warning threshold**: Warns in dry-run mode if `estimatedTokensK > 150`.\n\n**Timeout override**: Uses `Math.max(config.ai.timeoutMs, 600_000)` ensuring minimum 10-minute timeout regardless of config value.\n\n**Summary format**: `\"Tokens: {totalInputTokens} in / {totalOutputTokens} out | Duration: {totalDurationMs/1000:.1f}s | Output: {outputPath}\"`.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 1367,
      "cacheReadTokens": 0,
      "cacheCreationTokens": 28715,
      "latencyMs": 30865,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/cli/specify.ts",
          "sizeBytes": 7826
        }
      ]
    },
    {
      "timestamp": "2026-02-09T17:44:19.863Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/cli/generate.ts\n\n```typescript\n/**\n * CLI generate command\n *\n * Creates and executes a documentation generation plan by:\n * 1. Discovering files to analyze\n * 2. Detecting file types and creating analysis tasks\n * 3. Resolving an AI CLI backend\n * 4. Running concurrent AI analysis via CommandRunner\n * 5. Producing .sum files, AGENTS.md, and root documents\n *\n * With --dry-run, shows the plan without making any AI calls.\n */\n\nimport * as path from 'node:path';\nimport pc from 'picocolors';\nimport { loadConfig } from '../config/loader.js';\nimport { createLogger } from '../output/logger.js';\nimport { discoverFiles } from '../discovery/run.js';\nimport { createOrchestrator, type GenerationPlan } from '../generation/orchestrator.js';\nimport { buildExecutionPlan } from '../generation/executor.js';\nimport {\n  AIService,\n  AIServiceError,\n  createBackendRegistry,\n  resolveBackend,\n  getInstallInstructions,\n} from '../ai/index.js';\nimport { CommandRunner, ProgressLog, createTraceWriter, cleanupOldTraces } from '../orchestration/index.js';\n\n/**\n * Options for the generate command.\n */\nexport interface GenerateOptions {\n  /** Dry run - show plan without generating */\n  dryRun?: boolean;\n  /** Number of concurrent AI calls */\n  concurrency?: number;\n  /** Stop on first file analysis failure */\n  failFast?: boolean;\n  /** Show AI prompts and backend details */\n  debug?: boolean;\n  /** Enable concurrency tracing to .agents-reverse-engineer/traces/ */\n  trace?: boolean;\n}\n\n/**\n * Format the generation plan for display.\n */\nfunction formatPlan(plan: GenerationPlan): string {\n  const lines: string[] = [];\n\n  lines.push(`\\n=== Generation Plan ===\\n`);\n\n  // File summary\n  lines.push(`Files to analyze: ${plan.files.length}`);\n  lines.push(`Tasks to execute: ${plan.tasks.length}`);\n  lines.push('');\n\n  // Complexity\n  lines.push('Complexity:');\n  lines.push(`  Files: ${plan.complexity.fileCount}`);\n  lines.push(`  Directory depth: ${plan.complexity.directoryDepth}`);\n  lines.push('');\n\n  return lines.join('\\n');\n}\n\n/**\n * Generate command - discovers files, plans analysis, and executes AI-driven\n * documentation generation.\n *\n * Default behavior: resolves an AI CLI backend, builds an execution plan,\n * and runs concurrent AI analysis via the CommandRunner. Produces .sum files,\n * AGENTS.md per directory, and root documents (CLAUDE.md, ARCHITECTURE.md, etc.).\n *\n * @param targetPath - Directory to generate documentation for\n * @param options - Command options (concurrency, failFast, debug, etc.)\n */\nexport async function generateCommand(\n  targetPath: string,\n  options: GenerateOptions\n): Promise<void> {\n  const absolutePath = path.resolve(targetPath);\n  const logger = createLogger({ colors: true });\n\n  logger.info(`Generating documentation plan for: ${absolutePath}`);\n\n  // Create trace writer (moved earlier to use in config loading and discovery)\n  const tracer = createTraceWriter(absolutePath, options.trace ?? false);\n  if (options.trace && tracer.filePath) {\n    console.error(pc.dim(`[trace] Writing to ${tracer.filePath}`));\n  }\n\n  // Load configuration\n  const config = await loadConfig(absolutePath, {\n    tracer,\n    debug: options.debug,\n  });\n\n  // Discover files\n  logger.info('Discovering files...');\n\n  const filterResult = await discoverFiles(absolutePath, config, {\n    tracer,\n    debug: options.debug,\n  });\n\n  // Create discovery result for orchestrator\n  const discoveryResult = {\n    files: filterResult.included,\n    excluded: filterResult.excluded.map(e => ({ path: e.path, reason: e.reason })),\n  };\n\n  logger.info(`Found ${discoveryResult.files.length} files to analyze`);\n\n  // Create generation plan\n  logger.info('Creating generation plan...');\n  const orchestrator = createOrchestrator(\n    config,\n    absolutePath,\n    { tracer, debug: options.debug }\n  );\n  const plan = await orchestrator.createPlan(discoveryResult);\n\n  // Display plan\n  console.log(formatPlan(plan));\n\n  // ---------------------------------------------------------------------------\n  // Dry-run: show execution plan summary without making AI calls\n  // ---------------------------------------------------------------------------\n\n  if (options.dryRun) {\n    const executionPlan = buildExecutionPlan(plan, absolutePath);\n    const dirCount = Object.keys(executionPlan.directoryFileMap).length;\n\n    console.log(pc.bold('\\n--- Dry Run Summary ---\\n'));\n    console.log(`  Files to analyze:     ${pc.cyan(String(executionPlan.fileTasks.length))}`);\n    console.log(`  Directories:          ${pc.cyan(String(dirCount))}`);\n    console.log(`  Root documents:       ${pc.cyan(String(executionPlan.rootTasks.length))}`);\n    console.log(`  Estimated AI calls:   ${pc.cyan(String(executionPlan.tasks.length))}`);\n    console.log('');\n    console.log(pc.dim('Files:'));\n    for (const task of executionPlan.fileTasks) {\n      console.log(pc.dim(`  ${task.path}`));\n    }\n    console.log('');\n    console.log(pc.dim('No AI calls made (dry run).'));\n    return;\n  }\n\n  // ---------------------------------------------------------------------------\n  // Resolve backend and run AI analysis\n  // ---------------------------------------------------------------------------\n\n  // Resolve AI CLI backend\n  const registry = createBackendRegistry();\n  let backend;\n  try {\n    backend = await resolveBackend(registry, config.ai.backend);\n  } catch (error) {\n    if (error instanceof AIServiceError && error.code === 'CLI_NOT_FOUND') {\n      console.error(pc.red('Error: No AI CLI found.\\n'));\n      console.error(getInstallInstructions(registry));\n      process.exit(2);\n    }\n    throw error;\n  }\n\n  // Debug: log backend info\n  if (options.debug) {\n    console.log(pc.dim(`[debug] Backend: ${backend.name}`));\n    console.log(pc.dim(`[debug] CLI command: ${backend.cliCommand}`));\n    console.log(pc.dim(`[debug] Model: ${config.ai.model}`));\n  }\n\n  // Create AI service\n  const aiService = new AIService(backend, {\n    timeoutMs: config.ai.timeoutMs,\n    maxRetries: config.ai.maxRetries,\n    model: config.ai.model,\n    telemetry: { keepRuns: config.ai.telemetry.keepRuns },\n  });\n\n  if (options.debug) {\n    aiService.setDebug(true);\n  }\n\n  // Build execution plan\n  const executionPlan = buildExecutionPlan(plan, absolutePath);\n\n  // Determine concurrency\n  const concurrency = options.concurrency ?? config.ai.concurrency;\n\n  // Enable subprocess output logging alongside tracing\n  if (options.trace) {\n    const logDir = path.join(\n      absolutePath, '.agents-reverse-engineer', 'subprocess-logs',\n      new Date().toISOString().replace(/[:.]/g, '-'),\n    );\n    aiService.setSubprocessLogDir(logDir);\n    console.error(pc.dim(`[trace] Subprocess logs → ${logDir}`));\n  }\n\n  // Create progress log for tail -f monitoring\n  const progressLog = ProgressLog.create(absolutePath);\n  progressLog.write(`=== ARE Generate (${new Date().toISOString()}) ===`);\n  progressLog.write(`Project: ${absolutePath}`);\n  progressLog.write(`Files: ${executionPlan.fileTasks.length} | Directories: ${executionPlan.directoryTasks.length}`);\n  progressLog.write('');\n\n  // Create command runner\n  const runner = new CommandRunner(aiService, {\n    concurrency,\n    failFast: options.failFast,\n    debug: options.debug,\n    tracer,\n    progressLog,\n  });\n\n  // Execute the three-phase pipeline\n  const summary = await runner.executeGenerate(executionPlan);\n\n  // Write telemetry run log\n  await aiService.finalize(absolutePath);\n\n  // Finalize trace, progress log, and clean up old trace files\n  await progressLog.finalize();\n  await tracer.finalize();\n  if (options.trace) {\n    await cleanupOldTraces(absolutePath);\n  }\n\n  // Determine exit code from RunSummary\n  //   0: all files succeeded\n  //   1: some files failed (partial failure)\n  //   2: no files succeeded (total failure)\n  if (summary.filesProcessed === 0 && summary.filesFailed > 0) {\n    process.exit(2);\n  } else if (summary.filesFailed > 0) {\n    process.exit(1);\n  }\n  // Exit code 0 -- all files succeeded (or no files to process)\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**generateCommand orchestrates three-phase AI-driven documentation generation with concurrent file analysis, directory aggregation, and root document synthesis via CommandRunner execution of GenerationPlan tasks.**\n\n## Exported Interface\n\n**generateCommand(targetPath: string, options: GenerateOptions): Promise<void>** — CLI entry point that discovers files, builds GenerationPlan via createOrchestrator, resolves AI backend, and executes three-phase pipeline through CommandRunner.executeGenerate.\n\n**GenerateOptions** — Configuration interface with fields:\n- `dryRun?: boolean` — Show execution plan without AI calls\n- `concurrency?: number` — Override worker pool size (1-10)\n- `failFast?: boolean` — Abort on first task failure\n- `debug?: boolean` — Enable subprocess logging with heap/RSS metrics\n- `trace?: boolean` — Emit NDJSON events to `.agents-reverse-engineer/traces/`\n\n**formatPlan(plan: GenerationPlan): string** — Formats GenerationPlan summary showing file count, task count, complexity metrics (fileCount, directoryDepth).\n\n## Execution Flow\n\n**generateCommand** coordinates six sequential phases:\n\n1. **Configuration Loading**: Calls loadConfig with tracer and debug options, returns Config with ai.backend, ai.concurrency, ai.timeoutMs, exclude patterns.\n\n2. **File Discovery**: Invokes discoverFiles with absolutePath and config, applies gitignore/binary/vendor filters, returns FilterResult with included/excluded arrays. Creates DiscoveryResult for orchestrator by mapping FilterResult.excluded to `{path, reason}` tuples.\n\n3. **Plan Creation**: Instantiates createOrchestrator with config and absolutePath, calls orchestrator.createPlan(discoveryResult) returning GenerationPlan with files[], tasks[], complexity. Logs formatPlan output showing file count, task count, complexity.fileCount, complexity.directoryDepth.\n\n4. **Dry-Run Bailout** (conditional): If options.dryRun, calls buildExecutionPlan(plan, absolutePath) returning ExecutionPlan with fileTasks[], directoryTasks[], rootTasks[], directoryFileMap. Logs summary: file count (executionPlan.fileTasks.length), directory count (Object.keys(executionPlan.directoryFileMap).length), root document count (executionPlan.rootTasks.length), estimated AI calls (executionPlan.tasks.length). Prints each fileTasks[].path via pc.dim, exits without AI calls.\n\n5. **Backend Resolution**: Calls createBackendRegistry() returning BackendRegistry, resolveBackend(registry, config.ai.backend) returning Backend. Catches AIServiceError with code 'CLI_NOT_FOUND', logs getInstallInstructions(registry), exits with code 2. If options.debug, logs backend.name, backend.cliCommand, config.ai.model.\n\n6. **AI Service Execution**: Instantiates AIService(backend, {timeoutMs, maxRetries, model, telemetry.keepRuns}). Calls aiService.setDebug(true) if options.debug. If options.trace, calls aiService.setSubprocessLogDir with path `.agents-reverse-engineer/subprocess-logs/<timestamp>`. Builds executionPlan via buildExecutionPlan. Creates ProgressLog.create(absolutePath), writes header with ISO timestamp, project path, file/directory counts. Instantiates CommandRunner(aiService, {concurrency, failFast, debug, tracer, progressLog}). Calls runner.executeGenerate(executionPlan) returning RunSummary. Calls aiService.finalize(absolutePath) for telemetry, progressLog.finalize(), tracer.finalize(), cleanupOldTraces(absolutePath) if options.trace.\n\n## Exit Code Strategy\n\nDerived from RunSummary fields (filesProcessed, filesFailed):\n- `process.exit(2)` — Total failure: filesProcessed === 0 && filesFailed > 0\n- `process.exit(1)` — Partial failure: filesFailed > 0 (but some files succeeded)\n- `process.exit(0)` — Success: filesFailed === 0 (implicit default)\n\n## Trace and Telemetry Integration\n\n**createTraceWriter** invoked with absolutePath and options.trace before loadConfig/discoverFiles to thread tracer through all subsystems. If options.trace && tracer.filePath, logs trace file path via pc.dim. Tracer passed to loadConfig, discoverFiles, createOrchestrator, CommandRunner constructor. cleanupOldTraces(absolutePath) called after tracer.finalize() to enforce retention limit (500 traces).\n\n**ProgressLog.create** instantiated with absolutePath, writes human-readable log to `.agents-reverse-engineer/progress.log` with header showing ISO timestamp, project path, file count (executionPlan.fileTasks.length), directory count (executionPlan.directoryTasks.length). Passed to CommandRunner for streaming updates. Finalized after runner.executeGenerate completes.\n\n**AIService.setSubprocessLogDir** called when options.trace enabled, writes per-subprocess stdout/stderr to `.agents-reverse-engineer/subprocess-logs/<timestamp>/<taskId>.log`. Logs directory path via pc.dim.\n\n## Concurrency Configuration\n\nDetermines worker pool size via `options.concurrency ?? config.ai.concurrency`. Config default: 2 for WSL environments (resource-constrained), 5 elsewhere. Passed to CommandRunner constructor as concurrency option.\n\n## Dependencies\n\n- `loadConfig` (src/config/loader.ts) — YAML config parsing with Zod validation\n- `discoverFiles` (src/discovery/run.ts) — Gitignore-aware file walking returning FilterResult\n- `createOrchestrator` (src/generation/orchestrator.ts) — Builds GenerationPlan with file/directory/root tasks\n- `buildExecutionPlan` (src/generation/executor.ts) — Transforms GenerationPlan into ExecutionPlan with phase breakdown\n- `AIService` (src/ai/service.ts) — Backend-agnostic AI service with subprocess management, retry logic, telemetry\n- `resolveBackend`, `createBackendRegistry`, `getInstallInstructions` (src/ai/index.ts) — Backend detection and CLI installation messaging\n- `CommandRunner` (src/orchestration/runner.ts) — Executes ExecutionPlan via worker pool with progress reporting\n- `createTraceWriter`, `cleanupOldTraces` (src/orchestration/trace.ts) — NDJSON trace emission and retention management\n- `ProgressLog` (src/orchestration/progress.ts) — Human-readable streaming log for tail -f monitoring\n- `createLogger` (src/output/logger.ts) — Picocolors-formatted console output\n\n## Error Handling\n\nAIServiceError with code 'CLI_NOT_FOUND' caught during resolveBackend: logs error message via pc.red, calls getInstallInstructions(registry) to show installation commands for Claude Code/Gemini/OpenCode, exits with code 2.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 1676,
      "cacheReadTokens": 17367,
      "cacheCreationTokens": 11433,
      "latencyMs": 37906,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/cli/generate.ts",
          "sizeBytes": 7977
        }
      ]
    },
    {
      "timestamp": "2026-02-09T17:44:19.760Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/cli/discover.ts\n\n```typescript\n/**\n * `are discover` command - Discover files to analyze\n *\n * Walks a directory tree and applies filters (gitignore, vendor, binary, custom)\n * to identify files suitable for analysis.\n */\n\nimport path from 'node:path';\nimport { access, mkdir, writeFile } from 'node:fs/promises';\nimport { constants } from 'node:fs';\nimport pc from 'picocolors';\nimport { loadConfig } from '../config/loader.js';\nimport { discoverFiles } from '../discovery/run.js';\nimport { createLogger } from '../output/logger.js';\nimport { createOrchestrator } from '../generation/orchestrator.js';\nimport { buildExecutionPlan, formatExecutionPlanAsMarkdown } from '../generation/executor.js';\nimport { ProgressLog } from '../orchestration/index.js';\nimport type { DiscoveryResult } from '../types/index.js';\nimport type { ITraceWriter } from '../orchestration/trace.js';\n\n/**\n * Options for the discover command.\n */\nexport interface DiscoverOptions {\n  /**\n   * Optional trace writer for emitting discovery events.\n   */\n  tracer?: ITraceWriter;\n\n  /**\n   * Enable debug output.\n   * @default false\n   */\n  debug?: boolean;\n}\n\n/**\n * Execute the `are discover` command.\n *\n * Discovers files in the target directory, applying all configured filters\n * (gitignore, vendor, binary, custom patterns).\n *\n * @param targetPath - Directory to scan (defaults to current working directory)\n * @param options - Command options\n *\n * @example\n * ```typescript\n * await discoverCommand('.', {});\n * ```\n */\nexport async function discoverCommand(\n  targetPath: string,\n  options: DiscoverOptions\n): Promise<void> {\n  // Resolve to absolute path (default to cwd)\n  const resolvedPath = path.resolve(targetPath || process.cwd());\n\n  // Load configuration (uses defaults if no config file)\n  const config = await loadConfig(resolvedPath);\n\n  const logger = createLogger({ colors: config.output.colors });\n\n  // Verify target path exists\n  try {\n    await access(resolvedPath, constants.R_OK);\n  } catch (err) {\n    const error = err as NodeJS.ErrnoException;\n    if (error.code === 'ENOENT') {\n      logger.error(`Directory not found: ${resolvedPath}`);\n      process.exit(1);\n    }\n    if (error.code === 'EACCES' || error.code === 'EPERM') {\n      logger.error(`Permission denied: ${resolvedPath}`);\n      process.exit(1);\n    }\n    throw error;\n  }\n\n  // Create progress log for tail -f monitoring\n  const progressLog = ProgressLog.create(resolvedPath);\n  progressLog.write(`=== ARE Discover (${new Date().toISOString()}) ===`);\n  progressLog.write(`Project: ${resolvedPath}`);\n  progressLog.write('');\n\n  logger.info(`Discovering files in ${resolvedPath}...`);\n  logger.info('');\n  progressLog.write(`Discovering files in ${resolvedPath}...`);\n\n  // Emit discovery start trace event\n  const discoveryStartTime = process.hrtime.bigint();\n  options.tracer?.emit({\n    type: 'discovery:start',\n    targetPath: resolvedPath,\n  });\n\n  if (options.debug) {\n    console.error(pc.dim(`[debug] Discovering files in: ${resolvedPath}`));\n  }\n\n  // Run shared discovery pipeline (walk + filter)\n  const result = await discoverFiles(resolvedPath, config, {\n    tracer: options.tracer,\n    debug: options.debug,\n  });\n\n  // Emit discovery end trace event\n  const discoveryEndTime = process.hrtime.bigint();\n  const discoveryDurationMs = Number(discoveryEndTime - discoveryStartTime) / 1_000_000;\n  options.tracer?.emit({\n    type: 'discovery:end',\n    filesIncluded: result.included.length,\n    filesExcluded: result.excluded.length,\n    durationMs: discoveryDurationMs,\n  });\n\n  if (options.debug) {\n    console.error(\n      pc.dim(\n        `[debug] Discovery complete: ${result.included.length} files included, ${result.excluded.length} excluded`\n      )\n    );\n  }\n\n  // Log results\n  // Make paths relative for cleaner output\n  const relativePath = (absPath: string): string =>\n    path.relative(resolvedPath, absPath);\n\n  // Show each included file\n  for (const file of result.included) {\n    const rel = relativePath(file);\n    logger.file(rel);\n    progressLog.write(`  + ${rel}`);\n  }\n\n  // Show each excluded file\n  for (const excluded of result.excluded) {\n    const rel = relativePath(excluded.path);\n    logger.excluded(rel, excluded.reason, excluded.filter);\n    progressLog.write(`  - ${rel} (${excluded.reason}: ${excluded.filter})`);\n  }\n\n  // Summary\n  logger.summary(result.included.length, result.excluded.length);\n  progressLog.write(`\\nDiscovered ${result.included.length} files (${result.excluded.length} excluded)`);\n\n  // Generate GENERATION-PLAN.md\n  {\n    logger.info('');\n    logger.info('Generating execution plan...');\n    progressLog.write('');\n    progressLog.write('Generating execution plan...');\n\n    // Create discovery result for orchestrator\n    const discoveryResult: DiscoveryResult = {\n      files: result.included,\n      excluded: result.excluded.map(e => ({ path: e.path, reason: e.reason })),\n    };\n\n    // Create orchestrator and build generation plan\n    const orchestrator = createOrchestrator(config, resolvedPath);\n    const generationPlan = await orchestrator.createPlan(discoveryResult);\n\n    // Build execution plan with post-order traversal\n    const executionPlan = buildExecutionPlan(generationPlan, resolvedPath);\n\n    // Format as markdown\n    const markdown = formatExecutionPlanAsMarkdown(executionPlan);\n\n    // Write to .agents-reverse-engineer/GENERATION-PLAN.md\n    const configDir = path.join(resolvedPath, '.agents-reverse-engineer');\n    const planPath = path.join(configDir, 'GENERATION-PLAN.md');\n\n    try {\n      await mkdir(configDir, { recursive: true });\n      await writeFile(planPath, markdown, 'utf8');\n      const planRelPath = path.relative(resolvedPath, planPath);\n      logger.info(`Created ${planRelPath}`);\n      progressLog.write(`Created ${planRelPath}`);\n    } catch (err) {\n      const msg = (err as Error).message;\n      logger.error(`Failed to write plan: ${msg}`);\n      progressLog.write(`Error: Failed to write plan: ${msg}`);\n      await progressLog.finalize();\n      process.exit(1);\n    }\n  }\n\n  await progressLog.finalize();\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**`discover.ts` executes the `are discover` command which walks the target directory applying gitignore/vendor/binary/custom filters, writes discovered files to `progress.log`, generates `GENERATION-PLAN.md` with post-order directory traversal, and emits trace events for discovery lifecycle.**\n\n## Exported Interface\n\n**`discoverCommand(targetPath: string, options: DiscoverOptions): Promise<void>`** — Main entry point for the `are discover` command. Resolves `targetPath` to absolute path (defaults to `process.cwd()`), loads configuration via `loadConfig()`, verifies directory accessibility, runs `discoverFiles()` discovery pipeline, logs included/excluded files to console and `ProgressLog`, creates `GenerationPlan` via `orchestrator.createPlan()`, builds `ExecutionPlan` via `buildExecutionPlan()`, formats as markdown via `formatExecutionPlanAsMarkdown()`, and writes to `.agents-reverse-engineer/GENERATION-PLAN.md`.\n\n**`DiscoverOptions`** — Interface with optional `tracer?: ITraceWriter` for trace event emission and `debug?: boolean` flag for verbose output (defaults to `false`).\n\n## Workflow Steps\n\n1. **Path Resolution**: Converts `targetPath` to absolute via `path.resolve()`, falling back to `process.cwd()` if empty\n2. **Configuration Loading**: Calls `loadConfig(resolvedPath)` to load `.agents-reverse-engineer/config.yaml` or use defaults\n3. **Access Verification**: Uses `fs.access()` with `constants.R_OK` to verify directory readability, exits on `ENOENT`/`EACCES`/`EPERM` errors\n4. **Progress Log Initialization**: Creates `ProgressLog` via `ProgressLog.create(resolvedPath)` for tail monitoring, writes header with ISO 8601 timestamp\n5. **Discovery Execution**: Calls `discoverFiles(resolvedPath, config, { tracer, debug })` to run filter chain\n6. **Trace Emission**: Emits `discovery:start` before discovery, `discovery:end` after with `filesIncluded`/`filesExcluded`/`durationMs` fields using `process.hrtime.bigint()` for nanosecond precision\n7. **Result Logging**: Iterates `result.included` logging via `logger.file()` and `progressLog.write()` with relative paths, iterates `result.excluded` logging via `logger.excluded()` with `reason` and `filter` fields\n8. **Plan Generation**: Creates `DiscoveryResult` object wrapping `files` and `excluded` arrays, calls `createOrchestrator(config, resolvedPath)`, invokes `orchestrator.createPlan(discoveryResult)`, calls `buildExecutionPlan()` for post-order directory sorting, formats via `formatExecutionPlanAsMarkdown()`\n9. **Plan Persistence**: Creates `.agents-reverse-engineer/` directory via `mkdir({ recursive: true })`, writes markdown to `GENERATION-PLAN.md` via `writeFile()`, logs relative path via `logger.info()` and `progressLog.write()`\n10. **Finalization**: Calls `progressLog.finalize()` to flush buffered writes\n\n## Error Handling\n\n**Access Errors**: Catches `fs.access()` exceptions, pattern-matches `error.code` against `'ENOENT'`/`'EACCES'`/`'EPERM'`, logs via `logger.error()`, calls `process.exit(1)` for known codes, rethrows unknown errors.\n\n**Plan Write Errors**: Catches `writeFile()` exceptions, extracts `(err as Error).message`, logs via `logger.error()` and `progressLog.write()`, finalizes progress log, calls `process.exit(1)`.\n\n## Dependencies\n\n**Internal Modules**:\n- `../config/loader.js` — `loadConfig()` for YAML config loading\n- `../discovery/run.js` — `discoverFiles()` for filter chain execution\n- `../output/logger.js` — `createLogger()` for terminal output with picocolors\n- `../generation/orchestrator.js` — `createOrchestrator()` for generation plan creation\n- `../generation/executor.js` — `buildExecutionPlan()` and `formatExecutionPlanAsMarkdown()` for post-order traversal\n- `../orchestration/index.js` — `ProgressLog` for tail monitoring\n- `../types/index.js` — `DiscoveryResult` interface\n- `../orchestration/trace.js` — `ITraceWriter` interface\n\n**Node.js APIs**:\n- `node:path` — Path resolution and relative path computation\n- `node:fs/promises` — `access()`, `mkdir()`, `writeFile()` for async file I/O\n- `node:fs` — `constants.R_OK` for access mode flags\n- `picocolors` — Terminal color formatting via `pc.dim()`\n\n## Trace Events\n\n**`discovery:start`** — Emitted before `discoverFiles()` with `targetPath: string` field.\n\n**`discovery:end`** — Emitted after `discoverFiles()` with `filesIncluded: number`, `filesExcluded: number`, `durationMs: number` fields. Duration computed via `process.hrtime.bigint()` nanosecond delta converted to milliseconds by dividing by `1_000_000`.\n\n## Output Behavior\n\n**Console Logging**: Uses `logger.file()` for included paths, `logger.excluded(path, reason, filter)` for excluded paths, `logger.summary(includedCount, excludedCount)` for summary, `logger.info()` for plan creation, `logger.error()` for failures.\n\n**Progress Log**: Mirrors console output to `.agents-reverse-engineer/progress.log` with prefix patterns: `+ ` for included files, `- ` for excluded files with parenthesized reason and filter.\n\n**Debug Output**: When `options.debug` is truthy, writes to `console.error()` with `pc.dim()` formatting for `[debug]` prefixed messages showing target path, file counts, and discovery completion.\n\n## Path Handling\n\n**Relative Path Computation**: Defines local `relativePath(absPath: string): string` helper using `path.relative(resolvedPath, absPath)` to convert absolute discovery results to project-relative paths for cleaner output.\n\n**Plan Path Construction**: Builds plan file path via `path.join(resolvedPath, '.agents-reverse-engineer', 'GENERATION-PLAN.md')`, computes relative path for logging via `path.relative(resolvedPath, planPath)`.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 1593,
      "cacheReadTokens": 17367,
      "cacheCreationTokens": 10888,
      "latencyMs": 40342,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/cli/discover.ts",
          "sizeBytes": 6071
        }
      ]
    },
    {
      "timestamp": "2026-02-09T17:44:44.459Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/generation/complexity.ts\n\n```typescript\nimport * as path from 'node:path';\n\n/**\n * Metrics about codebase complexity.\n */\nexport interface ComplexityMetrics {\n  /** Total number of source files */\n  fileCount: number;\n  /** Maximum directory depth */\n  directoryDepth: number;\n  /** List of source file paths */\n  files: string[];\n  /** Unique directory paths */\n  directories: Set<string>;\n}\n\n/**\n * Calculate maximum directory depth from file paths.\n */\nfunction calculateDirectoryDepth(files: string[], projectRoot: string): number {\n  let maxDepth = 0;\n\n  for (const file of files) {\n    const relativePath = path.relative(projectRoot, file);\n    const depth = relativePath.split(path.sep).length - 1; // -1 for the file itself\n    maxDepth = Math.max(maxDepth, depth);\n  }\n\n  return maxDepth;\n}\n\n/**\n * Extract unique directories from file paths.\n */\nfunction extractDirectories(files: string[]): Set<string> {\n  const directories = new Set<string>();\n\n  for (const file of files) {\n    let dir = path.dirname(file);\n    while (dir && dir !== '.') {\n      directories.add(dir);\n      const parent = path.dirname(dir);\n      if (parent === dir) break; // Reached root\n      dir = parent;\n    }\n  }\n\n  return directories;\n}\n\n/**\n * Analyze codebase complexity from discovered files.\n *\n * @param files - List of source file paths\n * @param projectRoot - Project root directory\n * @returns Complexity metrics\n */\nexport function analyzeComplexity(\n  files: string[],\n  projectRoot: string\n): ComplexityMetrics {\n  return {\n    fileCount: files.length,\n    directoryDepth: calculateDirectoryDepth(files, projectRoot),\n    files,\n    directories: extractDirectories(files),\n  };\n}\n\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**complexity.ts computes codebase structure metrics from discovered file paths: file count, maximum directory depth, unique directory set, and file list for generation phase cost estimation and complexity-aware orchestration.**\n\n## Exported Interface\n\n`ComplexityMetrics` — structure containing `fileCount: number`, `directoryDepth: number`, `files: string[]`, and `directories: Set<string>` computed from discovered source files.\n\n## Exported Function\n\n`analyzeComplexity(files: string[], projectRoot: string): ComplexityMetrics` — orchestrates complexity analysis by invoking `calculateDirectoryDepth()` and `extractDirectories()`, returning aggregate metrics used by generation orchestrator to estimate Phase 2 directory aggregation workload and log complexity warnings.\n\n## Algorithm Details\n\n`calculateDirectoryDepth(files: string[], projectRoot: string): number` — computes maximum directory nesting by calling `path.relative()` on each file path against `projectRoot`, splitting by `path.sep`, subtracting 1 (file itself not counted), and tracking max depth across all files via `Math.max()`.\n\n`extractDirectories(files: string[]): Set<string>` — extracts unique directory paths by iterating each file, calling `path.dirname()` on file path, then walking upward via repeated `path.dirname(parent)` until `parent === dir` (root reached) or `dir === '.'`, adding each intermediate directory to `Set<string>`.\n\n## Integration Points\n\nCalled by `src/generation/orchestrator.ts` after discovery phase completes, metrics passed to `FileAnalysisOrchestrator` constructor for logging complexity warnings (e.g., \"Large codebase detected: N files across M directories, depth D\"). `directories` set used to validate Phase 2 post-order traversal completeness and detect orphaned `AGENTS.md` files during incremental updates.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 428,
      "cacheReadTokens": 17367,
      "cacheCreationTokens": 9577,
      "latencyMs": 15945,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/generation/complexity.ts",
          "sizeBytes": 1642
        }
      ]
    },
    {
      "timestamp": "2026-02-09T17:44:09.568Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/ai/service.ts\n\n```typescript\n/**\n * AI service orchestrator.\n *\n * The {@link AIService} class is the main entry point for making AI calls.\n * It ties together the subprocess wrapper, retry logic, backend selection,\n * and telemetry logging into a clean `call()` method.\n *\n * @module\n */\n\nimport { writeFile, mkdir } from 'node:fs/promises';\nimport path from 'node:path';\nimport type { AIBackend, AICallOptions, AIResponse, SubprocessResult, TelemetryEntry, RunLog, FileRead } from './types.js';\nimport { AIServiceError } from './types.js';\nimport { runSubprocess } from './subprocess.js';\nimport { withRetry, DEFAULT_RETRY_OPTIONS } from './retry.js';\nimport { TelemetryLogger } from './telemetry/logger.js';\nimport { writeRunLog } from './telemetry/run-log.js';\nimport { cleanupOldLogs } from './telemetry/cleanup.js';\nimport type { ITraceWriter } from '../orchestration/trace.js';\n\n// ---------------------------------------------------------------------------\n// Rate-limit detection patterns\n// ---------------------------------------------------------------------------\n\n/** Patterns in stderr that indicate a transient rate-limit error */\nconst RATE_LIMIT_PATTERNS = [\n  'rate limit',\n  '429',\n  'too many requests',\n  'overloaded',\n];\n\n/**\n * Check whether stderr text contains rate-limit indicators.\n *\n * @param stderr - Standard error output from the subprocess\n * @returns `true` if any rate-limit pattern matches (case-insensitive)\n */\nfunction isRateLimitStderr(stderr: string): boolean {\n  const lower = stderr.toLowerCase();\n  return RATE_LIMIT_PATTERNS.some((pattern) => lower.includes(pattern));\n}\n\n/**\n * Format bytes as a human-readable string.\n */\nfunction formatBytes(bytes: number): string {\n  if (bytes < 1024) return `${bytes}B`;\n  if (bytes < 1024 * 1024) return `${(bytes / 1024).toFixed(1)}KB`;\n  return `${(bytes / (1024 * 1024)).toFixed(1)}MB`;\n}\n\n// ---------------------------------------------------------------------------\n// AIService options\n// ---------------------------------------------------------------------------\n\n/**\n * Configuration options for the {@link AIService}.\n *\n * These are typically sourced from the config schema's `ai` section.\n */\nexport interface AIServiceOptions {\n  /** Default subprocess timeout in milliseconds */\n  timeoutMs: number;\n  /** Maximum number of retries for transient errors */\n  maxRetries: number;\n  /** Default model identifier (e.g., \"sonnet\", \"opus\") applied to all calls unless overridden per-call */\n  model?: string;\n  /** Telemetry settings */\n  telemetry: {\n    /** Number of most recent run logs to keep on disk */\n    keepRuns: number;\n  };\n}\n\n// ---------------------------------------------------------------------------\n// AIService\n// ---------------------------------------------------------------------------\n\n/**\n * Orchestrates AI CLI calls with retry, timeout, and telemetry.\n *\n * Create one instance per CLI run. Call {@link call} for each AI invocation.\n * Call {@link finalize} at the end to write the run log and clean up old files.\n *\n * @example\n * ```typescript\n * import { AIService } from './service.js';\n * import { resolveBackend, createBackendRegistry } from './registry.js';\n *\n * const registry = createBackendRegistry();\n * const backend = await resolveBackend(registry, 'auto');\n * const service = new AIService(backend, {\n *   timeoutMs: 120_000,\n *   maxRetries: 3,\n *   telemetry: { keepRuns: 10 },\n * });\n *\n * const response = await service.call({ prompt: 'Summarize this codebase' });\n * console.log(response.text);\n *\n * const { logPath, summary } = await service.finalize('/path/to/project');\n * console.log(`Log written to ${logPath}`);\n * ```\n */\nexport class AIService {\n  /** The backend adapter used for CLI invocations */\n  private readonly backend: AIBackend;\n\n  /** Service configuration */\n  private readonly options: AIServiceOptions;\n\n  /** In-memory telemetry logger for this run */\n  private readonly logger: TelemetryLogger;\n\n  /** Running count of calls made (used for entry tracking) */\n  private callCount: number = 0;\n\n  /** Trace writer for concurrency debugging (may be no-op) */\n  private tracer: ITraceWriter | null = null;\n\n  /** Whether debug mode is enabled */\n  private debug: boolean = false;\n\n  /** Number of currently active subprocesses */\n  private activeSubprocesses: number = 0;\n\n  /** Directory for subprocess output logs (null = disabled) */\n  private subprocessLogDir: string | null = null;\n\n  /** Serializes log writes so concurrent workers don't interleave mkdirs */\n  private logWriteQueue: Promise<void> = Promise.resolve();\n\n  /**\n   * Create a new AI service instance.\n   *\n   * @param backend - The resolved backend adapter\n   * @param options - Service configuration (timeout, retries, telemetry)\n   */\n  constructor(backend: AIBackend, options: AIServiceOptions) {\n    this.backend = backend;\n    this.options = options;\n    this.logger = new TelemetryLogger(new Date().toISOString());\n  }\n\n  /**\n   * Set the trace writer for subprocess and retry event tracing.\n   *\n   * @param tracer - The trace writer instance\n   */\n  setTracer(tracer: ITraceWriter): void {\n    this.tracer = tracer;\n  }\n\n  /**\n   * Enable debug mode for verbose subprocess logging to stderr.\n   */\n  setDebug(enabled: boolean): void {\n    this.debug = enabled;\n  }\n\n  /**\n   * Set a directory for writing subprocess stdout/stderr log files.\n   *\n   * When set, each subprocess invocation writes a `.log` file containing\n   * the metadata header, stdout, and stderr. Useful for diagnosing\n   * timed-out or failed subprocesses whose output would otherwise be lost.\n   *\n   * @param dir - Absolute path to the log directory (created on first write)\n   */\n  setSubprocessLogDir(dir: string): void {\n    this.subprocessLogDir = dir;\n  }\n\n  /**\n   * Make an AI call with retry logic and telemetry recording.\n   *\n   * The call flow:\n   * 1. Build CLI args via the backend adapter\n   * 2. Wrap the subprocess invocation in retry logic\n   * 3. On success: parse response via backend, record telemetry entry\n   * 4. On failure: record error telemetry entry, throw the error\n   *\n   * Retries are attempted for `RATE_LIMIT` and `TIMEOUT` errors only.\n   * All other errors are treated as permanent failures.\n   *\n   * @param options - The call options (prompt, model, timeout, etc.)\n   * @returns The normalized AI response\n   * @throws {AIServiceError} On timeout, rate limit exhaustion, parse error, or subprocess failure\n   */\n  async call(options: AICallOptions): Promise<AIResponse> {\n    this.callCount++;\n    const callStart = Date.now();\n    const timestamp = new Date().toISOString();\n    const taskLabel = options.taskLabel ?? 'unknown';\n\n    // Merge service-level model as default (per-call options.model wins)\n    const effectiveOptions: AICallOptions = {\n      ...options,\n      model: options.model ?? this.options.model,\n    };\n\n    const args = this.backend.buildArgs(effectiveOptions);\n    const timeoutMs = options.timeoutMs ?? this.options.timeoutMs;\n\n    let retryCount = 0;\n\n    try {\n      const response = await withRetry(\n        async () => {\n          if (this.debug) {\n            const mem = process.memoryUsage();\n            console.error(\n              `[debug] Spawning subprocess for \"${taskLabel}\" ` +\n              `(active: ${this.activeSubprocesses}, ` +\n              `heapUsed: ${formatBytes(mem.heapUsed)}, ` +\n              `rss: ${formatBytes(mem.rss)}, ` +\n              `timeout: ${(timeoutMs / 1000).toFixed(0)}s)`,\n            );\n          }\n\n          this.activeSubprocesses++;\n\n          const result = await runSubprocess(this.backend.cliCommand, args, {\n            timeoutMs,\n            input: options.prompt,\n            onSpawn: (pid) => {\n              // Emit subprocess:spawn at actual spawn time (not after completion)\n              this.tracer?.emit({\n                type: 'subprocess:spawn',\n                childPid: pid ?? -1,\n                command: this.backend.cliCommand,\n                taskLabel,\n              });\n            },\n          });\n\n          this.activeSubprocesses--;\n\n          // Emit subprocess:exit after completion\n          if (this.tracer && result.childPid !== undefined) {\n            this.tracer.emit({\n              type: 'subprocess:exit',\n              childPid: result.childPid,\n              command: this.backend.cliCommand,\n              taskLabel,\n              exitCode: result.exitCode,\n              signal: result.signal,\n              durationMs: result.durationMs,\n              timedOut: result.timedOut,\n            });\n          }\n\n          // Write subprocess output log (fire-and-forget, non-critical)\n          this.enqueueSubprocessLog(result, taskLabel);\n\n          if (result.timedOut) {\n            console.error(\n              `[warn] Subprocess timed out after ${(result.durationMs / 1000).toFixed(1)}s ` +\n              `for \"${taskLabel}\" (PID ${result.childPid ?? 'unknown'}, ` +\n              `timeout was ${(timeoutMs / 1000).toFixed(0)}s)`,\n            );\n            throw new AIServiceError('TIMEOUT', 'Subprocess timed out');\n          }\n\n          if (this.debug) {\n            console.error(\n              `[debug] Subprocess exited for \"${taskLabel}\" ` +\n              `(PID ${result.childPid ?? 'unknown'}, ` +\n              `exitCode: ${result.exitCode}, ` +\n              `duration: ${(result.durationMs / 1000).toFixed(1)}s, ` +\n              `active: ${this.activeSubprocesses})`,\n            );\n          }\n\n          if (result.exitCode !== 0) {\n            if (isRateLimitStderr(result.stderr)) {\n              throw new AIServiceError(\n                'RATE_LIMIT',\n                `Rate limited by ${this.backend.name}: ${result.stderr.slice(0, 200)}`,\n              );\n            }\n            throw new AIServiceError(\n              'SUBPROCESS_ERROR',\n              `${this.backend.name} CLI exited with code ${result.exitCode}: ${result.stderr.slice(0, 500)}`,\n            );\n          }\n\n          // Parse the response -- wrap in try/catch for parse errors\n          try {\n            return this.backend.parseResponse(result.stdout, result.durationMs, result.exitCode);\n          } catch (error) {\n            if (error instanceof AIServiceError) {\n              throw error;\n            }\n            const message = error instanceof Error ? error.message : String(error);\n            throw new AIServiceError('PARSE_ERROR', `Failed to parse response: ${message}`);\n          }\n        },\n        {\n          ...DEFAULT_RETRY_OPTIONS,\n          maxRetries: this.options.maxRetries,\n          isRetryable: (error: unknown): boolean => {\n            // Only retry rate limits. Timeouts are NOT retried because\n            // spawning another heavyweight subprocess on a system that's\n            // already struggling (or against an unresponsive API) makes\n            // things worse and can exhaust system resources.\n            return (\n              error instanceof AIServiceError &&\n              error.code === 'RATE_LIMIT'\n            );\n          },\n          onRetry: (attempt: number, error: unknown) => {\n            retryCount++;\n\n            const errorCode = error instanceof AIServiceError ? error.code : 'UNKNOWN';\n\n            // Always warn on retry (not just debug) -- retries are noteworthy\n            console.error(\n              `[warn] Retrying \"${taskLabel}\" (attempt ${attempt}/${this.options.maxRetries}, reason: ${errorCode})`,\n            );\n\n            // Emit retry trace event\n            if (this.tracer) {\n              this.tracer.emit({\n                type: 'retry',\n                attempt,\n                taskLabel,\n                errorCode,\n              });\n            }\n          },\n        },\n      );\n\n      // Record successful call\n      this.logger.addEntry({\n        timestamp,\n        prompt: options.prompt,\n        systemPrompt: options.systemPrompt,\n        response: response.text,\n        model: response.model,\n        inputTokens: response.inputTokens,\n        outputTokens: response.outputTokens,\n        cacheReadTokens: response.cacheReadTokens,\n        cacheCreationTokens: response.cacheCreationTokens,\n        latencyMs: response.durationMs,\n        exitCode: response.exitCode,\n        retryCount,\n        thinking: 'not supported',\n        filesRead: [],\n      });\n\n      return response;\n    } catch (error) {\n      // Record failed call\n      const latencyMs = Date.now() - callStart;\n      const errorMessage = error instanceof Error ? error.message : String(error);\n\n      this.logger.addEntry({\n        timestamp,\n        prompt: options.prompt,\n        systemPrompt: options.systemPrompt,\n        response: '',\n        model: options.model ?? 'unknown',\n        inputTokens: 0,\n        outputTokens: 0,\n        cacheReadTokens: 0,\n        cacheCreationTokens: 0,\n        latencyMs,\n        exitCode: 1,\n        error: errorMessage,\n        retryCount,\n        thinking: 'not supported',\n        filesRead: [],\n      });\n\n      throw error;\n    }\n  }\n\n  /**\n   * Finalize the run: write the run log to disk and clean up old files.\n   *\n   * Call this once at the end of a CLI invocation, after all `call()`\n   * invocations have completed (or failed).\n   *\n   * @param projectRoot - Absolute path to the project root directory\n   * @returns The log file path and the run summary\n   */\n  async finalize(projectRoot: string): Promise<{ logPath: string; summary: RunLog['summary'] }> {\n    const runLog = this.logger.toRunLog();\n    const logPath = await writeRunLog(projectRoot, runLog);\n    await cleanupOldLogs(projectRoot, this.options.telemetry.keepRuns);\n    return { logPath, summary: runLog.summary };\n  }\n\n  /**\n   * Attach file-read metadata to the most recent telemetry entry.\n   *\n   * Called by the command runner after an AI call completes, to record\n   * which source files were sent as context for that call.\n   *\n   * @param filesRead - Array of file-read records (path + size)\n   */\n  addFilesReadToLastEntry(filesRead: FileRead[]): void {\n    this.logger.setFilesReadOnLastEntry(filesRead);\n  }\n\n  /**\n   * Get the current run summary without finalizing.\n   *\n   * Useful for displaying progress during a run.\n   *\n   * @returns Current summary statistics\n   */\n  getSummary(): RunLog['summary'] {\n    return this.logger.getSummary();\n  }\n\n  /**\n   * Enqueue a subprocess output log write.\n   *\n   * Serializes writes via a promise chain to avoid concurrent mkdir races.\n   * Failures are silently swallowed -- log writing is non-critical.\n   */\n  private enqueueSubprocessLog(result: SubprocessResult, taskLabel: string): void {\n    if (this.subprocessLogDir === null) return;\n\n    const dir = this.subprocessLogDir;\n    const sanitized = taskLabel.replace(/\\//g, '--').replace(/[^a-zA-Z0-9._-]/g, '_');\n    const filename = `${sanitized}_pid${result.childPid ?? 0}.log`;\n    const filePath = path.join(dir, filename);\n\n    const content =\n      `task:      ${taskLabel}\\n` +\n      `pid:       ${result.childPid ?? 'unknown'}\\n` +\n      `command:   ${this.backend.cliCommand}\\n` +\n      `exit:      ${result.exitCode}\\n` +\n      `signal:    ${result.signal ?? 'none'}\\n` +\n      `duration:  ${result.durationMs}ms\\n` +\n      `timed_out: ${result.timedOut}\\n` +\n      `\\n--- stdout ---\\n` +\n      result.stdout +\n      `\\n--- stderr ---\\n` +\n      result.stderr;\n\n    this.logWriteQueue = this.logWriteQueue\n      .then(async () => {\n        await mkdir(dir, { recursive: true });\n        await writeFile(filePath, content, 'utf-8');\n      })\n      .catch(() => { /* non-critical -- log loss is acceptable */ });\n  }\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**AIService orchestrates AI CLI subprocess execution with retry logic, telemetry logging, timeout enforcement, and rate-limit detection for agents-reverse-engineer's three-phase documentation pipeline.**\n\n## Exported Interface\n\n**class AIService** — Main entry point for AI calls with integrated subprocess management, exponential backoff retry, telemetry accumulation, and run log finalization.\n\n**constructor(backend: AIBackend, options: AIServiceOptions)** — Initializes service with backend adapter (Claude/Gemini/OpenCode) and configuration for timeouts, retries, telemetry retention.\n\n**async call(options: AICallOptions): Promise<AIResponse>** — Executes AI call with retry wrapper around `runSubprocess()`, records `TelemetryEntry` on success/failure, merges service-level `model` default with per-call override, emits `subprocess:spawn/exit` and `retry` trace events via `ITraceWriter`.\n\n**async finalize(projectRoot: string): Promise<{ logPath: string; summary: RunLog['summary'] }>** — Writes accumulated run log via `writeRunLog()`, invokes `cleanupOldLogs()` for retention enforcement, returns log path and summary statistics.\n\n**setTracer(tracer: ITraceWriter): void** — Attaches trace writer for subprocess lifecycle and retry event emission.\n\n**setDebug(enabled: boolean): void** — Enables stderr logging of spawn metadata (active subprocess count, heap/RSS, timeout, PID, exit code, duration).\n\n**setSubprocessLogDir(dir: string): void** — Enables per-subprocess `.log` file writes containing task label, PID, command, exit code, signal, duration, stdout, stderr.\n\n**addFilesReadToLastEntry(filesRead: FileRead[]): void** — Attaches file-read metadata to most recent `TelemetryEntry` via `TelemetryLogger.setFilesReadOnLastEntry()`.\n\n**getSummary(): RunLog['summary']** — Returns current aggregated statistics without finalizing.\n\n**interface AIServiceOptions** — Service configuration with `timeoutMs`, `maxRetries`, optional `model`, and nested `telemetry.keepRuns`.\n\n## Call Execution Flow\n\n`call()` increments `callCount`, merges effective options (service-level `model` as default), builds CLI args via `backend.buildArgs()`, wraps `runSubprocess()` in `withRetry()` configured to retry only `RATE_LIMIT` errors (timeouts are NOT retried per MEMORY.md resource constraint rationale), increments/decrements `activeSubprocesses` counter before/after subprocess completion, emits `subprocess:spawn` via `onSpawn` callback at actual spawn time (not after completion), emits `subprocess:exit` after completion with `childPid`, `exitCode`, `signal`, `durationMs`, `timedOut`, invokes `enqueueSubprocessLog()` fire-and-forget for optional log writes, parses response via `backend.parseResponse()` wrapped in try-catch throwing `AIServiceError('PARSE_ERROR')` on parse failure, records `TelemetryEntry` with prompt, systemPrompt, response, model, tokens (input/output/cacheRead/cacheCreation), latencyMs, exitCode, retryCount, thinking='not supported', filesRead=[] (populated later via `addFilesReadToLastEntry`), throws `AIServiceError('TIMEOUT')` on `SubprocessResult.timedOut`, throws `AIServiceError('RATE_LIMIT')` when stderr matches patterns via `isRateLimitStderr()`, throws `AIServiceError('SUBPROCESS_ERROR')` on non-zero exit codes not matching rate-limit patterns.\n\n## Rate Limit Detection\n\n**const RATE_LIMIT_PATTERNS = ['rate limit', '429', 'too many requests', 'overloaded']** — Substring patterns for detecting transient rate-limit errors in subprocess stderr.\n\n**function isRateLimitStderr(stderr: string): boolean** — Case-insensitive substring search across `RATE_LIMIT_PATTERNS`.\n\n## Retry Configuration\n\n`withRetry()` invoked with spread `DEFAULT_RETRY_OPTIONS` (maxRetries=3, baseDelayMs=1000, maxDelayMs=8000, multiplier=2) plus custom `isRetryable` predicate accepting only `AIServiceError.code === 'RATE_LIMIT'` (timeouts excluded per resource constraint mitigation), `onRetry` callback emitting `console.error()` warnings and `tracer.emit({ type: 'retry', attempt, taskLabel, errorCode })` events.\n\n## Subprocess Logging\n\n**private enqueueSubprocessLog(result: SubprocessResult, taskLabel: string): void** — Fire-and-forget serialized write via `logWriteQueue` promise chain, sanitizes taskLabel by replacing `/` with `--` and non-alphanumeric chars with `_`, writes `${sanitized}_pid${childPid}.log` containing metadata header (task/pid/command/exit/signal/duration/timed_out) plus stdout/stderr sections, silently swallows errors (log loss acceptable).\n\n## Debug Output\n\nWhen `debug=true`, `call()` logs to stderr before spawn (active count, heap/RSS via `formatBytes()`, timeout), after exit (PID, exitCode, duration, active count), plus timeout warnings showing elapsed time exceeding configured `timeoutMs`.\n\n**function formatBytes(bytes: number): string** — Human-readable byte formatting with B/KB/MB units.\n\n## Telemetry Accumulation\n\n**private readonly logger: TelemetryLogger** — In-memory accumulator instantiated with `runId = new Date().toISOString()`.\n\n**private callCount: number = 0** — Running total of `call()` invocations.\n\n**private activeSubprocesses: number = 0** — Tracks concurrent subprocess count for debug logging.\n\n**private subprocessLogDir: string | null = null** — Optional directory for subprocess output logs.\n\n**private logWriteQueue: Promise<void> = Promise.resolve()** — Serializes `mkdir()` and `writeFile()` operations to prevent race conditions from concurrent workers.\n\n## Dependencies\n\nImports `AIBackend`, `AICallOptions`, `AIResponse`, `SubprocessResult`, `TelemetryEntry`, `RunLog`, `FileRead`, `AIServiceError` from `./types.js`; `runSubprocess` from `./subprocess.js`; `withRetry`, `DEFAULT_RETRY_OPTIONS` from `./retry.js`; `TelemetryLogger` from `./telemetry/logger.js`; `writeRunLog` from `./telemetry/run-log.js`; `cleanupOldLogs` from `./telemetry/cleanup.js`; `ITraceWriter` from `../orchestration/trace.js`; `writeFile`, `mkdir` from `node:fs/promises`; `path` from `node:path`.\n\n## Integration Pattern\n\nUsed by `src/orchestration/runner.ts` which creates one `AIService` instance per CLI command execution, calls `setTracer()` and `setDebug()` based on CLI flags, invokes `call()` for each file/directory/root task via worker pool or sequential executor, calls `addFilesReadToLastEntry()` after each call to attach file metadata from prompt builder, calls `finalize()` at end of run to persist telemetry and enforce retention limits.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 10,
      "outputTokens": 2165,
      "cacheReadTokens": 48190,
      "cacheCreationTokens": 29949,
      "latencyMs": 53349,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/ai/service.ts",
          "sizeBytes": 15566
        }
      ]
    },
    {
      "timestamp": "2026-02-09T17:44:44.259Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/generation/collector.ts\n\n```typescript\nimport * as path from 'node:path';\nimport { readdir, readFile } from 'node:fs/promises';\n\n/**\n * A collected AGENTS.md document with its project-relative path and content.\n */\nexport type AgentsDocs = Array<{ relativePath: string; content: string }>;\n\n/** Directories to skip during recursive AGENTS.md collection. */\nconst SKIP_DIRS = new Set([\n  'node_modules', '.git', '.agents-reverse-engineer',\n  'vendor', 'dist', 'build', '__pycache__', '.next',\n  'venv', '.venv', 'target', '.cargo', '.gradle',\n]);\n\n/**\n * Recursively collect all AGENTS.md files under `projectRoot`,\n * returning their relative paths and content sorted alphabetically.\n *\n * Skips vendor/build/meta directories and gracefully handles\n * unreadable directories or files.\n */\nexport async function collectAgentsDocs(projectRoot: string): Promise<AgentsDocs> {\n  const results: AgentsDocs = [];\n\n  async function walk(currentDir: string): Promise<void> {\n    let entries;\n    try {\n      entries = await readdir(currentDir, { withFileTypes: true });\n    } catch {\n      return; // Permission denied or inaccessible\n    }\n    for (const entry of entries) {\n      if (entry.isDirectory() && !SKIP_DIRS.has(entry.name)) {\n        await walk(path.join(currentDir, entry.name));\n      } else if (entry.name === 'AGENTS.md') {\n        try {\n          const filePath = path.join(currentDir, entry.name);\n          const content = await readFile(filePath, 'utf-8');\n          results.push({\n            relativePath: path.relative(projectRoot, filePath),\n            content,\n          });\n        } catch {\n          // Skip unreadable files\n        }\n      }\n    }\n  }\n\n  await walk(projectRoot);\n  results.sort((a, b) => a.relativePath.localeCompare(b.relativePath));\n  return results;\n}\n\n/**\n * Recursively collect all `.annex.md` files under `projectRoot`,\n * returning their relative paths and content sorted alphabetically.\n *\n * Uses the same skip-list as `collectAgentsDocs()`.\n */\nexport async function collectAnnexFiles(projectRoot: string): Promise<AgentsDocs> {\n  const results: AgentsDocs = [];\n\n  async function walk(currentDir: string): Promise<void> {\n    let entries;\n    try {\n      entries = await readdir(currentDir, { withFileTypes: true });\n    } catch {\n      return;\n    }\n    for (const entry of entries) {\n      if (entry.isDirectory() && !SKIP_DIRS.has(entry.name)) {\n        await walk(path.join(currentDir, entry.name));\n      } else if (entry.isFile() && entry.name.endsWith('.annex.md')) {\n        try {\n          const filePath = path.join(currentDir, entry.name);\n          const content = await readFile(filePath, 'utf-8');\n          results.push({\n            relativePath: path.relative(projectRoot, filePath),\n            content,\n          });\n        } catch {\n          // Skip unreadable files\n        }\n      }\n    }\n  }\n\n  await walk(projectRoot);\n  results.sort((a, b) => a.relativePath.localeCompare(b.relativePath));\n  return results;\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**`collector.ts` exports `collectAgentsDocs()` and `collectAnnexFiles()` for recursive traversal of the project tree to aggregate `AGENTS.md` and `.annex.md` files into sorted arrays of relative paths and content.**\n\n## Exported Functions\n\n**`collectAgentsDocs(projectRoot: string): Promise<AgentsDocs>`**\nRecursively walks `projectRoot` starting from the top level, collecting all files named exactly `AGENTS.md`. Returns `AgentsDocs` array sorted alphabetically by `relativePath` (computed via `path.relative(projectRoot, filePath)`). Skips directories in `SKIP_DIRS` set. Gracefully handles permission-denied errors by catching exceptions from `readdir()` and `readFile()`, silently omitting unreadable entries.\n\n**`collectAnnexFiles(projectRoot: string): Promise<AgentsDocs>`**\nRecursively walks `projectRoot` collecting all files ending with `.annex.md` (matched via `entry.name.endsWith('.annex.md')`). Uses identical skip logic and error handling as `collectAgentsDocs()`. Returns `AgentsDocs` array sorted alphabetically by `relativePath`.\n\n## Type Definitions\n\n**`AgentsDocs`**\nType alias for `Array<{ relativePath: string; content: string }>`. Represents a collection of documentation files with their project-relative paths and UTF-8 text content.\n\n## Directory Exclusion\n\n**`SKIP_DIRS`**\nImmutable `Set` containing 13 directory names skipped during traversal: `'node_modules'`, `'.git'`, `'.agents-reverse-engineer'`, `'vendor'`, `'dist'`, `'build'`, `'__pycache__'`, `'.next'`, `'venv'`, `'.venv'`, `'target'`, `'.cargo'`, `'.gradle'`. Applied in `entry.isDirectory() && !SKIP_DIRS.has(entry.name)` guard.\n\n## Traversal Algorithm\n\nBoth functions use nested `async function walk(currentDir: string)` implementing depth-first recursive traversal:\n1. Call `readdir(currentDir, { withFileTypes: true })` to get `Dirent[]` entries\n2. For each entry, if directory and not in `SKIP_DIRS`, recurse into `path.join(currentDir, entry.name)`\n3. For file entries, check name match condition (`entry.name === 'AGENTS.md'` or `entry.name.endsWith('.annex.md')`)\n4. Read file via `readFile(filePath, 'utf-8')`, push `{ relativePath, content }` to `results[]`\n5. Catch and ignore all exceptions (both at directory and file level)\n6. After traversal completes, sort `results` via `results.sort((a, b) => a.relativePath.localeCompare(b.relativePath))`\n\n## Integration Points\n\nConsumed by Phase 3 root document synthesis in `src/generation/orchestrator.ts` to aggregate all `AGENTS.md` files into unified corpus for `CLAUDE.md`/`GEMINI.md`/`OPENCODE.md` generation. The `collectAnnexFiles()` function supports annex references mentioned in documentation pipeline (large string constants extracted to companion `.annex.md` files).",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 792,
      "cacheReadTokens": 17367,
      "cacheCreationTokens": 9974,
      "latencyMs": 20285,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/generation/collector.ts",
          "sizeBytes": 2949
        }
      ]
    },
    {
      "timestamp": "2026-02-09T17:44:27.062Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/cli/update.ts\n\n```typescript\n/**\n * CLI update command\n *\n * Updates documentation incrementally based on git changes since last run.\n * Uses the AI service for real file analysis (not placeholders) and processes\n * changed files concurrently via the CommandRunner orchestration engine.\n * Regenerates AGENTS.md for affected directories after analysis.\n */\nimport * as path from 'node:path';\nimport { readFile } from 'node:fs/promises';\nimport pc from 'picocolors';\nimport { loadConfig } from '../config/loader.js';\nimport { createLogger } from '../output/logger.js';\nimport {\n  createUpdateOrchestrator,\n  type UpdatePlan,\n} from '../update/index.js';\nimport { writeAgentsMd, GENERATED_MARKER } from '../generation/writers/agents-md.js';\nimport { buildDirectoryPrompt } from '../generation/prompts/index.js';\nimport {\n  AIService,\n  AIServiceError,\n  createBackendRegistry,\n  resolveBackend,\n  getInstallInstructions,\n} from '../ai/index.js';\nimport { CommandRunner, ProgressReporter, ProgressLog, createTraceWriter, cleanupOldTraces } from '../orchestration/index.js';\n\n/**\n * Options for the update command.\n */\nexport interface UpdateCommandOptions {\n  /** Include uncommitted changes (staged + working directory) */\n  uncommitted?: boolean;\n  /** Dry run - show plan without making changes */\n  dryRun?: boolean;\n  /** Number of concurrent AI calls */\n  concurrency?: number;\n  /** Stop on first file analysis failure */\n  failFast?: boolean;\n  /** Show AI prompts and backend details */\n  debug?: boolean;\n  /** Enable concurrency tracing to .agents-reverse-engineer/traces/ */\n  trace?: boolean;\n}\n\n/**\n * Format cleanup results for display.\n */\nfunction formatCleanup(plan: UpdatePlan): string[] {\n  const lines: string[] = [];\n\n  if (plan.cleanup.deletedSumFiles.length > 0) {\n    lines.push(pc.yellow('Cleanup (deleted .sum files):'));\n    for (const file of plan.cleanup.deletedSumFiles) {\n      lines.push(`  ${pc.red('-')} ${file}`);\n    }\n  }\n\n  if (plan.cleanup.deletedAgentsMd.length > 0) {\n    lines.push(pc.yellow('Cleanup (deleted AGENTS.md from empty dirs):'));\n    for (const file of plan.cleanup.deletedAgentsMd) {\n      lines.push(`  ${pc.red('-')} ${file}`);\n    }\n  }\n\n  return lines;\n}\n\n/**\n * Format the update plan for display.\n */\nfunction formatPlan(plan: UpdatePlan): string {\n  const lines: string[] = [];\n\n  // Header\n  lines.push('');\n  lines.push(pc.bold('=== Update Plan ==='));\n  lines.push('');\n\n  // Baseline info\n  if (plan.isFirstRun) {\n    lines.push(pc.yellow('First run detected. Use \"are generate\" for initial documentation.'));\n    lines.push('');\n  } else {\n    lines.push(`Current commit: ${pc.dim(plan.currentCommit.slice(0, 7))}`);\n    lines.push('');\n  }\n\n  // Summary\n  const analyzeCount = plan.filesToAnalyze.length;\n  const skipCount = plan.filesToSkip.length;\n  const cleanupCount = plan.cleanup.deletedSumFiles.length + plan.cleanup.deletedAgentsMd.length;\n\n  if (analyzeCount === 0 && skipCount === 0 && cleanupCount === 0) {\n    lines.push(pc.green('No changes detected since last run.'));\n    lines.push('');\n    return lines.join('\\n');\n  }\n\n  lines.push(`Files to analyze: ${pc.cyan(String(analyzeCount))}`);\n  lines.push(`Files unchanged: ${pc.dim(String(skipCount))}`);\n  if (cleanupCount > 0) {\n    lines.push(`Cleanup actions: ${pc.yellow(String(cleanupCount))}`);\n  }\n  lines.push('');\n\n  // File list with status markers\n  if (plan.filesToAnalyze.length > 0) {\n    lines.push(pc.cyan('Files to analyze:'));\n    for (const file of plan.filesToAnalyze) {\n      const status = file.status === 'added' ? pc.green('+') :\n                    file.status === 'renamed' ? pc.blue('R') :\n                    pc.yellow('M');\n      lines.push(`  ${status} ${file.path}`);\n      if (file.status === 'renamed' && file.oldPath) {\n        lines.push(`    ${pc.dim(`(was: ${file.oldPath})`)}`);\n      }\n    }\n    lines.push('');\n  }\n\n  if (plan.filesToSkip.length > 0) {\n    lines.push(pc.dim('Files unchanged (skipped):'));\n    for (const file of plan.filesToSkip) {\n      lines.push(`  ${pc.dim('=')} ${pc.dim(file)}`);\n    }\n    lines.push('');\n  }\n\n  // Cleanup\n  lines.push(...formatCleanup(plan));\n\n  // Affected directories\n  if (plan.affectedDirs.length > 0) {\n    lines.push('');\n    lines.push(pc.cyan('Directories for AGENTS.md regeneration:'));\n    for (const dir of plan.affectedDirs) {\n      lines.push(`  ${dir}`);\n    }\n  }\n\n  lines.push('');\n  return lines.join('\\n');\n}\n\n/**\n * Update command - incrementally updates documentation based on git changes.\n *\n * This command:\n * 1. Checks git repository status\n * 2. Detects files changed since last run (via content-hash comparison)\n * 3. Cleans up orphaned .sum files\n * 4. Resolves an AI CLI backend and creates the AI service\n * 5. Analyzes changed files concurrently via CommandRunner\n * 6. Regenerates AGENTS.md for affected directories\n * 7. Writes telemetry run log and prints run summary\n *\n * Exit codes: 0 = all success, 1 = partial failure, 2 = total failure / no CLI\n */\nexport async function updateCommand(\n  targetPath: string,\n  options: UpdateCommandOptions\n): Promise<void> {\n  const absolutePath = path.resolve(targetPath);\n  const logger = createLogger({ colors: true });\n\n  logger.info(`Checking for updates in: ${absolutePath}`);\n\n  // Create trace writer (moved earlier to use in config loading and orchestrator)\n  const tracer = createTraceWriter(absolutePath, options.trace ?? false);\n  if (options.trace && tracer.filePath) {\n    console.error(pc.dim(`[trace] Writing to ${tracer.filePath}`));\n  }\n\n  // Load configuration\n  const config = await loadConfig(absolutePath, {\n    tracer,\n    debug: options.debug,\n  });\n\n  // Create orchestrator\n  const orchestrator = createUpdateOrchestrator(config, absolutePath, {\n    tracer,\n    debug: options.debug,\n  });\n\n  try {\n    // Prepare update plan\n    const plan = await orchestrator.preparePlan({\n      includeUncommitted: options.uncommitted,\n      dryRun: options.dryRun,\n    });\n\n    // Display plan\n    console.log(formatPlan(plan));\n\n    // Handle first run\n    if (plan.isFirstRun) {\n      console.log(pc.yellow('Hint: Run \"are generate\" first to create initial documentation.'));\n      console.log(pc.yellow('Then run \"are update\" after making changes.'));\n      return;\n    }\n\n    // Handle no changes\n    if (plan.filesToAnalyze.length === 0 &&\n        plan.cleanup.deletedSumFiles.length === 0 &&\n        plan.cleanup.deletedAgentsMd.length === 0) {\n      console.log(pc.green('All files are up to date.'));\n      return;\n    }\n\n    if (options.dryRun) {\n      logger.info('Dry run complete. No files written.');\n      return;\n    }\n\n    // -------------------------------------------------------------------------\n    // Backend resolution (same pattern as generate command)\n    // -------------------------------------------------------------------------\n\n    const registry = createBackendRegistry();\n    let backend;\n    try {\n      backend = await resolveBackend(registry, config.ai.backend);\n    } catch (error) {\n      if (error instanceof AIServiceError && error.code === 'CLI_NOT_FOUND') {\n        console.error(pc.red('Error: No AI CLI found.\\n'));\n        console.error(getInstallInstructions(registry));\n        process.exit(2);\n      }\n      throw error;\n    }\n\n    // Debug: log backend info\n    if (options.debug) {\n      console.log(pc.dim(`[debug] Backend: ${backend.name}`));\n      console.log(pc.dim(`[debug] CLI command: ${backend.cliCommand}`));\n      console.log(pc.dim(`[debug] Model: ${config.ai.model}`));\n    }\n\n    // -------------------------------------------------------------------------\n    // AI service setup\n    // -------------------------------------------------------------------------\n\n    const aiService = new AIService(backend, {\n      timeoutMs: config.ai.timeoutMs,\n      maxRetries: config.ai.maxRetries,\n      model: config.ai.model,\n      telemetry: { keepRuns: config.ai.telemetry.keepRuns },\n    });\n\n    if (options.debug) {\n      aiService.setDebug(true);\n    }\n\n    // Determine concurrency\n    const concurrency = options.concurrency ?? config.ai.concurrency;\n\n    // Enable subprocess output logging alongside tracing\n    if (options.trace) {\n      const logDir = path.join(\n        absolutePath, '.agents-reverse-engineer', 'subprocess-logs',\n        new Date().toISOString().replace(/[:.]/g, '-'),\n      );\n      aiService.setSubprocessLogDir(logDir);\n      console.error(pc.dim(`[trace] Subprocess logs → ${logDir}`));\n    }\n\n    // Create progress log for tail -f monitoring\n    const progressLog = ProgressLog.create(absolutePath);\n    progressLog.write(`=== ARE Update (${new Date().toISOString()}) ===`);\n    progressLog.write(`Project: ${absolutePath}`);\n    progressLog.write(`Files to analyze: ${plan.filesToAnalyze.length} | Directories: ${plan.affectedDirs.length}`);\n    progressLog.write('');\n\n    // Create command runner\n    const runner = new CommandRunner(aiService, {\n      concurrency,\n      failFast: options.failFast,\n      debug: options.debug,\n      tracer,\n      progressLog,\n    });\n\n    // -------------------------------------------------------------------------\n    // Phase 1: File analysis via CommandRunner (concurrent AI calls)\n    // -------------------------------------------------------------------------\n\n    const summary = await runner.executeUpdate(\n      plan.filesToAnalyze,\n      absolutePath,\n      config,\n    );\n\n    // -------------------------------------------------------------------------\n    // Phase 2: AGENTS.md regeneration for affected directories\n    // -------------------------------------------------------------------------\n\n    if (plan.affectedDirs.length > 0) {\n      const knownDirs = new Set(plan.affectedDirs);\n      const phase2Start = Date.now();\n      let dirsCompleted = 0;\n      let dirsFailed = 0;\n\n      // Emit phase start\n      tracer.emit({\n        type: 'phase:start',\n        phase: 'update-phase-dir-regen',\n        taskCount: plan.affectedDirs.length,\n        concurrency: 1,\n      });\n\n      const dirReporter = new ProgressReporter(0, plan.affectedDirs.length, progressLog);\n      for (const dir of plan.affectedDirs) {\n        const taskStart = Date.now();\n        const taskLabel = dir || '.';\n\n        // Emit task:start\n        tracer.emit({\n          type: 'task:start',\n          taskLabel,\n          phase: 'update-phase-dir-regen',\n        });\n\n        const dirPath = dir === '.' ? absolutePath : path.join(absolutePath, dir);\n        dirReporter.onDirectoryStart(dir || '.');\n        try {\n          // Read existing generated AGENTS.md for incremental update context\n          let existingAgentsMd: string | undefined;\n          try {\n            const agentsContent = await readFile(path.join(dirPath, 'AGENTS.md'), 'utf-8');\n            if (agentsContent.includes(GENERATED_MARKER)) {\n              existingAgentsMd = agentsContent;\n            }\n          } catch {\n            // No existing AGENTS.md — will generate from scratch\n          }\n\n          const prompt = await buildDirectoryPrompt(dirPath, absolutePath, options.debug, knownDirs, undefined, existingAgentsMd);\n          const response = await aiService.call({\n            prompt: prompt.user,\n            systemPrompt: prompt.system,\n          });\n          await writeAgentsMd(dirPath, absolutePath, response.text);\n          const dirDurationMs = Date.now() - taskStart;\n          dirReporter.onDirectoryDone(\n            dir || '.',\n            dirDurationMs,\n            response.inputTokens,\n            response.outputTokens,\n            response.model,\n            response.cacheReadTokens,\n            response.cacheCreationTokens,\n          );\n          dirsCompleted++;\n\n          // Emit task:done (success)\n          tracer.emit({\n            type: 'task:done',\n            workerId: 0,\n            taskIndex: dirsCompleted - 1,\n            taskLabel,\n            durationMs: Date.now() - taskStart,\n            success: true,\n            activeTasks: 0,\n          });\n        } catch (error) {\n          dirsFailed++;\n          const errorMsg = error instanceof Error ? error.message : String(error);\n          console.log(`${pc.dim('[dir]')} ${pc.yellow('WARN')} ${dir || '.'}: ${errorMsg}`);\n\n          // Emit task:done (failure)\n          tracer.emit({\n            type: 'task:done',\n            workerId: 0,\n            taskIndex: dirsCompleted + dirsFailed - 1,\n            taskLabel,\n            durationMs: Date.now() - taskStart,\n            success: false,\n            error: errorMsg,\n            activeTasks: 0,\n          });\n        }\n      }\n\n      // Emit phase end\n      tracer.emit({\n        type: 'phase:end',\n        phase: 'update-phase-dir-regen',\n        durationMs: Date.now() - phase2Start,\n        tasksCompleted: dirsCompleted,\n        tasksFailed: dirsFailed,\n      });\n    }\n\n    // -------------------------------------------------------------------------\n    // Telemetry finalization\n    // -------------------------------------------------------------------------\n\n    await aiService.finalize(absolutePath);\n\n    // Finalize progress log, trace, and clean up old trace files\n    await progressLog.finalize();\n    await tracer.finalize();\n    if (options.trace) {\n      await cleanupOldTraces(absolutePath);\n    }\n\n    // -------------------------------------------------------------------------\n    // Record run state (no-op in frontmatter mode, kept for API compatibility)\n    // -------------------------------------------------------------------------\n\n    const filesSkipped = plan.filesToSkip.length;\n    await orchestrator.recordRun(\n      plan.currentCommit,\n      summary.filesProcessed,\n      filesSkipped\n    );\n\n    // -------------------------------------------------------------------------\n    // Exit code: 0 = all success, 1 = partial failure, 2 = total failure\n    // -------------------------------------------------------------------------\n\n    if (summary.filesProcessed === 0 && summary.filesFailed > 0) {\n      process.exit(2);\n    } else if (summary.filesFailed > 0) {\n      process.exit(1);\n    }\n    // Exit code 0 -- all files succeeded (or no files to process)\n\n  } finally {\n    orchestrator.close();\n  }\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**`update.ts` implements the CLI entry point for incremental documentation updates via `updateCommand()`, detecting changed files through SHA-256 hash comparison, cleaning orphaned artifacts, resolving AI backends, executing concurrent file analysis via `CommandRunner`, and regenerating `AGENTS.md` for affected directories.**\n\n## Exported Interface\n\n`updateCommand(targetPath: string, options: UpdateCommandOptions): Promise<void>` — main entry point for the `are update` CLI command, orchestrates the incremental update workflow including plan preparation, backend resolution, AI service initialization, concurrent file analysis, and directory-level documentation regeneration. Exit codes: 0 (all success), 1 (partial failure), 2 (total failure/no CLI found).\n\n`UpdateCommandOptions` — configuration interface with fields:\n- `uncommitted?: boolean` — include staged + working directory changes via `git status --porcelain`\n- `dryRun?: boolean` — display update plan without writing files\n- `concurrency?: number` — override worker pool size (1-10, default from config)\n- `failFast?: boolean` — abort on first task failure\n- `debug?: boolean` — enable verbose subprocess logging with heap/RSS metrics\n- `trace?: boolean` — emit NDJSON trace events to `.agents-reverse-engineer/traces/`\n\n## Update Workflow\n\n`updateCommand()` executes a six-phase incremental workflow:\n\n1. **Trace initialization**: `createTraceWriter()` before config loading to enable tracing throughout orchestrator\n2. **Plan preparation**: `orchestrator.preparePlan({ includeUncommitted, dryRun })` returns `UpdatePlan` with `filesToAnalyze`, `filesToSkip`, `cleanup` (deleted `.sum` files, deleted `AGENTS.md`), `affectedDirs`, `currentCommit`, `isFirstRun`\n3. **Backend resolution**: `resolveBackend(registry, config.ai.backend)` with error handling for `CLI_NOT_FOUND` (prints install instructions, exits with code 2)\n4. **AI service setup**: instantiates `AIService` with timeout/retry config, enables debug mode if `--debug`, sets subprocess log directory if `--trace` (creates timestamped subdirectory in `.agents-reverse-engineer/subprocess-logs/`)\n5. **File analysis**: `runner.executeUpdate(plan.filesToAnalyze, absolutePath, config)` processes changed files concurrently, returns `summary` with `filesProcessed`, `filesFailed`\n6. **Directory regeneration**: iterates `plan.affectedDirs` sequentially (concurrency=1), reads existing `AGENTS.md` via `readFile()` checking for `GENERATED_MARKER`, calls `buildDirectoryPrompt()` with `existingAgentsMd` parameter for incremental context, invokes `aiService.call()`, writes via `writeAgentsMd()`\n\n## Display Formatting\n\n`formatPlan(plan: UpdatePlan): string` — renders update plan with status markers:\n- File statuses: `+` (added, green), `R` (renamed, blue), `M` (modified, yellow), `=` (unchanged, dim)\n- Displays `currentCommit` (7-char truncation), file counts (analyze/skip/cleanup), renamed file oldPath with `(was: ...)` annotation\n- Special case: first run detection via `plan.isFirstRun` → suggests `are generate` for initial documentation\n- Empty plan detection: all counts zero → \"No changes detected since last run\"\n\n`formatCleanup(plan: UpdatePlan): string[]` — renders cleanup actions:\n- `cleanup.deletedSumFiles` — orphaned `.sum` files (deleted/renamed sources)\n- `cleanup.deletedAgentsMd` — `AGENTS.md` removed from empty directories\n\n## Phase 2: Directory Regeneration\n\nSequential iteration over `plan.affectedDirs` with:\n- `knownDirs` set prevents phantom path validation errors by passing to `buildDirectoryPrompt()`\n- Reads existing `AGENTS.md` via `readFile()`, checks for `GENERATED_MARKER` substring (user-authored files skipped)\n- Passes `existingAgentsMd` to `buildDirectoryPrompt()` for incremental update context (preserves user annotations)\n- Emits trace events: `phase:start` (phase: `'update-phase-dir-regen'`), `task:start`/`task:done` for each directory\n- Uses `ProgressReporter.onDirectoryStart()`/`onDirectoryDone()` for console/log output with token counts\n- Error handling: logs `WARN` without aborting, increments `dirsFailed` counter\n\n## Telemetry and State Management\n\nAfter phase completion:\n- `aiService.finalize(absolutePath)` writes run log to `.agents-reverse-engineer/logs/run-<timestamp>.json`\n- `progressLog.finalize()` flushes human-readable `.agents-reverse-engineer/progress.log`\n- `tracer.finalize()` writes buffered NDJSON trace events via promise-chain serialization\n- `cleanupOldTraces(absolutePath)` if `--trace` enabled, retains 500 most recent trace files\n- `orchestrator.recordRun(currentCommit, filesProcessed, filesSkipped)` — no-op in frontmatter mode (SHA-256 hashing), kept for API compatibility with legacy metadata tracking\n\n## Progress Monitoring\n\n`ProgressLog.create(absolutePath)` creates `.agents-reverse-engineer/progress.log` for real-time monitoring:\n- Writes session header with ISO 8601 timestamp, project path, file/directory counts\n- `progressLog.write()` called for each directory start/done event\n- Enables `tail -f .agents-reverse-engineer/progress.log` pattern for long-running updates\n\n## Exit Code Strategy\n\nDetermines exit code based on `summary.filesProcessed` and `summary.filesFailed`:\n- Code 2: total failure (zero files processed, nonzero failed) or CLI not found\n- Code 1: partial failure (some files processed, some failed)\n- Code 0: all success (zero failures) or no files to process (no-op update)",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 1463,
      "cacheReadTokens": 17367,
      "cacheCreationTokens": 13159,
      "latencyMs": 37703,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/cli/update.ts",
          "sizeBytes": 14176
        }
      ]
    },
    {
      "timestamp": "2026-02-09T17:44:29.078Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/config/loader.ts\n\n```typescript\n/**\n * Configuration loader for agents-reverse\n *\n * Loads and validates configuration from `.agents-reverse/config.yaml`.\n * Returns sensible defaults when no config file exists.\n */\n\nimport { readFile, writeFile, mkdir, access } from 'node:fs/promises';\nimport { constants } from 'node:fs';\nimport path from 'node:path';\nimport { parse, stringify } from 'yaml';\nimport { ZodError } from 'zod';\nimport pc from 'picocolors';\nimport { ConfigSchema, Config } from './schema.js';\nimport { DEFAULT_VENDOR_DIRS, DEFAULT_BINARY_EXTENSIONS, DEFAULT_MAX_FILE_SIZE, DEFAULT_EXCLUDE_PATTERNS, getDefaultConcurrency } from './defaults.js';\nimport type { ITraceWriter } from '../orchestration/trace.js';\n\n/**\n * Quote a string value for YAML output if it contains characters that\n * would be misinterpreted (e.g. `*` is the YAML alias indicator).\n */\nfunction yamlScalar(value: string): string {\n  // Characters that require quoting: *, {, }, [, ], ?, :, #, &, !, |, >, etc.\n  if (/[*{}\\[\\]?,:#&!|>'\"%@`]/.test(value)) {\n    return `\"${value.replace(/\\\\/g, '\\\\\\\\').replace(/\"/g, '\\\\\"')}\"`;\n  }\n  return value;\n}\n\n/** Directory name for agents-reverse-engineer configuration */\nexport const CONFIG_DIR = '.agents-reverse-engineer';\n\n/** Configuration file name */\nexport const CONFIG_FILE = 'config.yaml';\n\n/**\n * Error thrown when configuration parsing or validation fails\n */\nexport class ConfigError extends Error {\n  constructor(\n    message: string,\n    public readonly filePath: string,\n    public readonly cause?: Error\n  ) {\n    super(message);\n    this.name = 'ConfigError';\n  }\n}\n\n/**\n * Load configuration from `.agents-reverse/config.yaml`.\n *\n * If the file doesn't exist, returns default configuration.\n * If the file exists but is invalid, throws a ConfigError with details.\n *\n * @param root - Root directory containing `.agents-reverse/` folder\n * @param options - Optional configuration loading options\n * @param options.tracer - Trace writer for emitting config:loaded events\n * @param options.debug - Enable debug output for configuration loading\n * @returns Validated configuration object with all defaults applied\n * @throws ConfigError if the config file exists but is invalid\n *\n * @example\n * ```typescript\n * const config = await loadConfig('/path/to/project');\n * console.log(config.exclude.vendorDirs);\n * ```\n */\nexport async function loadConfig(\n  root: string,\n  options?: { tracer?: ITraceWriter; debug?: boolean }\n): Promise<Config> {\n  const configPath = path.join(root, CONFIG_DIR, CONFIG_FILE);\n\n  try {\n    const content = await readFile(configPath, 'utf-8');\n    const raw = parse(content);\n\n    try {\n      const config = ConfigSchema.parse(raw);\n\n      // Emit trace event\n      options?.tracer?.emit({\n        type: 'config:loaded',\n        configPath: path.relative(root, configPath),\n        model: config.ai.model,\n        concurrency: config.ai.concurrency,\n      });\n\n      // Debug output\n      if (options?.debug) {\n        console.error(pc.dim(`[debug] Config loaded from: ${path.relative(root, configPath)}`));\n        console.error(\n          pc.dim(`[debug] Model: ${config.ai.model}, Concurrency: ${config.ai.concurrency}`)\n        );\n      }\n\n      return config;\n    } catch (err) {\n      if (err instanceof ZodError) {\n        const issues = err.issues\n          .map((issue) => `  - ${issue.path.join('.')}: ${issue.message}`)\n          .join('\\n');\n        throw new ConfigError(\n          `Invalid configuration in ${configPath}:\\n${issues}`,\n          configPath,\n          err\n        );\n      }\n      throw err;\n    }\n  } catch (err) {\n    // File not found - return defaults\n    if ((err as NodeJS.ErrnoException).code === 'ENOENT') {\n      const config = ConfigSchema.parse({});\n\n      // Emit trace event for defaults\n      options?.tracer?.emit({\n        type: 'config:loaded',\n        configPath: '(defaults)',\n        model: config.ai.model,\n        concurrency: config.ai.concurrency,\n      });\n\n      // Debug output\n      if (options?.debug) {\n        console.error(pc.dim(`[debug] Config file not found, using defaults`));\n        console.error(\n          pc.dim(`[debug] Model: ${config.ai.model}, Concurrency: ${config.ai.concurrency}`)\n        );\n      }\n\n      return config;\n    }\n\n    // Re-throw ConfigError as-is\n    if (err instanceof ConfigError) {\n      throw err;\n    }\n\n    // YAML parse error\n    throw new ConfigError(\n      `Failed to parse ${configPath}: ${(err as Error).message}`,\n      configPath,\n      err as Error\n    );\n  }\n}\n\n/**\n * Check if a configuration file exists.\n *\n * @param root - Root directory to check\n * @returns true if `.agents-reverse/config.yaml` exists\n *\n * @example\n * ```typescript\n * if (!await configExists('.')) {\n *   console.log('Run `are init` to create configuration');\n * }\n * ```\n */\nexport async function configExists(root: string): Promise<boolean> {\n  const configPath = path.join(root, CONFIG_DIR, CONFIG_FILE);\n  try {\n    await access(configPath, constants.F_OK);\n    return true;\n  } catch {\n    return false;\n  }\n}\n\n/**\n * Write a default configuration file with helpful comments.\n *\n * Creates the `.agents-reverse/` directory if it doesn't exist.\n * The generated file includes comments explaining each option.\n *\n * @param root - Root directory where `.agents-reverse/` will be created\n *\n * @example\n * ```typescript\n * await writeDefaultConfig('/path/to/project');\n * // Creates /path/to/project/.agents-reverse/config.yaml\n * ```\n */\nexport async function writeDefaultConfig(root: string): Promise<void> {\n  const configDir = path.join(root, CONFIG_DIR);\n  const configPath = path.join(configDir, CONFIG_FILE);\n\n  // Create directory if needed\n  await mkdir(configDir, { recursive: true });\n\n  // Generate config content with comments\n  const configContent = `# agents-reverse-engineer configuration\n# See: https://github.com/your-org/agents-reverse-engineer\n\n# ============================================================================\n# FILE & DIRECTORY EXCLUSIONS\n# ============================================================================\nexclude:\n  # Custom glob patterns to exclude (e.g., [\"*.log\", \"temp/**\"])\n  # Default patterns exclude AI-generated documentation files\n  patterns:\n${DEFAULT_EXCLUDE_PATTERNS.map((pattern) => `    - ${yamlScalar(pattern)}`).join('\\n')}\n\n  # Vendor directories to exclude from analysis\n  # These are typically package managers, build outputs, or version control\n  vendorDirs:\n${DEFAULT_VENDOR_DIRS.map((dir) => `    - ${dir}`).join('\\n')}\n\n  # Binary file extensions to exclude from analysis\n  # These files cannot be meaningfully analyzed as text\n  binaryExtensions:\n${DEFAULT_BINARY_EXTENSIONS.map((ext) => `    - ${ext}`).join('\\n')}\n\n# ============================================================================\n# DISCOVERY OPTIONS\n# ============================================================================\noptions:\n  # Whether to follow symbolic links during traversal\n  followSymlinks: false\n\n  # Maximum file size in bytes (files larger than this are skipped)\n  # Default: ${DEFAULT_MAX_FILE_SIZE} (1MB)\n  maxFileSize: ${DEFAULT_MAX_FILE_SIZE}\n\n# ============================================================================\n# OUTPUT FORMATTING\n# ============================================================================\noutput:\n  # Whether to use colors in terminal output\n  colors: true\n\n# ============================================================================\n# AI SERVICE CONFIGURATION\n# ============================================================================\nai:\n  # AI CLI backend to use\n  # Options: 'claude', 'gemini', 'opencode', 'auto' (auto-detect from PATH)\n  backend: auto\n\n  # Model identifier (backend-specific)\n  # Examples: \"sonnet\", \"opus\", \"haiku\" (for Claude)\n  model: sonnet\n\n  # Subprocess timeout in milliseconds\n  # Default: 300,000ms (5 minutes)\n  # Increase for very large files or slow connections\n  timeoutMs: 300000\n\n  # Maximum number of retries for transient errors\n  # Default: 3\n  maxRetries: 3\n\n  # Number of concurrent AI calls (parallelism)\n  # Range: 1-20, Default: auto-detected from CPU cores and available memory\n  # Current machine default: ${getDefaultConcurrency()}\n  # Uncomment to override:\n  # concurrency: ${getDefaultConcurrency()}\n\n  # Telemetry settings\n  telemetry:\n    # Number of most recent run logs to keep on disk\n    # Logs stored in .agents-reverse-engineer/logs/\n    keepRuns: 50\n`;\n\n  await writeFile(configPath, configContent, 'utf-8');\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**loadConfig() loads and validates `.agents-reverse-engineer/config.yaml` configuration via Zod schema, returns defaults when file absent, throws ConfigError on validation failure, emits config:loaded trace events, and writeDefaultConfig() generates annotated YAML with comment blocks explaining exclusion patterns, discovery options, and AI service settings.**\n\n## Exported Functions\n\n**loadConfig(root: string, options?: { tracer?: ITraceWriter; debug?: boolean }): Promise<Config>**\nReads `config.yaml` from `path.join(root, CONFIG_DIR, CONFIG_FILE)` where `CONFIG_DIR = '.agents-reverse-engineer'` and `CONFIG_FILE = 'config.yaml'`. Parses YAML via `parse()` from `yaml` library, validates against `ConfigSchema` from `./schema.js`. Returns `Config` object with defaults applied via `ConfigSchema.parse(raw)`. Throws `ConfigError` wrapping `ZodError` with formatted issue paths like `\"ai.concurrency: Expected number\"`. Returns `ConfigSchema.parse({})` defaults when file not found (`ENOENT`). Emits `config:loaded` trace event with `configPath`, `model`, `concurrency` fields via `options?.tracer?.emit()`. Logs debug output with `pc.dim()` colored messages showing model and concurrency when `options?.debug` true.\n\n**configExists(root: string): Promise<boolean>**\nReturns true if `path.join(root, CONFIG_DIR, CONFIG_FILE)` accessible via `access(configPath, constants.F_OK)`, false on exception.\n\n**writeDefaultConfig(root: string): Promise<void>**\nCreates `.agents-reverse-engineer/` directory via `mkdir(configDir, { recursive: true })`, writes `config.yaml` with multi-line comment blocks sectioned by `# ============================================================================` dividers. Injects `DEFAULT_EXCLUDE_PATTERNS`, `DEFAULT_VENDOR_DIRS`, `DEFAULT_BINARY_EXTENSIONS` from `./defaults.js` as YAML list items. Calls `yamlScalar(value)` to quote patterns containing regex chars `[*{}\\[\\]?,:#&!|>'\"%@\\`]` with backslash-escaped double quotes. Embeds `DEFAULT_MAX_FILE_SIZE` and `getDefaultConcurrency()` numeric defaults directly in YAML comment text like `# Default: ${DEFAULT_MAX_FILE_SIZE} (1MB)`. Contains commented-out `# concurrency: ${getDefaultConcurrency()}` line showing override syntax.\n\n## Error Handling\n\n**ConfigError** extends Error with `filePath: string` and optional `cause?: Error` properties, sets `name = 'ConfigError'`. Constructor signature: `(message: string, filePath: string, cause?: Error)`. Thrown for three failure modes: ZodError validation with formatted issue list joined by `\\n`, YAML parse errors re-wrapped with message prefix `\"Failed to parse ${configPath}: ${err.message}\"`, and propagated ConfigError instances re-thrown as-is without wrapping.\n\n## Trace Integration\n\nEmits `config:loaded` event via `options?.tracer?.emit()` with payload shape `{ type: 'config:loaded', configPath: string, model: string | null, concurrency: number }`. Event emitted twice per loadConfig() call: once after successful parse with relative path `path.relative(root, configPath)`, once for defaults with literal string `'(defaults)'` as configPath. Trace events logged even when `options?.debug` false.\n\n## YAML Serialization\n\n**yamlScalar(value: string): string**\nTests input against `/[*{}\\[\\]?,:#&!|>'\"%@\\`]/` pattern matching YAML metacharacters (asterisk for alias, braces/brackets for flow collections, question/colon for mappings, hash for comments, ampersand for anchor, pipe/angle for block scalars, quotes/percent/at/backtick for string edge cases). Returns double-quoted string with backslash escapes for literal backslashes `replace(/\\\\/g, '\\\\\\\\')` and double quotes `replace(/\"/g, '\\\\\"')` when pattern matches. Returns unquoted string for simple values.\n\n## Configuration File Structure\n\nGenerated YAML contains five comment-delimited sections: FILE & DIRECTORY EXCLUSIONS (patterns, vendorDirs, binaryExtensions arrays), DISCOVERY OPTIONS (followSymlinks boolean, maxFileSize number), OUTPUT FORMATTING (colors boolean), AI SERVICE CONFIGURATION (backend/model strings, timeoutMs/maxRetries/concurrency numbers, telemetry.keepRuns number). Each array rendered as YAML list with `- ` prefix and 4-space indentation. Vendor directories and binary extensions interpolated without yamlScalar() quoting. Exclude patterns passed through yamlScalar() due to glob wildcards like `*.log` and `temp/**`. Concurrency line commented out with `# concurrency: ${getDefaultConcurrency()}` showing override example but defaulting to auto-detection when absent.\n\n## Debug Output\n\nConsole writes via `console.error()` with `pc.dim()` gray formatting when `options?.debug` true. Emits two messages per load: `\"[debug] Config loaded from: ${path.relative(root, configPath)}\"` or `\"[debug] Config file not found, using defaults\"` for file status, then `\"[debug] Model: ${config.ai.model}, Concurrency: ${config.ai.concurrency}\"` for parsed values. Debug output follows trace emission to maintain event ordering.\n\n## Dependencies\n\nImports `readFile`, `writeFile`, `mkdir`, `access` from `node:fs/promises` for async file I/O. Imports `constants` from `node:fs` for `F_OK` flag. Uses `path.join()` and `path.relative()` from `node:path`. Parses YAML via `parse()` and `stringify()` from `yaml` package (stringify unused in current implementation). Validates with `ZodError` from `zod`. Colors via `pc` (picocolors alias). Imports `ConfigSchema` and `Config` type from `./schema.js`, defaults from `./defaults.js` (`DEFAULT_VENDOR_DIRS`, `DEFAULT_BINARY_EXTENSIONS`, `DEFAULT_MAX_FILE_SIZE`, `DEFAULT_EXCLUDE_PATTERNS`, `getDefaultConcurrency`), `ITraceWriter` from `../orchestration/trace.js`.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 1499,
      "cacheReadTokens": 17367,
      "cacheCreationTokens": 11461,
      "latencyMs": 37262,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/config/loader.ts",
          "sizeBytes": 8501
        }
      ]
    },
    {
      "timestamp": "2026-02-09T17:44:53.429Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/imports/index.ts\n\n```typescript\nexport { extractImports, extractDirectoryImports, formatImportMap } from './extractor.js';\nexport type { ImportEntry, FileImports } from './types.js';\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**Barrel export module re-exporting import extraction functionality from `extractor.ts` and type definitions from `types.ts`.**\n\n## Exported Functions\n\n- `extractImports` — Extracts import statements from a single source file\n- `extractDirectoryImports` — Aggregates imports across all files within a directory\n- `formatImportMap` — Formats collected import data into human-readable map representation\n\n## Exported Types\n\n- `ImportEntry` — Represents a single import statement with source module and imported identifiers\n- `FileImports` — Container type associating a file path with its array of ImportEntry records\n\n## Integration Points\n\nThis module serves as the public API surface for the `src/imports/` subsystem, consumed by `src/generation/prompts/builder.ts` during Phase 2 directory aggregation to construct import maps included in directory-level AGENTS.md prompts via `extractDirectoryImports()`.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 212,
      "cacheReadTokens": 17367,
      "cacheCreationTokens": 9135,
      "latencyMs": 14912,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/imports/index.ts",
          "sizeBytes": 151
        }
      ]
    },
    {
      "timestamp": "2026-02-09T17:44:52.734Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/generation/types.ts\n\n```typescript\n/**\n * Types for the documentation generation pipeline\n */\n\n/**\n * Result of file analysis (populated by LLM via host)\n */\nexport interface AnalysisResult {\n  /** Generated summary text */\n  summary: string;\n  /** Extracted metadata */\n  metadata: SummaryMetadata;\n}\n\n/**\n * Metadata extracted during analysis\n */\nexport interface SummaryMetadata {\n  /** Primary purpose of the file */\n  purpose: string;\n  /** Only security/breaking issues */\n  criticalTodos?: string[];\n  /** Tightly coupled siblings */\n  relatedFiles?: string[];\n}\n\n/**\n * Options for summary generation\n */\nexport interface SummaryOptions {\n  /** Target length for generated summaries */\n  targetLength: 'short' | 'standard' | 'detailed';\n  /** Whether to include code snippets in output */\n  includeCodeSnippets: boolean;\n}\n\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**Defines TypeScript interfaces for the documentation generation pipeline's analysis results, metadata extraction, and summary configuration.**\n\n## Exported Types\n\n### AnalysisResult\n```typescript\ninterface AnalysisResult {\n  summary: string;\n  metadata: SummaryMetadata;\n}\n```\nRepresents the output from LLM-based file analysis, containing both the generated markdown summary text and structured metadata extracted during analysis.\n\n### SummaryMetadata\n```typescript\ninterface SummaryMetadata {\n  purpose: string;\n  criticalTodos?: string[];\n  relatedFiles?: string[];\n}\n```\nEncapsulates metadata extracted from source files during Phase 1 analysis. The `purpose` field stores the one-line purpose statement. The optional `criticalTodos` array contains only security or breaking issues (not general TODOs). The optional `relatedFiles` array lists tightly coupled sibling files.\n\n### SummaryOptions\n```typescript\ninterface SummaryOptions {\n  targetLength: 'short' | 'standard' | 'detailed';\n  includeCodeSnippets: boolean;\n}\n```\nConfiguration options for controlling summary generation behavior. The `targetLength` discriminated union determines verbosity levels. The `includeCodeSnippets` boolean flag controls whether code examples appear in output summaries.\n\n## Integration Points\n\nThese types are consumed by:\n- `src/generation/executor.ts` — Uses `AnalysisResult` as return type from `AIService.call()`\n- `src/generation/writers/sum.ts` — Serializes `SummaryMetadata` to YAML frontmatter in `.sum` files\n- `src/generation/prompts/builder.ts` — May consume `SummaryOptions` to tailor prompt instructions (implementation-dependent)\n\nThe `SummaryMetadata` structure directly maps to the YAML frontmatter schema documented in CLAUDE.md:\n```yaml\n---\npurpose: [purpose field]\ncritical_todos: [criticalTodos array]\nrelated_files: [relatedFiles array]\n---\n```",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 478,
      "cacheReadTokens": 17367,
      "cacheCreationTokens": 9317,
      "latencyMs": 16122,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/generation/types.ts",
          "sizeBytes": 797
        }
      ]
    },
    {
      "timestamp": "2026-02-09T17:44:53.800Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/imports/types.ts\n\n```typescript\n/** A single import statement extracted from source. */\nexport interface ImportEntry {\n  /** The raw import specifier as written in source (e.g., '../ai/index.js') */\n  specifier: string;\n  /** Imported symbols (e.g., ['AIService', 'AIResponse']) */\n  symbols: string[];\n  /** Whether this is a type-only import */\n  typeOnly: boolean;\n}\n\n/** All imports from a single source file. */\nexport interface FileImports {\n  /** Relative file path (e.g., 'runner.ts') */\n  fileName: string;\n  /** External imports (from other modules, not same directory) */\n  externalImports: ImportEntry[];\n  /** Internal imports (from same directory) */\n  internalImports: ImportEntry[];\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**Defines TypeScript interfaces for representing import statements extracted from source files via static analysis.**\n\n## Exported Types\n\n### ImportEntry\nRepresents a single import statement extracted from source code.\n\n**Properties:**\n- `specifier: string` — Raw import path as written in source (e.g., `'../ai/index.js'`)\n- `symbols: string[]` — Array of imported identifiers (e.g., `['AIService', 'AIResponse']`)\n- `typeOnly: boolean` — Discriminates TypeScript type-only imports (`import type { ... }`)\n\n### FileImports\nAggregates all imports discovered in a single source file, partitioned by locality.\n\n**Properties:**\n- `fileName: string` — Relative file path (e.g., `'runner.ts'`)\n- `externalImports: ImportEntry[]` — Imports from other modules or parent/sibling directories\n- `internalImports: ImportEntry[]` — Imports from same directory as the source file\n\n## Design Patterns\n\n**Locality-based partitioning:** The `externalImports`/`internalImports` separation supports dependency graph construction where internal imports indicate tight coupling within a directory module, while external imports reveal cross-module dependencies. This distinction is used by `src/generation/prompts/builder.ts` when constructing import maps for AI prompts during Phase 1 file analysis.\n\n## Integration Points\n\nConsumed by:\n- `src/imports/extractor.ts` — Parses TypeScript/JavaScript source via regex to produce `FileImports` instances\n- `src/generation/prompts/builder.ts` — Includes `externalImports` in file analysis prompts to provide dependency context\n- `src/generation/prompts/templates.ts` — Aggregates directory-level imports from child `FileImports` for `AGENTS.md` generation",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 426,
      "cacheReadTokens": 17367,
      "cacheCreationTokens": 9281,
      "latencyMs": 17101,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/imports/types.ts",
          "sizeBytes": 669
        }
      ]
    },
    {
      "timestamp": "2026-02-09T17:44:54.516Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/installer/banner.ts\n\n```typescript\n/**\n * ASCII banner and styled output for the installer\n *\n * Provides colored banner display, help text, and styled message helpers.\n * Uses picocolors for terminal coloring.\n */\n\nimport pc from 'picocolors';\nimport { getVersion } from '../version.js';\n\n/** Package version read from package.json */\nexport const VERSION = getVersion();\n\n/**\n * Display the ASCII banner at installer launch\n *\n * Shows big ASCII art \"ARE\" letters in green with version and tagline.\n */\nexport function displayBanner(): void {\n  const art = pc.green;\n  const dim = pc.dim;\n\n  console.log();\n  console.log(art('  █████╗ ██████╗ ███████╗'));\n  console.log(art(' ██╔══██╗██╔══██╗██╔════╝'));\n  console.log(art(' ███████║██████╔╝█████╗  '));\n  console.log(art(' ██╔══██║██╔══██╗██╔══╝  '));\n  console.log(art(' ██║  ██║██║  ██║███████╗'));\n  console.log(art(' ╚═╝  ╚═╝╚═╝  ╚═╝╚══════╝'));\n  console.log();\n  console.log(dim(` agents-reverse-engineer v${VERSION}`));\n  console.log(dim(' AI-friendly codebase documentation'));\n  console.log();\n}\n\n/**\n * Display help text showing usage, flags, and examples\n */\nexport function showHelp(): void {\n  console.log(pc.bold('Usage:') + ' npx agents-reverse-engineer [options]');\n  console.log();\n  console.log(pc.bold('Options:'));\n  console.log('  --runtime <runtime>  Select runtime: claude, opencode, gemini, or all');\n  console.log('  -g, --global         Install to global config (~/.claude, etc.)');\n  console.log('  -l, --local          Install to local project (./.claude, etc.)');\n  console.log('  -u, --uninstall      Remove installed files');\n  console.log('  --force              Overwrite existing files');\n  console.log('  -q, --quiet          Suppress banner and info messages');\n  console.log('  -h, --help           Show this help');\n  console.log();\n  console.log(pc.bold('Examples:'));\n  console.log('  npx agents-reverse-engineer');\n  console.log('    Interactive mode - prompts for runtime and location');\n  console.log();\n  console.log('  npx agents-reverse-engineer --runtime claude -g');\n  console.log('    Install Claude Code commands globally');\n  console.log();\n  console.log('  npx agents-reverse-engineer --runtime all -l');\n  console.log('    Install commands for all runtimes to local project');\n  console.log();\n  console.log('  npx agents-reverse-engineer --runtime claude -g -u');\n  console.log('    Uninstall global Claude Code commands');\n}\n\n/**\n * Display a success message with green checkmark prefix\n *\n * @param msg - Message to display\n */\nexport function showSuccess(msg: string): void {\n  console.log(pc.green('✓') + ' ' + msg);\n}\n\n/**\n * Display an error message with red X prefix\n *\n * @param msg - Message to display\n */\nexport function showError(msg: string): void {\n  console.log(pc.red('✗') + ' ' + msg);\n}\n\n/**\n * Display a warning message with yellow exclamation prefix\n *\n * @param msg - Message to display\n */\nexport function showWarning(msg: string): void {\n  console.log(pc.yellow('!') + ' ' + msg);\n}\n\n/**\n * Display an info message with cyan arrow prefix\n *\n * @param msg - Message to display\n */\nexport function showInfo(msg: string): void {\n  console.log(pc.cyan('>') + ' ' + msg);\n}\n\n/**\n * Display post-install next steps\n *\n * Shows what to do after installation with helpful links.\n *\n * @param runtime - Which runtime was installed\n * @param filesCreated - Number of files created\n */\nexport function showNextSteps(runtime: string, filesCreated: number): void {\n  console.log();\n  console.log(pc.bold('Installation complete!'));\n  console.log(pc.dim(`${filesCreated} files installed for ${runtime}`));\n  console.log();\n  console.log(pc.bold('Next steps:'));\n  console.log('  1. Run ' + pc.cyan('/are-help') + ' in your AI assistant to verify');\n  console.log('  2. Run ' + pc.cyan('/are-init') + ' to initialize a project');\n  console.log('  3. Run ' + pc.cyan('/are-discover') + ' to create the generation plan');\n  console.log('  4. Run ' + pc.cyan('/are-generate') + ' to generate documentation');\n  console.log('  5. Run ' + pc.cyan('/are-update') + ' to update documentation after changes');\n  console.log('  6. Run ' + pc.cyan('/are-specify') + ' to generate a specification document');\n  console.log('  7. Run ' + pc.cyan('/are-clean') + ' to remove all generated artifacts');\n  console.log();\n  console.log(pc.dim('Docs: https://github.com/GeoloeG-IsT/agents-reverse-engineer'));\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**banner.ts provides styled terminal output for the installer including ASCII art, help text, and status messages using picocolors.**\n\n## Exported Functions\n\n**displayBanner()** renders the \"ARE\" ASCII art logo in green via `pc.green()` with version string from `getVersion()` and tagline \"AI-friendly codebase documentation\".\n\n**showHelp()** prints usage syntax, option flags (`--runtime`, `-g`/`--global`, `-l`/`--local`, `-u`/`--uninstall`, `--force`, `-q`/`--quiet`, `-h`/`--help`), and example commands for interactive mode, global/local installation, and uninstallation workflows.\n\n**showSuccess(msg: string)** outputs message prefixed with green checkmark `pc.green('✓')`.\n\n**showError(msg: string)** outputs message prefixed with red X `pc.red('✗')`.\n\n**showWarning(msg: string)** outputs message prefixed with yellow exclamation `pc.yellow('!')`.\n\n**showInfo(msg: string)** outputs message prefixed with cyan arrow `pc.cyan('>')`.\n\n**showNextSteps(runtime: string, filesCreated: number)** displays post-installation instructions listing commands `/are-help`, `/are-init`, `/are-discover`, `/are-generate`, `/are-update`, `/are-specify`, `/are-clean` with documentation link to GitHub repository.\n\n## Exported Constants\n\n**VERSION** holds package version string obtained from `getVersion()` imported from `../version.js`.\n\n## Dependencies\n\nImports `pc` from `picocolors` for terminal color formatting (`green()`, `red()`, `yellow()`, `cyan()`, `dim()`, `bold()`). Imports `getVersion()` from `../version.js` for package version retrieval.\n\n## ASCII Art Pattern\n\nBanner uses Unicode box-drawing characters (U+2588 full block, U+2550-U+2557 box drawing) to render:\n```\n █████╗ ██████╗ ███████╗\n██╔══██╗██╔══██╗██╔════╝\n███████║██████╔╝█████╗  \n██╔══██║██╔══██╗██╔══╝  \n██║  ██║██║  ██║███████╗\n╚═╝  ╚═╝╚═╝  ╚═╝╚══════╝\n```",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 590,
      "cacheReadTokens": 17367,
      "cacheCreationTokens": 10501,
      "latencyMs": 17174,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/installer/banner.ts",
          "sizeBytes": 4637
        }
      ]
    },
    {
      "timestamp": "2026-02-09T17:44:52.905Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/imports/extractor.ts\n\n```typescript\nimport { readFile } from 'node:fs/promises';\nimport * as path from 'node:path';\nimport type { ImportEntry, FileImports } from './types.js';\n\n/**\n * Regex matching TypeScript/JavaScript import statements.\n *\n * Captures:\n * - Group 1: 'type' keyword if present (type-only import)\n * - Group 2: named symbols (between braces)\n * - Group 3: namespace import (* as name)\n * - Group 4: default import (bare identifier)\n * - Group 5: module specifier (the string after 'from')\n */\nconst IMPORT_REGEX =\n  /^import\\s+(type\\s+)?(?:\\{([^}]*)\\}|(\\*\\s+as\\s+\\w+)|(\\w+))\\s+from\\s+['\"]([^'\"]+)['\"]/gm;\n\n/**\n * Extract import statements from source content.\n *\n * Only processes lines starting with 'import' to avoid matching\n * dynamic imports or imports inside comments/strings.\n */\nexport function extractImports(sourceContent: string): ImportEntry[] {\n  const entries: ImportEntry[] = [];\n  let match;\n  IMPORT_REGEX.lastIndex = 0;\n\n  while ((match = IMPORT_REGEX.exec(sourceContent)) !== null) {\n    const typeOnly = !!match[1];\n    const namedSymbols = match[2]; // { Foo, Bar }\n    const namespaceImport = match[3]; // * as name\n    const defaultImport = match[4]; // name\n    const specifier = match[5];\n\n    let symbols: string[] = [];\n    if (namedSymbols) {\n      symbols = namedSymbols\n        .split(',')\n        .map((s) => s.trim().replace(/\\s+as\\s+\\w+/, ''))\n        .filter(Boolean);\n    } else if (namespaceImport) {\n      symbols = [namespaceImport.replace('* as ', '').trim()];\n    } else if (defaultImport) {\n      symbols = [defaultImport];\n    }\n\n    entries.push({ specifier, symbols, typeOnly });\n  }\n\n  return entries;\n}\n\n/**\n * Extract and classify imports for all source files in a directory.\n *\n * Reads only the first 100 lines of each file (imports are at the top)\n * for performance. Classifies imports as internal (same directory via './')\n * or external (everything else).\n *\n * Skips node: built-ins and bare package specifiers (npm packages).\n */\nexport async function extractDirectoryImports(\n  dirPath: string,\n  fileNames: string[],\n): Promise<FileImports[]> {\n  const results: FileImports[] = [];\n\n  for (const fileName of fileNames) {\n    const filePath = path.join(dirPath, fileName);\n    try {\n      const content = await readFile(filePath, 'utf-8');\n      // Only read import region (first 100 lines) for performance\n      const importRegion = content.split('\\n').slice(0, 100).join('\\n');\n      const imports = extractImports(importRegion);\n\n      // Filter out bare specifiers (npm packages) and node: builtins\n      const relativeImports = imports.filter(\n        (i) => i.specifier.startsWith('.') || i.specifier.startsWith('..'),\n      );\n\n      const internal = relativeImports.filter((i) =>\n        i.specifier.startsWith('./'),\n      );\n      const external = relativeImports.filter((i) =>\n        i.specifier.startsWith('../'),\n      );\n\n      if (external.length > 0 || internal.length > 0) {\n        results.push({\n          fileName,\n          externalImports: external,\n          internalImports: internal,\n        });\n      }\n    } catch {\n      // Skip unreadable files\n    }\n  }\n\n  return results;\n}\n\n/**\n * Format import data as a structured text block for LLM prompts.\n *\n * Example output:\n * ```\n * runner.ts:\n *   ../ai/index.js → AIService\n *   ../generation/executor.js → ExecutionPlan, ExecutionTask\n *\n * pool.ts:\n *   ./trace.js → ITraceWriter (type)\n * ```\n */\nexport function formatImportMap(fileImports: FileImports[]): string {\n  const sections: string[] = [];\n\n  for (const fi of fileImports) {\n    const lines: string[] = [`${fi.fileName}:`];\n    for (const imp of fi.externalImports) {\n      const typeTag = imp.typeOnly ? ' (type)' : '';\n      lines.push(`  ${imp.specifier} → ${imp.symbols.join(', ')}${typeTag}`);\n    }\n    if (lines.length > 1) {\n      sections.push(lines.join('\\n'));\n    }\n  }\n\n  return sections.join('\\n\\n');\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**extractor.ts extracts TypeScript/JavaScript import statements from source files via regex parsing and formats them as structured import maps for LLM prompt consumption.**\n\n## Exported Functions\n\n### extractImports\n\n```typescript\nfunction extractImports(sourceContent: string): ImportEntry[]\n```\n\nParses source content using `IMPORT_REGEX` to extract all static import statements. Returns array of `ImportEntry` objects containing `specifier`, `symbols[]`, and `typeOnly` flag. Matches named imports (`{ Foo, Bar }`), namespace imports (`* as name`), and default imports. Strips `as` aliases from named symbols via `.replace(/\\s+as\\s+\\w+/, '')`. Resets `IMPORT_REGEX.lastIndex = 0` before matching to ensure clean state.\n\n### extractDirectoryImports\n\n```typescript\nasync function extractDirectoryImports(\n  dirPath: string,\n  fileNames: string[]\n): Promise<FileImports[]>\n```\n\nReads first 100 lines of each file in `fileNames` array (import region optimization), calls `extractImports()` on truncated content, classifies imports as `internal` (specifiers starting with `./`) or `external` (specifiers starting with `../`). Filters out bare package specifiers (npm modules) and `node:` built-ins. Returns `FileImports[]` containing `fileName`, `externalImports`, `internalImports` for files with relative imports. Silently skips unreadable files via empty catch block.\n\n### formatImportMap\n\n```typescript\nfunction formatImportMap(fileImports: FileImports[]): string\n```\n\nConverts `FileImports[]` into human-readable text block for LLM prompts. Groups external imports by file, appends `(type)` suffix for type-only imports. Output format: filename as header, indented specifier-symbol pairs (`  ../ai/index.js → AIService`). Only includes files with external imports (skips internal-only files).\n\n## Regular Expression\n\n### IMPORT_REGEX\n\n```typescript\nconst IMPORT_REGEX =\n  /^import\\s+(type\\s+)?(?:\\{([^}]*)\\}|(\\*\\s+as\\s+\\w+)|(\\w+))\\s+from\\s+['\"]([^'\"]+)['\"]/gm\n```\n\nMultiline regex matching static import statements at line start (`^import`). Capture groups: (1) `type` keyword for type-only imports, (2) named symbols between braces, (3) namespace import `* as name`, (4) default import identifier, (5) module specifier string. Uses `/gm` flags for global multiline matching. Does not match dynamic imports (`import()`) or imports inside comments/strings.\n\n## Import Classification Rules\n\n- **Internal imports**: Specifiers starting with `./` (same directory)\n- **External imports**: Specifiers starting with `../` (parent/sibling directories)\n- **Filtered out**: Bare specifiers (npm packages like `'react'`), `node:` built-ins (like `'node:fs'`)\n\n## Performance Optimization\n\nReads only first 100 lines of source files via `content.split('\\n').slice(0, 100).join('\\n')` before regex matching. Assumes all imports appear at top of file (standard ES module convention).\n\n## Integration Points\n\nUsed by `src/generation/prompts/builder.ts` to construct import context for directory-level `AGENTS.md` generation. Provides dependency coupling information via `formatImportMap()` output embedded in AI prompts.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 834,
      "cacheReadTokens": 17367,
      "cacheCreationTokens": 10356,
      "latencyMs": 23076,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/imports/extractor.ts",
          "sizeBytes": 3904
        }
      ]
    },
    {
      "timestamp": "2026-02-09T17:45:04.768Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/integration/detect.ts\n\n```typescript\n/**\n * Environment detection for AI coding assistants\n *\n * Detects which AI coding assistant environments are present in a project\n * by checking for their configuration directories and files.\n */\n\nimport { existsSync } from 'node:fs';\nimport * as path from 'node:path';\nimport type { DetectedEnvironment, EnvironmentType } from './types.js';\n\n/**\n * Detect all AI coding assistant environments present in a project\n *\n * Checks for:\n * - Claude Code: .claude/ directory OR CLAUDE.md file\n * - OpenCode: .opencode/ directory\n * - Aider: .aider.conf.yml file OR .aider/ directory\n *\n * @param projectRoot - Root directory to check for environments\n * @returns Array of detected environments (may include multiple)\n */\nexport function detectEnvironments(projectRoot: string): DetectedEnvironment[] {\n  const environments: DetectedEnvironment[] = [];\n\n  // Check for Claude Code\n  const claudeDir = path.join(projectRoot, '.claude');\n  const claudeMd = path.join(projectRoot, 'CLAUDE.md');\n  if (existsSync(claudeDir) || existsSync(claudeMd)) {\n    environments.push({\n      type: 'claude',\n      configDir: '.claude',\n      detected: true,\n    });\n  }\n\n  // Check for OpenCode\n  const openCodeDir = path.join(projectRoot, '.opencode');\n  if (existsSync(openCodeDir)) {\n    environments.push({\n      type: 'opencode',\n      configDir: '.opencode',\n      detected: true,\n    });\n  }\n\n  // Check for Aider\n  const aiderConfig = path.join(projectRoot, '.aider.conf.yml');\n  const aiderDir = path.join(projectRoot, '.aider');\n  if (existsSync(aiderConfig) || existsSync(aiderDir)) {\n    environments.push({\n      type: 'aider',\n      configDir: '.aider',\n      detected: true,\n    });\n  }\n\n  return environments;\n}\n\n/**\n * Check if a specific AI coding assistant environment is present\n *\n * @param projectRoot - Root directory to check\n * @param type - Environment type to check for\n * @returns true if the environment is detected\n */\nexport function hasEnvironment(\n  projectRoot: string,\n  type: EnvironmentType\n): boolean {\n  const environments = detectEnvironments(projectRoot);\n  return environments.some((env) => env.type === type);\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**detectEnvironments() and hasEnvironment() identify which AI coding assistant environments (Claude Code, OpenCode, Aider) exist in a project by checking for their configuration directories and marker files.**\n\n## Exported Functions\n\n**detectEnvironments(projectRoot: string): DetectedEnvironment[]**\nReturns array of all detected AI assistant environments in the given project root. Checks for Claude Code (`.claude/` directory OR `CLAUDE.md` file), OpenCode (`.opencode/` directory), and Aider (`.aider.conf.yml` file OR `.aider/` directory). Returns `DetectedEnvironment[]` with `type`, `configDir`, and `detected: true` fields.\n\n**hasEnvironment(projectRoot: string, type: EnvironmentType): boolean**\nReturns true if the specified `EnvironmentType` ('claude' | 'opencode' | 'aider') is present in the project. Delegates to `detectEnvironments()` and filters results.\n\n## Detection Logic\n\ndetectEnvironments() uses `existsSync()` from `node:fs` to check for:\n- **Claude Code**: `path.join(projectRoot, '.claude')` OR `path.join(projectRoot, 'CLAUDE.md')`\n- **OpenCode**: `path.join(projectRoot, '.opencode')`\n- **Aider**: `path.join(projectRoot, '.aider.conf.yml')` OR `path.join(projectRoot, '.aider')`\n\nAll detected environments return `configDir` values: `'.claude'`, `'.opencode'`, or `'.aider'`.\n\n## Type Imports\n\nImports `DetectedEnvironment` and `EnvironmentType` from `./types.js`. `EnvironmentType` discriminates the `type` field returned in detection results.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 416,
      "cacheReadTokens": 17367,
      "cacheCreationTokens": 9726,
      "latencyMs": 14168,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/integration/detect.ts",
          "sizeBytes": 2136
        }
      ]
    },
    {
      "timestamp": "2026-02-09T17:44:52.057Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/generation/orchestrator.ts\n\n```typescript\n/**\n * Generation orchestrator\n *\n * Coordinates the documentation generation workflow:\n * - Discovers and prepares files for analysis\n * - Creates file analysis tasks with prompts\n * - Creates directory tasks for LLM-generated descriptions\n */\n\nimport { readFile } from 'node:fs/promises';\nimport * as path from 'node:path';\nimport pc from 'picocolors';\nimport type { Config } from '../config/schema.js';\nimport type { DiscoveryResult } from '../types/index.js';\nimport { buildFilePrompt } from './prompts/index.js';\nimport { analyzeComplexity } from './complexity.js';\nimport type { ComplexityMetrics } from './complexity.js';\nimport type { ITraceWriter } from '../orchestration/trace.js';\n\n/**\n * A file prepared for analysis.\n */\nexport interface PreparedFile {\n  /** Absolute path to the file */\n  filePath: string;\n  /** Relative path from project root */\n  relativePath: string;\n  /** File content */\n  content: string;\n}\n\n/**\n * Analysis task for a file.\n */\nexport interface AnalysisTask {\n  /** Type of task */\n  type: 'file' | 'directory';\n  /** File or directory path */\n  filePath: string;\n  /** System prompt (set for file tasks; directory prompts built at execution time) */\n  systemPrompt?: string;\n  /** User prompt (set for file tasks; directory prompts built at execution time) */\n  userPrompt?: string;\n  /** Directory info for directory tasks */\n  directoryInfo?: {\n    /** Paths of .sum files in this directory */\n    sumFiles: string[];\n    /** Number of files analyzed */\n    fileCount: number;\n  };\n}\n\n/**\n * Result of the generation planning process.\n */\nexport interface GenerationPlan {\n  /** Files to be analyzed */\n  files: PreparedFile[];\n  /** Analysis tasks to execute */\n  tasks: AnalysisTask[];\n  /** Complexity metrics */\n  complexity: ComplexityMetrics;\n  /** Compact project directory listing for bird's-eye context */\n  projectStructure?: string;\n}\n\n/**\n * Orchestrates the documentation generation workflow.\n */\nexport class GenerationOrchestrator {\n  private config: Config;\n  private projectRoot: string;\n  private tracer?: ITraceWriter;\n  private debug: boolean;\n\n  constructor(\n    config: Config,\n    projectRoot: string,\n    options?: { tracer?: ITraceWriter; debug?: boolean }\n  ) {\n    this.config = config;\n    this.projectRoot = projectRoot;\n    this.tracer = options?.tracer;\n    this.debug = options?.debug ?? false;\n  }\n\n  /**\n   * Prepare files for analysis by reading content and detecting types.\n   */\n  async prepareFiles(discoveryResult: DiscoveryResult): Promise<PreparedFile[]> {\n    const prepared: PreparedFile[] = [];\n\n    for (let i = 0; i < discoveryResult.files.length; i++) {\n      const filePath = discoveryResult.files[i];\n      try {\n        const content = await readFile(filePath, 'utf-8');\n        const relativePath = path.relative(this.projectRoot, filePath);\n\n        prepared.push({\n          filePath,\n          relativePath,\n          content,\n        });\n      } catch {\n        // Skip files that can't be read (permission errors, etc.)\n        // Silently ignore - these files won't appear in the plan\n      }\n    }\n\n    return prepared;\n  }\n\n  /**\n   * Build a compact project structure listing from prepared files.\n   * Groups files by directory to give the AI bird's-eye context.\n   */\n  private buildProjectStructure(files: PreparedFile[]): string {\n    const byDir = new Map<string, string[]>();\n    for (const file of files) {\n      const dir = path.dirname(file.relativePath) || '.';\n      const group = byDir.get(dir) ?? [];\n      group.push(path.basename(file.relativePath));\n      byDir.set(dir, group);\n    }\n\n    const lines: string[] = [];\n    for (const [dir, dirFiles] of [...byDir.entries()].sort(([a], [b]) => a.localeCompare(b))) {\n      lines.push(`${dir}/`);\n      for (const f of dirFiles.sort()) {\n        lines.push(`  ${f}`);\n      }\n    }\n    return lines.join('\\n');\n  }\n\n  /**\n   * Create analysis tasks for all files.\n   */\n  createFileTasks(files: PreparedFile[], projectStructure?: string): AnalysisTask[] {\n    const tasks: AnalysisTask[] = [];\n\n    for (const file of files) {\n      const prompt = buildFilePrompt({\n        filePath: file.filePath,\n        content: file.content,\n        projectPlan: projectStructure,\n      }, this.debug);\n\n      tasks.push({\n        type: 'file',\n        filePath: file.relativePath,\n        systemPrompt: prompt.system,\n        userPrompt: prompt.user,\n      });\n    }\n\n    return tasks;\n  }\n\n  /**\n   * Create directory tasks for LLM-generated directory descriptions.\n   * These tasks run after all files in a directory are analyzed, allowing\n   * the LLM to synthesize a richer directory overview from the .sum files.\n   * Prompts are built at execution time by buildDirectoryPrompt().\n   */\n  createDirectoryTasks(files: PreparedFile[]): AnalysisTask[] {\n    const tasks: AnalysisTask[] = [];\n\n    // Group files by directory\n    const filesByDir = new Map<string, PreparedFile[]>();\n    for (const file of files) {\n      const dir = path.dirname(file.relativePath);\n      const dirFiles = filesByDir.get(dir) ?? [];\n      dirFiles.push(file);\n      filesByDir.set(dir, dirFiles);\n    }\n\n    // Create a directory task for each directory with analyzed files\n    for (const [dir, dirFiles] of Array.from(filesByDir.entries())) {\n      const sumFilePaths = dirFiles.map(f => `${f.relativePath}.sum`);\n\n      tasks.push({\n        type: 'directory',\n        filePath: dir || '.',\n        directoryInfo: {\n          sumFiles: sumFilePaths,\n          fileCount: dirFiles.length,\n        },\n      });\n    }\n\n    return tasks;\n  }\n\n  /**\n   * Create a complete generation plan.\n   */\n  async createPlan(discoveryResult: DiscoveryResult): Promise<GenerationPlan> {\n    const planStartTime = process.hrtime.bigint();\n\n    // Emit phase start\n    this.tracer?.emit({\n      type: 'phase:start',\n      phase: 'plan-creation',\n      taskCount: discoveryResult.files.length,\n      concurrency: 1,\n    });\n\n    if (this.debug) {\n      console.error(pc.dim('[debug] Preparing files: reading and detecting types...'));\n    }\n\n    const files = await this.prepareFiles(discoveryResult);\n\n    if (this.debug) {\n      console.error(pc.dim(`[debug] Analyzing complexity...`));\n    }\n\n    const complexity = analyzeComplexity(\n      files.map(f => f.filePath),\n      this.projectRoot\n    );\n\n    if (this.debug) {\n      console.error(pc.dim(`[debug] Complexity analysis: depth=${complexity.directoryDepth}`));\n    }\n\n    const projectStructure = this.buildProjectStructure(files);\n    const fileTasks = this.createFileTasks(files, projectStructure);\n\n    // Add directory tasks for LLM-generated directory descriptions\n    // These run after file analysis to synthesize richer directory overviews\n    const dirTasks = this.createDirectoryTasks(files);\n    const tasks = [...fileTasks, ...dirTasks];\n\n    if (this.debug) {\n      console.error(\n        pc.dim(\n          `[debug] Generation plan: ${files.length} files, ${tasks.length} tasks (${dirTasks.length} directories)`\n        )\n      );\n    }\n\n    // Release file content from PreparedFile objects to free memory.\n    // Content has already been embedded into task prompts by createFileTasks()\n    // and is no longer needed. The runner re-reads files from disk.\n    for (const file of files) {\n      (file as { content: string }).content = '';\n    }\n\n    const plan: GenerationPlan = {\n      files,\n      tasks,\n      complexity,\n      projectStructure,\n    };\n\n    // Emit plan created event\n    // +1 for root CLAUDE.md task added later by buildExecutionPlan()\n    this.tracer?.emit({\n      type: 'plan:created',\n      planType: 'generate',\n      fileCount: files.length,\n      taskCount: tasks.length + 1,\n    });\n\n    // Emit phase end\n    const planEndTime = process.hrtime.bigint();\n    const planDurationMs = Number(planEndTime - planStartTime) / 1_000_000;\n    this.tracer?.emit({\n      type: 'phase:end',\n      phase: 'plan-creation',\n      durationMs: planDurationMs,\n      tasksCompleted: 1,\n      tasksFailed: 0,\n    });\n\n    return plan;\n  }\n}\n\n/**\n * Create a generation orchestrator with default config.\n */\nexport function createOrchestrator(\n  config: Config,\n  projectRoot: string,\n  options?: { tracer?: ITraceWriter; debug?: boolean }\n): GenerationOrchestrator {\n  return new GenerationOrchestrator(config, projectRoot, options);\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**GenerationOrchestrator coordinates three-phase documentation generation: prepares files by reading content, creates file analysis tasks with prompts from buildFilePrompt, creates directory tasks for AGENTS.md synthesis, computes complexity metrics via analyzeComplexity, and emits trace events for plan creation.**\n\n## Exported Types\n\n**PreparedFile** represents a file ready for analysis with `filePath: string` (absolute), `relativePath: string` (from project root), `content: string` (file text).\n\n**AnalysisTask** discriminated union with `type: 'file' | 'directory'`. File tasks include `systemPrompt?: string` and `userPrompt?: string` set during creation. Directory tasks include `directoryInfo?: { sumFiles: string[], fileCount: number }` with prompts built at execution time by buildDirectoryPrompt (not in this file).\n\n**GenerationPlan** aggregates `files: PreparedFile[]`, `tasks: AnalysisTask[]`, `complexity: ComplexityMetrics`, `projectStructure?: string` (compact directory listing for AI context).\n\n## Class: GenerationOrchestrator\n\nConstructor accepts `config: Config`, `projectRoot: string`, `options?: { tracer?: ITraceWriter, debug?: boolean }`.\n\n**prepareFiles(discoveryResult: DiscoveryResult): Promise<PreparedFile[]>** reads file content via `readFile(filePath, 'utf-8')`, computes relative paths, silently skips unreadable files.\n\n**createFileTasks(files: PreparedFile[], projectStructure?: string): AnalysisTask[]** calls `buildFilePrompt({ filePath, content, projectPlan }, debug)` for each file, returns tasks with `type: 'file'`, `systemPrompt`, `userPrompt` populated.\n\n**createDirectoryTasks(files: PreparedFile[]): AnalysisTask[]** groups files by `path.dirname(relativePath)`, creates tasks with `type: 'directory'`, `directoryInfo.sumFiles` as array of `${relativePath}.sum` paths, `directoryInfo.fileCount` set to group size. Prompts not built here—deferred to execution time.\n\n**createPlan(discoveryResult: DiscoveryResult): Promise<GenerationPlan>** orchestrates plan creation: emits `phase:start` with `phase: 'plan-creation'`, calls `prepareFiles`, `analyzeComplexity`, `buildProjectStructure`, `createFileTasks`, `createDirectoryTasks`, concatenates file and directory tasks, emits `plan:created` with `planType: 'generate'` and `taskCount: tasks.length + 1` (accounting for root CLAUDE.md task added later), clears `PreparedFile.content` to free memory after prompt embedding, emits `phase:end` with `durationMs` computed via `process.hrtime.bigint()` delta.\n\n**buildProjectStructure(files: PreparedFile[]): string** (private) groups files into `Map<dirPath, fileNames[]>`, sorts directories and filenames, formats as indented text: `${dir}/\\n  ${file}` for bird's-eye AI context.\n\n## Factory Function\n\n**createOrchestrator(config: Config, projectRoot: string, options?: { tracer?: ITraceWriter, debug?: boolean }): GenerationOrchestrator** instantiates GenerationOrchestrator with provided parameters.\n\n## Integration Points\n\nImports `buildFilePrompt` from `./prompts/index.js` to construct system/user prompts for file tasks. Imports `analyzeComplexity` from `./complexity.js` to compute `ComplexityMetrics` (directory depth). Imports `ITraceWriter` from `../orchestration/trace.js` for event emission. Imports `Config` from `../config/schema.js` and `DiscoveryResult` from `../types/index.js`.\n\n## Trace Events\n\nEmits `phase:start` with `{ type: 'phase:start', phase: 'plan-creation', taskCount, concurrency: 1 }` at plan start. Emits `plan:created` with `{ type: 'plan:created', planType: 'generate', fileCount, taskCount }` after task creation. Emits `phase:end` with `{ type: 'phase:end', phase: 'plan-creation', durationMs, tasksCompleted: 1, tasksFailed: 0 }` at plan completion.\n\n## Memory Management\n\nAfter `createFileTasks` embeds file content into prompts, `createPlan` clears `PreparedFile.content` fields via `(file as { content: string }).content = ''` to free memory. Comment notes runner re-reads files from disk during execution.\n\n## Debug Logging\n\nWhen `debug: true`, logs to stderr via `console.error(pc.dim(...))` for: file preparation start, complexity analysis depth, plan summary with file/task/directory counts. Uses `picocolors` for dim formatting.\n\n## Task Count Calculation\n\nComment in `createPlan` notes `taskCount: tasks.length + 1` because `buildExecutionPlan()` (external to this file) adds root CLAUDE.md synthesis task after plan creation.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 1230,
      "cacheReadTokens": 17367,
      "cacheCreationTokens": 11537,
      "latencyMs": 28328,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/generation/orchestrator.ts",
          "sizeBytes": 8340
        }
      ]
    },
    {
      "timestamp": "2026-02-09T17:44:45.975Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/generation/executor.ts\n\n```typescript\n/**\n * Plan executor for documentation generation\n *\n * Builds execution plans from generation plans:\n * - File tasks as individual analysis jobs\n * - Directory completion tracking\n * - Markdown plan output for dry-run display\n */\n\nimport * as path from 'node:path';\nimport type { GenerationPlan } from './orchestrator.js';\nimport { sumFileExists } from './writers/sum.js';\n\n/**\n * Execution task ready for AI processing.\n */\nexport interface ExecutionTask {\n  /** Unique task ID */\n  id: string;\n  /** Task type */\n  type: 'file' | 'directory' | 'root-doc';\n  /** File or directory path (relative) */\n  path: string;\n  /** Absolute path */\n  absolutePath: string;\n  /** System prompt for AI */\n  systemPrompt: string;\n  /** User prompt for AI */\n  userPrompt: string;\n  /** Dependencies (task IDs that must complete first) */\n  dependencies: string[];\n  /** Output path for generated content */\n  outputPath: string;\n  /** Metadata for tracking */\n  metadata: {\n    directoryFiles?: string[];\n    /** Directory depth (for post-order traversal) */\n    depth?: number;\n    /** Package root path (for supplementary docs) */\n    packageRoot?: string;\n  };\n}\n\n/**\n * Execution plan with dependency graph.\n */\nexport interface ExecutionPlan {\n  /** Project root */\n  projectRoot: string;\n  /** All tasks in execution order */\n  tasks: ExecutionTask[];\n  /** File tasks (can run in parallel) */\n  fileTasks: ExecutionTask[];\n  /** Directory tasks (depend on file tasks) */\n  directoryTasks: ExecutionTask[];\n  /** Root document tasks (depend on directories) */\n  rootTasks: ExecutionTask[];\n  /** Directory to file mapping */\n  directoryFileMap: Record<string, string[]>;\n  /** Compact project directory listing for directory prompt context */\n  projectStructure?: string;\n}\n\n/**\n * Calculate directory depth (number of path segments).\n * Root \".\" has depth 0, \"src\" has depth 1, \"src/cli\" has depth 2, etc.\n */\nfunction getDirectoryDepth(dir: string): number {\n  if (dir === '.') return 0;\n  return dir.split(path.sep).length;\n}\n\n/**\n * Build execution plan from generation plan.\n *\n * Directory tasks are sorted using post-order traversal (deepest directories first)\n * so child AGENTS.md files are generated before their parents.\n */\nexport function buildExecutionPlan(\n  plan: GenerationPlan,\n  projectRoot: string\n): ExecutionPlan {\n  const fileTasks: ExecutionTask[] = [];\n  const directoryTasks: ExecutionTask[] = [];\n  const rootTasks: ExecutionTask[] = [];\n  const directoryFileMap: Record<string, string[]> = {};\n\n  // Track files by directory\n  for (const file of plan.files) {\n    const dir = path.dirname(file.relativePath);\n    if (!directoryFileMap[dir]) {\n      directoryFileMap[dir] = [];\n    }\n    directoryFileMap[dir].push(file.relativePath);\n  }\n\n  // Create file tasks\n  for (const task of plan.tasks) {\n    if (task.type === 'file') {\n      const absolutePath = path.join(projectRoot, task.filePath);\n      fileTasks.push({\n        id: `file:${task.filePath}`,\n        type: 'file',\n        path: task.filePath,\n        absolutePath,\n        systemPrompt: task.systemPrompt!,\n        userPrompt: task.userPrompt!,\n        dependencies: [],\n        outputPath: `${absolutePath}.sum`,\n        metadata: {},\n      });\n    }\n  }\n\n  // Sort file tasks by directory depth (deepest first) for post-order traversal\n  fileTasks.sort((a, b) => {\n    const depthA = getDirectoryDepth(path.dirname(a.path));\n    const depthB = getDirectoryDepth(path.dirname(b.path));\n    return depthB - depthA;\n  });\n\n  // Create directory tasks in post-order (deepest first)\n  // Sort directories by depth descending so children are processed before parents\n  const sortedDirs = Object.entries(directoryFileMap).sort(\n    ([dirA], [dirB]) => getDirectoryDepth(dirB) - getDirectoryDepth(dirA)\n  );\n\n  for (const [dir, files] of sortedDirs) {\n    const dirAbsPath = path.join(projectRoot, dir);\n    const fileTaskIds = files.map(f => `file:${f}`);\n\n    directoryTasks.push({\n      id: `dir:${dir}`,\n      type: 'directory',\n      path: dir,\n      absolutePath: dirAbsPath,\n      systemPrompt: 'Built at execution time by buildDirectoryPrompt()',\n      userPrompt: `Directory \"${dir}\" — ${files.length} files. Prompt populated from .sum files at runtime.`,\n      dependencies: fileTaskIds,\n      outputPath: path.join(dirAbsPath, 'AGENTS.md'),\n      metadata: {\n        directoryFiles: files,\n        depth: getDirectoryDepth(dir),\n      },\n    });\n  }\n\n  // Create root document tasks\n  const allDirTaskIds = Object.keys(directoryFileMap).map(d => `dir:${d}`);\n\n  rootTasks.push({\n    id: 'root:CLAUDE.md',\n    type: 'root-doc',\n    path: 'CLAUDE.md',\n    absolutePath: path.join(projectRoot, 'CLAUDE.md'),\n    // Prompts are built at runtime by buildRootPrompt() in runner.ts Phase 3.\n    // These placeholders exist only for plan display and dependency tracking.\n    systemPrompt: 'Built at runtime by buildRootPrompt()',\n    userPrompt: 'Root document — prompt populated from AGENTS.md files at runtime.',\n    dependencies: allDirTaskIds,\n    outputPath: path.join(projectRoot, 'CLAUDE.md'),\n    metadata: {},\n  });\n\n  return {\n    projectRoot,\n    tasks: [...fileTasks, ...directoryTasks, ...rootTasks],\n    fileTasks,\n    directoryTasks,\n    rootTasks,\n    directoryFileMap,\n    projectStructure: plan.projectStructure,\n  };\n}\n\n/**\n * Check if all files in a directory have been analyzed (.sum files exist).\n */\nexport async function isDirectoryComplete(\n  dirPath: string,\n  expectedFiles: string[],\n  projectRoot: string\n): Promise<{ complete: boolean; missing: string[] }> {\n  const missing: string[] = [];\n\n  for (const relativePath of expectedFiles) {\n    const absolutePath = path.join(projectRoot, relativePath);\n    const exists = await sumFileExists(absolutePath);\n    if (!exists) {\n      missing.push(relativePath);\n    }\n  }\n\n  return {\n    complete: missing.length === 0,\n    missing,\n  };\n}\n\n/**\n * Get all directories that are ready for AGENTS.md generation.\n * A directory is ready when all its files have .sum files.\n */\nexport async function getReadyDirectories(\n  executionPlan: ExecutionPlan\n): Promise<string[]> {\n  const ready: string[] = [];\n\n  for (const [dir, files] of Object.entries(executionPlan.directoryFileMap)) {\n    const { complete } = await isDirectoryComplete(\n      dir,\n      files,\n      executionPlan.projectRoot\n    );\n    if (complete) {\n      ready.push(dir);\n    }\n  }\n\n  return ready;\n}\n\n/**\n * Format execution plan as markdown for GENERATION-PLAN.md.\n * Uses post-order traversal (deepest directories first).\n */\nexport function formatExecutionPlanAsMarkdown(plan: ExecutionPlan): string {\n  const lines: string[] = [];\n  const today = new Date().toISOString().split('T')[0];\n\n  // Header\n  lines.push('# Documentation Generation Plan');\n  lines.push('');\n  lines.push(`Generated: ${today}`);\n  lines.push(`Project: ${plan.projectRoot}`);\n  lines.push('');\n\n  // Summary\n  lines.push('## Summary');\n  lines.push('');\n  lines.push(`- **Total Tasks**: ${plan.tasks.length}`);\n  lines.push(`- **File Tasks**: ${plan.fileTasks.length}`);\n  lines.push(`- **Directory Tasks**: ${plan.directoryTasks.length}`);\n  lines.push(`- **Root Tasks**: ${plan.rootTasks.length}`);\n  lines.push('- **Traversal**: Post-order (children before parents)');\n  lines.push('');\n  lines.push('---');\n  lines.push('');\n\n  // Phase 1: File Analysis\n  lines.push('## Phase 1: File Analysis (Post-Order Traversal)');\n  lines.push('');\n\n  // Group files by directory, use directory task order (already post-order)\n  // Deduplicate paths\n  const filesByDir: Record<string, Set<string>> = {};\n  for (const task of plan.fileTasks) {\n    const dir = task.path.includes('/')\n      ? task.path.substring(0, task.path.lastIndexOf('/'))\n      : '.';\n    if (!filesByDir[dir]) filesByDir[dir] = new Set();\n    filesByDir[dir].add(task.path);\n  }\n\n  // Output files grouped by directory in post-order (using directoryTasks order)\n  for (const dirTask of plan.directoryTasks) {\n    const dir = dirTask.path;\n    const filesSet = filesByDir[dir];\n    if (filesSet && filesSet.size > 0) {\n      const files = Array.from(filesSet);\n      const depth = dirTask.metadata.depth ?? 0;\n      lines.push(`### Depth ${depth}: ${dir}/ (${files.length} files)`);\n      for (const file of files) {\n        lines.push(`- [ ] \\`${file}\\``);\n      }\n      lines.push('');\n    }\n  }\n\n  lines.push('---');\n  lines.push('');\n\n  // Phase 2: Directory AGENTS.md\n  lines.push(`## Phase 2: Directory AGENTS.md (Post-Order Traversal, ${plan.directoryTasks.length} directories)`);\n  lines.push('');\n\n  // Group by depth\n  const dirsByDepth: Record<number, string[]> = {};\n  for (const task of plan.directoryTasks) {\n    const depth = task.metadata.depth ?? 0;\n    if (!dirsByDepth[depth]) dirsByDepth[depth] = [];\n    dirsByDepth[depth].push(task.path);\n  }\n\n  // Output in depth order (descending)\n  const depths = Object.keys(dirsByDepth).map(Number).sort((a, b) => b - a);\n  for (const depth of depths) {\n    lines.push(`### Depth ${depth}`);\n    for (const dir of dirsByDepth[depth]) {\n      const suffix = dir === '.' ? ' (root)' : '';\n      lines.push(`- [ ] \\`${dir}/AGENTS.md\\`${suffix}`);\n    }\n    lines.push('');\n  }\n\n  lines.push('---');\n  lines.push('');\n\n  // Phase 3: Root Documents\n  lines.push('## Phase 3: Root Documents');\n  lines.push('');\n  lines.push('- [ ] `CLAUDE.md`');\n  lines.push('');\n\n  return lines.join('\\n');\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**executor.ts builds ExecutionPlan from GenerationPlan with dependency graph, post-order directory traversal sorting (deepest-first), directory completion tracking via sumFileExists(), and markdown plan formatting for GENERATION-PLAN.md output.**\n\n## Exported Types\n\n**ExecutionTask** represents a single AI processing job with fields:\n- `id: string` — unique identifier with format `\"file:{path}\"`, `\"dir:{path}\"`, or `\"root:{docname}\"`\n- `type: 'file' | 'directory' | 'root-doc'` — discriminates task category\n- `path: string` — relative path to file or directory\n- `absolutePath: string` — resolved absolute path\n- `systemPrompt: string` — AI system prompt (placeholder for dir/root tasks, built at runtime)\n- `userPrompt: string` — AI user prompt (placeholder for dir/root tasks, built at runtime)\n- `dependencies: string[]` — array of task IDs that must complete first\n- `outputPath: string` — destination path for generated content (`.sum` for files, `AGENTS.md` for dirs, root doc name for root)\n- `metadata: { directoryFiles?: string[], depth?: number, packageRoot?: string }` — tracking metadata\n\n**ExecutionPlan** aggregates all tasks with dependency relationships:\n- `projectRoot: string` — absolute project root path\n- `tasks: ExecutionTask[]` — all tasks in execution order\n- `fileTasks: ExecutionTask[]` — Phase 1 file analysis tasks (parallel eligible)\n- `directoryTasks: ExecutionTask[]` — Phase 2 directory aggregation tasks (post-order sorted)\n- `rootTasks: ExecutionTask[]` — Phase 3 root document synthesis tasks (sequential)\n- `directoryFileMap: Record<string, string[]>` — maps directory path to relative file paths\n- `projectStructure?: string` — optional compact directory listing for prompt context\n\n## Core Functions\n\n**buildExecutionPlan(plan: GenerationPlan, projectRoot: string): ExecutionPlan** constructs dependency graph from GenerationPlan:\n1. Populates `directoryFileMap` by extracting `path.dirname()` from each `file.relativePath`\n2. Creates file tasks with `id: \"file:{filePath}\"`, `type: 'file'`, `dependencies: []`, `outputPath: \"{absolutePath}.sum\"`\n3. Sorts file tasks by directory depth descending via `getDirectoryDepth(path.dirname(a.path))` comparison (deepest first for post-order traversal)\n4. Sorts directories by depth descending via `Object.entries(directoryFileMap).sort(([dirA], [dirB]) => getDirectoryDepth(dirB) - getDirectoryDepth(dirA))`\n5. Creates directory tasks with `id: \"dir:{dir}\"`, `type: 'directory'`, `dependencies: fileTaskIds` (all file tasks in directory), `outputPath: \"{dirAbsPath}/AGENTS.md\"`, `metadata: { directoryFiles, depth }`\n6. Creates root tasks with `id: \"root:CLAUDE.md\"`, `type: 'root-doc'`, `dependencies: allDirTaskIds`, `outputPath: \"{projectRoot}/CLAUDE.md\"`\n7. Returns ExecutionPlan with fileTasks/directoryTasks/rootTasks arrays and directoryFileMap\n\n**getDirectoryDepth(dir: string): number** calculates path segment count:\n- Returns `0` for root directory `\".\"`\n- Returns `dir.split(path.sep).length` for non-root directories\n\n**isDirectoryComplete(dirPath: string, expectedFiles: string[], projectRoot: string): Promise<{ complete: boolean; missing: string[] }>** checks if all files have `.sum` outputs:\n- Iterates `expectedFiles` array, resolves absolute path via `path.join(projectRoot, relativePath)`\n- Calls `sumFileExists(absolutePath)` for each file\n- Returns `{ complete: missing.length === 0, missing }` with array of paths lacking `.sum` files\n\n**getReadyDirectories(executionPlan: ExecutionPlan): Promise<string[]>** identifies directories eligible for AGENTS.md generation:\n- Iterates `executionPlan.directoryFileMap` entries\n- Calls `isDirectoryComplete(dir, files, executionPlan.projectRoot)` for each directory\n- Returns array of directory paths where `complete === true`\n\n**formatExecutionPlanAsMarkdown(plan: ExecutionPlan): string** generates GENERATION-PLAN.md content:\n- Writes header with ISO date (`new Date().toISOString().split('T')[0]`) and `plan.projectRoot`\n- Writes summary section with task counts: `plan.tasks.length`, `plan.fileTasks.length`, `plan.directoryTasks.length`, `plan.rootTasks.length`\n- Writes Phase 1 section grouping files by directory, outputting in post-order using `plan.directoryTasks` traversal order\n- Writes Phase 2 section grouping directories by depth (descending), outputting `{dir}/AGENTS.md` with `(root)` suffix for `dir === \".\"`\n- Writes Phase 3 section listing `CLAUDE.md`\n- Returns markdown string with checkbox format: `- [ ] \\`{path}\\``\n\n## Post-Order Traversal Strategy\n\nbuildExecutionPlan() enforces post-order traversal (children before parents) via two sorting operations:\n1. **File tasks**: sorted by `getDirectoryDepth(path.dirname(a.path))` descending so deepest files process first\n2. **Directory tasks**: sorted by `getDirectoryDepth(dirB) - getDirectoryDepth(dirA)` descending so child directories generate AGENTS.md before parent directories\n\nThis ensures child AGENTS.md files exist before parent directory aggregation prompts consume them (referenced in src/generation/prompts/builder.ts buildDirectoryPrompt() via collectAgentsDocs() recursive traversal).\n\n## Integration Points\n\n- **GenerationPlan import** from `./orchestrator.js` consumed by buildExecutionPlan()\n- **sumFileExists()** from `./writers/sum.js` checks for `.sum` file presence in isDirectoryComplete()\n- **Dependency tracking** via `dependencies: string[]` enforces Phase 1 → Phase 2 → Phase 3 sequencing in src/orchestration/runner.ts\n- **Prompt placeholders** for directory/root tasks (`\"Built at runtime by buildRootPrompt()\"`) signal runtime prompt construction in runner.ts Phase 2/3\n\n## Placeholder Prompt Pattern\n\nExecutionTask includes `systemPrompt` and `userPrompt` fields populated with placeholders for directory/root tasks:\n- Directory tasks: `systemPrompt: \"Built at execution time by buildDirectoryPrompt()\"`, `userPrompt: \"Directory \\\"{dir}\\\" — {files.length} files. Prompt populated from .sum files at runtime.\"`\n- Root tasks: `systemPrompt: \"Built at runtime by buildRootPrompt()\"`, `userPrompt: \"Root document — prompt populated from AGENTS.md files at runtime.\"`\n\nThese placeholders exist for plan display (formatExecutionPlanAsMarkdown()) and dependency tracking; actual prompts constructed at execution time in runner.ts via buildDirectoryPrompt() and buildRootPrompt() from src/generation/prompts/builder.ts.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 1760,
      "cacheReadTokens": 17367,
      "cacheCreationTokens": 12038,
      "latencyMs": 35051,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/generation/executor.ts",
          "sizeBytes": 9421
        }
      ]
    },
    {
      "timestamp": "2026-02-09T17:45:00.107Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/installer/paths.ts\n\n```typescript\n/**\n * Path resolution for installer module\n *\n * Provides cross-platform path resolution for AI coding assistant runtimes.\n * Uses os.homedir() for global paths and path.join() for cross-platform compatibility.\n */\n\nimport * as os from 'node:os';\nimport * as path from 'node:path';\nimport { stat } from 'node:fs/promises';\nimport type { Runtime, Location, RuntimePaths } from './types.js';\n\n/**\n * Get all supported runtime identifiers (excludes 'all' meta-runtime)\n *\n * @returns Array of concrete runtime identifiers\n */\nexport function getAllRuntimes(): Array<Exclude<Runtime, 'all'>> {\n  return ['claude', 'opencode', 'gemini'];\n}\n\n/**\n * Get path configuration for a specific runtime\n *\n * Returns global and local installation paths plus settings file location.\n * All paths are cross-platform using os.homedir() and path.join().\n *\n * Environment variable overrides (in priority order):\n * - Claude: CLAUDE_CONFIG_DIR\n * - OpenCode: OPENCODE_CONFIG_DIR > XDG_CONFIG_HOME/opencode\n * - Gemini: GEMINI_CONFIG_DIR\n *\n * @param runtime - Target runtime (claude, opencode, or gemini)\n * @returns Path configuration object with global, local, and settingsFile paths\n */\nexport function getRuntimePaths(runtime: Exclude<Runtime, 'all'>): RuntimePaths {\n  const home = os.homedir();\n\n  switch (runtime) {\n    case 'claude': {\n      // CLAUDE_CONFIG_DIR overrides default ~/.claude\n      const globalPath = process.env.CLAUDE_CONFIG_DIR || path.join(home, '.claude');\n      return {\n        global: globalPath,\n        local: '.claude',\n        settingsFile: path.join(globalPath, 'settings.json'),\n      };\n    }\n\n    case 'opencode': {\n      // OPENCODE_CONFIG_DIR > XDG_CONFIG_HOME/opencode > ~/.config/opencode\n      const xdgConfig = process.env.XDG_CONFIG_HOME || path.join(home, '.config');\n      const globalPath = process.env.OPENCODE_CONFIG_DIR || path.join(xdgConfig, 'opencode');\n      return {\n        global: globalPath,\n        local: '.opencode',\n        settingsFile: path.join(globalPath, 'settings.json'),\n      };\n    }\n\n    case 'gemini': {\n      // GEMINI_CONFIG_DIR overrides default ~/.gemini\n      const globalPath = process.env.GEMINI_CONFIG_DIR || path.join(home, '.gemini');\n      return {\n        global: globalPath,\n        local: '.gemini',\n        settingsFile: path.join(globalPath, 'settings.json'),\n      };\n    }\n  }\n}\n\n/**\n * Resolve full installation path for a runtime and location\n *\n * For global location, returns the absolute global path.\n * For local location, returns the local path joined with project root.\n *\n * @param runtime - Target runtime (claude, opencode, or gemini)\n * @param location - Installation location (global or local)\n * @param projectRoot - Project root directory for local installs (defaults to cwd)\n * @returns Resolved absolute path for installation\n */\nexport function resolveInstallPath(\n  runtime: Exclude<Runtime, 'all'>,\n  location: Location,\n  projectRoot?: string,\n): string {\n  const paths = getRuntimePaths(runtime);\n\n  if (location === 'global') {\n    return paths.global;\n  }\n\n  // Local installation - join with project root or cwd\n  const root = projectRoot || process.cwd();\n  return path.join(root, paths.local);\n}\n\n/**\n * Get the settings file path for a runtime\n *\n * Settings files are used for hook registration (Claude Code uses settings.json).\n *\n * @param runtime - Target runtime (claude, opencode, or gemini)\n * @returns Absolute path to the settings file\n */\nexport function getSettingsPath(runtime: Exclude<Runtime, 'all'>): string {\n  return getRuntimePaths(runtime).settingsFile;\n}\n\n/**\n * Check if a runtime is installed locally in a project.\n *\n * Checks for the presence of the local config directory (e.g., .claude, .opencode, .gemini).\n *\n * @param runtime - Target runtime (claude, opencode, or gemini)\n * @param projectRoot - Project root directory to check\n * @returns True if the runtime's local config directory exists\n */\nexport async function isRuntimeInstalledLocally(\n  runtime: Exclude<Runtime, 'all'>,\n  projectRoot: string\n): Promise<boolean> {\n  const paths = getRuntimePaths(runtime);\n  const localPath = path.join(projectRoot, paths.local);\n\n  try {\n    const stats = await stat(localPath);\n    return stats.isDirectory();\n  } catch {\n    return false;\n  }\n}\n\n/**\n * Check if a runtime is installed globally.\n *\n * Checks for the presence of the global config directory.\n *\n * @param runtime - Target runtime (claude, opencode, or gemini)\n * @returns True if the runtime's global config directory exists\n */\nexport async function isRuntimeInstalledGlobally(\n  runtime: Exclude<Runtime, 'all'>\n): Promise<boolean> {\n  const paths = getRuntimePaths(runtime);\n\n  try {\n    const stats = await stat(paths.global);\n    return stats.isDirectory();\n  } catch {\n    return false;\n  }\n}\n\n/**\n * Get list of runtimes installed locally in a project.\n *\n * @param projectRoot - Project root directory to check\n * @returns Array of runtime identifiers that are installed locally\n */\nexport async function getInstalledRuntimes(\n  projectRoot: string\n): Promise<Array<Exclude<Runtime, 'all'>>> {\n  const runtimes = getAllRuntimes();\n  const installed: Array<Exclude<Runtime, 'all'>> = [];\n\n  for (const runtime of runtimes) {\n    if (await isRuntimeInstalledLocally(runtime, projectRoot)) {\n      installed.push(runtime);\n    }\n  }\n\n  return installed;\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**paths.ts resolves cross-platform installation paths for AI runtime configurations (Claude Code, OpenCode, Gemini) with environment variable overrides and directory existence checks.**\n\n## Exported Functions\n\n**getAllRuntimes()** → `Array<Exclude<Runtime, 'all'>>`\nReturns hardcoded array `['claude', 'opencode', 'gemini']` excluding the meta-runtime `'all'`.\n\n**getRuntimePaths(runtime: Exclude<Runtime, 'all'>)** → `RuntimePaths`\nReturns path configuration object with `global` (absolute path to runtime config dir), `local` (relative project-local dir name), and `settingsFile` (absolute path to settings.json). Environment overrides: `CLAUDE_CONFIG_DIR` for Claude (`~/.claude` fallback), `OPENCODE_CONFIG_DIR` or `XDG_CONFIG_HOME/opencode` for OpenCode (`~/.config/opencode` fallback), `GEMINI_CONFIG_DIR` for Gemini (`~/.gemini` fallback). Uses `os.homedir()` for home resolution and `path.join()` for cross-platform compatibility.\n\n**resolveInstallPath(runtime, location, projectRoot?)** → `string`\nResolves absolute installation path. For `location === 'global'`, returns `getRuntimePaths(runtime).global`. For local, joins `projectRoot || process.cwd()` with local path name (`.claude`, `.opencode`, or `.gemini`).\n\n**getSettingsPath(runtime: Exclude<Runtime, 'all'>)** → `string`\nReturns `getRuntimePaths(runtime).settingsFile` absolute path. Used for hook registration in Claude Code's settings.json.\n\n**isRuntimeInstalledLocally(runtime, projectRoot)** → `Promise<boolean>`\nChecks if local config directory exists via `stat()` and `stats.isDirectory()`. Joins `projectRoot` with local path name. Returns `false` on error.\n\n**isRuntimeInstalledGlobally(runtime)** → `Promise<boolean>`\nChecks if global config directory exists via `stat()` and `stats.isDirectory()` on `getRuntimePaths(runtime).global`. Returns `false` on error.\n\n**getInstalledRuntimes(projectRoot)** → `Promise<Array<Exclude<Runtime, 'all'>>>`\nIterates `getAllRuntimes()`, filters via `isRuntimeInstalledLocally()`, accumulates installed runtime identifiers into returned array.\n\n## Environment Variable Overrides\n\n- **Claude**: `CLAUDE_CONFIG_DIR` overrides `~/.claude`\n- **OpenCode**: `OPENCODE_CONFIG_DIR` > `XDG_CONFIG_HOME/opencode` > `~/.config/opencode` (priority order)\n- **Gemini**: `GEMINI_CONFIG_DIR` overrides `~/.gemini`\n\n## Path Patterns\n\n**Global paths:**\n- Claude: `~/.claude` or `$CLAUDE_CONFIG_DIR`\n- OpenCode: `~/.config/opencode`, `$XDG_CONFIG_HOME/opencode`, or `$OPENCODE_CONFIG_DIR`\n- Gemini: `~/.gemini` or `$GEMINI_CONFIG_DIR`\n\n**Local paths:**\n- Claude: `.claude` (relative)\n- OpenCode: `.opencode` (relative)\n- Gemini: `.gemini` (relative)\n\n**Settings files:**\n- All runtimes: `<global_path>/settings.json`\n\n## Dependencies\n\nImports `os.homedir()` for home directory resolution, `path.join()` for cross-platform path construction, `stat()` from `node:fs/promises` for directory existence checks, and `Runtime`, `Location`, `RuntimePaths` types from `./types.js`.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 900,
      "cacheReadTokens": 17367,
      "cacheCreationTokens": 10689,
      "latencyMs": 21030,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/installer/paths.ts",
          "sizeBytes": 5356
        }
      ]
    },
    {
      "timestamp": "2026-02-09T17:45:08.860Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/integration/types.ts\n\n```typescript\n/**\n * Integration types for AI coding assistant environments\n *\n * Defines types for detecting AI assistant environments (Claude Code, OpenCode, etc.)\n * and generating appropriate integration templates (command files, hooks).\n */\n\n/**\n * Supported AI coding assistant environment types\n */\nexport type EnvironmentType = 'claude' | 'opencode' | 'aider' | 'gemini';\n\n/**\n * Result of detecting an AI coding assistant environment\n */\nexport interface DetectedEnvironment {\n  /** Type of AI assistant detected */\n  type: EnvironmentType;\n  /** Configuration directory for this environment (e.g., '.claude', '.opencode') */\n  configDir: string;\n  /** Whether this environment was detected in the project */\n  detected: boolean;\n}\n\n/**\n * Template for integration files (command files, hooks, etc.)\n */\nexport interface IntegrationTemplate {\n  /** File name (e.g., 'generate.md') */\n  filename: string;\n  /** Relative path from project root (e.g., '.claude/commands/ar/generate.md') */\n  path: string;\n  /** Template content to write to the file */\n  content: string;\n}\n\n/**\n * Result of generating integration files for an environment\n */\nexport interface IntegrationResult {\n  /** Environment type that was configured */\n  environment: EnvironmentType;\n  /** Files that were successfully created */\n  filesCreated: string[];\n  /** Files that were skipped (already exist) */\n  filesSkipped: string[];\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**Defines TypeScript interfaces and discriminated union types for AI coding assistant environment detection (Claude Code, OpenCode, Aider, Gemini) and integration template generation.**\n\n## Exported Types\n\n### EnvironmentType\nUnion type `'claude' | 'opencode' | 'aider' | 'gemini'` representing supported AI assistant platforms.\n\n### DetectedEnvironment\nInterface describing environment detection results with properties:\n- `type: EnvironmentType` — Platform identifier\n- `configDir: string` — Configuration directory path (e.g., `.claude`, `.opencode`)\n- `detected: boolean` — Whether environment was found in project\n\n### IntegrationTemplate\nInterface defining template structure for command files and hooks with properties:\n- `filename: string` — File name (e.g., `generate.md`)\n- `path: string` — Relative path from project root (e.g., `.claude/commands/ar/generate.md`)\n- `content: string` — Template content to write\n\n### IntegrationResult\nInterface describing integration file generation outcome with properties:\n- `environment: EnvironmentType` — Platform that was configured\n- `filesCreated: string[]` — Successfully written file paths\n- `filesSkipped: string[]` — Paths skipped due to existing files\n\n## Integration with Module\n\nConsumed by `src/integration/detect.ts` for environment detection logic, `src/integration/generate.ts` for template instantiation, and `src/integration/templates.ts` for platform-specific content generation. Supports installer workflow in `src/installer/` for command/hook registration during `npx agents-reverse-engineer --runtime <env>` execution.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 382,
      "cacheReadTokens": 0,
      "cacheCreationTokens": 26831,
      "latencyMs": 14614,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/integration/types.ts",
          "sizeBytes": 1400
        }
      ]
    },
    {
      "timestamp": "2026-02-09T17:45:02.926Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/installer/types.ts\n\n```typescript\n/**\n * Installer types for npx installation workflow\n *\n * Defines types for the interactive installer that copies command files and hooks\n * to runtime-specific directories (Claude Code, OpenCode, Gemini).\n */\n\n/**\n * Supported AI coding assistant runtimes for installation\n *\n * - 'claude': Claude Code (~/.claude or .claude)\n * - 'opencode': OpenCode (~/.config/opencode or .opencode)\n * - 'gemini': Gemini (~/.gemini or .gemini)\n * - 'all': Install to all supported runtimes\n */\nexport type Runtime = 'claude' | 'opencode' | 'gemini' | 'all';\n\n/**\n * Installation location target\n *\n * - 'global': User-level installation (~/.claude, ~/.config/opencode, etc.)\n * - 'local': Project-level installation (.claude, .opencode, etc.)\n */\nexport type Location = 'global' | 'local';\n\n/**\n * Arguments parsed from installer command line\n *\n * Supports both interactive mode (prompts) and non-interactive mode (flags).\n */\nexport interface InstallerArgs {\n  /** Target runtime (claude, opencode, gemini, or all) */\n  runtime?: Runtime;\n  /** Install to global/user location */\n  global: boolean;\n  /** Install to local/project location */\n  local: boolean;\n  /** Uninstall instead of install */\n  uninstall: boolean;\n  /** Force overwrite existing files */\n  force: boolean;\n  /** Show help and exit */\n  help: boolean;\n  /** Suppress banner and info messages */\n  quiet: boolean;\n}\n\n/**\n * Result of an installation operation for a single runtime/location\n */\nexport interface InstallerResult {\n  /** Whether the installation succeeded */\n  success: boolean;\n  /** Runtime that was installed */\n  runtime: Exclude<Runtime, 'all'>;\n  /** Location that was installed to */\n  location: Location;\n  /** Files that were successfully created */\n  filesCreated: string[];\n  /** Files that were skipped (already exist, no --force) */\n  filesSkipped: string[];\n  /** Error messages if any */\n  errors: string[];\n  /** Whether hook was registered in settings.json (Claude only) */\n  hookRegistered?: boolean;\n  /** Whether VERSION file was written */\n  versionWritten?: boolean;\n}\n\n/**\n * Path configuration for a specific runtime\n *\n * Contains resolved paths for global and local installation locations.\n */\nexport interface RuntimePaths {\n  /** Global installation path (e.g., ~/.claude) */\n  global: string;\n  /** Local installation path (e.g., .claude) */\n  local: string;\n  /** Path to settings.json for hook registration */\n  settingsFile: string;\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**Defines TypeScript interfaces for npx installer workflow supporting multi-runtime installation (Claude Code, OpenCode, Gemini) with global/local location targeting, force overwrite, and result tracking.**\n\n## Exported Types\n\n**`Runtime`** — Union type `'claude' | 'opencode' | 'gemini' | 'all'` specifying target AI assistant runtime for installation. Maps to directory paths: `claude` → `~/.claude`, `opencode` → `~/.config/opencode`, `gemini` → `~/.gemini`, `all` → installs to all three runtimes sequentially.\n\n**`Location`** — Union type `'global' | 'local'` controlling installation scope. `global` targets user-level directories (`~/.claude`), `local` targets project-level directories (`.claude`).\n\n**`InstallerArgs`** — Command-line arguments interface with fields:\n- `runtime?: Runtime` — Optional target runtime (if undefined, prompts interactively)\n- `global: boolean` — Enable global installation flag\n- `local: boolean` — Enable local installation flag\n- `uninstall: boolean` — Uninstall mode flag\n- `force: boolean` — Overwrite existing files without prompting\n- `help: boolean` — Display help and exit\n- `quiet: boolean` — Suppress banner and informational output\n\n**`InstallerResult`** — Operation result interface with fields:\n- `success: boolean` — Overall operation success status\n- `runtime: Exclude<Runtime, 'all'>` — Excludes `'all'` union member, enforces single runtime per result\n- `location: Location` — Installation target location\n- `filesCreated: string[]` — Absolute paths of successfully written files\n- `filesSkipped: string[]` — Files skipped due to pre-existence without `--force`\n- `errors: string[]` — Error messages encountered during operation\n- `hookRegistered?: boolean` — Optional field for Claude runtime indicating SessionStart/SessionEnd hook registration in `settings.json`\n- `versionWritten?: boolean` — Optional field indicating VERSION file creation (used by update checker hooks)\n\n**`RuntimePaths`** — Resolved path configuration interface with fields:\n- `global: string` — Absolute path to global runtime directory (e.g., `~/.claude`)\n- `local: string` — Absolute path to local runtime directory (e.g., `.claude`)\n- `settingsFile: string` — Absolute path to runtime settings file for hook registration (e.g., `~/.claude/settings.json`)\n\n## Type Constraints\n\n**`Exclude<Runtime, 'all'>`** in `InstallerResult.runtime` prevents reporting aggregate results for multi-runtime installations. Enforces atomic result objects per runtime/location pair. Installer orchestrator must decompose `'all'` into separate operations yielding distinct `InstallerResult` instances for `'claude'`, `'opencode'`, `'gemini'`.\n\n## Integration Points\n\nConsumed by `src/installer/operations.ts` for file copying logic, `src/installer/prompts.ts` for interactive runtime selection, `src/installer/paths.ts` for directory resolution with environment variable overrides (`CLAUDE_CONFIG_DIR`, `OPENCODE_CONFIG_DIR`, `GEMINI_CONFIG_DIR`), and `src/installer/uninstall.ts` for file deletion workflow.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 786,
      "cacheReadTokens": 17367,
      "cacheCreationTokens": 9788,
      "latencyMs": 22145,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/installer/types.ts",
          "sizeBytes": 2442
        }
      ]
    },
    {
      "timestamp": "2026-02-09T17:44:57.446Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/installer/index.ts\n\n```typescript\n/**\n * Main installer entry point for agents-reverse-engineer\n *\n * Provides the runInstaller function for npx installation workflow.\n * Supports interactive prompts and non-interactive flags for CI/scripted installs.\n */\n\nimport type { InstallerArgs, InstallerResult, Runtime, Location } from './types.js';\nimport { getAllRuntimes, resolveInstallPath } from './paths.js';\nimport {\n  displayBanner,\n  showHelp,\n  showSuccess,\n  showError,\n  showWarning,\n  showInfo,\n  showNextSteps,\n} from './banner.js';\nimport { selectRuntime, selectLocation, confirmAction, isInteractive } from './prompts.js';\nimport { installFiles, verifyInstallation, formatInstallResult } from './operations.js';\nimport { uninstallFiles, deleteConfigFolder } from './uninstall.js';\n\n// Re-export types for external consumers\nexport type { InstallerArgs, InstallerResult, Runtime, Location, RuntimePaths } from './types.js';\nexport { getRuntimePaths, getAllRuntimes, resolveInstallPath, getSettingsPath } from './paths.js';\nexport { displayBanner, showHelp, showSuccess, showError, showWarning, showInfo, showNextSteps, VERSION } from './banner.js';\nexport { selectRuntime, selectLocation, confirmAction, isInteractive } from './prompts.js';\n\n/**\n * Parse command-line arguments for the installer\n *\n * Handles both short (-g, -l, -h) and long (--global, --local, --help) flags.\n * Uses pattern from cli/index.ts for consistency.\n *\n * @param args - Command line arguments (process.argv.slice(2))\n * @returns Parsed installer arguments\n */\nexport function parseInstallerArgs(args: string[]): InstallerArgs {\n  const flags = new Set<string>();\n  const values = new Map<string, string>();\n\n  for (let i = 0; i < args.length; i++) {\n    const arg = args[i];\n\n    if (arg === '--runtime' && i + 1 < args.length) {\n      // --runtime requires a value\n      values.set('runtime', args[++i]);\n    } else if (arg === '-g' || arg === '--global') {\n      flags.add('global');\n    } else if (arg === '-l' || arg === '--local') {\n      flags.add('local');\n    } else if (arg === '--force') {\n      flags.add('force');\n    } else if (arg === '-q' || arg === '--quiet') {\n      flags.add('quiet');\n    } else if (arg === '-h' || arg === '--help') {\n      flags.add('help');\n    }\n  }\n\n  // Validate runtime value if provided\n  const runtimeValue = values.get('runtime');\n  const validRuntimes: Runtime[] = ['claude', 'opencode', 'gemini', 'all'];\n  const runtime = runtimeValue && validRuntimes.includes(runtimeValue as Runtime)\n    ? (runtimeValue as Runtime)\n    : undefined;\n\n  return {\n    runtime,\n    global: flags.has('global'),\n    local: flags.has('local'),\n    uninstall: false, // Set by 'uninstall' command, not flags\n    force: flags.has('force'),\n    help: flags.has('help'),\n    quiet: flags.has('quiet'),\n  };\n}\n\n/**\n * Determine installation location from args or return undefined for prompt\n *\n * @param args - Parsed installer arguments\n * @returns Location if specified, undefined if needs prompt\n */\nfunction determineLocation(args: InstallerArgs): Location | undefined {\n  if (args.global && !args.local) {\n    return 'global';\n  }\n  if (args.local && !args.global) {\n    return 'local';\n  }\n  // Both or neither - needs interactive prompt\n  return undefined;\n}\n\n/**\n * Determine target runtimes from args\n *\n * @param runtime - Runtime from args (may be 'all' or specific runtime)\n * @returns Array of specific runtimes to install to\n */\nfunction determineRuntimes(runtime: Runtime | undefined): Array<Exclude<Runtime, 'all'>> {\n  if (!runtime) {\n    // No runtime specified - will need interactive prompt\n    return [];\n  }\n  if (runtime === 'all') {\n    return getAllRuntimes();\n  }\n  return [runtime];\n}\n\n/**\n * Run the installer workflow\n *\n * This is the main entry point for the installation process.\n * Supports both interactive mode (prompts) and non-interactive mode (flags).\n *\n * @param args - Parsed installer arguments\n * @returns Array of installation results (one per runtime/location combination)\n */\nexport async function runInstaller(args: InstallerArgs): Promise<InstallerResult[]> {\n  // Handle help flag\n  if (args.help) {\n    showHelp();\n    return [];\n  }\n\n  // Display banner unless quiet mode\n  if (!args.quiet) {\n    displayBanner();\n  }\n\n  // Determine location and runtimes from flags\n  let location = determineLocation(args);\n  const runtimeArg = args.runtime;\n\n  // Non-interactive mode: require all flags\n  if (!isInteractive()) {\n    if (!runtimeArg) {\n      showError('Missing --runtime flag (required in non-interactive mode)');\n      process.exit(1);\n    }\n    if (!location) {\n      showError('Missing -g/--global or -l/--local flag (required in non-interactive mode)');\n      process.exit(1);\n    }\n  }\n\n  // Interactive mode: prompt for missing values\n  const mode = args.uninstall ? 'uninstall' : 'install';\n  let selectedRuntime: Runtime | undefined = runtimeArg;\n  if (!selectedRuntime && isInteractive()) {\n    selectedRuntime = await selectRuntime(mode);\n  }\n\n  if (!location && isInteractive()) {\n    location = await selectLocation(mode);\n  }\n\n  // Safety check - should not reach here without values\n  if (!selectedRuntime || !location) {\n    showError('Unable to determine runtime and location');\n    process.exit(1);\n  }\n\n  // UNINSTALL MODE\n  if (args.uninstall) {\n    return runUninstall(selectedRuntime, location, args.quiet);\n  }\n\n  // INSTALL MODE\n  return runInstall(selectedRuntime, location, args.force, args.quiet);\n}\n\n/**\n * Run the installation workflow\n *\n * @param runtime - Target runtime or 'all'\n * @param location - Installation location\n * @param force - Overwrite existing files\n * @param quiet - Suppress output\n * @returns Array of installation results\n */\nasync function runInstall(\n  runtime: Runtime,\n  location: Location,\n  force: boolean,\n  quiet: boolean,\n): Promise<InstallerResult[]> {\n  // Install files\n  const results = installFiles(runtime, location, { force, dryRun: false });\n\n  // Verify installation\n  const allCreatedFiles = results.flatMap((r) => r.filesCreated);\n  const verification = verifyInstallation(allCreatedFiles);\n\n  if (!verification.success) {\n    showError('Installation verification failed - some files missing:');\n    for (const missing of verification.missing) {\n      showWarning(`  Missing: ${missing}`);\n    }\n  }\n\n  // Display results\n  if (!quiet) {\n    displayInstallResults(results);\n  }\n\n  return results;\n}\n\n/**\n * Run the uninstallation workflow\n *\n * @param runtime - Target runtime or 'all'\n * @param location - Installation location\n * @param quiet - Suppress output\n * @returns Array of uninstallation results\n */\nfunction runUninstall(\n  runtime: Runtime,\n  location: Location,\n  quiet: boolean,\n): InstallerResult[] {\n  const results = uninstallFiles(runtime, location, false);\n\n  // Delete .agents-reverse-engineer config folder (local only)\n  const configDeleted = deleteConfigFolder(location, false);\n\n  // Display results\n  if (!quiet) {\n    displayUninstallResults(results, configDeleted);\n  }\n\n  return results;\n}\n\n/**\n * Display installation results with styled output\n *\n * Shows checkmarks for successful actions, warnings for skipped files,\n * and next steps for using the installed commands.\n *\n * @param results - Array of installation results\n */\nfunction displayInstallResults(results: InstallerResult[]): void {\n  console.log();\n\n  let totalCreated = 0;\n  let totalSkipped = 0;\n  let hooksRegistered = 0;\n\n  for (const result of results) {\n    if (result.success) {\n      showSuccess(`Installed ${result.runtime} (${result.location})`);\n    } else {\n      showError(`Failed to install ${result.runtime} (${result.location})`);\n      for (const err of result.errors) {\n        showWarning(`  ${err}`);\n      }\n    }\n\n    totalCreated += result.filesCreated.length;\n    totalSkipped += result.filesSkipped.length;\n\n    if (result.hookRegistered) {\n      hooksRegistered++;\n    }\n  }\n\n  // Summary\n  console.log();\n  if (totalCreated > 0) {\n    showSuccess(`Created ${totalCreated} command files`);\n  }\n  if (hooksRegistered > 0) {\n    showSuccess(`Registered ${hooksRegistered} session hook(s)`);\n  }\n  if (totalSkipped > 0) {\n    showWarning(`Skipped ${totalSkipped} existing files (use --force to overwrite)`);\n  }\n\n  // Next steps\n  const primaryRuntime = results[0]?.runtime || 'claude';\n  showNextSteps(primaryRuntime, totalCreated);\n\n  // GitHub link\n  console.log();\n  showInfo('Docs: https://github.com/GeoloeG-IsT/agents-reverse-engineer');\n}\n\n/**\n * Display uninstallation results with styled output\n *\n * @param results - Array of uninstallation results\n * @param configDeleted - Whether the .agents-reverse-engineer folder was deleted\n */\nfunction displayUninstallResults(results: InstallerResult[], configDeleted: boolean = false): void {\n  console.log();\n\n  let totalDeleted = 0;\n  let hooksUnregistered = 0;\n\n  for (const result of results) {\n    // In uninstall context, filesCreated tracks deleted files\n    const deletedCount = result.filesCreated.length;\n    const notFoundCount = result.filesSkipped.length;\n\n    if (result.success) {\n      if (deletedCount > 0) {\n        showSuccess(`Uninstalled ${result.runtime} (${result.location}) - ${deletedCount} files removed`);\n      } else {\n        showInfo(`No ${result.runtime} files found in ${result.location}`);\n      }\n    } else {\n      showError(`Failed to uninstall ${result.runtime} (${result.location})`);\n      for (const err of result.errors) {\n        showWarning(`  ${err}`);\n      }\n    }\n\n    totalDeleted += deletedCount;\n\n    // hookRegistered is repurposed for uninstall to mean \"hook was unregistered\"\n    if (result.hookRegistered) {\n      hooksUnregistered++;\n    }\n  }\n\n  // Summary\n  console.log();\n  if (totalDeleted > 0) {\n    showSuccess(`Removed ${totalDeleted} files`);\n  }\n  if (hooksUnregistered > 0) {\n    showSuccess(`Unregistered ${hooksUnregistered} session hook(s)`);\n  }\n  if (configDeleted) {\n    showSuccess(`Removed .agents-reverse-engineer folder`);\n  }\n  if (totalDeleted === 0 && !configDeleted) {\n    showInfo('No files were removed');\n  }\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**src/installer/index.ts orchestrates the npx installation workflow for agents-reverse-engineer with interactive prompts, argument parsing, and coordinated installation/uninstallation of commands and hooks across Claude/OpenCode/Gemini runtimes.**\n\n## Exported Functions\n\n**parseInstallerArgs(args: string[]): InstallerArgs** — Parses command-line arguments supporting short flags (`-g`, `-l`, `-h`, `-q`) and long flags (`--global`, `--local`, `--runtime`, `--force`, `--quiet`, `--help`). Validates `--runtime` value against `['claude', 'opencode', 'gemini', 'all']`. Returns InstallerArgs with `runtime`, `global`, `local`, `uninstall`, `force`, `help`, `quiet` properties.\n\n**runInstaller(args: InstallerArgs): Promise<InstallerResult[]>** — Main entry point executing installation workflow. Returns empty array if `args.help` is true (displays help via `showHelp()`). Calls `displayBanner()` unless `args.quiet`. Validates non-interactive mode requires `--runtime` and location flags, exits with `process.exit(1)` on missing values. Prompts for missing `runtime` via `selectRuntime(mode)` and `location` via `selectLocation(mode)` when `isInteractive()` returns true. Routes to `runUninstall()` if `args.uninstall`, otherwise `runInstall()`.\n\n## Internal Workflow Functions\n\n**determineLocation(args: InstallerArgs): Location | undefined** — Returns `'global'` if `args.global && !args.local`, returns `'local'` if `args.local && !args.global`, returns `undefined` if both or neither set (triggers interactive prompt).\n\n**determineRuntimes(runtime: Runtime | undefined): Array<Exclude<Runtime, 'all'>>** — Returns empty array if `runtime` is undefined (needs prompt). Returns `getAllRuntimes()` array if `runtime === 'all'`. Returns single-element array `[runtime]` for specific runtime.\n\n**runInstall(runtime: Runtime, location: Location, force: boolean, quiet: boolean): Promise<InstallerResult[]>** — Executes installation workflow: calls `installFiles(runtime, location, { force, dryRun: false })`, flattens `filesCreated` arrays via `flatMap()`, calls `verifyInstallation(allCreatedFiles)`, displays errors via `showError()` and `showWarning()` if verification fails, calls `displayInstallResults(results)` unless `quiet`.\n\n**runUninstall(runtime: Runtime, location: Location, quiet: boolean): InstallerResult[]** — Executes uninstallation workflow: calls `uninstallFiles(runtime, location, false)`, calls `deleteConfigFolder(location, false)` to remove `.agents-reverse-engineer/` directory, calls `displayUninstallResults(results, configDeleted)` unless `quiet`.\n\n## Display Functions\n\n**displayInstallResults(results: InstallerResult[]): void** — Iterates results calling `showSuccess()` for `result.success`, `showError()` and `showWarning()` for failures. Accumulates `totalCreated` from `result.filesCreated.length`, `totalSkipped` from `result.filesSkipped.length`, `hooksRegistered` from `result.hookRegistered` boolean. Displays summary via `showSuccess()` for created files/hooks, `showWarning()` for skipped files with \"use --force to overwrite\" message. Calls `showNextSteps(primaryRuntime, totalCreated)` with first result's runtime. Displays GitHub link via `showInfo('Docs: https://github.com/GeoloeG-IsT/agents-reverse-engineer')`.\n\n**displayUninstallResults(results: InstallerResult[], configDeleted: boolean): void** — Iterates results accumulating `totalDeleted` from `result.filesCreated.length` (repurposed for deleted file count), `hooksUnregistered` from `result.hookRegistered` (repurposed for unregistration status). Calls `showSuccess()` for deletions, `showInfo()` for \"No files found\" cases, `showError()` for failures. Displays summary with removed file count, unregistered hook count, and config folder deletion status.\n\n## Re-exported Symbols\n\nModule re-exports types and functions from sibling modules:\n- **Types**: `InstallerArgs`, `InstallerResult`, `Runtime`, `Location`, `RuntimePaths` from `./types.js`\n- **Path utilities**: `getRuntimePaths`, `getAllRuntimes`, `resolveInstallPath`, `getSettingsPath` from `./paths.js`\n- **Banner/display**: `displayBanner`, `showHelp`, `showSuccess`, `showError`, `showWarning`, `showInfo`, `showNextSteps`, `VERSION` from `./banner.js`\n- **Prompts**: `selectRuntime`, `selectLocation`, `confirmAction`, `isInteractive` from `./prompts.js`\n\n## Validation Logic\n\nNon-interactive mode (when `isInteractive()` returns false) enforces required flags: exits with `showError('Missing --runtime flag')` if `runtimeArg` undefined, exits with `showError('Missing -g/--global or -l/--local flag')` if `location` undefined. Interactive mode prompts for missing values via `selectRuntime()` and `selectLocation()`. Final safety check exits with `showError('Unable to determine runtime and location')` if either remains undefined.\n\n## Valid Runtime Values\n\nPattern: `const validRuntimes: Runtime[] = ['claude', 'opencode', 'gemini', 'all']` — Used in `parseInstallerArgs()` to validate `--runtime` argument via `validRuntimes.includes(runtimeValue as Runtime)`.\n\n## Operation Mode Routing\n\nVariable `mode` set to `'uninstall'` if `args.uninstall`, otherwise `'install'` — passed to `selectRuntime(mode)` and `selectLocation(mode)` to customize prompt text. Uninstall workflow triggered by `args.uninstall` flag (not directly parsed in this file, set by caller in CLI layer).",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 1480,
      "cacheReadTokens": 17367,
      "cacheCreationTokens": 12123,
      "latencyMs": 31671,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/installer/index.ts",
          "sizeBytes": 10066
        }
      ]
    },
    {
      "timestamp": "2026-02-09T17:45:11.700Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/orchestration/plan-tracker.ts\n\n```typescript\n/**\n * Progress tracker that updates GENERATION-PLAN.md checkboxes during generation.\n *\n * Maintains the markdown content in memory for fast updates and serializes\n * disk writes via a promise chain to handle concurrent Phase 1 completions.\n *\n * @module\n */\n\nimport { writeFile, mkdir } from 'node:fs/promises';\nimport * as path from 'node:path';\nimport { CONFIG_DIR } from '../config/loader.js';\n\n/**\n * Tracks generation progress by ticking checkboxes in GENERATION-PLAN.md.\n *\n * Create one instance at the start of `executeGenerate()`, call `markDone()`\n * as tasks complete, and `flush()` before returning.\n */\nexport class PlanTracker {\n  private content: string;\n  private readonly planPath: string;\n  private writeQueue: Promise<void> = Promise.resolve();\n\n  constructor(projectRoot: string, initialMarkdown: string) {\n    this.planPath = path.join(projectRoot, CONFIG_DIR, 'GENERATION-PLAN.md');\n    this.content = initialMarkdown;\n  }\n\n  /** Write the initial plan file to disk. */\n  async initialize(): Promise<void> {\n    try {\n      await mkdir(path.dirname(this.planPath), { recursive: true });\n      await writeFile(this.planPath, this.content, 'utf8');\n    } catch {\n      // Non-critical — generation continues without tracking\n    }\n  }\n\n  /**\n   * Mark a task as done by replacing its checkbox.\n   *\n   * The caller must pass the exact path as it appears in the markdown:\n   * - File: `src/cli/init.ts`\n   * - Directory: `src/cli/AGENTS.md`  (caller appends `/AGENTS.md`)\n   * - Root doc: `CLAUDE.md`\n   */\n  markDone(itemPath: string): void {\n    const before = this.content;\n    this.content = this.content.replace(\n      `- [ ] \\`${itemPath}\\``,\n      `- [x] \\`${itemPath}\\``,\n    );\n\n    if (this.content === before) return; // no match — skip write\n\n    // Queue a serialized write so concurrent markDone() calls don't corrupt the file\n    this.writeQueue = this.writeQueue\n      .then(() => writeFile(this.planPath, this.content, 'utf8'))\n      .catch(() => {/* non-critical */});\n  }\n\n  /** Wait for all queued writes to finish. */\n  async flush(): Promise<void> {\n    await this.writeQueue;\n  }\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**PlanTracker maintains in-memory GENERATION-PLAN.md markdown content and serializes concurrent checkbox updates via promise-chain writes to prevent file corruption.**\n\n## Exported Class\n\n`PlanTracker` provides three public methods:\n- `constructor(projectRoot: string, initialMarkdown: string)` — stores markdown in `this.content`, computes `this.planPath` as `path.join(projectRoot, CONFIG_DIR, 'GENERATION-PLAN.md')`\n- `async initialize(): Promise<void>` — creates parent directory via `mkdir(..., { recursive: true })`, writes `this.content` to `this.planPath`, swallows errors (non-critical)\n- `markDone(itemPath: string): void` — replaces checkbox pattern `` `- [ ] \\`${itemPath}\\`` `` with `` `- [x] \\`${itemPath}\\`` `` in `this.content`, chains write to `this.writeQueue` promise with `.catch()` suppression\n- `async flush(): Promise<void>` — awaits `this.writeQueue` to ensure all serialized writes complete\n\n## Concurrency Strategy\n\nMaintains `private writeQueue: Promise<void> = Promise.resolve()` to serialize disk writes. Each `markDone()` call appends `writeFile()` to the promise chain via `this.writeQueue = this.writeQueue.then(...)`, preventing race conditions when concurrent Phase 1 workers complete tasks simultaneously.\n\n## Usage Pattern\n\nInstantiated once at start of `executeGenerate()` in `src/generation/orchestrator.ts`, passed to worker pool, invoked by `markDone(itemPath)` after each task completion, finalized by `flush()` before function return.\n\n## Item Path Conventions\n\nCaller must pass exact path as it appears in markdown:\n- File analysis: `src/cli/init.ts`\n- Directory aggregation: `src/cli/AGENTS.md` (caller appends `/AGENTS.md` suffix)\n- Root synthesis: `CLAUDE.md`\n\n## Dependencies\n\nImports `writeFile` and `mkdir` from `node:fs/promises`, `path` from `node:path`, `CONFIG_DIR` constant from `src/config/loader.ts`.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 520,
      "cacheReadTokens": 17367,
      "cacheCreationTokens": 9746,
      "latencyMs": 17443,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/orchestration/plan-tracker.ts",
          "sizeBytes": 2130
        }
      ]
    },
    {
      "timestamp": "2026-02-09T17:45:00.411Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/installer/prompts.ts\n\n```typescript\n/**\n * Interactive prompts module for the installer\n *\n * Provides arrow key selection in TTY mode with numbered fallback for CI/non-interactive.\n * Uses Node.js readline module with raw mode for keypress handling.\n *\n * CRITICAL: Raw mode is always cleaned up via try/finally and process exit handlers.\n */\n\nimport * as readline from 'node:readline';\nimport pc from 'picocolors';\nimport type { Runtime, Location } from './types.js';\n\n/**\n * Check if stdin is a TTY (interactive terminal)\n *\n * @returns true if running in interactive terminal, false for CI/piped input\n */\nexport function isInteractive(): boolean {\n  return process.stdin.isTTY === true;\n}\n\n/**\n * Option type for selection prompts\n */\ninterface SelectOption<T> {\n  label: string;\n  value: T;\n}\n\n/**\n * Raw mode state tracker for cleanup\n */\nlet rawModeActive = false;\n\n/**\n * Cleanup function to restore terminal state\n */\nfunction cleanupRawMode(): void {\n  if (rawModeActive && process.stdin.isTTY) {\n    try {\n      process.stdin.setRawMode(false);\n      process.stdin.pause();\n    } catch {\n      // Ignore errors during cleanup\n    }\n    rawModeActive = false;\n  }\n}\n\n// Register global cleanup handlers\nprocess.on('exit', cleanupRawMode);\nprocess.on('SIGINT', () => {\n  cleanupRawMode();\n  process.exit(0);\n});\n\n/**\n * Generic option selector that uses arrow keys in TTY, numbered in non-TTY\n *\n * @param prompt - Question to display\n * @param options - Array of options with labels and values\n * @returns Selected value\n */\nexport async function selectOption<T>(\n  prompt: string,\n  options: SelectOption<T>[],\n): Promise<T> {\n  if (isInteractive()) {\n    return arrowKeySelect(prompt, options);\n  }\n  return numberedSelect(prompt, options);\n}\n\n/**\n * Arrow key selection for interactive terminals\n *\n * Uses raw mode to capture keypresses for up/down/enter navigation.\n * Always cleans up raw mode even on error or Ctrl+C.\n *\n * @param prompt - Question to display\n * @param options - Array of options with labels and values\n * @returns Selected value\n */\nasync function arrowKeySelect<T>(\n  prompt: string,\n  options: SelectOption<T>[],\n): Promise<T> {\n  return new Promise((resolve) => {\n    let selectedIndex = 0;\n\n    // Render the current selection state\n    const render = (clear: boolean = false): void => {\n      // Move cursor up and clear lines if re-rendering\n      if (clear) {\n        process.stdout.write(`\\x1b[${options.length + 1}A`);\n        for (let i = 0; i <= options.length; i++) {\n          process.stdout.write('\\x1b[2K\\x1b[1B');\n        }\n        process.stdout.write(`\\x1b[${options.length + 1}A`);\n      }\n\n      console.log(pc.bold(prompt));\n      options.forEach((opt, idx) => {\n        const prefix = idx === selectedIndex ? pc.cyan('> ') : '  ';\n        const label = idx === selectedIndex ? pc.cyan(opt.label) : opt.label;\n        console.log(prefix + label);\n      });\n    };\n\n    // Handle keypress events\n    const handleKeypress = (\n      _str: string | undefined,\n      key: { name?: string; ctrl?: boolean },\n    ): void => {\n      if (key.ctrl && key.name === 'c') {\n        cleanupRawMode();\n        process.exit(0);\n      }\n\n      switch (key.name) {\n        case 'up':\n          selectedIndex = Math.max(0, selectedIndex - 1);\n          render(true);\n          break;\n        case 'down':\n          selectedIndex = Math.min(options.length - 1, selectedIndex + 1);\n          render(true);\n          break;\n        case 'return':\n          // Cleanup and resolve\n          process.stdin.off('keypress', handleKeypress);\n          cleanupRawMode();\n          console.log();\n          resolve(options[selectedIndex].value);\n          break;\n      }\n    };\n\n    try {\n      // Setup raw mode for keypress handling\n      readline.emitKeypressEvents(process.stdin);\n      if (process.stdin.isTTY) {\n        process.stdin.setRawMode(true);\n        rawModeActive = true;\n      }\n      process.stdin.resume();\n      process.stdin.on('keypress', handleKeypress);\n\n      // Initial render\n      render(false);\n    } catch (err) {\n      cleanupRawMode();\n      throw err;\n    }\n  });\n}\n\n/**\n * Numbered selection for non-interactive environments\n *\n * Prints numbered options and reads a number from stdin.\n * Used in CI environments or when stdin is piped.\n *\n * @param prompt - Question to display\n * @param options - Array of options with labels and values\n * @returns Selected value\n */\nasync function numberedSelect<T>(\n  prompt: string,\n  options: SelectOption<T>[],\n): Promise<T> {\n  return new Promise((resolve, reject) => {\n    console.log(pc.bold(prompt));\n    options.forEach((opt, idx) => {\n      console.log(`  ${idx + 1}. ${opt.label}`);\n    });\n\n    const rl = readline.createInterface({\n      input: process.stdin,\n      output: process.stdout,\n    });\n\n    rl.question('Enter number: ', (answer) => {\n      rl.close();\n\n      const num = parseInt(answer, 10);\n      if (isNaN(num) || num < 1 || num > options.length) {\n        reject(new Error(`Invalid selection: ${answer}. Expected 1-${options.length}`));\n        return;\n      }\n\n      resolve(options[num - 1].value);\n    });\n  });\n}\n\n/**\n * Prompt user to select a runtime\n *\n * @param mode - 'install' or 'uninstall' to customize prompt text\n * @returns Selected runtime value\n */\nexport async function selectRuntime(mode: 'install' | 'uninstall' = 'install'): Promise<Runtime> {\n  const prompt = mode === 'uninstall' ? 'Select runtime to uninstall:' : 'Select runtime to install:';\n  return selectOption<Runtime>(prompt, [\n    { label: 'Claude Code', value: 'claude' },\n    { label: 'OpenCode', value: 'opencode' },\n    { label: 'Gemini CLI', value: 'gemini' },\n    { label: 'All runtimes', value: 'all' },\n  ]);\n}\n\n/**\n * Prompt user to select installation location\n *\n * @param mode - 'install' or 'uninstall' to customize prompt text\n * @returns Selected location value\n */\nexport async function selectLocation(mode: 'install' | 'uninstall' = 'install'): Promise<Location> {\n  const prompt = mode === 'uninstall' ? 'Select uninstallation location:' : 'Select installation location:';\n  return selectOption<Location>(prompt, [\n    { label: 'Global (~/.claude, ~/.config/opencode, etc.)', value: 'global' },\n    { label: 'Local (./.claude, ./.opencode, etc.)', value: 'local' },\n  ]);\n}\n\n/**\n * Prompt user to confirm an action\n *\n * @param message - Confirmation message to display\n * @returns true if confirmed, false if declined\n */\nexport async function confirmAction(message: string): Promise<boolean> {\n  return selectOption<boolean>(message, [\n    { label: 'Yes', value: true },\n    { label: 'No', value: false },\n  ]);\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**Interactive prompt orchestrator for installer CLI providing TTY arrow-key selection with numbered fallback, raw mode cleanup guarantees, and runtime/location/confirmation flows.**\n\n## Exported Functions\n\n**isInteractive(): boolean** — Returns `true` if `process.stdin.isTTY === true`, otherwise `false` for CI/piped input detection.\n\n**selectOption<T>(prompt: string, options: SelectOption<T>[]): Promise<T>** — Generic selector routing to `arrowKeySelect()` in TTY mode or `numberedSelect()` in non-interactive mode based on `isInteractive()` result.\n\n**selectRuntime(mode: 'install' | 'uninstall' = 'install'): Promise<Runtime>** — Prompts for runtime selection with options: `'claude'`, `'opencode'`, `'gemini'`, `'all'`. Uses `selectOption<Runtime>()` with labels \"Claude Code\", \"OpenCode\", \"Gemini CLI\", \"All runtimes\".\n\n**selectLocation(mode: 'install' | 'uninstall' = 'install'): Promise<Location>** — Prompts for location selection with options: `'global'` (\"Global (~/.claude, ~/.config/opencode, etc.)\"), `'local'` (\"Local (./.claude, ./.opencode, etc.)\"). Uses `selectOption<Location>()`.\n\n**confirmAction(message: string): Promise<boolean>** — Prompts for Yes/No confirmation using `selectOption<boolean>()` with labels \"Yes\" (`true`), \"No\" (`false`).\n\n## Internal Selection Implementations\n\n**arrowKeySelect<T>(prompt: string, options: SelectOption<T>[]): Promise<T>** — Interactive arrow-key navigator using `readline.emitKeypressEvents()` + `process.stdin.setRawMode(true)`. Listens for keypress events: `'up'` decrements `selectedIndex` (clamped to `0`), `'down'` increments `selectedIndex` (clamped to `options.length - 1`), `'return'` resolves with `options[selectedIndex].value`. Renders prompt with `pc.bold()` and options with `pc.cyan('> ')` prefix for selected index. Uses ANSI escape sequences `\\x1b[${n}A` (move cursor up), `\\x1b[2K` (clear line), `\\x1b[1B` (move cursor down) for re-rendering on navigation. Handles Ctrl+C via `key.ctrl && key.name === 'c'` check calling `cleanupRawMode()` + `process.exit(0)`. Always executes cleanup in try/catch wrapper.\n\n**numberedSelect<T>(prompt: string, options: SelectOption<T>[]): Promise<T>** — Non-interactive numbered fallback printing `pc.bold(prompt)` and options as `\"  ${idx + 1}. ${opt.label}\"`. Creates `readline.createInterface()` with stdin/stdout, prompts `'Enter number: '`, parses input via `parseInt(answer, 10)`, validates range `1-${options.length}`, resolves with `options[num - 1].value`. Rejects with `Error('Invalid selection: ${answer}. Expected 1-${options.length}')` on parse failure or out-of-range.\n\n## Raw Mode State Management\n\n**rawModeActive: boolean** — Module-level flag tracking `process.stdin.setRawMode(true)` state to prevent double-cleanup errors.\n\n**cleanupRawMode(): void** — Restores terminal by calling `process.stdin.setRawMode(false)` + `process.stdin.pause()` when `rawModeActive && process.stdin.isTTY`. Sets `rawModeActive = false`. Swallows exceptions during cleanup via empty catch block.\n\n**Global exit handlers** — Registers `process.on('exit', cleanupRawMode)` and `process.on('SIGINT', ...)` with `cleanupRawMode()` + `process.exit(0)` to guarantee terminal restoration on process termination or Ctrl+C interruption.\n\n## Type Definitions\n\n**SelectOption<T>** — Interface with `label: string` (display text) and `value: T` (resolved value). Used for generic option construction in `selectOption()`, `selectRuntime()`, `selectLocation()`, `confirmAction()`.\n\n## Dependencies\n\nUses `node:readline` for `emitKeypressEvents()`, `createInterface()`, and raw mode stdin manipulation. Uses `picocolors` (`pc`) for `pc.bold()`, `pc.cyan()` terminal formatting. Imports `Runtime`, `Location` from `'./types.js'` for typed option values.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 1120,
      "cacheReadTokens": 17367,
      "cacheCreationTokens": 11134,
      "latencyMs": 31484,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/installer/prompts.ts",
          "sizeBytes": 6609
        }
      ]
    },
    {
      "timestamp": "2026-02-09T17:45:06.344Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/integration/generate.ts\n\n```typescript\n/**\n * Integration file generation for AI coding assistants\n *\n * Generates command files and hooks for detected AI assistant environments.\n * Handles file creation with directory creation and skip-if-exists behavior.\n */\n\nimport { existsSync, mkdirSync, writeFileSync, readFileSync } from 'node:fs';\nimport * as path from 'node:path';\nimport { fileURLToPath } from 'node:url';\nimport type { IntegrationResult, EnvironmentType } from './types.js';\nimport { detectEnvironments } from './detect.js';\nimport {\n  getClaudeTemplates,\n  getOpenCodeTemplates,\n  getGeminiTemplates,\n} from './templates.js';\n\n/**\n * Get the path to a bundled hook file\n *\n * @param hookName - Name of the hook file (e.g., 'are-session-end.js')\n * @returns Absolute path to the bundled hook file\n */\nfunction getBundledHookPath(hookName: string): string {\n  const __filename = fileURLToPath(import.meta.url);\n  const __dirname = path.dirname(__filename);\n  // From dist/integration/ go up two levels to project root, then to hooks/dist/\n  return path.join(__dirname, '..', '..', 'hooks', 'dist', hookName);\n}\n\n/**\n * Read bundled hook content\n *\n * @param hookName - Name of the hook file\n * @returns Hook file content as string\n * @throws Error if hook file not found\n */\nfunction readBundledHook(hookName: string): string {\n  const hookPath = getBundledHookPath(hookName);\n  if (!existsSync(hookPath)) {\n    throw new Error(`Bundled hook not found: ${hookPath}`);\n  }\n  return readFileSync(hookPath, 'utf-8');\n}\n\n/**\n * Options for generating integration files\n */\nexport interface GenerateOptions {\n  /** If true, don't actually write files - just report what would be done */\n  dryRun?: boolean;\n  /** If true, overwrite existing files instead of skipping them */\n  force?: boolean;\n  /** Specific environment to generate for (bypasses auto-detection) */\n  environment?: EnvironmentType;\n}\n\n/**\n * Ensure parent directories exist for a file path\n *\n * @param filePath - Full path to the file\n */\nfunction ensureDir(filePath: string): void {\n  const dir = path.dirname(filePath);\n  if (!existsSync(dir)) {\n    mkdirSync(dir, { recursive: true });\n  }\n}\n\n/**\n * Generate integration files for all detected AI assistant environments\n *\n * For each detected environment:\n * - Gets appropriate templates (command files)\n * - Creates files if they don't exist (or if force=true)\n * - For Claude: also creates the session-end hook\n *\n * @param projectRoot - Root directory of the project\n * @param options - Generation options\n * @returns Array of results, one per environment\n *\n * @example\n * ```typescript\n * const results = await generateIntegrationFiles('/path/to/project');\n * // [{ environment: 'claude', filesCreated: ['...'], filesSkipped: [] }]\n * ```\n */\nexport async function generateIntegrationFiles(\n  projectRoot: string,\n  options: GenerateOptions = {}\n): Promise<IntegrationResult[]> {\n  const { dryRun = false, force = false, environment: specificEnv } = options;\n  const results: IntegrationResult[] = [];\n\n  // Use specific environment if provided, otherwise auto-detect\n  let environments: { type: EnvironmentType; configDir: string }[];\n  if (specificEnv) {\n    // Map environment type to config directory\n    const configDirMap: Record<EnvironmentType, string> = {\n      claude: '.claude',\n      opencode: '.opencode',\n      aider: '.aider',\n      gemini: '.gemini',\n    };\n    environments = [{ type: specificEnv, configDir: configDirMap[specificEnv] }];\n  } else {\n    // Detect which environments are present\n    environments = detectEnvironments(projectRoot);\n  }\n\n  for (const env of environments) {\n    const result: IntegrationResult = {\n      environment: env.type,\n      filesCreated: [],\n      filesSkipped: [],\n    };\n\n    // Get templates for this environment\n    const templates = getTemplatesForEnvironment(env.type);\n\n    // Process each template\n    for (const template of templates) {\n      const fullPath = path.join(projectRoot, template.path);\n\n      if (existsSync(fullPath) && !force) {\n        // File exists and force is not set - skip it\n        result.filesSkipped.push(template.path);\n      } else {\n        // Create the file\n        if (!dryRun) {\n          ensureDir(fullPath);\n          writeFileSync(fullPath, template.content, 'utf-8');\n        }\n        result.filesCreated.push(template.path);\n      }\n    }\n\n    // For Claude, also generate the hook file\n    if (env.type === 'claude') {\n      const hookPath = '.claude/hooks/are-session-end.js';\n      const fullHookPath = path.join(projectRoot, hookPath);\n\n      if (existsSync(fullHookPath) && !force) {\n        result.filesSkipped.push(hookPath);\n      } else {\n        if (!dryRun) {\n          ensureDir(fullHookPath);\n          const hookContent = readBundledHook('are-session-end.js');\n          writeFileSync(fullHookPath, hookContent, 'utf-8');\n        }\n        result.filesCreated.push(hookPath);\n      }\n    }\n\n    results.push(result);\n  }\n\n  return results;\n}\n\n/**\n * Get templates for a specific environment type\n *\n * @param type - Environment type\n * @returns Array of templates for that environment\n */\nfunction getTemplatesForEnvironment(\n  type: EnvironmentType\n): ReturnType<typeof getClaudeTemplates> {\n  switch (type) {\n    case 'claude':\n      return getClaudeTemplates();\n    case 'opencode':\n      return getOpenCodeTemplates();\n    case 'gemini':\n      return getGeminiTemplates();\n    case 'aider':\n      // Aider doesn't have command files yet - return empty\n      return [];\n    default:\n      return [];\n  }\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**generateIntegrationFiles orchestrates AI assistant integration file creation with environment-specific template selection, directory creation, skip-if-exists logic, and bundled hook deployment.**\n\n## Exported Functions\n\n**generateIntegrationFiles(projectRoot: string, options?: GenerateOptions): Promise<IntegrationResult[]>** — main orchestrator that detects or uses specified AI assistant environments, retrieves templates via `getTemplatesForEnvironment()`, writes files with `ensureDir()` + `writeFileSync()`, deploys bundled hooks for Claude via `readBundledHook()`, returns array of `IntegrationResult` with `filesCreated[]` and `filesSkipped[]` paths.\n\n**GenerateOptions** — configuration interface with `dryRun?: boolean` (preview mode without writes), `force?: boolean` (overwrite existing files), `environment?: EnvironmentType` (bypass auto-detection).\n\n## Integration Points\n\nImports `detectEnvironments()` from `./detect.js` for auto-discovery of `.claude/`, `.opencode/`, `.gemini/`, `.aider/` directories. Imports template getters `getClaudeTemplates()`, `getOpenCodeTemplates()`, `getGeminiTemplates()` from `./templates.js`. Uses `IntegrationResult` and `EnvironmentType` from `./types.js`.\n\n## Hook Deployment\n\n**getBundledHookPath(hookName: string): string** — resolves bundled hook location via `import.meta.url` + `fileURLToPath()`, navigates from `dist/integration/` up two levels to project root, then to `hooks/dist/${hookName}`.\n\n**readBundledHook(hookName: string): string** — reads bundled hook via `readFileSync()` with `utf-8` encoding, throws `Error` with message `\"Bundled hook not found: ${hookPath}\"` if `existsSync()` returns false.\n\nFor `env.type === 'claude'`, appends `.claude/hooks/are-session-end.js` to result by reading bundled `are-session-end.js` via `readBundledHook()`, writing with `ensureDir()` unless exists and `!force`.\n\n## File Creation Workflow\n\n**ensureDir(filePath: string): void** — extracts parent directory via `path.dirname()`, creates with `mkdirSync({ recursive: true })` if `!existsSync()`.\n\nFor each template, constructs `fullPath` via `path.join(projectRoot, template.path)`, checks `existsSync(fullPath) && !force` → `filesSkipped.push()`, otherwise `ensureDir()` + `writeFileSync(fullPath, template.content, 'utf-8')` + `filesCreated.push()` unless `dryRun`.\n\n## Environment Routing\n\n**getTemplatesForEnvironment(type: EnvironmentType)** — switch statement dispatching to `getClaudeTemplates()`, `getOpenCodeTemplates()`, `getGeminiTemplates()` for respective types, returns empty array `[]` for `'aider'` (no command files yet) and default case.\n\nWhen `options.environment` provided, bypasses `detectEnvironments()` and constructs single-element array `[{ type: specificEnv, configDir: configDirMap[specificEnv] }]` using literal map `{ claude: '.claude', opencode: '.opencode', aider: '.aider', gemini: '.gemini' }`.\n\n## Return Value Structure\n\nReturns `Promise<IntegrationResult[]>` with one element per environment, each containing `environment: EnvironmentType`, `filesCreated: string[]` (paths relative to projectRoot), `filesSkipped: string[]` (existing files not overwritten).",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 882,
      "cacheReadTokens": 17367,
      "cacheCreationTokens": 10734,
      "latencyMs": 27750,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/integration/generate.ts",
          "sizeBytes": 5521
        }
      ]
    },
    {
      "timestamp": "2026-02-09T17:45:15.984Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/orchestration/pool.ts\n\n```typescript\n/**\n * Iterator-based concurrency pool.\n *\n * Provides a zero-dependency concurrency limiter using the shared-iterator\n * worker pattern. Workers pull from a shared iterator so exactly N tasks\n * execute concurrently, with new tasks starting as previous ones complete.\n *\n * This avoids the \"batch\" anti-pattern where `Promise.all` on chunks of N\n * tasks idles workers while waiting for the slowest task in each batch.\n *\n * @module\n */\n\nimport type { ITraceWriter } from './trace.js';\n\n// ---------------------------------------------------------------------------\n// Types\n// ---------------------------------------------------------------------------\n\n/**\n * Options for the concurrency pool.\n */\nexport interface PoolOptions {\n  /** Maximum number of concurrent workers */\n  concurrency: number;\n  /** Stop pulling new tasks on first error */\n  failFast?: boolean;\n  /** Trace writer for concurrency debugging (no-op when tracing is off) */\n  tracer?: ITraceWriter;\n  /** Phase label for trace events (e.g., 'phase-1-files') */\n  phaseLabel?: string;\n  /** Labels for each task by index (e.g., file paths). Used in trace events. */\n  taskLabels?: string[];\n}\n\n/**\n * Result of a single task execution within the pool.\n *\n * Indexed by the task's position in the original array so callers\n * can correlate results back to their inputs.\n */\nexport interface TaskResult<T> {\n  /** Zero-based index of the task in the original array */\n  index: number;\n  /** Whether the task completed successfully */\n  success: boolean;\n  /** The resolved value (present when success is true) */\n  value?: T;\n  /** The error (present when success is false) */\n  error?: Error;\n}\n\n// ---------------------------------------------------------------------------\n// Pool implementation\n// ---------------------------------------------------------------------------\n\n/**\n * Run an array of async task factories through a concurrency-limited pool.\n *\n * Uses the shared-iterator pattern: all workers iterate over the same\n * `entries()` iterator, so each task is picked up by exactly one worker.\n * When a worker finishes a task, it immediately pulls the next one from\n * the iterator, keeping all worker slots busy.\n *\n * @typeParam T - The resolved type of each task\n * @param tasks - Array of zero-argument async functions to execute\n * @param options - Pool configuration (concurrency, failFast, tracing)\n * @param onComplete - Optional callback invoked after each task settles\n * @returns Array of results indexed by original task position (may be sparse if aborted)\n *\n * @example\n * ```typescript\n * const results = await runPool(\n *   urls.map(url => () => fetch(url).then(r => r.json())),\n *   { concurrency: 5, failFast: false },\n *   (result) => console.log(`Task ${result.index}: ${result.success}`),\n * );\n * ```\n */\nexport async function runPool<T>(\n  tasks: Array<() => Promise<T>>,\n  options: PoolOptions,\n  onComplete?: (result: TaskResult<T>) => void,\n): Promise<TaskResult<T>[]> {\n  const results: TaskResult<T>[] = [];\n\n  if (tasks.length === 0) {\n    return results;\n  }\n\n  const tracer = options.tracer;\n  const phase = options.phaseLabel ?? 'unknown';\n  const taskLabels = options.taskLabels;\n\n  // Shared abort flag -- workers check before pulling next task\n  let aborted = false;\n\n  // Active task counter for trace snapshots\n  let activeTasks = 0;\n\n  async function worker(\n    iterator: IterableIterator<[number, () => Promise<T>]>,\n    workerId: number,\n  ): Promise<void> {\n    let tasksExecuted = 0;\n\n    tracer?.emit({ type: 'worker:start', workerId, phase });\n\n    for (const [index, task] of iterator) {\n      if (aborted) break;\n\n      activeTasks++;\n      const taskStart = Date.now();\n      const label = taskLabels?.[index] ?? `task-${index}`;\n\n      tracer?.emit({\n        type: 'task:pickup',\n        workerId,\n        taskIndex: index,\n        taskLabel: label,\n        activeTasks,\n      });\n\n      try {\n        const value = await task();\n        activeTasks--;\n        tasksExecuted++;\n\n        tracer?.emit({\n          type: 'task:done',\n          workerId,\n          taskIndex: index,\n          taskLabel: label,\n          durationMs: Date.now() - taskStart,\n          success: true,\n          activeTasks,\n        });\n\n        const result: TaskResult<T> = { index, success: true, value };\n        results[index] = result;\n        onComplete?.(result);\n      } catch (err) {\n        activeTasks--;\n        tasksExecuted++;\n\n        const error = err instanceof Error ? err : new Error(String(err));\n\n        tracer?.emit({\n          type: 'task:done',\n          workerId,\n          taskIndex: index,\n          taskLabel: label,\n          durationMs: Date.now() - taskStart,\n          success: false,\n          error: error.message,\n          activeTasks,\n        });\n\n        const result: TaskResult<T> = { index, success: false, error };\n        results[index] = result;\n        onComplete?.(result);\n\n        if (options.failFast) {\n          aborted = true;\n          break;\n        }\n      }\n    }\n\n    tracer?.emit({ type: 'worker:end', workerId, phase, tasksExecuted });\n  }\n\n  // Spawn workers, capped at the number of available tasks\n  const effectiveConcurrency = Math.min(options.concurrency, tasks.length);\n  const iterator = tasks.entries();\n  const workers = Array.from({ length: effectiveConcurrency }, (_, workerId) =>\n    worker(iterator, workerId),\n  );\n\n  await Promise.allSettled(workers);\n\n  return results;\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**pool.ts implements iterator-based concurrency limiting via shared-iterator worker pattern for parallel task execution with configurable worker count, fail-fast abort, and optional trace emission.**\n\n## Exported Functions\n\n**runPool<T>(tasks, options, onComplete?): Promise<TaskResult<T>[]>**\n\nExecutes array of async task factories through concurrency-limited pool using shared iterator pattern where N workers pull from single `tasks.entries()` iterator. Each worker invokes tasks sequentially until iterator exhaustion or abort flag set. Returns array of `TaskResult<T>` indexed by original task position (may be sparse if aborted via fail-fast).\n\nParameters:\n- `tasks: Array<() => Promise<T>>` — zero-argument async task factories\n- `options: PoolOptions` — pool configuration (concurrency, failFast, tracer, phaseLabel, taskLabels)\n- `onComplete?: (result: TaskResult<T>) => void` — optional callback invoked after each task settles\n\nWorker lifecycle: emits `worker:start` with workerId and phase, iterates over shared iterator checking `aborted` flag before each pickup, emits `task:pickup` with `activeTasks` snapshot, awaits task execution, catches errors converting non-Error values via `String(err)`, emits `task:done` with `durationMs` and `success` boolean, invokes `onComplete` callback with result, sets `aborted=true` on error if `failFast` enabled, emits `worker:end` with `tasksExecuted` count.\n\nEffective concurrency computed via `Math.min(options.concurrency, tasks.length)` to avoid spawning idle workers. All workers awaited via `Promise.allSettled()` ensuring completion even if individual workers reject.\n\n## Types\n\n**PoolOptions**\n- `concurrency: number` — maximum concurrent workers\n- `failFast?: boolean` — abort on first error (sets shared `aborted` flag preventing new task pickups)\n- `tracer?: ITraceWriter` — trace writer for emit calls (no-op when undefined)\n- `phaseLabel?: string` — phase identifier for trace events (defaults to `'unknown'`)\n- `taskLabels?: string[]` — per-task labels indexed by task position (used in `task:pickup`/`task:done` events, falls back to `'task-${index}'`)\n\n**TaskResult<T>**\n- `index: number` — zero-based task position in original array\n- `success: boolean` — settlement status\n- `value?: T` — resolved value when `success=true`\n- `error?: Error` — rejection error when `success=false`\n\n## Concurrency Strategy\n\nShared-iterator pattern prevents \"batch anti-pattern\" where `Promise.all()` on fixed-size chunks idles workers waiting for slowest task in batch. Single `tasks.entries()` iterator consumed by all workers ensures each task picked exactly once, with workers immediately pulling next task upon completion maintaining full worker utilization until iterator exhaustion.\n\n## Trace Events\n\nEmits via `tracer?.emit()` optional chaining (no-op when tracer undefined):\n- `worker:start` — `{ type, workerId, phase }`\n- `task:pickup` — `{ type, workerId, taskIndex, taskLabel, activeTasks }`\n- `task:done` — `{ type, workerId, taskIndex, taskLabel, durationMs, success, error?, activeTasks }`\n- `worker:end` — `{ type, workerId, phase, tasksExecuted }`\n\n`activeTasks` counter incremented at pickup, decremented at settlement, providing snapshot of concurrent task count at trace event emission time. Duration computed via `Date.now() - taskStart` without high-resolution timer.\n\n## Abort Mechanism\n\nShared mutable `aborted` boolean flag checked at loop start (`for (const [index, task] of iterator) { if (aborted) break; }`). Set to `true` in catch block when `options.failFast` enabled, causing all workers to stop pulling new tasks. Workers already executing tasks run to completion before checking flag. Results array may be sparse with undefined indices for tasks never started due to abort.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 968,
      "cacheReadTokens": 17367,
      "cacheCreationTokens": 10604,
      "latencyMs": 24994,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/orchestration/pool.ts",
          "sizeBytes": 5458
        }
      ]
    },
    {
      "timestamp": "2026-02-09T17:45:08.354Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/integration/templates.ts\n\n```typescript\n/**\n * Template generators for AI coding assistant integration files\n *\n * Generates command file templates for Claude Code, OpenCode, Gemini CLI, and session hooks.\n */\n\nimport type { IntegrationTemplate } from './types.js';\n\n// =============================================================================\n// Shared Command Content\n// =============================================================================\n\nconst COMMANDS = {\n  generate: {\n    description: 'Generate AI-friendly documentation for the entire codebase',\n    argumentHint: '[path] [--dry-run] [--concurrency N] [--fail-fast] [--debug] [--trace]',\n    content: `Generate comprehensive documentation for this codebase using agents-reverse-engineer.\n\n<execution>\nRun the generate command in the background and monitor progress in real time.\n\n## Steps\n\n1. **Display version**: Read \\`VERSION_FILE_PATH\\` and show the user: \\`agents-reverse-engineer vX.Y.Z\\`\n\n2. **Run the generate command in the background** using \\`run_in_background: true\\`:\n   \\`\\`\\`bash\n   npx agents-reverse-engineer@latest generate $ARGUMENTS\n   \\`\\`\\`\n\n3. **Monitor progress by polling** \\`.agents-reverse-engineer/progress.log\\`:\n   - Wait ~15 seconds (use \\`sleep 15\\` in Bash), then use the **Read** tool to read \\`.agents-reverse-engineer/progress.log\\` (use the \\`offset\\` parameter to read only the last ~20 lines for long files)\n   - Show the user a brief progress update (e.g. \"32/96 files analyzed, ~12m remaining\")\n   - Check whether the background task has completed using \\`TaskOutput\\` with \\`block: false\\`\n   - Repeat until the background task finishes\n   - **Important**: Keep polling even if progress.log doesn't exist yet (the command takes a few seconds to start writing)\n\n4. **On completion**, read the full background task output and summarize:\n   - Number of files analyzed and any failures\n   - Number of directories documented\n   - Root and per-package documents generated\n   - Any inconsistency warnings from the quality report\n\nThis executes a three-phase pipeline:\n\n1. **Discovery & Planning**: Walks the directory tree, applies filters (gitignore, vendor, binary, custom), detects file types, and creates a generation plan.\n\n2. **File Analysis** (concurrent): Analyzes each source file via AI and writes \\`.sum\\` summary files with YAML frontmatter (\\`content_hash\\`, \\`file_type\\`, \\`purpose\\`, \\`public_interface\\`, \\`dependencies\\`, \\`patterns\\`).\n\n3. **Directory & Root Documents** (sequential):\n   - Generates \\`AGENTS.md\\` per directory in post-order traversal (deepest first, so child summaries feed into parents)\n   - Creates root document: \\`CLAUDE.md\\`\n\n**Options:**\n- \\`--dry-run\\`: Preview the plan without making AI calls\n- \\`--concurrency N\\`: Control number of parallel AI calls (default: auto)\n- \\`--fail-fast\\`: Stop on first file analysis failure\n- \\`--debug\\`: Show AI prompts and backend details\n- \\`--trace\\`: Enable concurrency tracing to \\`.agents-reverse-engineer/traces/\\`\n</execution>`,\n  },\n\n  update: {\n    description: 'Incrementally update documentation for changed files',\n    argumentHint: '[path] [--uncommitted] [--dry-run] [--concurrency N] [--fail-fast] [--debug] [--trace]',\n    content: `Update documentation for files that changed since last run.\n\n<execution>\nRun the update command in the background and monitor progress in real time.\n\n## Steps\n\n1. **Display version**: Read \\`VERSION_FILE_PATH\\` and show the user: \\`agents-reverse-engineer vX.Y.Z\\`\n\n2. **Run the update command in the background** using \\`run_in_background: true\\`:\n   \\`\\`\\`bash\n   npx agents-reverse-engineer@latest update $ARGUMENTS\n   \\`\\`\\`\n\n3. **Monitor progress by polling** \\`.agents-reverse-engineer/progress.log\\`:\n   - Wait ~15 seconds (use \\`sleep 15\\` in Bash), then use the **Read** tool to read \\`.agents-reverse-engineer/progress.log\\` (use the \\`offset\\` parameter to read only the last ~20 lines for long files)\n   - Show the user a brief progress update (e.g. \"12/30 files updated, ~5m remaining\")\n   - Check whether the background task has completed using \\`TaskOutput\\` with \\`block: false\\`\n   - Repeat until the background task finishes\n   - **Important**: Keep polling even if progress.log doesn't exist yet (the command takes a few seconds to start writing)\n\n4. **On completion**, read the full background task output and summarize:\n   - Files updated\n   - Files unchanged\n   - Any orphaned docs cleaned up\n\n**Options:**\n- \\`--uncommitted\\`: Include staged but uncommitted changes\n- \\`--dry-run\\`: Show what would be updated without writing\n- \\`--concurrency N\\`: Control number of parallel AI calls (default: auto)\n- \\`--fail-fast\\`: Stop on first file analysis failure\n- \\`--debug\\`: Show AI prompts and backend details\n- \\`--trace\\`: Enable concurrency tracing to \\`.agents-reverse-engineer/traces/\\`\n</execution>`,\n  },\n\n  init: {\n    description: 'Initialize agents-reverse-engineer configuration',\n    argumentHint: '',\n    content: `Initialize agents-reverse-engineer configuration in this project.\n\n<execution>\n1. **Display version**: Read \\`VERSION_FILE_PATH\\` and show the user: \\`agents-reverse-engineer vX.Y.Z\\`\n\n2. Run the agents-reverse-engineer init command:\n\n\\`\\`\\`bash\nnpx agents-reverse-engineer@latest init\n\\`\\`\\`\n\nThis creates \\`.agents-reverse-engineer/config.yaml\\` configuration file.\n</execution>`,\n  },\n\n  discover: {\n    description: 'Discover files in codebase',\n    argumentHint: '[path] [--debug] [--trace]',\n    content: `List files that would be analyzed for documentation.\n\n<execution>\n## STRICT RULES - VIOLATION IS FORBIDDEN\n\n1. Run ONLY this exact command: \\`npx agents-reverse-engineer@latest discover $ARGUMENTS\\`\n2. DO NOT add ANY flags the user did not explicitly type\n3. If user typed nothing after \\`COMMAND_PREFIXdiscover\\`, run with ZERO flags\n\n## Steps\n\n1. **Display version**: Read \\`VERSION_FILE_PATH\\` and show the user: \\`agents-reverse-engineer vX.Y.Z\\`\n\n2. **Run the discover command in the background** using \\`run_in_background: true\\`:\n   \\`\\`\\`bash\n   npx agents-reverse-engineer@latest discover $ARGUMENTS\n   \\`\\`\\`\n\n3. **Monitor progress by polling** \\`.agents-reverse-engineer/progress.log\\`:\n   - Wait ~10 seconds (use \\`sleep 10\\` in Bash), then use the **Read** tool to read \\`.agents-reverse-engineer/progress.log\\` (use the \\`offset\\` parameter to read only the last ~20 lines for long files)\n   - Show the user a brief progress update\n   - Check whether the background task has completed using \\`TaskOutput\\` with \\`block: false\\`\n   - Repeat until the background task finishes\n   - **Important**: Keep polling even if progress.log doesn't exist yet (the command takes a few seconds to start writing)\n\n4. **On completion**, read the full background task output and report number of files found.\n</execution>`,\n  },\n\n  clean: {\n    description: 'Delete all generated documentation artifacts (.sum, AGENTS.md, plan)',\n    argumentHint: '[path] [--dry-run]',\n    content: `Remove all documentation artifacts generated by agents-reverse-engineer.\n\n<execution>\n## STRICT RULES - VIOLATION IS FORBIDDEN\n\n1. Run ONLY this exact command: \\`npx agents-reverse-engineer@latest clean $ARGUMENTS\\`\n2. DO NOT add ANY flags the user did not explicitly type\n3. If user typed nothing after \\`COMMAND_PREFIXclean\\`, run with ZERO flags\n\n**Display version**: Read \\`VERSION_FILE_PATH\\` and show the user: \\`agents-reverse-engineer vX.Y.Z\\`\n\n\\`\\`\\`bash\nnpx agents-reverse-engineer@latest clean $ARGUMENTS\n\\`\\`\\`\n\nReport number of files deleted.\n</execution>`,\n  },\n\n  specify: {\n    description: 'Generate project specification from AGENTS.md docs',\n    argumentHint: '[path] [--dry-run] [--output <path>] [--multi-file] [--force] [--debug] [--trace]',\n    content: `Generate a project specification from existing AGENTS.md documentation.\n\n<execution>\nRun the specify command in the background and monitor progress in real time.\n\n## Steps\n\n1. **Display version**: Read \\`VERSION_FILE_PATH\\` and show the user: \\`agents-reverse-engineer vX.Y.Z\\`\n\n2. **Run the specify command in the background** using \\`run_in_background: true\\`:\n   \\`\\`\\`bash\n   npx agents-reverse-engineer@latest specify $ARGUMENTS\n   \\`\\`\\`\n\n3. **Monitor progress by polling** \\`.agents-reverse-engineer/progress.log\\`:\n   - Wait ~15 seconds (use \\`sleep 15\\` in Bash), then use the **Read** tool to read \\`.agents-reverse-engineer/progress.log\\` (use the \\`offset\\` parameter to read only the last ~20 lines for long files)\n   - Show the user a brief progress update\n   - Check whether the background task has completed using \\`TaskOutput\\` with \\`block: false\\`\n   - Repeat until the background task finishes\n   - **Important**: Keep polling even if progress.log doesn't exist yet (the command takes a few seconds to start writing)\n\n4. **On completion**, read the full background task output and summarize:\n   - Number of AGENTS.md files collected\n   - Output file(s) written\n\nThis collects all AGENTS.md files, synthesizes them via AI, and writes a comprehensive project specification.\n\nIf no AGENTS.md files exist, it will auto-run \\`generate\\` first.\n\n**Options:**\n- \\`--dry-run\\`: Show input statistics without making AI calls\n- \\`--output <path>\\`: Custom output path (default: specs/SPEC.md)\n- \\`--multi-file\\`: Split specification into multiple files\n- \\`--force\\`: Overwrite existing specification\n- \\`--debug\\`: Show AI prompts and backend details\n- \\`--trace\\`: Enable concurrency tracing to \\`.agents-reverse-engineer/traces/\\`\n</execution>`,\n  },\n\n  help: {\n    description: 'Show available ARE commands and usage guide',\n    argumentHint: '',\n    // Content uses COMMAND_PREFIX placeholder, replaced per platform\n    content: `<objective>\nDisplay the complete ARE command reference.\n\n**First**: Read \\`VERSION_FILE_PATH\\` and show the user the version: \\`agents-reverse-engineer vX.Y.Z\\`\n\n**Then**: Output ONLY the reference content below. Do NOT add:\n- Project-specific analysis\n- Git status or file context\n- Next-step suggestions\n- Any commentary beyond the reference\n</objective>\n\n<reference>\n# agents-reverse-engineer (ARE) Command Reference\n\n**ARE** generates AI-friendly documentation for codebases, creating structured summaries optimized for AI assistants.\n\n## Quick Start\n\n1. \\`COMMAND_PREFIXinit\\` — Create configuration file\n2. \\`COMMAND_PREFIXgenerate\\` — Generate documentation for the codebase\n3. \\`COMMAND_PREFIXupdate\\` — Keep docs in sync after code changes\n\n## Commands Reference\n\n### \\`COMMAND_PREFIXinit\\`\nInitialize configuration in this project.\n\nCreates \\`.agents-reverse-engineer/config.yaml\\` with customizable settings.\n\n**Usage:** \\`COMMAND_PREFIXinit\\`\n**CLI:** \\`npx are init\\`\n\n---\n\n### \\`COMMAND_PREFIXdiscover\\`\nDiscover files that would be analyzed for documentation.\n\nShows included files, excluded files with reasons, and generates a \\`GENERATION-PLAN.md\\` execution plan.\n\n**Options:**\n| Flag | Description |\n|------|-------------|\n| \\`[path]\\` | Target directory (default: current directory) |\n| \\`--debug\\` | Show verbose debug output |\n| \\`--trace\\` | Enable concurrency tracing to \\`.agents-reverse-engineer/traces/\\` |\n**Usage:**\n- \\`COMMAND_PREFIXdiscover\\` — Discover files and generate execution plan\n\n**CLI:**\n\\`\\`\\`bash\nnpx are discover\nnpx are discover ./src\n\\`\\`\\`\n\n---\n\n### \\`COMMAND_PREFIXgenerate\\`\nGenerate comprehensive documentation for the codebase.\n\n**Options:**\n| Flag | Description |\n|------|-------------|\n| \\`[path]\\` | Target directory (default: current directory) |\n| \\`--concurrency N\\` | Number of concurrent AI calls (default: auto) |\n| \\`--dry-run\\` | Show what would be generated without writing |\n| \\`--fail-fast\\` | Stop on first file analysis failure |\n| \\`--debug\\` | Show AI prompts and backend details |\n| \\`--trace\\` | Enable concurrency tracing to \\`.agents-reverse-engineer/traces/\\` |\n**Usage:**\n- \\`COMMAND_PREFIXgenerate\\` — Generate docs\n- \\`COMMAND_PREFIXgenerate --dry-run\\` — Preview without writing\n- \\`COMMAND_PREFIXgenerate --concurrency 3\\` — Limit parallel AI calls\n\n**CLI:**\n\\`\\`\\`bash\nnpx are generate\nnpx are generate --dry-run\nnpx are generate ./my-project --concurrency 3\nnpx are generate --debug --trace\n\\`\\`\\`\n\n**How it works:**\n1. Discovers files, applies filters, detects file types, and creates a generation plan\n2. Analyzes each file via concurrent AI calls, writes \\`.sum\\` summary files\n3. Generates \\`AGENTS.md\\` for each directory (post-order traversal)\n4. Creates root document: \\`CLAUDE.md\\`\n\n---\n\n### \\`COMMAND_PREFIXupdate\\`\nIncrementally update documentation for changed files.\n\n**Options:**\n| Flag | Description |\n|------|-------------|\n| \\`[path]\\` | Target directory (default: current directory) |\n| \\`--uncommitted\\` | Include staged but uncommitted changes |\n| \\`--dry-run\\` | Show what would be updated without writing |\n| \\`--concurrency N\\` | Number of concurrent AI calls (default: auto) |\n| \\`--fail-fast\\` | Stop on first file analysis failure |\n| \\`--debug\\` | Show AI prompts and backend details |\n| \\`--trace\\` | Enable concurrency tracing to \\`.agents-reverse-engineer/traces/\\` |\n**Usage:**\n- \\`COMMAND_PREFIXupdate\\` — Update docs for committed changes\n- \\`COMMAND_PREFIXupdate --uncommitted\\` — Include uncommitted changes\n\n**CLI:**\n\\`\\`\\`bash\nnpx are update\nnpx are update --uncommitted\nnpx are update --dry-run\nnpx are update ./my-project --concurrency 3\n\\`\\`\\`\n\n---\n\n### \\`COMMAND_PREFIXspecify\\`\nGenerate a project specification from AGENTS.md documentation.\n\nCollects all AGENTS.md files, synthesizes them via AI, and writes a comprehensive project specification. Auto-runs \\`generate\\` if no AGENTS.md files exist.\n\n**Options:**\n| Flag | Description |\n|------|-------------|\n| \\`[path]\\` | Target directory (default: current directory) |\n| \\`--output <path>\\` | Custom output path (default: specs/SPEC.md) |\n| \\`--multi-file\\` | Split specification into multiple files |\n| \\`--force\\` | Overwrite existing specification |\n| \\`--dry-run\\` | Show input statistics without making AI calls |\n| \\`--debug\\` | Show AI prompts and backend details |\n| \\`--trace\\` | Enable concurrency tracing to \\`.agents-reverse-engineer/traces/\\` |\n**Usage:**\n- \\`COMMAND_PREFIXspecify\\` — Generate specification\n- \\`COMMAND_PREFIXspecify --dry-run\\` — Preview without calling AI\n- \\`COMMAND_PREFIXspecify --output ./docs/spec.md --force\\` — Custom output path\n\n**CLI:**\n\\`\\`\\`bash\nnpx are specify\nnpx are specify --dry-run\nnpx are specify --output ./docs/spec.md --force\nnpx are specify --multi-file\n\\`\\`\\`\n\n---\n\n### \\`COMMAND_PREFIXclean\\`\nRemove all generated documentation artifacts.\n\n**Options:**\n| Flag | Description |\n|------|-------------|\n| \\`--dry-run\\` | Show what would be deleted without deleting |\n\n**What gets deleted:**\n- \\`.agents-reverse-engineer/GENERATION-PLAN.md\\`\n- All \\`*.sum\\` files\n- All \\`AGENTS.md\\` files\n- Root docs: \\`CLAUDE.md\\`\n\n**Usage:**\n- \\`COMMAND_PREFIXclean --dry-run\\` — Preview deletions\n- \\`COMMAND_PREFIXclean\\` — Delete all artifacts\n\n**CLI:**\n\\`\\`\\`bash\nnpx are clean --dry-run\nnpx are clean\n\\`\\`\\`\n\n---\n\n### \\`COMMAND_PREFIXhelp\\`\nShow this command reference.\n\n## CLI Installation\n\nInstall ARE commands to your AI assistant:\n\n\\`\\`\\`bash\nnpx agents-reverse-engineer install              # Interactive mode\nnpx agents-reverse-engineer install --runtime claude -g  # Global Claude\nnpx agents-reverse-engineer install --runtime claude -l  # Local project\nnpx agents-reverse-engineer install --runtime all -g     # All runtimes\n\\`\\`\\`\n\n**Install/Uninstall Options:**\n| Flag | Description |\n|------|-------------|\n| \\`--runtime <name>\\` | Target: \\`claude\\`, \\`opencode\\`, \\`gemini\\`, \\`all\\` |\n| \\`-g, --global\\` | Install to global config directory |\n| \\`-l, --local\\` | Install to current project directory |\n| \\`--force\\` | Overwrite existing files (install only) |\n\n## Configuration\n\n**File:** \\`.agents-reverse-engineer/config.yaml\\`\n\n\\`\\`\\`yaml\n# Exclusion patterns\nexclude:\n  patterns:\n    - \"**/*.test.ts\"\n    - \"**/__mocks__/**\"\n  vendorDirs:\n    - node_modules\n    - dist\n    - .git\n  binaryExtensions:\n    - .png\n    - .jpg\n    - .pdf\n\n# Options\noptions:\n  followSymlinks: false\n  maxFileSize: 100000\n\n# Output settings\noutput:\n  colors: true\n\\`\\`\\`\n\n## Generated Files\n\n### Per Source File\n\n**\\`*.sum\\`** — File summaries with YAML frontmatter + detailed prose.\n\n\\`\\`\\`yaml\n---\nfile_type: service\ngenerated_at: 2025-01-15T10:30:00Z\ncontent_hash: abc123...\npurpose: Handles user authentication and session management\npublic_interface: [login(), logout(), refreshToken(), AuthService]\ndependencies: [express, jsonwebtoken, ./user-model]\npatterns: [singleton, factory, observer]\nrelated_files: [./types.ts, ./middleware.ts]\n---\n\n<300-500 word summary covering implementation, patterns, edge cases>\n\\`\\`\\`\n\n### Per Directory\n\n**\\`AGENTS.md\\`** — Directory overview synthesized from \\`.sum\\` files. Groups files by purpose and links to subdirectories.\n\n### Root Documents\n\n| File | Purpose |\n|------|---------|\n| \\`CLAUDE.md\\` | Project entry point — synthesizes all AGENTS.md |\n\n## Common Workflows\n\n**Initial documentation:**\n\\`\\`\\`\nCOMMAND_PREFIXinit\nCOMMAND_PREFIXgenerate\n\\`\\`\\`\n\n**After code changes:**\n\\`\\`\\`\nCOMMAND_PREFIXupdate\n\\`\\`\\`\n\n**Full regeneration:**\n\\`\\`\\`\nCOMMAND_PREFIXclean\nCOMMAND_PREFIXgenerate\n\\`\\`\\`\n\n**Preview before generating:**\n\\`\\`\\`\nCOMMAND_PREFIXdiscover                   # Check files and exclusions\nCOMMAND_PREFIXgenerate --dry-run         # Preview generation\n\\`\\`\\`\n\n## Tips\n\n- **Custom exclusions**: Edit \\`.agents-reverse-engineer/config.yaml\\` to skip files\n- **Hook auto-update**: Install creates a session-end hook that auto-runs update\n\n## Resources\n\n- **Repository:** https://github.com/GeoloeG-IsT/agents-reverse-engineer\n- **Update:** \\`npx agents-reverse-engineer@latest install --force\\`\n</reference>`,\n  },\n} as const;\n\n// =============================================================================\n// Platform-specific template generators\n// =============================================================================\n\ntype Platform = 'claude' | 'opencode' | 'gemini';\n\ninterface PlatformConfig {\n  commandPrefix: string; // /are- (claude, opencode) or /are: (gemini)\n  pathPrefix: string; // .claude/commands/are/ or .opencode/commands/ etc\n  filenameSeparator: string; // . or -\n  extraFrontmatter?: string; // e.g., \"agent: build\" for OpenCode\n  usesName: boolean; // Claude uses \"name:\" in frontmatter\n  versionFilePath: string; // .claude/ARE-VERSION, .opencode/ARE-VERSION, etc.\n}\n\nconst PLATFORM_CONFIGS: Record<Platform, PlatformConfig> = {\n  claude: {\n    commandPrefix: '/are-',\n    pathPrefix: '.claude/skills/',\n    filenameSeparator: '.',\n    usesName: true,\n    versionFilePath: '.claude/ARE-VERSION',\n  },\n  opencode: {\n    commandPrefix: '/are-',\n    pathPrefix: '.opencode/commands/',\n    filenameSeparator: '-',\n    extraFrontmatter: 'agent: build',\n    usesName: false,\n    versionFilePath: '.opencode/ARE-VERSION',\n  },\n  gemini: {\n    commandPrefix: '/are-',\n    pathPrefix: '.gemini/commands/',\n    filenameSeparator: '-',\n    usesName: false,\n    versionFilePath: '.gemini/ARE-VERSION',\n  },\n};\n\nfunction buildFrontmatter(\n  platform: Platform,\n  commandName: string,\n  description: string\n): string {\n  const config = PLATFORM_CONFIGS[platform];\n  const lines = ['---'];\n\n  if (config.usesName) {\n    lines.push(`name: are-${commandName}`);\n  }\n\n  lines.push(`description: ${description}`);\n\n  if (config.extraFrontmatter) {\n    lines.push(config.extraFrontmatter);\n  }\n\n  lines.push('---');\n  return lines.join('\\n');\n}\n\n/**\n * Build TOML content for Gemini CLI commands\n *\n * Gemini uses TOML format with description and prompt fields.\n * See: https://geminicli.com/docs/cli/custom-commands/\n */\nfunction buildGeminiToml(\n  commandName: string,\n  command: (typeof COMMANDS)[keyof typeof COMMANDS]\n): string {\n  const config = PLATFORM_CONFIGS.gemini;\n  // Replace placeholders in content\n  const promptContent = command.content\n    .replace(/COMMAND_PREFIX/g, config.commandPrefix)\n    .replace(/VERSION_FILE_PATH/g, config.versionFilePath);\n\n  // Build TOML content\n  // Use triple quotes for multi-line prompt\n  const lines = [`description = \"${command.description}\"`];\n\n  if (command.argumentHint) {\n    lines.push(`# Arguments: ${command.argumentHint}`);\n  }\n\n  lines.push(`prompt = \"\"\"`);\n  lines.push(promptContent);\n  lines.push(`\"\"\"`);\n\n  return lines.join('\\n');\n}\n\nfunction buildTemplate(\n  platform: Platform,\n  commandName: string,\n  command: (typeof COMMANDS)[keyof typeof COMMANDS]\n): IntegrationTemplate {\n  const config = PLATFORM_CONFIGS[platform];\n\n  // Platform-specific file naming:\n  // - Claude: .claude/skills/are-{command}/SKILL.md\n  // - OpenCode: .opencode/commands/are-{command}.md\n  // - Gemini: .gemini/commands/are-{command}.toml (TOML format)\n  if (platform === 'gemini') {\n    const filename = `are-${commandName}.toml`;\n    const path = `${config.pathPrefix}${filename}`;\n    const content = buildGeminiToml(commandName, command);\n\n    return {\n      filename,\n      path,\n      content: `${content}\\n`,\n    };\n  }\n\n  const filename = platform === 'claude' ? 'SKILL.md' : `are-${commandName}.md`;\n  const path =\n    platform === 'claude'\n      ? `${config.pathPrefix}are-${commandName}/${filename}`\n      : `${config.pathPrefix}${filename}`;\n\n  const frontmatter = buildFrontmatter(\n    platform,\n    commandName,\n    command.description\n  );\n\n  // Replace placeholders in content\n  const content = command.content\n    .replace(/COMMAND_PREFIX/g, config.commandPrefix)\n    .replace(/VERSION_FILE_PATH/g, config.versionFilePath);\n\n  return {\n    filename,\n    path,\n    content: `${frontmatter}\\n\\n${content}\\n`,\n  };\n}\n\nfunction getTemplatesForPlatform(platform: Platform): IntegrationTemplate[] {\n  return Object.entries(COMMANDS).map(([name, command]) =>\n    buildTemplate(platform, name, command)\n  );\n}\n\n// =============================================================================\n// Public API\n// =============================================================================\n\n/**\n * Get Claude Code command file templates\n */\nexport function getClaudeTemplates(): IntegrationTemplate[] {\n  return getTemplatesForPlatform('claude');\n}\n\n/**\n * Get OpenCode command file templates\n */\nexport function getOpenCodeTemplates(): IntegrationTemplate[] {\n  return getTemplatesForPlatform('opencode');\n}\n\n/**\n * Get Gemini CLI command file templates\n */\nexport function getGeminiTemplates(): IntegrationTemplate[] {\n  return getTemplatesForPlatform('gemini');\n}\n\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**Exports getClaudeTemplates(), getOpenCodeTemplates(), and getGeminiTemplates() to generate platform-specific command integration templates for IDE installation, encapsulating COMMANDS object with prompt content, PLATFORM_CONFIGS with naming conventions, and buildTemplate()/buildGeminiToml() factories to produce IntegrationTemplate objects with filename/path/content.**\n\n## Exported Functions\n\n- `getClaudeTemplates(): IntegrationTemplate[]` — Returns array of Claude Code skill templates with `.claude/skills/are-{command}/SKILL.md` paths and `name:` frontmatter\n- `getOpenCodeTemplates(): IntegrationTemplate[]` — Returns array of OpenCode command templates with `.opencode/commands/are-{command}.md` paths and `agent: build` frontmatter\n- `getGeminiTemplates(): IntegrationTemplate[]` — Returns array of Gemini CLI command templates with `.gemini/commands/are-{command}.toml` paths and TOML `description`/`prompt` fields\n\n## Command Definitions\n\nThe `COMMANDS` constant defines seven command configurations: `generate`, `update`, `init`, `discover`, `clean`, `specify`, `help`. Each entry contains:\n\n- `description: string` — One-line command summary\n- `argumentHint: string` — CLI argument syntax (e.g., `'[path] [--dry-run] [--concurrency N]'`)\n- `content: string` — Multi-line markdown prompt with `<execution>` blocks, placeholder variables `COMMAND_PREFIX` and `VERSION_FILE_PATH`\n\n### Command Behavior Templates\n\n- **generate**: Background execution workflow with 15s poll intervals, reads `.agents-reverse-engineer/progress.log` using offset parameter, checks TaskOutput with `block: false`, summarizes three-phase pipeline (discovery → file analysis → directory/root docs)\n- **update**: Background execution identical to generate but reports files updated/unchanged/orphaned, includes `--uncommitted` flag for staged changes\n- **init**: Synchronous execution creates `.agents-reverse-engineer/config.yaml`\n- **discover**: Background execution with 10s poll intervals, reports file counts from discovery scan\n- **clean**: Synchronous execution with STRICT RULES comment block enforcing zero flag additions, reports deletion counts\n- **specify**: Background execution with 15s poll intervals, auto-runs generate if no AGENTS.md files exist, supports `--multi-file` and `--output` flags\n- **help**: Outputs comprehensive reference with `COMMAND_PREFIX` placeholder for per-platform command syntax (e.g., `/are-generate` vs `/are:generate`)\n\n## Platform Configuration\n\nThe `PLATFORM_CONFIGS` object maps three Platform types (`'claude' | 'opencode' | 'gemini'`) to PlatformConfig objects:\n\n- **claude**: `commandPrefix: '/are-'`, `pathPrefix: '.claude/skills/'`, `filenameSeparator: '.'`, `usesName: true`, `versionFilePath: '.claude/ARE-VERSION'`\n- **opencode**: `commandPrefix: '/are-'`, `pathPrefix: '.opencode/commands/'`, `filenameSeparator: '-'`, `extraFrontmatter: 'agent: build'`, `usesName: false`, `versionFilePath: '.opencode/ARE-VERSION'`\n- **gemini**: `commandPrefix: '/are-'`, `pathPrefix: '.gemini/commands/'`, `filenameSeparator: '-'`, `usesName: false`, `versionFilePath: '.gemini/ARE-VERSION'`\n\n## Template Generation Logic\n\n- `buildFrontmatter(platform, commandName, description): string` — Constructs YAML frontmatter with optional `name:` field (if usesName true) and optional extraFrontmatter string\n- `buildGeminiToml(commandName, command): string` — Generates TOML format with `description = \"...\"` and `prompt = \"\"\"...\"\"\"` triple-quoted multiline strings after replacing COMMAND_PREFIX and VERSION_FILE_PATH placeholders\n- `buildTemplate(platform, commandName, command): IntegrationTemplate` — Delegates to buildGeminiToml() for gemini platform, otherwise constructs markdown with frontmatter + content, performs placeholder substitution for COMMAND_PREFIX and VERSION_FILE_PATH\n- `getTemplatesForPlatform(platform): IntegrationTemplate[]` — Maps Object.entries(COMMANDS) through buildTemplate() for the specified platform\n\n## Path Naming Patterns\n\n- Claude: Nested directory structure `.claude/skills/are-{command}/SKILL.md` (e.g., `.claude/skills/are-generate/SKILL.md`)\n- OpenCode: Flat file structure `.opencode/commands/are-{command}.md` (e.g., `.opencode/commands/are-generate.md`)\n- Gemini: Flat TOML structure `.gemini/commands/are-{command}.toml` (e.g., `.gemini/commands/are-generate.toml`)\n\n## Template Content Patterns\n\nAll command content blocks include:\n- Version display instruction: `Read VERSION_FILE_PATH and show the user: agents-reverse-engineer vX.Y.Z`\n- Background execution via `npx agents-reverse-engineer@latest {command} $ARGUMENTS`\n- Progress polling loop: `sleep N`, Read tool with offset parameter, TaskOutput with `block: false`\n- Completion summary with metrics (files analyzed/updated/deleted, phase counts, inconsistency warnings)\n\nCommands with long execution times (generate, update, specify) use 15s poll intervals. Discover uses 10s intervals. Clean and init execute synchronously without polling.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 1324,
      "cacheReadTokens": 17367,
      "cacheCreationTokens": 15855,
      "latencyMs": 34791,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/integration/templates.ts",
          "sizeBytes": 22502
        }
      ]
    },
    {
      "timestamp": "2026-02-09T17:45:23.478Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/output/logger.ts\n\n```typescript\n/**\n * Terminal logger for agents-reverse\n *\n * Provides colored output.\n * Output format follows CONTEXT.md human-readable specification.\n */\n\nimport pc from 'picocolors';\n\n/**\n * Logger interface for CLI output.\n */\nexport interface Logger {\n  /** Log an informational message */\n  info(message: string): void;\n\n  /** Log a discovered file */\n  file(path: string): void;\n\n  /** Log an excluded file with reason */\n  excluded(path: string, reason: string, filter: string): void;\n\n  /** Log discovery summary */\n  summary(included: number, excluded: number): void;\n\n  /** Log a warning message */\n  warn(message: string): void;\n\n  /** Log an error message */\n  error(message: string): void;\n}\n\n/**\n * Options for creating a logger instance.\n */\nexport interface LoggerOptions {\n  /**\n   * Use colors in terminal output.\n   * @default true\n   */\n  colors: boolean;\n}\n\n/**\n * Color functions type - either picocolors or identity functions\n */\ninterface ColorFunctions {\n  green: (s: string) => string;\n  dim: (s: string) => string;\n  red: (s: string) => string;\n  bold: (s: string) => string;\n  yellow: (s: string) => string;\n}\n\n/**\n * Identity function for no-color mode\n */\nconst identity = (s: string): string => s;\n\n/**\n * No-color formatter that returns strings unchanged\n */\nconst noColor: ColorFunctions = {\n  green: identity,\n  dim: identity,\n  red: identity,\n  bold: identity,\n  yellow: identity,\n};\n\n/**\n * Create a logger instance with the given options.\n *\n * Output format per CONTEXT.md (human-readable):\n * - file: green \"  +\" prefix + relative path\n * - excluded: dim \"  -\" prefix + path + reason (when shown)\n * - summary: bold count + dim excluded count\n * - warn: yellow \"Warning:\" prefix\n * - error: red \"Error:\" prefix\n *\n * @param options - Logger configuration\n * @returns Logger instance\n *\n * @example\n * ```typescript\n * const log = createLogger({ colors: true });\n *\n * log.file('src/index.ts');\n * log.summary(42, 10);\n * ```\n */\nexport function createLogger(options: LoggerOptions): Logger {\n  const c: ColorFunctions = options.colors ? pc : noColor;\n\n  return {\n    info(message: string): void {\n      console.log(message);\n    },\n\n    file(path: string): void {\n      console.log(c.green('  +') + ' ' + path);\n    },\n\n    excluded(path: string, reason: string, filter: string): void {\n      console.log(c.dim('  -') + ' ' + path + c.dim(` (${reason}: ${filter})`));\n    },\n\n    summary(included: number, excluded: number): void {\n      console.log(\n        c.bold(`\\nDiscovered ${included} files`) +\n          c.dim(` (${excluded} excluded)`)\n      );\n    },\n\n    warn(message: string): void {\n      console.warn(c.yellow('Warning: ') + message);\n    },\n\n    error(message: string): void {\n      console.error(c.red('Error: ') + message);\n    },\n  };\n}\n\n/**\n * Create a silent logger that produces no output.\n *\n * Useful for testing or programmatic usage.\n *\n * @returns Logger instance with all no-op methods\n */\nexport function createSilentLogger(): Logger {\n  const noop = (): void => {};\n  return {\n    info: noop,\n    file: noop,\n    excluded: noop,\n    summary: noop,\n    warn: noop,\n    error: noop,\n  };\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**logger.ts provides terminal output formatting with optional picocolors styling, exporting createLogger() and createSilentLogger() factories for CLI messages.**\n\n## Exported Interfaces\n\n**Logger** defines six methods for CLI output:\n- `info(message: string): void` — informational messages\n- `file(path: string): void` — discovered file paths\n- `excluded(path: string, reason: string, filter: string): void` — excluded files with reasoning\n- `summary(included: number, excluded: number): void` — discovery count summary\n- `warn(message: string): void` — warning messages\n- `error(message: string): void` — error messages\n\n**LoggerOptions** configures logger instances:\n- `colors: boolean` — enables/disables ANSI color codes (default true)\n\n## Factory Functions\n\n**createLogger(options: LoggerOptions): Logger** constructs color-aware logger writing to console.log/warn/error with picocolors formatting when `options.colors === true`:\n- `file()` outputs `green(\"  +\") + \" \" + path`\n- `excluded()` outputs `dim(\"  -\") + \" \" + path + dim(\" (reason: filter)\")`\n- `summary()` outputs `bold(\"Discovered N files\") + dim(\" (M excluded)\")`\n- `warn()` outputs `yellow(\"Warning: \") + message`\n- `error()` outputs `red(\"Error: \") + message`\n- `info()` outputs plain `message`\n\n**createSilentLogger(): Logger** returns no-op Logger instance with all methods bound to empty function, used for testing or programmatic usage suppressing output.\n\n## Color System\n\n**ColorFunctions** interface abstracts picocolors API with five transform methods: `green`, `dim`, `red`, `bold`, `yellow`. When `options.colors === false`, logger uses **noColor** object mapping all transforms to **identity(s: string): string** pass-through function preserving original strings unchanged.\n\n## Output Format Specification\n\nLogger implements CONTEXT.md human-readable format with specific prefixes and styling:\n- Discovered files: `\"  +\"` prefix in green\n- Excluded files: `\"  -\"` prefix dimmed with parenthetical reason\n- Summary line: bold count followed by dimmed exclusion count\n- Warnings: `\"Warning: \"` prefix in yellow\n- Errors: `\"Error: \"` prefix in red\n\n## Dependencies\n\nImports `picocolors` as `pc` for ANSI color code generation. No other external dependencies beyond Node.js built-in `console` object.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 594,
      "cacheReadTokens": 17367,
      "cacheCreationTokens": 10076,
      "latencyMs": 19866,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/output/logger.ts",
          "sizeBytes": 3144
        }
      ]
    },
    {
      "timestamp": "2026-02-09T17:44:57.774Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/installer/operations.ts\n\n```typescript\n/**\n * File operations for installer module\n *\n * Handles copying command/hook files to runtime directories,\n * verifying installations, and registering hooks in settings.json.\n */\n\nimport { existsSync, mkdirSync, writeFileSync, readFileSync } from 'node:fs';\nimport * as path from 'node:path';\nimport { fileURLToPath } from 'node:url';\nimport type { Runtime, Location, InstallerResult } from './types.js';\nimport { resolveInstallPath, getAllRuntimes } from './paths.js';\nimport {\n  getClaudeTemplates,\n  getOpenCodeTemplates,\n  getGeminiTemplates,\n} from '../integration/templates.js';\n\n/**\n * Options for install operations\n */\nexport interface InstallOptions {\n  /** Overwrite existing files */\n  force: boolean;\n  /** Preview mode - don't write files */\n  dryRun: boolean;\n}\n\n/**\n * Ensure directory exists for a file path\n *\n * @param filePath - Full path to the file\n */\nfunction ensureDir(filePath: string): void {\n  const dir = path.dirname(filePath);\n  if (!existsSync(dir)) {\n    mkdirSync(dir, { recursive: true });\n  }\n}\n\n/**\n * Get the path to a bundled hook file\n *\n * Hooks are bundled in hooks/dist/ during npm prepublishOnly.\n *\n * @param hookName - Name of the hook file (e.g., 'are-session-end.js')\n * @returns Absolute path to the bundled hook file\n */\nfunction getBundledHookPath(hookName: string): string {\n  // Navigate from dist/installer/operations.js to hooks/dist/\n  const __filename = fileURLToPath(import.meta.url);\n  const __dirname = path.dirname(__filename);\n  // From dist/installer/ go up two levels to project root, then to hooks/dist/\n  return path.join(__dirname, '..', '..', 'hooks', 'dist', hookName);\n}\n\n/**\n * Read bundled hook content\n *\n * @param hookName - Name of the hook file\n * @returns Hook file content as string\n * @throws Error if hook file not found\n */\nfunction readBundledHook(hookName: string): string {\n  const hookPath = getBundledHookPath(hookName);\n  if (!existsSync(hookPath)) {\n    throw new Error(`Bundled hook not found: ${hookPath}`);\n  }\n  return readFileSync(hookPath, 'utf-8');\n}\n\n/**\n * Get templates for a specific runtime\n *\n * @param runtime - Target runtime (claude, opencode, or gemini)\n * @returns Array of template objects for the runtime\n */\nfunction getTemplatesForRuntime(runtime: Exclude<Runtime, 'all'>) {\n  switch (runtime) {\n    case 'claude':\n      return getClaudeTemplates();\n    case 'opencode':\n      return getOpenCodeTemplates();\n    case 'gemini':\n      return getGeminiTemplates();\n  }\n}\n\n/**\n * Install files for one or all runtimes\n *\n * If runtime is 'all', installs to all supported runtimes.\n * Otherwise, installs to the specified runtime only.\n *\n * @param runtime - Target runtime or 'all'\n * @param location - Installation location (global or local)\n * @param options - Install options (force, dryRun)\n * @returns Array of installation results (one per runtime)\n */\nexport function installFiles(\n  runtime: Runtime,\n  location: Location,\n  options: InstallOptions,\n): InstallerResult[] {\n  if (runtime === 'all') {\n    return getAllRuntimes().map((r) => installFilesForRuntime(r, location, options));\n  }\n  return [installFilesForRuntime(runtime, location, options)];\n}\n\n/**\n * Install files for a specific runtime\n *\n * Copies command templates and hook files to the installation directory.\n * Skips existing files unless force=true.\n *\n * @param runtime - Target runtime (claude, opencode, or gemini)\n * @param location - Installation location (global or local)\n * @param options - Install options (force, dryRun)\n * @returns Installation result with files created/skipped\n */\nfunction installFilesForRuntime(\n  runtime: Exclude<Runtime, 'all'>,\n  location: Location,\n  options: InstallOptions,\n): InstallerResult {\n  const basePath = resolveInstallPath(runtime, location);\n  const templates = getTemplatesForRuntime(runtime);\n  const filesCreated: string[] = [];\n  const filesSkipped: string[] = [];\n  const errors: string[] = [];\n\n  // Install command templates\n  for (const template of templates) {\n    // Template path is relative (e.g., .claude/commands/are/generate.md)\n    // Extract the part after the runtime directory (e.g., commands/are/generate.md)\n    const relativePath = template.path.split('/').slice(1).join('/');\n    const fullPath = path.join(basePath, relativePath);\n\n    if (existsSync(fullPath) && !options.force) {\n      filesSkipped.push(fullPath);\n    } else {\n      if (!options.dryRun) {\n        try {\n          ensureDir(fullPath);\n          writeFileSync(fullPath, template.content, 'utf-8');\n        } catch (err) {\n          errors.push(`Failed to write ${fullPath}: ${err}`);\n          continue;\n        }\n      }\n      filesCreated.push(fullPath);\n    }\n  }\n\n  // Install hooks/plugins based on runtime\n  let hookRegistered = false;\n  if (runtime === 'claude' || runtime === 'gemini') {\n    // Claude and Gemini: install session hooks\n    for (const hookDef of ARE_HOOKS) {\n      const hookPath = path.join(basePath, 'hooks', hookDef.filename);\n      if (existsSync(hookPath) && !options.force) {\n        filesSkipped.push(hookPath);\n      } else {\n        if (!options.dryRun) {\n          try {\n            ensureDir(hookPath);\n            const hookContent = readBundledHook(hookDef.filename);\n            writeFileSync(hookPath, hookContent, 'utf-8');\n          } catch (err) {\n            errors.push(`Failed to write hook ${hookPath}: ${err}`);\n          }\n        }\n        if (!errors.some((e) => e.includes(hookPath))) {\n          filesCreated.push(hookPath);\n        }\n      }\n    }\n\n    // Register hooks in settings.json\n    hookRegistered = registerHooks(basePath, runtime, options.dryRun);\n\n    // Register permissions for Claude (reduces friction for users)\n    if (runtime === 'claude') {\n      const settingsPath = path.join(basePath, 'settings.json');\n      registerPermissions(settingsPath, options.dryRun);\n    }\n  } else if (runtime === 'opencode') {\n    // OpenCode: install plugins (auto-loaded from plugins/ directory)\n    for (const pluginDef of ARE_PLUGINS) {\n      const pluginPath = path.join(basePath, 'plugins', pluginDef.destFilename);\n      if (existsSync(pluginPath) && !options.force) {\n        filesSkipped.push(pluginPath);\n      } else {\n        if (!options.dryRun) {\n          try {\n            ensureDir(pluginPath);\n            const pluginContent = readBundledHook(pluginDef.srcFilename);\n            writeFileSync(pluginPath, pluginContent, 'utf-8');\n          } catch (err) {\n            errors.push(`Failed to write plugin ${pluginPath}: ${err}`);\n          }\n        }\n        if (!errors.some((e) => e.includes(pluginPath))) {\n          filesCreated.push(pluginPath);\n          hookRegistered = true;\n        }\n      }\n    }\n  }\n\n  // Write VERSION file if files were created and not dry run\n  let versionWritten = false;\n  if (filesCreated.length > 0 && !options.dryRun) {\n    try {\n      writeVersionFile(basePath, options.dryRun);\n      versionWritten = true;\n    } catch {\n      // Non-fatal, don't add to errors\n    }\n  }\n\n  return {\n    success: errors.length === 0,\n    runtime,\n    location,\n    filesCreated,\n    filesSkipped,\n    errors,\n    hookRegistered,\n    versionWritten,\n  };\n}\n\n/**\n * Verify that installed files exist\n *\n * @param files - Array of file paths to verify\n * @returns Object with success status and list of missing files\n */\nexport function verifyInstallation(files: string[]): { success: boolean; missing: string[] } {\n  const missing = files.filter((f) => !existsSync(f));\n  return {\n    success: missing.length === 0,\n    missing,\n  };\n}\n\n/**\n * Session hook configuration for settings.json\n */\ninterface SessionHook {\n  type: 'command';\n  command: string;\n}\n\ninterface HookEvent {\n  hooks: SessionHook[];\n}\n\ninterface SettingsJson {\n  hooks?: {\n    SessionStart?: HookEvent[];\n    SessionEnd?: HookEvent[];\n  };\n  permissions?: {\n    allow?: string[];\n    deny?: string[];\n  };\n  [key: string]: unknown;\n}\n\n/**\n * Gemini hook configuration (simpler format)\n */\ninterface GeminiHook {\n  name: string;\n  type: 'command';\n  command: string;\n}\n\ninterface GeminiSettingsJson {\n  hooks?: {\n    SessionStart?: GeminiHook[];\n    SessionEnd?: GeminiHook[];\n  };\n  [key: string]: unknown;\n}\n\n/**\n * Hook definitions for ARE (Claude and Gemini)\n */\ninterface HookDefinition {\n  event: 'SessionStart' | 'SessionEnd';\n  filename: string;\n  name: string; // For Gemini format\n}\n\nconst ARE_HOOKS: HookDefinition[] = [\n  // { event: 'SessionStart', filename: 'are-check-update.js', name: 'are-check-update' },\n  // { event: 'SessionEnd', filename: 'are-session-end.js', name: 'are-session-end' }, // Disabled - causing issues\n];\n\n/**\n * Plugin definitions for ARE (OpenCode)\n *\n * OpenCode uses a plugin system (.opencode/plugins/) instead of hooks.\n * Plugins are JS/TS modules that export async functions returning event handlers.\n */\ninterface PluginDefinition {\n  /** Source filename in hooks/dist/ (prefixed with opencode-) */\n  srcFilename: string;\n  /** Destination filename in .opencode/plugins/ */\n  destFilename: string;\n}\n\nconst ARE_PLUGINS: PluginDefinition[] = [\n  { srcFilename: 'opencode-are-check-update.js', destFilename: 'are-check-update.js' },\n  // { srcFilename: 'opencode-are-session-end.js', destFilename: 'are-session-end.js' }, // Disabled - causing issues\n];\n\n/**\n * Register ARE hooks in settings.json\n *\n * Registers both SessionStart (update check) and SessionEnd (auto-update) hooks.\n * Supports both Claude Code and Gemini CLI formats.\n * Merges with existing hooks, doesn't overwrite.\n *\n * @param basePath - Base installation path (e.g., ~/.claude or ~/.gemini)\n * @param runtime - Target runtime (claude or gemini)\n * @param dryRun - If true, don't write changes\n * @returns true if any hook was added, false if all already existed\n */\nexport function registerHooks(\n  basePath: string,\n  runtime: Exclude<Runtime, 'all'>,\n  dryRun: boolean,\n): boolean {\n  // Only for Claude and Gemini installations\n  if (runtime !== 'claude' && runtime !== 'gemini') {\n    return false;\n  }\n\n  const settingsPath = path.join(basePath, 'settings.json');\n  const runtimeDir = runtime === 'claude' ? '.claude' : '.gemini';\n\n  if (runtime === 'gemini') {\n    return registerGeminiHooks(settingsPath, runtimeDir, dryRun);\n  }\n\n  return registerClaudeHooks(settingsPath, runtimeDir, dryRun);\n}\n\n/**\n * Register ARE hooks in Claude Code settings.json format\n */\nfunction registerClaudeHooks(settingsPath: string, runtimeDir: string, dryRun: boolean): boolean {\n  // Load or create settings\n  let settings: SettingsJson = {};\n  if (existsSync(settingsPath)) {\n    try {\n      const content = readFileSync(settingsPath, 'utf-8');\n      settings = JSON.parse(content) as SettingsJson;\n    } catch {\n      // If can't parse, start fresh\n      settings = {};\n    }\n  }\n\n  // Ensure hooks structure exists\n  if (!settings.hooks) {\n    settings.hooks = {};\n  }\n\n  let addedAny = false;\n\n  for (const hookDef of ARE_HOOKS) {\n    const hookCommand = `node ${runtimeDir}/hooks/${hookDef.filename}`;\n\n    // Ensure event array exists\n    if (!settings.hooks[hookDef.event]) {\n      settings.hooks[hookDef.event] = [];\n    }\n\n    // Check if hook already exists (by command string match)\n    const hookExists = settings.hooks[hookDef.event]!.some((event) =>\n      event.hooks?.some((h) => h.command === hookCommand),\n    );\n\n    if (!hookExists) {\n      // Define our hook (Claude format: nested hooks array)\n      const newHook: HookEvent = {\n        hooks: [\n          {\n            type: 'command',\n            command: hookCommand,\n          },\n        ],\n      };\n      settings.hooks[hookDef.event]!.push(newHook);\n      addedAny = true;\n    }\n  }\n\n  if (!addedAny) {\n    return false;\n  }\n\n  // Write settings if not dry run\n  if (!dryRun) {\n    ensureDir(settingsPath);\n    writeFileSync(settingsPath, JSON.stringify(settings, null, 2), 'utf-8');\n  }\n\n  return true;\n}\n\n/**\n * Permissions to auto-allow for ARE commands\n */\nconst ARE_PERMISSIONS = [\n  'Bash(npx agents-reverse-engineer@latest init*)',\n  'Bash(npx agents-reverse-engineer@latest discover*)',\n  'Bash(npx agents-reverse-engineer@latest generate*)',\n  'Bash(npx agents-reverse-engineer@latest update*)',\n  'Bash(npx agents-reverse-engineer@latest clean*)',\n  'Bash(rm -f .agents-reverse-engineer/progress.log*)',\n  'Bash(sleep *)',\n];\n\n/**\n * Register ARE permissions in Claude Code settings.json\n *\n * Adds bash command permissions for ARE commands to reduce friction.\n *\n * @param settingsPath - Path to settings.json\n * @param dryRun - If true, don't write changes\n * @returns true if permissions were added, false if already existed\n */\nexport function registerPermissions(settingsPath: string, dryRun: boolean): boolean {\n  // Load or create settings\n  let settings: SettingsJson = {};\n  if (existsSync(settingsPath)) {\n    try {\n      const content = readFileSync(settingsPath, 'utf-8');\n      settings = JSON.parse(content) as SettingsJson;\n    } catch {\n      // If can't parse, start fresh\n      settings = {};\n    }\n  }\n\n  // Ensure permissions structure exists\n  if (!settings.permissions) {\n    settings.permissions = {};\n  }\n  if (!settings.permissions.allow) {\n    settings.permissions.allow = [];\n  }\n\n  // Add any missing ARE permissions\n  let addedAny = false;\n  for (const perm of ARE_PERMISSIONS) {\n    if (!settings.permissions.allow.includes(perm)) {\n      settings.permissions.allow.push(perm);\n      addedAny = true;\n    }\n  }\n\n  if (!addedAny) {\n    return false;\n  }\n\n  // Write settings if not dry run\n  if (!dryRun) {\n    ensureDir(settingsPath);\n    writeFileSync(settingsPath, JSON.stringify(settings, null, 2), 'utf-8');\n  }\n\n  return true;\n}\n\n/**\n * Register ARE hooks in Gemini CLI settings.json format\n */\nfunction registerGeminiHooks(settingsPath: string, runtimeDir: string, dryRun: boolean): boolean {\n  // Load or create settings\n  let settings: GeminiSettingsJson = {};\n  if (existsSync(settingsPath)) {\n    try {\n      const content = readFileSync(settingsPath, 'utf-8');\n      settings = JSON.parse(content) as GeminiSettingsJson;\n    } catch {\n      // If can't parse, start fresh\n      settings = {};\n    }\n  }\n\n  // Ensure hooks structure exists\n  if (!settings.hooks) {\n    settings.hooks = {};\n  }\n\n  let addedAny = false;\n\n  for (const hookDef of ARE_HOOKS) {\n    const hookCommand = `node ${runtimeDir}/hooks/${hookDef.filename}`;\n\n    // Ensure event array exists\n    if (!settings.hooks[hookDef.event]) {\n      settings.hooks[hookDef.event] = [];\n    }\n\n    // Check if hook already exists (by command string match)\n    const hookExists = settings.hooks[hookDef.event]!.some((h) => h.command === hookCommand);\n\n    if (!hookExists) {\n      // Define our hook (Gemini format: flat object with name)\n      const newHook: GeminiHook = {\n        name: hookDef.name,\n        type: 'command',\n        command: hookCommand,\n      };\n      settings.hooks[hookDef.event]!.push(newHook);\n      addedAny = true;\n    }\n  }\n\n  if (!addedAny) {\n    return false;\n  }\n\n  // Write settings if not dry run\n  if (!dryRun) {\n    ensureDir(settingsPath);\n    writeFileSync(settingsPath, JSON.stringify(settings, null, 2), 'utf-8');\n  }\n\n  return true;\n}\n\n/**\n * Get package version from package.json\n *\n * @returns Version string or 'unknown' if can't read\n */\nexport function getPackageVersion(): string {\n  try {\n    // Navigate from dist/installer/operations.js to package.json\n    // In ESM, we need to use import.meta.url\n    const __filename = fileURLToPath(import.meta.url);\n    const __dirname = path.dirname(__filename);\n    // From src/installer/ go up two levels to project root\n    const packagePath = path.join(__dirname, '..', '..', 'package.json');\n    const content = readFileSync(packagePath, 'utf-8');\n    const pkg = JSON.parse(content) as { version?: string };\n    return pkg.version || 'unknown';\n  } catch {\n    return 'unknown';\n  }\n}\n\n/**\n * Write ARE-VERSION file to track installed version\n *\n * @param basePath - Base installation path\n * @param dryRun - If true, don't write the file\n */\nexport function writeVersionFile(basePath: string, dryRun: boolean): void {\n  if (dryRun) {\n    return;\n  }\n\n  const versionPath = path.join(basePath, 'ARE-VERSION');\n  const version = getPackageVersion();\n\n  ensureDir(versionPath);\n  writeFileSync(versionPath, version, 'utf-8');\n}\n\n/**\n * Format installation result for display\n *\n * Generates human-readable lines showing created/skipped files.\n *\n * @param result - Installation result to format\n * @returns Array of formatted lines for display\n */\nexport function formatInstallResult(result: InstallerResult): string[] {\n  const lines: string[] = [];\n\n  // Header with runtime and location\n  lines.push(`  ${result.runtime} (${result.location}):`);\n\n  // Created files\n  for (const file of result.filesCreated) {\n    lines.push(`    Created: ${file}`);\n  }\n\n  // Skipped files\n  for (const file of result.filesSkipped) {\n    lines.push(`    Skipped: ${file} (already exists)`);\n  }\n\n  // Hook registration status (Claude only)\n  if (result.hookRegistered) {\n    lines.push(`    Registered: SessionEnd hook in settings.json`);\n  }\n\n  // Summary line\n  const created = result.filesCreated.length;\n  const skipped = result.filesSkipped.length;\n  lines.push(`    ${created} files installed, ${skipped} skipped`);\n\n  return lines;\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**operations.ts implements file installation, verification, and settings.json hook/permission registration for ARE command/hook deployment across Claude Code, OpenCode, and Gemini CLI runtimes.**\n\n## Exported Functions\n\n`installFiles(runtime: Runtime, location: Location, options: InstallOptions): InstallerResult[]` — installs command templates and hook/plugin files for specified runtime ('claude' | 'opencode' | 'gemini' | 'all'), returns array of InstallerResult with filesCreated/filesSkipped/errors/hookRegistered/versionWritten status.\n\n`verifyInstallation(files: string[]): { success: boolean; missing: string[] }` — checks existsSync() for each file path, returns verification status with missing file list.\n\n`registerHooks(basePath: string, runtime: Exclude<Runtime, 'all'>, dryRun: boolean): boolean` — writes hook configuration to settings.json for Claude/Gemini, returns true if hooks added, false if already existed.\n\n`registerPermissions(settingsPath: string, dryRun: boolean): boolean` — adds ARE_PERMISSIONS bash command patterns to Claude Code settings.json permissions.allow array.\n\n`getPackageVersion(): string` — reads version field from package.json via import.meta.url resolution, returns 'unknown' on error.\n\n`writeVersionFile(basePath: string, dryRun: boolean): void` — writes getPackageVersion() result to ARE-VERSION file in basePath.\n\n`formatInstallResult(result: InstallerResult): string[]` — generates human-readable lines showing runtime/location, created/skipped files, hook registration status, summary counts.\n\n## Exported Interfaces\n\n`InstallOptions { force: boolean; dryRun: boolean }` — controls file overwrite behavior and preview mode for install operations.\n\n## Hook/Plugin Definitions\n\n`ARE_HOOKS: HookDefinition[]` — array defining SessionStart/SessionEnd hooks with filename/event/name mappings (currently empty - hooks disabled due to issues).\n\n`HookDefinition { event: 'SessionStart' | 'SessionEnd'; filename: string; name: string }` — hook metadata for Claude/Gemini format conversion.\n\n`ARE_PLUGINS: PluginDefinition[]` — array mapping OpenCode plugin source filenames (prefixed 'opencode-') to destination filenames in .opencode/plugins/ (are-check-update.js; are-session-end.js disabled).\n\n`PluginDefinition { srcFilename: string; destFilename: string }` — source-to-destination mapping for OpenCode plugin installation.\n\n## Settings.json Schema\n\n`SettingsJson { hooks?: { SessionStart?: HookEvent[]; SessionEnd?: HookEvent[] }; permissions?: { allow?: string[]; deny?: string[] } }` — Claude Code settings.json structure with nested hooks arrays and permissions.\n\n`HookEvent { hooks: SessionHook[] }` — Claude format wraps hooks in nested array.\n\n`SessionHook { type: 'command'; command: string }` — individual hook definition with command string.\n\n`GeminiSettingsJson { hooks?: { SessionStart?: GeminiHook[]; SessionEnd?: GeminiHook[] } }` — Gemini CLI settings.json structure with flat hook arrays.\n\n`GeminiHook { name: string; type: 'command'; command: string }` — Gemini format includes explicit name field.\n\n## Hook Registration Logic\n\n`registerClaudeHooks(settingsPath: string, runtimeDir: string, dryRun: boolean): boolean` — loads settings.json via readFileSync(), merges ARE_HOOKS into nested HookEvent structure, writes JSON if addedAny=true, returns false if all hooks pre-existing.\n\n`registerGeminiHooks(settingsPath: string, runtimeDir: string, dryRun: boolean): boolean` — loads settings.json, merges ARE_HOOKS into flat GeminiHook array structure, uses hookDef.name field for Gemini format.\n\nHook command pattern: `node ${runtimeDir}/hooks/${hookDef.filename}` where runtimeDir='.claude' or '.gemini'.\n\nDuplicate detection: checks existing hooks via `event.hooks?.some((h) => h.command === hookCommand)` for Claude, `hooks[event].some((h) => h.command === hookCommand)` for Gemini.\n\n## Permission Registration\n\n`ARE_PERMISSIONS: string[]` — bash command patterns for auto-allow:\n- `'Bash(npx agents-reverse-engineer@latest init*)'`\n- `'Bash(npx agents-reverse-engineer@latest discover*)'`\n- `'Bash(npx agents-reverse-engineer@latest generate*)'`\n- `'Bash(npx agents-reverse-engineer@latest update*)'`\n- `'Bash(npx agents-reverse-engineer@latest clean*)'`\n- `'Bash(rm -f .agents-reverse-engineer/progress.log*)'`\n- `'Bash(sleep *)'`\n\nMerges via `settings.permissions.allow.includes(perm)` check before push, writes settings.json if addedAny=true.\n\n## File Operations\n\n`installFilesForRuntime(runtime: Exclude<Runtime, 'all'>, location: Location, options: InstallOptions): InstallerResult` — resolves basePath via resolveInstallPath(), fetches templates via getTemplatesForRuntime(), writes command files, installs hooks/plugins based on runtime type, calls registerHooks()/registerPermissions() for Claude, writeVersionFile() on success.\n\nTemplate path extraction: splits template.path by '/' and slices first segment (runtime directory prefix) to get relativePath joined to basePath.\n\nFile write flow: existsSync() skip check unless options.force=true, ensureDir() via mkdirSync recursive, writeFileSync() with UTF-8, error capture to errors[] array.\n\nHook/plugin installation branches:\n- Claude/Gemini: iterate ARE_HOOKS, call readBundledHook(), write to basePath/hooks/, invoke registerHooks()\n- OpenCode: iterate ARE_PLUGINS, call readBundledHook(pluginDef.srcFilename), write to basePath/plugins/, set hookRegistered=true\n\n## Bundled Hook Resolution\n\n`getBundledHookPath(hookName: string): string` — resolves from dist/installer/operations.js up two levels to project root, then joins 'hooks/dist/' + hookName.\n\n`readBundledHook(hookName: string): string` — reads hook content via getBundledHookPath(), throws Error if !existsSync().\n\nPath navigation: uses fileURLToPath(import.meta.url) for ESM compatibility, dirname resolution via path.dirname(), relative join via `__dirname/../../../hooks/dist/`.\n\n## Template Retrieval\n\n`getTemplatesForRuntime(runtime: Exclude<Runtime, 'all'>)` — switch dispatches to getClaudeTemplates() | getOpenCodeTemplates() | getGeminiTemplates() from src/integration/templates.ts.\n\n## Utility Functions\n\n`ensureDir(filePath: string): void` — creates parent directory via mkdirSync({ recursive: true }) if !existsSync(dirname).\n\n## Integration Points\n\nImports resolveInstallPath() and getAllRuntimes() from `./paths.js` for runtime-specific directory resolution.\n\nImports getClaudeTemplates(), getOpenCodeTemplates(), getGeminiTemplates() from `../integration/templates.js` for command file content.\n\nRuntime type discriminated union: `Runtime = 'claude' | 'opencode' | 'gemini' | 'all'` determines installation target and hook format.\n\nLocation type: `Location = 'global' | 'local'` controls base path resolution (e.g., ~/.claude vs ./.claude).\n\n## Error Handling\n\nNon-blocking errors: JSON parse failures in registerHooks() start with empty settings object, writeVersionFile() failures suppressed (non-fatal), ARE-VERSION write errors don't populate errors[] array.\n\nBlocking errors: readBundledHook() throws if hook not found, file write failures push to errors[] and set success=false in InstallerResult.\n\n## Dry-Run Mode\n\nWhen options.dryRun=true: skips writeFileSync() calls for templates/hooks/settings.json/ARE-VERSION, still populates filesCreated[] for preview output, sets versionWritten=false.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 1992,
      "cacheReadTokens": 17367,
      "cacheCreationTokens": 14522,
      "latencyMs": 46364,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/installer/operations.ts",
          "sizeBytes": 17303
        }
      ]
    },
    {
      "timestamp": "2026-02-09T17:45:29.147Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/specify/index.ts\n\n```typescript\nexport { buildSpecPrompt } from './prompts.js';\nexport type { SpecPrompt } from './prompts.js';\n\nexport { writeSpec, SpecExistsError } from './writer.js';\nexport type { WriteSpecOptions } from './writer.js';\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**Re-exports specification synthesis functionality: `buildSpecPrompt`, `SpecPrompt`, `writeSpec`, `SpecExistsError`, and `WriteSpecOptions` from `prompts.js` and `writer.js` submodules.**\n\n## Exported Symbols\n\n- `buildSpecPrompt` — function from `./prompts.js` that constructs AI prompts for synthesizing project specifications from AGENTS.md corpus\n- `SpecPrompt` — type from `./prompts.js` defining the structure of specification generation prompts\n- `writeSpec` — function from `./writer.js` that writes generated specification content to `specs/SPEC.md` or split multi-file outputs\n- `SpecExistsError` — error class from `./writer.js` thrown when specification files already exist and `--force` flag is not set\n- `WriteSpecOptions` — type from `./writer.js` defining options for `writeSpec` including `force` override and `multiFile` split mode\n\n## Module Role\n\nServes as barrel export for the specification synthesis subsystem invoked by `src/cli/specify.ts` command. Aggregates AGENTS.md documentation tree into executable project specifications via AI-driven synthesis, supporting both single-file (`specs/SPEC.md`) and multi-file (`specs/<dirname>.md`) output modes.\n\n## Integration Points\n\nConsumed by `src/cli/specify.ts` which orchestrates the `are specify` command workflow: collect AGENTS.md files via `src/generation/collector.ts`, invoke `buildSpecPrompt` with aggregated content, execute AI call via `src/ai/service.ts`, write output via `writeSpec` with conflict detection.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 386,
      "cacheReadTokens": 17367,
      "cacheCreationTokens": 9157,
      "latencyMs": 17669,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/specify/index.ts",
          "sizeBytes": 208
        }
      ]
    },
    {
      "timestamp": "2026-02-09T17:45:04.547Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/installer/uninstall.ts\n\n```typescript\n/**\n * Uninstall module for agents-reverse-engineer installer\n *\n * Handles removing installed command files, hooks, and hook registrations.\n * Mirrors the installation logic in operations.ts for clean reversal.\n */\n\nimport { existsSync, unlinkSync, readFileSync, writeFileSync, readdirSync, rmdirSync, rmSync } from 'node:fs';\nimport * as path from 'node:path';\nimport type { Runtime, Location, InstallerResult } from './types.js';\nimport { resolveInstallPath, getAllRuntimes, getRuntimePaths } from './paths.js';\nimport {\n  getClaudeTemplates,\n  getOpenCodeTemplates,\n  getGeminiTemplates,\n} from '../integration/templates.js';\n\n/**\n * Session hook configuration for settings.json (matches operations.ts)\n */\ninterface SessionHook {\n  type: 'command';\n  command: string;\n}\n\ninterface HookEvent {\n  hooks: SessionHook[];\n}\n\ninterface SettingsJson {\n  hooks?: {\n    SessionStart?: HookEvent[];\n    SessionEnd?: HookEvent[];\n  };\n  permissions?: {\n    allow?: string[];\n    deny?: string[];\n  };\n  [key: string]: unknown;\n}\n\n/**\n * Hook definitions for ARE (must match operations.ts)\n */\ninterface HookDefinition {\n  event: 'SessionStart' | 'SessionEnd';\n  filename: string;\n}\n\nconst ARE_HOOKS: HookDefinition[] = [\n  { event: 'SessionStart', filename: 'are-check-update.js' },\n  { event: 'SessionEnd', filename: 'are-session-end.js' },\n];\n\n/**\n * Plugin definitions for ARE OpenCode (must match operations.ts)\n */\nconst ARE_PLUGIN_FILENAMES = [\n  'are-check-update.js',\n  'are-session-end.js',\n];\n\n/**\n * Permissions to remove during uninstall (must match operations.ts)\n */\nconst ARE_PERMISSIONS = [\n  'Bash(npx agents-reverse-engineer@latest init*)',\n  'Bash(npx agents-reverse-engineer@latest discover*)',\n  'Bash(npx agents-reverse-engineer@latest generate*)',\n  'Bash(npx agents-reverse-engineer@latest update*)',\n  'Bash(npx agents-reverse-engineer@latest clean*)',\n];\n\n/**\n * Get templates for a specific runtime\n *\n * @param runtime - Target runtime (claude, opencode, or gemini)\n * @returns Array of template objects for the runtime\n */\nfunction getTemplatesForRuntime(runtime: Exclude<Runtime, 'all'>) {\n  switch (runtime) {\n    case 'claude':\n      return getClaudeTemplates();\n    case 'opencode':\n      return getOpenCodeTemplates();\n    case 'gemini':\n      return getGeminiTemplates();\n  }\n}\n\n/**\n * Uninstall files for one or all runtimes\n *\n * If runtime is 'all', uninstalls from all supported runtimes.\n * Otherwise, uninstalls from the specified runtime only.\n *\n * @param runtime - Target runtime or 'all'\n * @param location - Installation location (global or local)\n * @param dryRun - If true, don't actually delete files\n * @returns Array of uninstallation results (one per runtime)\n */\nexport function uninstallFiles(\n  runtime: Runtime,\n  location: Location,\n  dryRun: boolean = false,\n): InstallerResult[] {\n  if (runtime === 'all') {\n    return getAllRuntimes().map((r) => uninstallFilesForRuntime(r, location, dryRun));\n  }\n  return [uninstallFilesForRuntime(runtime, location, dryRun)];\n}\n\n/**\n * Uninstall files for a specific runtime\n *\n * Removes command templates, hook files, and VERSION file from the installation directory.\n * Also unregisters hooks from settings.json for Claude global installs.\n *\n * @param runtime - Target runtime (claude, opencode, or gemini)\n * @param location - Installation location (global or local)\n * @param dryRun - If true, don't actually delete files\n * @returns Uninstallation result with files deleted\n */\nfunction uninstallFilesForRuntime(\n  runtime: Exclude<Runtime, 'all'>,\n  location: Location,\n  dryRun: boolean,\n): InstallerResult {\n  const basePath = resolveInstallPath(runtime, location);\n  const templates = getTemplatesForRuntime(runtime);\n  const filesCreated: string[] = []; // In uninstall context, this tracks files deleted\n  const filesSkipped: string[] = []; // Files that didn't exist\n  const errors: string[] = [];\n\n  // Remove command templates\n  for (const template of templates) {\n    // Template path is relative (e.g., .claude/commands/are/generate.md)\n    // Extract the part after the runtime directory (e.g., commands/are/generate.md)\n    const relativePath = template.path.split('/').slice(1).join('/');\n    const fullPath = path.join(basePath, relativePath);\n\n    if (existsSync(fullPath)) {\n      if (!dryRun) {\n        try {\n          unlinkSync(fullPath);\n        } catch (err) {\n          errors.push(`Failed to delete ${fullPath}: ${err}`);\n          continue;\n        }\n      }\n      filesCreated.push(fullPath); // Track as \"deleted\"\n    } else {\n      filesSkipped.push(fullPath); // File didn't exist\n    }\n  }\n\n  // Remove hooks/plugins based on runtime\n  let hookUnregistered = false;\n  if (runtime === 'claude' || runtime === 'gemini') {\n    // Remove all ARE hook files\n    for (const hookDef of ARE_HOOKS) {\n      const hookPath = path.join(basePath, 'hooks', hookDef.filename);\n      if (existsSync(hookPath)) {\n        if (!dryRun) {\n          try {\n            unlinkSync(hookPath);\n          } catch (err) {\n            errors.push(`Failed to delete hook ${hookPath}: ${err}`);\n          }\n        }\n        if (!errors.some((e) => e.includes(hookPath))) {\n          filesCreated.push(hookPath);\n        }\n      }\n    }\n\n    // Unregister hooks from settings.json\n    hookUnregistered = unregisterHooks(basePath, runtime, dryRun);\n\n    // Unregister permissions from settings.json (Claude only)\n    if (runtime === 'claude') {\n      unregisterPermissions(basePath, dryRun);\n    }\n  } else if (runtime === 'opencode') {\n    // Remove all ARE plugin files\n    for (const pluginFilename of ARE_PLUGIN_FILENAMES) {\n      const pluginPath = path.join(basePath, 'plugins', pluginFilename);\n      if (existsSync(pluginPath)) {\n        if (!dryRun) {\n          try {\n            unlinkSync(pluginPath);\n          } catch (err) {\n            errors.push(`Failed to delete plugin ${pluginPath}: ${err}`);\n          }\n        }\n        if (!errors.some((e) => e.includes(pluginPath))) {\n          filesCreated.push(pluginPath);\n          hookUnregistered = true;\n        }\n      }\n    }\n  }\n\n  // Remove ARE-VERSION file if exists\n  const versionPath = path.join(basePath, 'ARE-VERSION');\n  if (existsSync(versionPath)) {\n    if (!dryRun) {\n      try {\n        unlinkSync(versionPath);\n      } catch (err) {\n        errors.push(`Failed to delete ARE-VERSION: ${err}`);\n      }\n    }\n    if (!errors.some((e) => e.includes('ARE-VERSION'))) {\n      filesCreated.push(versionPath);\n    }\n  }\n\n  // Try to clean up empty directories\n  if (!dryRun) {\n    if (runtime === 'claude') {\n      // Claude uses skills format: clean up skills/are-* directories\n      const skillsDir = path.join(basePath, 'skills');\n      cleanupAreSkillDirs(skillsDir);\n      cleanupEmptyDirs(skillsDir);\n    } else if (runtime === 'gemini') {\n      // Gemini uses flat commands/ directory for TOML files\n      const commandsDir = path.join(basePath, 'commands');\n      cleanupEmptyDirs(commandsDir);\n      // Clean up legacy files from old installations\n      cleanupLegacyGeminiFiles(commandsDir);\n    } else {\n      // OpenCode uses commands format with flat .md files\n      const commandsDir = path.join(basePath, 'commands');\n      cleanupEmptyDirs(commandsDir);\n    }\n\n    // Clean up hooks/plugins directory if empty\n    if (runtime === 'claude' || runtime === 'gemini') {\n      const hooksDir = path.join(basePath, 'hooks');\n      cleanupEmptyDirs(hooksDir);\n    } else if (runtime === 'opencode') {\n      const pluginsDir = path.join(basePath, 'plugins');\n      cleanupEmptyDirs(pluginsDir);\n    }\n  }\n\n  return {\n    success: errors.length === 0,\n    runtime,\n    location,\n    filesCreated, // Actually files deleted in uninstall context\n    filesSkipped, // Files that didn't exist\n    errors,\n    hookRegistered: hookUnregistered, // Repurpose: true if hook was unregistered\n  };\n}\n\n/**\n * Gemini hook configuration (simpler format)\n */\ninterface GeminiHook {\n  name: string;\n  type: 'command';\n  command: string;\n}\n\ninterface GeminiSettingsJson {\n  hooks?: {\n    SessionStart?: GeminiHook[];\n    SessionEnd?: GeminiHook[];\n  };\n  [key: string]: unknown;\n}\n\n/**\n * Unregister ARE hooks from settings.json\n *\n * Removes all ARE hook entries from SessionStart and SessionEnd arrays.\n * Cleans up empty hooks structures. Handles both old and new hook paths.\n *\n * @param basePath - Base installation path (e.g., ~/.claude or ~/.gemini)\n * @param runtime - Target runtime (claude or gemini)\n * @param dryRun - If true, don't write changes\n * @returns true if any hook was removed, false if none found\n */\nexport function unregisterHooks(\n  basePath: string,\n  runtime: Exclude<Runtime, 'all'>,\n  dryRun: boolean,\n): boolean {\n  if (runtime === 'gemini') {\n    return unregisterGeminiHooks(basePath, dryRun);\n  }\n  return unregisterClaudeHooks(basePath, dryRun);\n}\n\n/**\n * Build hook command patterns for matching (includes legacy paths)\n */\nfunction getHookPatterns(runtimeDir: string): string[] {\n  const patterns: string[] = [];\n  for (const hookDef of ARE_HOOKS) {\n    // Current path format\n    patterns.push(`node ${runtimeDir}/hooks/${hookDef.filename}`);\n    // Legacy path format\n    patterns.push(`node hooks/${hookDef.filename}`);\n  }\n  return patterns;\n}\n\n/**\n * Unregister ARE hooks from Claude Code settings.json\n */\nfunction unregisterClaudeHooks(basePath: string, dryRun: boolean): boolean {\n  const settingsPath = path.join(basePath, 'settings.json');\n\n  // Settings file must exist\n  if (!existsSync(settingsPath)) {\n    return false;\n  }\n\n  // Load settings\n  let settings: SettingsJson;\n  try {\n    const content = readFileSync(settingsPath, 'utf-8');\n    settings = JSON.parse(content) as SettingsJson;\n  } catch {\n    return false;\n  }\n\n  if (!settings.hooks) {\n    return false;\n  }\n\n  const hookPatterns = getHookPatterns('.claude');\n  let removedAny = false;\n\n  // Process both SessionStart and SessionEnd\n  for (const eventType of ['SessionStart', 'SessionEnd'] as const) {\n    if (!settings.hooks[eventType]) {\n      continue;\n    }\n\n    const originalLength = settings.hooks[eventType]!.length;\n\n    settings.hooks[eventType] = settings.hooks[eventType]!.filter(\n      (event) => !event.hooks?.some((h) => hookPatterns.includes(h.command)),\n    );\n\n    if (settings.hooks[eventType]!.length < originalLength) {\n      removedAny = true;\n    }\n\n    // Clean up empty array\n    if (settings.hooks[eventType]!.length === 0) {\n      delete settings.hooks[eventType];\n    }\n  }\n\n  if (!removedAny) {\n    return false;\n  }\n\n  // Clean up empty hooks object\n  if (Object.keys(settings.hooks).length === 0) {\n    delete settings.hooks;\n  }\n\n  // Write updated settings\n  if (!dryRun) {\n    writeFileSync(settingsPath, JSON.stringify(settings, null, 2), 'utf-8');\n  }\n\n  return true;\n}\n\n/**\n * Unregister ARE permissions from Claude Code settings.json\n *\n * Removes all ARE-related bash command permissions from the allow list.\n *\n * @param basePath - Base installation path (e.g., ~/.claude)\n * @param dryRun - If true, don't write changes\n * @returns true if any permissions were removed, false if none found\n */\nexport function unregisterPermissions(basePath: string, dryRun: boolean): boolean {\n  const settingsPath = path.join(basePath, 'settings.json');\n\n  // Settings file must exist\n  if (!existsSync(settingsPath)) {\n    return false;\n  }\n\n  // Load settings\n  let settings: SettingsJson;\n  try {\n    const content = readFileSync(settingsPath, 'utf-8');\n    settings = JSON.parse(content) as SettingsJson;\n  } catch {\n    return false;\n  }\n\n  // Check if permissions.allow exists\n  if (!settings.permissions?.allow) {\n    return false;\n  }\n\n  const originalLength = settings.permissions.allow.length;\n\n  // Remove all ARE permissions\n  settings.permissions.allow = settings.permissions.allow.filter(\n    (perm) => !ARE_PERMISSIONS.includes(perm),\n  );\n\n  // Check if we actually removed something\n  if (settings.permissions.allow.length === originalLength) {\n    return false;\n  }\n\n  // Clean up empty structures\n  if (settings.permissions.allow.length === 0) {\n    delete settings.permissions.allow;\n  }\n\n  if (settings.permissions && Object.keys(settings.permissions).length === 0) {\n    delete settings.permissions;\n  }\n\n  // Write updated settings\n  if (!dryRun) {\n    writeFileSync(settingsPath, JSON.stringify(settings, null, 2), 'utf-8');\n  }\n\n  return true;\n}\n\n/**\n * Unregister ARE hooks from Gemini CLI settings.json\n */\nfunction unregisterGeminiHooks(basePath: string, dryRun: boolean): boolean {\n  const settingsPath = path.join(basePath, 'settings.json');\n\n  // Settings file must exist\n  if (!existsSync(settingsPath)) {\n    return false;\n  }\n\n  // Load settings\n  let settings: GeminiSettingsJson;\n  try {\n    const content = readFileSync(settingsPath, 'utf-8');\n    settings = JSON.parse(content) as GeminiSettingsJson;\n  } catch {\n    return false;\n  }\n\n  if (!settings.hooks) {\n    return false;\n  }\n\n  const hookPatterns = getHookPatterns('.gemini');\n  let removedAny = false;\n\n  // Process both SessionStart and SessionEnd\n  for (const eventType of ['SessionStart', 'SessionEnd'] as const) {\n    if (!settings.hooks[eventType]) {\n      continue;\n    }\n\n    const originalLength = settings.hooks[eventType]!.length;\n\n    settings.hooks[eventType] = settings.hooks[eventType]!.filter(\n      (h) => !hookPatterns.includes(h.command),\n    );\n\n    if (settings.hooks[eventType]!.length < originalLength) {\n      removedAny = true;\n    }\n\n    // Clean up empty array\n    if (settings.hooks[eventType]!.length === 0) {\n      delete settings.hooks[eventType];\n    }\n  }\n\n  if (!removedAny) {\n    return false;\n  }\n\n  // Clean up empty hooks object\n  if (Object.keys(settings.hooks).length === 0) {\n    delete settings.hooks;\n  }\n\n  // Write updated settings\n  if (!dryRun) {\n    writeFileSync(settingsPath, JSON.stringify(settings, null, 2), 'utf-8');\n  }\n\n  return true;\n}\n\n/**\n * Clean up ARE skill directories\n *\n * Removes all empty are-* skill directories from the skills folder.\n *\n * @param skillsDir - Path to the skills directory\n */\nfunction cleanupAreSkillDirs(skillsDir: string): void {\n  try {\n    if (!existsSync(skillsDir)) {\n      return;\n    }\n\n    const entries = readdirSync(skillsDir);\n    for (const entry of entries) {\n      // Only clean up are-* directories\n      if (entry.startsWith('are-')) {\n        const skillDir = path.join(skillsDir, entry);\n        cleanupEmptyDirs(skillDir);\n      }\n    }\n  } catch {\n    // Ignore errors\n  }\n}\n\n/**\n * Clean up empty directories recursively\n *\n * Removes a directory if it's empty, then tries parent directories.\n * Stops when hitting a non-empty directory or the runtime root.\n *\n * @param dirPath - Directory path to check and potentially remove\n */\nfunction cleanupEmptyDirs(dirPath: string): void {\n  try {\n    if (!existsSync(dirPath)) {\n      return;\n    }\n\n    const entries = readdirSync(dirPath);\n    if (entries.length === 0) {\n      rmdirSync(dirPath);\n\n      // Try parent directory (but don't go above runtime root)\n      const parent = path.dirname(dirPath);\n      // Stop at common runtime roots (.claude, .opencode, .gemini)\n      const baseName = path.basename(parent);\n      if (\n        baseName !== '.claude' &&\n        baseName !== '.opencode' &&\n        baseName !== '.gemini' &&\n        baseName !== '.config'\n      ) {\n        cleanupEmptyDirs(parent);\n      }\n    }\n  } catch {\n    // Ignore errors - directory might be in use or we don't have permissions\n  }\n}\n\n/**\n * Clean up legacy Gemini files from old installations\n *\n * Removes:\n * - Old are-*.md files from .gemini/commands/ (pre-TOML format)\n * - Old .toml files from .gemini/commands/are/ (pre-flat structure)\n *\n * @param commandsDir - Path to the commands directory\n */\nfunction cleanupLegacyGeminiFiles(commandsDir: string): void {\n  try {\n    if (!existsSync(commandsDir)) {\n      return;\n    }\n\n    const entries = readdirSync(commandsDir);\n    for (const entry of entries) {\n      // Remove legacy are-*.md files\n      if (entry.startsWith('are-') && entry.endsWith('.md')) {\n        const filePath = path.join(commandsDir, entry);\n        try {\n          unlinkSync(filePath);\n        } catch {\n          // Ignore errors\n        }\n      }\n    }\n\n    // Remove legacy nested are/ directory (old /are:* namespace structure)\n    const areDir = path.join(commandsDir, 'are');\n    if (existsSync(areDir)) {\n      const areEntries = readdirSync(areDir);\n      for (const entry of areEntries) {\n        if (entry.endsWith('.toml')) {\n          try {\n            unlinkSync(path.join(areDir, entry));\n          } catch {\n            // Ignore errors\n          }\n        }\n      }\n      cleanupEmptyDirs(areDir);\n    }\n  } catch {\n    // Ignore errors\n  }\n}\n\n/**\n * Configuration directory name (matches config/loader.ts)\n */\nconst CONFIG_DIR = '.agents-reverse-engineer';\n\n/**\n * Delete the .agents-reverse-engineer configuration folder\n *\n * Only applicable for local installations. Removes the entire folder\n * including configuration files and generation plans.\n *\n * @param location - Installation location (only 'local' triggers deletion)\n * @param dryRun - If true, don't actually delete\n * @returns true if folder was deleted, false if not found or not local\n */\nexport function deleteConfigFolder(location: Location, dryRun: boolean): boolean {\n  // Only delete for local installations\n  if (location !== 'local') {\n    return false;\n  }\n\n  const configPath = path.join(process.cwd(), CONFIG_DIR);\n\n  if (!existsSync(configPath)) {\n    return false;\n  }\n\n  if (!dryRun) {\n    try {\n      rmSync(configPath, { recursive: true, force: true });\n    } catch {\n      return false;\n    }\n  }\n\n  return true;\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**Removes installed ARE command files, hooks, plugins, permissions, and configuration directories with runtime-specific cleanup strategies matching installation mirror logic.**\n\n## Exported Functions\n\n- `uninstallFiles(runtime: Runtime, location: Location, dryRun: boolean = false): InstallerResult[]` — Orchestrates uninstallation for single runtime or all runtimes via `getAllRuntimes()` iteration when `runtime === 'all'`, returning array of results\n- `unregisterHooks(basePath: string, runtime: Exclude<Runtime, 'all'>, dryRun: boolean): boolean` — Removes ARE hook entries from `settings.json` SessionStart/SessionEnd arrays, dispatches to `unregisterClaudeHooks()` or `unregisterGeminiHooks()` based on runtime, returns true if any hook removed\n- `unregisterPermissions(basePath: string, dryRun: boolean): boolean` — Filters `ARE_PERMISSIONS` array from `settings.json` permissions.allow, cleans up empty structures, returns true if changes made\n- `deleteConfigFolder(location: Location, dryRun: boolean): boolean` — Recursively deletes `.agents-reverse-engineer` directory via `rmSync()` with `recursive: true, force: true` only for `location === 'local'`\n\n## Hook and Plugin Definitions\n\n- `ARE_HOOKS: HookDefinition[]` — Array of `{ event: 'SessionStart' | 'SessionEnd', filename: string }` pairs: `are-check-update.js` (SessionStart), `are-session-end.js` (SessionEnd)\n- `ARE_PLUGIN_FILENAMES: string[]` — OpenCode plugin filenames: `['are-check-update.js', 'are-session-end.js']`\n- `ARE_PERMISSIONS: string[]` — Bash command permission patterns: `'Bash(npx agents-reverse-engineer@latest init*)'`, `discover*`, `generate*`, `update*`, `clean*`\n- `CONFIG_DIR = '.agents-reverse-engineer'` — Configuration directory constant matching `config/loader.ts`\n\n## Runtime-Specific Uninstallation\n\n`uninstallFilesForRuntime(runtime: Exclude<Runtime, 'all'>, location: Location, dryRun: boolean): InstallerResult` performs sequential cleanup:\n\n1. **Command template removal**: Iterates `getTemplatesForRuntime()` output, extracts relative paths after runtime directory prefix, calls `unlinkSync()` on full paths constructed via `path.join(basePath, relativePath)`\n2. **Hook/plugin removal**: For Claude/Gemini, deletes files from `hooks/` subdirectory using `ARE_HOOKS` array; for OpenCode, deletes from `plugins/` using `ARE_PLUGIN_FILENAMES`\n3. **Settings.json modification**: Calls `unregisterHooks()` for all runtimes, additionally calls `unregisterPermissions()` for Claude only\n4. **Version file cleanup**: Deletes `ARE-VERSION` from basePath via `unlinkSync()`\n5. **Directory cleanup**: Invokes `cleanupAreSkillDirs()` for Claude skills format, `cleanupEmptyDirs()` for all runtimes, `cleanupLegacyGeminiFiles()` for Gemini\n\nReturns `InstallerResult` with `filesCreated` repurposed to track deleted files, `filesSkipped` for nonexistent paths, `hookRegistered` repurposed to indicate hook unregistration success.\n\n## Settings File Manipulation\n\n`unregisterClaudeHooks(basePath: string, dryRun: boolean): boolean` implements pattern-based filtering:\n\n- Builds hook command patterns via `getHookPatterns('.claude')` including current format `node .claude/hooks/${filename}` and legacy `node hooks/${filename}`\n- Filters `settings.hooks.SessionStart` and `SessionEnd` arrays removing events whose `hooks[].command` matches patterns via `event.hooks?.some(h => hookPatterns.includes(h.command))`\n- Cleans empty arrays via `delete settings.hooks[eventType]`, empty hooks object via `delete settings.hooks`\n- Writes updated JSON via `writeFileSync()` with 2-space indentation\n\n`unregisterGeminiHooks()` follows identical logic but operates on simpler `GeminiHook[]` arrays (direct `command` property without nested `hooks[]` structure) and uses `.gemini` runtime directory in patterns.\n\n`unregisterPermissions()` filters `settings.permissions.allow` array via `!ARE_PERMISSIONS.includes(perm)`, removes empty `allow` array and `permissions` object structures.\n\n## Type Definitions\n\n- `SessionHook` — `{ type: 'command', command: string }` for Claude/Gemini hook registration\n- `HookEvent` — `{ hooks: SessionHook[] }` wrapper\n- `SettingsJson` — Claude settings schema with `hooks?: { SessionStart?: HookEvent[], SessionEnd?: HookEvent[] }`, `permissions?: { allow?: string[], deny?: string[] }`\n- `GeminiHook` — Simpler format `{ name: string, type: 'command', command: string }` without nested hooks array\n- `GeminiSettingsJson` — Gemini settings schema with `hooks?: { SessionStart?: GeminiHook[], SessionEnd?: GeminiHook[] }`\n- `HookDefinition` — `{ event: 'SessionStart' | 'SessionEnd', filename: string }`\n\n## Directory Cleanup Strategies\n\n`cleanupAreSkillDirs(skillsDir: string)` iterates `readdirSync()` output filtering entries via `entry.startsWith('are-')`, calls `cleanupEmptyDirs()` on each matching skill directory.\n\n`cleanupEmptyDirs(dirPath: string)` implements recursive bottom-up removal:\n\n- Checks `readdirSync().length === 0` predicate\n- Calls `rmdirSync()` on empty directory\n- Recurses to parent via `path.dirname()` with terminal condition checking `baseName` against runtime roots: `.claude`, `.opencode`, `.gemini`, `.config`\n\n`cleanupLegacyGeminiFiles(commandsDir: string)` removes obsolete formats:\n\n- Deletes `are-*.md` files (pre-TOML Markdown format) via `entry.startsWith('are-') && entry.endsWith('.md')` predicate\n- Recursively deletes `.toml` files from nested `commands/are/` directory (old namespace structure), then removes empty `are/` directory\n\n## Template Resolution\n\n`getTemplatesForRuntime(runtime: Exclude<Runtime, 'all'>)` dispatches to integration layer functions:\n\n- `'claude'` → `getClaudeTemplates()` from `src/integration/templates.ts`\n- `'opencode'` → `getOpenCodeTemplates()`\n- `'gemini'` → `getGeminiTemplates()`\n\nEach returns array of template objects with `.path` property in format `.<runtime>/<subdir>/<filename>` requiring `.split('/').slice(1).join('/')` to extract relative installation path.\n\n## Error Handling\n\nAll file system operations wrapped in try-catch blocks with errors collected into `InstallerResult.errors` array. Pattern: `catch (err) { errors.push(\\`Failed to delete ${path}: ${err}\\`) }`. Missing files tracked separately in `filesSkipped` array via `existsSync()` checks before deletion attempts. Directory cleanup operations silently ignore errors via empty catch blocks to handle permission issues and in-use directories.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 1816,
      "cacheReadTokens": 17367,
      "cacheCreationTokens": 14625,
      "latencyMs": 43341,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/installer/uninstall.ts",
          "sizeBytes": 17689
        }
      ]
    },
    {
      "timestamp": "2026-02-09T17:45:29.121Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/quality/types.ts\n\n```typescript\n/**\n * Shared types for quality analysis: inconsistency detection and density measurement.\n */\n\n/** Severity level for detected inconsistencies. */\nexport type InconsistencySeverity = 'info' | 'warning' | 'error';\n\n/**\n * Inconsistency between .sum documentation and source code.\n * Detected by comparing exported symbols against .sum content.\n */\nexport interface CodeDocInconsistency {\n  type: 'code-vs-doc';\n  severity: InconsistencySeverity;\n  /** Path to the source file */\n  filePath: string;\n  /** Path to the corresponding .sum file */\n  sumPath: string;\n  description: string;\n  details: {\n    /** Symbols exported in source but not mentioned in .sum */\n    missingFromDoc: string[];\n    /** Symbols mentioned in .sum but not found in source */\n    missingFromCode: string[];\n    /** Purpose statement that contradicts observable behavior */\n    purposeMismatch?: string;\n  };\n}\n\n/**\n * Inconsistency across multiple source files.\n * Detected by comparing patterns and exports across files.\n */\nexport interface CodeCodeInconsistency {\n  type: 'code-vs-code';\n  severity: InconsistencySeverity;\n  /** Paths to the conflicting files */\n  files: string[];\n  description: string;\n  /** Pattern that was detected (e.g. 'duplicate-export') */\n  pattern: string;\n}\n\n/**\n * Path reference in generated documentation that doesn't resolve to a real file/directory.\n */\nexport interface PhantomPathInconsistency {\n  type: 'phantom-path';\n  severity: InconsistencySeverity;\n  /** Path to the AGENTS.md file containing the phantom reference */\n  agentsMdPath: string;\n  description: string;\n  details: {\n    /** The phantom path as written in the document */\n    referencedPath: string;\n    /** What it was resolved against (project root or AGENTS.md location) */\n    resolvedTo: string;\n    /** The line of text containing the phantom reference */\n    context: string;\n  };\n}\n\n/** Union of all inconsistency types. */\nexport type Inconsistency = CodeDocInconsistency | CodeCodeInconsistency | PhantomPathInconsistency;\n\n/** Structured report produced by inconsistency analysis. */\nexport interface InconsistencyReport {\n  /** Run metadata */\n  metadata: {\n    timestamp: string;\n    projectRoot: string;\n    filesChecked: number;\n    durationMs: number;\n  };\n  /** All detected inconsistencies */\n  issues: Inconsistency[];\n  /** Summary counts */\n  summary: {\n    total: number;\n    codeVsDoc: number;\n    codeVsCode: number;\n    phantomPaths: number;\n    errors: number;\n    warnings: number;\n    info: number;\n  };\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**types.ts defines the discriminated union of quality inconsistency types and structured report format for code-documentation validation.**\n\n## Exported Types\n\n**InconsistencySeverity** — String literal union `'info' | 'warning' | 'error'` classifying issue priority.\n\n**CodeDocInconsistency** — Inconsistency between `.sum` documentation and source code exports detected via symbol extraction and substring search. Fields: `type: 'code-vs-doc'`, `severity`, `filePath` (source), `sumPath` (.sum file), `description`, `details.missingFromDoc` (exported symbols absent from .sum), `details.missingFromCode` (symbols in .sum not found in source), `details.purposeMismatch` (optional purpose statement contradiction).\n\n**CodeCodeInconsistency** — Cross-file inconsistency detected by pattern analysis across multiple source files. Fields: `type: 'code-vs-code'`, `severity`, `files` (conflicting paths), `description`, `pattern` (e.g., `'duplicate-export'` for symbols appearing in multiple files).\n\n**PhantomPathInconsistency** — Unresolvable path reference found in `AGENTS.md` via regex extraction and `existsSync()` validation. Fields: `type: 'phantom-path'`, `severity`, `agentsMdPath` (containing document), `description`, `details.referencedPath` (original string from markdown), `details.resolvedTo` (attempted resolution target), `details.context` (surrounding text line).\n\n**Inconsistency** — Discriminated union `CodeDocInconsistency | CodeCodeInconsistency | PhantomPathInconsistency` with `type` discriminator enabling exhaustive pattern matching.\n\n**InconsistencyReport** — Structured quality validation output. Fields: `metadata` containing `timestamp`, `projectRoot`, `filesChecked`, `durationMs`; `issues[]` array of `Inconsistency` instances; `summary` object with `total`, `codeVsDoc`, `codeVsCode`, `phantomPaths`, `errors`, `warnings`, `info` counts.\n\n## Integration Context\n\nConsumed by `src/quality/inconsistency/code-vs-doc.ts` (export extraction + substring validation), `src/quality/inconsistency/code-vs-code.ts` (duplicate symbol detection), `src/quality/phantom-paths/validator.ts` (markdown path resolution), and `src/quality/inconsistency/reporter.ts` (aggregation and summary computation). The `type` discriminator enables type-safe narrowing in switch statements for severity assignment and detail extraction.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 617,
      "cacheReadTokens": 17367,
      "cacheCreationTokens": 9818,
      "latencyMs": 20413,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/quality/types.ts",
          "sizeBytes": 2518
        }
      ]
    },
    {
      "timestamp": "2026-02-09T17:45:21.028Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/orchestration/trace.ts\n\n```typescript\n/**\n * Concurrency tracing system for debugging task/subprocess lifecycle.\n *\n * Produces append-only NDJSON trace files in `.agents-reverse-engineer/traces/`\n * when the `--trace` CLI flag is set. When disabled, the {@link NullTraceWriter}\n * ensures zero overhead -- every call site can unconditionally call `emit()`\n * without branching.\n *\n * Uses promise-chain serialization (same pattern as {@link PlanTracker}) to\n * handle concurrent writes from multiple pool workers safely.\n *\n * @module\n */\n\nimport { open, mkdir, readdir, unlink } from 'node:fs/promises';\nimport type { FileHandle } from 'node:fs/promises';\nimport * as path from 'node:path';\n\n// ---------------------------------------------------------------------------\n// Trace directory\n// ---------------------------------------------------------------------------\n\n/** Directory for trace files (relative to project root) */\nconst TRACES_DIR = '.agents-reverse-engineer/traces';\n\n// ---------------------------------------------------------------------------\n// Trace event types\n// ---------------------------------------------------------------------------\n\n/** Common fields present on every trace event */\ninterface TraceEventBase {\n  /** Monotonically increasing sequence number (per-run) */\n  seq: number;\n  /** ISO 8601 timestamp at event creation time */\n  ts: string;\n  /** process.pid of the Node.js parent process */\n  pid: number;\n  /** High-resolution elapsed time since run start (ms, fractional) */\n  elapsedMs: number;\n}\n\n/** Emitted when a phase begins execution */\ninterface PhaseStartEvent extends TraceEventBase {\n  type: 'phase:start';\n  phase: string;\n  taskCount: number;\n  concurrency: number;\n}\n\n/** Emitted when a phase completes */\ninterface PhaseEndEvent extends TraceEventBase {\n  type: 'phase:end';\n  phase: string;\n  durationMs: number;\n  tasksCompleted: number;\n  tasksFailed: number;\n}\n\n/** Emitted when a pool worker starts pulling from the shared iterator */\ninterface WorkerStartEvent extends TraceEventBase {\n  type: 'worker:start';\n  workerId: number;\n  phase: string;\n}\n\n/** Emitted when a pool worker exhausts the iterator or is aborted */\ninterface WorkerEndEvent extends TraceEventBase {\n  type: 'worker:end';\n  workerId: number;\n  phase: string;\n  tasksExecuted: number;\n}\n\n/** Emitted when a worker picks up a task from the iterator */\ninterface TaskPickupEvent extends TraceEventBase {\n  type: 'task:pickup';\n  workerId: number;\n  taskIndex: number;\n  taskLabel: string;\n  activeTasks: number;\n}\n\n/** Emitted when a task completes (success or failure) */\ninterface TaskDoneEvent extends TraceEventBase {\n  type: 'task:done';\n  workerId: number;\n  taskIndex: number;\n  taskLabel: string;\n  durationMs: number;\n  success: boolean;\n  error?: string;\n  activeTasks: number;\n}\n\n/** Emitted when a child process is spawned */\ninterface SubprocessSpawnEvent extends TraceEventBase {\n  type: 'subprocess:spawn';\n  childPid: number;\n  command: string;\n  taskLabel: string;\n}\n\n/** Emitted when a child process exits */\ninterface SubprocessExitEvent extends TraceEventBase {\n  type: 'subprocess:exit';\n  childPid: number;\n  command: string;\n  taskLabel: string;\n  exitCode: number;\n  signal: string | null;\n  durationMs: number;\n  timedOut: boolean;\n}\n\n/** Emitted before a retry attempt */\ninterface RetryEvent extends TraceEventBase {\n  type: 'retry';\n  attempt: number;\n  taskLabel: string;\n  errorCode: string;\n}\n\n/** Emitted when a non-pool task starts execution */\ninterface TaskStartEvent extends TraceEventBase {\n  type: 'task:start';\n  taskLabel: string;\n  phase: string;\n}\n\n/** Emitted when file discovery begins */\ninterface DiscoveryStartEvent extends TraceEventBase {\n  type: 'discovery:start';\n  targetPath: string;\n}\n\n/** Emitted when file discovery completes */\ninterface DiscoveryEndEvent extends TraceEventBase {\n  type: 'discovery:end';\n  filesIncluded: number;\n  filesExcluded: number;\n  durationMs: number;\n}\n\n/** Emitted when a filter is applied during discovery */\ninterface FilterAppliedEvent extends TraceEventBase {\n  type: 'filter:applied';\n  filterName: string;\n  filesMatched: number;\n  filesRejected: number;\n}\n\n/** Emitted when a generation/update plan is created */\ninterface PlanCreatedEvent extends TraceEventBase {\n  type: 'plan:created';\n  planType: 'generate' | 'update';\n  fileCount: number;\n  taskCount: number;\n}\n\n/** Emitted when configuration is loaded */\ninterface ConfigLoadedEvent extends TraceEventBase {\n  type: 'config:loaded';\n  configPath: string;\n  model: string;\n  concurrency: number;\n}\n\n/** Discriminated union of all trace event types */\nexport type TraceEvent =\n  | PhaseStartEvent\n  | PhaseEndEvent\n  | WorkerStartEvent\n  | WorkerEndEvent\n  | TaskPickupEvent\n  | TaskDoneEvent\n  | TaskStartEvent\n  | SubprocessSpawnEvent\n  | SubprocessExitEvent\n  | RetryEvent\n  | DiscoveryStartEvent\n  | DiscoveryEndEvent\n  | FilterAppliedEvent\n  | PlanCreatedEvent\n  | ConfigLoadedEvent;\n\n/** Keys auto-populated by the trace writer */\ntype BaseKeys = 'seq' | 'ts' | 'pid' | 'elapsedMs';\n\n/** Distributive Omit that works correctly across union members */\ntype DistributiveOmit<T, K extends PropertyKey> = T extends unknown ? Omit<T, K> : never;\n\n/** Event payload without auto-populated base fields */\nexport type TraceEventPayload = DistributiveOmit<TraceEvent, BaseKeys>;\n\n// ---------------------------------------------------------------------------\n// ITraceWriter interface\n// ---------------------------------------------------------------------------\n\n/**\n * Public interface for trace event emission.\n *\n * All consumers depend only on this interface, allowing the no-op\n * implementation to be swapped in when tracing is disabled.\n */\nexport interface ITraceWriter {\n  /** Emit a trace event. Base fields (seq, ts, pid, elapsedMs) are auto-populated. */\n  emit(event: TraceEventPayload): void;\n  /** Flush all pending writes and close the file handle. */\n  finalize(): Promise<void>;\n  /** Absolute path to the trace file (empty string for NullTraceWriter). */\n  readonly filePath: string;\n}\n\n// ---------------------------------------------------------------------------\n// NullTraceWriter (no-op)\n// ---------------------------------------------------------------------------\n\n/**\n * No-op trace writer. Returned when `--trace` is not set.\n * All methods are empty -- zero overhead at call sites.\n */\nclass NullTraceWriter implements ITraceWriter {\n  readonly filePath = '';\n  emit(): void { /* no-op */ }\n  async finalize(): Promise<void> { /* no-op */ }\n}\n\n// ---------------------------------------------------------------------------\n// TraceWriter (real implementation)\n// ---------------------------------------------------------------------------\n\n/**\n * Append-only NDJSON trace writer.\n *\n * Each `emit()` call serializes the event to a single JSON line and\n * enqueues a file append via a promise chain. This guarantees correct\n * ordering even when multiple pool workers emit concurrently.\n */\nclass TraceWriter implements ITraceWriter {\n  readonly filePath: string;\n\n  private seq = 0;\n  private readonly nodePid = process.pid;\n  private readonly startHr = process.hrtime.bigint();\n  private writeQueue: Promise<void> = Promise.resolve();\n  private fd: FileHandle | null = null;\n\n  constructor(filePath: string) {\n    this.filePath = filePath;\n  }\n\n  emit(partial: TraceEventPayload): void {\n    const event = {\n      ...partial,\n      seq: this.seq++,\n      ts: new Date().toISOString(),\n      pid: this.nodePid,\n      elapsedMs: Number(process.hrtime.bigint() - this.startHr) / 1_000_000,\n    };\n    const line = JSON.stringify(event) + '\\n';\n\n    this.writeQueue = this.writeQueue\n      .then(async () => {\n        if (!this.fd) {\n          await mkdir(path.dirname(this.filePath), { recursive: true });\n          this.fd = await open(this.filePath, 'a');\n        }\n        await this.fd.write(line);\n      })\n      .catch(() => { /* non-critical -- trace loss is acceptable */ });\n  }\n\n  async finalize(): Promise<void> {\n    await this.writeQueue;\n    if (this.fd) {\n      await this.fd.close();\n      this.fd = null;\n    }\n  }\n}\n\n// ---------------------------------------------------------------------------\n// Factory\n// ---------------------------------------------------------------------------\n\n/**\n * Create a trace writer.\n *\n * Returns a {@link NullTraceWriter} when `enabled` is false (zero overhead).\n * Otherwise returns a {@link TraceWriter} that appends NDJSON to\n * `.agents-reverse-engineer/traces/trace-{timestamp}.ndjson`.\n *\n * @param projectRoot - Absolute path to the project root directory\n * @param enabled - Whether tracing is enabled (typically from `--trace` flag)\n * @returns A trace writer instance\n */\nexport function createTraceWriter(projectRoot: string, enabled: boolean): ITraceWriter {\n  if (!enabled) return new NullTraceWriter();\n  const safeTimestamp = new Date().toISOString().replace(/[:.]/g, '-');\n  const filePath = path.join(projectRoot, TRACES_DIR, `trace-${safeTimestamp}.ndjson`);\n  return new TraceWriter(filePath);\n}\n\n// ---------------------------------------------------------------------------\n// Cleanup\n// ---------------------------------------------------------------------------\n\n/**\n * Remove old trace files, keeping only the most recent ones.\n *\n * Mirrors the pattern in `src/ai/telemetry/cleanup.ts`.\n *\n * @param projectRoot - Absolute path to the project root directory\n * @param keepCount - Number of most recent trace files to retain (default: 500)\n * @returns Number of files deleted\n */\nexport async function cleanupOldTraces(projectRoot: string, keepCount: number = 500): Promise<number> {\n  const tracesDir = path.join(projectRoot, TRACES_DIR);\n\n  let entries: string[];\n  try {\n    const allEntries = await readdir(tracesDir);\n    entries = allEntries.filter(\n      (name) => name.startsWith('trace-') && name.endsWith('.ndjson'),\n    );\n  } catch (error) {\n    if ((error as NodeJS.ErrnoException).code === 'ENOENT') {\n      return 0;\n    }\n    throw error;\n  }\n\n  // Sort newest first (ISO timestamps sort lexicographically)\n  entries.sort();\n  entries.reverse();\n\n  const toDelete = entries.slice(keepCount);\n\n  for (const filename of toDelete) {\n    await unlink(path.join(tracesDir, filename));\n  }\n\n  return toDelete.length;\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**trace.ts implements NDJSON concurrency tracing for task/subprocess lifecycle debugging via promise-chain serialization, emitting events to `.agents-reverse-engineer/traces/` with automatic base field population and zero-overhead no-op mode.**\n\n## Exported Interfaces\n\n`ITraceWriter` defines the public tracing API with three members: `emit(event: TraceEventPayload)` appends a trace event with auto-populated `seq`, `ts`, `pid`, `elapsedMs` fields; `finalize(): Promise<void>` flushes pending writes and closes the file handle; `readonly filePath: string` returns the absolute path to the trace file (empty string for `NullTraceWriter`).\n\n`TraceEvent` is a discriminated union of 15 event types: `PhaseStartEvent` (`type: 'phase:start'`, `phase: string`, `taskCount: number`, `concurrency: number`), `PhaseEndEvent` (`type: 'phase:end'`, `phase: string`, `durationMs: number`, `tasksCompleted: number`, `tasksFailed: number`), `WorkerStartEvent` (`type: 'worker:start'`, `workerId: number`, `phase: string`), `WorkerEndEvent` (`type: 'worker:end'`, `workerId: number`, `phase: string`, `tasksExecuted: number`), `TaskPickupEvent` (`type: 'task:pickup'`, `workerId: number`, `taskIndex: number`, `taskLabel: string`, `activeTasks: number`), `TaskDoneEvent` (`type: 'task:done'`, `workerId: number`, `taskIndex: number`, `taskLabel: string`, `durationMs: number`, `success: boolean`, `error?: string`, `activeTasks: number`), `TaskStartEvent` (`type: 'task:start'`, `taskLabel: string`, `phase: string`), `SubprocessSpawnEvent` (`type: 'subprocess:spawn'`, `childPid: number`, `command: string`, `taskLabel: string`), `SubprocessExitEvent` (`type: 'subprocess:exit'`, `childPid: number`, `command: string`, `taskLabel: string`, `exitCode: number`, `signal: string | null`, `durationMs: number`, `timedOut: boolean`), `RetryEvent` (`type: 'retry'`, `attempt: number`, `taskLabel: string`, `errorCode: string`), `DiscoveryStartEvent` (`type: 'discovery:start'`, `targetPath: string`), `DiscoveryEndEvent` (`type: 'discovery:end'`, `filesIncluded: number`, `filesExcluded: number`, `durationMs: number`), `FilterAppliedEvent` (`type: 'filter:applied'`, `filterName: string`, `filesMatched: number`, `filesRejected: number`), `PlanCreatedEvent` (`type: 'plan:created'`, `planType: 'generate' | 'update'`, `fileCount: number`, `taskCount: number`), `ConfigLoadedEvent` (`type: 'config:loaded'`, `configPath: string`, `model: string`, `concurrency: number`).\n\n`TraceEventPayload` is a distributive omit type removing auto-populated base keys (`seq`, `ts`, `pid`, `elapsedMs`) from `TraceEvent`, computed via `DistributiveOmit<TraceEvent, BaseKeys>` where `DistributiveOmit<T, K extends PropertyKey> = T extends unknown ? Omit<T, K> : never` ensures correct union distribution.\n\n`TraceEventBase` defines common fields present on all events: `seq: number` (monotonically increasing sequence number per-run), `ts: string` (ISO 8601 timestamp at event creation time), `pid: number` (Node.js parent process PID), `elapsedMs: number` (high-resolution elapsed time since run start in fractional milliseconds).\n\n## Factory Function\n\n`createTraceWriter(projectRoot: string, enabled: boolean): ITraceWriter` returns `NullTraceWriter` when `enabled` is false (zero overhead), otherwise returns `TraceWriter` that appends NDJSON to `.agents-reverse-engineer/traces/trace-{timestamp}.ndjson` where timestamp uses `new Date().toISOString().replace(/[:.]/g, '-')` for filesystem-safe naming.\n\n## Implementation Classes\n\n`NullTraceWriter` implements `ITraceWriter` with no-op methods: `filePath = ''`, `emit()` is empty, `finalize()` returns resolved promise. Used when `--trace` flag is absent to eliminate runtime overhead.\n\n`TraceWriter` implements `ITraceWriter` with promise-chain serialization mirroring `PlanTracker` pattern from `src/orchestration/plan-tracker.ts`. Private fields: `seq: number` (starts at 0), `nodePid` (captured from `process.pid`), `startHr` (captured via `process.hrtime.bigint()`), `writeQueue: Promise<void>` (serialization chain initialized to `Promise.resolve()`), `fd: FileHandle | null` (opened lazily on first `emit()` call). Constructor accepts `filePath: string` as absolute path parameter.\n\n`TraceWriter.emit(partial: TraceEventPayload)` constructs full event by spreading `partial` and adding `seq: this.seq++`, `ts: new Date().toISOString()`, `pid: this.nodePid`, `elapsedMs: Number(process.hrtime.bigint() - this.startHr) / 1_000_000`. Serializes to JSON line via `JSON.stringify(event) + '\\n'`. Chains write operation onto `this.writeQueue` via `.then()` that lazily opens file handle using `open(this.filePath, 'a')` after `mkdir(path.dirname(this.filePath), { recursive: true })`, then calls `this.fd.write(line)`. Catches errors silently since trace loss is acceptable for debugging infrastructure.\n\n`TraceWriter.finalize()` awaits `this.writeQueue` to flush all pending writes, then closes `this.fd` via `this.fd.close()` if open and sets `this.fd = null`.\n\n## Cleanup Utility\n\n`cleanupOldTraces(projectRoot: string, keepCount: number = 500): Promise<number>` removes old trace files from `.agents-reverse-engineer/traces/`, keeping only the most recent `keepCount` files. Mirrors pattern from `src/ai/telemetry/cleanup.ts`. Reads directory via `readdir()`, filters for entries matching `startsWith('trace-')` and `endsWith('.ndjson')`, sorts lexicographically (ISO timestamps), reverses to newest-first ordering, slices entries beyond `keepCount`, deletes via `unlink()`, returns deletion count. Returns 0 on `ENOENT` (directory not found), re-throws other errors.\n\n## Constants\n\n`TRACES_DIR = '.agents-reverse-engineer/traces'` defines relative directory path for trace file storage.\n\n## Integration Pattern\n\nThreading occurs via `CommandRunOptions.tracer` parameter passed through pool workers (`src/orchestration/pool.ts`), AI service calls (`src/ai/service.ts`), and phase runners (`src/orchestration/runner.ts`). Call sites unconditionally invoke `emit()` without branching, relying on `NullTraceWriter` to optimize away overhead when tracing is disabled.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 1788,
      "cacheReadTokens": 17367,
      "cacheCreationTokens": 11969,
      "latencyMs": 34759,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/orchestration/trace.ts",
          "sizeBytes": 10277
        }
      ]
    },
    {
      "timestamp": "2026-02-09T17:45:31.902Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/specify/prompts.ts\n\n```typescript\nimport type { AgentsDocs } from '../generation/collector.js';\n\n/**\n * Prompt pair for spec generation: system instructions and user content.\n */\nexport interface SpecPrompt {\n  system: string;\n  user: string;\n}\n\n/**\n * System prompt for AI-driven specification synthesis.\n *\n * Enforces conceptual grouping by concern, prohibits folder-mirroring\n * and exact file path prescription, and targets AI agent consumption.\n */\nexport const SPEC_SYSTEM_PROMPT = `You produce software specifications from documentation.\n\nTASK:\nGenerate a comprehensive specification document from the provided AGENTS.md content. The specification must contain enough detail for an AI agent to reconstruct the entire project from scratch without seeing the original source code.\n\nAUDIENCE: AI agents (LLMs) — use structured, precise, instruction-oriented language. Every statement should be actionable.\n\nORGANIZATION (MANDATORY):\nGroup content by CONCERN, not by directory structure. Use these conceptual sections in order:\n\n1. Project Overview — purpose, core value proposition, problem solved, technology stack with versions\n2. Architecture — system design, module boundaries, data flow patterns, key design decisions and their rationale\n3. Public API Surface — all exported interfaces, function signatures with full parameter and return types, type definitions, error contracts\n4. Data Structures & State — key types, schemas, config objects, state management patterns, serialization formats\n5. Configuration — all config options with types, defaults, validation rules, environment variables\n6. Dependencies — each external dependency with exact version and rationale for inclusion\n7. Behavioral Contracts — Split into two subsections:\n   a. Runtime Behavior: error handling strategies (exact error types/codes and when thrown), retry logic (formulas, delay values), concurrency model, lifecycle hooks, resource management\n   b. Implementation Contracts: every regex pattern used for parsing/validation/extraction (verbatim in backticks), every format string and output template (exact structure with examples), every magic constant and sentinel value with its meaning, every environment variable with expected values, every file format specification (YAML schemas, NDJSON structures). These are reproduction-critical — an AI agent needs them to rebuild the system with identical observable behavior.\n8. Test Contracts — what each module's tests should verify: scenarios, edge cases, expected behaviors, error conditions\n9. Build Plan — phased implementation sequence: what to build first and why, dependency order between modules, incremental milestones\n10. Prompt Templates & System Instructions — every AI prompt template, system prompt, and user prompt template used by the system. Reproduce the FULL text verbatim from annex files or AGENTS.md content. Organize by pipeline phase or functional area. Include placeholder syntax exactly as defined (e.g., {{FILE_PATH}}). These are reproduction-critical — without them, a rebuilder cannot produce functionally equivalent AI output.\n11. IDE Integration & Installer — command templates per platform, platform configuration objects (path prefixes, filename conventions, frontmatter formats), installer permission lists, hook definitions and their activation status. Reproduce template content verbatim from annex files or AGENTS.md content.\n\nRULES:\n- Describe MODULE BOUNDARIES and their interfaces — not file paths or directory layouts\n- Use exact function, type, and constant names as they appear in the documentation\n- Include FULL type signatures for all public APIs (parameters, return types, generics)\n- Do NOT prescribe exact filenames or file paths — describe what each module does and exports\n- Do NOT mirror the project's folder structure in your section organization\n- Do NOT use directory names as section headings\n- Include version numbers for ALL external dependencies\n- The Build Plan MUST list implementation phases with explicit dependency ordering\n- Each Build Plan phase must state what it depends on and what it enables\n- Behavioral Contracts must specify exact error types/codes and when they are thrown\n- Behavioral Contracts MUST include verbatim regex patterns, format strings, and magic constants from the source documents — do NOT paraphrase regex patterns into prose descriptions\n- When multiple modules reference the same constant or pattern, consolidate into a single definition with cross-references to the modules that use it\n\nREPRODUCTION-CRITICAL CONTENT (MANDATORY):\nThe source documents may include annex files containing full verbatim source code\nfor reproduction-critical modules (prompt templates, configuration defaults, IDE\ntemplates, installer configs). These are provided as fenced code blocks.\n\nFor ALL reproduction-critical content:\n- Reproduce the FULL content verbatim in the appropriate spec section (10 or 11)\n- Do NOT summarize, paraphrase, abbreviate, or \"improve\" the text\n- Use fenced code blocks to preserve formatting\n- If content contains placeholder syntax ({{TOKEN}}), preserve it exactly\n- If no annex files or reproduction-critical sections are provided, omit sections 10-11\n\nOUTPUT: Raw markdown. No preamble. No meta-commentary. No \"Here is...\" or \"I've generated...\" prefix.`;\n\n/**\n * Build the system + user prompt pair for spec generation.\n *\n * Injects all collected AGENTS.md content with section delimiters.\n *\n * @param docs - Collected AGENTS.md documents from collectAgentsDocs()\n * @returns SpecPrompt with system and user prompt strings\n */\nexport function buildSpecPrompt(docs: AgentsDocs, annexFiles?: AgentsDocs): SpecPrompt {\n  const agentsSections = docs.map(\n    (doc) => `### ${doc.relativePath}\\n\\n${doc.content}`,\n  );\n\n  const userSections: string[] = [\n    'Generate a comprehensive project specification from the following documentation.',\n    '',\n    `## AGENTS.md Files (${docs.length} directories)`,\n    '',\n    ...agentsSections,\n  ];\n\n  if (annexFiles && annexFiles.length > 0) {\n    const annexSections = annexFiles.map(\n      (doc) => `### ${doc.relativePath}\\n\\n${doc.content}`,\n    );\n    userSections.push(\n      '',\n      `## Annex Files (${annexFiles.length} reproduction-critical source files)`,\n      '',\n      ...annexSections,\n    );\n  }\n\n  userSections.push(\n    '',\n    '## Output Requirements',\n    '',\n    'The specification MUST include these sections in order:',\n    '1. Project Overview (purpose, value, tech stack)',\n    '2. Architecture (module boundaries, data flow, design decisions)',\n    '3. Public API Surface (all exported interfaces, full type signatures)',\n    '4. Data Structures & State (types, schemas, config objects)',\n    '5. Configuration (options, types, defaults, validation)',\n    '6. Dependencies (each with version and rationale)',\n    '7. Behavioral Contracts (error handling, concurrency, lifecycle, PLUS verbatim regex patterns, format specs, magic constants, templates)',\n    '8. Test Contracts (per-module test scenarios and edge cases)',\n    '9. Build Plan (phased implementation order with dependencies)',\n    '10. Prompt Templates & System Instructions (FULL verbatim text from annex content)',\n    '11. IDE Integration & Installer (command templates, platform configs, permission lists — all verbatim from annex content)',\n    '',\n    'Sections 10 and 11 MUST reproduce annex content verbatim.',\n    'Do NOT summarize prompt templates or IDE templates into prose descriptions.',\n    '',\n    'Output ONLY the markdown content. No preamble.',\n  );\n\n  return {\n    system: SPEC_SYSTEM_PROMPT,\n    user: userSections.join('\\n'),\n  };\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**Defines prompt engineering templates for AI-driven project specification synthesis via `buildSpecPrompt()` function and `SPEC_SYSTEM_PROMPT` constant.**\n\n## Exported Types\n\n`SpecPrompt` interface contains `system: string` and `user: string` properties representing the system instructions and user content for AI spec generation.\n\n## Exported Constants\n\n`SPEC_SYSTEM_PROMPT` defines the system prompt enforcing 11-section conceptual organization: (1) Project Overview, (2) Architecture, (3) Public API Surface, (4) Data Structures & State, (5) Configuration, (6) Dependencies, (7) Behavioral Contracts (split into Runtime Behavior and Implementation Contracts), (8) Test Contracts, (9) Build Plan, (10) Prompt Templates & System Instructions, (11) IDE Integration & Installer. Prohibits folder-mirroring and exact file path prescription. Mandates verbatim reproduction of reproduction-critical content from annex files: regex patterns, format strings, magic constants, prompt templates, IDE templates, environment variable names.\n\n## Exported Functions\n\n`buildSpecPrompt(docs: AgentsDocs, annexFiles?: AgentsDocs): SpecPrompt` constructs the prompt pair by concatenating all AGENTS.md content with section delimiters (`### ${doc.relativePath}\\n\\n${doc.content}`), appending optional annex file content (reproduction-critical source files), and injecting output requirements listing the mandatory 11-section structure. Returns `SpecPrompt` with `system` set to `SPEC_SYSTEM_PROMPT` and `user` containing structured markdown with section headers for AGENTS.md files, annex files (if provided), and explicit output requirements.\n\n## Integration Context\n\nConsumed by `src/specify/index.ts` which calls `buildSpecPrompt()` with output from `collectAgentsDocs()` (recursive AGENTS.md traversal) and optionally `collectAnnexFiles()` (reproduction-critical source extraction). The resulting `SpecPrompt` is passed to `AIService.call()` for synthesis into `specs/SPEC.md` or split multi-file specs.\n\n## Behavioral Contracts\n\nSection 7 of `SPEC_SYSTEM_PROMPT` mandates two subsections: (a) Runtime Behavior — error types/codes, retry formulas/delays, concurrency model, lifecycle hooks; (b) Implementation Contracts — verbatim regex patterns in backticks, format strings with examples, magic constants with meanings, environment variable names with expected values, file format specifications (YAML schemas, NDJSON structures). Explicitly forbids paraphrasing regex patterns into prose descriptions.\n\nSection 10 (Prompt Templates & System Instructions) requires FULL verbatim text reproduction from annex files or AGENTS.md content, organized by pipeline phase or functional area, preserving placeholder syntax (e.g., `{{FILE_PATH}}`).\n\nSection 11 (IDE Integration & Installer) requires verbatim reproduction of command templates per platform, platform configuration objects (path prefixes, filename conventions, frontmatter formats), installer permission lists, hook definitions.\n\n## Output Format Rules\n\n`SPEC_SYSTEM_PROMPT` enforces \"Raw markdown. No preamble. No meta-commentary. No 'Here is...' or 'I've generated...' prefix.\" `buildSpecPrompt()` user prompt concludes with \"Output ONLY the markdown content. No preamble.\" and \"Sections 10 and 11 MUST reproduce annex content verbatim. Do NOT summarize prompt templates or IDE templates into prose descriptions.\"",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 817,
      "cacheReadTokens": 17367,
      "cacheCreationTokens": 10864,
      "latencyMs": 24046,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/specify/prompts.ts",
          "sizeBytes": 7632
        }
      ]
    },
    {
      "timestamp": "2026-02-09T17:45:25.083Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/quality/index.ts\n\n```typescript\n/**\n * Public API for the quality analysis module.\n *\n * Re-exports all quality analysis types and functions from the\n * inconsistency detection and density validation submodules.\n *\n * @module\n */\n\n// ---------------------------------------------------------------------------\n// Shared types\n// ---------------------------------------------------------------------------\n\nexport type {\n  InconsistencySeverity,\n  CodeDocInconsistency,\n  CodeCodeInconsistency,\n  PhantomPathInconsistency,\n  Inconsistency,\n  InconsistencyReport,\n} from './types.js';\n\n// ---------------------------------------------------------------------------\n// Inconsistency detection: code-vs-doc\n// ---------------------------------------------------------------------------\n\nexport { extractExports, checkCodeVsDoc } from './inconsistency/code-vs-doc.js';\n\n// ---------------------------------------------------------------------------\n// Inconsistency detection: code-vs-code\n// ---------------------------------------------------------------------------\n\nexport { checkCodeVsCode } from './inconsistency/code-vs-code.js';\n\n// ---------------------------------------------------------------------------\n// Inconsistency reporting\n// ---------------------------------------------------------------------------\n\nexport { buildInconsistencyReport, formatReportForCli } from './inconsistency/reporter.js';\n\n// ---------------------------------------------------------------------------\n// Phantom path detection\n// ---------------------------------------------------------------------------\n\nexport { checkPhantomPaths } from './phantom-paths/index.js';\n\n// ---------------------------------------------------------------------------\n// Density validation\n// ---------------------------------------------------------------------------\n\nexport { validateFindability } from './density/validator.js';\nexport type { FindabilityResult } from './density/validator.js';\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**Public API barrel module for quality analysis system, re-exporting inconsistency detection validators (`extractExports`, `checkCodeVsDoc`, `checkCodeVsCode`, `checkPhantomPaths`), density validation (`validateFindability`), reporting utilities (`buildInconsistencyReport`, `formatReportForCli`), and all shared types (`Inconsistency`, `InconsistencyReport`, `FindabilityResult`) from submodules.**\n\n## Exported Types\n\n- `InconsistencySeverity` — Severity level enumeration from `./types.js`\n- `CodeDocInconsistency` — Code-vs-documentation mismatch structure with `missingFromDoc` array tracking exported symbols absent from `.sum` summary text\n- `CodeCodeInconsistency` — Code-vs-code duplicate symbol structure detecting symbols appearing in multiple files within same directory\n- `PhantomPathInconsistency` — Unresolved path reference structure capturing file paths mentioned in `AGENTS.md` that fail `existsSync()` validation\n- `Inconsistency` — Discriminated union of all inconsistency types\n- `InconsistencyReport` — Aggregated validation report with `metadata` (timestamp, projectRoot, filesChecked, durationMs), `issues[]` array, and `summary` counts by type/severity\n- `FindabilityResult` — Density validation result structure from `./density/validator.js`\n\n## Exported Functions\n\n- `extractExports(filePath: string): string[]` — Regex-based export extraction using pattern `/^[ \\t]*export\\s+(?:default\\s+)?(?:function|class|const|let|var|type|interface|enum)\\s+(\\w+)/gm` from `./inconsistency/code-vs-doc.js`\n- `checkCodeVsDoc(filePath: string, sumContent: string): CodeDocInconsistency | null` — Verifies all extracted exports appear in summary via substring search, returns inconsistency object with `missingFromDoc` array or null if all exports documented\n- `checkCodeVsCode(files: string[], baseDir: string): CodeCodeInconsistency[]` — Aggregates exports across file groups into `Map<symbol, string[]>`, reports duplicate symbols with pattern `'duplicate-export'` from `./inconsistency/code-vs-code.js`\n- `buildInconsistencyReport(issues: Inconsistency[], metadata: object): InconsistencyReport` — Constructs aggregated report with summary counts from `./inconsistency/reporter.js`\n- `formatReportForCli(report: InconsistencyReport): string` — Formats inconsistency report for terminal output with picocolors formatting from `./inconsistency/reporter.js`\n- `checkPhantomPaths(agentsMdPath: string, projectRoot: string): PhantomPathInconsistency[]` — Extracts path-like strings via three regex patterns (markdown links, backtick-quoted paths, prose-embedded paths), resolves against AGENTS.md directory and project root with `.ts`/`.js` fallback, returns array of unresolved references from `./phantom-paths/index.js`\n- `validateFindability(sumFilePath: string): FindabilityResult` — Density validation function from `./density/validator.js` (currently disabled after structured `publicInterface` removal from `SumFileContent` schema per CLAUDE.md Known Limitations)\n\n## Module Organization\n\nIndex module delegates implementation to five submodules organized by validation concern:\n1. `./types.js` — Shared type definitions for all validators\n2. `./inconsistency/code-vs-doc.js` — Export extraction and summary verification\n3. `./inconsistency/code-vs-code.js` — Duplicate symbol detection across directory files\n4. `./inconsistency/reporter.js` — Report aggregation and CLI formatting\n5. `./phantom-paths/index.js` — Path resolution validation for documentation references\n6. `./density/validator.js` — Findability validation (feature disabled)\n\n## Integration Points\n\nConsumed by `src/generation/orchestrator.ts` post-generation validation phase and `src/cli/generate.ts` for terminal reporting. Report format matches telemetry schema in `src/ai/telemetry/run-log.ts` for `qualityMetrics` aggregation. All validators operate on generated artifacts (`.sum`, `AGENTS.md`) rather than source code, enforcing post-generation validation contract.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 1054,
      "cacheReadTokens": 17367,
      "cacheCreationTokens": 9443,
      "latencyMs": 31820,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/quality/index.ts",
          "sizeBytes": 1936
        }
      ]
    },
    {
      "timestamp": "2026-02-09T17:45:18.939Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/orchestration/progress.ts\n\n```typescript\n/**\n * Streaming build-log progress reporter with ETA calculation.\n *\n * Outputs one line per event (start, done, fail, dir-done, root-done)\n * using `console.log` for atomic, non-corrupting concurrent output.\n * Each line shows progress counter, status, file path, timing, and\n * token counts using colored output via `picocolors`.\n *\n * ETA is computed via a moving average of the last 10 completion times,\n * displayed after 2 or more files have completed.\n *\n * Optionally mirrors all output to a plain-text progress log file\n * (`.agents-reverse-engineer/progress.log`) via {@link ProgressLog},\n * enabling `tail -f` monitoring when running inside buffered environments\n * (e.g. Claude Code's Bash tool).\n *\n * @module\n */\n\nimport { open, mkdir } from 'node:fs/promises';\nimport type { FileHandle } from 'node:fs/promises';\nimport * as path from 'node:path';\nimport pc from 'picocolors';\nimport type { RunSummary } from './types.js';\n\n// ---------------------------------------------------------------------------\n// ANSI stripping\n// ---------------------------------------------------------------------------\n\n/** Strip ANSI escape sequences from a string for plain-text output. */\nfunction stripAnsi(str: string): string {\n  // Matches all common ANSI escape codes (SGR, cursor, erase, etc.)\n  // eslint-disable-next-line no-control-regex\n  return str.replace(/\\x1b\\[[0-9;]*m/g, '');\n}\n\n// ---------------------------------------------------------------------------\n// ProgressLog\n// ---------------------------------------------------------------------------\n\n/** Relative path for the progress log file */\nconst PROGRESS_LOG_FILENAME = 'progress.log';\n\n/**\n * Plain-text progress log file writer.\n *\n * Mirrors console progress output to `.agents-reverse-engineer/progress.log`\n * without ANSI escape codes, enabling real-time monitoring via `tail -f`\n * when the CLI runs inside buffered environments (e.g. Claude Code).\n *\n * Uses promise-chain serialization (same pattern as {@link TraceWriter})\n * to handle concurrent writes from multiple pool workers safely.\n *\n * @example\n * ```typescript\n * const log = new ProgressLog('/project/.agents-reverse-engineer/progress.log');\n * log.write('=== ARE Generate (2026-02-09) ===');\n * log.write('[1/10] ANALYZING src/index.ts');\n * await log.finalize();\n * ```\n */\nexport class ProgressLog {\n  private writeQueue: Promise<void> = Promise.resolve();\n  private fd: FileHandle | null = null;\n\n  constructor(private readonly filePath: string) {}\n\n  /**\n   * Create a ProgressLog for a project root.\n   *\n   * @param projectRoot - Absolute path to the project root directory\n   * @returns A new ProgressLog instance\n   */\n  static create(projectRoot: string): ProgressLog {\n    return new ProgressLog(\n      path.join(projectRoot, '.agents-reverse-engineer', PROGRESS_LOG_FILENAME),\n    );\n  }\n\n  /**\n   * Append a line to the progress log file.\n   *\n   * On first call, creates the parent directory and opens the file\n   * in truncate mode ('w'). Subsequent writes append to the open handle.\n   * Write failures are silently swallowed (non-critical telemetry).\n   */\n  write(line: string): void {\n    this.writeQueue = this.writeQueue\n      .then(async () => {\n        if (!this.fd) {\n          await mkdir(path.dirname(this.filePath), { recursive: true });\n          this.fd = await open(this.filePath, 'w');\n        }\n        await this.fd.write(line + '\\n');\n      })\n      .catch(() => { /* non-critical -- progress log loss is acceptable */ });\n  }\n\n  /** Flush all pending writes and close the file handle. */\n  async finalize(): Promise<void> {\n    await this.writeQueue;\n    if (this.fd) {\n      await this.fd.close();\n      this.fd = null;\n    }\n  }\n}\n\n// ---------------------------------------------------------------------------\n// ProgressReporter\n// ---------------------------------------------------------------------------\n\n/**\n * Streaming build-log progress reporter.\n *\n * Create one instance per command run. Call the event methods as files\n * are processed. Call {@link printSummary} at the end of the run.\n *\n * @example\n * ```typescript\n * const reporter = new ProgressReporter(fileCount);\n * reporter.onFileStart('src/index.ts');\n * reporter.onFileDone('src/index.ts', 1200, 500, 300, 'sonnet');\n * reporter.printSummary(summary);\n * ```\n */\nexport class ProgressReporter {\n  /** Total number of file tasks in this run */\n  private readonly totalFiles: number;\n\n  /** Number of files that have started processing */\n  private started: number = 0;\n\n  /** Number of files completed successfully */\n  private completed: number = 0;\n\n  /** Number of files that failed */\n  private failed: number = 0;\n\n  /** Sliding window of recent completion durations for ETA */\n  private readonly completionTimes: number[] = [];\n\n  /** Maximum window size for ETA moving average */\n  private readonly windowSize: number = 10;\n\n  /** Timestamp when the reporter was created */\n  private readonly startTime: number = Date.now();\n\n  /** Total number of directory tasks in this run */\n  private totalDirectories: number = 0;\n\n  /** Number of directory tasks that have started */\n  private dirStarted: number = 0;\n\n  /** Number of directory tasks completed */\n  private dirCompleted: number = 0;\n\n  /** Sliding window of recent directory completion durations for ETA */\n  private readonly dirCompletionTimes: number[] = [];\n\n  /** Optional file-based progress log for tail -f monitoring */\n  private readonly progressLog: ProgressLog | null;\n\n  /**\n   * Create a new progress reporter.\n   *\n   * @param totalFiles - Total number of file tasks to process\n   * @param totalDirectories - Total number of directory tasks to process\n   * @param progressLog - Optional progress log for file-based output mirroring\n   */\n  constructor(totalFiles: number, totalDirectories: number = 0, progressLog?: ProgressLog) {\n    this.totalFiles = totalFiles;\n    this.totalDirectories = totalDirectories;\n    this.progressLog = progressLog ?? null;\n  }\n\n  /**\n   * Log the start of file analysis.\n   *\n   * Output format: `[X/Y] ANALYZING path`\n   *\n   * @param filePath - Relative path to the file being analyzed\n   */\n  onFileStart(filePath: string): void {\n    this.started++;\n    const line = `${pc.dim(`[${this.started}/${this.totalFiles}]`)} ${pc.cyan('ANALYZING')} ${filePath}`;\n    console.log(line);\n    this.progressLog?.write(stripAnsi(line));\n  }\n\n  /**\n   * Log the successful completion of file analysis.\n   *\n   * Output format: `[X/Y] DONE path Xs in/out tok ~Ns remaining`\n   *\n   * Records the completion time for ETA calculation.\n   *\n   * @param filePath - Relative path to the completed file\n   * @param durationMs - Wall-clock duration of the AI call\n   * @param tokensIn - Number of input tokens consumed (non-cached)\n   * @param tokensOut - Number of output tokens generated\n   * @param model - Model identifier used for this call\n   * @param cacheReadTokens - Number of cache read input tokens\n   */\n  onFileDone(\n    filePath: string,\n    durationMs: number,\n    tokensIn: number,\n    tokensOut: number,\n    model: string,\n    cacheReadTokens = 0,\n    cacheCreationTokens = 0,\n  ): void {\n    this.completed++;\n\n    // Record completion time for ETA\n    this.completionTimes.push(durationMs);\n    if (this.completionTimes.length > this.windowSize) {\n      this.completionTimes.shift();\n    }\n\n    const counter = pc.dim(`[${this.completed + this.failed}/${this.totalFiles}]`);\n    const time = pc.dim(`${(durationMs / 1000).toFixed(1)}s`);\n    // Total prompt size = non-cached + cache read + cache creation tokens\n    const totalIn = tokensIn + cacheReadTokens + cacheCreationTokens;\n    const tokens = pc.dim(`${totalIn}/${tokensOut} tok`);\n    const modelLabel = pc.dim(model);\n    const eta = this.formatETA();\n\n    const line = `${counter} ${pc.green('DONE')} ${filePath} ${time} ${tokens} ${modelLabel}${eta}`;\n    console.log(line);\n    this.progressLog?.write(stripAnsi(line));\n  }\n\n  /**\n   * Log a file analysis failure.\n   *\n   * Output format: `[X/Y] FAIL path error`\n   *\n   * @param filePath - Relative path to the failed file\n   * @param error - Error message describing the failure\n   */\n  onFileError(filePath: string, error: string): void {\n    this.failed++;\n\n    const line = `${pc.dim(`[${this.completed + this.failed}/${this.totalFiles}]`)} ${pc.red('FAIL')} ${filePath} ${pc.dim(error)}`;\n    console.log(line);\n    this.progressLog?.write(stripAnsi(line));\n  }\n\n  /**\n   * Log the start of directory AGENTS.md generation.\n   *\n   * Output format: `[dir X/Y] ANALYZING dirPath/AGENTS.md`\n   *\n   * @param dirPath - Path to the directory\n   */\n  onDirectoryStart(dirPath: string): void {\n    this.dirStarted++;\n    const line = `${pc.dim(`[dir ${this.dirStarted}/${this.totalDirectories}]`)} ${pc.cyan('ANALYZING')} ${dirPath}/AGENTS.md`;\n    console.log(line);\n    this.progressLog?.write(stripAnsi(line));\n  }\n\n  /**\n   * Log the completion of directory AGENTS.md generation.\n   *\n   * Output format: `[dir X/Y] DONE dirPath/AGENTS.md Xs in/out tok model ~ETA`\n   *\n   * @param dirPath - Path to the directory\n   * @param durationMs - Wall-clock duration of the AI call\n   * @param tokensIn - Number of input tokens consumed (non-cached)\n   * @param tokensOut - Number of output tokens generated\n   * @param model - Model identifier used for this call\n   * @param cacheReadTokens - Number of cache read input tokens\n   */\n  onDirectoryDone(\n    dirPath: string,\n    durationMs: number,\n    tokensIn: number,\n    tokensOut: number,\n    model: string,\n    cacheReadTokens = 0,\n    cacheCreationTokens = 0,\n  ): void {\n    this.dirCompleted++;\n\n    // Record completion time for directory ETA\n    this.dirCompletionTimes.push(durationMs);\n    if (this.dirCompletionTimes.length > this.windowSize) {\n      this.dirCompletionTimes.shift();\n    }\n\n    const counter = pc.dim(`[dir ${this.dirCompleted}/${this.totalDirectories}]`);\n    const time = pc.dim(`${(durationMs / 1000).toFixed(1)}s`);\n    // Total prompt size = non-cached + cache read + cache creation tokens\n    const totalIn = tokensIn + cacheReadTokens + cacheCreationTokens;\n    const tokens = pc.dim(`${totalIn}/${tokensOut} tok`);\n    const modelLabel = pc.dim(model);\n    const eta = this.formatDirectoryETA();\n\n    const line = `${counter} ${pc.blue('DONE')} ${dirPath}/AGENTS.md ${time} ${tokens} ${modelLabel}${eta}`;\n    console.log(line);\n    this.progressLog?.write(stripAnsi(line));\n  }\n\n  /**\n   * Log the completion of a root document generation.\n   *\n   * Output format: `[root] DONE docPath`\n   *\n   * @param docPath - Path to the root document\n   */\n  onRootDone(docPath: string): void {\n    const line = `${pc.dim('[root]')} ${pc.blue('DONE')} ${docPath}`;\n    console.log(line);\n    this.progressLog?.write(stripAnsi(line));\n  }\n\n  /**\n   * Print the end-of-run summary.\n   *\n   * Shows files processed, token counts, files read with unique dedup,\n   * time elapsed, errors, and retries.\n   *\n   * @param summary - Aggregated run summary\n   */\n  printSummary(summary: RunSummary): void {\n    const elapsed = ((Date.now() - this.startTime) / 1000).toFixed(1);\n\n    const lines: string[] = [];\n    lines.push('');\n    lines.push(pc.bold('=== Run Summary ==='));\n    lines.push(`  ARE version:     ${summary.version}`);\n    lines.push(`  Files processed: ${pc.green(String(summary.filesProcessed))}`);\n    if (summary.filesFailed > 0) {\n      lines.push(`  Files failed:    ${pc.red(String(summary.filesFailed))}`);\n    }\n    if (summary.filesSkipped > 0) {\n      lines.push(`  Files skipped:   ${pc.yellow(String(summary.filesSkipped))}`);\n    }\n    lines.push(`  Total calls:     ${summary.totalCalls}`);\n    const totalIn = summary.totalInputTokens + summary.totalCacheReadTokens + summary.totalCacheCreationTokens;\n    lines.push(`  Tokens:          ${totalIn} in / ${summary.totalOutputTokens} out`);\n    if (summary.totalCacheReadTokens > 0) {\n      lines.push(`  Cache:           ${summary.totalCacheReadTokens} read / ${summary.totalCacheCreationTokens} created`);\n    }\n\n    if (summary.totalFilesRead > 0) {\n      lines.push(`  Files read:      ${summary.totalFilesRead} (${summary.uniqueFilesRead} unique)`);\n    }\n\n    lines.push(`  Total time:      ${elapsed}s`);\n    lines.push(`  Errors:          ${summary.errorCount}`);\n    if (summary.retryCount > 0) {\n      lines.push(`  Retries:         ${summary.retryCount}`);\n    }\n\n    for (const line of lines) {\n      console.log(line);\n      this.progressLog?.write(stripAnsi(line));\n    }\n  }\n\n  // -------------------------------------------------------------------------\n  // ETA calculation\n  // -------------------------------------------------------------------------\n\n  /**\n   * Compute and format the estimated time remaining.\n   *\n   * Uses a moving average of the last 10 completion times.\n   * Returns an empty string if fewer than 2 completions have occurred.\n   *\n   * @returns Formatted ETA string like ` ~12s remaining` or ` ~2m 30s remaining`\n   */\n  private formatETA(): string {\n    if (this.completionTimes.length < 2) return '';\n\n    const avg =\n      this.completionTimes.reduce((a, b) => a + b, 0) /\n      this.completionTimes.length;\n    const remaining = this.totalFiles - this.completed - this.failed;\n\n    if (remaining <= 0) return '';\n\n    const etaMs = avg * remaining;\n    const seconds = Math.round(etaMs / 1000);\n\n    if (seconds < 60) {\n      return pc.dim(` ~${seconds}s remaining`);\n    }\n\n    const minutes = Math.floor(seconds / 60);\n    const secs = seconds % 60;\n    return pc.dim(` ~${minutes}m ${secs}s remaining`);\n  }\n\n  /**\n   * Compute and format the estimated time remaining for directory tasks.\n   *\n   * Uses a moving average of the last 10 directory completion times.\n   * Returns an empty string if fewer than 2 completions have occurred.\n   */\n  private formatDirectoryETA(): string {\n    if (this.dirCompletionTimes.length < 2) return '';\n\n    const avg =\n      this.dirCompletionTimes.reduce((a, b) => a + b, 0) /\n      this.dirCompletionTimes.length;\n    const remaining = this.totalDirectories - this.dirCompleted;\n\n    if (remaining <= 0) return '';\n\n    const etaMs = avg * remaining;\n    const seconds = Math.round(etaMs / 1000);\n\n    if (seconds < 60) {\n      return pc.dim(` ~${seconds}s remaining`);\n    }\n\n    const minutes = Math.floor(seconds / 60);\n    const secs = seconds % 60;\n    return pc.dim(` ~${minutes}m ${secs}s remaining`);\n  }\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**ProgressReporter provides streaming build-log progress reporting with ETA calculation via moving averages, outputting colored console lines and optionally mirroring plain-text output to `.agents-reverse-engineer/progress.log` for tail monitoring in buffered environments.**\n\n## Exported Classes\n\n**ProgressLog** — Plain-text progress log file writer mirroring console output without ANSI codes.\n\n```typescript\nclass ProgressLog {\n  constructor(private readonly filePath: string)\n  static create(projectRoot: string): ProgressLog\n  write(line: string): void\n  async finalize(): Promise<void>\n}\n```\n\nUses promise-chain serialization (`this.writeQueue = this.writeQueue.then(...)`) to serialize concurrent writes from multiple pool workers. Opens file handle in truncate mode (`'w'`) on first write, appends to open handle on subsequent writes. Silently swallows write failures (non-critical telemetry). Creates parent directory via `mkdir(path.dirname(this.filePath), { recursive: true })`. Static factory `create()` constructs path as `<projectRoot>/.agents-reverse-engineer/progress.log` using constant `PROGRESS_LOG_FILENAME = 'progress.log'`.\n\n**ProgressReporter** — Streaming build-log progress reporter tracking file and directory tasks with ETA calculation.\n\n```typescript\nclass ProgressReporter {\n  constructor(totalFiles: number, totalDirectories: number = 0, progressLog?: ProgressLog)\n  onFileStart(filePath: string): void\n  onFileDone(filePath: string, durationMs: number, tokensIn: number, tokensOut: number, model: string, cacheReadTokens?: number, cacheCreationTokens?: number): void\n  onFileError(filePath: string, error: string): void\n  onDirectoryStart(dirPath: string): void\n  onDirectoryDone(dirPath: string, durationMs: number, tokensIn: number, tokensOut: number, model: string, cacheReadTokens?: number, cacheCreationTokens?: number): void\n  onRootDone(docPath: string): void\n  printSummary(summary: RunSummary): void\n}\n```\n\nTracks counters: `started`, `completed`, `failed` for file tasks; `dirStarted`, `dirCompleted` for directory tasks. Maintains sliding windows `completionTimes` and `dirCompletionTimes` (max size `windowSize = 10`) for ETA moving averages. Records `startTime` via `Date.now()` for elapsed time calculation.\n\n## Output Formats\n\nFile analysis start: `[X/Y] ANALYZING path` (cyan ANALYZING)  \nFile analysis done: `[X/Y] DONE path Xs in/out tok model ~Ns remaining` (green DONE)  \nFile analysis fail: `[X/Y] FAIL path error` (red FAIL)  \nDirectory start: `[dir X/Y] ANALYZING dirPath/AGENTS.md` (cyan ANALYZING)  \nDirectory done: `[dir X/Y] DONE dirPath/AGENTS.md Xs in/out tok model ~ETA` (blue DONE)  \nRoot document done: `[root] DONE docPath` (blue DONE)\n\nAll output uses `console.log()` for atomic non-corrupting concurrent writes. Color formatting via `picocolors` aliases: `pc.dim()`, `pc.cyan()`, `pc.green()`, `pc.red()`, `pc.blue()`, `pc.yellow()`, `pc.bold()`.\n\n## ETA Calculation\n\n`formatETA()` computes file task ETA using moving average of `completionTimes` array. Requires minimum 2 completions before displaying ETA. Calculates `avg = sum(completionTimes) / completionTimes.length`, `remaining = totalFiles - completed - failed`, `etaMs = avg * remaining`. Formats as `~Ns remaining` for <60s, `~Mm Ss remaining` for ≥60s. Returns empty string if `remaining <= 0` or fewer than 2 completions.\n\n`formatDirectoryETA()` uses identical algorithm on `dirCompletionTimes` for directory tasks with `remaining = totalDirectories - dirCompleted`.\n\n## Token Counting\n\n`onFileDone()` and `onDirectoryDone()` accept `tokensIn` (non-cached input), `tokensOut` (output), `cacheReadTokens` (cache hits), `cacheCreationTokens` (cache writes). Displays total prompt size as `totalIn = tokensIn + cacheReadTokens + cacheCreationTokens` in format `${totalIn}/${tokensOut} tok`.\n\n## Summary Output\n\n`printSummary(summary: RunSummary)` prints multi-line report:\n- `ARE version` from `summary.version`\n- `Files processed` (green), `Files failed` (red if >0), `Files skipped` (yellow if >0)\n- `Total calls`, `Tokens: X in / Y out`\n- `Cache: X read / Y created` (if `totalCacheReadTokens > 0`)\n- `Files read: X (Y unique)` (if `totalFilesRead > 0`)\n- `Total time`, `Errors`, `Retries` (if `retryCount > 0`)\n\nComputes elapsed time as `(Date.now() - this.startTime) / 1000` formatted to 1 decimal place.\n\n## ANSI Stripping\n\n`stripAnsi(str: string)` removes ANSI escape codes via regex `/\\x1b\\[[0-9;]*m/g` before writing to ProgressLog. Matches all SGR, cursor, and erase sequences.\n\n## Integration Points\n\nReceives `RunSummary` type from `./types.js` with fields: `version`, `filesProcessed`, `filesFailed`, `filesSkipped`, `totalCalls`, `totalInputTokens`, `totalOutputTokens`, `totalCacheReadTokens`, `totalCacheCreationTokens`, `totalFilesRead`, `uniqueFilesRead`, `errorCount`, `retryCount`. Instantiated per command run with total file/directory counts from discovery phase. Event methods called by worker pool during task execution. `printSummary()` invoked by orchestrator after all phases complete.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 1507,
      "cacheReadTokens": 17367,
      "cacheCreationTokens": 13448,
      "latencyMs": 38165,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/orchestration/progress.ts",
          "sizeBytes": 14428
        }
      ]
    },
    {
      "timestamp": "2026-02-09T17:45:40.984Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/types/index.ts\n\n```typescript\n/**\n * Shared types used across modules\n */\n\n/**\n * Information about an excluded file\n */\nexport interface ExcludedFile {\n  /** Absolute or relative path to the excluded file */\n  path: string;\n  /** Reason for exclusion (e.g., \"gitignore pattern\", \"binary file\", \"vendor directory\") */\n  reason: string;\n}\n\n/**\n * Result of the file discovery process\n */\nexport interface DiscoveryResult {\n  /** List of file paths that should be analyzed */\n  files: string[];\n  /** List of files that were excluded with reasons */\n  excluded: ExcludedFile[];\n}\n\n/**\n * Statistics about the discovery process\n */\nexport interface DiscoveryStats {\n  /** Total number of files found */\n  totalFiles: number;\n  /** Number of files included for analysis */\n  includedFiles: number;\n  /** Number of files excluded */\n  excludedFiles: number;\n  /** Breakdown of exclusion reasons */\n  exclusionReasons: Record<string, number>;\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**Defines shared type interfaces for file discovery results used across discovery, orchestration, and CLI modules.**\n\n## Exported Types\n\n### ExcludedFile\nInterface representing files filtered out during discovery.\n\n**Properties:**\n- `path: string` — Absolute or relative path to the excluded file\n- `reason: string` — Exclusion rationale (examples: \"gitignore pattern\", \"binary file\", \"vendor directory\")\n\n### DiscoveryResult\nInterface aggregating discovery phase output.\n\n**Properties:**\n- `files: string[]` — File paths eligible for analysis (passed to Phase 1 worker pool)\n- `excluded: ExcludedFile[]` — Files filtered out with exclusion metadata\n\n### DiscoveryStats\nInterface providing discovery metrics for CLI reporting.\n\n**Properties:**\n- `totalFiles: number` — Count of all files encountered during traversal\n- `includedFiles: number` — Count of files passing filters (equivalent to `DiscoveryResult.files.length`)\n- `excludedFiles: number` — Count of filtered files (equivalent to `DiscoveryResult.excluded.length`)\n- `exclusionReasons: Record<string, number>` — Histogram mapping exclusion reasons to occurrence counts (aggregated from `ExcludedFile.reason` values)\n\n## Usage Context\n\n`DiscoveryResult` is returned by `runDiscovery()` in `src/discovery/run.ts` and consumed by:\n- `discover` command (`src/cli/discover.ts`) for plan generation\n- `generate` command (`src/cli/generate.ts`) for Phase 1 file analysis\n- `update` command (`src/cli/update.ts`) for change detection input\n\n`DiscoveryStats` is computed from `DiscoveryResult` for terminal output in CLI commands via `src/output/logger.ts`.\n\n`ExcludedFile` is populated by filter chain in `src/discovery/walker.ts` using filter modules (`src/discovery/filters/`): `gitignore.ts`, `binary.ts`, `vendor.ts`, `custom.ts`.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 479,
      "cacheReadTokens": 17367,
      "cacheCreationTokens": 9346,
      "latencyMs": 17706,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/types/index.ts",
          "sizeBytes": 909
        }
      ]
    },
    {
      "timestamp": "2026-02-09T17:45:43.148Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/update/index.ts\n\n```typescript\n/**\n * Update module\n *\n * Provides incremental documentation update functionality.\n * Coordinates state management, change detection, and orphan cleanup.\n */\nexport {\n  UpdateOrchestrator,\n  createUpdateOrchestrator,\n  type UpdatePlan,\n} from './orchestrator.js';\n\nexport {\n  cleanupOrphans,\n  cleanupEmptyDirectoryDocs,\n  getAffectedDirectories,\n} from './orphan-cleaner.js';\n\nexport type {\n  UpdateOptions,\n  UpdateResult,\n  UpdateProgress,\n  CleanupResult,\n} from './types.js';\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**Barrel module exporting UpdateOrchestrator, orphan cleanup utilities, and TypeScript interfaces for incremental documentation updates via hash-based change detection and affected directory recomputation.**\n\n## Exported Types\n\n- `UpdateOrchestrator` — Class orchestrating incremental update workflow (from `./orchestrator.js`)\n- `createUpdateOrchestrator` — Factory function returning configured UpdateOrchestrator instance\n- `UpdatePlan` — Interface describing files to analyze, skip, and affected directories\n- `UpdateOptions` — Configuration for update execution (includes `uncommitted` flag for git working tree changes)\n- `UpdateResult` — Result summary with counts of regenerated files and directories\n- `UpdateProgress` — Progress event payload for streaming updates\n- `CleanupResult` — Result of orphan cleanup operations with deleted file/directory counts\n\n## Exported Functions\n\n- `cleanupOrphans(orphanSumFiles: string[], orphanAgentsMdFiles: string[]): Promise<CleanupResult>` — Deletes `.sum` files for removed sources and `AGENTS.md` files from directories with no remaining sources\n- `cleanupEmptyDirectoryDocs(affectedDirs: string[], projectRoot: string): Promise<void>` — Removes `AGENTS.md` files from directories that no longer contain source files or child directories with documentation\n- `getAffectedDirectories(changedFiles: string[], projectRoot: string): string[]` — Computes set of directories requiring `AGENTS.md` regeneration by walking parent directories of changed files\n\n## Module Organization\n\nRe-exports from three internal modules:\n- `./orchestrator.js` — UpdateOrchestrator class and factory, UpdatePlan interface\n- `./orphan-cleaner.js` — Cleanup utilities for stale artifacts and affected directory computation\n- `./types.js` — UpdateOptions, UpdateResult, UpdateProgress, CleanupResult interfaces\n\n## Integration Points\n\nUpdateOrchestrator coordinates:\n- `src/change-detection/` — SHA-256 hash comparison via `content_hash` in `.sum` YAML frontmatter\n- `src/generation/` — Phase 1 pool execution for `filesToAnalyze`, sequential AGENTS.md regeneration for `affectedDirs`\n- `src/orchestration/` — Worker pool and progress reporter for parallel `.sum` regeneration\n- `src/discovery/` — File discovery results for orphan detection\n\nOrphan cleanup removes `.sum` files when source files deleted/renamed and `AGENTS.md` files when directories become empty after updates.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 586,
      "cacheReadTokens": 0,
      "cacheCreationTokens": 26613,
      "latencyMs": 20274,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/update/index.ts",
          "sizeBytes": 482
        }
      ]
    },
    {
      "timestamp": "2026-02-09T17:45:46.823Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/update/types.ts\n\n```typescript\n/**\n * Types for incremental update workflow\n */\nimport type { FileChange } from '../change-detection/types.js';\n\n/**\n * Result of orphan cleanup\n */\nexport interface CleanupResult {\n  /** .sum files that were deleted */\n  deletedSumFiles: string[];\n  /** AGENTS.md files that were deleted (from empty directories) */\n  deletedAgentsMd: string[];\n}\n\n/**\n * Options for the update command\n */\nexport interface UpdateOptions {\n  /** Include uncommitted changes (staged + working directory) */\n  includeUncommitted?: boolean;\n  /** Dry run - show what would change without making changes */\n  dryRun?: boolean;\n}\n\n/**\n * Result of an update run\n */\nexport interface UpdateResult {\n  /** Files that were analyzed (added or modified) */\n  analyzedFiles: string[];\n  /** Files that were skipped (unchanged) */\n  skippedFiles: string[];\n  /** Cleanup result (deleted .sum and AGENTS.md files) */\n  cleanup: CleanupResult;\n  /** Directories whose AGENTS.md was regenerated */\n  regeneratedDirs: string[];\n  /** Git commit hash at start of update */\n  baseCommit: string;\n  /** Git commit hash at end of update */\n  currentCommit: string;\n  /** Whether this was a dry run */\n  dryRun: boolean;\n}\n\n/**\n * Progress callback for update operations\n */\nexport interface UpdateProgress {\n  /** Called when a file is about to be processed */\n  onFileStart?: (path: string, status: 'analyzing' | 'skipping') => void;\n  /** Called when a file is done processing */\n  onFileDone?: (path: string, status: 'analyzed' | 'skipped' | 'error') => void;\n  /** Called when cleanup deletes a file */\n  onCleanup?: (path: string, type: 'sum' | 'agents-md') => void;\n  /** Called when a directory AGENTS.md is regenerated */\n  onDirRegenerate?: (path: string) => void;\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**Defines TypeScript interfaces for incremental update workflow including change results, cleanup tracking, progress callbacks, and dry-run configuration.**\n\n## Exported Interfaces\n\n`CleanupResult` tracks orphan cleanup outcomes with two fields:\n- `deletedSumFiles: string[]` — paths of removed `.sum` files\n- `deletedAgentsMd: string[]` — paths of removed `AGENTS.md` files from empty directories\n\n`UpdateOptions` configures update command behavior:\n- `includeUncommitted?: boolean` — merge staged and working directory changes into detection\n- `dryRun?: boolean` — preview changes without writing files\n\n`UpdateResult` aggregates update execution outcomes:\n- `analyzedFiles: string[]` — files regenerated due to added/modified status\n- `skippedFiles: string[]` — files with matching `content_hash` in frontmatter\n- `cleanup: CleanupResult` — orphan deletion summary\n- `regeneratedDirs: string[]` — directories whose `AGENTS.md` was rewritten\n- `baseCommit: string` — git SHA at update start\n- `currentCommit: string` — git SHA at update end\n- `dryRun: boolean` — whether execution was preview-only\n\n`UpdateProgress` provides lifecycle callbacks for streaming progress:\n- `onFileStart?: (path: string, status: 'analyzing' | 'skipping') => void` — invoked before processing file\n- `onFileDone?: (path: string, status: 'analyzed' | 'skipped' | 'error') => void` — invoked after file completion\n- `onCleanup?: (path: string, type: 'sum' | 'agents-md') => void` — invoked when orphan deleted\n- `onDirRegenerate?: (path: string) => void` — invoked when directory `AGENTS.md` regenerated\n\n## Dependencies\n\nImports `FileChange` from `../change-detection/types.js` to represent added/modified/deleted/renamed file states with `status` discriminator and optional `oldPath` for renames.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 485,
      "cacheReadTokens": 0,
      "cacheCreationTokens": 26975,
      "latencyMs": 17136,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/update/types.ts",
          "sizeBytes": 1741
        }
      ]
    },
    {
      "timestamp": "2026-02-09T17:45:21.142Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/orchestration/types.ts\n\n```typescript\n/**\n * Shared types for the orchestration module.\n *\n * These types are used across the concurrency pool, progress reporter,\n * and command runner to represent task results, run summaries, progress\n * events, and command options.\n *\n * @module\n */\n\nimport type { InconsistencyReport } from '../quality/index.js';\nimport type { ProgressLog } from './progress.js';\nimport type { ITraceWriter } from './trace.js';\n\n// ---------------------------------------------------------------------------\n// File task result\n// ---------------------------------------------------------------------------\n\n/**\n * Result of processing a single file through AI analysis.\n *\n * Produced by the command runner for each file task, carrying token\n * counts and timing data needed for the run summary.\n */\nexport interface FileTaskResult {\n  /** Relative path to the source file */\n  path: string;\n  /** Whether the AI call succeeded */\n  success: boolean;\n  /** Number of input tokens consumed (non-cached) */\n  tokensIn: number;\n  /** Number of output tokens generated */\n  tokensOut: number;\n  /** Number of cache read input tokens */\n  cacheReadTokens: number;\n  /** Number of cache creation input tokens */\n  cacheCreationTokens: number;\n  /** Wall-clock duration in milliseconds */\n  durationMs: number;\n  /** Model identifier used for this call */\n  model: string;\n  /** Error message if the call failed */\n  error?: string;\n}\n\n// ---------------------------------------------------------------------------\n// Run summary\n// ---------------------------------------------------------------------------\n\n/**\n * Aggregated summary of a command run.\n *\n * Produced at the end of a generate or update command execution,\n * combining per-file results into totals for display and telemetry.\n */\nexport interface RunSummary {\n  /** agents-reverse-engineer version that produced this run */\n  version: string;\n  /** Number of files that were successfully processed */\n  filesProcessed: number;\n  /** Number of files that failed processing */\n  filesFailed: number;\n  /** Number of files that were skipped (e.g., dry-run) */\n  filesSkipped: number;\n  /** Total number of AI calls made */\n  totalCalls: number;\n  /** Sum of input tokens across all calls */\n  totalInputTokens: number;\n  /** Sum of output tokens across all calls */\n  totalOutputTokens: number;\n  /** Sum of cache read tokens across all calls */\n  totalCacheReadTokens: number;\n  /** Sum of cache creation tokens across all calls */\n  totalCacheCreationTokens: number;\n  /** Total wall-clock duration in milliseconds */\n  totalDurationMs: number;\n  /** Number of errors encountered */\n  errorCount: number;\n  /** Number of retries that occurred */\n  retryCount: number;\n  /** Total file reads across all calls */\n  totalFilesRead: number;\n  /** Unique files read (deduped by path) */\n  uniqueFilesRead: number;\n  /** Number of code-vs-doc inconsistencies detected */\n  inconsistenciesCodeVsDoc?: number;\n  /** Number of code-vs-code inconsistencies detected */\n  inconsistenciesCodeVsCode?: number;\n  /** Number of phantom path references detected in AGENTS.md files */\n  phantomPaths?: number;\n  /** Full inconsistency report (undefined if no checks ran) */\n  inconsistencyReport?: InconsistencyReport;\n}\n\n// ---------------------------------------------------------------------------\n// Progress events\n// ---------------------------------------------------------------------------\n\n/**\n * Event emitted by the command runner to the progress reporter.\n *\n * Each event type carries different optional fields:\n * - `start`: filePath, index, total\n * - `done`: filePath, index, total, durationMs, tokensIn, tokensOut, model\n * - `error`: filePath, index, total, error\n * - `dir-done`: filePath (directory path)\n * - `root-done`: filePath (root document path)\n */\nexport interface ProgressEvent {\n  /** Event type */\n  type: 'start' | 'done' | 'error' | 'dir-done' | 'root-done';\n  /** File or directory path */\n  filePath: string;\n  /** Zero-based index of this task in the current phase */\n  index: number;\n  /** Total number of tasks in the current phase */\n  total: number;\n  /** Wall-clock duration in milliseconds (for 'done' events) */\n  durationMs?: number;\n  /** Input tokens consumed (for 'done' events) */\n  tokensIn?: number;\n  /** Output tokens generated (for 'done' events) */\n  tokensOut?: number;\n  /** Model identifier (for 'done' events) */\n  model?: string;\n  /** Error message (for 'error' events) */\n  error?: string;\n}\n\n// ---------------------------------------------------------------------------\n// Command run options\n// ---------------------------------------------------------------------------\n\n/**\n * Options that control how commands execute.\n *\n * These are populated from a combination of config file defaults\n * and CLI flag overrides.\n */\nexport interface CommandRunOptions {\n  /** Maximum number of concurrent AI calls */\n  concurrency: number;\n  /** Stop pulling new tasks on first error */\n  failFast?: boolean;\n  /** Show debug information (exact prompts sent) */\n  debug?: boolean;\n  /** List files that would be processed without executing */\n  dryRun?: boolean;\n  /** Trace writer for concurrency debugging (no-op when tracing is off) */\n  tracer?: ITraceWriter;\n  /** Progress log for file-based output mirroring (tail -f monitoring) */\n  progressLog?: ProgressLog;\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**types.ts defines TypeScript interfaces for orchestration module contracts: task results from AI subprocess execution, aggregated run summaries with token/cost metrics, progress events for streaming reporter updates, and command execution options threading concurrency/debug/trace configuration across the three-phase pipeline.**\n\n## Exported Interfaces\n\n### FileTaskResult\nPer-file AI analysis outcome returned by command runner to pool coordinator.\n\n```typescript\ninterface FileTaskResult {\n  path: string;                    // Relative source file path\n  success: boolean;                // AI subprocess succeeded\n  tokensIn: number;                // Non-cached input tokens consumed\n  tokensOut: number;               // Generated output tokens\n  cacheReadTokens: number;         // Cache-hit input tokens\n  cacheCreationTokens: number;     // Cache-write input tokens\n  durationMs: number;              // Wall-clock execution time\n  model: string;                   // AI model identifier used\n  error?: string;                  // Failure message if success=false\n}\n```\n\nProduced by `src/orchestration/runner.ts` for each completed file task, aggregated into `RunSummary` for telemetry logging.\n\n### RunSummary\nCommand-level aggregation of all `FileTaskResult` instances plus quality metrics.\n\n```typescript\ninterface RunSummary {\n  version: string;                      // ARE version (from src/version.ts)\n  filesProcessed: number;               // Success count\n  filesFailed: number;                  // Error count\n  filesSkipped: number;                 // Dry-run count\n  totalCalls: number;                   // AI subprocess invocations\n  totalInputTokens: number;             // Sum of tokensIn across calls\n  totalOutputTokens: number;            // Sum of tokensOut\n  totalCacheReadTokens: number;         // Sum of cacheReadTokens\n  totalCacheCreationTokens: number;     // Sum of cacheCreationTokens\n  totalDurationMs: number;              // Wall-clock total\n  errorCount: number;                   // Failed tasks\n  retryCount: number;                   // Exponential backoff retry attempts\n  totalFilesRead: number;               // Sum of filesRead[] lengths\n  uniqueFilesRead: number;              // Deduped count via Set<path>\n  inconsistenciesCodeVsDoc?: number;    // Code-vs-doc validator findings\n  inconsistenciesCodeVsCode?: number;   // Duplicate export detector findings\n  phantomPaths?: number;                // Unresolved path references in AGENTS.md\n  inconsistencyReport?: InconsistencyReport; // Full quality report from src/quality/\n}\n```\n\nWritten to `.agents-reverse-engineer/logs/run-<timestamp>.json` by `src/ai/telemetry/logger.ts`. Optional quality fields populated only when validators execute post-generation.\n\n### ProgressEvent\nDiscriminated union for streaming progress updates from runner to `ProgressLog`.\n\n```typescript\ninterface ProgressEvent {\n  type: 'start' | 'done' | 'error' | 'dir-done' | 'root-done';\n  filePath: string;                // File or directory path\n  index: number;                   // Zero-based task position in phase\n  total: number;                   // Total tasks in current phase\n  durationMs?: number;             // Wall-clock time (type='done')\n  tokensIn?: number;               // Input tokens (type='done')\n  tokensOut?: number;              // Output tokens (type='done')\n  model?: string;                  // Model identifier (type='done')\n  error?: string;                  // Error message (type='error')\n}\n```\n\nEvent types:\n- `start` — Task picked up by worker (requires `filePath`, `index`, `total`)\n- `done` — Task completed successfully (adds `durationMs`, `tokensIn`, `tokensOut`, `model`)\n- `error` — Task failed (adds `error` message)\n- `dir-done` — Directory `AGENTS.md` written (Phase 2 completion marker)\n- `root-done` — Root document written (`CLAUDE.md`/`GEMINI.md`/`OPENCODE.md`, Phase 3 marker)\n\nConsumed by `src/orchestration/progress.ts` for console spinner updates and ETA calculation (moving average of last 10 `durationMs` values).\n\n### CommandRunOptions\nConfiguration threading object passed from CLI → orchestrator → pool → AIService.\n\n```typescript\ninterface CommandRunOptions {\n  concurrency: number;            // Worker pool size (1-10, default 2 WSL/5 other)\n  failFast?: boolean;             // Abort on first task failure\n  debug?: boolean;                // Log exact prompts + subprocess heap/RSS metrics\n  dryRun?: boolean;               // Preview without file writes\n  tracer?: ITraceWriter;          // NDJSON trace emitter (NullTraceWriter when --trace off)\n  progressLog?: ProgressLog;      // File-based progress mirror for tail -f monitoring\n}\n```\n\n`tracer` field populated by `src/orchestration/trace.ts:TraceWriter` when `--trace` flag active, otherwise `NullTraceWriter` (no-op). `progressLog` initialized by `src/cli/generate.ts` and `src/cli/update.ts` to mirror console output to `.agents-reverse-engineer/progress.log`.\n\n## Integration Points\n\n- **FileTaskResult**: Produced by `src/orchestration/runner.ts:runFileAnalysis()`, consumed by pool workers in `src/orchestration/pool.ts:runPool()` for aggregation\n- **RunSummary**: Built by `src/generation/orchestrator.ts:generateDocs()` and `src/update/orchestrator.ts:runUpdate()`, passed to `src/ai/telemetry/logger.ts:logRun()`\n- **ProgressEvent**: Emitted by runner via `emit(event)` callback, handled by `src/orchestration/progress.ts:ProgressReporter.update()` for spinner/ETA/file-log writes\n- **CommandRunOptions**: Merged from config defaults (`src/config/schema.ts`) + CLI flag overrides (`src/cli/generate.ts`, `src/cli/update.ts`) before passing to orchestrator\n- **InconsistencyReport** (type dependency): Imported from `src/quality/index.ts`, contains discriminated union of `CodeDocInconsistency | CodeCodeInconsistency | PhantomPathInconsistency` with metadata (timestamp, projectRoot, filesChecked, durationMs)\n- **ITraceWriter** (type dependency): Imported from `src/orchestration/trace.ts`, interface with `trace(event)` method for NDJSON event emission\n- **ProgressLog** (type dependency): Imported from `src/orchestration/progress.ts`, interface with `write(msg)` method for `.agents-reverse-engineer/progress.log` writes",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 1656,
      "cacheReadTokens": 0,
      "cacheCreationTokens": 27832,
      "latencyMs": 43196,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/orchestration/types.ts",
          "sizeBytes": 5350
        }
      ]
    },
    {
      "timestamp": "2026-02-09T17:45:34.102Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/specify/writer.ts\n\n```typescript\nimport { writeFile, mkdir, access } from 'node:fs/promises';\nimport { constants } from 'node:fs';\nimport * as path from 'node:path';\n\n/**\n * Options controlling spec output writing behavior.\n */\nexport interface WriteSpecOptions {\n  /** Full path to the output file (e.g., /project/specs/SPEC.md). */\n  outputPath: string;\n  /** Overwrite existing files without error. */\n  force: boolean;\n  /** Split AI output into multiple files by top-level `# ` headings. */\n  multiFile: boolean;\n}\n\n/**\n * Thrown when writeSpec() detects existing file(s) and force=false.\n * Callers should catch this and present a user-friendly message.\n */\nexport class SpecExistsError extends Error {\n  /** Paths of the files that already exist. */\n  readonly paths: string[];\n\n  constructor(paths: string[]) {\n    const list = paths.map((p) => `  - ${p}`).join('\\n');\n    super(`Spec file(s) already exist:\\n${list}\\nUse --force to overwrite.`);\n    this.name = 'SpecExistsError';\n    this.paths = paths;\n  }\n}\n\n/**\n * Check whether a file exists at the given path.\n */\nasync function fileExists(filePath: string): Promise<boolean> {\n  try {\n    await access(filePath, constants.F_OK);\n    return true;\n  } catch {\n    return false;\n  }\n}\n\n/**\n * Sanitize a heading string into a filename-safe slug.\n *\n * Lowercases, replaces whitespace with hyphens, strips non-alphanumeric\n * characters (except hyphens), collapses consecutive hyphens, and trims\n * leading/trailing hyphens.\n */\nfunction slugify(heading: string): string {\n  return heading\n    .toLowerCase()\n    .replace(/\\s+/g, '-')\n    .replace(/[^a-z0-9-]/g, '')\n    .replace(/-+/g, '-')\n    .replace(/^-|-$/g, '');\n}\n\n/**\n * Split markdown content on top-level `# ` headings into named sections.\n *\n * Returns an array of `{ filename, content }` pairs. Any content before\n * the first `# ` heading is placed into `00-preamble.md`.\n */\nfunction splitByHeadings(content: string): Array<{ filename: string; content: string }> {\n  const sections: Array<{ filename: string; content: string }> = [];\n\n  // Split on lines that start with exactly \"# \" (top-level heading)\n  const parts = content.split(/^(?=# )/m);\n\n  for (const part of parts) {\n    const trimmed = part.trimEnd();\n    if (!trimmed) continue;\n\n    const headingMatch = trimmed.match(/^# (.+)/);\n    if (headingMatch) {\n      const slug = slugify(headingMatch[1]);\n      const filename = slug ? `${slug}.md` : '00-preamble.md';\n      sections.push({ filename, content: trimmed + '\\n' });\n    } else {\n      // Content before the first heading\n      sections.push({ filename: '00-preamble.md', content: trimmed + '\\n' });\n    }\n  }\n\n  return sections;\n}\n\n/**\n * Write spec output to disk with overwrite protection.\n *\n * In single-file mode, writes content directly to `outputPath`.\n * In multi-file mode, splits content on top-level `# ` headings and\n * writes each section to a separate file in the directory of `outputPath`.\n *\n * @param content - The full AI-generated spec markdown\n * @param options - Output path, force flag, and multi-file mode\n * @returns Array of absolute paths to all written files\n * @throws SpecExistsError if files exist and force=false\n */\nexport async function writeSpec(content: string, options: WriteSpecOptions): Promise<string[]> {\n  const { outputPath, force, multiFile } = options;\n\n  if (!multiFile) {\n    // Single-file mode\n    if (!force && await fileExists(outputPath)) {\n      throw new SpecExistsError([outputPath]);\n    }\n\n    await mkdir(path.dirname(outputPath), { recursive: true });\n    await writeFile(outputPath, content, 'utf-8');\n    return [outputPath];\n  }\n\n  // Multi-file mode: split on top-level headings\n  const outputDir = path.dirname(outputPath);\n  const sections = splitByHeadings(content);\n\n  // Check all target files for existence before writing any\n  if (!force) {\n    const conflicts: string[] = [];\n    for (const section of sections) {\n      const filePath = path.join(outputDir, section.filename);\n      if (await fileExists(filePath)) {\n        conflicts.push(filePath);\n      }\n    }\n    if (conflicts.length > 0) {\n      throw new SpecExistsError(conflicts);\n    }\n  }\n\n  // Create output directory and write all sections\n  await mkdir(outputDir, { recursive: true });\n\n  const writtenPaths: string[] = [];\n  for (const section of sections) {\n    const filePath = path.join(outputDir, section.filename);\n    await writeFile(filePath, section.content, 'utf-8');\n    writtenPaths.push(filePath);\n  }\n\n  return writtenPaths;\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**writer.ts implements spec output file writing with overwrite protection, single/multi-file mode support, and markdown heading-based content splitting.**\n\n## Exported Interface\n\n`WriteSpecOptions` interface defines output control:\n- `outputPath: string` — absolute path to output file (e.g., `/project/specs/SPEC.md`)\n- `force: boolean` — overwrite existing files without throwing `SpecExistsError`\n- `multiFile: boolean` — split AI output into separate files by top-level `# ` headings\n\n## Exported Error Class\n\n`SpecExistsError` extends `Error` and is thrown when `writeSpec()` detects existing files and `force=false`:\n- `paths: string[]` — readonly array of conflicting file paths\n- Constructor formats paths as bulleted list with instruction: `\"Use --force to overwrite.\"`\n- `name` property set to `'SpecExistsError'` for discriminated error handling\n\n## Main Export Function\n\n`writeSpec(content: string, options: WriteSpecOptions): Promise<string[]>` writes spec output with two modes:\n\n**Single-file mode** (`multiFile=false`):\n- Checks `fileExists(outputPath)` via `access(filePath, constants.F_OK)` and throws `SpecExistsError([outputPath])` if exists and `force=false`\n- Creates parent directory via `mkdir(path.dirname(outputPath), { recursive: true })`\n- Writes `content` to `outputPath` with `writeFile(outputPath, content, 'utf-8')`\n- Returns `[outputPath]` array\n\n**Multi-file mode** (`multiFile=true`):\n- Calls `splitByHeadings(content)` to partition content by top-level `# ` headings\n- Returns `Array<{ filename: string; content: string }>` with slugified filenames\n- Pre-flight checks all target paths `path.join(path.dirname(outputPath), section.filename)` for existence\n- Throws `SpecExistsError(conflicts)` with all conflicting paths if any exist and `force=false`\n- Creates output directory `mkdir(outputDir, { recursive: true })`\n- Writes each section via `writeFile(filePath, section.content, 'utf-8')`\n- Returns array of all written absolute paths\n\n## Content Splitting Logic\n\n`splitByHeadings(content: string): Array<{ filename: string; content: string }>` partitions markdown:\n- Splits on regex `/^(?=# )/m` matching lines starting with exactly `# ` (top-level headings)\n- Iterates parts, extracts heading text via `/^# (.+)/` match\n- Passes heading text to `slugify()` for filename generation\n- Content before first `# ` heading assigned to `'00-preamble.md'`\n- Trims trailing whitespace via `trimEnd()` and appends single newline to each section's `content`\n\n## Filename Sanitization\n\n`slugify(heading: string): string` transforms heading into filesystem-safe slug:\n- Lowercases entire string\n- Replaces whitespace sequences (`/\\s+/g`) with single hyphen\n- Strips non-alphanumeric characters except hyphens (`/[^a-z0-9-]/g` → empty string)\n- Collapses consecutive hyphens (`/-+/g` → single hyphen)\n- Trims leading/trailing hyphens via `/^-|-$/g` replacement\n- Returns slugified string used as `${slug}.md` filename\n\n## File Existence Check\n\n`fileExists(filePath: string): Promise<boolean>` wraps Node.js `fs/promises`:\n- Calls `access(filePath, constants.F_OK)` to test file accessibility\n- Returns `true` on success, `false` on caught exception (file not found or inaccessible)\n\n## Dependencies\n\nImports from Node.js core:\n- `writeFile`, `mkdir`, `access` from `node:fs/promises`\n- `constants` from `node:fs` (specifically `constants.F_OK`)\n- `path` module for `path.dirname()` and `path.join()`\n\n## Integration Points\n\nCalled by `src/specify/index.ts` `generateProjectSpec()` with options from CLI:\n- `outputPath` derived from project root and `--output` flag (default `specs/SPEC.md`)\n- `force` flag from `--force` CLI option\n- `multiFile` flag from `--multi-file` CLI option\n- Caller catches `SpecExistsError` and logs `error.message` to stderr",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 1098,
      "cacheReadTokens": 17367,
      "cacheCreationTokens": 10475,
      "latencyMs": 30484,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/specify/writer.ts",
          "sizeBytes": 4490
        }
      ]
    },
    {
      "timestamp": "2026-02-09T17:45:47.895Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/docs/INPUT.md\n\n```markdown\n# Agents Reverse Engineer:\n\nA light-weight and powerful tool to reverse engineer a brownfield project and generate adequate AGENTS.md and CLAUDE.md among others... Compatible with Claude Code, OpenCode and many other agent tools.\n\n## Inception\n\nAgents Reverse Engineer was born out of the need to streamline the process of documenting and understanding existing projects. It aims to provide agents with a clear and concise overview of their codebase, making it easier to maintain the project over time.\nIt can be used in conjunction with other tools to enhance the overall development workflow like:\n- SpecKit: https://github.com/github/spec-kit\n- BMAD: https://github.com/bmad-code-org/BMAD-METHOD\n- Get Shit Done (GSD): https://github.com/glittercowboy/get-shit-done\n\n## Features\n\n- It should provide a command that will execute script usin Recursive Language Model with Claude Calude or and other LLM Agents tool. For Claude, it will be a command, ffor other, the available alternative.\n- It should be possible to add a hook on the end of a session to update the impacted files\n- It should generate AGENTS.md file in every directory of the structure\n- Each AGENTS.md file will be describing the content of the current directory and sub-structure. They can reference other files like ARCHITECTURE.md, STRUCTURE.md, STACK.md, INTEGRATIONS.md, INFRASTRUCTURE.md, CONVENTIONS.md, TESTING.md, PATTERNS.md, CONCERNS.md, etc...\n- The RLM will work as follow:\n    * build the project structure tree\n    * with start by executing the call at the first leaf and build recursively backward\n    * since the leaf is a file, analyse the file and generate a summary {filename}.sum\n    * when all leaf of a directory are summarized, analyse the directory and generate AGENTS.md file and other files if needed\n    * continue recursively until the root of the project is reached\n\n## Reaserch\n\nAnalyse in details GSD and BMAD.\n\nWe'll use a similar repository structure mainly to GSD and a bit of BMAD.\n\nLook at how they are approaching a brownfiled project, speical command and details about how they are doing. Check their codebase in details if needed.\n\n## Usage\n\nTo use Agents Reverse Engineer, simply run the following command in your terminal:\n\n```bash\n/are-generate\n/are-update\n```\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**INPUT.md documents the original project vision and Recursive Language Model (RLM) algorithm specification that defines agents-reverse-engineer's three-phase brownfield documentation strategy.**\n\n## Vision Statement\n\nDescribes agents-reverse-engineer as a lightweight tool for generating `AGENTS.md`, `CLAUDE.md`, and platform-specific documentation from brownfield codebases. Compatible with Claude Code, OpenCode, and other agent tools. Created to provide agents with clear codebase overviews for long-term maintenance.\n\n## Inspirational Projects\n\nReferences three complementary tools for integration context:\n- **SpecKit**: https://github.com/github/spec-kit — GitHub's specification toolkit\n- **BMAD**: https://github.com/bmad-code-org/BMAD-METHOD — BMAD methodology framework\n- **Get Shit Done (GSD)**: https://github.com/glittercowboy/get-shit-done — GSD workflow system\n\nInstructs detailed analysis of GSD and BMAD approaches to brownfield projects, repository structure patterns, special commands, and implementation details via codebase inspection.\n\n## Feature Requirements\n\n**Command execution:**\n- Implement RLM algorithm via Claude Code commands or platform-specific alternatives for other LLM agent tools\n\n**Session lifecycle:**\n- Hook mechanism on session end to update impacted files automatically\n\n**Directory documentation:**\n- Generate `AGENTS.md` in every project directory describing current directory content and substructure\n- Support references to specialized documentation: `ARCHITECTURE.md`, `STRUCTURE.md`, `STACK.md`, `INTEGRATIONS.md`, `INFRASTRUCTURE.md`, `CONVENTIONS.md`, `TESTING.md`, `PATTERNS.md`, `CONCERNS.md`\n\n## RLM Algorithm Specification\n\n**Execution flow:**\n1. Build project structure tree via directory traversal\n2. Execute analysis starting at first leaf node, proceeding recursively backward (post-order traversal)\n3. **File-level analysis**: Generate `{filename}.sum` summary file for each source file leaf\n4. **Directory-level synthesis**: When all leaves in directory are summarized, analyze directory and generate `AGENTS.md` plus additional referenced documentation files if needed\n5. **Root convergence**: Continue recursive aggregation until project root is reached\n\n## CLI Commands\n\nDefines two primary user-facing commands:\n- `/are-generate` — Full documentation generation from scratch\n- `/are-update` — Incremental update of impacted files",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 558,
      "cacheReadTokens": 17367,
      "cacheCreationTokens": 9660,
      "latencyMs": 19580,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "docs/INPUT.md",
          "sizeBytes": 2272
        }
      ]
    },
    {
      "timestamp": "2026-02-09T17:45:44.141Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/update/orphan-cleaner.ts\n\n```typescript\n/**\n * Orphan cleanup for stale .sum and AGENTS.md files\n *\n * Handles:\n * - Deleting .sum files when source files are deleted\n * - Deleting .sum files for renamed files (at old path)\n * - Deleting AGENTS.md from directories with no remaining source files\n */\nimport { unlink, readdir, stat } from 'node:fs/promises';\nimport * as path from 'node:path';\nimport type { FileChange } from '../change-detection/types.js';\nimport type { CleanupResult } from './types.js';\n\n/**\n * Files to ignore when checking if a directory has source files.\n * These are generated files, not source files.\n */\nconst GENERATED_FILES = new Set([\n  'AGENTS.md',\n  'CLAUDE.md',\n]);\n\n/**\n * Clean up orphaned .sum files and empty AGENTS.md files.\n *\n * @param projectRoot - Absolute path to project root\n * @param changes - List of file changes (deleted and renamed files need cleanup)\n * @param dryRun - If true, don't actually delete files\n * @returns Cleanup result with lists of deleted files\n */\nexport async function cleanupOrphans(\n  projectRoot: string,\n  changes: FileChange[],\n  dryRun: boolean = false\n): Promise<CleanupResult> {\n  const result: CleanupResult = {\n    deletedSumFiles: [],\n    deletedAgentsMd: [],\n  };\n\n  // Collect paths that need .sum cleanup\n  const pathsToClean: string[] = [];\n\n  for (const change of changes) {\n    if (change.status === 'deleted') {\n      pathsToClean.push(change.path);\n    } else if (change.status === 'renamed' && change.oldPath) {\n      // For renames, clean up the old path's .sum\n      pathsToClean.push(change.oldPath);\n    }\n  }\n\n  // Delete .sum and .annex.md files for deleted/renamed source files\n  for (const relativePath of pathsToClean) {\n    const sumPath = path.join(projectRoot, `${relativePath}.sum`);\n    const deleted = await deleteIfExists(sumPath, dryRun);\n    if (deleted) {\n      result.deletedSumFiles.push(relativePath + '.sum');\n    }\n    const annexPath = path.join(projectRoot, `${relativePath}.annex.md`);\n    const annexDeleted = await deleteIfExists(annexPath, dryRun);\n    if (annexDeleted) {\n      result.deletedSumFiles.push(relativePath + '.annex.md');\n    }\n  }\n\n  // Collect affected directories for AGENTS.md cleanup\n  const affectedDirs = new Set<string>();\n  for (const relativePath of pathsToClean) {\n    const dir = path.dirname(relativePath);\n    if (dir && dir !== '.') {\n      affectedDirs.add(dir);\n    }\n  }\n\n  // Check each affected directory for empty AGENTS.md\n  for (const dir of affectedDirs) {\n    const dirPath = path.join(projectRoot, dir);\n    const cleaned = await cleanupEmptyDirectoryDocs(dirPath, dryRun);\n    if (cleaned) {\n      result.deletedAgentsMd.push(path.join(dir, 'AGENTS.md'));\n    }\n  }\n\n  return result;\n}\n\n/**\n * Delete a file if it exists.\n *\n * @returns true if file was deleted (or would be in dry run)\n */\nasync function deleteIfExists(filePath: string, dryRun: boolean): Promise<boolean> {\n  try {\n    await stat(filePath);\n    if (!dryRun) {\n      await unlink(filePath);\n    }\n    return true;\n  } catch {\n    // File doesn't exist\n    return false;\n  }\n}\n\n/**\n * Check if a directory has any source files remaining.\n * If not, delete its AGENTS.md.\n *\n * @param dirPath - Absolute path to directory\n * @param dryRun - If true, don't actually delete\n * @returns true if AGENTS.md was deleted\n */\nexport async function cleanupEmptyDirectoryDocs(\n  dirPath: string,\n  dryRun: boolean = false\n): Promise<boolean> {\n  try {\n    const entries = await readdir(dirPath);\n\n    // Check if directory has any source files\n    // Source files are: not .sum files, not generated docs, not hidden files\n    const hasSourceFiles = entries.some(entry => {\n      // Skip hidden files\n      if (entry.startsWith('.')) return false;\n      // Skip .sum and .annex.md files\n      if (entry.endsWith('.sum')) return false;\n      if (entry.endsWith('.annex.md')) return false;\n      // Skip known generated files\n      if (GENERATED_FILES.has(entry)) return false;\n      // Everything else counts as a source file\n      return true;\n    });\n\n    if (!hasSourceFiles) {\n      const agentsPath = path.join(dirPath, 'AGENTS.md');\n      return await deleteIfExists(agentsPath, dryRun);\n    }\n\n    return false;\n  } catch {\n    // Directory doesn't exist or can't be read\n    return false;\n  }\n}\n\n/**\n * Get list of directories that should have their AGENTS.md regenerated.\n *\n * Includes parent directories of changed files up to project root.\n *\n * @param changes - List of file changes (non-deleted files)\n * @returns Set of relative directory paths\n */\nexport function getAffectedDirectories(changes: FileChange[]): Set<string> {\n  const dirs = new Set<string>();\n\n  for (const change of changes) {\n    // Skip deleted files - they don't affect directory docs\n    if (change.status === 'deleted') continue;\n\n    // Add all parent directories up to root\n    let dir = path.dirname(change.path);\n    while (dir && dir !== '.' && !path.isAbsolute(dir)) {\n      dirs.add(dir);\n      dir = path.dirname(dir);\n    }\n    // Include root directory\n    dirs.add('.');\n  }\n\n  return dirs;\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**orphan-cleaner.ts removes stale `.sum`, `.annex.md`, and `AGENTS.md` files after source file deletions or renames during incremental updates.**\n\n## Exported Functions\n\n**`cleanupOrphans(projectRoot: string, changes: FileChange[], dryRun: boolean = false): Promise<CleanupResult>`** — orchestrates orphan cleanup by processing deleted and renamed files from `changes` array, deleting corresponding `.sum` and `.annex.md` files via `deleteIfExists()`, collecting affected directories via `path.dirname()` traversal, invoking `cleanupEmptyDirectoryDocs()` for each affected directory, and returning `CleanupResult` with `deletedSumFiles` and `deletedAgentsMd` arrays.\n\n**`cleanupEmptyDirectoryDocs(dirPath: string, dryRun: boolean = false): Promise<boolean>`** — reads directory entries via `readdir()`, determines if directory contains source files by filtering out hidden files (starting with `.`), `.sum` and `.annex.md` extensions, and entries in `GENERATED_FILES` set, deletes `AGENTS.md` via `deleteIfExists()` if no source files remain, returns `true` if deletion occurred.\n\n**`getAffectedDirectories(changes: FileChange[]): Set<string>`** — extracts unique directory paths requiring `AGENTS.md` regeneration by iterating `changes` array (skipping `status === 'deleted'` entries), walking parent directories via repeated `path.dirname()` calls until reaching `.` or absolute path, collecting all parent paths into `Set<string>`, including root directory `.`.\n\n## Cleanup Logic\n\n**Deletion trigger paths:** Extracts `change.path` for `status === 'deleted'` and `change.oldPath` for `status === 'renamed'` into `pathsToClean` array. For each path, constructs `.sum` path via `path.join(projectRoot, \\`${relativePath}.sum\\`)` and `.annex.md` path via `path.join(projectRoot, \\`${relativePath}.annex.md\\`)`, appends relative suffix (`.sum` or `.annex.md`) to `result.deletedSumFiles` on successful deletion.\n\n**Directory emptiness criteria:** `cleanupEmptyDirectoryDocs()` considers directory empty if no entries pass filter: not starting with `.`, not ending with `.sum` or `.annex.md`, not contained in `GENERATED_FILES` set (`['AGENTS.md', 'CLAUDE.md']`). Deletes `AGENTS.md` only when zero source files remain.\n\n**Affected directory traversal:** `getAffectedDirectories()` walks upward from each changed file's `path.dirname()` until `dir === '.'` or `path.isAbsolute(dir)` returns `true`, ensuring all intermediate directories and root (`.`) appear in returned `Set<string>`.\n\n## File System Operations\n\n**`deleteIfExists(filePath: string, dryRun: boolean): Promise<boolean>`** — internal helper calling `stat()` to verify file existence, executing `unlink()` if `dryRun === false`, returning `true` on successful deletion or when file exists in dry-run mode, returning `false` on stat error (file does not exist). All file system errors caught and interpreted as nonexistence.\n\n## Types\n\n**`FileChange`** — imported from `../change-detection/types.js`, discriminated union with `status: 'added' | 'modified' | 'deleted' | 'renamed'`, `path: string`, optional `oldPath?: string` (present when `status === 'renamed'`).\n\n**`CleanupResult`** — imported from `./types.js`, object shape `{ deletedSumFiles: string[], deletedAgentsMd: string[] }` accumulating relative paths of deleted artifacts.\n\n## Constants\n\n**`GENERATED_FILES`** — `Set<string>` containing `'AGENTS.md'` and `'CLAUDE.md'`, used by `cleanupEmptyDirectoryDocs()` to exclude generated documentation files from source file detection, preventing false positives when determining directory emptiness.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 978,
      "cacheReadTokens": 17367,
      "cacheCreationTokens": 10670,
      "latencyMs": 25893,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/update/orphan-cleaner.ts",
          "sizeBytes": 5065
        }
      ]
    },
    {
      "timestamp": "2026-02-09T17:45:58.695Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/version.ts\n\n```typescript\nimport { readFileSync } from 'node:fs';\nimport { dirname, join } from 'node:path';\nimport { fileURLToPath } from 'node:url';\n\n/**\n * Get package version from package.json.\n */\nexport function getVersion(): string {\n  try {\n    const __dirname = dirname(fileURLToPath(import.meta.url));\n    const packagePath = join(__dirname, '..', 'package.json');\n    const packageJson = JSON.parse(readFileSync(packagePath, 'utf-8'));\n    return packageJson.version || 'unknown';\n  } catch {\n    return 'unknown';\n  }\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**getVersion() reads the package.json version field from the parent directory of the compiled module, returning 'unknown' on any error.**\n\n## Exported Function\n\n- `getVersion(): string` — Synchronously reads `../package.json` relative to the compiled module location, parses JSON, extracts `version` field, returns it or `'unknown'` if parsing/reading fails.\n\n## Implementation Details\n\nUses `import.meta.url` to resolve current module path via `fileURLToPath()`, then `dirname()` to get containing directory, constructs `join(__dirname, '..', 'package.json')` path pointing to project root package.json. Reads file synchronously with `readFileSync(packagePath, 'utf-8')`, parses via `JSON.parse()`, accesses `packageJson.version` with fallback `|| 'unknown'`. Wraps entire logic in try-catch returning `'unknown'` on any exception (file not found, invalid JSON, missing version field).\n\n## Integration Points\n\nCalled by CLI entry point (`src/cli/index.ts`) to display version in `--version` flag handler and by session lifecycle hooks (`hooks/are-check-update.js`) to compare local version against npm registry for update notifications.\n\n## Runtime Assumptions\n\nAssumes build output structure places compiled `version.js` in `dist/` directory with `package.json` at `dist/../package.json` (project root). ES module context required for `import.meta.url` support (Node.js ≥18.0.0).",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 352,
      "cacheReadTokens": 17367,
      "cacheCreationTokens": 9252,
      "latencyMs": 15484,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/version.ts",
          "sizeBytes": 506
        }
      ]
    },
    {
      "timestamp": "2026-02-09T17:45:55.951Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/hooks/opencode-are-check-update.js\n\n```javascript\n#!/usr/bin/env node\n/**\n * ARE Update Check Plugin for OpenCode\n *\n * Checks for ARE updates in background, writes result to cache.\n * OpenCode equivalent of the Claude/Gemini SessionStart hook.\n *\n * Cache file: ~/.config/opencode/cache/are-update-check.json\n */\n\nimport { existsSync, mkdirSync, readFileSync, writeFileSync } from 'fs';\nimport { homedir } from 'os';\nimport { join } from 'path';\nimport { spawn } from 'child_process';\n\nexport const AreCheckUpdate = async () => {\n  return {\n    event: {\n      'session.created': async () => {\n        const homeDir = homedir();\n        const cwd = process.cwd();\n        const cacheDir = join(homeDir, '.config', 'opencode', 'cache');\n        const cacheFile = join(cacheDir, 'are-update-check.json');\n\n        // ARE-VERSION file locations (check project first, then global)\n        const projectVersionFile = join(cwd, '.opencode', 'ARE-VERSION');\n        const globalVersionFile = join(homeDir, '.config', 'opencode', 'ARE-VERSION');\n\n        // Ensure cache directory exists\n        if (!existsSync(cacheDir)) {\n          mkdirSync(cacheDir, { recursive: true });\n        }\n\n        // Run check in background (spawn background process)\n        const child = spawn(process.execPath, ['-e', `\n          const fs = require('fs');\n          const { execSync } = require('child_process');\n\n          const cacheFile = ${JSON.stringify(cacheFile)};\n          const projectVersionFile = ${JSON.stringify(projectVersionFile)};\n          const globalVersionFile = ${JSON.stringify(globalVersionFile)};\n\n          // Check project directory first (local install), then global\n          let installed = '0.0.0';\n          try {\n            if (fs.existsSync(projectVersionFile)) {\n              installed = fs.readFileSync(projectVersionFile, 'utf8').trim();\n            } else if (fs.existsSync(globalVersionFile)) {\n              installed = fs.readFileSync(globalVersionFile, 'utf8').trim();\n            }\n          } catch (e) {}\n\n          let latest = null;\n          try {\n            latest = execSync('npm view agents-reverse-engineer version', { encoding: 'utf8', timeout: 10000, windowsHide: true }).trim();\n          } catch (e) {}\n\n          const result = {\n            update_available: latest && installed !== latest,\n            installed,\n            latest: latest || 'unknown',\n            checked: Math.floor(Date.now() / 1000)\n          };\n\n          fs.writeFileSync(cacheFile, JSON.stringify(result));\n        `], {\n          stdio: 'ignore',\n          detached: true,\n          windowsHide: true,\n        });\n\n        child.unref();\n      },\n    },\n  };\n};\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**OpenCode session lifecycle plugin that spawns detached background process on `session.created` event to check npm registry for ARE version updates and cache results to `~/.config/opencode/cache/are-update-check.json`.**\n\n## Exported Interface\n\n`AreCheckUpdate()` async factory function returns OpenCode plugin object with `event['session.created']` handler.\n\n## Version Resolution Strategy\n\nChecks project-local `<cwd>/.opencode/ARE-VERSION` first, falls back to global `~/.config/opencode/ARE-VERSION` to read installed version string. Defaults to `'0.0.0'` if neither file exists.\n\n## Background Process Execution\n\nSpawns detached child process via `spawn(process.execPath, ['-e', scriptString], { stdio: 'ignore', detached: true, windowsHide: true })` with serialized inline script string. Script injects JSON-stringified path constants via template substitution: `JSON.stringify(cacheFile)`, `JSON.stringify(projectVersionFile)`, `JSON.stringify(globalVersionFile)`.\n\n## npm Registry Query\n\nExecutes `npm view agents-reverse-engineer version` via `execSync()` with 10s timeout (`{ encoding: 'utf8', timeout: 10000, windowsHide: true }`). Returns latest version string or `'unknown'` on error.\n\n## Cache Output Format\n\nWrites JSON object to cache file with keys: `update_available` (boolean comparing `installed !== latest`), `installed` (version string), `latest` (version string or `'unknown'`), `checked` (Unix epoch seconds from `Math.floor(Date.now() / 1000)`).\n\n## Error Handling\n\nTry-catch blocks around `readFileSync()` for version files and `execSync()` for npm query suppress all errors, allowing graceful degradation with defaults (`installed='0.0.0'`, `latest='unknown'`).\n\n## Cache Directory Creation\n\nEnsures `~/.config/opencode/cache` exists via `mkdirSync(cacheDir, { recursive: true })` before spawning background process. Directory resolution uses `homedir()` from `os` module.\n\n## Process Lifecycle\n\n`child.unref()` detaches plugin from child process lifetime, allowing OpenCode session to exit without waiting for npm query completion. Background process writes cache asynchronously after plugin handler returns.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 551,
      "cacheReadTokens": 17367,
      "cacheCreationTokens": 9837,
      "latencyMs": 18230,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "hooks/opencode-are-check-update.js",
          "sizeBytes": 2627
        }
      ]
    },
    {
      "timestamp": "2026-02-09T17:45:49.537Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/hooks/are-check-update.js\n\n```javascript\n#!/usr/bin/env node\n/**\n * ARE Update Check Hook\n *\n * Checks for ARE updates in background, writes result to cache.\n * Called by SessionStart hook - runs once per session.\n *\n * Cache file: ~/.claude/cache/are-update-check.json\n */\n\nimport { existsSync, mkdirSync, readFileSync, writeFileSync } from 'fs';\nimport { homedir } from 'os';\nimport { join } from 'path';\nimport { spawn, execSync } from 'child_process';\n\nconst homeDir = homedir();\nconst cwd = process.cwd();\nconst cacheDir = join(homeDir, '.claude', 'cache');\nconst cacheFile = join(cacheDir, 'are-update-check.json');\n\n// ARE-VERSION file locations (check project first, then global)\nconst projectVersionFile = join(cwd, '.claude', 'ARE-VERSION');\nconst globalVersionFile = join(homeDir, '.claude', 'ARE-VERSION');\n\n// Ensure cache directory exists\nif (!existsSync(cacheDir)) {\n  mkdirSync(cacheDir, { recursive: true });\n}\n\n// Run check in background (spawn background process)\nconst child = spawn(process.execPath, ['-e', `\n  const fs = require('fs');\n  const { execSync } = require('child_process');\n\n  const cacheFile = ${JSON.stringify(cacheFile)};\n  const projectVersionFile = ${JSON.stringify(projectVersionFile)};\n  const globalVersionFile = ${JSON.stringify(globalVersionFile)};\n\n  // Check project directory first (local install), then global\n  let installed = '0.0.0';\n  try {\n    if (fs.existsSync(projectVersionFile)) {\n      installed = fs.readFileSync(projectVersionFile, 'utf8').trim();\n    } else if (fs.existsSync(globalVersionFile)) {\n      installed = fs.readFileSync(globalVersionFile, 'utf8').trim();\n    }\n  } catch (e) {}\n\n  let latest = null;\n  try {\n    latest = execSync('npm view agents-reverse-engineer version', { encoding: 'utf8', timeout: 10000, windowsHide: true }).trim();\n  } catch (e) {}\n\n  const result = {\n    update_available: latest && installed !== latest,\n    installed,\n    latest: latest || 'unknown',\n    checked: Math.floor(Date.now() / 1000)\n  };\n\n  fs.writeFileSync(cacheFile, JSON.stringify(result));\n`], {\n  stdio: 'ignore',\n  detached: true,\n  windowsHide: true,\n});\n\nchild.unref();\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**are-check-update.js spawns detached background process to check npm registry for agents-reverse-engineer updates, caches result to ~/.claude/cache/are-update-check.json for SessionStart hook consumption.**\n\n## Execution Model\n\nShebang `#!/usr/bin/env node` enables direct execution. Spawns detached child process via `spawn(process.execPath, ['-e', scriptString], { stdio: 'ignore', detached: true, windowsHide: true })` followed by `child.unref()` to allow parent process exit without blocking. Background process runs asynchronously, parent exits immediately after spawn.\n\n## Version File Resolution\n\nChecks two ARE-VERSION file locations in priority order:\n1. `projectVersionFile = join(cwd, '.claude', 'ARE-VERSION')` — local project install\n2. `globalVersionFile = join(homeDir, '.claude', 'ARE-VERSION')` — global npm install\n\nBackground process reads first existing file via `readFileSync(versionFile, 'utf8').trim()`, defaults to `'0.0.0'` if neither exists.\n\n## Update Check Logic\n\nBackground child process executes `execSync('npm view agents-reverse-engineer version', { encoding: 'utf8', timeout: 10000, windowsHide: true })` to query npm registry for latest published version. Compares against installed version from ARE-VERSION file via inequality check `installed !== latest`. Timeout enforced at 10000ms to prevent indefinite hangs on network failures.\n\n## Cache File Format\n\nWrites JSON object to `cacheFile` (path: `~/.claude/cache/are-update-check.json`) with structure:\n```javascript\n{\n  update_available: boolean,  // true if latest !== installed\n  installed: string,          // version from ARE-VERSION file or '0.0.0'\n  latest: string,             // npm registry version or 'unknown' on error\n  checked: number             // Unix timestamp (seconds) via Math.floor(Date.now() / 1000)\n}\n```\n\nCache directory created via `mkdirSync(cacheDir, { recursive: true })` if not exists.\n\n## Error Handling\n\nTry-catch blocks in background process isolate failures:\n- Version file read failure defaults `installed` to `'0.0.0'`\n- npm registry query failure sets `latest` to `null`, then `'unknown'` in result object\n- Network timeout (10000ms) throws caught exception, triggers unknown fallback\n\nNo error propagation to parent process — all failures silently degrade to cached result with `update_available: false`.\n\n## Integration Points\n\nCalled by SessionStart hook (`~/.claude/hooks/session-start/are-check-update.js` symlink). Session initialization triggers check once per session. Cache file consumed by ARE CLI or future session-start handlers to display update notifications without blocking session startup.\n\n## Platform Compatibility\n\n`windowsHide: true` flag in both `spawn()` and `execSync()` suppresses console window creation on Windows. `process.execPath` resolves to Node.js binary path across platforms (Linux/macOS/Windows). Uses `homedir()` and `join()` for cross-platform path resolution.\n\n## Dependencies\n\n- `fs`: `existsSync`, `mkdirSync`, `readFileSync`, `writeFileSync`\n- `os`: `homedir`\n- `path`: `join`\n- `child_process`: `spawn`, `execSync`\n\nNo external npm dependencies — relies solely on Node.js built-in modules.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 822,
      "cacheReadTokens": 17367,
      "cacheCreationTokens": 9759,
      "latencyMs": 25014,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "hooks/are-check-update.js",
          "sizeBytes": 2097
        }
      ]
    },
    {
      "timestamp": "2026-02-09T17:45:56.906Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/hooks/opencode-are-session-end.js\n\n```javascript\n#!/usr/bin/env node\n/**\n * ARE Session End Plugin for OpenCode\n *\n * Triggers `are update` when session ends (if there are uncommitted changes).\n * OpenCode equivalent of the Claude/Gemini SessionEnd hook.\n *\n * Runs in background to avoid blocking session close.\n *\n * Disable via:\n * - Environment variable: ARE_DISABLE_HOOK=1\n * - Config file: hook_enabled: false in .agents-reverse-engineer.yaml\n */\n\nimport { execSync, spawn } from 'child_process';\nimport { existsSync, readFileSync } from 'fs';\n\nexport const AreSessionEnd = async () => {\n  return {\n    event: {\n      'session.deleted': async () => {\n        // Check for disable flag\n        if (process.env.ARE_DISABLE_HOOK === '1') return;\n\n        // Check config file for permanent disable\n        const configPath = '.agents-reverse-engineer.yaml';\n        if (existsSync(configPath)) {\n          const config = readFileSync(configPath, 'utf-8');\n          if (config.includes('hook_enabled: false')) return;\n        }\n\n        // Check git status - skip if no changes\n        try {\n          const status = execSync('git status --porcelain', { encoding: 'utf-8' });\n          if (!status.trim()) return;\n        } catch {\n          // Not a git repo or git not available - exit silently\n          return;\n        }\n\n        // Run update in background (don't block session close)\n        const child = spawn('npx', ['agents-reverse-engineer@latest', 'update', '--quiet'], {\n          stdio: 'ignore',\n          detached: true,\n        });\n        child.unref();\n      },\n    },\n  };\n};\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**opencode-are-session-end.js exports `AreSessionEnd` async factory returning an OpenCode plugin that triggers `npx agents-reverse-engineer@latest update --quiet` as a detached background process when `'session.deleted'` events fire, conditional on git working tree changes detected via `git status --porcelain`.**\n\n## Exported API\n\n`AreSessionEnd` — async factory function returning OpenCode plugin object with `event['session.deleted']` handler. No parameters. Returns `Promise<{ event: { 'session.deleted': () => Promise<void> } }>`.\n\n## Event Handler Logic\n\nHandler checks two disable mechanisms before executing update:\n- Environment variable: exits if `process.env.ARE_DISABLE_HOOK === '1'`\n- Config file: reads `.agents-reverse-engineer.yaml` via `existsSync` + `readFileSync`, exits if `config.includes('hook_enabled: false')` (substring search, no YAML parser)\n\nGit change detection via `execSync('git status --porcelain', { encoding: 'utf-8' })` wrapped in try-catch. Returns early if `status.trim()` empty or if `execSync` throws (non-git repo or missing git binary).\n\n## Background Process Spawning\n\nSpawns `npx agents-reverse-engineer@latest update --quiet` via `spawn()` with detached process pattern:\n```javascript\nspawn('npx', ['agents-reverse-engineer@latest', 'update', '--quiet'], {\n  stdio: 'ignore',\n  detached: true,\n})\n```\nCalls `.unref()` on child to allow parent process exit without waiting for completion. Prevents blocking OpenCode session close.\n\n## OpenCode Plugin Contract\n\nOpenCode plugins export async factory functions returning objects with `event` property mapping event names to async handlers. This module implements `'session.deleted'` lifecycle hook, equivalent to Claude/Gemini `SessionEnd` hook but using OpenCode's plugin system conventions.\n\n## Dependencies\n\nImports `execSync`, `spawn` from `child_process`, `existsSync`, `readFileSync` from `fs`. No external npm dependencies.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 495,
      "cacheReadTokens": 17367,
      "cacheCreationTokens": 9542,
      "latencyMs": 18059,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "hooks/opencode-are-session-end.js",
          "sizeBytes": 1550
        }
      ]
    },
    {
      "timestamp": "2026-02-09T17:45:10.904Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/orchestration/index.ts\n\n```typescript\n/**\n * Public API for the orchestration module.\n *\n * This barrel export is the single import point for the orchestration\n * engine. It re-exports the concurrency pool, progress reporter,\n * command runner, and all shared types.\n *\n * @module\n *\n * @example\n * ```typescript\n * import {\n *   CommandRunner,\n *   ProgressReporter,\n *   runPool,\n * } from './orchestration/index.js';\n *\n * const runner = new CommandRunner(aiService, { concurrency: 5 });\n * const summary = await runner.executeGenerate(plan);\n * ```\n */\n\n// ---------------------------------------------------------------------------\n// Types\n// ---------------------------------------------------------------------------\n\nexport type {\n  FileTaskResult,\n  RunSummary,\n  ProgressEvent,\n  CommandRunOptions,\n} from './types.js';\n\n// ---------------------------------------------------------------------------\n// Concurrency pool\n// ---------------------------------------------------------------------------\n\nexport { runPool } from './pool.js';\nexport type { PoolOptions, TaskResult } from './pool.js';\n\n// ---------------------------------------------------------------------------\n// Progress reporter\n// ---------------------------------------------------------------------------\n\nexport { ProgressReporter, ProgressLog } from './progress.js';\n\n// ---------------------------------------------------------------------------\n// Plan tracker\n// ---------------------------------------------------------------------------\n\nexport { PlanTracker } from './plan-tracker.js';\n\n// ---------------------------------------------------------------------------\n// Tracing\n// ---------------------------------------------------------------------------\n\nexport type { ITraceWriter, TraceEvent, TraceEventPayload } from './trace.js';\nexport { createTraceWriter, cleanupOldTraces } from './trace.js';\n\n// ---------------------------------------------------------------------------\n// Command runner\n// ---------------------------------------------------------------------------\n\nexport { CommandRunner } from './runner.js';\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**Barrel export consolidating orchestration module: `runPool`, `ProgressReporter`, `ProgressLog`, `PlanTracker`, `createTraceWriter`, `cleanupOldTraces`, `CommandRunner`, and shared types (`FileTaskResult`, `RunSummary`, `ProgressEvent`, `CommandRunOptions`, `PoolOptions`, `TaskResult`, `ITraceWriter`, `TraceEvent`, `TraceEventPayload`).**\n\n## Exported Types\n\n**FileTaskResult** — result of single file AI analysis with `path`, `success`, `tokensIn`, `tokensOut`, `cacheReadTokens`, `cacheCreationTokens`, `durationMs`, `model`, `error?`.\n\n**RunSummary** — aggregated run summary with `version`, `filesProcessed`, `filesFailed`, `filesSkipped`, `totalCalls`, `totalInputTokens`, `totalOutputTokens`, `totalCacheReadTokens`, `totalCacheCreationTokens`, `totalDurationMs`, `errorCount`, `retryCount`, `totalFilesRead`, `uniqueFilesRead`, `inconsistenciesCodeVsDoc?`, `inconsistenciesCodeVsCode?`, `phantomPaths?`, `inconsistencyReport?`.\n\n**ProgressEvent** — event emitted to `ProgressReporter` with discriminated `type` (`start` | `done` | `error` | `dir-done` | `root-done`), `filePath`, `index`, `total`, `durationMs?`, `tokensIn?`, `tokensOut?`, `model?`, `error?`.\n\n**CommandRunOptions** — execution options with `concurrency`, `failFast?`, `debug?`, `dryRun?`, `tracer?: ITraceWriter`, `progressLog?: ProgressLog`.\n\n**PoolOptions** — pool configuration with `concurrency`, `failFast?`, `tracer?: ITraceWriter`, `phaseLabel?`, `taskLabels?: string[]`.\n\n**TaskResult<T>** — pool task result with `index`, `success`, `value?: T`, `error?: Error`.\n\n**ITraceWriter** — interface for trace event emission with `emit(event: TraceEventPayload): void`, `finalize(): Promise<void>`, `readonly filePath: string`.\n\n**TraceEvent** — discriminated union of 14 event types: `PhaseStartEvent`, `PhaseEndEvent`, `WorkerStartEvent`, `WorkerEndEvent`, `TaskPickupEvent`, `TaskDoneEvent`, `TaskStartEvent`, `SubprocessSpawnEvent`, `SubprocessExitEvent`, `RetryEvent`, `DiscoveryStartEvent`, `DiscoveryEndEvent`, `FilterAppliedEvent`, `PlanCreatedEvent`, `ConfigLoadedEvent`.\n\n**TraceEventPayload** — `DistributiveOmit<TraceEvent, 'seq' | 'ts' | 'pid' | 'elapsedMs'>` for event payload without auto-populated base fields.\n\n## Exported Functions\n\n**runPool<T>(tasks: Array<() => Promise<T>>, options: PoolOptions, onComplete?: (result: TaskResult<T>) => void): Promise<TaskResult<T>[]>** — iterator-based concurrency pool executing tasks via shared iterator pattern with N workers, optional `onComplete` callback per task settlement, failFast abort on first error, trace event emission for `worker:start/end`, `task:pickup/done`.\n\n**createTraceWriter(projectRoot: string, enabled: boolean): ITraceWriter** — factory returning `NullTraceWriter` when disabled (zero overhead) or `TraceWriter` appending NDJSON to `.agents-reverse-engineer/traces/trace-{timestamp}.ndjson` with promise-chain serialization for concurrent-safe writes.\n\n**cleanupOldTraces(projectRoot: string, keepCount?: number): Promise<number>** — removes old trace files from `.agents-reverse-engineer/traces/`, keeps most recent `keepCount` (default 500), returns deletion count.\n\n## Exported Classes\n\n**ProgressReporter** — streaming build-log reporter with `onFileStart(filePath)`, `onFileDone(filePath, durationMs, tokensIn, tokensOut, model, cacheReadTokens?, cacheCreationTokens?)`, `onFileError(filePath, error)`, `onDirectoryStart(dirPath)`, `onDirectoryDone(dirPath, durationMs, tokensIn, tokensOut, model, cacheReadTokens?, cacheCreationTokens?)`, `onRootDone(docPath)`, `printSummary(summary: RunSummary)`. Computes ETA via moving average of last 10 completion times (window size 10), displays after 2+ completions. Constructor: `new ProgressReporter(totalFiles: number, totalDirectories?: number, progressLog?: ProgressLog)`.\n\n**ProgressLog** — file-based progress log mirroring console output to `.agents-reverse-engineer/progress.log` with ANSI stripping via regex `/\\x1b\\[[0-9;]*m/g`, promise-chain serialized writes, `write(line: string): void`, `finalize(): Promise<void>`. Static factory: `ProgressLog.create(projectRoot: string): ProgressLog`.\n\n**PlanTracker** — updates `GENERATION-PLAN.md` checkboxes during generation via `markDone(itemPath: string): void`, replaces `- [ ] \\`itemPath\\`` → `- [x] \\`itemPath\\``, serializes writes via promise chain, `flush(): Promise<void>`. Constructor: `new PlanTracker(projectRoot: string, initialMarkdown: string)`. Methods: `initialize(): Promise<void>` creates parent directory and writes initial content.\n\n**CommandRunner** — three-phase orchestrator with `executeGenerate(plan: ExecutionPlan): Promise<RunSummary>` (concurrent file analysis, post-order directory AGENTS.md, sequential root docs) and `executeUpdate(filesToAnalyze: FileChange[], projectRoot: string, config: Config): Promise<RunSummary>` (Phase 1 only, no directory/root generation). Constructor: `new CommandRunner(aiService: AIService, options: CommandRunOptions)`. Wires `tracer` into `AIService.setTracer()` for subprocess/retry events.\n\n## Integration Pattern\n\n`CommandRunner` orchestrates three-phase pipeline by invoking `runPool` for Phase 1 file tasks and Phase 2 directory tasks (grouped by depth), emitting trace events via `tracer?.emit()`, reporting progress via `ProgressReporter`, tracking GENERATION-PLAN.md checkboxes via `PlanTracker`, caching old `.sum` content for stale doc detection in Pre-Phase 1 (concurrency 20), running inconsistency checks post-Phase 1 via throttled directory groups (concurrency 10), validating phantom paths post-Phase 2, and sequentially executing Phase 3 root tasks with explicit `task:start/done` trace events.\n\n## Concurrency Control\n\nPhase 1 file analysis uses `this.options.concurrency` (user-configured). Phase 2 directory tasks group by depth and execute depth levels sequentially (descending order, deepest first) with `Math.min(concurrency, dirsAtDepth.length)` parallelism per depth level. Phase 3 root tasks execute sequentially (concurrency 1).\n\n## Trace Event Flow\n\n`CommandRunner.executeGenerate()` emits: `phase:start` (pre-phase-1-cache, phase-1-files, post-phase-1-quality, phase-2-dirs-depth-N, post-phase-2-quality, phase-3-root), `phase:end` with `tasksCompleted`/`tasksFailed`/`durationMs`. Phase 3 emits `task:start`/`task:done` per root document. `runPool` emits `worker:start`/`end`, `task:pickup`/`done`. `AIService` emits `subprocess:spawn`/`exit`, `retry`.\n\n## Quality Validation Timing\n\nPre-Phase 1: reads existing `.sum` files via `readSumFile()` into `oldSumCache`. Post-Phase 1: groups files by directory, throttles inconsistency checks at concurrency 10, compares old `.sum` (stale) and new `.sum` (LLM omissions) via `checkCodeVsDoc()`, runs `checkCodeVsCode()` per directory group, emits `formatReportForCli()` to stderr. Post-Phase 2: validates phantom paths in AGENTS.md via `checkPhantomPaths()`.\n\n## Preamble Stripping\n\n**stripPreamble(responseText: string): string** — removes LLM preamble via two patterns: (1) content after `\\n---\\n` separator within first 500 chars, (2) content starting with bold purpose line (`**[A-Z]`) preceded by <300 chars preamble without `##` headers.\n\n**extractPurpose(responseText: string): string** — skips lines matching preamble prefixes (`now i`, `perfect`, `based on`, `let me`, `here is`, `i'll`, `i will`, `great`, `okay`, `sure`, `certainly`, `alright`) or headers/separators, strips bold markdown wrapper `**...**`, truncates to 120 chars with `...` suffix.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 10,
      "outputTokens": 2619,
      "cacheReadTokens": 44227,
      "cacheCreationTokens": 38104,
      "latencyMs": 64529,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/orchestration/index.ts",
          "sizeBytes": 2076
        }
      ]
    },
    {
      "timestamp": "2026-02-09T17:45:55.789Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/hooks/are-session-end.js\n\n```javascript\n#!/usr/bin/env node\n/**\n * ARE Session End Hook\n *\n * Triggers `are update` when session ends (if there are uncommitted changes).\n * Runs in background to avoid blocking session close.\n *\n * Disable via:\n * - Environment variable: ARE_DISABLE_HOOK=1\n * - Config file: hook_enabled: false in .agents-reverse-engineer.yaml\n */\n\nimport { execSync, spawn } from 'child_process';\nimport { existsSync, readFileSync } from 'fs';\n\n// Check for disable flag\nif (process.env.ARE_DISABLE_HOOK === '1') {\n  process.exit(0);\n}\n\n// Check config file for permanent disable\nconst configPath = '.agents-reverse-engineer.yaml';\nif (existsSync(configPath)) {\n  const config = readFileSync(configPath, 'utf-8');\n  if (config.includes('hook_enabled: false')) {\n    process.exit(0);\n  }\n}\n\n// Check git status - skip if no changes\ntry {\n  const status = execSync('git status --porcelain', { encoding: 'utf-8' });\n  if (!status.trim()) {\n    // No changes since last run - exit silently\n    process.exit(0);\n  }\n} catch {\n  // Not a git repo or git not available - exit silently\n  process.exit(0);\n}\n\n// Run update in background (don't block session close)\nconst child = spawn('npx', ['agents-reverse-engineer@latest', 'update', '--quiet'], {\n  stdio: 'ignore',\n  detached: true,\n});\nchild.unref();\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**are-session-end.js executes background `are update` when Claude Code/Gemini sessions terminate, conditioned on git working tree changes and disable flags.**\n\n## Execution Context\n\nRuns as SessionEnd lifecycle hook registered in `~/.claude/hooks/` or `~/.gemini/hooks/`. Invoked automatically by IDE runtime when user closes session. Uses Node.js shebang `#!/usr/bin/env node` for direct execution without explicit interpreter invocation.\n\n## Disable Mechanisms\n\nExits with code 0 (silent success) when either condition is true:\n\n- Environment variable `ARE_DISABLE_HOOK` equals `'1'`\n- Config file `.agents-reverse-engineer.yaml` exists and contains substring `'hook_enabled: false'` (no YAML parser used, raw string search via `readFileSync()` + `String.includes()`)\n\n## Git Status Check\n\nInvokes `execSync('git status --porcelain', { encoding: 'utf-8' })` to detect uncommitted changes. Exits silently (code 0) if:\n\n- Output is empty after `String.trim()` (no changes since last commit)\n- `execSync()` throws (not a git repository or git command unavailable)\n\n## Background Update Invocation\n\nSpawns detached background process when changes detected:\n\n```javascript\nspawn('npx', ['agents-reverse-engineer@latest', 'update', '--quiet'], {\n  stdio: 'ignore',\n  detached: true,\n})\nchild.unref()\n```\n\nProcess characteristics:\n\n- `stdio: 'ignore'` — discards stdout/stderr/stdin (no TTY attachment)\n- `detached: true` — runs in separate process group, survives parent termination\n- `child.unref()` — allows Node.js event loop to exit without waiting for child completion\n- `npx agents-reverse-engineer@latest` — always fetches latest published version from npm registry\n- `--quiet` flag — suppresses progress output (writes to `.agents-reverse-engineer/progress.log` only)\n\n## Integration Points\n\n- **Installer**: Copied to hook directories via `src/installer/operations.ts` → `installCommands()` → hook file copying\n- **Build**: Duplicated to `hooks/dist/` via `scripts/build-hooks.js` for npm tarball inclusion\n- **Companion Hook**: `are-check-update.js` (SessionStart) notifies users of available updates, this hook (SessionEnd) performs automatic documentation refresh\n- **OpenCode Equivalent**: `opencode-are-session-end.js` wraps identical logic in plugin factory returning `event['session.deleted']` handler",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 620,
      "cacheReadTokens": 17367,
      "cacheCreationTokens": 9504,
      "latencyMs": 19804,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "hooks/are-session-end.js",
          "sizeBytes": 1276
        }
      ]
    },
    {
      "timestamp": "2026-02-09T17:45:57.108Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/scripts/build-hooks.js\n\n```javascript\n#!/usr/bin/env node\n/**\n * Build Hooks Script\n *\n * Copies hook source files from hooks/ to hooks/dist/ for npm bundling.\n * Run via: npm run build:hooks\n * Called automatically during: npm run prepublishOnly\n */\n\nimport { copyFileSync, existsSync, mkdirSync, readdirSync } from 'fs';\nimport { join, dirname } from 'path';\nimport { fileURLToPath } from 'url';\n\nconst __filename = fileURLToPath(import.meta.url);\nconst __dirname = dirname(__filename);\nconst projectRoot = join(__dirname, '..');\n\nconst HOOKS_SRC = join(projectRoot, 'hooks');\nconst HOOKS_DIST = join(projectRoot, 'hooks', 'dist');\n\n// Ensure dist directory exists\nif (!existsSync(HOOKS_DIST)) {\n  mkdirSync(HOOKS_DIST, { recursive: true });\n}\n\n// Copy all .js files from hooks/ to hooks/dist/\nconst hookFiles = readdirSync(HOOKS_SRC).filter(\n  (f) => f.endsWith('.js') && f !== 'dist'\n);\n\nconsole.log('Building hooks...');\nfor (const file of hookFiles) {\n  const src = join(HOOKS_SRC, file);\n  const dest = join(HOOKS_DIST, file);\n  copyFileSync(src, dest);\n  console.log(`  Copied: ${file} -> hooks/dist/${file}`);\n}\n\nconsole.log(`Done. ${hookFiles.length} hook(s) built.`);\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**build-hooks.js copies hook source files from hooks/ to hooks/dist/ for npm package distribution during prepublishOnly lifecycle.**\n\n## Execution Context\n\nInvoked via `npm run build:hooks` or automatically during `npm run prepublishOnly`. Runs as standalone Node.js script with `#!/usr/bin/env node` shebang.\n\n## File Operations\n\nResolves `projectRoot` via `fileURLToPath(import.meta.url)` and `dirname(__dirname)` to compute absolute paths. Defines `HOOKS_SRC = join(projectRoot, 'hooks')` and `HOOKS_DIST = join(projectRoot, 'hooks', 'dist')`.\n\nCreates `HOOKS_DIST` directory via `mkdirSync(HOOKS_DIST, { recursive: true })` if not present via `existsSync()` check.\n\nFilters hook files using `readdirSync(HOOKS_SRC).filter(f => f.endsWith('.js') && f !== 'dist')` to exclude `dist` directory itself and non-JavaScript files. Copies each filtered file via `copyFileSync(join(HOOKS_SRC, file), join(HOOKS_DIST, file))`.\n\n## Console Output\n\nLogs `Building hooks...` header, per-file progress `Copied: ${file} -> hooks/dist/${file}`, and completion summary `Done. ${hookFiles.length} hook(s) built.`.\n\n## Integration Points\n\nCalled by package.json `prepublishOnly` script which chains `npm run build && npm run build:hooks`. Ensures hooks/ directory contents (are-check-update.js, are-session-end.js, opencode-are-check-update.js, opencode-are-session-end.js) exist in hooks/dist/ for npm tarball inclusion referenced by installer/operations.ts hook installation logic.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 433,
      "cacheReadTokens": 17367,
      "cacheCreationTokens": 9485,
      "latencyMs": 20821,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "scripts/build-hooks.js",
          "sizeBytes": 1141
        }
      ]
    },
    {
      "timestamp": "2026-02-09T17:45:43.349Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/update/orchestrator.ts\n\n```typescript\n/**\n * Update orchestrator\n *\n * Coordinates incremental documentation updates using frontmatter-based change detection:\n * 1. Check git repository status\n * 2. Scan for existing .sum files\n * 3. Compare content hashes from frontmatter with current file hashes\n * 4. Clean up orphaned .sum files\n * 5. Prepare analysis tasks for changed files\n * 6. Track affected directories for AGENTS.md regeneration\n */\nimport * as path from 'node:path';\nimport pc from 'picocolors';\nimport type { Config } from '../config/schema.js';\nimport {\n  isGitRepo,\n  getCurrentCommit,\n  computeContentHash,\n  type FileChange,\n} from '../change-detection/index.js';\nimport { cleanupOrphans, getAffectedDirectories } from './orphan-cleaner.js';\nimport { readSumFile, getSumPath } from '../generation/writers/sum.js';\nimport type { UpdateOptions, CleanupResult } from './types.js';\nimport { discoverFiles as runDiscovery } from '../discovery/run.js';\nimport type { ITraceWriter } from '../orchestration/trace.js';\n\n/**\n * Result of update preparation (before analysis).\n */\nexport interface UpdatePlan {\n  /** Files to analyze (added or modified) */\n  filesToAnalyze: FileChange[];\n  /** Files to skip (unchanged based on content hash) */\n  filesToSkip: string[];\n  /** Cleanup result (files to delete) */\n  cleanup: CleanupResult;\n  /** Directories that need AGENTS.md regeneration */\n  affectedDirs: string[];\n  /** Base commit (not used in frontmatter mode, kept for compatibility) */\n  baseCommit: string;\n  /** Current commit */\n  currentCommit: string;\n  /** Whether this is first run (no .sum files exist) */\n  isFirstRun: boolean;\n}\n\n/**\n * Orchestrates incremental documentation updates using frontmatter-based change detection.\n */\nexport class UpdateOrchestrator {\n  private config: Config;\n  private projectRoot: string;\n  private tracer?: ITraceWriter;\n  private debug: boolean;\n\n  constructor(\n    config: Config,\n    projectRoot: string,\n    options?: { tracer?: ITraceWriter; debug?: boolean }\n  ) {\n    this.config = config;\n    this.projectRoot = projectRoot;\n    this.tracer = options?.tracer;\n    this.debug = options?.debug ?? false;\n  }\n\n  /**\n   * Close resources (no-op in frontmatter mode, kept for API compatibility).\n   */\n  close(): void {\n    // No database to close in frontmatter mode\n  }\n\n  /**\n   * Check prerequisites for update.\n   *\n   * @throws Error if not in a git repository\n   */\n  async checkPrerequisites(): Promise<void> {\n    const isRepo = await isGitRepo(this.projectRoot);\n    if (!isRepo) {\n      throw new Error(\n        `Not a git repository: ${this.projectRoot}\\n` +\n        'The update command requires a git repository for change detection.'\n      );\n    }\n  }\n\n  /**\n   * Discover all source files in the project.\n   */\n  private async discoverFiles(): Promise<string[]> {\n    const filterResult = await runDiscovery(this.projectRoot, this.config, {\n      tracer: this.tracer,\n      debug: this.debug,\n    });\n\n    // Walker returns absolute paths; convert to relative for consistent usage\n    return filterResult.included.map((f: string) => path.relative(this.projectRoot, f));\n  }\n\n  /**\n   * Prepare update plan without executing analysis.\n   *\n   * Uses frontmatter-based change detection:\n   * - Reads content_hash from each .sum file\n   * - Compares with current file content hash\n   * - Files with mismatched hashes need re-analysis\n   *\n   * @param options - Update options\n   * @returns Update plan with files to analyze and cleanup actions\n   */\n  async preparePlan(options: UpdateOptions = {}): Promise<UpdatePlan> {\n    const planStartTime = process.hrtime.bigint();\n\n    // Emit phase start\n    this.tracer?.emit({\n      type: 'phase:start',\n      phase: 'update-plan-creation',\n      taskCount: 0, // Will be determined after discovery\n      concurrency: 1,\n    });\n\n    if (this.debug) {\n      console.error(pc.dim('[debug] Creating update plan with change detection...'));\n    }\n\n    await this.checkPrerequisites();\n\n    // Get current commit for reference\n    const currentCommit = await getCurrentCommit(this.projectRoot);\n\n    if (this.debug) {\n      console.error(pc.dim(`[debug] Git commit: ${currentCommit.slice(0, 7)}`));\n    }\n\n    // Discover all source files\n    if (this.debug) {\n      console.error(pc.dim('[debug] Discovering files...'));\n    }\n\n    const allFiles = await this.discoverFiles();\n\n    const filesToAnalyze: FileChange[] = [];\n    const filesToSkip: string[] = [];\n    const deletedOrRenamed: FileChange[] = [];\n\n    // Track which .sum files we've seen (to detect orphans)\n    const seenSumFiles = new Set<string>();\n\n    // Check each file against its .sum file\n    for (const relativePath of allFiles) {\n      const filePath = path.join(this.projectRoot, relativePath);\n      const sumPath = getSumPath(filePath);\n      seenSumFiles.add(sumPath);\n\n      try {\n        // Read existing .sum file\n        const sumContent = await readSumFile(sumPath);\n\n        if (!sumContent) {\n          // No .sum file exists - file needs analysis\n          filesToAnalyze.push({ path: relativePath, status: 'added' });\n          continue;\n        }\n\n        // Compare content hashes\n        const currentHash = await computeContentHash(filePath);\n        const storedHash = sumContent.contentHash;\n\n        if (!storedHash || storedHash !== currentHash) {\n          // Hash mismatch or no hash stored - file needs re-analysis\n          filesToAnalyze.push({ path: relativePath, status: 'modified' });\n        } else {\n          // Hash matches - skip this file\n          filesToSkip.push(relativePath);\n        }\n      } catch {\n        // Error reading file - skip it\n        filesToSkip.push(relativePath);\n      }\n    }\n\n    // Cleanup orphans (deleted files whose .sum files still exist)\n    const cleanup = await cleanupOrphans(\n      this.projectRoot,\n      deletedOrRenamed,\n      options.dryRun ?? false\n    );\n\n    // Get directories affected by changes (for AGENTS.md regeneration)\n    // Sort by depth descending (deepest first) so children are processed before parents\n    const affectedDirs = Array.from(getAffectedDirectories(filesToAnalyze))\n      .sort((a, b) => {\n        const depthA = a === '.' ? 0 : a.split(path.sep).length;\n        const depthB = b === '.' ? 0 : b.split(path.sep).length;\n        return depthB - depthA;\n      });\n\n    if (this.debug) {\n      console.error(\n        pc.dim(\n          `[debug] Change detection: ${filesToAnalyze.length} changed, ${filesToSkip.length} unchanged, ${cleanup.deletedSumFiles.length} orphaned`\n        )\n      );\n      console.error(pc.dim(`[debug] Affected directories: ${affectedDirs.length}`));\n    }\n\n    // Emit plan created event\n    this.tracer?.emit({\n      type: 'plan:created',\n      planType: 'update',\n      fileCount: filesToAnalyze.length,\n      taskCount: filesToAnalyze.length + affectedDirs.length, // File tasks + dir regen tasks\n    });\n\n    // Emit phase end\n    const planEndTime = process.hrtime.bigint();\n    const planDurationMs = Number(planEndTime - planStartTime) / 1_000_000;\n    this.tracer?.emit({\n      type: 'phase:end',\n      phase: 'update-plan-creation',\n      durationMs: planDurationMs,\n      tasksCompleted: 1,\n      tasksFailed: 0,\n    });\n\n    // Determine if this is first run (no files to skip means no existing .sum files)\n    const isFirstRun = filesToSkip.length === 0 && filesToAnalyze.length > 0;\n    return {\n      filesToAnalyze,\n      filesToSkip,\n      cleanup,\n      affectedDirs,\n      baseCommit: currentCommit, // Not used in frontmatter mode\n      currentCommit,\n      isFirstRun,\n    };\n  }\n\n  /**\n   * Record file analyzed (no-op in frontmatter mode - hash is stored in .sum file).\n   * Kept for API compatibility.\n   */\n  async recordFileAnalyzed(\n    _relativePath: string,\n    _contentHash: string,\n    _currentCommit: string\n  ): Promise<void> {\n    // No-op: content hash is stored in .sum file frontmatter\n  }\n\n  /**\n   * Remove file from state (no-op in frontmatter mode).\n   * Kept for API compatibility.\n   */\n  async removeFileState(_relativePath: string): Promise<void> {\n    // No-op: .sum file cleanup is handled separately\n  }\n\n  /**\n   * Record a completed update run (no-op in frontmatter mode).\n   * Kept for API compatibility.\n   */\n  async recordRun(\n    _commitHash: string,\n    _filesAnalyzed: number,\n    _filesSkipped: number\n  ): Promise<number> {\n    // No-op: no run history in frontmatter mode\n    return 0;\n  }\n\n  /**\n   * Get last run information (not available in frontmatter mode).\n   * Kept for API compatibility.\n   */\n  async getLastRun(): Promise<undefined> {\n    // No run history in frontmatter mode\n    return undefined;\n  }\n\n  /**\n   * Check if this is the first run.\n   * In frontmatter mode, checks if any .sum files exist.\n   */\n  async isFirstRun(): Promise<boolean> {\n    const plan = await this.preparePlan({ dryRun: true });\n    return plan.isFirstRun;\n  }\n}\n\n/**\n * Create an update orchestrator.\n */\nexport function createUpdateOrchestrator(\n  config: Config,\n  projectRoot: string,\n  options?: { tracer?: ITraceWriter; debug?: boolean }\n): UpdateOrchestrator {\n  return new UpdateOrchestrator(config, projectRoot, options);\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**UpdateOrchestrator coordinates incremental documentation updates via SHA-256 content hash comparison stored in .sum file YAML frontmatter, orchestrating file change detection, orphan cleanup, and affected directory tracking without requiring git diff parsing.**\n\n## Exported Interface\n\n**UpdateOrchestrator** class constructor accepts `Config`, `projectRoot: string`, `options?: { tracer?: ITraceWriter; debug?: boolean }`. Exposes methods:\n\n- `checkPrerequisites(): Promise<void>` — throws if `projectRoot` is not a git repository via `isGitRepo()`\n- `preparePlan(options?: UpdateOptions): Promise<UpdatePlan>` — discovers all source files via `runDiscovery()`, reads existing `.sum` files via `readSumFile()`, compares stored `content_hash` from YAML frontmatter against `computeContentHash()` output, returns `UpdatePlan` with `filesToAnalyze` (added/modified), `filesToSkip` (unchanged hashes), `cleanup` (orphaned `.sum` files), `affectedDirs` (sorted by depth descending), `currentCommit`, `isFirstRun` flag\n- `recordFileAnalyzed(relativePath: string, contentHash: string, currentCommit: string): Promise<void>` — no-op, kept for API compatibility (hash stored in `.sum` frontmatter)\n- `removeFileState(relativePath: string): Promise<void>` — no-op, kept for API compatibility\n- `recordRun(commitHash: string, filesAnalyzed: number, filesSkipped: number): Promise<number>` — no-op returning `0`, kept for API compatibility\n- `getLastRun(): Promise<undefined>` — returns `undefined` (no run history in frontmatter mode), kept for API compatibility\n- `isFirstRun(): Promise<boolean>` — checks if any `.sum` files exist by calling `preparePlan({ dryRun: true })`\n- `close(): void` — no-op, kept for API compatibility\n\n**createUpdateOrchestrator** factory function returns new `UpdateOrchestrator` instance.\n\n## UpdatePlan Structure\n\n`UpdatePlan` interface defines:\n- `filesToAnalyze: FileChange[]` — files with hash mismatches or missing `.sum` files, status `'added'` or `'modified'`\n- `filesToSkip: string[]` — relative paths where stored hash matches current `computeContentHash()` result\n- `cleanup: CleanupResult` — output from `cleanupOrphans()` with `.sum` files to delete\n- `affectedDirs: string[]` — unique directories needing `AGENTS.md` regeneration via `getAffectedDirectories()`, sorted by `path.sep` depth descending (deepest first)\n- `baseCommit: string` — not used in frontmatter mode, kept for compatibility, set to `currentCommit`\n- `currentCommit: string` — SHA from `getCurrentCommit()`\n- `isFirstRun: boolean` — true when `filesToSkip.length === 0 && filesToAnalyze.length > 0`\n\n## Change Detection Algorithm\n\n`preparePlan()` executes:\n\n1. Calls `checkPrerequisites()` to verify git repository\n2. Obtains `currentCommit` via `getCurrentCommit(projectRoot)`\n3. Discovers all source files via `runDiscovery()`, converts absolute paths to relative via `path.relative(projectRoot, f)`\n4. Iterates each file:\n   - Computes `sumPath` via `getSumPath(filePath)`\n   - Adds `sumPath` to `seenSumFiles` set for orphan detection\n   - Calls `readSumFile(sumPath)` returning `SumFileContent | null`\n   - If no `.sum` exists, adds to `filesToAnalyze` with `status: 'added'`\n   - If `.sum` exists, calls `computeContentHash(filePath)` and compares against `sumContent.contentHash`\n   - Hash mismatch or missing hash → adds to `filesToAnalyze` with `status: 'modified'`\n   - Hash match → adds to `filesToSkip`\n5. Calls `cleanupOrphans(projectRoot, deletedOrRenamed, options.dryRun ?? false)` returning `CleanupResult`\n6. Calls `getAffectedDirectories(filesToAnalyze)` returning `Set<string>`, converts to sorted array via depth descending comparator\n7. Emits `plan:created` trace event with `fileCount`, `taskCount: filesToAnalyze.length + affectedDirs.length`\n\n## Trace Event Emissions\n\n`preparePlan()` emits:\n- `phase:start` with `phase: 'update-plan-creation'`, `taskCount: 0`, `concurrency: 1`\n- `plan:created` with `planType: 'update'`, `fileCount: filesToAnalyze.length`, `taskCount: filesToAnalyze.length + affectedDirs.length`\n- `phase:end` with `phase: 'update-plan-creation'`, `durationMs` computed via `process.hrtime.bigint()` delta, `tasksCompleted: 1`, `tasksFailed: 0`\n\n## Integration Points\n\nDepends on:\n- `discoverFiles` from `../discovery/run.js` returning `FilterResult` with `.included: string[]` absolute paths\n- `readSumFile` from `../generation/writers/sum.js` parsing YAML frontmatter, returning `SumFileContent` with `contentHash: string`\n- `getSumPath` from `../generation/writers/sum.js` computing `.sum` file path from source file path\n- `computeContentHash` from `../change-detection/index.js` returning SHA-256 hex string\n- `cleanupOrphans` from `./orphan-cleaner.js` accepting `projectRoot`, `deletedOrRenamed: FileChange[]`, `dryRun: boolean`, returning `CleanupResult`\n- `getAffectedDirectories` from `./orphan-cleaner.js` returning `Set<string>` of unique parent directories\n\n## Debug Logging\n\nWhen `debug: true`, writes to `console.error` via `pc.dim()`:\n- `[debug] Creating update plan with change detection...`\n- `[debug] Git commit: <currentCommit.slice(0, 7)>`\n- `[debug] Discovering files...`\n- `[debug] Change detection: N changed, N unchanged, N orphaned`\n- `[debug] Affected directories: N`\n\n## API Compatibility No-Ops\n\nMethods retained for backward compatibility but perform no operations in frontmatter-based mode:\n- `recordFileAnalyzed` — hash stored in `.sum` YAML frontmatter by generation phase\n- `removeFileState` — `.sum` file cleanup handled by `cleanupOrphans()`\n- `recordRun` — no run history database in frontmatter mode\n- `getLastRun` — returns `undefined`, no run history available\n- `close` — no resources to dispose\n\n## Error Handling\n\n`checkPrerequisites()` throws `Error` with message `'Not a git repository: <projectRoot>\\nThe update command requires a git repository for change detection.'` when `isGitRepo()` returns `false`.\n\nFile iteration in `preparePlan()` catches exceptions from `readSumFile()` or `computeContentHash()` and adds file to `filesToSkip` on error.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 1798,
      "cacheReadTokens": 17367,
      "cacheCreationTokens": 11821,
      "latencyMs": 35780,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/update/orchestrator.ts",
          "sizeBytes": 9165
        }
      ]
    },
    {
      "timestamp": "2026-02-09T17:46:03.962Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/LICENSE\n\n```text\nMIT License\n\nCopyright (c) 2026 GeoloeG-IsT\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**LICENSE declares MIT license grant for agents-reverse-engineer software, granting unrestricted usage, modification, and distribution rights to all recipients.**\n\n## License Terms\n\nGrants permission without restriction for:\n- use\n- copy\n- modify\n- merge\n- publish\n- distribute\n- sublicense\n- sell\n\nof the Software and associated documentation files.\n\n## Copyright Holder\n\nCopyright (c) 2026 GeoloeG-IsT\n\n## Warranty Disclaimer\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.\n\n## Liability Waiver\n\nIN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n## Inclusion Requirement\n\nThis copyright notice and permission notice shall be included in all copies or substantial portions of the Software.\n\n## Legal Effect\n\nThis is a standard MIT License text providing maximum permissiveness while disclaiming all warranties and liability. No exports, no executable code—purely a legal artifact required for npm package publication and downstream redistribution compliance.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 281,
      "cacheReadTokens": 0,
      "cacheCreationTokens": 26687,
      "latencyMs": 16291,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "LICENSE",
          "sizeBytes": 1068
        }
      ]
    },
    {
      "timestamp": "2026-02-09T17:46:03.425Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/LANGUAGES-MANIFEST.md\n\n```markdown\n# Package Manifest Files by Language\n\n| Language | Manifest File(s) | Package Manager | Supported |\n|----------|------------------|-----------------|-----------|\n| JavaScript/TypeScript | `package.json` | npm, yarn, pnpm | ✓ |\n| Python | `requirements.txt`, `pyproject.toml`, `setup.py`, `Pipfile` | pip, poetry, pipenv | ✓ |\n| Ruby | `Gemfile` | bundler | |\n| Go | `go.mod` | go modules | ✓ |\n| Rust | `Cargo.toml` | cargo | ✓ |\n| Java | `pom.xml` (Maven), `build.gradle` (Gradle) | Maven, Gradle | |\n| Kotlin | `build.gradle.kts`, `build.gradle` | Gradle | |\n| C#/.NET | `*.csproj`, `packages.config`, `*.fsproj` | NuGet | |\n| PHP | `composer.json` | Composer | |\n| Swift | `Package.swift` | Swift PM | |\n| Elixir | `mix.exs` | Mix | |\n| Erlang | `rebar.config` | rebar3 | |\n| Scala | `build.sbt` | sbt | |\n| Clojure | `deps.edn`, `project.clj` | clj, Leiningen | |\n| Haskell | `package.yaml`, `*.cabal`, `stack.yaml` | cabal, stack | |\n| Dart/Flutter | `pubspec.yaml` | pub | |\n| Lua | `*.rockspec` | LuaRocks | |\n| R | `DESCRIPTION` | CRAN | |\n| Julia | `Project.toml` | Pkg | |\n| Zig | `build.zig.zon` | zig | |\n| Nim | `*.nimble` | nimble | |\n| OCaml | `dune-project`, `*.opam` | dune, opam | |\n| C/C++ | `CMakeLists.txt`, `conanfile.txt`, `vcpkg.json` | CMake, Conan, vcpkg | |\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**LANGUAGES-MANIFEST.md enumerates package manifest file types across 25 programming language ecosystems with their associated package managers and current support status in ARE's manifest detection system.**\n\n## Manifest Detection Coverage\n\nThe table documents 9 supported manifest types used by `extractDirectoryImports()` and directory aggregation prompts:\n\n- **JavaScript/TypeScript**: `package.json` (npm/yarn/pnpm)\n- **Python**: `requirements.txt`, `pyproject.toml`, `setup.py`, `Pipfile` (pip/poetry/pipenv)\n- **Go**: `go.mod` (Go modules)\n- **Rust**: `Cargo.toml` (cargo)\n- **Java**: `pom.xml` (Maven), `build.gradle` (Gradle)\n- **PHP**: `composer.json` (Composer)\n- **C/C++**: `CMakeLists.txt`, `Makefile` (CMake/Make)\n\nSupport checkmarks (✓) indicate manifest types actively detected during Phase 2 directory aggregation to include dependency/build configuration context in `AGENTS.md` generation prompts.\n\n## Unsupported Ecosystems\n\nThe table lists 16 additional language ecosystems without current support:\n\n- **Ruby**: `Gemfile` (bundler)\n- **Kotlin**: `build.gradle.kts`, `build.gradle` (Gradle)\n- **C#/.NET**: `*.csproj`, `packages.config`, `*.fsproj` (NuGet)\n- **Swift**: `Package.swift` (Swift PM)\n- **Elixir**: `mix.exs` (Mix)\n- **Erlang**: `rebar.config` (rebar3)\n- **Scala**: `build.sbt` (sbt)\n- **Clojure**: `deps.edn`, `project.clj` (clj/Leiningen)\n- **Haskell**: `package.yaml`, `*.cabal`, `stack.yaml` (cabal/stack)\n- **Dart/Flutter**: `pubspec.yaml` (pub)\n- **Lua**: `*.rockspec` (LuaRocks)\n- **R**: `DESCRIPTION` (CRAN)\n- **Julia**: `Project.toml` (Pkg)\n- **Zig**: `build.zig.zon` (zig)\n- **Nim**: `*.nimble` (nimble)\n- **OCaml**: `dune-project`, `*.opam` (dune/opam)\n\nAdditional manifest support requires extending `src/generation/prompts/builder.ts` manifest detection logic and `src/generation/prompts/templates.ts` directory aggregation prompt to include discovered manifest content.\n\n## Reference Source\n\nThis table serves as specification document for cross-language manifest detection expansion. AI coding assistants implementing manifest-aware prompting should prioritize ecosystems marked with ✓ and treat unmarked entries as roadmap items requiring detection pattern additions to `extractDirectoryImports()` and conditional inclusion logic in directory aggregation prompt construction.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 720,
      "cacheReadTokens": 17367,
      "cacheCreationTokens": 9631,
      "latencyMs": 20431,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "LANGUAGES-MANIFEST.md",
          "sizeBytes": 1293
        }
      ]
    },
    {
      "timestamp": "2026-02-09T17:46:04.590Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/package.json\n\n```json\n{\n  \"name\": \"agents-reverse-engineer\",\n  \"version\": \"0.6.6\",\n  \"description\": \"CLI tool for reverse-engineering codebase documentation for AI agents\",\n  \"type\": \"module\",\n  \"bin\": {\n    \"agents-reverse-engineer\": \"dist/cli/index.js\",\n    \"are\": \"dist/cli/index.js\"\n  },\n  \"scripts\": {\n    \"build\": \"tsc\",\n    \"build:hooks\": \"node scripts/build-hooks.js\",\n    \"prepack\": \"rm -f LICENSE.sum README.md.sum\",\n    \"prepublishOnly\": \"npm run build && npm run build:hooks\",\n    \"dev\": \"tsx watch src/cli/index.ts\"\n  },\n  \"dependencies\": {\n    \"fast-glob\": \"^3.3.3\",\n    \"ignore\": \"^7.0.3\",\n    \"isbinaryfile\": \"^5.0.4\",\n    \"ora\": \"^8.1.1\",\n    \"picocolors\": \"^1.1.1\",\n    \"simple-git\": \"^3.27.0\",\n    \"yaml\": \"^2.7.0\",\n    \"zod\": \"^3.24.1\"\n  },\n  \"devDependencies\": {\n    \"@types/node\": \"^22.10.7\",\n    \"tsx\": \"^4.19.2\",\n    \"typescript\": \"^5.7.3\"\n  },\n  \"engines\": {\n    \"node\": \">=18.0.0\"\n  },\n  \"keywords\": [\n    \"documentation\",\n    \"codebase\",\n    \"ai\",\n    \"agents\",\n    \"reverse-engineering\"\n  ],\n  \"author\": \"GeoloeG-IsT\",\n  \"license\": \"MIT\",\n  \"main\": \"dist/cli/index.js\",\n  \"files\": [\n    \"dist\",\n    \"hooks/dist\",\n    \"README.md\",\n    \"LICENSE\"\n  ],\n  \"repository\": {\n    \"type\": \"git\",\n    \"url\": \"git+https://github.com/GeoloeG-IsT/agents-reverse-engineer.git\"\n  },\n  \"bugs\": {\n    \"url\": \"https://github.com/GeoloeG-IsT/agents-reverse-engineer/issues\"\n  },\n  \"homepage\": \"https://github.com/GeoloeG-IsT/agents-reverse-engineer#readme\"\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**package.json defines npm package metadata, CLI entry points, build scripts, runtime dependencies, and distribution configuration for the agents-reverse-engineer project.**\n\n## Package Identity\n\n`name` is `\"agents-reverse-engineer\"`, `version` is `\"0.6.6\"`, `description` is `\"CLI tool for reverse-engineering codebase documentation for AI agents\"`. `author` is `\"GeoloeG-IsT\"`, `license` is `\"MIT\"`. `type` is `\"module\"` (ES modules). Repository URL is `git+https://github.com/GeoloeG-IsT/agents-reverse-engineer.git`.\n\n## Binary Entry Points\n\n`bin` maps two CLI commands to the same entry point:\n- `\"agents-reverse-engineer\"` → `\"dist/cli/index.js\"`\n- `\"are\"` → `\"dist/cli/index.js\"`\n\nBoth invoke the compiled TypeScript CLI dispatcher from `src/cli/index.ts`.\n\n## Build Scripts\n\n`scripts` section defines lifecycle and development commands:\n- `\"build\"`: `\"tsc\"` — compiles TypeScript source to `dist/` via TypeScript compiler\n- `\"build:hooks\"`: `\"node scripts/build-hooks.js\"` — copies `hooks/` directory to `hooks/dist/` for npm tarball inclusion\n- `\"prepack\"`: `\"rm -f LICENSE.sum README.md.sum\"` — removes generated summary files before packaging to prevent accidental inclusion\n- `\"prepublishOnly\"`: `\"npm run build && npm run build:hooks\"` — ensures TypeScript compilation and hook distribution before npm publish\n- `\"dev\"`: `\"tsx watch src/cli/index.ts\"` — hot-reload development mode using tsx runtime\n\n## Runtime Dependencies\n\nProduction dependencies required at runtime:\n- `fast-glob` ^3.3.3 — file discovery with glob patterns\n- `ignore` ^7.0.3 — gitignore parsing for file filtering\n- `isbinaryfile` ^5.0.4 — binary file detection via extension and content analysis\n- `ora` ^8.1.1 — terminal spinner UI for progress indication\n- `picocolors` ^1.1.1 — ANSI terminal color formatting\n- `simple-git` ^3.27.0 — git diff parsing for change detection\n- `yaml` ^2.7.0 — YAML configuration file parsing\n- `zod` ^3.24.1 — schema validation for configuration\n\n## Development Dependencies\n\nBuild-time dependencies not included in published package:\n- `@types/node` ^22.10.7 — TypeScript type definitions for Node.js APIs\n- `tsx` ^4.19.2 — TypeScript execution and watch mode for development\n- `typescript` ^5.7.3 — TypeScript compiler\n\n## Distribution Configuration\n\n`files` array specifies npm tarball contents:\n- `\"dist\"` — compiled JavaScript output from TypeScript build\n- `\"hooks/dist\"` — session lifecycle hooks copied by `build:hooks` script\n- `\"README.md\"` — project documentation\n- `\"LICENSE\"` — MIT license text\n\nExcluded from tarball: source TypeScript files (`src/`), configuration files (`tsconfig.json`), build scripts (`scripts/`), original hooks (`hooks/*.js` without `dist/` prefix).\n\n## Engine Requirements\n\n`engines.node` specifies `\">=18.0.0\"` — requires Node.js version 18.0.0 or higher for ES module support and modern JavaScript features.\n\n## Package Metadata\n\n`keywords` array for npm search discoverability: `[\"documentation\", \"codebase\", \"ai\", \"agents\", \"reverse-engineering\"]`. `main` points to `\"dist/cli/index.js\"` as CommonJS entry point fallback. `bugs.url` directs to GitHub issues. `homepage` links to repository README.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 907,
      "cacheReadTokens": 17367,
      "cacheCreationTokens": 9692,
      "latencyMs": 24547,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "package.json",
          "sizeBytes": 1445
        }
      ]
    },
    {
      "timestamp": "2026-02-09T17:46:07.485Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/tsconfig.json\n\n```json\n{\n  \"compilerOptions\": {\n    \"target\": \"ES2022\",\n    \"module\": \"NodeNext\",\n    \"moduleResolution\": \"NodeNext\",\n    \"lib\": [\"ES2022\"],\n    \"outDir\": \"dist\",\n    \"rootDir\": \"src\",\n    \"strict\": true,\n    \"esModuleInterop\": true,\n    \"skipLibCheck\": true,\n    \"forceConsistentCasingInFileNames\": true,\n    \"declaration\": true,\n    \"declarationMap\": true,\n    \"sourceMap\": true,\n    \"resolveJsonModule\": true,\n    \"isolatedModules\": true\n  },\n  \"include\": [\"src/**/*\"],\n  \"exclude\": [\"node_modules\", \"dist\"]\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**tsconfig.json configures TypeScript compiler for ES2022/NodeNext module system with strict type-checking, declaration map generation, and source map output to the dist/ directory.**\n\n## Compiler Target and Module System\n\n- `target: \"ES2022\"` compiles TypeScript to ECMAScript 2022 syntax\n- `module: \"NodeNext\"` enables Node.js native ES module support with package.json \"type\": \"module\"\n- `moduleResolution: \"NodeNext\"` resolves imports using Node.js ESM algorithm (requires .js extensions in imports)\n- `lib: [\"ES2022\"]` provides ES2022 standard library type definitions\n\n## Output Configuration\n\n- `outDir: \"dist\"` emits compiled JavaScript to dist/ directory (mirrors src/ structure)\n- `rootDir: \"src\"` establishes src/ as compilation root (prevents ../node_modules pollution in dist/)\n- `declaration: true` generates .d.ts type declaration files alongside .js output\n- `declarationMap: true` creates .d.ts.map files for IDE \"Go to Definition\" navigation to source .ts files\n- `sourceMap: true` emits .js.map files for runtime debugging with original TypeScript line numbers\n\n## Type Safety Settings\n\n- `strict: true` enables all strict type-checking options (noImplicitAny, strictNullChecks, strictFunctionTypes, strictBindCallApply, strictPropertyInitialization, noImplicitThis, alwaysStrict, useUnknownInCatchVariables)\n- `forceConsistentCasingInFileNames: true` enforces case-sensitive imports (prevents cross-platform file resolution issues)\n- `skipLibCheck: true` skips type-checking of .d.ts files in node_modules (improves compile speed)\n- `isolatedModules: true` ensures each file can be transpiled independently (required for build tools like esbuild, enforces no const enums or ambient module declarations)\n\n## Module Interoperability\n\n- `esModuleInterop: true` enables default import syntax for CommonJS modules (import fs from 'fs' instead of import * as fs)\n- `resolveJsonModule: true` allows importing .json files with type inference (e.g., import pkg from './package.json')\n\n## Compilation Scope\n\n- `include: [\"src/**/*\"]` compiles all TypeScript files recursively under src/\n- `exclude: [\"node_modules\", \"dist\"]` prevents compilation of dependencies and previous build output\n\n## Build Integration\n\nReferenced by npm scripts in package.json:\n- `npm run build` invokes `tsc` which reads this configuration\n- `npm run dev` runs `tsx watch src/cli/index.ts` (bypasses tsc, uses tsx runtime transpiler)\n- `prepublishOnly` hook executes `npm run build` before publishing to npm registry\n\n## ES Module Constraints\n\nNodeNext module resolution requires:\n- Explicit .js extensions in relative imports (import { foo } from './bar.js' even though source is bar.ts)\n- Package.json with `\"type\": \"module\"` at project root\n- File extensions must match: .ts → .js, .mts → .mjs, .cts → .cjs",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 745,
      "cacheReadTokens": 17367,
      "cacheCreationTokens": 9273,
      "latencyMs": 22846,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "tsconfig.json",
          "sizeBytes": 506
        }
      ]
    },
    {
      "timestamp": "2026-02-09T17:45:20.388Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/src/orchestration/runner.ts\n\n```typescript\n/**\n * Three-phase command runner for AI-driven documentation generation.\n *\n * Wires together {@link AIService}, {@link ExecutionPlan}, the concurrency\n * pool, and the progress reporter into a cohesive execution engine.\n *\n * The three execution phases match the {@link ExecutionPlan} dependency graph:\n * 1. **File analysis** -- concurrent AI calls with configurable parallelism\n * 2. **Directory docs** -- concurrent per depth level, post-order AGENTS.md generation\n * 3. **Root documents** -- sequential AI calls for CLAUDE.md, ARCHITECTURE.md, etc.\n *\n * @module\n */\n\nimport * as path from 'node:path';\nimport { readFile, writeFile } from 'node:fs/promises';\nimport type { AIService } from '../ai/index.js';\nimport type { AIResponse } from '../ai/types.js';\nimport type { ExecutionPlan, ExecutionTask } from '../generation/executor.js';\nimport { writeSumFile, readSumFile, writeAnnexFile } from '../generation/writers/sum.js';\nimport type { SumFileContent } from '../generation/writers/sum.js';\nimport { writeAgentsMd } from '../generation/writers/agents-md.js';\nimport { computeContentHashFromString } from '../change-detection/index.js';\nimport type { FileChange } from '../change-detection/types.js';\nimport { buildFilePrompt, buildDirectoryPrompt, buildRootPrompt } from '../generation/prompts/index.js';\nimport type { Config } from '../config/schema.js';\nimport { CONFIG_DIR } from '../config/loader.js';\nimport {\n  checkCodeVsDoc,\n  checkCodeVsCode,\n  checkPhantomPaths,\n  buildInconsistencyReport,\n  formatReportForCli,\n} from '../quality/index.js';\nimport type { Inconsistency } from '../quality/index.js';\nimport { formatExecutionPlanAsMarkdown } from '../generation/executor.js';\nimport { runPool } from './pool.js';\nimport { PlanTracker } from './plan-tracker.js';\nimport { ProgressReporter } from './progress.js';\nimport type { ITraceWriter } from './trace.js';\nimport type {\n  FileTaskResult,\n  RunSummary,\n  CommandRunOptions,\n} from './types.js';\nimport { getVersion } from '../version.js';\n\n// ---------------------------------------------------------------------------\n// CommandRunner\n// ---------------------------------------------------------------------------\n\n/**\n * Orchestrates AI-driven documentation generation.\n *\n * Create one instance per command invocation. The runner holds references\n * to the AI service and run options, then executes plans or file lists\n * through the three-phase pipeline.\n *\n * @example\n * ```typescript\n * const runner = new CommandRunner(aiService, {\n *   concurrency: 5,\n *   failFast: false,\n * });\n *\n * const summary = await runner.executeGenerate(plan);\n * console.log(`Processed ${summary.filesProcessed} files`);\n * ```\n */\nexport class CommandRunner {\n  /** AI service instance for making calls */\n  private readonly aiService: AIService;\n\n  /** Command execution options */\n  private readonly options: CommandRunOptions;\n\n  /** Trace writer for concurrency debugging */\n  private readonly tracer: ITraceWriter | undefined;\n\n  /**\n   * Create a new command runner.\n   *\n   * @param aiService - The AI service instance (should be created per CLI run)\n   * @param options - Execution options (concurrency, failFast, etc.)\n   */\n  constructor(aiService: AIService, options: CommandRunOptions) {\n    this.aiService = aiService;\n    this.options = options;\n    this.tracer = options.tracer;\n\n    // Wire the tracer into the AI service for subprocess/retry events\n    if (this.tracer) {\n      this.aiService.setTracer(this.tracer);\n    }\n  }\n\n  /** Progress log instance (if provided via options) for ProgressReporter mirroring */\n  private get progressLog() { return this.options.progressLog; }\n\n  /**\n   * Execute the `generate` command using a pre-built execution plan.\n   *\n   * Runs all three phases:\n   * 1. File tasks concurrently through the pool\n   * 2. Directory AGENTS.md generation (post-order)\n   * 3. Root document generation (sequential)\n   *\n   * @param plan - The execution plan from the generation orchestrator\n   * @returns Aggregated run summary\n   */\n  async executeGenerate(plan: ExecutionPlan): Promise<RunSummary> {\n    const reporter = new ProgressReporter(plan.fileTasks.length, plan.directoryTasks.length, this.progressLog);\n\n    // Initialize plan tracker (writes GENERATION-PLAN.md with checkboxes)\n    const planTracker = new PlanTracker(\n      plan.projectRoot,\n      formatExecutionPlanAsMarkdown(plan),\n    );\n    await planTracker.initialize();\n\n    const runStart = Date.now();\n    let filesProcessed = 0;\n    let filesFailed = 0;\n\n    // -------------------------------------------------------------------\n    // Pre-Phase 1: Cache old .sum content for stale documentation detection\n    // Throttled to avoid opening too many file descriptors at once.\n    // -------------------------------------------------------------------\n\n    const prePhase1Start = Date.now();\n    this.tracer?.emit({\n      type: 'phase:start',\n      phase: 'pre-phase-1-cache',\n      taskCount: plan.fileTasks.length,\n      concurrency: 20,\n    });\n\n    const oldSumCache = new Map<string, SumFileContent>();\n    const sumReadTasks = plan.fileTasks.map(\n      (task) => async () => {\n        try {\n          const existing = await readSumFile(`${task.absolutePath}.sum`);\n          if (existing) {\n            oldSumCache.set(task.path, existing);\n          }\n        } catch {\n          // No old .sum to compare -- skip\n        }\n      },\n    );\n    await runPool(sumReadTasks, {\n      concurrency: 20,\n      tracer: this.tracer,\n      phaseLabel: 'pre-phase-1-cache',\n      taskLabels: plan.fileTasks.map(t => t.path),\n    });\n\n    this.tracer?.emit({\n      type: 'phase:end',\n      phase: 'pre-phase-1-cache',\n      durationMs: Date.now() - prePhase1Start,\n      tasksCompleted: plan.fileTasks.length,\n      tasksFailed: 0,\n    });\n\n    // -------------------------------------------------------------------\n    // Phase 1: File analysis (concurrent)\n    // -------------------------------------------------------------------\n\n    const phase1Start = Date.now();\n    this.tracer?.emit({\n      type: 'phase:start',\n      phase: 'phase-1-files',\n      taskCount: plan.fileTasks.length,\n      concurrency: this.options.concurrency,\n    });\n\n    // Cache source content during Phase 1, reused for inconsistency detection\n    const sourceContentCache = new Map<string, string>();\n\n    const fileTasks = plan.fileTasks.map(\n      (task: ExecutionTask, taskIndex: number) => async (): Promise<FileTaskResult> => {\n        reporter.onFileStart(task.path);\n\n        const callStart = Date.now();\n\n        // Read the source file\n        const sourceContent = await readFile(task.absolutePath, 'utf-8');\n        sourceContentCache.set(task.path, sourceContent);\n\n        // Call AI with the task's prompts\n        const response: AIResponse = await this.aiService.call({\n          prompt: task.userPrompt,\n          systemPrompt: task.systemPrompt,\n          taskLabel: task.path,\n        });\n\n        // Track file size for telemetry (from in-memory content, avoids stat syscall)\n        this.aiService.addFilesReadToLastEntry([{\n          path: task.path,\n          sizeBytes: Buffer.byteLength(sourceContent, 'utf-8'),\n        }]);\n\n        // Compute content hash from already-loaded content (avoids second readFile)\n        const contentHash = computeContentHashFromString(sourceContent);\n\n        // Build .sum file content\n        const cleanedText = stripPreamble(response.text);\n        const sumContent: SumFileContent = {\n          summary: cleanedText,\n          metadata: {\n            purpose: extractPurpose(cleanedText),\n          },\n          generatedAt: new Date().toISOString(),\n          contentHash,\n        };\n\n        // Write .sum file\n        await writeSumFile(task.absolutePath, sumContent);\n\n        // Write annex file if LLM identified reproduction-critical constants\n        if (cleanedText.includes('## Annex References')) {\n          await writeAnnexFile(task.absolutePath, sourceContent);\n        }\n\n        const durationMs = Date.now() - callStart;\n\n        return {\n          path: task.path,\n          success: true,\n          tokensIn: response.inputTokens,\n          tokensOut: response.outputTokens,\n          cacheReadTokens: response.cacheReadTokens,\n          cacheCreationTokens: response.cacheCreationTokens,\n          durationMs,\n          model: response.model,\n        };\n      },\n    );\n\n    const poolResults = await runPool(\n      fileTasks,\n      {\n        concurrency: this.options.concurrency,\n        failFast: this.options.failFast,\n        tracer: this.tracer,\n        phaseLabel: 'phase-1-files',\n        taskLabels: plan.fileTasks.map(t => t.path),\n      },\n      (result) => {\n        if (result.success && result.value) {\n          const v = result.value;\n          filesProcessed++;\n          reporter.onFileDone(v.path, v.durationMs, v.tokensIn, v.tokensOut, v.model, v.cacheReadTokens, v.cacheCreationTokens);\n          planTracker.markDone(v.path);\n        } else {\n          filesFailed++;\n          const errorMsg = result.error?.message ?? 'Unknown error';\n          const taskPath = plan.fileTasks[result.index]?.path ?? `task-${result.index}`;\n          reporter.onFileError(taskPath, errorMsg);\n        }\n      },\n    );\n\n    this.tracer?.emit({\n      type: 'phase:end',\n      phase: 'phase-1-files',\n      durationMs: Date.now() - phase1Start,\n      tasksCompleted: filesProcessed,\n      tasksFailed: filesFailed,\n    });\n\n    // -------------------------------------------------------------------\n    // Post-Phase 1: Inconsistency detection (non-throwing)\n    // -------------------------------------------------------------------\n\n    let inconsistenciesCodeVsDoc = 0;\n    let inconsistenciesCodeVsCode = 0;\n    let inconsistencyReport: import('../quality/index.js').InconsistencyReport | undefined;\n\n    try {\n      const inconsistencyStart = Date.now();\n      const allIssues: Inconsistency[] = [];\n\n      // Collect successfully processed file paths from pool results\n      const processedPaths: string[] = [];\n      for (const result of poolResults) {\n        if (result.success && result.value) {\n          processedPaths.push(result.value.path);\n        }\n      }\n\n      // Group files by directory\n      const dirGroups = new Map<string, string[]>();\n      for (const filePath of processedPaths) {\n        const dir = path.dirname(filePath);\n        const group = dirGroups.get(dir);\n        if (group) {\n          group.push(filePath);\n        } else {\n          dirGroups.set(dir, [filePath]);\n        }\n      }\n\n      // Run checks per directory group (throttled to avoid excessive parallel I/O)\n      const dirEntries = Array.from(dirGroups.entries());\n\n      this.tracer?.emit({\n        type: 'phase:start',\n        phase: 'post-phase-1-quality',\n        taskCount: dirEntries.length,\n        concurrency: 10,\n      });\n\n      const dirCheckResults: Inconsistency[][] = [];\n      const dirCheckTasks = dirEntries.map(\n        ([, groupPaths], groupIndex) => async () => {\n          const dirIssues: Inconsistency[] = [];\n          const filesForCodeVsCode: Array<{ path: string; content: string }> = [];\n\n          // Process files within this group sequentially to limit I/O\n          for (const filePath of groupPaths) {\n            const absoluteFilePath = `${plan.projectRoot}/${filePath}`;\n\n            // Use cached content from Phase 1 (avoids re-read)\n            let sourceContent = sourceContentCache.get(filePath);\n            if (!sourceContent) {\n              try {\n                sourceContent = await readFile(absoluteFilePath, 'utf-8');\n              } catch {\n                continue; // File unreadable, skip\n              }\n            }\n\n            filesForCodeVsCode.push({ path: filePath, content: sourceContent });\n\n            // Old-doc check: detects stale documentation\n            const oldSum = oldSumCache.get(filePath);\n            if (oldSum) {\n              const oldIssue = checkCodeVsDoc(sourceContent, oldSum, filePath);\n              if (oldIssue) {\n                oldIssue.description += ' (stale documentation)';\n                dirIssues.push(oldIssue);\n              }\n            }\n\n            // New-doc check: detects LLM omissions in freshly generated .sum\n            try {\n              const newSum = await readSumFile(`${absoluteFilePath}.sum`);\n              if (newSum) {\n                const newIssue = checkCodeVsDoc(sourceContent, newSum, filePath);\n                if (newIssue) {\n                  dirIssues.push(newIssue);\n                }\n              }\n            } catch {\n              // Freshly written .sum unreadable -- skip\n            }\n          }\n\n          // Code-vs-code check scoped to this directory group\n          const codeIssues = checkCodeVsCode(filesForCodeVsCode);\n          dirIssues.push(...codeIssues);\n\n          dirCheckResults[groupIndex] = dirIssues;\n        },\n      );\n      await runPool(dirCheckTasks, {\n        concurrency: 10,\n        tracer: this.tracer,\n        phaseLabel: 'post-phase-1-quality',\n        taskLabels: dirEntries.map(([dirPath]) => dirPath),\n      });\n\n      this.tracer?.emit({\n        type: 'phase:end',\n        phase: 'post-phase-1-quality',\n        durationMs: Date.now() - inconsistencyStart,\n        tasksCompleted: dirEntries.length,\n        tasksFailed: 0,\n      });\n\n      const allIssuesFlat = dirCheckResults.filter(Boolean).flat();\n      allIssues.push(...allIssuesFlat);\n\n      // Release cached source content to free memory\n      sourceContentCache.clear();\n\n      if (allIssues.length > 0) {\n        const report = buildInconsistencyReport(allIssues, {\n          projectRoot: plan.projectRoot,\n          filesChecked: processedPaths.length,\n          durationMs: Date.now() - inconsistencyStart,\n        });\n\n        inconsistenciesCodeVsDoc = report.summary.codeVsDoc;\n        inconsistenciesCodeVsCode = report.summary.codeVsCode;\n        inconsistencyReport = report;\n\n        console.error(formatReportForCli(report));\n      }\n    } catch (err) {\n      // Inconsistency detection must not break the pipeline\n      console.error(`[quality] Inconsistency detection failed: ${err instanceof Error ? err.message : String(err)}`);\n    }\n\n    // -------------------------------------------------------------------\n    // Phase 2: Directory docs (concurrent per depth level, post-order)\n    // -------------------------------------------------------------------\n\n    // Build set of directories in the plan (for filtering in buildDirectoryPrompt)\n    const knownDirs = new Set(plan.directoryTasks.map(t => t.path));\n\n    // Group directory tasks by depth so same-depth dirs run in parallel\n    // while maintaining post-order (children before parents)\n    const dirsByDepth = new Map<number, typeof plan.directoryTasks>();\n    for (const dirTask of plan.directoryTasks) {\n      const depth = (dirTask.metadata.depth as number) ?? 0;\n      const group = dirsByDepth.get(depth);\n      if (group) {\n        group.push(dirTask);\n      } else {\n        dirsByDepth.set(depth, [dirTask]);\n      }\n    }\n\n    // Process depth levels in descending order (deepest first = post-order)\n    const depthLevels = Array.from(dirsByDepth.keys()).sort((a, b) => b - a);\n\n    for (const depth of depthLevels) {\n      const dirsAtDepth = dirsByDepth.get(depth)!;\n      const phaseLabel = `phase-2-dirs-depth-${depth}`;\n      const dirConcurrency = Math.min(this.options.concurrency, dirsAtDepth.length);\n\n      const phase2Start = Date.now();\n      this.tracer?.emit({\n        type: 'phase:start',\n        phase: phaseLabel,\n        taskCount: dirsAtDepth.length,\n        concurrency: dirConcurrency,\n      });\n\n      const dirTasks = dirsAtDepth.map(\n        (dirTask) => async () => {\n          reporter.onDirectoryStart(dirTask.path);\n          const dirCallStart = Date.now();\n          const prompt = await buildDirectoryPrompt(dirTask.absolutePath, plan.projectRoot, this.options.debug, knownDirs, plan.projectStructure);\n          const dirResponse: AIResponse = await this.aiService.call({\n            prompt: prompt.user,\n            systemPrompt: prompt.system,\n            taskLabel: `${dirTask.path}/AGENTS.md`,\n          });\n          await writeAgentsMd(dirTask.absolutePath, plan.projectRoot, dirResponse.text);\n          const dirDurationMs = Date.now() - dirCallStart;\n          reporter.onDirectoryDone(\n            dirTask.path,\n            dirDurationMs,\n            dirResponse.inputTokens,\n            dirResponse.outputTokens,\n            dirResponse.model,\n            dirResponse.cacheReadTokens,\n            dirResponse.cacheCreationTokens,\n          );\n          planTracker.markDone(`${dirTask.path}/AGENTS.md`);\n        },\n      );\n\n      const phase2Results = await runPool(dirTasks, {\n        concurrency: dirConcurrency,\n        failFast: this.options.failFast,\n        tracer: this.tracer,\n        phaseLabel,\n        taskLabels: dirsAtDepth.map(t => t.path),\n      });\n\n      const phase2Succeeded = phase2Results.filter(r => r.success).length;\n      const phase2Failed = phase2Results.filter(r => !r.success).length;\n\n      this.tracer?.emit({\n        type: 'phase:end',\n        phase: phaseLabel,\n        durationMs: Date.now() - phase2Start,\n        tasksCompleted: phase2Succeeded,\n        tasksFailed: phase2Failed,\n      });\n    }\n\n    // -------------------------------------------------------------------\n    // Post-Phase 2: Phantom path validation (non-throwing)\n    // -------------------------------------------------------------------\n\n    let phantomPathCount = 0;\n    try {\n      const phantomIssues: Inconsistency[] = [];\n      for (const dirTask of plan.directoryTasks) {\n        const agentsMdPath = path.join(dirTask.absolutePath, 'AGENTS.md');\n        try {\n          const content = await readFile(agentsMdPath, 'utf-8');\n          const issues = checkPhantomPaths(agentsMdPath, content, plan.projectRoot);\n          phantomIssues.push(...issues);\n        } catch {\n          // AGENTS.md not yet written or read error — skip\n        }\n      }\n      phantomPathCount = phantomIssues.length;\n\n      if (phantomIssues.length > 0) {\n        const phantomReport = buildInconsistencyReport(phantomIssues, {\n          projectRoot: plan.projectRoot,\n          filesChecked: plan.directoryTasks.length,\n          durationMs: 0,\n        });\n        console.error(formatReportForCli(phantomReport));\n      }\n    } catch (err) {\n      console.error(`[quality] Phantom path validation failed: ${err instanceof Error ? err.message : String(err)}`);\n    }\n\n    // -------------------------------------------------------------------\n    // Phase 3: Root documents (sequential)\n    // -------------------------------------------------------------------\n\n    const phase3Start = Date.now();\n    this.tracer?.emit({\n      type: 'phase:start',\n      phase: 'phase-3-root',\n      taskCount: plan.rootTasks.length,\n      concurrency: 1,\n    });\n\n    let rootTasksCompleted = 0;\n    let rootTasksFailed = 0;\n    for (const rootTask of plan.rootTasks) {\n      const taskStart = Date.now();\n\n      // Emit task:start event\n      this.tracer?.emit({\n        type: 'task:start',\n        taskLabel: rootTask.path,\n        phase: 'phase-3-root',\n      });\n\n      try {\n        // Build prompt at runtime with all AGENTS.md content injected\n        const rootPrompt = await buildRootPrompt(plan.projectRoot, this.options.debug);\n\n        const response = await this.aiService.call({\n          prompt: rootPrompt.user,\n          systemPrompt: rootPrompt.system,\n          taskLabel: rootTask.path,\n          maxTurns: 1, // All context in prompt; no tool use needed\n        });\n\n        // Strip conversational preamble if the LLM still adds one\n        let content = response.text;\n        const mdStart = content.indexOf('# ');\n        if (mdStart > 0) {\n          const preamble = content.slice(0, mdStart).trim();\n          if (preamble && !preamble.startsWith('#') && !preamble.startsWith('<!--')) {\n            content = content.slice(mdStart);\n          }\n        }\n\n        await writeFile(rootTask.outputPath, content, 'utf-8');\n        reporter.onRootDone(rootTask.path);\n        planTracker.markDone(rootTask.path);\n        rootTasksCompleted++;\n\n        // Emit task:done event (success)\n        this.tracer?.emit({\n          type: 'task:done',\n          workerId: 0, // Sequential execution, single worker\n          taskIndex: rootTasksCompleted - 1,\n          taskLabel: rootTask.path,\n          durationMs: Date.now() - taskStart,\n          success: true,\n          activeTasks: 0, // Sequential, only one active at a time\n        });\n      } catch (error) {\n        rootTasksFailed++;\n\n        // Emit task:done event (failure)\n        this.tracer?.emit({\n          type: 'task:done',\n          workerId: 0,\n          taskIndex: rootTasksCompleted + rootTasksFailed - 1,\n          taskLabel: rootTask.path,\n          durationMs: Date.now() - taskStart,\n          success: false,\n          error: error instanceof Error ? error.message : String(error),\n          activeTasks: 0,\n        });\n        throw error; // Re-throw to maintain existing error handling\n      }\n    }\n\n    this.tracer?.emit({\n      type: 'phase:end',\n      phase: 'phase-3-root',\n      durationMs: Date.now() - phase3Start,\n      tasksCompleted: rootTasksCompleted,\n      tasksFailed: rootTasksFailed,\n    });\n\n    // Ensure all plan tracker writes are flushed\n    await planTracker.flush();\n\n    // -------------------------------------------------------------------\n    // Build and print summary\n    // -------------------------------------------------------------------\n\n    const aiSummary = this.aiService.getSummary();\n    const totalDurationMs = Date.now() - runStart;\n\n    const summary: RunSummary = {\n      version: getVersion(),\n      filesProcessed,\n      filesFailed,\n      filesSkipped: 0,\n      totalCalls: aiSummary.totalCalls,\n      totalInputTokens: aiSummary.totalInputTokens,\n      totalOutputTokens: aiSummary.totalOutputTokens,\n      totalCacheReadTokens: aiSummary.totalCacheReadTokens,\n      totalCacheCreationTokens: aiSummary.totalCacheCreationTokens,\n      totalDurationMs,\n      errorCount: aiSummary.errorCount,\n      retryCount: 0,\n      totalFilesRead: aiSummary.totalFilesRead,\n      uniqueFilesRead: aiSummary.uniqueFilesRead,\n      inconsistenciesCodeVsDoc,\n      inconsistenciesCodeVsCode,\n      phantomPaths: phantomPathCount,\n      inconsistencyReport,\n    };\n\n    reporter.printSummary(summary);\n\n    return summary;\n  }\n\n  /**\n   * Execute the `update` command for a set of changed files.\n   *\n   * Runs only Phase 1 (file analysis) for the specified files. Does NOT\n   * generate directory or root documents -- the update command handles\n   * AGENTS.md regeneration itself based on which directories were affected.\n   *\n   * @param filesToAnalyze - Array of changed files to re-analyze\n   * @param projectRoot - Absolute path to the project root\n   * @param config - Project configuration for prompt building\n   * @returns Aggregated run summary\n   */\n  async executeUpdate(\n    filesToAnalyze: FileChange[],\n    projectRoot: string,\n    config: Config,\n  ): Promise<RunSummary> {\n    const reporter = new ProgressReporter(filesToAnalyze.length, 0, this.progressLog);\n\n    const runStart = Date.now();\n    let filesProcessed = 0;\n    let filesFailed = 0;\n\n    // -------------------------------------------------------------------\n    // Phase 1: File analysis (concurrent)\n    // -------------------------------------------------------------------\n\n    const phase1Start = Date.now();\n    this.tracer?.emit({\n      type: 'phase:start',\n      phase: 'update-phase-1-files',\n      taskCount: filesToAnalyze.length,\n      concurrency: this.options.concurrency,\n    });\n\n    // Cache source content during update, reused for inconsistency detection\n    const updateSourceCache = new Map<string, string>();\n\n    // Attempt to read existing project plan for bird's-eye context\n    let projectPlan: string | undefined;\n    try {\n      const planPath = path.join(projectRoot, CONFIG_DIR, 'GENERATION-PLAN.md');\n      projectPlan = await readFile(planPath, 'utf-8');\n    } catch {\n      // No plan file from previous generate run — proceed without project structure context\n    }\n\n    const updateTasks = filesToAnalyze.map(\n      (file: FileChange, fileIndex: number) => async (): Promise<FileTaskResult> => {\n        reporter.onFileStart(file.path);\n\n        const callStart = Date.now();\n        const absolutePath = `${projectRoot}/${file.path}`;\n\n        // Read the source file\n        const sourceContent = await readFile(absolutePath, 'utf-8');\n        updateSourceCache.set(file.path, sourceContent);\n\n        // Read existing .sum for incremental update context\n        const existingSumContent = await readSumFile(`${absolutePath}.sum`);\n\n        // Build prompt\n        const prompt = buildFilePrompt({\n          filePath: file.path,\n          content: sourceContent,\n          projectPlan,\n          existingSum: existingSumContent?.summary,\n        }, this.options.debug);\n\n        // Call AI\n        const response: AIResponse = await this.aiService.call({\n          prompt: prompt.user,\n          systemPrompt: prompt.system,\n          taskLabel: file.path,\n        });\n\n        // Track file size for telemetry (from in-memory content, avoids stat syscall)\n        this.aiService.addFilesReadToLastEntry([{\n          path: file.path,\n          sizeBytes: Buffer.byteLength(sourceContent, 'utf-8'),\n        }]);\n\n        // Compute content hash from already-loaded content (avoids second readFile)\n        const contentHash = computeContentHashFromString(sourceContent);\n\n        // Build .sum file content\n        const cleanedText = stripPreamble(response.text);\n        const sumContent: SumFileContent = {\n          summary: cleanedText,\n          metadata: {\n            purpose: extractPurpose(cleanedText),\n          },\n          generatedAt: new Date().toISOString(),\n          contentHash,\n        };\n\n        // Write .sum file\n        await writeSumFile(absolutePath, sumContent);\n\n        // Write annex file if LLM identified reproduction-critical constants\n        if (cleanedText.includes('## Annex References')) {\n          await writeAnnexFile(absolutePath, sourceContent);\n        }\n\n        const durationMs = Date.now() - callStart;\n\n        return {\n          path: file.path,\n          success: true,\n          tokensIn: response.inputTokens,\n          tokensOut: response.outputTokens,\n          cacheReadTokens: response.cacheReadTokens,\n          cacheCreationTokens: response.cacheCreationTokens,\n          durationMs,\n          model: response.model,\n        };\n      },\n    );\n\n    const poolResults = await runPool(\n      updateTasks,\n      {\n        concurrency: this.options.concurrency,\n        failFast: this.options.failFast,\n        tracer: this.tracer,\n        phaseLabel: 'update-phase-1-files',\n        taskLabels: filesToAnalyze.map(f => f.path),\n      },\n      (result) => {\n        if (result.success && result.value) {\n          const v = result.value;\n          filesProcessed++;\n          reporter.onFileDone(v.path, v.durationMs, v.tokensIn, v.tokensOut, v.model, v.cacheReadTokens, v.cacheCreationTokens);\n        } else {\n          filesFailed++;\n          const errorMsg = result.error?.message ?? 'Unknown error';\n          const filePath = filesToAnalyze[result.index]?.path ?? `file-${result.index}`;\n          reporter.onFileError(filePath, errorMsg);\n        }\n      },\n    );\n\n    this.tracer?.emit({\n      type: 'phase:end',\n      phase: 'update-phase-1-files',\n      durationMs: Date.now() - phase1Start,\n      tasksCompleted: filesProcessed,\n      tasksFailed: filesFailed,\n    });\n\n    // -------------------------------------------------------------------\n    // Post-analysis: Inconsistency detection (non-throwing)\n    // -------------------------------------------------------------------\n\n    let updateInconsistenciesCodeVsDoc = 0;\n    let updateInconsistenciesCodeVsCode = 0;\n    let updateInconsistencyReport: import('../quality/index.js').InconsistencyReport | undefined;\n\n    try {\n      const inconsistencyStart = Date.now();\n      const allIssues: Inconsistency[] = [];\n\n      // Collect successfully processed file paths\n      const processedPaths: string[] = [];\n      for (const result of poolResults) {\n        if (result.success && result.value) {\n          processedPaths.push(result.value.path);\n        }\n      }\n\n      // Group files by directory\n      const dirGroups = new Map<string, string[]>();\n      for (const filePath of processedPaths) {\n        const dir = path.dirname(filePath);\n        const group = dirGroups.get(dir);\n        if (group) {\n          group.push(filePath);\n        } else {\n          dirGroups.set(dir, [filePath]);\n        }\n      }\n\n      // Run checks per directory group (throttled to avoid excessive parallel I/O)\n      const updateDirEntries = Array.from(dirGroups.entries());\n\n      this.tracer?.emit({\n        type: 'phase:start',\n        phase: 'update-post-phase-1-quality',\n        taskCount: updateDirEntries.length,\n        concurrency: 10,\n      });\n\n      const updateDirResults: Inconsistency[][] = [];\n      const updateDirCheckTasks = updateDirEntries.map(\n        ([, groupPaths], groupIndex) => async () => {\n          const dirIssues: Inconsistency[] = [];\n          const filesForCodeVsCode: Array<{ path: string; content: string }> = [];\n\n          for (const filePath of groupPaths) {\n            const absoluteFilePath = `${projectRoot}/${filePath}`;\n\n            // Use cached content from update phase (avoids re-read)\n            let sourceContent = updateSourceCache.get(filePath);\n            if (!sourceContent) {\n              try {\n                sourceContent = await readFile(absoluteFilePath, 'utf-8');\n              } catch {\n                continue;\n              }\n            }\n\n            filesForCodeVsCode.push({ path: filePath, content: sourceContent });\n\n            // New-doc check: detects LLM omissions in freshly generated .sum\n            try {\n              const newSum = await readSumFile(`${absoluteFilePath}.sum`);\n              if (newSum) {\n                const newIssue = checkCodeVsDoc(sourceContent, newSum, filePath);\n                if (newIssue) {\n                  dirIssues.push(newIssue);\n                }\n              }\n            } catch {\n              // .sum unreadable -- skip\n            }\n          }\n\n          // Code-vs-code check scoped to this directory group\n          const codeIssues = checkCodeVsCode(filesForCodeVsCode);\n          dirIssues.push(...codeIssues);\n\n          updateDirResults[groupIndex] = dirIssues;\n        },\n      );\n      await runPool(updateDirCheckTasks, {\n        concurrency: 10,\n        tracer: this.tracer,\n        phaseLabel: 'update-post-phase-1-quality',\n        taskLabels: updateDirEntries.map(([dirPath]) => dirPath),\n      });\n\n      this.tracer?.emit({\n        type: 'phase:end',\n        phase: 'update-post-phase-1-quality',\n        durationMs: Date.now() - inconsistencyStart,\n        tasksCompleted: updateDirEntries.length,\n        tasksFailed: 0,\n      });\n\n      const allIssuesFlat = updateDirResults.filter(Boolean).flat();\n      allIssues.push(...allIssuesFlat);\n\n      // Release cached source content to free memory\n      updateSourceCache.clear();\n\n      if (allIssues.length > 0) {\n        const report = buildInconsistencyReport(allIssues, {\n          projectRoot,\n          filesChecked: processedPaths.length,\n          durationMs: Date.now() - inconsistencyStart,\n        });\n\n        updateInconsistenciesCodeVsDoc = report.summary.codeVsDoc;\n        updateInconsistenciesCodeVsCode = report.summary.codeVsCode;\n        updateInconsistencyReport = report;\n\n        console.error(formatReportForCli(report));\n      }\n    } catch (err) {\n      console.error(`[quality] Inconsistency detection failed: ${err instanceof Error ? err.message : String(err)}`);\n    }\n\n    // Build and print summary\n    const aiSummary = this.aiService.getSummary();\n    const totalDurationMs = Date.now() - runStart;\n\n    const summary: RunSummary = {\n      version: getVersion(),\n      filesProcessed,\n      filesFailed,\n      filesSkipped: 0,\n      totalCalls: aiSummary.totalCalls,\n      totalInputTokens: aiSummary.totalInputTokens,\n      totalOutputTokens: aiSummary.totalOutputTokens,\n      totalCacheReadTokens: aiSummary.totalCacheReadTokens,\n      totalCacheCreationTokens: aiSummary.totalCacheCreationTokens,\n      totalDurationMs,\n      errorCount: aiSummary.errorCount,\n      retryCount: 0,\n      totalFilesRead: aiSummary.totalFilesRead,\n      uniqueFilesRead: aiSummary.uniqueFilesRead,\n      inconsistenciesCodeVsDoc: updateInconsistenciesCodeVsDoc,\n      inconsistenciesCodeVsCode: updateInconsistenciesCodeVsCode,\n      inconsistencyReport: updateInconsistencyReport,\n    };\n\n    reporter.printSummary(summary);\n\n    return summary;\n  }\n}\n\n// ---------------------------------------------------------------------------\n// Helpers\n// ---------------------------------------------------------------------------\n\n/**\n * Strip LLM preamble from response text.\n * Detects common preamble patterns and removes everything before the actual content.\n */\nfunction stripPreamble(responseText: string): string {\n  // Pattern 1: Content after a --- separator (LLM uses --- before real content)\n  const separatorIndex = responseText.indexOf('\\n---\\n');\n  if (separatorIndex >= 0 && separatorIndex < 500) {\n    const afterSeparator = responseText.slice(separatorIndex + 5).trim();\n    if (afterSeparator.length > 0) {\n      return afterSeparator;\n    }\n  }\n\n  // Pattern 2: Content starts with a bold purpose line (**)\n  const boldMatch = responseText.match(/^[\\s\\S]{0,500}?(\\*\\*[A-Z])/);\n  if (boldMatch && boldMatch.index !== undefined) {\n    const before = responseText.slice(0, boldMatch.index).trim();\n    // Only strip if what comes before looks like preamble (no identifiers, short)\n    if (before.length > 0 && before.length < 300 && !before.includes('##')) {\n      return responseText.slice(boldMatch.index);\n    }\n  }\n\n  return responseText;\n}\n\nconst PREAMBLE_PREFIXES = [\n  'now i', 'perfect', 'based on', 'let me', 'here is', 'i\\'ll', 'i will',\n  'great', 'okay', 'sure', 'certainly', 'alright',\n];\n\n/**\n * Extract the purpose from AI response text.\n *\n * Skips lines that look like LLM preamble, markdown headers, or separators.\n * Falls back to empty string if the response is empty.\n *\n * @param responseText - The AI-generated summary text\n * @returns A single-line purpose string\n */\nfunction extractPurpose(responseText: string): string {\n  const lines = responseText.split('\\n');\n  for (const line of lines) {\n    const trimmed = line.trim();\n    if (!trimmed || trimmed.startsWith('#') || trimmed === '---') continue;\n\n    // Skip lines that look like LLM preamble\n    const lower = trimmed.toLowerCase();\n    if (PREAMBLE_PREFIXES.some(p => lower.startsWith(p))) continue;\n\n    // Strip bold markdown wrapper if present\n    const purpose = trimmed.replace(/^\\*\\*(.+)\\*\\*$/, '$1');\n    return purpose.length > 120 ? purpose.slice(0, 117) + '...' : purpose;\n  }\n  return '';\n}\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**CommandRunner orchestrates three-phase AI-driven documentation generation via concurrent worker pools, executing file analysis (Phase 1), post-order directory aggregation (Phase 2), and sequential root synthesis (Phase 3) with integrated quality validation, trace emission, and progress tracking.**\n\n## Public Interface\n\n### CommandRunner Class\n\n```typescript\nclass CommandRunner {\n  constructor(aiService: AIService, options: CommandRunOptions)\n  async executeGenerate(plan: ExecutionPlan): Promise<RunSummary>\n  async executeUpdate(\n    filesToAnalyze: FileChange[],\n    projectRoot: string,\n    config: Config\n  ): Promise<RunSummary>\n}\n```\n\nCommandRunner holds references to `AIService` and `CommandRunOptions`, wires tracer into AIService via `setTracer()`, manages lifecycle of `ProgressReporter` and `PlanTracker` instances.\n\n## Three-Phase Execution Pipeline\n\n### executeGenerate() Orchestration\n\n**Pre-Phase 1 (Cache Old Sum):**\n- Reads existing `.sum` files concurrently (concurrency=20) via `readSumFile()` to populate `oldSumCache` Map\n- Stores `SumFileContent` keyed by relative path for stale documentation detection\n- Emits `phase:start`/`phase:end` trace events with `phase: 'pre-phase-1-cache'`\n\n**Phase 1 (File Analysis):**\n- Builds file tasks from `plan.fileTasks` array, each reading source via `readFile()`, caching content in `sourceContentCache` Map\n- Calls `aiService.call()` with prompts from task, tracks file size via `aiService.addFilesReadToLastEntry([{path, sizeBytes}])`\n- Computes `contentHash` via `computeContentHashFromString()` from in-memory content (avoids second readFile)\n- Strips preamble via `stripPreamble()`, extracts purpose via `extractPurpose()`, writes `SumFileContent` via `writeSumFile()`\n- Detects `## Annex References` marker in cleaned text, writes annex via `writeAnnexFile()` if present\n- Executes via `runPool()` with `concurrency: options.concurrency`, emits `phase:start`/`phase:end` with label `'phase-1-files'`\n- Updates `PlanTracker` via `markDone(path)`, reports progress via `ProgressReporter.onFileDone()` or `onFileError()`\n\n**Post-Phase 1 (Quality Validation):**\n- Groups processed files by directory via `Map<dirname, paths[]>`\n- Runs quality checks per directory group concurrently (concurrency=10) via `runPool()` with phase label `'post-phase-1-quality'`\n- For each file: reads cached source from `sourceContentCache`, checks stale docs via `checkCodeVsDoc(source, oldSum)` (marks with `' (stale documentation)'` suffix), checks fresh docs via `checkCodeVsDoc(source, newSum)` from `readSumFile()`\n- Aggregates directory-scoped code-vs-code issues via `checkCodeVsCode(filesForCodeVsCode)` where `filesForCodeVsCode` is `Array<{path, content}>`\n- Clears `sourceContentCache` after validation to free memory\n- Builds `InconsistencyReport` via `buildInconsistencyReport()`, prints via `formatReportForCli()`, stores counts in `inconsistenciesCodeVsDoc`/`inconsistenciesCodeVsCode`\n- Non-throwing: catches errors and logs to stderr with `[quality]` prefix\n\n**Phase 2 (Directory Docs):**\n- Builds `knownDirs` Set from `plan.directoryTasks.map(t => t.path)` for prompt filtering\n- Groups directory tasks by depth via `Map<depth, tasks[]>` where depth from `task.metadata.depth`\n- Processes depth levels in descending order (deepest first) via `Array.from(dirsByDepth.keys()).sort((a,b) => b-a)` for post-order traversal\n- Per depth level: creates phase label `phase-2-dirs-depth-${depth}`, sets concurrency to `Math.min(options.concurrency, dirsAtDepth.length)`\n- Builds prompts via `buildDirectoryPrompt(absolutePath, projectRoot, debug, knownDirs, projectStructure)`, calls `aiService.call()`, writes via `writeAgentsMd()`\n- Updates tracker via `markDone('${path}/AGENTS.md')`, reports via `ProgressReporter.onDirectoryDone()`\n\n**Post-Phase 2 (Phantom Path Validation):**\n- Iterates `plan.directoryTasks`, reads `AGENTS.md` via `readFile(path.join(absolutePath, 'AGENTS.md'))`\n- Calls `checkPhantomPaths(agentsMdPath, content, projectRoot)` to extract/resolve path references\n- Builds phantom report via `buildInconsistencyReport()`, prints via `formatReportForCli()`, stores count in `phantomPathCount`\n- Non-throwing: catches errors and logs to stderr with `[quality]` prefix\n\n**Phase 3 (Root Documents):**\n- Executes `plan.rootTasks` sequentially (concurrency=1) with phase label `'phase-3-root'`\n- Builds prompt via `buildRootPrompt(projectRoot, debug)`, calls `aiService.call()` with `maxTurns: 1` (no tool use)\n- Strips conversational preamble: finds first `# ` header via `indexOf()`, checks if preceding text is preamble (no `#`, no `<!--`), slices from header start\n- Writes output via `writeFile(rootTask.outputPath)`, updates tracker/reporter\n- Emits `task:start`/`task:done` trace events with `workerId: 0` for sequential execution\n\n**Finalization:**\n- Flushes `planTracker` via `await planTracker.flush()` to ensure serialized writes complete\n- Builds `RunSummary` from `aiService.getSummary()` with counts: `filesProcessed`, `filesFailed`, `inconsistenciesCodeVsDoc`, `inconsistenciesCodeVsCode`, `phantomPaths`, `inconsistencyReport`\n- Calls `reporter.printSummary(summary)`, returns summary\n\n### executeUpdate() Orchestration\n\n**Phase 1 (File Analysis):**\n- Caches source in `updateSourceCache` Map keyed by relative path\n- Reads existing project plan from `${CONFIG_DIR}/GENERATION-PLAN.md` for bird's-eye context (fallback to undefined on error)\n- Reads existing `.sum` via `readSumFile()`, passes to `buildFilePrompt()` as `existingSum` for incremental update context\n- Computes hash via `computeContentHashFromString()`, writes `.sum` via `writeSumFile()`, writes annex if `## Annex References` detected\n- Executes via `runPool()` with phase label `'update-phase-1-files'`\n\n**Post-Phase 1 (Quality Validation):**\n- Groups files by directory, runs checks concurrently (concurrency=10) with phase label `'update-post-phase-1-quality'`\n- Skips stale documentation check (no `oldSumCache` in update path)\n- Checks fresh docs via `checkCodeVsDoc(source, newSum)`, aggregates code-vs-code via `checkCodeVsCode()`\n- Clears `updateSourceCache` after validation, builds/prints report if issues detected\n- Non-throwing error handling with `[quality]` prefix\n\n**No Phase 2/3:**\n- Update command regenerates only `.sum` files; parent update orchestrator handles affected `AGENTS.md` regeneration separately\n\n## Helper Functions\n\n### stripPreamble(responseText: string): string\n\n- **Pattern 1 (separator)**: Detects `\\n---\\n` within first 500 chars, returns content after separator+5 if non-empty\n- **Pattern 2 (bold header)**: Matches `/^[\\s\\S]{0,500}?(\\*\\*[A-Z])/`, strips preceding content if <300 chars, no `##` markers\n- Returns original text if no preamble detected\n\n### extractPurpose(responseText: string): string\n\n- Splits on newlines, skips empty lines, `#` headers, `---` separators\n- Skips lines starting with preamble prefixes: `'now i'`, `'perfect'`, `'based on'`, `'let me'`, `'here is'`, `'i\\'ll'`, `'i will'`, `'great'`, `'okay'`, `'sure'`, `'certainly'`, `'alright'` (case-insensitive)\n- Strips bold wrapper via `/^\\*\\*(.+)\\*\\*$/`, truncates to 120 chars with `'...'` suffix\n- Returns empty string if no valid purpose line found\n\n## Integration Points\n\n**Dependencies on generation module:**\n- Consumes `ExecutionPlan` from `executor.ts` with `fileTasks`, `directoryTasks`, `rootTasks`, `projectRoot`, `projectStructure`\n- Calls `buildFilePrompt()`, `buildDirectoryPrompt()`, `buildRootPrompt()` from `prompts/index.ts`\n- Calls `writeSumFile()`, `readSumFile()`, `writeAnnexFile()` from `writers/sum.ts`, `writeAgentsMd()` from `writers/agents-md.ts`\n- Calls `formatExecutionPlanAsMarkdown()` from `executor.ts` for plan tracker initialization\n\n**Dependencies on orchestration module:**\n- Calls `runPool()` from `pool.ts` with `PoolOptions` (concurrency, failFast, tracer, phaseLabel, taskLabels)\n- Instantiates `PlanTracker` from `plan-tracker.ts`, calls `initialize()`, `markDone()`, `flush()`\n- Instantiates `ProgressReporter` from `progress.ts`, calls `onFileStart/Done/Error`, `onDirectoryStart/Done`, `onRootDone`, `printSummary()`\n\n**Dependencies on quality module:**\n- Calls `checkCodeVsDoc(source, sum, path)`, `checkCodeVsCode(files)`, `checkPhantomPaths(path, content, root)` from `quality/index.ts`\n- Calls `buildInconsistencyReport(issues, metadata)`, `formatReportForCli(report)` from `quality/index.ts`\n\n**Dependencies on AIService:**\n- Calls `aiService.call({prompt, systemPrompt, taskLabel, maxTurns?})` returning `AIResponse`\n- Calls `aiService.addFilesReadToLastEntry([{path, sizeBytes}])` for telemetry tracking\n- Calls `aiService.getSummary()` returning `{totalCalls, totalInputTokens, totalOutputTokens, totalCacheReadTokens, totalCacheCreationTokens, errorCount, totalFilesRead, uniqueFilesRead}`\n- Calls `aiService.setTracer(tracer)` to wire trace events for subprocess/retry tracking\n\n**Dependencies on change-detection:**\n- Calls `computeContentHashFromString(content)` from `change-detection/index.ts` returning SHA-256 hex string\n- Consumes `FileChange[]` array in `executeUpdate()`\n\n## Trace Event Emission\n\n**Phase-level events:**\n- `phase:start` with `{phase, taskCount, concurrency}` for pre-phase-1-cache, phase-1-files, post-phase-1-quality, phase-2-dirs-depth-N, post-phase-2-phantom, phase-3-root, update-phase-1-files, update-post-phase-1-quality\n- `phase:end` with `{phase, durationMs, tasksCompleted, tasksFailed}` for same phase labels\n\n**Task-level events (Phase 3 only):**\n- `task:start` with `{taskLabel, phase: 'phase-3-root'}` before root document generation\n- `task:done` with `{workerId: 0, taskIndex, taskLabel, durationMs, success, error?, activeTasks: 0}` after root task completion or failure\n\n**Pool-level events (delegated to runPool):**\n- `worker:start/end`, `task:pickup/done` for Phase 1, Phase 2, quality checks (emitted by pool.ts)\n\n## Return Value Schema\n\n**RunSummary:**\n```typescript\n{\n  version: string,              // from getVersion()\n  filesProcessed: number,       // Successful Phase 1 file tasks\n  filesFailed: number,          // Failed Phase 1 file tasks\n  filesSkipped: number,         // Always 0 in runner (incremental logic in update orchestrator)\n  totalCalls: number,           // From aiService.getSummary()\n  totalInputTokens: number,\n  totalOutputTokens: number,\n  totalCacheReadTokens: number,\n  totalCacheCreationTokens: number,\n  totalDurationMs: number,      // Elapsed time from run start\n  errorCount: number,\n  retryCount: number,           // Always 0 (retry tracking in AIService)\n  totalFilesRead: number,\n  uniqueFilesRead: number,\n  inconsistenciesCodeVsDoc?: number,\n  inconsistenciesCodeVsCode?: number,\n  phantomPaths?: number,\n  inconsistencyReport?: InconsistencyReport,\n}\n```\n\n## Error Handling Strategy\n\n- Pool execution with `failFast: options.failFast` controls early termination on task failures\n- Quality validation failures logged to stderr with `[quality]` prefix but never throw (non-blocking)\n- Phase 3 root task failures re-throw after emitting `task:done` event with `success: false`\n- Missing project plan in `executeUpdate()` caught silently (proceeds with undefined `projectPlan`)\n\n## Resource Management\n\n- `sourceContentCache`/`updateSourceCache` Maps explicitly cleared via `clear()` after quality validation to free memory\n- Concurrent I/O throttled via pool concurrency limits: 20 for sum reads, 10 for quality checks, `options.concurrency` for AI calls\n- Cached content reused across phases: source read once in Phase 1, consumed by quality checks (avoids redundant `readFile()`)",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 3534,
      "cacheReadTokens": 17367,
      "cacheCreationTokens": 19107,
      "latencyMs": 84544,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/orchestration/runner.ts",
          "sizeBytes": 35266
        }
      ]
    },
    {
      "timestamp": "2026-02-09T17:46:04.342Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: /home/pascal/wks/agents-reverse-engineer/README.md\n\n```markdown\n<div align=\"center\">\n\n# AGENTS REVERSE ENGINEER (ARE)\n\n**Reverse engineer your codebase into AI-friendly documentation.**\n\n**Generate `.sum` files, `AGENTS.md`, and root docs for Claude Code, OpenCode, and any AI assistant that supports `AGENTS.md`.**\n\n[![npm version](https://img.shields.io/npm/v/agents-reverse-engineer?style=for-the-badge&logo=npm&logoColor=white&color=CB3837)](https://www.npmjs.com/package/agents-reverse-engineer)\n[![License](https://img.shields.io/badge/license-MIT-blue?style=for-the-badge)](LICENSE)\n\n<br>\n\n```bash\nnpx agents-reverse-engineer@latest\n```\n\n**Interactive installer with runtime and location selection.**\n\n**Works on Mac, Windows, and Linux.**\n\n<br>\n\n_\"Finally, my AI assistant actually understands my codebase structure.\"_\n\n_\"No more explaining the same architecture in every conversation.\"_\n\n<br>\n\n[Why This Exists](#why-this-exists) · [How It Works](#how-it-works) · [Commands](#commands) · [Generated Docs](#generated-documentation)\n\n</div>\n\n---\n\n## Why This Exists\n\nAI coding assistants are powerful, but they don't know your codebase. Every session starts fresh. You explain the same architecture, the same patterns, the same file locations — over and over.\n\n**agents-reverse-engineer** fixes that. It generates documentation that AI assistants actually read:\n\n- **`.sum` files** — Per-file summaries with purpose, exports, dependencies\n- **`AGENTS.md`** — Per-directory overviews with file organization (standard format)\n- **`CLAUDE.md`** / **`GEMINI.md`** / **`OPENCODE.md`** — Runtime-specific project entry points\n\nThe result: Your AI assistant understands your codebase from the first message.\n\n---\n\n## Who This Is For\n\nDevelopers using AI coding assistants (Claude Code, OpenCode, Gemini CLI, or any tool supporting `AGENTS.md`) who want their assistant to actually understand their project structure — without manually writing documentation or repeating context every session.\n\n---\n\n## Getting Started\n\n### 1. Install Commands\n\n```bash\nnpx agents-reverse-engineer@latest\n```\n\nThe interactive installer prompts you to:\n\n1. **Select runtime** — Claude Code, OpenCode, Gemini CLI, or all\n2. **Select location** — Global (`~/.claude/`) or local (`./.claude/`)\n\nThis installs:\n\n- **Commands** — `/are-init`, `/are-discover`, `/are-generate`, `/are-update`, `/are-specify`, `/are-clean`\n- **Session hook** — Auto-updates docs when session ends (Claude/Gemini)\n\n### 2. Initialize Configuration\n\nAfter installation, create the configuration file in your AI assistant:\n\n```bash\n/are-init\n```\n\nThis creates `.agents-reverse-engineer/config.yaml` with default settings.\n\n### 3. Generate Documentation\n\nIn your AI assistant:\n\n```\n/are-discover\n/are-generate\n```\n\nThe assistant creates the plan and generates all documentation.\n\n### Non-Interactive Installation\n\n```bash\n# Install for Claude Code globally\nnpx agents-reverse-engineer@latest --runtime claude -g\n\n# Install for all runtimes locally\nnpx agents-reverse-engineer@latest --runtime all -l\n```\n\n### Uninstall\n\n```bash\nnpx agents-reverse-engineer@latest uninstall\n```\n\nRemoves:\n- Command files (`/are-*` commands)\n- Session hooks (Claude/Gemini)\n- ARE permissions from settings.json\n- `.agents-reverse-engineer` folder (local installs only)\n\nUse `--runtime` and `-g`/`-l` flags for specific targets.\n\n### Checking Version\n\n```bash\nnpx agents-reverse-engineer@latest --version\n```\n\n---\n\n## How It Works\n\n### 1. Install Commands\n\n```bash\nnpx agents-reverse-engineer@latest\n```\n\nInteractive installer installs commands and hooks for your chosen runtime(s).\n\n**Runtimes:** Claude Code, OpenCode, Gemini CLI (or all at once)\n\n---\n\n### 2. Initialize Configuration\n\n```\n/are-init\n```\n\nCreates `.agents-reverse-engineer/config.yaml` with exclusion patterns and options.\n\n---\n\n### 3. Discover & Plan\n\n```\n/are-discover\n```\n\nScans your codebase (respecting `.gitignore`), detects file types, and creates `GENERATION-PLAN.md` with all files to analyze.\n\nUses **post-order traversal** — deepest directories first, so child documentation exists before parent directories are documented.\n\n---\n\n### 4. Generate (in your AI assistant)\n\n```\n/are-generate\n```\n\nYour AI assistant executes the plan:\n\n1. **File Analysis** — Creates `.sum` file for each source file\n2. **Directory Docs** — Creates `AGENTS.md` for each directory\n3. **Root Docs** — Creates `CLAUDE.md`, `GEMINI.md`, `OPENCODE.md`\n\n---\n\n### 5. Update Incrementally\n\n```\n/are-update\n```\n\nOnly regenerates documentation for files that changed since last run.\n\n---\n\n### 6. Generate Specification\n\n```\n/are-specify\n```\n\nSynthesizes all AGENTS.md documentation into a single project specification document (`specs/SPEC.md`). Use `--multi-file` to split into separate files, or `--dry-run` to preview without calling the AI.\n\n---\n\n## Commands\n\n| Command                         | Description                      |\n| ------------------------------- | -------------------------------- |\n| `are`                           | Interactive installer (default)  |\n| `are install`                   | Install with prompts             |\n| `are install --runtime <rt> -g` | Install to runtime globally      |\n| `are install --runtime <rt> -l` | Install to runtime locally       |\n| `are install -u`                | Uninstall (remove files/hooks)   |\n| `are init`                      | Create configuration file        |\n| `are discover`                  | List files that will be analyzed |\n| `are discover --plan`           | Create GENERATION-PLAN.md        |\n| `are discover --show-excluded`  | Show excluded files with reasons |\n| `are generate`                  | Generate all documentation       |\n| `are update`                    | Update changed files only        |\n| `are specify`                   | Generate project specification   |\n| `are clean`                     | Remove all generated docs        |\n\n**Runtimes:** `claude`, `opencode`, `gemini`, `all`\n\n### AI Assistant Commands\n\n| Command         | Description                    | Supported Runtimes       |\n| --------------- | ------------------------------ | ------------------------ |\n| `/are-init`     | Initialize config and commands | Claude, OpenCode, Gemini |\n| `/are-discover` | Rediscover and regenerate plan | Claude, OpenCode, Gemini |\n| `/are-generate` | Generate all documentation     | Claude, OpenCode, Gemini |\n| `/are-update`   | Update changed files only      | Claude, OpenCode, Gemini |\n| `/are-specify`  | Generate project specification | Claude, OpenCode, Gemini |\n| `/are-clean`    | Remove all generated docs      | Claude, OpenCode, Gemini |\n\n---\n\n## Generated Documentation\n\n### `.sum` Files (Per File)\n\n```yaml\n---\nfile_type: service\ngenerated_at: 2026-01-30T12:00:00Z\n---\n\n## Purpose\nHandles user authentication via JWT tokens.\n\n## Public Interface\n- `authenticate(token: string): User`\n- `generateToken(user: User): string`\n\n## Dependencies\n- jsonwebtoken: Token signing/verification\n- ./user-repository: User data access\n\n## Implementation Notes\nTokens expire after 24 hours. Refresh handled by client.\n```\n\n### `AGENTS.md` (Per Directory)\n\nDirectory overview with:\n\n- Description of the directory's role\n- Files grouped by purpose (Types, Services, Utils, etc.)\n- Subdirectories with brief descriptions\n\n### Root Documents\n\n- **`CLAUDE.md`** — Project entry point for Claude Code (auto-loaded)\n- **`GEMINI.md`** — Project entry point for Gemini CLI\n- **`OPENCODE.md`** — Project entry point for OpenCode\n- **`AGENTS.md`** — Root directory overview (universal format)\n\n---\n\n## Configuration\n\nEdit `.agents-reverse-engineer/config.yaml`:\n\n```yaml\n# File and directory exclusions\nexclude:\n  patterns: []              # Custom glob patterns (e.g., [\"*.log\", \"temp/**\"])\n  vendorDirs:               # Directories to skip\n    - node_modules\n    - dist\n    - .git\n  binaryExtensions:         # File types to skip\n    - .png\n    - .jpg\n    - .pdf\n\n# Discovery options\noptions:\n  followSymlinks: false     # Follow symbolic links during traversal\n  maxFileSize: 1048576      # Max file size in bytes (1MB default)\n\n# Output formatting\noutput:\n  colors: true              # Use colors in terminal output\n  verbose: true             # Show each file as processed\n\n# AI service configuration\nai:\n  backend: auto             # Backend: 'claude', 'gemini', 'opencode', 'auto'\n  model: sonnet             # Model identifier (backend-specific)\n  timeoutMs: 300000         # Subprocess timeout in ms (5 minutes)\n  maxRetries: 3             # Max retries for transient errors\n  concurrency: 5            # Parallel AI calls (1-10, lower for WSL/constrained envs)\n\n  telemetry:\n    keepRuns: 50            # Number of run logs to keep\n    costThresholdUsd: 10.0  # Optional: warn when cost exceeds this (USD)\n\n  # Optional: Custom model pricing (override defaults)\n  pricing:\n    claude-opus-4:\n      inputCostPerMTok: 15.0    # USD per 1M input tokens\n      outputCostPerMTok: 75.0   # USD per 1M output tokens\n```\n\n### Key Config Options\n\n**Concurrency (`ai.concurrency`)**\n- Default: `5` (changed from 5 to 2 in WSL environments)\n- Range: `1-10`\n- Lower values recommended for resource-constrained environments\n- Higher values speed up generation but use more memory\n\n**Timeout (`ai.timeoutMs`)**\n- Default: `300000` (5 minutes)\n- AI subprocess timeout for each file analysis\n- Increase for very large files or slow connections\n\n---\n\n## Requirements\n\n- **Node.js 18+**\n- **AI Coding Assistant** — One of:\n  - [Claude Code](https://claude.ai/claude-code) (full support + session hooks)\n  - [Gemini CLI](https://github.com/google-gemini/gemini-cli) (full support + session hooks)\n  - [OpenCode](https://github.com/opencode-ai/opencode) (AGENTS.md supported)\n  - Any assistant supporting `AGENTS.md` format\n\n---\n\n## License\n\nMIT\n\n```\n\n\n## Project Structure\n\nFull project file listing for context:\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion annex file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**README.md provides user-facing documentation for agents-reverse-engineer CLI, covering installation via `npx agents-reverse-engineer@latest`, command workflows (`/are-init`, `/are-discover`, `/are-generate`, `/are-update`, `/are-specify`, `/are-clean`), generated artifact formats (`.sum` files with YAML frontmatter, `AGENTS.md` directory overviews, runtime-specific root documents), configuration schema for `.agents-reverse-engineer/config.yaml`, and integration patterns with Claude Code, OpenCode, and Gemini CLI.**\n\n## Installation Workflows\n\nInteractive installer invoked via `npx agents-reverse-engineer@latest` prompts for runtime selection (`claude`, `opencode`, `gemini`, `all`) and installation scope (global `-g` targeting `~/.claude/`, local `-l` targeting `./.claude/`). Non-interactive installation accepts `--runtime <rt>` flag with `-g`/`-l` flags. Uninstallation via `npx agents-reverse-engineer@latest uninstall` removes command files (`/are-*`), session hooks (Claude/Gemini), ARE permissions from settings.json, and `.agents-reverse-engineer` folder (local installs only).\n\n## Command Reference\n\nCLI commands execute via `are <command>` binary:\n- `are install` — Interactive installer with runtime/location prompts\n- `are install --runtime <rt> -g` — Install to runtime globally\n- `are install --runtime <rt> -l` — Install to runtime locally\n- `are install -u` — Uninstall (remove files/hooks)\n- `are init` — Create `.agents-reverse-engineer/config.yaml` configuration file\n- `are discover` — List files for analysis, optionally write `GENERATION-PLAN.md` with `--plan` flag\n- `are discover --show-excluded` — Display excluded files with exclusion reasons\n- `are generate` — Execute three-phase pipeline (file analysis → directory aggregation → root synthesis)\n- `are update` — Incremental regeneration for changed files only\n- `are specify` — Synthesize `specs/SPEC.md` from all `AGENTS.md` files, supports `--multi-file` for split output\n- `are clean` — Remove `.sum`, `AGENTS.md` (generated only), root docs, `GENERATION-PLAN.md`\n\nAI assistant commands mirror CLI with `/are-` prefix: `/are-init`, `/are-discover`, `/are-generate`, `/are-update`, `/are-specify`, `/are-clean`. Supported runtimes: Claude Code, OpenCode, Gemini CLI.\n\n## Generated Documentation Structure\n\n`.sum` files contain YAML frontmatter with `file_type`, `generated_at` timestamp, followed by markdown sections:\n- `## Purpose` — One-line role statement\n- `## Public Interface` — Exported functions/classes/types with signatures\n- `## Dependencies` — Import statements with usage descriptions\n- `## Implementation Notes` — Behavioral contracts, constraints, patterns\n\n`AGENTS.md` directory overviews include:\n- Directory role description\n- Files grouped by purpose (Types, Services, Utils)\n- Subdirectory briefs\n\nRoot documents (`CLAUDE.md`, `GEMINI.md`, `OPENCODE.md`) serve as runtime-specific project entry points auto-loaded by respective AI assistants. Universal `AGENTS.md` at project root provides standard format overview.\n\n## Configuration Schema\n\n`.agents-reverse-engineer/config.yaml` structure:\n\n```yaml\nexclude:\n  patterns: []              # Gitignore-style globs (e.g., [\"*.log\", \"temp/**\"])\n  vendorDirs:               # Skip directories (node_modules, dist, .git)\n  binaryExtensions:         # Skip file types (.png, .jpg, .pdf)\n\noptions:\n  followSymlinks: false     # Symbolic link traversal flag\n  maxFileSize: 1048576      # Binary detection threshold (1MB)\n\noutput:\n  colors: true              # ANSI color codes in terminal\n  verbose: true             # Per-file processing display\n\nai:\n  backend: auto             # Backend: 'claude', 'gemini', 'opencode', 'auto'\n  model: sonnet             # Backend-specific model identifier\n  timeoutMs: 300000         # Subprocess timeout (5 minutes default)\n  maxRetries: 3             # Exponential backoff retry attempts\n  concurrency: 5            # Worker pool size (1-10, WSL defaults to 2)\n  \n  telemetry:\n    keepRuns: 50            # Run log retention count\n    costThresholdUsd: 10.0  # Cost warning threshold (USD)\n  \n  pricing:                  # Per-model token cost overrides\n    claude-opus-4:\n      inputCostPerMTok: 15.0   # USD per 1M input tokens\n      outputCostPerMTok: 75.0  # USD per 1M output tokens\n```\n\nKey options:\n- `ai.concurrency` range `1-10` (lower for resource-constrained environments, WSL defaults to 2)\n- `ai.timeoutMs` default `300000` (increase for large files/slow connections)\n- `exclude.patterns` accepts glob patterns for custom exclusions\n- `options.maxFileSize` defines binary detection threshold in bytes\n\n## Three-Phase Generation Workflow\n\nPhase 1 (File Analysis): Concurrent subprocess pool generates `.sum` files for each source file discovered by `are discover`. Post-order traversal ensures deepest directories processed first.\n\nPhase 2 (Directory Aggregation): Sequential `AGENTS.md` generation for each directory, consuming child `.sum` files and subdirectory `AGENTS.md` files. Preserves user-authored content by renaming `AGENTS.md` → `AGENTS.local.md` before generation.\n\nPhase 3 (Root Synthesis): Sequential generation of `CLAUDE.md`, `GEMINI.md`, `OPENCODE.md` from aggregated `AGENTS.md` corpus. Each runtime receives tailored integration instructions.\n\nProgress logged to `.agents-reverse-engineer/progress.log`, viewable via `tail -f` during long-running operations.\n\n## Incremental Update Strategy\n\n`are update` workflow:\n1. Read `content_hash` from `.sum` YAML frontmatter\n2. Compute SHA-256 hash of current file content\n3. Hash mismatch → regenerate `.sum`\n4. Hash match → skip file\n5. Detect orphans (`.sum` for deleted source files)\n6. Cleanup orphaned `.sum` files and empty `AGENTS.md` directories\n7. Regenerate affected directory `AGENTS.md` via parent directory traversal\n\n## Requirements and Compatibility\n\n- Node.js ≥18.0.0\n- AI assistant supporting `AGENTS.md` format:\n  - Claude Code (full support + session hooks)\n  - Gemini CLI (full support + session hooks)\n  - OpenCode (AGENTS.md supported)\n  - Any assistant reading `AGENTS.md`\n\nSession hooks auto-update documentation on session end for Claude Code and Gemini CLI (disable via `ARE_DISABLE_HOOK=1` environment variable).\n\n## Messaging and Value Proposition\n\nUser testimonials:\n- _\"Finally, my AI assistant actually understands my codebase structure.\"_\n- _\"No more explaining the same architecture in every conversation.\"_\n\nProblem statement: AI coding assistants lack persistent codebase knowledge, requiring repeated context explanation across sessions.\n\nSolution: Generate AI-readable documentation (`.sum`, `AGENTS.md`, root docs) consumed by assistants for persistent understanding.\n\nTarget audience: Developers using Claude Code, OpenCode, Gemini CLI, or AGENTS.md-compatible assistants seeking to eliminate repetitive context provision.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 1827,
      "cacheReadTokens": 17367,
      "cacheCreationTokens": 11770,
      "latencyMs": 52862,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "README.md",
          "sizeBytes": 9778
        }
      ]
    },
    {
      "timestamp": "2026-02-09T17:46:57.254Z",
      "prompt": "Generate AGENTS.md for directory: \"src/quality/density\" (density)\n\n## File Summaries (1 files)\n\n### validator.ts\n**Purpose:** validator.ts implements a disabled findability validation system that previously checked whether exported symbols fro...\n\n**validator.ts implements a disabled findability validation system that previously checked whether exported symbols from .sum files appeared in parent AGENTS.md content, now returning empty results after structured metadata removal.**\n\n## Exported Interface\n\n`FindabilityResult` represents validation output for a single .sum file with fields:\n- `filePath: string` — path to the validated .sum file\n- `symbolsTested: string[]` — symbol names checked for presence\n- `symbolsFound: string[]` — symbols located in AGENTS.md content\n- `symbolsMissing: string[]` — symbols absent from AGENTS.md\n- `score: number` — ratio of found to tested symbols (0-1 range)\n\n## Exported Function\n\n`validateFindability(_agentsMdContent: string, _sumFiles: Map<string, SumFileContent>): FindabilityResult[]` accepts AGENTS.md full text and a map of file paths to parsed `SumFileContent` objects, returns an empty array since the `publicInterface` field was removed from the `SumFileContent` schema. Function signature preserved for future re-implementation via post-processing passes that extract structured symbols.\n\n## Implementation Status\n\nModule retained as stub after `metadata.publicInterface` removal from `SumFileContent` schema (referenced in comment). Originally performed heuristic string-based symbol matching without LLM calls to verify that key exports from source files appeared in aggregated directory documentation. The `validateFindability` function signature remains to support future structured extraction support, but currently no validation logic executes.\n\n## Dependencies\n\nImports `SumFileContent` type from `../../generation/writers/sum.js` for the `_sumFiles` parameter type annotation.\n\n## Import Map (verified — use these exact paths)\n\nvalidator.ts:\n  ../../generation/writers/sum.js → SumFileContent (type)\n\n## Project Directory Structure\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>",
      "systemPrompt": "You are generating an AGENTS.md file — a directory-level overview for AI coding assistants.\n\nCRITICAL: Output ONLY the raw markdown content. No code fences, no preamble, no explanations, no conversational text. Your entire response IS the AGENTS.md file content.\n\nOUTPUT FORMAT:\n- First line MUST be exactly: <!-- Generated by agents-reverse-engineer -->\n- Use a # heading with the directory name\n- Write a one-paragraph purpose statement for the directory\nusage\nADAPTIVE SECTIONS:\nAnalyze the directory contents and choose the most relevant sections. Do NOT use a fixed template. Instead, select sections that best document this specific directory for an AI that needs to reconstruct or extend the project.\n\nConsider these section types (choose what applies):\n- **Contents**: Group files by purpose/category under ## headings. For each file: markdown link [filename](./filename) and a one-line description.\n- **Subdirectories**: If subdirectories exist, list them with links [dirname/](./dirname/) and brief summaries.\n- **Architecture / Data Flow**: If files form a pipeline, request/response chain, or layered architecture, document it.\n- **Stack**: If this is a package root (has package.json, Cargo.toml, go.mod, etc.), document the technology stack, key scripts, and entry points.\n- **Structure**: If the directory layout follows a convention (feature-sliced, domain-driven, MVC, etc.), document it.\n- **Patterns**: If files share recurring design patterns (factory, strategy, middleware, barrel re-export), name and document them.\n- **Configuration**: If the directory contains config files, schemas, or environment definitions, document the config surface area.\n- **API Surface**: If the directory exports a public API (barrel index, route definitions, SDK), document the interface contract.\n- **File Relationships**: How files collaborate, depend on each other, or share state.\n- **Behavioral Contracts**: If files contain regex patterns, format specifications, magic constants, or template strings that define observable behavior, collect them in a dedicated section. Preserve verbatim patterns from file summaries — do NOT paraphrase regex into prose. This section is MANDATORY when file summaries contain behavioral artifacts.\n- **Reproduction-Critical Constants**: If file summaries reference annex files (via ## Annex References sections), list them with links. Example: \"Full prompt template text: [templates.ts.annex.md](./prompts/templates.ts.annex.md)\". Do NOT reproduce annex content in AGENTS.md — just link to it.\n\nChoose any relevant sections or create your own based on the directory contents. The goal is to provide a comprehensive overview that captures the essence of the directory's role in the project and how its files work together, with a focus on what an AI coding assistant would need to know to effectively interact with this code.\n\nSCOPE:\n- AGENTS.md is a NAVIGATIONAL INDEX — help an AI find the right file quickly\n- Focus on: what each file does, how files relate, directory-level patterns\n- Do NOT reproduce full architecture sections — those belong in the root CLAUDE.md\n\nPATH ACCURACY (MANDATORY):\n- When referencing files or modules outside this directory, use ONLY paths from the \"Import Map\" section\n- Do NOT invent, rename, or guess module paths — if a path isn't in the Import Map, don't reference it\n- Use the exact directory names from \"Project Directory Structure\" — do NOT rename directories\n  (e.g., if the directory is called \"cli\", write \"src/cli/\", NOT \"src/commands/\")\n- Cross-module references must use the specifier format from actual import statements\n  (e.g., \"../generation/writers/sum.js\", NOT \"../fs/sum-file.js\")\n- If you are unsure about a path, omit the cross-reference rather than guessing\n\nCONSISTENCY (MANDATORY):\n- Do not contradict yourself within the same document\n- If you describe a technique (e.g., \"regex-based\"), do not call it something else later (e.g., \"AST-based\")\n- When stating version numbers, engines, or config fields, use ONLY values present in the file summaries\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this directory\", \"this module\", \"provides\", \"responsible for\", \"is used to\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Per-file descriptions in Contents sections: 1-2 sentences maximum. Reference key symbols but do not reproduce full summaries.\n- Behavioral contracts (regex patterns, format specs, constants) belong in a separate Behavioral Contracts section, not in per-file descriptions.\n- Annex file references: link to .annex.md files, do not inline their content. One line per annex reference.\n- Subdirectory descriptions: 1-2 sentences maximum. Capture the directory's role, not its full contents.\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- Key exported symbols from file summaries MUST appear in the directory overview\n- Preserve exact casing of identifiers\n\nUSER NOTES:\n- If \"User Notes\" are provided in the prompt, they contain user-defined instructions that will be automatically prepended to your output\n- Do NOT repeat or paraphrase user notes in your generated content — they are included separately\n- You may reference information from user notes for context",
      "response": "<!-- Generated by agents-reverse-engineer -->\n\n# src/quality/density\n\nStub validation module for findability analysis (symbol presence in aggregated AGENTS.md), disabled after `SumFileContent.metadata.publicInterface` removal pending structured extraction re-implementation.\n\n## Contents\n\n**[validator.ts](./validator.ts)** — Exports `validateFindability(_agentsMdContent, _sumFiles)` returning empty `FindabilityResult[]` array; previously verified exported symbols from `.sum` files appeared in parent `AGENTS.md` via string matching, now disabled awaiting post-processing extraction support.\n\n## Implementation Status\n\n`validateFindability()` signature preserved for future re-implementation but returns `[]` unconditionally. Original logic performed heuristic substring search to detect whether key symbols from source file summaries appeared in aggregated directory documentation. Disabled when `SumFileContent` schema removed structured `publicInterface` field (see `src/generation/writers/sum.ts`). Module retained to support future LLM-based or AST-based symbol extraction passes.\n\n## Type Surface\n\n`FindabilityResult` interface with fields:\n- `filePath: string` — validated `.sum` file path\n- `symbolsTested: string[]` — symbols checked for presence\n- `symbolsFound: string[]` — symbols located in AGENTS.md\n- `symbolsMissing: string[]` — symbols absent from AGENTS.md\n- `score: number` — ratio `symbolsFound.length / symbolsTested.length` (0-1 range)\n\n## Dependencies\n\nImports `SumFileContent` type from `../../generation/writers/sum.js` for parameter type annotation. No runtime dependencies since function body returns empty array.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 392,
      "cacheReadTokens": 16334,
      "cacheCreationTokens": 10685,
      "latencyMs": 17208,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": []
    },
    {
      "timestamp": "2026-02-09T17:46:57.257Z",
      "prompt": "Generate AGENTS.md for directory: \"src/quality/phantom-paths\" (phantom-paths)\n\n## File Summaries (2 files)\n\n### index.ts\n**Purpose:** Barrel module re-exporting `checkPhantomPaths` validator function for phantom path resolution in generated AGENTS.md ...\n\n**Barrel module re-exporting `checkPhantomPaths` validator function for phantom path resolution in generated AGENTS.md documentation.**\n\n## Exported Interface\n\n- `checkPhantomPaths` — Validates path references in AGENTS.md files by resolving them against the filesystem and reporting unresolved paths as `PhantomPathInconsistency` issues.\n\n## Module Role\n\nSingle export statement serving as public API surface for the `src/quality/phantom-paths/` subsystem. Implementation resides in `./validator.js` which extracts path-like strings from AGENTS.md via three regex patterns (markdown link targets, backtick-quoted paths, prose-embedded paths), resolves against AGENTS.md directory and project root with `.ts`/`.js` fallback, and reports unresolved references in `InconsistencyReport` format.\n### validator.ts\n**Purpose:** validator.ts detects phantom path references in AGENTS.md files by extracting path-like strings via regex patterns, r...\n\n**validator.ts detects phantom path references in AGENTS.md files by extracting path-like strings via regex patterns, resolving them against filesystem locations, and reporting unresolved references as PhantomPathInconsistency objects.**\n\n## Exported Function\n\n`checkPhantomPaths(agentsMdPath: string, content: string, projectRoot: string): PhantomPathInconsistency[]` extracts path references from AGENTS.md content using `PATH_PATTERNS`, resolves each path against multiple base directories, verifies existence via `existsSync()`, and returns array of inconsistency objects for non-existent paths.\n\n## Path Extraction Patterns\n\n`PATH_PATTERNS` contains three regex patterns for detecting file references:\n- `/\\[(?:[^\\]]*)\\]\\((\\.[^)]+)\\)/g` matches Markdown link targets like `[text](./path)` or `[text](path)`\n- `` /`((?:src\\/|\\.\\.?\\/)[^`]+\\.[a-z]{1,4})`/g `` matches backtick-quoted paths starting with `src/`, `./`, or `../` followed by 1-4 character file extension\n- `/(?:from|in|by|via|see)\\s+`?(src\\/[\\w\\-./]+)`?/gi` matches prose-embedded paths following contextual keywords with optional backticks\n\n## Path Resolution Strategy\n\nFor each extracted path, `checkPhantomPaths()` attempts resolution in priority order:\n1. `path.resolve(agentsMdDir, rawPath)` resolves relative to AGENTS.md directory\n2. `path.resolve(projectRoot, rawPath)` resolves relative to project root for `src/` paths\n3. If `rawPath.endsWith('.js')`, appends `.replace(/\\.js$/, '.ts')` variants to `tryPaths[]` to handle TypeScript import convention where `.ts` source imports use `.js` extension\n\nCalls `existsSync()` on all candidate paths; if none exist, creates `PhantomPathInconsistency` with `type: 'phantom-path'`, `severity: 'warning'`.\n\n## Exclusion Patterns\n\n`SKIP_PATTERNS` array filters out non-file references before validation:\n- `/node_modules/`, `/\\.git\\//` exclude dependency/VCS paths\n- `/^https?:/` excludes URLs\n- `/\\{\\{/`, `/\\$\\{/` exclude template placeholders and template literals\n- `/\\*/`, `/\\{[^}]*,[^}]*\\}/` exclude glob patterns and brace expansions\n\n## Inconsistency Reporting\n\nEach phantom path generates `PhantomPathInconsistency` object with:\n- `agentsMdPath` normalized via `path.relative(projectRoot, agentsMdPath)`\n- `description` formatted as `\"Phantom path reference: \\\"${rawPath}\\\" does not exist\"`\n- `details.referencedPath` contains original extracted path string\n- `details.resolvedTo` shows attempted resolution via `path.relative(projectRoot, fromAgentsMd)`\n- `details.context` includes containing line via `lines.find((l) => l.includes(rawPath))` truncated to 120 characters\n\n## Deduplication\n\n`seen` Set tracks `rawPath` strings to prevent duplicate validation of same reference appearing multiple times in content. Skips processing when `seen.has(rawPath)` returns true.\n\n## Import Map (verified — use these exact paths)\n\nvalidator.ts:\n  ../types.js → PhantomPathInconsistency (type)\n\n## Project Directory Structure\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>",
      "systemPrompt": "You are generating an AGENTS.md file — a directory-level overview for AI coding assistants.\n\nCRITICAL: Output ONLY the raw markdown content. No code fences, no preamble, no explanations, no conversational text. Your entire response IS the AGENTS.md file content.\n\nOUTPUT FORMAT:\n- First line MUST be exactly: <!-- Generated by agents-reverse-engineer -->\n- Use a # heading with the directory name\n- Write a one-paragraph purpose statement for the directory\nusage\nADAPTIVE SECTIONS:\nAnalyze the directory contents and choose the most relevant sections. Do NOT use a fixed template. Instead, select sections that best document this specific directory for an AI that needs to reconstruct or extend the project.\n\nConsider these section types (choose what applies):\n- **Contents**: Group files by purpose/category under ## headings. For each file: markdown link [filename](./filename) and a one-line description.\n- **Subdirectories**: If subdirectories exist, list them with links [dirname/](./dirname/) and brief summaries.\n- **Architecture / Data Flow**: If files form a pipeline, request/response chain, or layered architecture, document it.\n- **Stack**: If this is a package root (has package.json, Cargo.toml, go.mod, etc.), document the technology stack, key scripts, and entry points.\n- **Structure**: If the directory layout follows a convention (feature-sliced, domain-driven, MVC, etc.), document it.\n- **Patterns**: If files share recurring design patterns (factory, strategy, middleware, barrel re-export), name and document them.\n- **Configuration**: If the directory contains config files, schemas, or environment definitions, document the config surface area.\n- **API Surface**: If the directory exports a public API (barrel index, route definitions, SDK), document the interface contract.\n- **File Relationships**: How files collaborate, depend on each other, or share state.\n- **Behavioral Contracts**: If files contain regex patterns, format specifications, magic constants, or template strings that define observable behavior, collect them in a dedicated section. Preserve verbatim patterns from file summaries — do NOT paraphrase regex into prose. This section is MANDATORY when file summaries contain behavioral artifacts.\n- **Reproduction-Critical Constants**: If file summaries reference annex files (via ## Annex References sections), list them with links. Example: \"Full prompt template text: [templates.ts.annex.md](./prompts/templates.ts.annex.md)\". Do NOT reproduce annex content in AGENTS.md — just link to it.\n\nChoose any relevant sections or create your own based on the directory contents. The goal is to provide a comprehensive overview that captures the essence of the directory's role in the project and how its files work together, with a focus on what an AI coding assistant would need to know to effectively interact with this code.\n\nSCOPE:\n- AGENTS.md is a NAVIGATIONAL INDEX — help an AI find the right file quickly\n- Focus on: what each file does, how files relate, directory-level patterns\n- Do NOT reproduce full architecture sections — those belong in the root CLAUDE.md\n\nPATH ACCURACY (MANDATORY):\n- When referencing files or modules outside this directory, use ONLY paths from the \"Import Map\" section\n- Do NOT invent, rename, or guess module paths — if a path isn't in the Import Map, don't reference it\n- Use the exact directory names from \"Project Directory Structure\" — do NOT rename directories\n  (e.g., if the directory is called \"cli\", write \"src/cli/\", NOT \"src/commands/\")\n- Cross-module references must use the specifier format from actual import statements\n  (e.g., \"../generation/writers/sum.js\", NOT \"../fs/sum-file.js\")\n- If you are unsure about a path, omit the cross-reference rather than guessing\n\nCONSISTENCY (MANDATORY):\n- Do not contradict yourself within the same document\n- If you describe a technique (e.g., \"regex-based\"), do not call it something else later (e.g., \"AST-based\")\n- When stating version numbers, engines, or config fields, use ONLY values present in the file summaries\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this directory\", \"this module\", \"provides\", \"responsible for\", \"is used to\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Per-file descriptions in Contents sections: 1-2 sentences maximum. Reference key symbols but do not reproduce full summaries.\n- Behavioral contracts (regex patterns, format specs, constants) belong in a separate Behavioral Contracts section, not in per-file descriptions.\n- Annex file references: link to .annex.md files, do not inline their content. One line per annex reference.\n- Subdirectory descriptions: 1-2 sentences maximum. Capture the directory's role, not its full contents.\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- Key exported symbols from file summaries MUST appear in the directory overview\n- Preserve exact casing of identifiers\n\nUSER NOTES:\n- If \"User Notes\" are provided in the prompt, they contain user-defined instructions that will be automatically prepended to your output\n- Do NOT repeat or paraphrase user notes in your generated content — they are included separately\n- You may reference information from user notes for context",
      "response": "<!-- Generated by agents-reverse-engineer -->\n\n# src/quality/phantom-paths\n\nValidates path references in `AGENTS.md` files by extracting path-like strings via regex patterns, resolving them against filesystem locations with TypeScript/JavaScript extension fallbacks, and reporting unresolved references as `PhantomPathInconsistency` objects in quality reports.\n\n## Contents\n\n**[index.ts](./index.ts)** — Barrel export of `checkPhantomPaths` validator function.\n\n**[validator.ts](./validator.ts)** — Extracts path references from `AGENTS.md` content using `PATH_PATTERNS` (markdown links, backtick-quoted paths, prose-embedded paths), resolves each path against `agentsMdDir` and `projectRoot` with `.ts`/`.js` extension fallback via `tryPaths[]`, filters excluded patterns (URLs, template literals, globs), and returns `PhantomPathInconsistency[]` array with deduplication via `seen` Set.\n\n## Path Extraction Patterns\n\n`PATH_PATTERNS` captures three reference types:\n- `/\\[(?:[^\\]]*)\\]\\((\\.[^)]+)\\)/g` — Markdown link targets\n- `` /`((?:src\\/|\\.\\.?\\/)[^`]+\\.[a-z]{1,4})`/g `` — Backtick-quoted paths with extensions\n- `/(?:from|in|by|via|see)\\s+`?(src\\/[\\w\\-./]+)`?/gi` — Prose-embedded paths following contextual keywords\n\n## Resolution Strategy\n\nFor each extracted path, `checkPhantomPaths()` attempts resolution in priority order:\n\n1. Relative to `AGENTS.md` directory: `path.resolve(agentsMdDir, rawPath)`\n2. Relative to project root: `path.resolve(projectRoot, rawPath)`\n3. TypeScript convention fallback: append `.replace(/\\.js$/, '.ts')` variants to `tryPaths[]` when `rawPath.endsWith('.js')`\n\nValidates existence via `existsSync()` on all candidate paths; returns `PhantomPathInconsistency` with `severity: 'warning'` when all attempts fail.\n\n## Exclusion Patterns\n\n`SKIP_PATTERNS` filters non-file references before validation:\n- `/node_modules/`, `/\\.git\\//` — Dependency/VCS paths\n- `/^https?:/` — URLs\n- `/\\{\\{/`, `/\\$\\{/` — Template placeholders/literals\n- `/\\*/`, `/\\{[^}]*,[^}]*\\}/` — Glob patterns/brace expansions\n\n## Inconsistency Reporting\n\nEach phantom path generates `PhantomPathInconsistency` containing:\n- `agentsMdPath` — Normalized via `path.relative(projectRoot, agentsMdPath)`\n- `description` — `\"Phantom path reference: \\\"${rawPath}\\\" does not exist\"`\n- `details.referencedPath` — Original extracted path\n- `details.resolvedTo` — Attempted resolution relative to project root\n- `details.context` — Containing line truncated to 120 characters",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 750,
      "cacheReadTokens": 16334,
      "cacheCreationTokens": 11351,
      "latencyMs": 25421,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": []
    },
    {
      "timestamp": "2026-02-09T17:46:57.265Z",
      "prompt": "Generate AGENTS.md for directory: \"src/ai/telemetry\" (telemetry)\n\n## File Summaries (3 files)\n\n### cleanup.ts\n**Purpose:** cleanupOldLogs removes expired telemetry run logs from `.agents-reverse-engineer/logs/` by lexicographic timestamp so...\n\n**cleanupOldLogs removes expired telemetry run logs from `.agents-reverse-engineer/logs/` by lexicographic timestamp sorting.**\n\n## Exported Function\n\n`cleanupOldLogs(projectRoot: string, keepCount: number): Promise<number>` deletes telemetry log files exceeding retention limit. Returns count of deleted files. Scans `LOGS_DIR` (`'.agents-reverse-engineer/logs'`) for files matching pattern `run-*.json`, sorts lexicographically in reverse order (newest first via ISO 8601 timestamp filenames), slices array at `keepCount` index to identify deletion candidates, unlinks each file via `fs.unlink()`, returns `toDelete.length`. Returns `0` when logs directory does not exist (`ENOENT` error code). Throws on other filesystem errors.\n\n## Algorithm\n\nReads directory entries with `fs.readdir(logsDir)`, filters for files where `name.startsWith('run-')` and `name.endsWith('.json')`, applies `entries.sort()` followed by `entries.reverse()` for descending lexicographic order, extracts slice `entries.slice(keepCount)` as deletion targets, iterates over `toDelete` array executing `fs.unlink(path.join(logsDir, filename))` for each entry.\n\n## Integration Point\n\nCalled by `TelemetryLogger` after writing run logs to enforce retention policy specified in `config.ai.telemetry.keepRuns` (default 50). Prevents unbounded log accumulation during repeated `are generate` and `are update` operations.\n\n## Directory Constant\n\n`LOGS_DIR` constant defines relative path `'.agents-reverse-engineer/logs'` joined with `projectRoot` via `path.join()` to compute absolute directory path.\n### logger.ts\n**Purpose:** TelemetryLogger accumulates per-call AI service metrics in memory during a CLI run and computes aggregate statistics ...\n\n**TelemetryLogger accumulates per-call AI service metrics in memory during a CLI run and computes aggregate statistics for serialization into RunLog objects.**\n\n## Exported Class\n\n`TelemetryLogger` — In-memory accumulator for `TelemetryEntry` instances created during a single CLI invocation. Tracks runId (unique identifier, typically ISO timestamp), startTime (ISO 8601 creation timestamp), and private entries array. Constructor accepts `runId: string`.\n\n## Public Methods\n\n`addEntry(entry: TelemetryEntry): void` — Appends a telemetry entry to the internal entries array after an AI service call completes.\n\n`getEntries(): readonly TelemetryEntry[]` — Returns immutable view of accumulated entries array.\n\n`setFilesReadOnLastEntry(filesRead: FileRead[]): void` — Mutates the most recent entry's `filesRead` field with file metadata. Returns early if entries array is empty. Called by AIService after command runner attaches file read records to the last call.\n\n`getSummary(): RunLog['summary']` — Computes aggregate statistics from all recorded entries on every invocation (not cached). Iterates entries array to sum `totalInputTokens`, `totalOutputTokens`, `totalCacheReadTokens`, `totalCacheCreationTokens`, `totalDurationMs`, counts entries with `error !== undefined` for `errorCount`, sums `filesRead.length` for `totalFilesRead`, deduplicates file paths into `Set<string>` for `uniqueFilesRead`. Returns object with `totalCalls: this.entries.length` plus computed totals.\n\n`toRunLog(): RunLog` — Assembles complete `RunLog` object for serialization. Sets `endTime` to `new Date().toISOString()`, spreads entries array into new array, calls `getSummary()` for summary field. Intended for single invocation when run finishes.\n\n## Integration Points\n\nConsumes `TelemetryEntry`, `RunLog`, `FileRead` types from `../types.js`. Created once per CLI invocation by telemetry system. Lifecycle: instantiate with runId → accumulate entries via `addEntry()` → optionally update last entry with `setFilesReadOnLastEntry()` → finalize via `toRunLog()` when run completes. Used by `src/ai/telemetry/run-log.ts` for writing NDJSON run logs to `.agents-reverse-engineer/logs/`.\n\n## Data Structures\n\nMaintains single-level flat array of `TelemetryEntry` objects. No hierarchical grouping or phase-aware partitioning. Summary computation uses transient `Set<string>` for path deduplication without persisting the set.\n### run-log.ts\n**Purpose:** run-log.ts persists completed AI telemetry RunLog objects as pretty-printed JSON files in `.agents-reverse-engineer/l...\n\n**run-log.ts persists completed AI telemetry RunLog objects as pretty-printed JSON files in `.agents-reverse-engineer/logs/` with ISO-8601-derived filenames.**\n\n## Exported Functions\n\n### writeRunLog\n\n```typescript\nasync function writeRunLog(projectRoot: string, runLog: RunLog): Promise<string>\n```\n\nWrites a completed `RunLog` to disk as pretty-printed JSON (2-space indentation). Creates `<projectRoot>/.agents-reverse-engineer/logs/` directory via `fs.mkdir(logsDir, { recursive: true })` if missing. Derives filename from `runLog.startTime` by replacing `:` and `.` characters with `-` to ensure cross-platform filesystem compatibility, producing pattern `run-2026-02-07T12-00-00-000Z.json`. Returns absolute path to written file.\n\n## Constants\n\n`LOGS_DIR` — String constant `'.agents-reverse-engineer/logs'` defining relative path from project root to telemetry log storage directory.\n\n## Dependencies\n\n- `node:fs/promises` — Async file operations (`mkdir`, `writeFile`)\n- `node:path` — Path manipulation (`join`)\n- `../types.js` — Imports `RunLog` type interface\n\n## Integration Points\n\nConsumed by telemetry logger (`src/ai/telemetry/logger.ts`) during AIService call completion to persist aggregated token counts, costs, durations, and file metadata. Output files feed into retention management via cleanup utilities (`src/ai/telemetry/cleanup.ts`) enforcing `keepRuns` limits from configuration schema.\n\n## Import Map (verified — use these exact paths)\n\nlogger.ts:\n  ../types.js → TelemetryEntry, RunLog, FileRead (type)\n\nrun-log.ts:\n  ../types.js → RunLog (type)\n\n## Project Directory Structure\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>",
      "systemPrompt": "You are generating an AGENTS.md file — a directory-level overview for AI coding assistants.\n\nCRITICAL: Output ONLY the raw markdown content. No code fences, no preamble, no explanations, no conversational text. Your entire response IS the AGENTS.md file content.\n\nOUTPUT FORMAT:\n- First line MUST be exactly: <!-- Generated by agents-reverse-engineer -->\n- Use a # heading with the directory name\n- Write a one-paragraph purpose statement for the directory\nusage\nADAPTIVE SECTIONS:\nAnalyze the directory contents and choose the most relevant sections. Do NOT use a fixed template. Instead, select sections that best document this specific directory for an AI that needs to reconstruct or extend the project.\n\nConsider these section types (choose what applies):\n- **Contents**: Group files by purpose/category under ## headings. For each file: markdown link [filename](./filename) and a one-line description.\n- **Subdirectories**: If subdirectories exist, list them with links [dirname/](./dirname/) and brief summaries.\n- **Architecture / Data Flow**: If files form a pipeline, request/response chain, or layered architecture, document it.\n- **Stack**: If this is a package root (has package.json, Cargo.toml, go.mod, etc.), document the technology stack, key scripts, and entry points.\n- **Structure**: If the directory layout follows a convention (feature-sliced, domain-driven, MVC, etc.), document it.\n- **Patterns**: If files share recurring design patterns (factory, strategy, middleware, barrel re-export), name and document them.\n- **Configuration**: If the directory contains config files, schemas, or environment definitions, document the config surface area.\n- **API Surface**: If the directory exports a public API (barrel index, route definitions, SDK), document the interface contract.\n- **File Relationships**: How files collaborate, depend on each other, or share state.\n- **Behavioral Contracts**: If files contain regex patterns, format specifications, magic constants, or template strings that define observable behavior, collect them in a dedicated section. Preserve verbatim patterns from file summaries — do NOT paraphrase regex into prose. This section is MANDATORY when file summaries contain behavioral artifacts.\n- **Reproduction-Critical Constants**: If file summaries reference annex files (via ## Annex References sections), list them with links. Example: \"Full prompt template text: [templates.ts.annex.md](./prompts/templates.ts.annex.md)\". Do NOT reproduce annex content in AGENTS.md — just link to it.\n\nChoose any relevant sections or create your own based on the directory contents. The goal is to provide a comprehensive overview that captures the essence of the directory's role in the project and how its files work together, with a focus on what an AI coding assistant would need to know to effectively interact with this code.\n\nSCOPE:\n- AGENTS.md is a NAVIGATIONAL INDEX — help an AI find the right file quickly\n- Focus on: what each file does, how files relate, directory-level patterns\n- Do NOT reproduce full architecture sections — those belong in the root CLAUDE.md\n\nPATH ACCURACY (MANDATORY):\n- When referencing files or modules outside this directory, use ONLY paths from the \"Import Map\" section\n- Do NOT invent, rename, or guess module paths — if a path isn't in the Import Map, don't reference it\n- Use the exact directory names from \"Project Directory Structure\" — do NOT rename directories\n  (e.g., if the directory is called \"cli\", write \"src/cli/\", NOT \"src/commands/\")\n- Cross-module references must use the specifier format from actual import statements\n  (e.g., \"../generation/writers/sum.js\", NOT \"../fs/sum-file.js\")\n- If you are unsure about a path, omit the cross-reference rather than guessing\n\nCONSISTENCY (MANDATORY):\n- Do not contradict yourself within the same document\n- If you describe a technique (e.g., \"regex-based\"), do not call it something else later (e.g., \"AST-based\")\n- When stating version numbers, engines, or config fields, use ONLY values present in the file summaries\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this directory\", \"this module\", \"provides\", \"responsible for\", \"is used to\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Per-file descriptions in Contents sections: 1-2 sentences maximum. Reference key symbols but do not reproduce full summaries.\n- Behavioral contracts (regex patterns, format specs, constants) belong in a separate Behavioral Contracts section, not in per-file descriptions.\n- Annex file references: link to .annex.md files, do not inline their content. One line per annex reference.\n- Subdirectory descriptions: 1-2 sentences maximum. Capture the directory's role, not its full contents.\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- Key exported symbols from file summaries MUST appear in the directory overview\n- Preserve exact casing of identifiers\n\nUSER NOTES:\n- If \"User Notes\" are provided in the prompt, they contain user-defined instructions that will be automatically prepended to your output\n- Do NOT repeat or paraphrase user notes in your generated content — they are included separately\n- You may reference information from user notes for context",
      "response": "<!-- Generated by agents-reverse-engineer -->\n\n# src/ai/telemetry\n\nTelemetry subsystem for accumulating AI service call metrics, writing JSON run logs with retention management, and tracking token costs, durations, and file metadata across concurrent worker pool operations.\n\n## Contents\n\n### [cleanup.ts](./cleanup.ts)\nExports `cleanupOldLogs(projectRoot, keepCount)` deleting expired telemetry logs from `.agents-reverse-engineer/logs/` by lexicographic timestamp sorting on `run-*.json` filenames. Returns count of deleted files.\n\n### [logger.ts](./logger.ts)\nExports `TelemetryLogger` class accumulating `TelemetryEntry` instances in memory during CLI runs. Methods: `addEntry()` appends call metrics, `setFilesReadOnLastEntry()` attaches file metadata to most recent entry, `getSummary()` computes aggregate statistics (token totals, error counts, unique file paths), `toRunLog()` serializes complete `RunLog` object.\n\n### [run-log.ts](./run-log.ts)\nExports `writeRunLog(projectRoot, runLog)` persisting `RunLog` as pretty-printed JSON with ISO-8601-derived filename pattern `run-2026-02-07T12-00-00-000Z.json`. Creates `.agents-reverse-engineer/logs/` directory via `mkdir(recursive: true)`.\n\n## Data Flow\n\n1. `TelemetryLogger` instantiated once per CLI invocation with unique `runId` (ISO timestamp)\n2. AIService calls `logger.addEntry(entry)` after each subprocess completion with token counts, cost, duration, error status\n3. Command runner invokes `logger.setFilesReadOnLastEntry(filesRead)` to attach file metadata to last entry\n4. On run completion, caller invokes `logger.toRunLog()` → serializes in-memory entries + summary\n5. `writeRunLog(projectRoot, runLog)` persists JSON to disk with timestamp-derived filename\n6. `cleanupOldLogs(projectRoot, keepCount)` enforces retention policy by deleting oldest logs exceeding `keepRuns` threshold (default 50)\n\n## Behavioral Contracts\n\n**Filename derivation pattern** (from `run-log.ts`):\n```typescript\nconst timestamp = runLog.startTime.replace(/:/g, '-').replace(/\\./g, '-');\nconst filename = `run-${timestamp}.json`;\n```\n\n**Log directory constant** (shared across all modules):\n```typescript\nconst LOGS_DIR = '.agents-reverse-engineer/logs';\n```\n\n**Cleanup sort order** (from `cleanup.ts`):\n```typescript\nentries.sort();      // Lexicographic ascending\nentries.reverse();   // Newest first\nconst toDelete = entries.slice(keepCount);  // Oldest logs\n```\n\n**Summary deduplication** (from `logger.ts`):\n```typescript\nconst uniquePaths = new Set<string>();\nfor (const entry of this.entries) {\n  for (const file of entry.filesRead || []) {\n    uniquePaths.add(file.path);\n  }\n}\nreturn { uniqueFilesRead: uniquePaths.size };\n```\n\n## Integration Points\n\n- **Created by**: `src/ai/service.ts` constructs `TelemetryLogger` when `config.ai.telemetry.enabled === true`\n- **Consumed by**: `src/orchestration/runner.ts` invokes `writeRunLog()` after Phase 3 completion, then calls `cleanupOldLogs()` with `config.ai.telemetry.keepRuns`\n- **Types imported from**: `../types.js` provides `TelemetryEntry`, `RunLog`, `FileRead` interfaces\n- **Related configuration**: `src/config/schema.ts` defines `ai.telemetry.enabled`, `ai.telemetry.keepRuns`, `ai.telemetry.costThresholdUsd` validation rules\n\n## Design Notes\n\n`TelemetryLogger` uses eager summary computation on every `getSummary()` invocation without caching — acceptable since `toRunLog()` called once per run. Single-level flat `entries` array without phase-aware partitioning simplifies aggregation logic at cost of phase-specific analytics. Cleanup operates on filename lexicographic order assuming ISO 8601 timestamp sort equivalence (year-month-day-hour-minute-second-millisecond).",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 1050,
      "cacheReadTokens": 16334,
      "cacheCreationTokens": 11832,
      "latencyMs": 26801,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": []
    },
    {
      "timestamp": "2026-02-09T17:46:57.272Z",
      "prompt": "Generate AGENTS.md for directory: \"src/quality/inconsistency\" (inconsistency)\n\n## File Summaries (3 files)\n\n### code-vs-code.ts\n**Purpose:** checkCodeVsCode() detects duplicate export symbols across source files within a scoped group (typically per-directory...\n\n**checkCodeVsCode() detects duplicate export symbols across source files within a scoped group (typically per-directory) using heuristic regex-based extraction, flagging symbols exported from multiple files as CodeCodeInconsistency warnings without AST analysis.**\n\n## Exported Function\n\n**checkCodeVsCode(files: Array<{ path: string; content: string }>): CodeCodeInconsistency[]**\n\nAccepts array of file objects with `path` and `content` properties. For each file, invokes `extractExports()` from `code-vs-doc.js` to extract exported symbol names via regex pattern `/^[ \\t]*export\\s+(?:default\\s+)?(?:function|class|const|let|var|type|interface|enum)\\s+(\\w+)/gm`. Aggregates symbols into `Map<string, string[]>` where key is export name and value is array of file paths exporting that symbol. Returns array of `CodeCodeInconsistency` objects for symbols appearing in more than one file.\n\n## Algorithm\n\n1. Initialize `exportMap` as `Map<string, string[]>` to track symbol-to-paths mapping\n2. Iterate each file in input array:\n   - Call `extractExports(file.content)` to get exported symbol names\n   - For each exported name:\n     - If name exists in map, push `file.path` to existing array\n     - Otherwise, create new entry with `[file.path]`\n3. Iterate exportMap entries:\n   - If `paths.length > 1`, construct `CodeCodeInconsistency` object with:\n     - `type: 'code-vs-code'`\n     - `severity: 'warning'`\n     - `files: paths` (all file paths exporting the symbol)\n     - `description: \"Symbol \\\"${name}\\\" exported from ${paths.length} files\"`\n     - `pattern: 'duplicate-export'`\n4. Return accumulated inconsistencies array\n\n## Dependencies\n\n**extractExports()** from `./code-vs-doc.js` — Regex-based export symbol extractor using pattern `/^[ \\t]*export\\s+(?:default\\s+)?(?:function|class|const|let|var|type|interface|enum)\\s+(\\w+)/gm`\n\n**CodeCodeInconsistency** from `../types.js` — Discriminated union member with `type: 'code-vs-code'`, `severity: 'warning'`, `files: string[]`, `description: string`, `pattern: 'duplicate-export'`\n\n## Usage Context\n\nCaller must scope input files to prevent false positives. Typical usage: per-directory validation in `src/quality/index.ts` where files are grouped by containing directory before invoking `checkCodeVsCode()`. Cross-directory duplicate detection would flag legitimate symbol reuse across unrelated modules (e.g., `index.ts` in multiple directories commonly exports same helper names).\n\n## Known Limitations\n\nHeuristic regex extraction misses:\n- Destructured exports: `export const { foo, bar } = obj`\n- Namespace exports: `export * as Utils from './utils'`\n- Dynamic exports: `export { [computedName]: value }`\n- Re-exports with renaming: `export { foo as bar } from './mod'`\n\nNo AST analysis to distinguish intentional duplication (e.g., interface/implementation pairs, test fixtures) from architectural violations. Operates purely on symbol name matching without semantic understanding of whether duplicates represent distinct types, overloads, or genuinely conflicting definitions.\n### code-vs-doc.ts\n**Purpose:** Extracts exported identifiers from TypeScript/JavaScript source and detects code-vs-doc inconsistencies by comparing ...\n\n**Extracts exported identifiers from TypeScript/JavaScript source and detects code-vs-doc inconsistencies by comparing exports against `.sum` file content.**\n\n## Exported Functions\n\n**`extractExports(sourceContent: string): string[]`**\nExtracts exported identifier names from TypeScript/JavaScript source using regex pattern `/^[ \\t]*export\\s+(?:default\\s+)?(?:function|class|const|let|var|type|interface|enum)\\s+(\\w+)/gm`. Matches export declarations including `export function`, `export class`, `export const`, `export default class`, etc. Ignores re-exports, commented lines, and non-exported declarations. Returns array of exported symbol names.\n\n**`checkCodeVsDoc(sourceContent: string, sumContent: SumFileContent, filePath: string): CodeDocInconsistency | null`**\nCompares source exports against `.sum` documentation content via case-sensitive substring matching. Detects `missingFromDoc` inconsistencies (symbols exported in source but absent from `sumContent.summary` text). Returns `CodeDocInconsistency` with `type: 'code-vs-doc'`, `severity: 'warning'`, `filePath`, `sumPath`, `description`, and `details: { missingFromDoc, missingFromCode }`. Returns `null` when all exports appear in summary text.\n\n## Dependencies\n\nImports `SumFileContent` from `../../generation/writers/sum.js` for parsed `.sum` file structure and `CodeDocInconsistency` from `../types.js` for inconsistency report format.\n\n## Validation Strategy\n\nUses regex-based export extraction combined with substring search in summary text. Does not perform AST analysis, so misses complex export patterns (destructured, namespace, dynamic exports). Substring matching yields false negatives when prose mentions symbols in non-API contexts. The `missingFromCode` array is always empty (legacy field from removed `publicInterface` schema).\n\n## Behavioral Contracts\n\nExport extraction pattern: `/^[ \\t]*export\\s+(?:default\\s+)?(?:function|class|const|let|var|type|interface|enum)\\s+(\\w+)/gm` matches line-start whitespace, `export` keyword, optional `default`, declaration keyword, and captures identifier `(\\w+)`.\n### reporter.ts\n**Purpose:** reporter.ts exports buildInconsistencyReport() and formatReportForCli() to aggregate validation issues into typed rep...\n\n**reporter.ts exports buildInconsistencyReport() and formatReportForCli() to aggregate validation issues into typed reports with summary counts and render them as plain text for CLI output.**\n\n## Exported Functions\n\n### buildInconsistencyReport\n\n```typescript\nfunction buildInconsistencyReport(\n  issues: Inconsistency[],\n  metadata: { projectRoot: string; filesChecked: number; durationMs: number }\n): InconsistencyReport\n```\n\nAggregates `Inconsistency[]` array into structured `InconsistencyReport` with computed summary counts. Iterates issues once, incrementing counters for type classification (`codeVsDoc`, `codeVsCode`, `phantomPaths`) and severity classification (`errors`, `warnings`, `info`). Returns report object containing `metadata` (timestamp via `new Date().toISOString()`, projectRoot, filesChecked, durationMs), `issues` array passthrough, and `summary` object with totals.\n\n### formatReportForCli\n\n```typescript\nfunction formatReportForCli(report: InconsistencyReport): string\n```\n\nTransforms `InconsistencyReport` into plain text string for stderr output. Renders header with metadata (`Checked N files in Nms`, `Found N issue(s)`), then iterates `report.issues` emitting severity tags (`[ERROR]`, `[WARN]`, `[INFO]`) with `issue.description`. Type-discriminated path formatting: `code-vs-doc` shows `filePath`, `phantom-path` shows `agentsMdPath` and `details.referencedPath`, `code-vs-code` shows `files.join(', ')`. Returns newline-joined string. No color formatting applied (pure text output for testing).\n\n## Type Dependencies\n\nImports `Inconsistency` and `InconsistencyReport` from `../types.js`. `Inconsistency` is discriminated union with `type` field (`'code-vs-doc' | 'code-vs-code' | 'phantom-path'`) and `severity` field (`'error' | 'warning' | 'info'`). `InconsistencyReport` contains `metadata`, `issues`, and `summary` with per-type/per-severity counts.\n\n## Integration Points\n\nCalled by quality validation orchestrator in `src/quality/index.ts` after running `findCodeVsDocInconsistencies()`, `findCodeVsCodeInconsistencies()`, and `findPhantomPaths()`. CLI layer in `src/cli/generate.ts` or `src/cli/update.ts` invokes `formatReportForCli()` for console output after validation completes. Terminal color application handled upstream (reporter remains color-agnostic).\n\n## Output Format\n\nPlain text structure emitted by `formatReportForCli()`:\n```\n=== Inconsistency Report ===\nChecked 42 files in 150ms\nFound 3 issue(s)\n\n[ERROR] Documentation out of sync: 2 exports undocumented, 1 ...\n  File: src/foo/bar.ts\n\n[WARN] Symbol \"Config\" exported from 2 files\n  Files: src/config/a.ts, src/config/b.ts\n```\n\nHuman-readable severity tags (`[ERROR]`, `[WARN]`, `[INFO]`) followed by description and type-specific paths. Empty line separates issues for readability.\n\n## Import Map (verified — use these exact paths)\n\ncode-vs-code.ts:\n  ../types.js → CodeCodeInconsistency (type)\n\ncode-vs-doc.ts:\n  ../../generation/writers/sum.js → SumFileContent (type)\n  ../types.js → CodeDocInconsistency (type)\n\nreporter.ts:\n  ../types.js → Inconsistency, InconsistencyReport (type)\n\n## Project Directory Structure\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>",
      "systemPrompt": "You are generating an AGENTS.md file — a directory-level overview for AI coding assistants.\n\nCRITICAL: Output ONLY the raw markdown content. No code fences, no preamble, no explanations, no conversational text. Your entire response IS the AGENTS.md file content.\n\nOUTPUT FORMAT:\n- First line MUST be exactly: <!-- Generated by agents-reverse-engineer -->\n- Use a # heading with the directory name\n- Write a one-paragraph purpose statement for the directory\nusage\nADAPTIVE SECTIONS:\nAnalyze the directory contents and choose the most relevant sections. Do NOT use a fixed template. Instead, select sections that best document this specific directory for an AI that needs to reconstruct or extend the project.\n\nConsider these section types (choose what applies):\n- **Contents**: Group files by purpose/category under ## headings. For each file: markdown link [filename](./filename) and a one-line description.\n- **Subdirectories**: If subdirectories exist, list them with links [dirname/](./dirname/) and brief summaries.\n- **Architecture / Data Flow**: If files form a pipeline, request/response chain, or layered architecture, document it.\n- **Stack**: If this is a package root (has package.json, Cargo.toml, go.mod, etc.), document the technology stack, key scripts, and entry points.\n- **Structure**: If the directory layout follows a convention (feature-sliced, domain-driven, MVC, etc.), document it.\n- **Patterns**: If files share recurring design patterns (factory, strategy, middleware, barrel re-export), name and document them.\n- **Configuration**: If the directory contains config files, schemas, or environment definitions, document the config surface area.\n- **API Surface**: If the directory exports a public API (barrel index, route definitions, SDK), document the interface contract.\n- **File Relationships**: How files collaborate, depend on each other, or share state.\n- **Behavioral Contracts**: If files contain regex patterns, format specifications, magic constants, or template strings that define observable behavior, collect them in a dedicated section. Preserve verbatim patterns from file summaries — do NOT paraphrase regex into prose. This section is MANDATORY when file summaries contain behavioral artifacts.\n- **Reproduction-Critical Constants**: If file summaries reference annex files (via ## Annex References sections), list them with links. Example: \"Full prompt template text: [templates.ts.annex.md](./prompts/templates.ts.annex.md)\". Do NOT reproduce annex content in AGENTS.md — just link to it.\n\nChoose any relevant sections or create your own based on the directory contents. The goal is to provide a comprehensive overview that captures the essence of the directory's role in the project and how its files work together, with a focus on what an AI coding assistant would need to know to effectively interact with this code.\n\nSCOPE:\n- AGENTS.md is a NAVIGATIONAL INDEX — help an AI find the right file quickly\n- Focus on: what each file does, how files relate, directory-level patterns\n- Do NOT reproduce full architecture sections — those belong in the root CLAUDE.md\n\nPATH ACCURACY (MANDATORY):\n- When referencing files or modules outside this directory, use ONLY paths from the \"Import Map\" section\n- Do NOT invent, rename, or guess module paths — if a path isn't in the Import Map, don't reference it\n- Use the exact directory names from \"Project Directory Structure\" — do NOT rename directories\n  (e.g., if the directory is called \"cli\", write \"src/cli/\", NOT \"src/commands/\")\n- Cross-module references must use the specifier format from actual import statements\n  (e.g., \"../generation/writers/sum.js\", NOT \"../fs/sum-file.js\")\n- If you are unsure about a path, omit the cross-reference rather than guessing\n\nCONSISTENCY (MANDATORY):\n- Do not contradict yourself within the same document\n- If you describe a technique (e.g., \"regex-based\"), do not call it something else later (e.g., \"AST-based\")\n- When stating version numbers, engines, or config fields, use ONLY values present in the file summaries\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this directory\", \"this module\", \"provides\", \"responsible for\", \"is used to\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Per-file descriptions in Contents sections: 1-2 sentences maximum. Reference key symbols but do not reproduce full summaries.\n- Behavioral contracts (regex patterns, format specs, constants) belong in a separate Behavioral Contracts section, not in per-file descriptions.\n- Annex file references: link to .annex.md files, do not inline their content. One line per annex reference.\n- Subdirectory descriptions: 1-2 sentences maximum. Capture the directory's role, not its full contents.\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- Key exported symbols from file summaries MUST appear in the directory overview\n- Preserve exact casing of identifiers\n\nUSER NOTES:\n- If \"User Notes\" are provided in the prompt, they contain user-defined instructions that will be automatically prepended to your output\n- Do NOT repeat or paraphrase user notes in your generated content — they are included separately\n- You may reference information from user notes for context",
      "response": "<!-- Generated by agents-reverse-engineer -->\n\n# src/quality/inconsistency\n\nDetects and reports three categories of code-documentation discrepancies: exported symbols missing from `.sum` summaries (code-vs-doc), duplicate exports across files (code-vs-code), and unresolvable path references in `AGENTS.md` (phantom-paths).\n\n## Contents\n\n### Validators\n\n**[code-vs-doc.ts](./code-vs-doc.ts)** — Exports `extractExports()` (regex-based identifier extraction via `/^[ \\t]*export\\s+(?:default\\s+)?(?:function|class|const|let|var|type|interface|enum)\\s+(\\w+)/gm`) and `checkCodeVsDoc()` (compares source exports against `SumFileContent.summary` via substring matching, returns `CodeDocInconsistency` with `missingFromDoc` array or `null`).\n\n**[code-vs-code.ts](./code-vs-code.ts)** — Exports `checkCodeVsCode()` aggregating exports across scoped file groups into `Map<symbol, paths[]>`, returns `CodeCodeInconsistency[]` for symbols appearing in multiple files with `pattern: 'duplicate-export'`.\n\n**[reporter.ts](./reporter.ts)** — Exports `buildInconsistencyReport()` (aggregates `Inconsistency[]` into `InconsistencyReport` with per-type/per-severity summary counts) and `formatReportForCli()` (renders plain text output with severity tags `[ERROR]`, `[WARN]`, `[INFO]` and type-discriminated path formatting).\n\n## Validation Strategy\n\n**Code-vs-Doc:** Regex extraction from source followed by case-sensitive substring search in `.sum` summary text. Does not use AST analysis, yielding false negatives for destructured/namespace/dynamic exports and prose mentions unrelated to API surface.\n\n**Code-vs-Code:** Symbol-name deduplication across per-directory file groups. Caller must scope input to prevent false positives from legitimate cross-directory symbol reuse (e.g., multiple `index.ts` files exporting `Config`).\n\n**Phantom Paths:** Not implemented in this directory. Handled by `../phantom-paths/validator.ts` which extracts markdown links, backtick paths, and prose-embedded paths from `AGENTS.md`, resolves via `existsSync()` with `.ts`/`.js` fallback.\n\n## File Relationships\n\n`code-vs-code.ts` imports `extractExports()` from `code-vs-doc.ts` for symbol extraction reuse. `reporter.ts` consumes discriminated union `Inconsistency` (from `../types.ts`) unifying `CodeDocInconsistency`, `CodeCodeInconsistency`, and `PhantomPathInconsistency`. Orchestrator in `../index.ts` invokes validators and passes results to `buildInconsistencyReport()` → `formatReportForCli()` pipeline.\n\n## Behavioral Contracts\n\n**Export extraction pattern:** `/^[ \\t]*export\\s+(?:default\\s+)?(?:function|class|const|let|var|type|interface|enum)\\s+(\\w+)/gm` matches line-start whitespace, `export` keyword, optional `default`, declaration keyword (`function`/`class`/`const`/`let`/`var`/`type`/`interface`/`enum`), captures identifier `(\\w+)`.\n\n**CLI output format:**\n```\n=== Inconsistency Report ===\nChecked 42 files in 150ms\nFound 3 issue(s)\n\n[ERROR] Documentation out of sync: 2 exports undocumented\n  File: src/foo/bar.ts\n\n[WARN] Symbol \"Config\" exported from 2 files\n  Files: src/config/a.ts, src/config/b.ts\n```\n\nHuman-readable severity tags followed by description and type-specific paths (code-vs-doc shows `filePath`, code-vs-code shows `files.join(', ')`, phantom-path shows `agentsMdPath` and `details.referencedPath`).\n\n## Known Limitations\n\nRegex-based extraction misses destructured exports (`export const { foo } = obj`), namespace exports (`export * as Utils from './utils'`), dynamic exports (`export { [computedName]: value }`), re-exports with renaming (`export { foo as bar } from './mod'`). No AST analysis to distinguish intentional duplication (interface/implementation pairs, test fixtures) from architectural violations. Substring matching yields false negatives when prose mentions symbols in non-API contexts.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 1098,
      "cacheReadTokens": 16334,
      "cacheCreationTokens": 12628,
      "latencyMs": 34390,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": []
    },
    {
      "timestamp": "2026-02-09T17:46:57.262Z",
      "prompt": "Generate AGENTS.md for directory: \"src/ai/backends\" (backends)\n\n## File Summaries (3 files)\n\n### claude.ts\n**Purpose:** ClaudeBackend implements AIBackend interface for Claude Code CLI, providing PATH detection via isCommandOnPath(), arg...\n\n**ClaudeBackend implements AIBackend interface for Claude Code CLI, providing PATH detection via isCommandOnPath(), argument construction with permission bypass flags, JSON response parsing with Zod validation against ClaudeResponseSchema, and installation instructions.**\n\n## Exported Interface\n\n`ClaudeBackend` class implements `AIBackend` interface with:\n- `name: 'claude'` — Backend identifier string\n- `cliCommand: 'claude'` — Executable name for PATH lookup\n- `isAvailable(): Promise<boolean>` — Delegates to `isCommandOnPath('claude')`\n- `buildArgs(options: AICallOptions): string[]` — Constructs CLI arguments array\n- `parseResponse(stdout: string, durationMs: number, exitCode: number): AIResponse` — Parses JSON output into normalized response\n- `getInstallInstructions(): string` — Returns npm install command for `@anthropic-ai/claude-code`\n\n`isCommandOnPath(command: string): Promise<boolean>` exported function splits `process.env.PATH` by platform delimiter, iterates through directories, checks file existence via `fs.stat()` (not `fs.access()` for cross-platform compatibility), handles Windows `PATHEXT` environment variable for extension variants (`.exe`, `.cmd`, `.bat`).\n\n## CLI Arguments Construction\n\n`buildArgs()` returns fixed arguments array:\n- `-p` — Non-interactive print mode\n- `--output-format json` — Structured JSON output\n- `--no-session-persistence` — Prevent session disk writes\n- `--permission-mode bypassPermissions` — Skip interactive permission prompts (PITFALLS.md §8)\n\nConditional arguments from `AICallOptions`:\n- `--model <model>` if `options.model` provided\n- `--system-prompt <systemPrompt>` if `options.systemPrompt` provided\n- `--max-turns <maxTurns>` if `options.maxTurns !== undefined`\n\nPrompt text NOT included in args array — delivered to stdin via `runSubprocess()` wrapper.\n\n## Response Schema Validation\n\n`ClaudeResponseSchema` validated against Claude CLI v2.1.31 JSON output (see RESEARCH.md):\n```typescript\n{\n  type: 'result',\n  subtype: 'success' | 'error',\n  is_error: boolean,\n  duration_ms: number,\n  duration_api_ms: number,\n  num_turns: number,\n  result: string,\n  session_id: string,\n  total_cost_usd: number,\n  usage: {\n    input_tokens: number,\n    cache_creation_input_tokens: number,\n    cache_read_input_tokens: number,\n    output_tokens: number,\n  },\n  modelUsage: Record<string, {\n    inputTokens: number,\n    outputTokens: number,\n    cacheReadInputTokens: number,\n    cacheCreationInputTokens: number,\n    costUSD: number,\n  }>\n}\n```\n\n`parseResponse()` defensively finds first `{` character via `stdout.indexOf('{')` to handle non-JSON prefix text (upgrade notices per RESEARCH.md Pitfall 4). Extracts model name from first key in `modelUsage` object (defaults to `'unknown'`). Throws `AIServiceError` with code `PARSE_ERROR` if JSON missing or Zod validation fails.\n\n## PATH Detection Algorithm\n\n`isCommandOnPath()` algorithm:\n1. Parse `process.env.PATH` with `path.delimiter` (`;` on Windows, `:` on Unix)\n2. Strip quote characters via `.replace(/[\"]+/g, '')`\n3. Parse `process.env.PATHEXT` on Windows for executable extensions (empty array on Unix defaults to `['']`)\n4. Nested loop: for each directory × extension, construct candidate path via `path.join(dir, command + ext)`\n5. Check `(await fs.stat(candidate)).isFile()` — returns `true` on first match, `false` if all candidates fail\n\nUses `fs.stat()` instead of `fs.access(fs.constants.X_OK)` because Windows lacks Unix execute permission bits.\n\n## Dependencies\n\n- `node:fs/promises` — `fs.stat()` for PATH detection\n- `node:path` — `path.join()`, `path.delimiter` for cross-platform path manipulation\n- `zod` — `ClaudeResponseSchema` validation, `z.infer<>` type extraction\n- `../types.js` — `AIBackend`, `AICallOptions`, `AIResponse`, `AIServiceError` imports\n\n## Integration Points\n\n`ClaudeBackend` consumed by `AIService` in `src/ai/service.ts` via backend registry. `runSubprocess()` in `src/ai/subprocess.ts` receives args from `buildArgs()` and stdout for `parseResponse()`. `isCommandOnPath()` utility reusable for other backend availability checks.\n### gemini.ts\n**Purpose:** GeminiBackend implements AIBackend interface as a stub adapter for the Gemini CLI, providing command detection and ar...\n\n**GeminiBackend implements AIBackend interface as a stub adapter for the Gemini CLI, providing command detection and argument construction but throwing AIServiceError on response parsing until JSON output format stabilizes.**\n\n## Implementation Status\n\nGeminiBackend is a non-functional stub demonstrating the extension pattern for future Gemini CLI integration. The `parseResponse()` method always throws `AIServiceError` with code `'SUBPROCESS_ERROR'` and message `'Gemini backend is not yet implemented. Use Claude backend.'` This deferred implementation awaits stable JSON output from the Gemini CLI (referenced as \"RESEARCH.md Open Question 2\" in module documentation).\n\n## Exported Class\n\n`export class GeminiBackend implements AIBackend` provides:\n\n- `readonly name = 'gemini'` — Backend identifier string used by AIBackendRegistry\n- `readonly cliCommand = 'gemini'` — Executable name for PATH resolution\n\n## Public Methods\n\n`async isAvailable(): Promise<boolean>` delegates to `isCommandOnPath(this.cliCommand)` imported from `claude.ts` to detect `gemini` binary on system PATH.\n\n`buildArgs(_options: AICallOptions): string[]` constructs CLI argument array `['-p', '--output-format', 'json']` following documented Gemini CLI flags. The prompt is injected via stdin by the subprocess wrapper in `src/ai/subprocess.ts`. The `_options` parameter is unused in this stub implementation.\n\n`parseResponse(_stdout: string, _durationMs: number, _exitCode: number): AIResponse` unconditionally throws `AIServiceError` with `'SUBPROCESS_ERROR'` code, blocking actual response parsing until full implementation.\n\n`getInstallInstructions(): string` returns multiline installation guide string with NPM install command `npm install -g @anthropic-ai/gemini-cli` and GitHub repository URL `https://github.com/google-gemini/gemini-cli`.\n\n## Dependencies\n\nImports `AIBackend`, `AICallOptions`, `AIResponse`, and `AIServiceError` from `../types.js`. Imports `isCommandOnPath` utility from `./claude.js` for command detection logic.\n\n## Integration Points\n\nRegistered with AIBackendRegistry in `src/ai/registry.ts` alongside ClaudeBackend and OpenCodeBackend. The `isAvailable()` check determines backend auto-detection fallback order when `ai.backend: 'auto'` configured in `.agents-reverse-engineer/config.yaml`. Despite registration, any attempt to use this backend for actual AI calls will fail at response parsing phase.\n### opencode.ts\n**Purpose:** OpenCodeBackend implements AIBackend interface as a stub demonstrating extension pattern for OpenCode CLI integration...\n\n**OpenCodeBackend implements AIBackend interface as a stub demonstrating extension pattern for OpenCode CLI integration, throwing \"not implemented\" errors until JSONL output parsing is built.**\n\n## Exported Class\n\n`OpenCodeBackend` implements `AIBackend` interface with four readonly/method members: `name: 'opencode'`, `cliCommand: 'opencode'`, `isAvailable(): Promise<boolean>`, `buildArgs(_options: AICallOptions): string[]`, `parseResponse(_stdout: string, _durationMs: number, _exitCode: number): AIResponse`, `getInstallInstructions(): string`.\n\n## Implementation Status\n\n`parseResponse()` throws `AIServiceError` with code `'SUBPROCESS_ERROR'` and message `'OpenCode backend is not yet implemented. Use Claude backend.'` blocking all AI calls until JSONL parsing implemented (deferred per RESEARCH.md Open Question 3).\n\n## CLI Interaction\n\n`isAvailable()` delegates to `isCommandOnPath(this.cliCommand)` imported from `claude.ts` module to detect `opencode` binary on PATH.\n\n`buildArgs()` returns `['run', '--format', 'json']` based on documented OpenCode CLI flags from RESEARCH.md, expects prompt via stdin through subprocess wrapper.\n\n`getInstallInstructions()` returns three-line string with curl install command `'curl -fsSL https://opencode.ai/install | bash'` and URL `'https://opencode.ai'`.\n\n## Integration Points\n\nImports `AIBackend`, `AICallOptions`, `AIResponse` from `../types.js` and `AIServiceError` from same module for error throwing. Imports `isCommandOnPath` utility from `./claude.ts` for CLI detection. Follows same interface contract as `ClaudeBackend` and `GeminiBackend` siblings, enabling registration in `src/ai/registry.ts` backend map.\n\n## Import Map (verified — use these exact paths)\n\nclaude.ts:\n  ../types.js → AIBackend, AICallOptions, AIResponse (type)\n  ../types.js → AIServiceError\n\ngemini.ts:\n  ../types.js → AIBackend, AICallOptions, AIResponse (type)\n  ../types.js → AIServiceError\n\nopencode.ts:\n  ../types.js → AIBackend, AICallOptions, AIResponse (type)\n  ../types.js → AIServiceError\n\n## Project Directory Structure\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>",
      "systemPrompt": "You are generating an AGENTS.md file — a directory-level overview for AI coding assistants.\n\nCRITICAL: Output ONLY the raw markdown content. No code fences, no preamble, no explanations, no conversational text. Your entire response IS the AGENTS.md file content.\n\nOUTPUT FORMAT:\n- First line MUST be exactly: <!-- Generated by agents-reverse-engineer -->\n- Use a # heading with the directory name\n- Write a one-paragraph purpose statement for the directory\nusage\nADAPTIVE SECTIONS:\nAnalyze the directory contents and choose the most relevant sections. Do NOT use a fixed template. Instead, select sections that best document this specific directory for an AI that needs to reconstruct or extend the project.\n\nConsider these section types (choose what applies):\n- **Contents**: Group files by purpose/category under ## headings. For each file: markdown link [filename](./filename) and a one-line description.\n- **Subdirectories**: If subdirectories exist, list them with links [dirname/](./dirname/) and brief summaries.\n- **Architecture / Data Flow**: If files form a pipeline, request/response chain, or layered architecture, document it.\n- **Stack**: If this is a package root (has package.json, Cargo.toml, go.mod, etc.), document the technology stack, key scripts, and entry points.\n- **Structure**: If the directory layout follows a convention (feature-sliced, domain-driven, MVC, etc.), document it.\n- **Patterns**: If files share recurring design patterns (factory, strategy, middleware, barrel re-export), name and document them.\n- **Configuration**: If the directory contains config files, schemas, or environment definitions, document the config surface area.\n- **API Surface**: If the directory exports a public API (barrel index, route definitions, SDK), document the interface contract.\n- **File Relationships**: How files collaborate, depend on each other, or share state.\n- **Behavioral Contracts**: If files contain regex patterns, format specifications, magic constants, or template strings that define observable behavior, collect them in a dedicated section. Preserve verbatim patterns from file summaries — do NOT paraphrase regex into prose. This section is MANDATORY when file summaries contain behavioral artifacts.\n- **Reproduction-Critical Constants**: If file summaries reference annex files (via ## Annex References sections), list them with links. Example: \"Full prompt template text: [templates.ts.annex.md](./prompts/templates.ts.annex.md)\". Do NOT reproduce annex content in AGENTS.md — just link to it.\n\nChoose any relevant sections or create your own based on the directory contents. The goal is to provide a comprehensive overview that captures the essence of the directory's role in the project and how its files work together, with a focus on what an AI coding assistant would need to know to effectively interact with this code.\n\nSCOPE:\n- AGENTS.md is a NAVIGATIONAL INDEX — help an AI find the right file quickly\n- Focus on: what each file does, how files relate, directory-level patterns\n- Do NOT reproduce full architecture sections — those belong in the root CLAUDE.md\n\nPATH ACCURACY (MANDATORY):\n- When referencing files or modules outside this directory, use ONLY paths from the \"Import Map\" section\n- Do NOT invent, rename, or guess module paths — if a path isn't in the Import Map, don't reference it\n- Use the exact directory names from \"Project Directory Structure\" — do NOT rename directories\n  (e.g., if the directory is called \"cli\", write \"src/cli/\", NOT \"src/commands/\")\n- Cross-module references must use the specifier format from actual import statements\n  (e.g., \"../generation/writers/sum.js\", NOT \"../fs/sum-file.js\")\n- If you are unsure about a path, omit the cross-reference rather than guessing\n\nCONSISTENCY (MANDATORY):\n- Do not contradict yourself within the same document\n- If you describe a technique (e.g., \"regex-based\"), do not call it something else later (e.g., \"AST-based\")\n- When stating version numbers, engines, or config fields, use ONLY values present in the file summaries\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this directory\", \"this module\", \"provides\", \"responsible for\", \"is used to\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Per-file descriptions in Contents sections: 1-2 sentences maximum. Reference key symbols but do not reproduce full summaries.\n- Behavioral contracts (regex patterns, format specs, constants) belong in a separate Behavioral Contracts section, not in per-file descriptions.\n- Annex file references: link to .annex.md files, do not inline their content. One line per annex reference.\n- Subdirectory descriptions: 1-2 sentences maximum. Capture the directory's role, not its full contents.\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- Key exported symbols from file summaries MUST appear in the directory overview\n- Preserve exact casing of identifiers\n\nUSER NOTES:\n- If \"User Notes\" are provided in the prompt, they contain user-defined instructions that will be automatically prepended to your output\n- Do NOT repeat or paraphrase user notes in your generated content — they are included separately\n- You may reference information from user notes for context",
      "response": "<!-- Generated by agents-reverse-engineer -->\n\n# src/ai/backends\n\n**AI backend adapter layer implementing AIBackend interface for Claude Code, Gemini, and OpenCode CLIs with PATH detection, argument construction, JSON response parsing, and installation instructions.**\n\n## Contents\n\n### Backend Adapters\n\n**[claude.ts](./claude.ts)** — ClaudeBackend with isCommandOnPath() PATH detection, buildArgs() permission bypass flags (`--permission-mode bypassPermissions`, `--no-session-persistence`), parseResponse() with ClaudeResponseSchema Zod validation, defensive JSON extraction via stdout.indexOf('{'), and modelUsage-based model name extraction.\n\n**[gemini.ts](./gemini.ts)** — GeminiBackend stub implementing AIBackend interface with isCommandOnPath() delegation, buildArgs() JSON output flags, parseResponse() throwing 'SUBPROCESS_ERROR' AIServiceError until stable JSON format available.\n\n**[opencode.ts](./opencode.ts)** — OpenCodeBackend stub implementing AIBackend interface with isCommandOnPath() delegation, buildArgs() returning `['run', '--format', 'json']`, parseResponse() throwing 'SUBPROCESS_ERROR' AIServiceError until JSONL parsing implemented.\n\n## Architecture\n\n### AIBackend Interface Contract\n\nAll backends implement four methods:\n- `isAvailable(): Promise<boolean>` — CLI executable detection via PATH scanning\n- `buildArgs(options: AICallOptions): string[]` — Argument array construction for subprocess spawn\n- `parseResponse(stdout: string, durationMs: number, exitCode: number): AIResponse` — JSON parsing with usage extraction\n- `getInstallInstructions(): string` — NPM/curl install commands for missing CLI\n\n### PATH Detection Algorithm\n\n`isCommandOnPath(command: string)` shared utility (exported from claude.ts):\n1. Parse `process.env.PATH` with `path.delimiter` (`;` Windows, `:` Unix)\n2. Parse `process.env.PATHEXT` on Windows for executable extensions (`.exe`, `.cmd`, `.bat`)\n3. Nested loop: directory × extension, construct candidate via `path.join(dir, command + ext)`\n4. Check `(await fs.stat(candidate)).isFile()` — return `true` on first match\n5. Uses `fs.stat()` not `fs.access(X_OK)` for Windows compatibility (no execute permission bits)\n\n### Response Parsing Strategy\n\n**ClaudeBackend (functional):**\n- Defensive JSON extraction: `stdout.substring(stdout.indexOf('{'))` skips upgrade notices\n- Zod validation against ClaudeResponseSchema from Claude CLI v2.1.31 output format\n- Extracts model name from first `modelUsage` object key (defaults to `'unknown'`)\n- Returns AIResponse with normalized `{ result, usage, modelName, durationMs }` structure\n- Throws AIServiceError with `'PARSE_ERROR'` code on validation failure\n\n**GeminiBackend/OpenCodeBackend (stubs):**\n- Unconditionally throw AIServiceError with `'SUBPROCESS_ERROR'` code in parseResponse()\n- Block backend usage until JSON format stabilizes (Gemini) or JSONL parsing implemented (OpenCode)\n- Demonstrate extension pattern for future implementations\n\n## Behavioral Contracts\n\n### ClaudeBackend CLI Arguments\n\nFixed arguments returned by buildArgs():\n```\n['-p', '--output-format', 'json', '--no-session-persistence', '--permission-mode', 'bypassPermissions']\n```\n\nConditional arguments from AICallOptions:\n- `'--model', options.model` when model specified\n- `'--system-prompt', options.systemPrompt` when systemPrompt provided\n- `'--max-turns', String(options.maxTurns)` when maxTurns defined\n\nPrompt delivered via stdin by `runSubprocess()` wrapper in `src/ai/subprocess.ts`.\n\n### ClaudeResponseSchema Structure\n\nZod schema validated against Claude CLI v2.1.31:\n```typescript\n{\n  type: 'result',\n  subtype: 'success' | 'error',\n  is_error: boolean,\n  duration_ms: number,\n  duration_api_ms: number,\n  num_turns: number,\n  result: string,\n  session_id: string,\n  total_cost_usd: number,\n  usage: {\n    input_tokens: number,\n    cache_creation_input_tokens: number,\n    cache_read_input_tokens: number,\n    output_tokens: number\n  },\n  modelUsage: Record<string, {\n    inputTokens: number,\n    outputTokens: number,\n    cacheReadInputTokens: number,\n    cacheCreationInputTokens: number,\n    costUSD: number\n  }>\n}\n```\n\n### Installation Instructions\n\n**ClaudeBackend:** `npm install -g @anthropic-ai/claude-code`  \n**GeminiBackend:** `npm install -g @anthropic-ai/gemini-cli` + `https://github.com/google-gemini/gemini-cli`  \n**OpenCodeBackend:** `curl -fsSL https://opencode.ai/install | bash` + `https://opencode.ai`\n\n## Integration Points\n\nBackends registered in `src/ai/registry.ts` AIBackendRegistry map with keys `'claude'`, `'gemini'`, `'opencode'`. Registry consumed by AIService in `src/ai/service.ts` for backend selection via `ai.backend` config field. Auto-detection iterates backends calling `isAvailable()` until first match.\n\n`buildArgs()` output consumed by `runSubprocess()` in `src/ai/subprocess.ts` for child_process.execFile() argument array. `parseResponse()` receives stdout/durationMs/exitCode from subprocess wrapper for normalization into AIResponse structure.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 1389,
      "cacheReadTokens": 16334,
      "cacheCreationTokens": 12675,
      "latencyMs": 38583,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": []
    },
    {
      "timestamp": "2026-02-09T17:46:57.292Z",
      "prompt": "Generate AGENTS.md for directory: \"src/discovery/filters\" (filters)\n\n## File Summaries (5 files)\n\n### binary.ts\n**Purpose:** binary.ts implements two-phase binary file detection for file discovery: extension-based fast path against `BINARY_EX...\n\n**binary.ts implements two-phase binary file detection for file discovery: extension-based fast path against `BINARY_EXTENSIONS` set followed by content analysis via `isBinaryFile()` for unknown extensions, with configurable file size thresholds.**\n\n## Exported Constants\n\n`BINARY_EXTENSIONS` is a `Set<string>` containing 80+ file extensions known to be binary, organized by category:\n- Images: `.png`, `.jpg`, `.jpeg`, `.gif`, `.bmp`, `.ico`, `.webp`, `.svg`, `.tiff`, `.tif`, `.psd`, `.raw`, `.heif`, `.heic`\n- Archives: `.zip`, `.tar`, `.gz`, `.rar`, `.7z`, `.bz2`, `.xz`, `.tgz`\n- Executables: `.exe`, `.dll`, `.so`, `.dylib`, `.bin`, `.msi`, `.app`, `.dmg`\n- Media: `.mp3`, `.mp4`, `.wav`, `.avi`, `.mov`, `.mkv`, `.flac`, `.ogg`, `.webm`, `.m4a`, `.aac`, `.wma`, `.wmv`, `.flv`\n- Documents: `.pdf`, `.doc`, `.docx`, `.xls`, `.xlsx`, `.ppt`, `.pptx`, `.odt`, `.ods`, `.odp`\n- Fonts: `.woff`, `.woff2`, `.ttf`, `.eot`, `.otf`\n- Compiled/bytecode: `.class`, `.pyc`, `.pyo`, `.o`, `.obj`, `.a`, `.lib`, `.wasm`\n- Database: `.db`, `.sqlite`, `.sqlite3`, `.mdb`\n- Other: `.ico`, `.icns`, `.cur`, `.deb`, `.rpm`, `.jar`, `.war`, `.ear`\n\n`DEFAULT_MAX_FILE_SIZE` constant is `1048576` (1MB in bytes).\n\n## Exported Interfaces\n\n`BinaryFilterOptions` configures `createBinaryFilter()` with optional fields:\n- `maxFileSize?: number` — Maximum file size in bytes (default: `DEFAULT_MAX_FILE_SIZE` = 1MB)\n- `additionalExtensions?: string[]` — Extra binary extensions beyond `BINARY_EXTENSIONS` defaults\n\n## Exported Functions\n\n`createBinaryFilter(options?: BinaryFilterOptions): FileFilter` returns a `FileFilter` implementation with `name: 'binary'` and `shouldExclude()` method. Detection algorithm:\n\n1. **Fast path:** Extract extension via `path.extname(absolutePath).toLowerCase()`, return `true` if present in merged `binaryExtensions` set (combines `BINARY_EXTENSIONS` + `additionalExtensions` with leading-dot normalization)\n2. **Size check:** Call `fs.stat(absolutePath)` and return `true` if `stats.size > maxFileSize`\n3. **Slow path:** For unknown extensions below size threshold, delegate to `isBinaryFile(absolutePath)` for content analysis\n4. **Error handling:** Return `true` for any `fs.stat()` or `isBinaryFile()` errors (skip unreadable files)\n\nThe `additionalExtensions` array undergoes normalization: non-dot-prefixed strings get `.` prepended via ternary `ext.startsWith('.') ? ext : \\`.${ext}\\``.\n\n## Dependencies\n\n- `isbinaryfile` — Content-based binary detection for unknown extensions\n- `node:fs/promises` — Async `fs.stat()` for file size retrieval\n- `node:path` — `path.extname()` for extension extraction\n- `../types.js` — `FileFilter` interface defining `name: string` and `shouldExclude(absolutePath: string): Promise<boolean>`\n\n## Integration Points\n\n`createBinaryFilter()` composes with other `FileFilter` implementations from `src/discovery/filters/` (gitignore, custom, vendor) via filter chains in discovery walker. The `maxFileSize` parameter enforces hard exclusion threshold distinct from config's `options.maxFileSize` used elsewhere for binary detection triggering.\n### custom.ts\n**Purpose:** createCustomFilter() constructs a FileFilter that excludes files matching user-provided gitignore-style patterns via ...\n\n**createCustomFilter() constructs a FileFilter that excludes files matching user-provided gitignore-style patterns via the `ignore` library with relative path normalization.**\n\n## Exported Interface\n\n`createCustomFilter(patterns: string[], root: string): FileFilter` — Returns a FileFilter named `'custom'` that checks absolute paths against gitignore-style exclusion patterns. Empty pattern array results in pass-through filter (excludes nothing).\n\n## Pattern Matching Strategy\n\nUses `ignore` library instance (`Ignore` type) initialized via `ignore()` factory. Patterns added via `ig.add(patterns)` bulk operation. Matching performed via `ig.ignores(relativePath)` against paths converted from absolute to relative using `path.relative(normalizedRoot, absolutePath)`.\n\n## Path Normalization\n\n`normalizedRoot` computed via `path.resolve(root)` to canonicalize user-provided root. `shouldExclude()` converts absolute paths to relative before matching. Paths outside root (starting with `..`) or empty relative paths return `false` (not excluded) as guard against incorrect pattern application.\n\n## Integration Point\n\nReturns `FileFilter` interface from `../types.js` with `name: 'custom'` discriminator and `shouldExclude(absolutePath: string): boolean` predicate. Composable with other filters in `src/discovery/filters/index.ts` filter chain.\n\n## Short-Circuit Behavior\n\n`shouldExclude()` returns `false` early when `patterns.length === 0` (no patterns configured). Second early return when relative path is empty or starts with `'..'` prevents matching against files outside project root.\n### gitignore.ts\n**Purpose:** createGitignoreFilter() returns a FileFilter that excludes files matching .gitignore patterns using the ignore librar...\n\n**createGitignoreFilter() returns a FileFilter that excludes files matching .gitignore patterns using the ignore library with relative path normalization.**\n\n## Exported Function\n\n**createGitignoreFilter(root: string): Promise<FileFilter>**\n- Resolves root directory via path.resolve()\n- Attempts to read .gitignore from `path.join(normalizedRoot, '.gitignore')` using fs.readFile() with utf-8 encoding\n- Creates Ignore instance via ignore() library\n- Adds .gitignore content to Ignore instance via ig.add(content)\n- Silently continues on read failure (missing .gitignore passes all paths)\n- Returns FileFilter object with name property `'gitignore'` and shouldExclude() method\n\n## FileFilter Implementation\n\n**shouldExclude(absolutePath: string): boolean**\n- Converts absolutePath to relativePath via path.relative(normalizedRoot, absolutePath)\n- Returns false if relativePath is empty or starts with `'..'` (paths outside root)\n- Delegates to ig.ignores(relativePath) for pattern matching\n- Passes relative paths without trailing slash (file-only mode, not directory mode)\n\n## Dependencies\n\n- `ignore` library (Ignore type) for .gitignore pattern parsing and matching\n- `node:fs/promises` for async file reading (fs.readFile)\n- `node:path` for path normalization (path.resolve, path.join, path.relative)\n- `../types.js` for FileFilter interface\n\n## Critical Behavior\n\nThe ignore library treats paths differently based on trailing slashes. This implementation passes relativePath without appending slash because walker returns files only, not directories. Directory exclusion would require `relativePath + '/'` pattern.\n### index.ts\n**Purpose:** src/discovery/filters/index.ts orchestrates filter chain execution for file discovery, re-exports filter creators, an...\n\n**src/discovery/filters/index.ts orchestrates filter chain execution for file discovery, re-exports filter creators, and implements bounded-concurrency file filtering with per-filter exclusion tracking and trace emission.**\n\n## Exported Functions\n\n**applyFilters(files: string[], filters: FileFilter[], options?: { tracer?: ITraceWriter; debug?: boolean }): Promise<FilterResult>** runs files through filter chain with short-circuit evaluation (first exclusion stops further checks), processes files with concurrency limit of 30 to prevent file descriptor exhaustion during binary detection I/O, returns `FilterResult` containing `included: string[]` and `excluded: ExcludedFile[]` arrays sorted by original file order.\n\n## Re-Exported Filter Creators\n\nExports `createGitignoreFilter` from `./gitignore.js`, `createVendorFilter` and `DEFAULT_VENDOR_DIRS` from `./vendor.js`, `createBinaryFilter`, `BINARY_EXTENSIONS`, and `BinaryFilterOptions` type from `./binary.js`, `createCustomFilter` from `./custom.js`.\n\n## Filter Chain Execution Strategy\n\n**applyFilters** spawns `CONCURRENCY` (30) worker promises sharing single `files.entries()` iterator to distribute work. Each worker calls `filter.shouldExclude(file)` for each filter in sequence until first exclusion or all filters pass. Collects results as `Array<{ index: number; file: string; excluded?: ExcludedFile }>`, sorts by original `index` to preserve input order, then segregates into `included` and `excluded` arrays.\n\n## Per-Filter Statistics Tracking\n\nMaintains `filterStats: Map<string, { matched: number; rejected: number }>` initialized for each filter before processing. Increments `rejected` counter when filter excludes file. Increments `matched` counter for all filters when file passes through entire chain. Emits `filter:applied` trace event for each filter with `filterName`, `filesMatched`, `filesRejected` fields. Writes debug output via `console.error(pc.dim(...))` for filters with `rejected > 0` when `options.debug` enabled.\n\n## Bounded Concurrency Pattern\n\nSets `effectiveConcurrency = Math.min(CONCURRENCY, files.length)` to avoid spawning excess workers. Uses iterator-based work stealing pattern where workers pull `[index, file]` pairs from shared iterator via `for (const [index, file] of iter)` loop. Worker function signature: `async function worker(iter: IterableIterator<[number, string]>): Promise<Array<{ index: number; file: string; excluded?: ExcludedFile }>>`.\n\n## Integration with Trace System\n\nAccepts optional `options.tracer: ITraceWriter` parameter threaded from CLI via `CommandRunOptions`. Emits `filter:applied` events after all workers complete, providing per-filter exclusion metrics for telemetry analysis. Debug flag `options.debug` enables console logging of rejection counts using `picocolors` dim formatting.\n### vendor.ts\n**Purpose:** createVendorFilter() constructs a FileFilter that excludes files within third-party dependency directories using both...\n\n**createVendorFilter() constructs a FileFilter that excludes files within third-party dependency directories using both single-segment matching and path pattern matching.**\n\n## Exports\n\n- **`DEFAULT_VENDOR_DIRS`**: `readonly string[]` — Constant array of 10 common vendor directory names: `'node_modules'`, `'vendor'`, `'.git'`, `'dist'`, `'build'`, `'__pycache__'`, `'.next'`, `'venv'`, `'.venv'`, `'target'`.\n\n- **`createVendorFilter(vendorDirs: string[]): FileFilter`** — Factory function that returns a `FileFilter` object with `name: 'vendor'` and `shouldExclude()` method.\n\n## Matching Strategy\n\ncreateVendorFilter() implements two-tier pattern matching:\n\n1. **Single-segment matching**: Directory names without path separators (e.g., `'node_modules'`) are stored in a `Set<string>` and matched against any segment in the absolute path via `absolutePath.split(path.sep)`.\n\n2. **Path pattern matching**: Patterns containing path separators (e.g., `'apps/vendor'`, `'.agents/skills'`) are normalized to the OS-specific separator via `dir.replace(/[\\\\/]/g, path.sep)` and matched via `absolutePath.includes(pattern)`.\n\n## FileFilter Interface\n\nThe returned object conforms to `FileFilter` from `../types.js`:\n\n```typescript\n{\n  name: string;\n  shouldExclude(absolutePath: string): boolean;\n}\n```\n\nshouldExclude() returns `true` if the absolute path contains any configured vendor directory as a path segment or substring pattern, `false` otherwise.\n\n## Integration Points\n\n- Consumed by filter chain in `src/discovery/filters/index.ts` as part of composable file exclusion logic.\n- DEFAULT_VENDOR_DIRS referenced in `src/config/defaults.ts` for default configuration schema.\n- Used during file discovery phase (`src/discovery/walker.ts`) to skip third-party code directories before binary detection and gitignore filtering.\n\n## Import Map (verified — use these exact paths)\n\nbinary.ts:\n  ../types.js → FileFilter (type)\n\ncustom.ts:\n  ../types.js → FileFilter (type)\n\ngitignore.ts:\n  ../types.js → FileFilter (type)\n\nindex.ts:\n  ../types.js → FileFilter, FilterResult, ExcludedFile (type)\n  ../../orchestration/trace.js → ITraceWriter (type)\n\nvendor.ts:\n  ../types.js → FileFilter (type)\n\n## Project Directory Structure\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>",
      "systemPrompt": "You are generating an AGENTS.md file — a directory-level overview for AI coding assistants.\n\nCRITICAL: Output ONLY the raw markdown content. No code fences, no preamble, no explanations, no conversational text. Your entire response IS the AGENTS.md file content.\n\nOUTPUT FORMAT:\n- First line MUST be exactly: <!-- Generated by agents-reverse-engineer -->\n- Use a # heading with the directory name\n- Write a one-paragraph purpose statement for the directory\nusage\nADAPTIVE SECTIONS:\nAnalyze the directory contents and choose the most relevant sections. Do NOT use a fixed template. Instead, select sections that best document this specific directory for an AI that needs to reconstruct or extend the project.\n\nConsider these section types (choose what applies):\n- **Contents**: Group files by purpose/category under ## headings. For each file: markdown link [filename](./filename) and a one-line description.\n- **Subdirectories**: If subdirectories exist, list them with links [dirname/](./dirname/) and brief summaries.\n- **Architecture / Data Flow**: If files form a pipeline, request/response chain, or layered architecture, document it.\n- **Stack**: If this is a package root (has package.json, Cargo.toml, go.mod, etc.), document the technology stack, key scripts, and entry points.\n- **Structure**: If the directory layout follows a convention (feature-sliced, domain-driven, MVC, etc.), document it.\n- **Patterns**: If files share recurring design patterns (factory, strategy, middleware, barrel re-export), name and document them.\n- **Configuration**: If the directory contains config files, schemas, or environment definitions, document the config surface area.\n- **API Surface**: If the directory exports a public API (barrel index, route definitions, SDK), document the interface contract.\n- **File Relationships**: How files collaborate, depend on each other, or share state.\n- **Behavioral Contracts**: If files contain regex patterns, format specifications, magic constants, or template strings that define observable behavior, collect them in a dedicated section. Preserve verbatim patterns from file summaries — do NOT paraphrase regex into prose. This section is MANDATORY when file summaries contain behavioral artifacts.\n- **Reproduction-Critical Constants**: If file summaries reference annex files (via ## Annex References sections), list them with links. Example: \"Full prompt template text: [templates.ts.annex.md](./prompts/templates.ts.annex.md)\". Do NOT reproduce annex content in AGENTS.md — just link to it.\n\nChoose any relevant sections or create your own based on the directory contents. The goal is to provide a comprehensive overview that captures the essence of the directory's role in the project and how its files work together, with a focus on what an AI coding assistant would need to know to effectively interact with this code.\n\nSCOPE:\n- AGENTS.md is a NAVIGATIONAL INDEX — help an AI find the right file quickly\n- Focus on: what each file does, how files relate, directory-level patterns\n- Do NOT reproduce full architecture sections — those belong in the root CLAUDE.md\n\nPATH ACCURACY (MANDATORY):\n- When referencing files or modules outside this directory, use ONLY paths from the \"Import Map\" section\n- Do NOT invent, rename, or guess module paths — if a path isn't in the Import Map, don't reference it\n- Use the exact directory names from \"Project Directory Structure\" — do NOT rename directories\n  (e.g., if the directory is called \"cli\", write \"src/cli/\", NOT \"src/commands/\")\n- Cross-module references must use the specifier format from actual import statements\n  (e.g., \"../generation/writers/sum.js\", NOT \"../fs/sum-file.js\")\n- If you are unsure about a path, omit the cross-reference rather than guessing\n\nCONSISTENCY (MANDATORY):\n- Do not contradict yourself within the same document\n- If you describe a technique (e.g., \"regex-based\"), do not call it something else later (e.g., \"AST-based\")\n- When stating version numbers, engines, or config fields, use ONLY values present in the file summaries\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this directory\", \"this module\", \"provides\", \"responsible for\", \"is used to\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Per-file descriptions in Contents sections: 1-2 sentences maximum. Reference key symbols but do not reproduce full summaries.\n- Behavioral contracts (regex patterns, format specs, constants) belong in a separate Behavioral Contracts section, not in per-file descriptions.\n- Annex file references: link to .annex.md files, do not inline their content. One line per annex reference.\n- Subdirectory descriptions: 1-2 sentences maximum. Capture the directory's role, not its full contents.\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- Key exported symbols from file summaries MUST appear in the directory overview\n- Preserve exact casing of identifiers\n\nUSER NOTES:\n- If \"User Notes\" are provided in the prompt, they contain user-defined instructions that will be automatically prepended to your output\n- Do NOT repeat or paraphrase user notes in your generated content — they are included separately\n- You may reference information from user notes for context",
      "response": "<!-- Generated by agents-reverse-engineer -->\n\n# src/discovery/filters\n\n**Five filter implementations for file discovery exclusion: gitignore pattern matching, vendor directory detection, binary file detection via extension/content analysis, custom glob patterns, and bounded-concurrency filter chain orchestration with per-filter statistics tracking.**\n\n## Contents\n\n### Filter Implementations\n\n**[binary.ts](./binary.ts)** — `createBinaryFilter()` implements two-phase binary detection: extension-based fast path against `BINARY_EXTENSIONS` set (80+ extensions across images/archives/executables/media/documents/fonts/compiled/database), size threshold check against `maxFileSize` (default 1MB), content analysis fallback via `isBinaryFile()` for unknown extensions.\n\n**[custom.ts](./custom.ts)** — `createCustomFilter()` matches absolute paths against user-provided gitignore-style patterns via `ignore` library with relative path normalization, returns pass-through filter when pattern array empty.\n\n**[gitignore.ts](./gitignore.ts)** — `createGitignoreFilter()` async factory reads `.gitignore` from project root, delegates exclusion to `ignore` library instance with relative path conversion, silently passes all files when `.gitignore` missing.\n\n**[vendor.ts](./vendor.ts)** — `createVendorFilter()` excludes third-party dependency directories via two-tier matching: single-segment patterns (`'node_modules'`) matched against all path segments, multi-segment patterns (`'apps/vendor'`) matched via substring inclusion with OS-specific separator normalization.\n\n**[index.ts](./index.ts)** — `applyFilters()` orchestrates bounded-concurrency filter chain execution (30 workers sharing iterator), short-circuits on first exclusion, tracks per-filter `matched`/`rejected` statistics, emits `filter:applied` trace events, re-exports all filter creators and constants.\n\n## Filter Chain Architecture\n\n**Execution Model**: Worker pool pattern with `CONCURRENCY=30` limit prevents file descriptor exhaustion during I/O-heavy binary detection. Shared `files.entries()` iterator across workers via `for (const [index, file] of iter)` loop. Sequential filter evaluation per file with early termination on first match.\n\n**Statistics Tracking**: `Map<string, { matched: number; rejected: number }>` initialized for each filter before processing. `rejected` increments when filter excludes file. `matched` increments for all filters when file passes entire chain.\n\n**Result Preservation**: Workers collect `Array<{ index: number; file: string; excluded?: ExcludedFile }>`, sort by original `index` to maintain input order, segregate into `included`/`excluded` arrays for `FilterResult` return value.\n\n## Behavioral Contracts\n\n### Binary Extensions Set (binary.ts)\n\n`BINARY_EXTENSIONS` contains 80+ extensions organized by category:\n- **Images**: `.png`, `.jpg`, `.jpeg`, `.gif`, `.bmp`, `.ico`, `.webp`, `.svg`, `.tiff`, `.tif`, `.psd`, `.raw`, `.heif`, `.heic`\n- **Archives**: `.zip`, `.tar`, `.gz`, `.rar`, `.7z`, `.bz2`, `.xz`, `.tgz`\n- **Executables**: `.exe`, `.dll`, `.so`, `.dylib`, `.bin`, `.msi`, `.app`, `.dmg`\n- **Media**: `.mp3`, `.mp4`, `.wav`, `.avi`, `.mov`, `.mkv`, `.flac`, `.ogg`, `.webm`, `.m4a`, `.aac`, `.wma`, `.wmv`, `.flv`\n- **Documents**: `.pdf`, `.doc`, `.docx`, `.xls`, `.xlsx`, `.ppt`, `.pptx`, `.odt`, `.ods`, `.odp`\n- **Fonts**: `.woff`, `.woff2`, `.ttf`, `.eot`, `.otf`\n- **Compiled/bytecode**: `.class`, `.pyc`, `.pyo`, `.o`, `.obj`, `.a`, `.lib`, `.wasm`\n- **Database**: `.db`, `.sqlite`, `.sqlite3`, `.mdb`\n\n### Default Vendor Directories (vendor.ts)\n\n`DEFAULT_VENDOR_DIRS`: `['node_modules', 'vendor', '.git', 'dist', 'build', '__pycache__', '.next', 'venv', '.venv', 'target']`\n\n### Concurrency Limit (index.ts)\n\n`CONCURRENCY = 30` workers — hard limit to prevent file descriptor exhaustion during binary file detection I/O operations.\n\n### Binary Size Threshold (binary.ts)\n\n`DEFAULT_MAX_FILE_SIZE = 1048576` (1MB) — files exceeding threshold excluded without content analysis.\n\n## Path Normalization Patterns\n\n**gitignore.ts**: `path.relative(normalizedRoot, absolutePath)` → guard against `'..'` prefix or empty string before `ig.ignores()` call.\n\n**custom.ts**: `path.relative(normalizedRoot, absolutePath)` → guard against `'..'` prefix or empty string before `ig.ignores()` call.\n\n**vendor.ts**: Multi-segment patterns normalized via `dir.replace(/[\\\\/]/g, path.sep)` for cross-platform separator handling before `absolutePath.includes(pattern)` check.\n\n**binary.ts**: Extension extraction via `path.extname(absolutePath).toLowerCase()`, additional extensions normalized via `ext.startsWith('.') ? ext : \\`.\\${ext}\\`` before merging with `BINARY_EXTENSIONS`.\n\n## Filter Interface Contract\n\nAll filter creators return `FileFilter` object:\n```typescript\n{\n  name: string;              // Discriminator: 'gitignore' | 'vendor' | 'binary' | 'custom'\n  shouldExclude(absolutePath: string): boolean | Promise<boolean>\n}\n```\n\n**Synchronous filters**: custom.ts, vendor.ts, binary.ts (async internally but return sync `shouldExclude`).\n\n**Async factory**: gitignore.ts returns `Promise<FileFilter>` due to `.gitignore` file read.\n\n## Integration Points\n\n**Consumed by**: `src/discovery/walker.ts` composes filters into chain via `createGitignoreFilter()`, `createVendorFilter()`, `createBinaryFilter()`, `createCustomFilter()`, then calls `applyFilters()`.\n\n**Configuration surface**: `BinaryFilterOptions` with `maxFileSize`/`additionalExtensions` threaded from `src/config/schema.ts` YAML config (`exclude.binaryExtensions`, `options.maxFileSize`).\n\n**Telemetry integration**: Accepts optional `ITraceWriter` via `applyFilters()` options, emits `filter:applied` events with per-filter metrics (`filterName`, `filesMatched`, `filesRejected`).\n\n**Debug output**: `options.debug` flag enables `console.error(pc.dim(...))` logging for filters with `rejected > 0` count.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 1657,
      "cacheReadTokens": 16334,
      "cacheCreationTokens": 13481,
      "latencyMs": 44479,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": []
    },
    {
      "timestamp": "2026-02-09T17:46:57.269Z",
      "prompt": "Generate AGENTS.md for directory: \"src/generation/writers\" (writers)\n\n## File Summaries (3 files)\n\n### agents-md.ts\n**Purpose:** Manages AGENTS.md file lifecycle: writes LLM-generated directory documentation, preserves user-authored content by re...\n\n**Manages AGENTS.md file lifecycle: writes LLM-generated directory documentation, preserves user-authored content by renaming to AGENTS.local.md, and prepends preserved content above generated sections with marker-based detection.**\n\n## Exported Constants\n\n`GENERATED_MARKER` — String constant `'<!-- Generated by agents-reverse-engineer -->'` embedded in all generated AGENTS.md files to distinguish them from user-authored versions.\n\n## Exported Functions\n\n### isGeneratedAgentsMd\n\n```typescript\nasync function isGeneratedAgentsMd(filePath: string): Promise<boolean>\n```\n\nReads file at `filePath`, returns `true` if content includes `GENERATED_MARKER` substring. Returns `false` on read failure (file not found, permission denied).\n\n### writeAgentsMd\n\n```typescript\nasync function writeAgentsMd(\n  dirPath: string,\n  _projectRoot: string,\n  content: string,\n): Promise<string>\n```\n\nWrites AGENTS.md to `dirPath` by composing marker + preserved user content + LLM-generated `content` parameter. Returns path to written AGENTS.md file (`path.join(dirPath, 'AGENTS.md')`).\n\n## User Content Preservation Strategy\n\nExecutes three-step preservation workflow:\n\n1. **Check existing AGENTS.md**: If file exists and lacks `GENERATED_MARKER`, renames to `AGENTS.local.md` via `rename()` and captures content as `userContent`\n2. **Fallback to AGENTS.local.md**: If no user content captured in step 1, attempts `readFile(localPath)` to load previously preserved content\n3. **Strip marker from LLM content**: Removes `GENERATED_MARKER` prefix via `slice()` if present, trims leading newlines with `/^\\n+/` regex\n\n## Final Content Assembly\n\nConstructs `finalContent` string from ordered parts array:\n\n```\n<!-- Generated by agents-reverse-engineer -->\n\n<!-- User-defined AGENTS.md preserved as AGENTS.local.md -->\n\n[userContent.trim()]\n\n---\n\n[llmContent]\n```\n\nUser content section (comment + content + separator) omitted if `userContent?.trim()` is falsy. Calls `mkdir(path.dirname(agentsPath), { recursive: true })` before `writeFile()` to ensure parent directory exists.\n\n## Integration Points\n\nInvoked by Phase 2 directory aggregation orchestrator (`src/generation/orchestrator.ts`) after LLM subprocess generates AGENTS.md content. Works with `src/generation/collector.ts` which reads AGENTS.local.md files when collecting subdirectory documentation for parent directory prompts.\n\n## Behavioral Contracts\n\n- **Marker detection pattern**: Substring search `content.includes(GENERATED_MARKER)` without regex or parsing\n- **Marker removal pattern**: `llmContent.startsWith(GENERATED_MARKER)` check followed by `slice(GENERATED_MARKER.length).replace(/^\\n+/, '')`\n- **File paths**: AGENTS.md (`path.join(dirPath, 'AGENTS.md')`), AGENTS.local.md (`path.join(dirPath, 'AGENTS.local.md')`)\n- **User content separator**: Triple-dash Markdown horizontal rule `'---'` surrounded by blank lines\n### index.ts\n**Purpose:** Barrel module re-exporting file and directory documentation writers for `.sum` files and `AGENTS.md` generation.\n\n**Barrel module re-exporting file and directory documentation writers for `.sum` files and `AGENTS.md` generation.**\n\n## Exported Symbols\n\n### From `./sum.js`\n\n- **`writeSumFile`** — writes `.sum` file with YAML frontmatter containing `generated_at`, `content_hash`, `purpose`, `critical_todos`, `related_files`, followed by markdown summary content\n- **`readSumFile`** — parses `.sum` file YAML frontmatter and markdown body, returns `SumFileContent` structure with metadata and summary text\n- **`getSumPath`** — computes `.sum` file path for given source file by appending `.sum` extension\n- **`sumFileExists`** — checks filesystem for existence of `.sum` file at computed path via `getSumPath()`\n- **`SumFileContent`** (type) — interface defining structure returned by `readSumFile()` with fields: `generatedAt: string`, `contentHash: string`, `purpose: string`, `criticalTodos?: string[]`, `relatedFiles?: string[]`, `summary: string`\n\n### From `./agents-md.js`\n\n- **`writeAgentsMd`** — writes directory-level `AGENTS.md` aggregation file during Phase 2 post-order traversal, preserves user-authored `AGENTS.local.md` content if present, inserts `<!-- Generated by agents-reverse-engineer -->` marker\n\n## Integration Role\n\nActs as single import point for Phase 1 (file analysis → `.sum` via `writeSumFile`) and Phase 2 (directory aggregation → `AGENTS.md` via `writeAgentsMd`) output generation. Consumed by `src/generation/orchestrator.ts` for three-phase pipeline execution and `src/update/orchestrator.ts` for incremental update workflow.\n### sum.ts\n**Purpose:** sum.ts manages .sum file I/O with YAML frontmatter serialization, parsing, and annex file generation for reproduction...\n\n**sum.ts manages .sum file I/O with YAML frontmatter serialization, parsing, and annex file generation for reproduction-critical source content.**\n\n## Exported Interfaces and Types\n\n`SumFileContent` interface defines `.sum` file structure with `summary: string`, `metadata: SummaryMetadata`, `generatedAt: string`, `contentHash: string` (SHA-256 hex digest for change detection).\n\n## Core I/O Functions\n\n`writeSumFile(sourcePath: string, content: SumFileContent): Promise<string>` appends `.sum` extension to `sourcePath`, creates parent directory via `mkdir(..., { recursive: true })`, formats content via `formatSumFile()`, writes with `writeFile()`, returns absolute `.sum` path.\n\n`readSumFile(sumPath: string): Promise<SumFileContent | null>` reads file via `readFile()`, delegates to `parseSumFile()`, returns `null` on read failure or parse error.\n\n`getSumPath(sourcePath: string): string` returns `${sourcePath}.sum` without filesystem access.\n\n`sumFileExists(sourcePath: string): Promise<boolean>` calls `readSumFile(getSumPath(sourcePath))` and checks for non-null result.\n\n## Frontmatter Parsing\n\n`parseSumFile(content: string): SumFileContent | null` extracts YAML frontmatter via regex `/^---\\n([\\s\\S]*?)\\n---\\n/`, parses `generated_at`, `content_hash`, `purpose` fields via individual regex patterns (`/generated_at:\\s*(.+)/`, `/content_hash:\\s*(.+)/`, `/purpose:\\s*(.+)/`), delegates array parsing to `parseYamlArray()` for `critical_todos` and `related_files`, returns `null` on parse failure.\n\n`parseYamlArray(frontmatter: string, key: string): string[]` supports inline format `key: [a, b, c]` via regex `new RegExp(\\`${key}:\\\\s*\\\\[([^\\\\]]*)\\\\]\\`)` and multi-line format via `new RegExp(\\`${key}:\\\\s*\\\\n((?:\\\\s+-\\\\s+.+\\\\n?)+)\\`, 'm')`, strips quotes and list markers, returns empty array if neither pattern matches.\n\n## Frontmatter Serialization\n\n`formatSumFile(content: SumFileContent): string` builds YAML frontmatter block with `---` delimiters, writes `generated_at`, `content_hash`, `purpose` as top-level fields, conditionally includes `critical_todos` and `related_files` via `formatYamlArray()` if arrays non-empty, appends summary body after closing `---\\n\\n`.\n\n`formatYamlArray(key: string, values: string[]): string` returns `key: []` for empty arrays, inline format `key: [val1, val2]` for arrays ≤3 elements with all values <40 chars, multi-line YAML list format `key:\\n  - val1\\n  - val2` otherwise.\n\n## Annex File Generation\n\n`writeAnnexFile(sourcePath: string, sourceContent: string): Promise<string>` creates `${sourcePath}.annex.md` with markdown structure: `GENERATED_MARKER` comment (imported from `agents-md.ts`), `# Annex: {basename}` heading, explanatory text referencing `.sum` file, triple-backtick code fence containing full `sourceContent`, writes via `writeFile()`, returns annex path.\n\n`getAnnexPath(sourcePath: string): string` returns `${sourcePath}.annex.md` without filesystem access.\n\n## Integration Points\n\nImports `GENERATED_MARKER` from `./agents-md.js` for consistent generated file annotation. Used by update workflow (`src/update/`) for hash comparison via `readSumFile()`, by file analysis phase (`src/generation/executor.ts`) for `.sum` output via `writeSumFile()`, by orphan cleanup (`src/update/orphan-cleaner.ts`) for stale file deletion.\n\n## YAML Format Contract\n\nFrontmatter delimiter: `---\\n...\\n---\\n` with exactly one newline after closing delimiter before summary body begins.\n\nScalar field pattern: `key: value` with optional trailing whitespace.\n\nArray formats:\n- Inline: `key: [item1, item2]` with comma-separated values\n- Multi-line: `key:\\n  - item1\\n  - item2` with two-space indent and `- ` prefix per line\n\nRequired fields: `generated_at` (ISO 8601 timestamp), `content_hash` (64-char hex string), `purpose` (single-line string).\n\nOptional fields: `critical_todos` (string array), `related_files` (string array).\n\n## Import Map (verified — use these exact paths)\n\nsum.ts:\n  ../types.js → SummaryMetadata (type)\n\n## Project Directory Structure\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>",
      "systemPrompt": "You are generating an AGENTS.md file — a directory-level overview for AI coding assistants.\n\nCRITICAL: Output ONLY the raw markdown content. No code fences, no preamble, no explanations, no conversational text. Your entire response IS the AGENTS.md file content.\n\nOUTPUT FORMAT:\n- First line MUST be exactly: <!-- Generated by agents-reverse-engineer -->\n- Use a # heading with the directory name\n- Write a one-paragraph purpose statement for the directory\nusage\nADAPTIVE SECTIONS:\nAnalyze the directory contents and choose the most relevant sections. Do NOT use a fixed template. Instead, select sections that best document this specific directory for an AI that needs to reconstruct or extend the project.\n\nConsider these section types (choose what applies):\n- **Contents**: Group files by purpose/category under ## headings. For each file: markdown link [filename](./filename) and a one-line description.\n- **Subdirectories**: If subdirectories exist, list them with links [dirname/](./dirname/) and brief summaries.\n- **Architecture / Data Flow**: If files form a pipeline, request/response chain, or layered architecture, document it.\n- **Stack**: If this is a package root (has package.json, Cargo.toml, go.mod, etc.), document the technology stack, key scripts, and entry points.\n- **Structure**: If the directory layout follows a convention (feature-sliced, domain-driven, MVC, etc.), document it.\n- **Patterns**: If files share recurring design patterns (factory, strategy, middleware, barrel re-export), name and document them.\n- **Configuration**: If the directory contains config files, schemas, or environment definitions, document the config surface area.\n- **API Surface**: If the directory exports a public API (barrel index, route definitions, SDK), document the interface contract.\n- **File Relationships**: How files collaborate, depend on each other, or share state.\n- **Behavioral Contracts**: If files contain regex patterns, format specifications, magic constants, or template strings that define observable behavior, collect them in a dedicated section. Preserve verbatim patterns from file summaries — do NOT paraphrase regex into prose. This section is MANDATORY when file summaries contain behavioral artifacts.\n- **Reproduction-Critical Constants**: If file summaries reference annex files (via ## Annex References sections), list them with links. Example: \"Full prompt template text: [templates.ts.annex.md](./prompts/templates.ts.annex.md)\". Do NOT reproduce annex content in AGENTS.md — just link to it.\n\nChoose any relevant sections or create your own based on the directory contents. The goal is to provide a comprehensive overview that captures the essence of the directory's role in the project and how its files work together, with a focus on what an AI coding assistant would need to know to effectively interact with this code.\n\nSCOPE:\n- AGENTS.md is a NAVIGATIONAL INDEX — help an AI find the right file quickly\n- Focus on: what each file does, how files relate, directory-level patterns\n- Do NOT reproduce full architecture sections — those belong in the root CLAUDE.md\n\nPATH ACCURACY (MANDATORY):\n- When referencing files or modules outside this directory, use ONLY paths from the \"Import Map\" section\n- Do NOT invent, rename, or guess module paths — if a path isn't in the Import Map, don't reference it\n- Use the exact directory names from \"Project Directory Structure\" — do NOT rename directories\n  (e.g., if the directory is called \"cli\", write \"src/cli/\", NOT \"src/commands/\")\n- Cross-module references must use the specifier format from actual import statements\n  (e.g., \"../generation/writers/sum.js\", NOT \"../fs/sum-file.js\")\n- If you are unsure about a path, omit the cross-reference rather than guessing\n\nCONSISTENCY (MANDATORY):\n- Do not contradict yourself within the same document\n- If you describe a technique (e.g., \"regex-based\"), do not call it something else later (e.g., \"AST-based\")\n- When stating version numbers, engines, or config fields, use ONLY values present in the file summaries\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this directory\", \"this module\", \"provides\", \"responsible for\", \"is used to\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Per-file descriptions in Contents sections: 1-2 sentences maximum. Reference key symbols but do not reproduce full summaries.\n- Behavioral contracts (regex patterns, format specs, constants) belong in a separate Behavioral Contracts section, not in per-file descriptions.\n- Annex file references: link to .annex.md files, do not inline their content. One line per annex reference.\n- Subdirectory descriptions: 1-2 sentences maximum. Capture the directory's role, not its full contents.\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- Key exported symbols from file summaries MUST appear in the directory overview\n- Preserve exact casing of identifiers\n\nUSER NOTES:\n- If \"User Notes\" are provided in the prompt, they contain user-defined instructions that will be automatically prepended to your output\n- Do NOT repeat or paraphrase user notes in your generated content — they are included separately\n- You may reference information from user notes for context",
      "response": "<!-- Generated by agents-reverse-engineer -->\n\n# generation/writers\n\nImplements `.sum` file and `AGENTS.md` serialization with YAML frontmatter parsing, user content preservation, and annex file generation for reproduction-critical source content.\n\n## Contents\n\n### [sum.ts](./sum.ts)\nYAML frontmatter I/O for `.sum` files with SHA-256 hash tracking, array serialization (inline/multi-line), annex file generation for reproduction-critical content. Exports `writeSumFile`, `readSumFile`, `getSumPath`, `sumFileExists`, `writeAnnexFile`, `getAnnexPath`, `parseSumFile`, `formatSumFile`, `SumFileContent` interface.\n\n### [agents-md.ts](./agents-md.ts)\nAGENTS.md lifecycle management preserving user-authored content via AGENTS.local.md rename, marker-based detection. Exports `writeAgentsMd`, `isGeneratedAgentsMd`, `GENERATED_MARKER` constant.\n\n### [index.ts](./index.ts)\nBarrel re-exporting `writeSumFile`, `readSumFile`, `getSumPath`, `sumFileExists`, `SumFileContent` from `sum.ts` and `writeAgentsMd` from `agents-md.ts`.\n\n## Data Flow\n\n**Phase 1 (File Analysis):**\n`src/generation/executor.ts` → `writeSumFile(sourcePath, content)` → appends `.sum` extension → `formatSumFile()` → `mkdir` + `writeFile` → emits `.sum` file with frontmatter delimiter `---\\n...\\n---\\n` + summary body.\n\n**Phase 2 (Directory Aggregation):**\n`src/generation/orchestrator.ts` → `writeAgentsMd(dirPath, projectRoot, llmContent)` → checks existing AGENTS.md via `isGeneratedAgentsMd()` → renames user file to AGENTS.local.md → prepends preserved content + separator `---` + LLM-generated sections.\n\n**Incremental Updates:**\n`src/update/orchestrator.ts` → `readSumFile(getSumPath(sourcePath))` → `parseSumFile()` extracts `contentHash` → SHA-256 comparison → hash mismatch triggers re-analysis.\n\n**Orphan Cleanup:**\n`src/update/orphan-cleaner.ts` → detects stale `.sum` files for deleted sources → calls `unlink(getSumPath(deletedPath))`.\n\n## YAML Frontmatter Format\n\n**Delimiter:** `---\\n` before and after metadata block, exactly one newline before summary body.\n\n**Scalar fields:**\n```yaml\ngenerated_at: 2026-02-09T12:34:56.789Z\ncontent_hash: a3f5d8e9... (SHA-256 hex, 64 chars)\npurpose: Single-line description\n```\n\n**Array fields (inline format <40 chars, ≤3 items):**\n```yaml\ncritical_todos: [Security issue, Performance bug]\nrelated_files: [path/to/file.ts]\n```\n\n**Array fields (multi-line format):**\n```yaml\ncritical_todos:\n  - Long security issue description exceeding 40 characters\n  - Another long description\nrelated_files:\n  - very/long/path/to/related/file/exceeding/character/limit.ts\n```\n\n## Parsing Strategies\n\n**Frontmatter extraction:** Regex `/^---\\n([\\s\\S]*?)\\n---\\n/` captures metadata block.\n\n**Scalar field extraction:** Individual regex per field (`/generated_at:\\s*(.+)/`, `/content_hash:\\s*(.+)/`, `/purpose:\\s*(.+)/`), captures group 1, trims whitespace.\n\n**Array parsing (inline):** `new RegExp(\\`${key}:\\\\s*\\\\[([^\\\\]]*)\\\\]\\`)` captures comma-separated values within brackets, splits on `,`, strips quotes and whitespace.\n\n**Array parsing (multi-line):** `new RegExp(\\`${key}:\\\\s*\\\\n((?:\\\\s+-\\\\s+.+\\\\n?)+)\\`, 'm')` captures indented list items, splits on newlines, strips `- ` prefix per line.\n\n## User Content Preservation Workflow\n\n1. `writeAgentsMd()` called with `dirPath` and LLM-generated `content`\n2. Constructs `agentsPath` (`dirPath/AGENTS.md`) and `localPath` (`dirPath/AGENTS.local.md`)\n3. If `agentsPath` exists and `isGeneratedAgentsMd(agentsPath)` returns `false`: `rename(agentsPath, localPath)`, capture content as `userContent`\n4. Else if `localPath` exists: `readFile(localPath)`, capture as `userContent`\n5. Strip `GENERATED_MARKER` prefix from LLM `content` if present via `slice()` + `/^\\n+/` trim\n6. Assemble `finalContent`:\n   - `GENERATED_MARKER` comment\n   - If `userContent.trim()` non-empty: preservation comment + `userContent.trim()` + `---` separator\n   - LLM-generated sections\n7. `mkdir(dirPath, { recursive: true })` + `writeFile(agentsPath, finalContent)`\n\n## Annex File Generation\n\n`writeAnnexFile(sourcePath, sourceContent)` creates `${sourcePath}.annex.md`:\n```markdown\n<!-- Generated by agents-reverse-engineer -->\n\n# Annex: <basename>\n\nThis annex file preserves the full source content of `<basename>` for AI coding assistants.\nThe primary summary is in `<basename>.sum`.\n\n```<ext>\n<sourceContent>\n```\n```\n\nUsed for files containing long regex patterns, template strings, or format specifications where verbatim content required for reproduction. `getAnnexPath()` computes path without filesystem access.\n\n## Behavioral Contracts\n\n**Marker detection:** `content.includes('<!-- Generated by agents-reverse-engineer -->')` substring search (no regex).\n\n**Marker removal:** `llmContent.startsWith(GENERATED_MARKER) ? llmContent.slice(GENERATED_MARKER.length).replace(/^\\n+/, '') : llmContent`.\n\n**File extensions:** `.sum` (metadata + summary), `.annex.md` (full source preservation).\n\n**User content separator:** Three-dash horizontal rule surrounded by blank lines (`\\n\\n---\\n\\n`).\n\n**Frontmatter regex:** `/^---\\n([\\s\\S]*?)\\n---\\n/` non-greedy capture of metadata block.\n\n**Array inline regex:** `` new RegExp(`${key}:\\\\s*\\\\[([^\\\\]]*)\\\\]`) `` captures values within square brackets.\n\n**Array multi-line regex:** `` new RegExp(`${key}:\\\\s*\\\\n((?:\\\\s+-\\\\s+.+\\\\n?)+)`, 'm') `` captures indented list items starting with `- `.\n\n## Integration Points\n\n**Imports:**\n- `../types.js` → `SummaryMetadata` (metadata structure for frontmatter)\n- `./agents-md.js` → `GENERATED_MARKER` (annex file header constant)\n\n**Consumed By:**\n- `src/generation/executor.ts` → `writeSumFile()` for Phase 1 output\n- `src/generation/orchestrator.ts` → `writeAgentsMd()` for Phase 2 output\n- `src/update/orchestrator.ts` → `readSumFile()` for hash comparison\n- `src/update/orphan-cleaner.ts` → `getSumPath()` for stale file deletion\n- `src/generation/collector.ts` → reads AGENTS.local.md when aggregating subdirectory docs",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 1883,
      "cacheReadTokens": 16334,
      "cacheCreationTokens": 12728,
      "latencyMs": 47770,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": []
    },
    {
      "timestamp": "2026-02-09T17:46:57.280Z",
      "prompt": "Generate AGENTS.md for directory: \"src/generation/prompts\" (prompts)\n\n## File Summaries (4 files)\n\n### builder.ts\n**Purpose:** builder.ts constructs AI prompts for three-phase documentation generation: buildFilePrompt() assembles file analysis ...\n\n**builder.ts constructs AI prompts for three-phase documentation generation: buildFilePrompt() assembles file analysis prompts with import context and optional incremental update sections, buildDirectoryPrompt() aggregates child .sum files and subdirectory AGENTS.md with import maps for directory synthesis, buildRootPrompt() collects all AGENTS.md files and package.json metadata for project-level CLAUDE.md generation.**\n\n## Exported Functions\n\n### buildFilePrompt\n```typescript\nfunction buildFilePrompt(context: PromptContext, debug = false): {\n  system: string;\n  user: string;\n}\n```\nConstructs file analysis prompt by detecting language via `detectLanguage()`, substituting `{{FILE_PATH}}`, `{{CONTENT}}`, `{{LANG}}`, `{{PROJECT_PLAN_SECTION}}` placeholders in `FILE_USER_PROMPT`, appending `context.contextFiles[]` as related file sections with syntax-highlighted code blocks, and conditionally appending `context.existingSum` with update instructions when incremental mode is active. Returns `FILE_SYSTEM_PROMPT` for fresh analysis or `FILE_UPDATE_SYSTEM_PROMPT` for incremental updates.\n\n### buildDirectoryPrompt\n```typescript\nasync function buildDirectoryPrompt(\n  dirPath: string,\n  projectRoot: string,\n  debug = false,\n  knownDirs?: Set<string>,\n  projectStructure?: string,\n  existingAgentsMd?: string,\n): Promise<{ system: string; user: string }>\n```\nAggregates directory documentation by reading all `.sum` files via `readSumFile()` and `getSumPath()`, collecting child `AGENTS.md` from subdirectories in `knownDirs` set, detecting user-authored content in `AGENTS.local.md` or non-generated `AGENTS.md` (checked via `GENERATED_MARKER` absence), extracting import maps via `extractDirectoryImports()` and `formatImportMap()`, enumerating manifest files from array `['package.json', 'Cargo.toml', 'go.mod', 'pyproject.toml', 'pom.xml', 'build.gradle', 'Gemfile', 'composer.json', 'CMakeLists.txt', 'Makefile']`, scanning for `.annex.md` files, and appending optional `projectStructure` in `<project-structure>` tags. Returns `DIRECTORY_SYSTEM_PROMPT` for fresh generation or `DIRECTORY_UPDATE_SYSTEM_PROMPT` when `existingAgentsMd` is provided. Filters source files by regex `/\\.(ts|tsx|js|jsx|py|go|rs|java|kt)$/` for import extraction.\n\n### buildRootPrompt\n```typescript\nasync function buildRootPrompt(\n  projectRoot: string,\n  debug = false,\n): Promise<{ system: string; user: string }>\n```\nSynthesizes project-level prompt by calling `collectAgentsDocs()` to gather all AGENTS.md files, parsing `package.json` to extract `name`, `version`, `description`, `packageManager`, and `scripts` object entries, embedding AGENTS.md content as `### {relativePath}` sections, and enforcing synthesis-only constraints with explicit instruction: \"Use ONLY the information provided below. Do NOT invent features, hooks, patterns, or APIs.\" Returns `ROOT_SYSTEM_PROMPT` with user prompt listing output requirements (architecture, key directories table, getting started, technologies).\n\n### detectLanguage\n```typescript\nfunction detectLanguage(filePath: string): string\n```\nMaps file extension via `path.extname()` to syntax identifier using `langMap` object covering 21 extensions: `.ts` → `'typescript'`, `.tsx` → `'tsx'`, `.js` → `'javascript'`, `.jsx` → `'jsx'`, `.py` → `'python'`, `.rb` → `'ruby'`, `.go` → `'go'`, `.rs` → `'rust'`, `.java` → `'java'`, `.kt` → `'kotlin'`, `.swift` → `'swift'`, `.cs` → `'csharp'`, `.php` → `'php'`, `.vue` → `'vue'`, `.svelte` → `'svelte'`, `.json` → `'json'`, `.yaml`/`.yml` → `'yaml'`, `.md` → `'markdown'`, `.css` → `'css'`, `.scss` → `'scss'`, `.html` → `'html'`. Fallback returns `'text'`.\n\n## Template Integration\n\nImports six prompt templates from `templates.js`: `FILE_SYSTEM_PROMPT`, `FILE_USER_PROMPT`, `FILE_UPDATE_SYSTEM_PROMPT`, `DIRECTORY_SYSTEM_PROMPT`, `DIRECTORY_UPDATE_SYSTEM_PROMPT`, `ROOT_SYSTEM_PROMPT`. `buildFilePrompt()` uses `FILE_USER_PROMPT` with mustache-style placeholder substitution pattern `/\\{\\{VARIABLE\\}\\}/g`. `buildDirectoryPrompt()` and `buildRootPrompt()` construct user prompts programmatically without template string substitution.\n\n## Incremental Update Strategy\n\nAll three builders accept optional existing content parameters: `context.existingSum` in `buildFilePrompt()`, `existingAgentsMd` in `buildDirectoryPrompt()`. When provided, appends existing content under heading `## Existing [Summary|AGENTS.md] (update this — preserve stable content, modify only what changed)` and switches to update-specific system prompts (`FILE_UPDATE_SYSTEM_PROMPT`, `DIRECTORY_UPDATE_SYSTEM_PROMPT`).\n\n## User Content Preservation\n\n`buildDirectoryPrompt()` checks for `AGENTS.local.md` first, falling back to checking existing `AGENTS.md` for absence of `GENERATED_MARKER` constant (imported from `../writers/agents-md.js`). User content appended as `## User Notes` section with reference link pattern `[AGENTS.local.md](./AGENTS.local.md)`. First-run detection message: \"This user-defined content will be preserved as [AGENTS.local.md](./AGENTS.local.md).\"\n\n## Debug Logging\n\nInternal `logTemplate()` function emits conditional debug output to stderr using `picocolors` formatting: `${pc.dim('[prompt]')} ${pc.cyan(action)} ${pc.dim('→')} ${relativePath} ${pc.dim(extra)}`. Called from all three builders with action names `'buildFilePrompt'`, `'buildDirectoryPrompt'`, `'buildRootPrompt'` and metadata strings like `lang=typescript`, `files=12 subdirs=3 imports=8`, `agents=15`.\n\n## Dependencies\n\n- `node:path`, `node:fs/promises` (readdir, readFile)\n- `picocolors` as `pc` for terminal colors\n- `./types.js` exports `PromptContext` interface\n- `./templates.js` exports six prompt constant strings\n- `../writers/sum.js` exports `readSumFile()`, `getSumPath()`\n- `../writers/agents-md.js` exports `GENERATED_MARKER` string constant\n- `../../imports/index.js` exports `extractDirectoryImports()`, `formatImportMap()`\n- `../collector.js` exports `collectAgentsDocs()`\n### index.ts\n**Purpose:** Barrel module exporting prompt construction API for three-phase documentation generation: file analysis, directory ag...\n\n**Barrel module exporting prompt construction API for three-phase documentation generation: file analysis, directory aggregation, and root synthesis.**\n\n## Exported Types\n\n- `PromptContext` — Context object passed to prompt builders containing file metadata, import maps, child summaries, and configuration\n- `SUMMARY_GUIDELINES` — Constant string containing behavioral contracts for AI-generated summaries (density rules, anchor term preservation, reproduction-critical content handling)\n\n## Exported Functions\n\n- `buildFilePrompt(context: PromptContext): string` — Constructs Phase 1 prompt for `.sum` file generation from source code with import maps and language detection\n- `buildDirectoryPrompt(context: PromptContext): string` — Constructs Phase 2 prompt for `AGENTS.md` synthesis from child `.sum` files, subdirectory docs, and manifest detection\n- `buildRootPrompt(context: PromptContext): string` — Constructs Phase 3 prompt for root integration documents (`CLAUDE.md`, `GEMINI.md`, `OPENCODE.md`) from aggregated `AGENTS.md` corpus\n- `detectLanguage(filePath: string): string` — Maps file extensions to language names for prompt context (e.g., `.ts` → `\"TypeScript\"`, `.py` → `\"Python\"`)\n\n## Module Organization\n\nRe-exports from `./types.js` provide shared interfaces and constants, while `./builder.js` contains concrete prompt construction logic. Separates prompt template strings (in `./templates.js`) from assembly logic.\n\n## Integration Points\n\nUsed by `src/generation/executor.ts` Phase 1/2/3 runners to construct AI service prompts. `PromptContext` populated by orchestrator with discovery results, import extractor output, and child document content via `src/generation/collector.ts`.\n### templates.ts\n**Purpose:** templates.ts exports six system prompt constants controlling AI-driven documentation generation across three pipeline...\n\n**templates.ts exports six system prompt constants controlling AI-driven documentation generation across three pipeline phases: file analysis, directory aggregation, and root synthesis.**\n\n## Exported Constants\n\n### FILE_SYSTEM_PROMPT\nSystem prompt for Phase 1 file analysis controlling `.sum` generation. Enforces density rules (every sentence must reference specific identifiers), anchor term preservation (exact export name casing), and behavioral contract extraction (verbatim regex patterns, format strings, magic constants, environment variables). Mandates output format starting with bold purpose statement without preamble. Defines REPRODUCTION-CRITICAL CONTENT strategy: files with large string constants should list them in `## Annex References` section rather than inlining content (e.g., `` `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)``), delegating extraction to pipeline automation.\n\n### FILE_USER_PROMPT\nUser prompt template for file analysis with two placeholders: `{{FILE_PATH}}` and `{{CONTENT}}`. Includes embedded `<project-structure>` listing all project files (hardcoded tree from root to `src/update/types.ts`). Instructs AI to lead with bold purpose statement and use `##` headings for organization.\n\n### FILE_UPDATE_SYSTEM_PROMPT\nSystem prompt for incremental file summary updates when `existingSum` is provided to `buildFilePrompt()`. Enforces preservation rules: keep unchanged sections verbatim, modify only content affected by code changes, preserve behavioral contracts (regex/constants) unless source changed them. Prohibits reorganizing or rephrasing stable text. Maintains same density rules and anchor term preservation as `FILE_SYSTEM_PROMPT`.\n\n### DIRECTORY_SYSTEM_PROMPT\nSystem prompt for Phase 2 `AGENTS.md` generation controlling directory-level aggregation. Mandates first line exactly `<!-- Generated by agents-reverse-engineer -->` followed by `#` heading with directory name. Defines adaptive section strategy: choose from Contents, Subdirectories, Architecture, Stack, Structure, Patterns, Configuration, API Surface, File Relationships, Behavioral Contracts, Reproduction-Critical Constants based on directory contents. Enforces PATH ACCURACY rules: use only paths from \"Import Map\" section, exact directory names from \"Project Directory Structure\", no path invention. Prohibits contradictions (e.g., calling technique \"regex-based\" then \"AST-based\"). USER NOTES section describes automatic prepending behavior: user-defined instructions are included separately, do not repeat them.\n\n### DIRECTORY_UPDATE_SYSTEM_PROMPT\nSystem prompt for incremental `AGENTS.md` updates when `existingAgentsMd` is provided to `buildDirectoryPrompt()`. Preserves structure/headings/descriptions for unchanged files, modifies only entries whose summaries changed, adds/removes entries for new/deleted files. Maintains same output format (first line `<!-- Generated by agents-reverse-engineer -->`), PATH ACCURACY rules, CONSISTENCY rules, and density rules as `DIRECTORY_SYSTEM_PROMPT`.\n\n### ROOT_SYSTEM_PROMPT\nSystem prompt for Phase 3 root document synthesis (`CLAUDE.md`, `GEMINI.md`, `OPENCODE.md`). Enforces output-only mode: no conversational text, preamble, or meta-commentary. CRITICAL CONSTRAINT: synthesize only from provided `AGENTS.md` content, do not invent features/hooks/APIs/patterns, omit missing sections rather than guessing, every claim must be traceable to specific `AGENTS.md` file.\n\n## Behavioral Contracts\n\nAll prompts prohibit filler phrases: `\"this file\"`, `\"this module\"`, `\"provides\"`, `\"responsible for\"`, `\"is used to\"`, `\"basically\"`, `\"essentially\"`, `\"provides functionality for\"`. Enforce pattern `\"[ExportName] does X\"` not `\"The ExportName function is responsible for doing X\"`.\n\nDensity rule sentinel: \"Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\".\n\nAnchor term preservation rule: \"All exported function/class/type/const names MUST appear in the summary exactly as written in source\".\n\nOutput format enforcement: \"Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\"\n\n## Integration Points\n\nUsed by `buildFilePrompt()` and `buildDirectoryPrompt()` in `src/generation/prompts/builder.ts` to construct AI prompts for Phase 1 (file analysis) and Phase 2 (directory aggregation). `ROOT_SYSTEM_PROMPT` consumed by `buildRootPrompt()` for Phase 3 synthesis. Update variants (`FILE_UPDATE_SYSTEM_PROMPT`, `DIRECTORY_UPDATE_SYSTEM_PROMPT`) selected conditionally when existing content is provided for incremental regeneration workflows.\n\n## Annex References\n\n- `FILE_SYSTEM_PROMPT` — system prompt for file analysis (129 lines)\n- `FILE_USER_PROMPT` — user prompt template with project structure (104 lines)\n- `FILE_UPDATE_SYSTEM_PROMPT` — incremental file update prompt (47 lines)\n- `DIRECTORY_SYSTEM_PROMPT` — directory aggregation prompt (93 lines)\n- `DIRECTORY_UPDATE_SYSTEM_PROMPT` — incremental directory update prompt (74 lines)\n- `ROOT_SYSTEM_PROMPT` — root synthesis constraint prompt (10 lines)\n### types.ts\n**Purpose:** Defines TypeScript interfaces and constants for prompt construction context in the three-phase documentation generati...\n\n**Defines TypeScript interfaces and constants for prompt construction context in the three-phase documentation generation pipeline.**\n\n## Exported Types\n\n**`PromptContext`** — Interface providing file analysis context to prompt builders during Phase 1 (file analysis) and Phase 2 (directory aggregation):\n- `filePath: string` — Absolute path to file being analyzed\n- `content: string` — Raw file content to analyze\n- `contextFiles?: Array<{path: string; content: string}>` — Optional related files for dependency context\n- `projectPlan?: string` — Optional GENERATION-PLAN.md content providing bird's-eye project structure\n- `existingSum?: string` — Optional existing `.sum` summary text for incremental update workflows (enables delta-aware regeneration)\n\n## Documentation Guidelines\n\n**`SUMMARY_GUIDELINES`** — Immutable configuration object (as const) defining documentation generation constraints consumed by `src/generation/prompts/builder.ts` and `src/generation/prompts/templates.ts`:\n\n**`targetLength`** — Word count boundaries: `{min: 300, max: 500}`\n\n**`include`** — Array of 8 mandatory content categories:\n1. Purpose and responsibility\n2. Public interface (exports, key functions)\n3. Key patterns and notable algorithms\n4. Dependencies with usage context\n5. Key function signatures as code snippets\n6. Tightly coupled sibling files\n7. Behavioral contracts: verbatim regex patterns, format strings, magic constants, sentinel values, output templates, environment variables\n8. Annex references: for files with large string constants (prompt templates, config arrays, IDE templates), list each constant name with a one-line description in an ## Annex References section\n\n**`exclude`** — Array of 3 prohibited content categories:\n1. Control flow minutiae (loop structures, variable naming, temporary state)\n2. Generic TODOs/FIXMEs (keep only security/breaking)\n3. Broad architectural relationships (handled by AGENTS.md)\n\n## Integration Points\n\nConsumed by `PromptBuilder` class in `src/generation/prompts/builder.ts` for constructing AI prompts with standardized documentation quality constraints. The `existingSum` field enables incremental update workflows orchestrated by `src/update/orchestrator.ts` where hash-mismatched files receive their previous summary as context for delta-aware regeneration.\n\n## Import Map (verified — use these exact paths)\n\nbuilder.ts:\n  ../writers/sum.js → readSumFile, getSumPath\n  ../writers/agents-md.js → GENERATED_MARKER\n  ../../imports/index.js → extractDirectoryImports, formatImportMap\n  ../collector.js → collectAgentsDocs\n\n## Project Directory Structure\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\n\n## Annex Files (reproduction-critical constants)\n\n- templates.ts.annex.md\n- types.ts.annex.md",
      "systemPrompt": "You are generating an AGENTS.md file — a directory-level overview for AI coding assistants.\n\nCRITICAL: Output ONLY the raw markdown content. No code fences, no preamble, no explanations, no conversational text. Your entire response IS the AGENTS.md file content.\n\nOUTPUT FORMAT:\n- First line MUST be exactly: <!-- Generated by agents-reverse-engineer -->\n- Use a # heading with the directory name\n- Write a one-paragraph purpose statement for the directory\nusage\nADAPTIVE SECTIONS:\nAnalyze the directory contents and choose the most relevant sections. Do NOT use a fixed template. Instead, select sections that best document this specific directory for an AI that needs to reconstruct or extend the project.\n\nConsider these section types (choose what applies):\n- **Contents**: Group files by purpose/category under ## headings. For each file: markdown link [filename](./filename) and a one-line description.\n- **Subdirectories**: If subdirectories exist, list them with links [dirname/](./dirname/) and brief summaries.\n- **Architecture / Data Flow**: If files form a pipeline, request/response chain, or layered architecture, document it.\n- **Stack**: If this is a package root (has package.json, Cargo.toml, go.mod, etc.), document the technology stack, key scripts, and entry points.\n- **Structure**: If the directory layout follows a convention (feature-sliced, domain-driven, MVC, etc.), document it.\n- **Patterns**: If files share recurring design patterns (factory, strategy, middleware, barrel re-export), name and document them.\n- **Configuration**: If the directory contains config files, schemas, or environment definitions, document the config surface area.\n- **API Surface**: If the directory exports a public API (barrel index, route definitions, SDK), document the interface contract.\n- **File Relationships**: How files collaborate, depend on each other, or share state.\n- **Behavioral Contracts**: If files contain regex patterns, format specifications, magic constants, or template strings that define observable behavior, collect them in a dedicated section. Preserve verbatim patterns from file summaries — do NOT paraphrase regex into prose. This section is MANDATORY when file summaries contain behavioral artifacts.\n- **Reproduction-Critical Constants**: If file summaries reference annex files (via ## Annex References sections), list them with links. Example: \"Full prompt template text: [templates.ts.annex.md](./prompts/templates.ts.annex.md)\". Do NOT reproduce annex content in AGENTS.md — just link to it.\n\nChoose any relevant sections or create your own based on the directory contents. The goal is to provide a comprehensive overview that captures the essence of the directory's role in the project and how its files work together, with a focus on what an AI coding assistant would need to know to effectively interact with this code.\n\nSCOPE:\n- AGENTS.md is a NAVIGATIONAL INDEX — help an AI find the right file quickly\n- Focus on: what each file does, how files relate, directory-level patterns\n- Do NOT reproduce full architecture sections — those belong in the root CLAUDE.md\n\nPATH ACCURACY (MANDATORY):\n- When referencing files or modules outside this directory, use ONLY paths from the \"Import Map\" section\n- Do NOT invent, rename, or guess module paths — if a path isn't in the Import Map, don't reference it\n- Use the exact directory names from \"Project Directory Structure\" — do NOT rename directories\n  (e.g., if the directory is called \"cli\", write \"src/cli/\", NOT \"src/commands/\")\n- Cross-module references must use the specifier format from actual import statements\n  (e.g., \"../generation/writers/sum.js\", NOT \"../fs/sum-file.js\")\n- If you are unsure about a path, omit the cross-reference rather than guessing\n\nCONSISTENCY (MANDATORY):\n- Do not contradict yourself within the same document\n- If you describe a technique (e.g., \"regex-based\"), do not call it something else later (e.g., \"AST-based\")\n- When stating version numbers, engines, or config fields, use ONLY values present in the file summaries\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this directory\", \"this module\", \"provides\", \"responsible for\", \"is used to\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Per-file descriptions in Contents sections: 1-2 sentences maximum. Reference key symbols but do not reproduce full summaries.\n- Behavioral contracts (regex patterns, format specs, constants) belong in a separate Behavioral Contracts section, not in per-file descriptions.\n- Annex file references: link to .annex.md files, do not inline their content. One line per annex reference.\n- Subdirectory descriptions: 1-2 sentences maximum. Capture the directory's role, not its full contents.\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- Key exported symbols from file summaries MUST appear in the directory overview\n- Preserve exact casing of identifiers\n\nUSER NOTES:\n- If \"User Notes\" are provided in the prompt, they contain user-defined instructions that will be automatically prepended to your output\n- Do NOT repeat or paraphrase user notes in your generated content — they are included separately\n- You may reference information from user notes for context",
      "response": "<!-- Generated by agents-reverse-engineer -->\n\n# generation/prompts\n\nPrompt construction engine for the three-phase documentation pipeline: `buildFilePrompt()` assembles file analysis prompts with import maps and incremental update sections, `buildDirectoryPrompt()` aggregates child `.sum` files and subdirectory `AGENTS.md` with import maps for directory synthesis, `buildRootPrompt()` collects all `AGENTS.md` files and package.json metadata for root document generation.\n\n## Contents\n\n### Prompt Builders\n\n**[builder.ts](./builder.ts)** — Implements `buildFilePrompt()` with `detectLanguage()` mapping 21 extensions to syntax identifiers, `buildDirectoryPrompt()` aggregating `.sum` files via `readSumFile()` and extracting import maps via `extractDirectoryImports()`, `buildRootPrompt()` calling `collectAgentsDocs()` for project-level synthesis. Substitutes placeholders `{{FILE_PATH}}`, `{{CONTENT}}`, `{{LANG}}`, `{{PROJECT_PLAN_SECTION}}` in user prompts. Switches between fresh generation prompts (`FILE_SYSTEM_PROMPT`, `DIRECTORY_SYSTEM_PROMPT`) and update prompts (`FILE_UPDATE_SYSTEM_PROMPT`, `DIRECTORY_UPDATE_SYSTEM_PROMPT`) based on `existingSum`/`existingAgentsMd` presence. Preserves user content from `AGENTS.local.md` by appending as `## User Notes` section. Debug logging emits template action/metadata pairs to stderr via `picocolors`.\n\n**[templates.ts](./templates.ts)** — Exports six system prompt constants: `FILE_SYSTEM_PROMPT` enforces density rules (every sentence references specific identifiers), anchor term preservation (exact export name casing), behavioral contract extraction (verbatim regex patterns, format strings, magic constants), `FILE_USER_PROMPT` template with `{{FILE_PATH}}`/`{{CONTENT}}` placeholders, `FILE_UPDATE_SYSTEM_PROMPT` for incremental regeneration preserving unchanged sections, `DIRECTORY_SYSTEM_PROMPT` mandating first line `<!-- Generated by agents-reverse-engineer -->` with adaptive section strategy (Contents/Subdirectories/Architecture/Stack/Structure/Patterns/Configuration/API Surface/File Relationships/Behavioral Contracts), `DIRECTORY_UPDATE_SYSTEM_PROMPT` for directory-level incremental updates, `ROOT_SYSTEM_PROMPT` enforcing synthesis-only mode prohibiting invention of features/hooks/APIs/patterns. All prompts prohibit filler phrases: `\"this file\"`, `\"this module\"`, `\"provides\"`, `\"responsible for\"`, `\"is used to\"`.\n\n**[types.ts](./types.ts)** — Defines `PromptContext` interface with fields `filePath`, `content`, `contextFiles[]`, `projectPlan`, `existingSum` for Phase 1/2 builders. Exports `SUMMARY_GUIDELINES` constant object with `targetLength: {min: 300, max: 500}`, `include[]` array of 8 mandatory content categories (purpose, public interface, patterns, dependencies, function signatures, coupled files, behavioral contracts, annex references), `exclude[]` array of 3 prohibited categories (control flow minutiae, generic TODOs, broad architectural relationships).\n\n**[index.ts](./index.ts)** — Barrel module re-exporting `buildFilePrompt`, `buildDirectoryPrompt`, `buildRootPrompt`, `detectLanguage` from `builder.ts` and `PromptContext`, `SUMMARY_GUIDELINES` from `types.ts`. Separates prompt template strings (in `templates.ts`) from assembly logic.\n\n## File Relationships\n\n`builder.ts` consumes templates from `templates.ts` (`FILE_SYSTEM_PROMPT`, `FILE_USER_PROMPT`, `FILE_UPDATE_SYSTEM_PROMPT`, `DIRECTORY_SYSTEM_PROMPT`, `DIRECTORY_UPDATE_SYSTEM_PROMPT`, `ROOT_SYSTEM_PROMPT`). `builder.ts` imports `GENERATED_MARKER` from `../writers/agents-md.js` for user content detection, `readSumFile()`/`getSumPath()` from `../writers/sum.js` for child summary aggregation, `extractDirectoryImports()`/`formatImportMap()` from `../../imports/index.js` for dependency graph construction, `collectAgentsDocs()` from `../collector.js` for Phase 3 synthesis. `index.ts` re-exports `PromptContext` from `types.ts` for consumer type safety.\n\n## Incremental Update Strategy\n\nAll three builders accept optional existing content: `context.existingSum` in `buildFilePrompt()`, `existingAgentsMd` in `buildDirectoryPrompt()`. When provided, appends existing content under heading `## Existing [Summary|AGENTS.md] (update this — preserve stable content, modify only what changed)` and switches to update-specific system prompts. `FILE_UPDATE_SYSTEM_PROMPT` enforces preservation rules: keep unchanged sections verbatim, modify only content affected by code changes, preserve behavioral contracts (regex/constants) unless source changed them. `DIRECTORY_UPDATE_SYSTEM_PROMPT` preserves structure/headings/descriptions for unchanged files, modifies only entries whose summaries changed, adds/removes entries for new/deleted files.\n\n## User Content Preservation\n\n`buildDirectoryPrompt()` checks for `AGENTS.local.md` first, falling back to checking existing `AGENTS.md` for absence of `GENERATED_MARKER` constant. User content appended as `## User Notes` section with reference link pattern `[AGENTS.local.md](./AGENTS.local.md)`. First-run detection message: \"This user-defined content will be preserved as [AGENTS.local.md](./AGENTS.local.md).\"\n\n## Behavioral Contracts\n\nAll templates enforce:\n\n- **Density rule**: Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- **Anchor term preservation**: All exported function/class/type/const names MUST appear in summary exactly as written in source\n- **Filler phrase prohibition**: `\"this file\"`, `\"this module\"`, `\"provides\"`, `\"responsible for\"`, `\"is used to\"`, `\"basically\"`, `\"essentially\"`, `\"provides functionality for\"`\n- **Output format**: Start response DIRECTLY with bold purpose statement without preamble\n- **PATH ACCURACY**: Use only paths from \"Import Map\" section, exact directory names from \"Project Directory Structure\", no path invention\n- **CONSISTENCY**: Do not contradict within same document (e.g., calling technique \"regex-based\" then \"AST-based\")\n\n`DIRECTORY_SYSTEM_PROMPT` enforces first line exactly `<!-- Generated by agents-reverse-engineer -->` followed by `#` heading with directory name.\n\n`FILE_SYSTEM_PROMPT` defines REPRODUCTION-CRITICAL CONTENT strategy: files with large string constants should list them in `## Annex References` section rather than inlining content, delegating extraction to pipeline automation.\n\n## Annex References\n\nFull prompt template text: [templates.ts.annex.md](./templates.ts.annex.md)  \nPromptContext interface specification: [types.ts.annex.md](./types.ts.annex.md)",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 1732,
      "cacheReadTokens": 16334,
      "cacheCreationTokens": 14437,
      "latencyMs": 50368,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": []
    },
    {
      "timestamp": "2026-02-09T17:47:47.680Z",
      "prompt": "Generate AGENTS.md for directory: \".github/workflows\" (workflows)\n\n## File Summaries (1 files)\n\n### publish.yml\n**Purpose:** GitHub Actions workflow automating npm package publication with Sigstore provenance attestation on release events or ...\n\n**GitHub Actions workflow automating npm package publication with Sigstore provenance attestation on release events or manual dispatch.**\n\n## Workflow Triggers\n\nActivates on two GitHub event types:\n- `release` with `types: [published]` when GitHub release is published\n- `workflow_dispatch` for manual execution via GitHub UI\n\n## Job Configuration\n\n`publish` job runs on `ubuntu-latest` with two permissions:\n- `contents: read` for repository checkout\n- `id-token: write` enabling OIDC token generation for Sigstore provenance signing\n\n## Build Steps\n\nExecutes five sequential steps:\n\n1. **Checkout** via `actions/checkout@v4` clones repository at triggered commit SHA\n2. **Setup Node.js** via `actions/setup-node@v4` with `node-version: '20'` and `registry-url: 'https://registry.npmjs.org'` configuring npm authentication context\n3. **Install dependencies** via `npm ci` performing clean install from `package-lock.json`\n4. **Build** via `npm run build` invoking `prepublishOnly` script executing `tsc` TypeScript compilation and `build:hooks` hook file copying\n5. **Publish to npm** via `npm publish --provenance --access public` with `NODE_AUTH_TOKEN` from GitHub secret `NPM_TOKEN`\n\n## Provenance Attestation\n\n`--provenance` flag generates cryptographic attestation linking published npm package to source commit SHA via Sigstore transparency log, requires `id-token: write` permission for OIDC token exchange.\n\n## Access Control\n\n`--access public` flag ensures scoped package `agents-reverse-engineer` publishes with public visibility on npm registry.\n\n## Environment Variables\n\n`NODE_AUTH_TOKEN` set from GitHub secret `secrets.NPM_TOKEN` authenticates npm publish operation via `.npmrc` configuration injected by `actions/setup-node`.\n\n## Project Directory Structure\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>",
      "systemPrompt": "You are generating an AGENTS.md file — a directory-level overview for AI coding assistants.\n\nCRITICAL: Output ONLY the raw markdown content. No code fences, no preamble, no explanations, no conversational text. Your entire response IS the AGENTS.md file content.\n\nOUTPUT FORMAT:\n- First line MUST be exactly: <!-- Generated by agents-reverse-engineer -->\n- Use a # heading with the directory name\n- Write a one-paragraph purpose statement for the directory\nusage\nADAPTIVE SECTIONS:\nAnalyze the directory contents and choose the most relevant sections. Do NOT use a fixed template. Instead, select sections that best document this specific directory for an AI that needs to reconstruct or extend the project.\n\nConsider these section types (choose what applies):\n- **Contents**: Group files by purpose/category under ## headings. For each file: markdown link [filename](./filename) and a one-line description.\n- **Subdirectories**: If subdirectories exist, list them with links [dirname/](./dirname/) and brief summaries.\n- **Architecture / Data Flow**: If files form a pipeline, request/response chain, or layered architecture, document it.\n- **Stack**: If this is a package root (has package.json, Cargo.toml, go.mod, etc.), document the technology stack, key scripts, and entry points.\n- **Structure**: If the directory layout follows a convention (feature-sliced, domain-driven, MVC, etc.), document it.\n- **Patterns**: If files share recurring design patterns (factory, strategy, middleware, barrel re-export), name and document them.\n- **Configuration**: If the directory contains config files, schemas, or environment definitions, document the config surface area.\n- **API Surface**: If the directory exports a public API (barrel index, route definitions, SDK), document the interface contract.\n- **File Relationships**: How files collaborate, depend on each other, or share state.\n- **Behavioral Contracts**: If files contain regex patterns, format specifications, magic constants, or template strings that define observable behavior, collect them in a dedicated section. Preserve verbatim patterns from file summaries — do NOT paraphrase regex into prose. This section is MANDATORY when file summaries contain behavioral artifacts.\n- **Reproduction-Critical Constants**: If file summaries reference annex files (via ## Annex References sections), list them with links. Example: \"Full prompt template text: [templates.ts.annex.md](./prompts/templates.ts.annex.md)\". Do NOT reproduce annex content in AGENTS.md — just link to it.\n\nChoose any relevant sections or create your own based on the directory contents. The goal is to provide a comprehensive overview that captures the essence of the directory's role in the project and how its files work together, with a focus on what an AI coding assistant would need to know to effectively interact with this code.\n\nSCOPE:\n- AGENTS.md is a NAVIGATIONAL INDEX — help an AI find the right file quickly\n- Focus on: what each file does, how files relate, directory-level patterns\n- Do NOT reproduce full architecture sections — those belong in the root CLAUDE.md\n\nPATH ACCURACY (MANDATORY):\n- When referencing files or modules outside this directory, use ONLY paths from the \"Import Map\" section\n- Do NOT invent, rename, or guess module paths — if a path isn't in the Import Map, don't reference it\n- Use the exact directory names from \"Project Directory Structure\" — do NOT rename directories\n  (e.g., if the directory is called \"cli\", write \"src/cli/\", NOT \"src/commands/\")\n- Cross-module references must use the specifier format from actual import statements\n  (e.g., \"../generation/writers/sum.js\", NOT \"../fs/sum-file.js\")\n- If you are unsure about a path, omit the cross-reference rather than guessing\n\nCONSISTENCY (MANDATORY):\n- Do not contradict yourself within the same document\n- If you describe a technique (e.g., \"regex-based\"), do not call it something else later (e.g., \"AST-based\")\n- When stating version numbers, engines, or config fields, use ONLY values present in the file summaries\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this directory\", \"this module\", \"provides\", \"responsible for\", \"is used to\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Per-file descriptions in Contents sections: 1-2 sentences maximum. Reference key symbols but do not reproduce full summaries.\n- Behavioral contracts (regex patterns, format specs, constants) belong in a separate Behavioral Contracts section, not in per-file descriptions.\n- Annex file references: link to .annex.md files, do not inline their content. One line per annex reference.\n- Subdirectory descriptions: 1-2 sentences maximum. Capture the directory's role, not its full contents.\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- Key exported symbols from file summaries MUST appear in the directory overview\n- Preserve exact casing of identifiers\n\nUSER NOTES:\n- If \"User Notes\" are provided in the prompt, they contain user-defined instructions that will be automatically prepended to your output\n- Do NOT repeat or paraphrase user notes in your generated content — they are included separately\n- You may reference information from user notes for context",
      "response": "<!-- Generated by agents-reverse-engineer -->\n\n# workflows\n\n**GitHub Actions CI/CD automation for npm package publication with Sigstore provenance attestation.**\n\n## Contents\n\n### Workflows\n\n**[publish.yml](./publish.yml)** — Publishes npm package on release events via `npm publish --provenance --access public` with OIDC-signed cryptographic attestation linking artifact to commit SHA.\n\n## Triggers\n\nWorkflow activates on two event types:\n- `release[published]` when GitHub release is published\n- `workflow_dispatch` for manual execution via UI\n\n## Job Pipeline\n\nSingle `publish` job on `ubuntu-latest` executes five steps:\n1. `actions/checkout@v4` clones repository at triggered commit\n2. `actions/setup-node@v4` with Node.js 20 and npm registry authentication context\n3. `npm ci` installs dependencies from lockfile\n4. `npm run build` invokes `prepublishOnly` script (`tsc` + `build:hooks`)\n5. `npm publish --provenance --access public` with `NPM_TOKEN` secret\n\n## Provenance Attestation\n\n`--provenance` flag generates Sigstore transparency log entry cryptographically binding published package to source commit SHA via OIDC token exchange (`id-token: write` permission required). Artifact consumers verify authenticity via `npm audit signatures`.\n\n## Permissions\n\n- `contents: read` — Repository checkout access\n- `id-token: write` — OIDC token generation for Sigstore signing\n\n## Environment Variables\n\n`NODE_AUTH_TOKEN` set from GitHub secret `secrets.NPM_TOKEN` via `.npmrc` injection by `actions/setup-node`.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 391,
      "cacheReadTokens": 17587,
      "cacheCreationTokens": 9456,
      "latencyMs": 23994,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": []
    },
    {
      "timestamp": "2026-02-09T17:47:47.691Z",
      "prompt": "Generate AGENTS.md for directory: \"src/types\" (types)\n\n## File Summaries (1 files)\n\n### index.ts\n**Purpose:** Defines shared type interfaces for file discovery results used across discovery, orchestration, and CLI modules.\n\n**Defines shared type interfaces for file discovery results used across discovery, orchestration, and CLI modules.**\n\n## Exported Types\n\n### ExcludedFile\nInterface representing files filtered out during discovery.\n\n**Properties:**\n- `path: string` — Absolute or relative path to the excluded file\n- `reason: string` — Exclusion rationale (examples: \"gitignore pattern\", \"binary file\", \"vendor directory\")\n\n### DiscoveryResult\nInterface aggregating discovery phase output.\n\n**Properties:**\n- `files: string[]` — File paths eligible for analysis (passed to Phase 1 worker pool)\n- `excluded: ExcludedFile[]` — Files filtered out with exclusion metadata\n\n### DiscoveryStats\nInterface providing discovery metrics for CLI reporting.\n\n**Properties:**\n- `totalFiles: number` — Count of all files encountered during traversal\n- `includedFiles: number` — Count of files passing filters (equivalent to `DiscoveryResult.files.length`)\n- `excludedFiles: number` — Count of filtered files (equivalent to `DiscoveryResult.excluded.length`)\n- `exclusionReasons: Record<string, number>` — Histogram mapping exclusion reasons to occurrence counts (aggregated from `ExcludedFile.reason` values)\n\n## Usage Context\n\n`DiscoveryResult` is returned by `runDiscovery()` in `src/discovery/run.ts` and consumed by:\n- `discover` command (`src/cli/discover.ts`) for plan generation\n- `generate` command (`src/cli/generate.ts`) for Phase 1 file analysis\n- `update` command (`src/cli/update.ts`) for change detection input\n\n`DiscoveryStats` is computed from `DiscoveryResult` for terminal output in CLI commands via `src/output/logger.ts`.\n\n`ExcludedFile` is populated by filter chain in `src/discovery/walker.ts` using filter modules (`src/discovery/filters/`): `gitignore.ts`, `binary.ts`, `vendor.ts`, `custom.ts`.\n\n## Project Directory Structure\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>",
      "systemPrompt": "You are generating an AGENTS.md file — a directory-level overview for AI coding assistants.\n\nCRITICAL: Output ONLY the raw markdown content. No code fences, no preamble, no explanations, no conversational text. Your entire response IS the AGENTS.md file content.\n\nOUTPUT FORMAT:\n- First line MUST be exactly: <!-- Generated by agents-reverse-engineer -->\n- Use a # heading with the directory name\n- Write a one-paragraph purpose statement for the directory\nusage\nADAPTIVE SECTIONS:\nAnalyze the directory contents and choose the most relevant sections. Do NOT use a fixed template. Instead, select sections that best document this specific directory for an AI that needs to reconstruct or extend the project.\n\nConsider these section types (choose what applies):\n- **Contents**: Group files by purpose/category under ## headings. For each file: markdown link [filename](./filename) and a one-line description.\n- **Subdirectories**: If subdirectories exist, list them with links [dirname/](./dirname/) and brief summaries.\n- **Architecture / Data Flow**: If files form a pipeline, request/response chain, or layered architecture, document it.\n- **Stack**: If this is a package root (has package.json, Cargo.toml, go.mod, etc.), document the technology stack, key scripts, and entry points.\n- **Structure**: If the directory layout follows a convention (feature-sliced, domain-driven, MVC, etc.), document it.\n- **Patterns**: If files share recurring design patterns (factory, strategy, middleware, barrel re-export), name and document them.\n- **Configuration**: If the directory contains config files, schemas, or environment definitions, document the config surface area.\n- **API Surface**: If the directory exports a public API (barrel index, route definitions, SDK), document the interface contract.\n- **File Relationships**: How files collaborate, depend on each other, or share state.\n- **Behavioral Contracts**: If files contain regex patterns, format specifications, magic constants, or template strings that define observable behavior, collect them in a dedicated section. Preserve verbatim patterns from file summaries — do NOT paraphrase regex into prose. This section is MANDATORY when file summaries contain behavioral artifacts.\n- **Reproduction-Critical Constants**: If file summaries reference annex files (via ## Annex References sections), list them with links. Example: \"Full prompt template text: [templates.ts.annex.md](./prompts/templates.ts.annex.md)\". Do NOT reproduce annex content in AGENTS.md — just link to it.\n\nChoose any relevant sections or create your own based on the directory contents. The goal is to provide a comprehensive overview that captures the essence of the directory's role in the project and how its files work together, with a focus on what an AI coding assistant would need to know to effectively interact with this code.\n\nSCOPE:\n- AGENTS.md is a NAVIGATIONAL INDEX — help an AI find the right file quickly\n- Focus on: what each file does, how files relate, directory-level patterns\n- Do NOT reproduce full architecture sections — those belong in the root CLAUDE.md\n\nPATH ACCURACY (MANDATORY):\n- When referencing files or modules outside this directory, use ONLY paths from the \"Import Map\" section\n- Do NOT invent, rename, or guess module paths — if a path isn't in the Import Map, don't reference it\n- Use the exact directory names from \"Project Directory Structure\" — do NOT rename directories\n  (e.g., if the directory is called \"cli\", write \"src/cli/\", NOT \"src/commands/\")\n- Cross-module references must use the specifier format from actual import statements\n  (e.g., \"../generation/writers/sum.js\", NOT \"../fs/sum-file.js\")\n- If you are unsure about a path, omit the cross-reference rather than guessing\n\nCONSISTENCY (MANDATORY):\n- Do not contradict yourself within the same document\n- If you describe a technique (e.g., \"regex-based\"), do not call it something else later (e.g., \"AST-based\")\n- When stating version numbers, engines, or config fields, use ONLY values present in the file summaries\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this directory\", \"this module\", \"provides\", \"responsible for\", \"is used to\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Per-file descriptions in Contents sections: 1-2 sentences maximum. Reference key symbols but do not reproduce full summaries.\n- Behavioral contracts (regex patterns, format specs, constants) belong in a separate Behavioral Contracts section, not in per-file descriptions.\n- Annex file references: link to .annex.md files, do not inline their content. One line per annex reference.\n- Subdirectory descriptions: 1-2 sentences maximum. Capture the directory's role, not its full contents.\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- Key exported symbols from file summaries MUST appear in the directory overview\n- Preserve exact casing of identifiers\n\nUSER NOTES:\n- If \"User Notes\" are provided in the prompt, they contain user-defined instructions that will be automatically prepended to your output\n- Do NOT repeat or paraphrase user notes in your generated content — they are included separately\n- You may reference information from user notes for context",
      "response": "<!-- Generated by agents-reverse-engineer -->\n\n# src/types\n\nShared type definitions for file discovery results and statistics consumed across discovery, orchestration, and CLI modules.\n\n## Contents\n\n### [index.ts](./index.ts)\nExports `DiscoveryResult`, `DiscoveryStats`, `ExcludedFile` interfaces defining discovery phase output schema, exclusion metadata, and aggregate metrics.\n\n## Exported Interfaces\n\n### `ExcludedFile`\nRepresents files filtered during discovery with `path: string` and `reason: string` (exclusion rationale: \"gitignore pattern\", \"binary file\", \"vendor directory\").\n\n### `DiscoveryResult`\nAggregates discovery output with `files: string[]` (paths for Phase 1 analysis) and `excluded: ExcludedFile[]` (filtered files with metadata).\n\n### `DiscoveryStats`\nProvides discovery metrics: `totalFiles`, `includedFiles`, `excludedFiles` (counts), `exclusionReasons: Record<string, number>` (histogram of `ExcludedFile.reason` values).\n\n## Data Flow\n\n**Producer:** `runDiscovery()` in `src/discovery/run.ts` returns `DiscoveryResult` after filter chain execution (`src/discovery/walker.ts` + filters: `gitignore.ts`, `binary.ts`, `vendor.ts`, `custom.ts`).\n\n**Consumers:**\n- `src/cli/discover.ts` — Generates `GENERATION-PLAN.md` from `DiscoveryResult.files`\n- `src/cli/generate.ts` — Feeds `DiscoveryResult.files` to Phase 1 worker pool\n- `src/cli/update.ts` — Inputs `DiscoveryResult` to `detectChanges()` for delta computation\n- `src/output/logger.ts` — Computes `DiscoveryStats` from `DiscoveryResult` for terminal output\n\n**Exclusion Metadata:** `ExcludedFile.reason` populated by filter modules:\n- `src/discovery/filters/gitignore.ts` — \"matched .gitignore pattern: `<pattern>`\"\n- `src/discovery/filters/binary.ts` — \"binary file\"\n- `src/discovery/filters/vendor.ts` — \"vendor directory\"\n- `src/discovery/filters/custom.ts` — \"matched exclude pattern: `<pattern>`\"",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 569,
      "cacheReadTokens": 17587,
      "cacheCreationTokens": 9493,
      "latencyMs": 24252,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": []
    },
    {
      "timestamp": "2026-02-09T17:47:47.708Z",
      "prompt": "Generate AGENTS.md for directory: \"src/imports\" (imports)\n\n## File Summaries (3 files)\n\n### extractor.ts\n**Purpose:** extractor.ts extracts TypeScript/JavaScript import statements from source files via regex parsing and formats them as...\n\n**extractor.ts extracts TypeScript/JavaScript import statements from source files via regex parsing and formats them as structured import maps for LLM prompt consumption.**\n\n## Exported Functions\n\n### extractImports\n\n```typescript\nfunction extractImports(sourceContent: string): ImportEntry[]\n```\n\nParses source content using `IMPORT_REGEX` to extract all static import statements. Returns array of `ImportEntry` objects containing `specifier`, `symbols[]`, and `typeOnly` flag. Matches named imports (`{ Foo, Bar }`), namespace imports (`* as name`), and default imports. Strips `as` aliases from named symbols via `.replace(/\\s+as\\s+\\w+/, '')`. Resets `IMPORT_REGEX.lastIndex = 0` before matching to ensure clean state.\n\n### extractDirectoryImports\n\n```typescript\nasync function extractDirectoryImports(\n  dirPath: string,\n  fileNames: string[]\n): Promise<FileImports[]>\n```\n\nReads first 100 lines of each file in `fileNames` array (import region optimization), calls `extractImports()` on truncated content, classifies imports as `internal` (specifiers starting with `./`) or `external` (specifiers starting with `../`). Filters out bare package specifiers (npm modules) and `node:` built-ins. Returns `FileImports[]` containing `fileName`, `externalImports`, `internalImports` for files with relative imports. Silently skips unreadable files via empty catch block.\n\n### formatImportMap\n\n```typescript\nfunction formatImportMap(fileImports: FileImports[]): string\n```\n\nConverts `FileImports[]` into human-readable text block for LLM prompts. Groups external imports by file, appends `(type)` suffix for type-only imports. Output format: filename as header, indented specifier-symbol pairs (`  ../ai/index.js → AIService`). Only includes files with external imports (skips internal-only files).\n\n## Regular Expression\n\n### IMPORT_REGEX\n\n```typescript\nconst IMPORT_REGEX =\n  /^import\\s+(type\\s+)?(?:\\{([^}]*)\\}|(\\*\\s+as\\s+\\w+)|(\\w+))\\s+from\\s+['\"]([^'\"]+)['\"]/gm\n```\n\nMultiline regex matching static import statements at line start (`^import`). Capture groups: (1) `type` keyword for type-only imports, (2) named symbols between braces, (3) namespace import `* as name`, (4) default import identifier, (5) module specifier string. Uses `/gm` flags for global multiline matching. Does not match dynamic imports (`import()`) or imports inside comments/strings.\n\n## Import Classification Rules\n\n- **Internal imports**: Specifiers starting with `./` (same directory)\n- **External imports**: Specifiers starting with `../` (parent/sibling directories)\n- **Filtered out**: Bare specifiers (npm packages like `'react'`), `node:` built-ins (like `'node:fs'`)\n\n## Performance Optimization\n\nReads only first 100 lines of source files via `content.split('\\n').slice(0, 100).join('\\n')` before regex matching. Assumes all imports appear at top of file (standard ES module convention).\n\n## Integration Points\n\nUsed by `src/generation/prompts/builder.ts` to construct import context for directory-level `AGENTS.md` generation. Provides dependency coupling information via `formatImportMap()` output embedded in AI prompts.\n### index.ts\n**Purpose:** Barrel export module re-exporting import extraction functionality from `extractor.ts` and type definitions from `type...\n\n**Barrel export module re-exporting import extraction functionality from `extractor.ts` and type definitions from `types.ts`.**\n\n## Exported Functions\n\n- `extractImports` — Extracts import statements from a single source file\n- `extractDirectoryImports` — Aggregates imports across all files within a directory\n- `formatImportMap` — Formats collected import data into human-readable map representation\n\n## Exported Types\n\n- `ImportEntry` — Represents a single import statement with source module and imported identifiers\n- `FileImports` — Container type associating a file path with its array of ImportEntry records\n\n## Integration Points\n\nThis module serves as the public API surface for the `src/imports/` subsystem, consumed by `src/generation/prompts/builder.ts` during Phase 2 directory aggregation to construct import maps included in directory-level AGENTS.md prompts via `extractDirectoryImports()`.\n### types.ts\n**Purpose:** Defines TypeScript interfaces for representing import statements extracted from source files via static analysis.\n\n**Defines TypeScript interfaces for representing import statements extracted from source files via static analysis.**\n\n## Exported Types\n\n### ImportEntry\nRepresents a single import statement extracted from source code.\n\n**Properties:**\n- `specifier: string` — Raw import path as written in source (e.g., `'../ai/index.js'`)\n- `symbols: string[]` — Array of imported identifiers (e.g., `['AIService', 'AIResponse']`)\n- `typeOnly: boolean` — Discriminates TypeScript type-only imports (`import type { ... }`)\n\n### FileImports\nAggregates all imports discovered in a single source file, partitioned by locality.\n\n**Properties:**\n- `fileName: string` — Relative file path (e.g., `'runner.ts'`)\n- `externalImports: ImportEntry[]` — Imports from other modules or parent/sibling directories\n- `internalImports: ImportEntry[]` — Imports from same directory as the source file\n\n## Design Patterns\n\n**Locality-based partitioning:** The `externalImports`/`internalImports` separation supports dependency graph construction where internal imports indicate tight coupling within a directory module, while external imports reveal cross-module dependencies. This distinction is used by `src/generation/prompts/builder.ts` when constructing import maps for AI prompts during Phase 1 file analysis.\n\n## Integration Points\n\nConsumed by:\n- `src/imports/extractor.ts` — Parses TypeScript/JavaScript source via regex to produce `FileImports` instances\n- `src/generation/prompts/builder.ts` — Includes `externalImports` in file analysis prompts to provide dependency context\n- `src/generation/prompts/templates.ts` — Aggregates directory-level imports from child `FileImports` for `AGENTS.md` generation\n\n## Project Directory Structure\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>",
      "systemPrompt": "You are generating an AGENTS.md file — a directory-level overview for AI coding assistants.\n\nCRITICAL: Output ONLY the raw markdown content. No code fences, no preamble, no explanations, no conversational text. Your entire response IS the AGENTS.md file content.\n\nOUTPUT FORMAT:\n- First line MUST be exactly: <!-- Generated by agents-reverse-engineer -->\n- Use a # heading with the directory name\n- Write a one-paragraph purpose statement for the directory\nusage\nADAPTIVE SECTIONS:\nAnalyze the directory contents and choose the most relevant sections. Do NOT use a fixed template. Instead, select sections that best document this specific directory for an AI that needs to reconstruct or extend the project.\n\nConsider these section types (choose what applies):\n- **Contents**: Group files by purpose/category under ## headings. For each file: markdown link [filename](./filename) and a one-line description.\n- **Subdirectories**: If subdirectories exist, list them with links [dirname/](./dirname/) and brief summaries.\n- **Architecture / Data Flow**: If files form a pipeline, request/response chain, or layered architecture, document it.\n- **Stack**: If this is a package root (has package.json, Cargo.toml, go.mod, etc.), document the technology stack, key scripts, and entry points.\n- **Structure**: If the directory layout follows a convention (feature-sliced, domain-driven, MVC, etc.), document it.\n- **Patterns**: If files share recurring design patterns (factory, strategy, middleware, barrel re-export), name and document them.\n- **Configuration**: If the directory contains config files, schemas, or environment definitions, document the config surface area.\n- **API Surface**: If the directory exports a public API (barrel index, route definitions, SDK), document the interface contract.\n- **File Relationships**: How files collaborate, depend on each other, or share state.\n- **Behavioral Contracts**: If files contain regex patterns, format specifications, magic constants, or template strings that define observable behavior, collect them in a dedicated section. Preserve verbatim patterns from file summaries — do NOT paraphrase regex into prose. This section is MANDATORY when file summaries contain behavioral artifacts.\n- **Reproduction-Critical Constants**: If file summaries reference annex files (via ## Annex References sections), list them with links. Example: \"Full prompt template text: [templates.ts.annex.md](./prompts/templates.ts.annex.md)\". Do NOT reproduce annex content in AGENTS.md — just link to it.\n\nChoose any relevant sections or create your own based on the directory contents. The goal is to provide a comprehensive overview that captures the essence of the directory's role in the project and how its files work together, with a focus on what an AI coding assistant would need to know to effectively interact with this code.\n\nSCOPE:\n- AGENTS.md is a NAVIGATIONAL INDEX — help an AI find the right file quickly\n- Focus on: what each file does, how files relate, directory-level patterns\n- Do NOT reproduce full architecture sections — those belong in the root CLAUDE.md\n\nPATH ACCURACY (MANDATORY):\n- When referencing files or modules outside this directory, use ONLY paths from the \"Import Map\" section\n- Do NOT invent, rename, or guess module paths — if a path isn't in the Import Map, don't reference it\n- Use the exact directory names from \"Project Directory Structure\" — do NOT rename directories\n  (e.g., if the directory is called \"cli\", write \"src/cli/\", NOT \"src/commands/\")\n- Cross-module references must use the specifier format from actual import statements\n  (e.g., \"../generation/writers/sum.js\", NOT \"../fs/sum-file.js\")\n- If you are unsure about a path, omit the cross-reference rather than guessing\n\nCONSISTENCY (MANDATORY):\n- Do not contradict yourself within the same document\n- If you describe a technique (e.g., \"regex-based\"), do not call it something else later (e.g., \"AST-based\")\n- When stating version numbers, engines, or config fields, use ONLY values present in the file summaries\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this directory\", \"this module\", \"provides\", \"responsible for\", \"is used to\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Per-file descriptions in Contents sections: 1-2 sentences maximum. Reference key symbols but do not reproduce full summaries.\n- Behavioral contracts (regex patterns, format specs, constants) belong in a separate Behavioral Contracts section, not in per-file descriptions.\n- Annex file references: link to .annex.md files, do not inline their content. One line per annex reference.\n- Subdirectory descriptions: 1-2 sentences maximum. Capture the directory's role, not its full contents.\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- Key exported symbols from file summaries MUST appear in the directory overview\n- Preserve exact casing of identifiers\n\nUSER NOTES:\n- If \"User Notes\" are provided in the prompt, they contain user-defined instructions that will be automatically prepended to your output\n- Do NOT repeat or paraphrase user notes in your generated content — they are included separately\n- You may reference information from user notes for context",
      "response": "<!-- Generated by agents-reverse-engineer -->\n\n# src/imports/\n\nStatic import analysis subsystem extracting TypeScript/JavaScript import statements via regex parsing to construct dependency graphs consumed by Phase 1 file analysis and Phase 2 directory aggregation prompts.\n\n## Contents\n\n### [extractor.ts](./extractor.ts)\nRegex-based parser matching static import statements via `IMPORT_REGEX` (`/^import\\s+(type\\s+)?(?:\\{([^}]*)\\}|(\\*\\s+as\\s+\\w+)|(\\w+))\\s+from\\s+['\"]([^'\"]+)['\"]/gm`). Exports `extractImports()` returning `ImportEntry[]` with `specifier`, `symbols[]`, `typeOnly` fields. Exports `extractDirectoryImports()` reading first 100 lines per file, classifying imports as `internal` (`./` prefix) or `external` (`../` prefix), filtering bare package specifiers and `node:` built-ins. Exports `formatImportMap()` converting `FileImports[]` to human-readable text blocks for LLM prompt embedding.\n\n### [types.ts](./types.ts)\nDefines `ImportEntry` interface with `specifier: string`, `symbols: string[]`, `typeOnly: boolean` representing single import statement. Defines `FileImports` interface with `fileName: string`, `externalImports: ImportEntry[]`, `internalImports: ImportEntry[]` partitioning imports by locality.\n\n### [index.ts](./index.ts)\nBarrel export re-exporting `extractImports`, `extractDirectoryImports`, `formatImportMap` from `extractor.ts` and `ImportEntry`, `FileImports` from `types.ts`.\n\n## Data Flow\n\n1. **File Analysis (Phase 1):** `extractDirectoryImports()` invoked by `src/generation/prompts/builder.ts` → reads first 100 lines per source file → applies `IMPORT_REGEX` → classifies imports by locality → returns `FileImports[]`\n2. **Prompt Embedding:** `formatImportMap()` serializes `FileImports[]` → produces text block with filename headers and indented specifier-symbol pairs → embedded in AI prompts via `buildFileAnalysisPrompt()`\n3. **Directory Aggregation (Phase 2):** Import maps from child files aggregated during directory-level `AGENTS.md` generation to provide cross-module dependency context\n\n## Import Classification\n\n| Category | Pattern | Included | Purpose |\n|----------|---------|----------|---------|\n| **Internal** | `./filename` | Yes | Same-directory coupling (tight cohesion signals) |\n| **External** | `../path/module` | Yes | Cross-directory dependencies (module boundaries) |\n| **Package** | `'react'` | No | Third-party npm dependencies (not project structure) |\n| **Built-in** | `'node:fs'` | No | Runtime APIs (not project structure) |\n\n## Performance Optimization\n\nReads only first 100 lines per file via `content.split('\\n').slice(0, 100).join('\\n')` before regex matching, assuming ES module convention of top-level imports. Skips unreadable files silently via empty catch blocks in `extractDirectoryImports()`.\n\n## Integration Points\n\n- **Consumed by:** `src/generation/prompts/builder.ts` embeds import maps in file analysis prompts via `extractDirectoryImports()` + `formatImportMap()`\n- **Prompts:** `src/generation/prompts/templates.ts` includes import map sections in Phase 1 file analysis and Phase 2 directory aggregation templates\n- **Output format:** Human-readable text blocks with filename headers (`external imports from foo.ts:`) and indented specifier-symbol pairs (`  ../ai/index.js → AIService`)",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 905,
      "cacheReadTokens": 0,
      "cacheCreationTokens": 28133,
      "latencyMs": 26831,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": []
    },
    {
      "timestamp": "2026-02-09T17:47:47.688Z",
      "prompt": "Generate AGENTS.md for directory: \"src/output\" (output)\n\n## File Summaries (1 files)\n\n### logger.ts\n**Purpose:** logger.ts provides terminal output formatting with optional picocolors styling, exporting createLogger() and createSi...\n\n**logger.ts provides terminal output formatting with optional picocolors styling, exporting createLogger() and createSilentLogger() factories for CLI messages.**\n\n## Exported Interfaces\n\n**Logger** defines six methods for CLI output:\n- `info(message: string): void` — informational messages\n- `file(path: string): void` — discovered file paths\n- `excluded(path: string, reason: string, filter: string): void` — excluded files with reasoning\n- `summary(included: number, excluded: number): void` — discovery count summary\n- `warn(message: string): void` — warning messages\n- `error(message: string): void` — error messages\n\n**LoggerOptions** configures logger instances:\n- `colors: boolean` — enables/disables ANSI color codes (default true)\n\n## Factory Functions\n\n**createLogger(options: LoggerOptions): Logger** constructs color-aware logger writing to console.log/warn/error with picocolors formatting when `options.colors === true`:\n- `file()` outputs `green(\"  +\") + \" \" + path`\n- `excluded()` outputs `dim(\"  -\") + \" \" + path + dim(\" (reason: filter)\")`\n- `summary()` outputs `bold(\"Discovered N files\") + dim(\" (M excluded)\")`\n- `warn()` outputs `yellow(\"Warning: \") + message`\n- `error()` outputs `red(\"Error: \") + message`\n- `info()` outputs plain `message`\n\n**createSilentLogger(): Logger** returns no-op Logger instance with all methods bound to empty function, used for testing or programmatic usage suppressing output.\n\n## Color System\n\n**ColorFunctions** interface abstracts picocolors API with five transform methods: `green`, `dim`, `red`, `bold`, `yellow`. When `options.colors === false`, logger uses **noColor** object mapping all transforms to **identity(s: string): string** pass-through function preserving original strings unchanged.\n\n## Output Format Specification\n\nLogger implements CONTEXT.md human-readable format with specific prefixes and styling:\n- Discovered files: `\"  +\"` prefix in green\n- Excluded files: `\"  -\"` prefix dimmed with parenthetical reason\n- Summary line: bold count followed by dimmed exclusion count\n- Warnings: `\"Warning: \"` prefix in yellow\n- Errors: `\"Error: \"` prefix in red\n\n## Dependencies\n\nImports `picocolors` as `pc` for ANSI color code generation. No other external dependencies beyond Node.js built-in `console` object.\n\n## Project Directory Structure\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>",
      "systemPrompt": "You are generating an AGENTS.md file — a directory-level overview for AI coding assistants.\n\nCRITICAL: Output ONLY the raw markdown content. No code fences, no preamble, no explanations, no conversational text. Your entire response IS the AGENTS.md file content.\n\nOUTPUT FORMAT:\n- First line MUST be exactly: <!-- Generated by agents-reverse-engineer -->\n- Use a # heading with the directory name\n- Write a one-paragraph purpose statement for the directory\nusage\nADAPTIVE SECTIONS:\nAnalyze the directory contents and choose the most relevant sections. Do NOT use a fixed template. Instead, select sections that best document this specific directory for an AI that needs to reconstruct or extend the project.\n\nConsider these section types (choose what applies):\n- **Contents**: Group files by purpose/category under ## headings. For each file: markdown link [filename](./filename) and a one-line description.\n- **Subdirectories**: If subdirectories exist, list them with links [dirname/](./dirname/) and brief summaries.\n- **Architecture / Data Flow**: If files form a pipeline, request/response chain, or layered architecture, document it.\n- **Stack**: If this is a package root (has package.json, Cargo.toml, go.mod, etc.), document the technology stack, key scripts, and entry points.\n- **Structure**: If the directory layout follows a convention (feature-sliced, domain-driven, MVC, etc.), document it.\n- **Patterns**: If files share recurring design patterns (factory, strategy, middleware, barrel re-export), name and document them.\n- **Configuration**: If the directory contains config files, schemas, or environment definitions, document the config surface area.\n- **API Surface**: If the directory exports a public API (barrel index, route definitions, SDK), document the interface contract.\n- **File Relationships**: How files collaborate, depend on each other, or share state.\n- **Behavioral Contracts**: If files contain regex patterns, format specifications, magic constants, or template strings that define observable behavior, collect them in a dedicated section. Preserve verbatim patterns from file summaries — do NOT paraphrase regex into prose. This section is MANDATORY when file summaries contain behavioral artifacts.\n- **Reproduction-Critical Constants**: If file summaries reference annex files (via ## Annex References sections), list them with links. Example: \"Full prompt template text: [templates.ts.annex.md](./prompts/templates.ts.annex.md)\". Do NOT reproduce annex content in AGENTS.md — just link to it.\n\nChoose any relevant sections or create your own based on the directory contents. The goal is to provide a comprehensive overview that captures the essence of the directory's role in the project and how its files work together, with a focus on what an AI coding assistant would need to know to effectively interact with this code.\n\nSCOPE:\n- AGENTS.md is a NAVIGATIONAL INDEX — help an AI find the right file quickly\n- Focus on: what each file does, how files relate, directory-level patterns\n- Do NOT reproduce full architecture sections — those belong in the root CLAUDE.md\n\nPATH ACCURACY (MANDATORY):\n- When referencing files or modules outside this directory, use ONLY paths from the \"Import Map\" section\n- Do NOT invent, rename, or guess module paths — if a path isn't in the Import Map, don't reference it\n- Use the exact directory names from \"Project Directory Structure\" — do NOT rename directories\n  (e.g., if the directory is called \"cli\", write \"src/cli/\", NOT \"src/commands/\")\n- Cross-module references must use the specifier format from actual import statements\n  (e.g., \"../generation/writers/sum.js\", NOT \"../fs/sum-file.js\")\n- If you are unsure about a path, omit the cross-reference rather than guessing\n\nCONSISTENCY (MANDATORY):\n- Do not contradict yourself within the same document\n- If you describe a technique (e.g., \"regex-based\"), do not call it something else later (e.g., \"AST-based\")\n- When stating version numbers, engines, or config fields, use ONLY values present in the file summaries\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this directory\", \"this module\", \"provides\", \"responsible for\", \"is used to\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Per-file descriptions in Contents sections: 1-2 sentences maximum. Reference key symbols but do not reproduce full summaries.\n- Behavioral contracts (regex patterns, format specs, constants) belong in a separate Behavioral Contracts section, not in per-file descriptions.\n- Annex file references: link to .annex.md files, do not inline their content. One line per annex reference.\n- Subdirectory descriptions: 1-2 sentences maximum. Capture the directory's role, not its full contents.\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- Key exported symbols from file summaries MUST appear in the directory overview\n- Preserve exact casing of identifiers\n\nUSER NOTES:\n- If \"User Notes\" are provided in the prompt, they contain user-defined instructions that will be automatically prepended to your output\n- Do NOT repeat or paraphrase user notes in your generated content — they are included separately\n- You may reference information from user notes for context",
      "response": "<!-- Generated by agents-reverse-engineer -->\n\n# src/output\n\nTerminal output formatting layer providing color-aware Logger interface for CLI messages with optional ANSI styling via picocolors.\n\n## Contents\n\n**[logger.ts](./logger.ts)** — Exports `createLogger(options)` factory constructing color-aware Logger instances writing to console.log/warn/error with picocolors-based ANSI formatting, `createSilentLogger()` returning no-op Logger for testing, `Logger` interface defining six output methods (info/file/excluded/summary/warn/error), and `LoggerOptions` schema controlling color enablement.\n\n## API Surface\n\n**Logger interface:**\n- `info(message: string): void` — informational messages (plain text)\n- `file(path: string): void` — discovered file paths (`\"  +\" + path` in green)\n- `excluded(path: string, reason: string, filter: string): void` — excluded files (`\"  -\" + path + \" (reason: filter)\"` dimmed)\n- `summary(included: number, excluded: number): void` — discovery count summary (`\"Discovered N files (M excluded)\"` with bold/dim styling)\n- `warn(message: string): void` — warning messages (`\"Warning: \" + message` in yellow)\n- `error(message: string): void` — error messages (`\"Error: \" + message` in red)\n\n**Factory functions:**\n- `createLogger(options: LoggerOptions): Logger` — constructs color-aware logger respecting `options.colors` boolean flag\n- `createSilentLogger(): Logger` — returns no-op logger with all methods bound to empty function\n\n**Configuration:**\n- `LoggerOptions` — schema with single `colors: boolean` field controlling ANSI escape code emission\n\n## Output Format Specification\n\nLogger implements CONTEXT.md-defined format with mandatory prefixes and styling:\n- Discovered files: `\"  +\"` prefix green-styled followed by space and path\n- Excluded files: `\"  -\"` prefix dimmed followed by path and parenthetical `(reason: filter)`\n- Summary line: bold `\"Discovered N files\"` followed by dimmed `\" (M excluded)\"`\n- Warnings: `\"Warning: \"` yellow-styled prefix\n- Errors: `\"Error: \"` red-styled prefix\n\nWhen `options.colors === false`, all picocolors transforms replaced with identity function via `noColor` object mapping `green`/`dim`/`red`/`bold`/`yellow` to pass-through.\n\n## Integration Points\n\nUsed by:\n- `src/discovery/run.ts` — `discoverFiles()` accepts Logger for real-time file discovery progress reporting\n- `src/cli/*.ts` — Command entry points construct logger via `createLogger({ colors: config.output.colors })` from loaded configuration\n- Test suites — `createSilentLogger()` suppresses output during programmatic invocation\n\nImports:\n- `picocolors` (aliased `pc`) — ANSI color code generation library",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 703,
      "cacheReadTokens": 17587,
      "cacheCreationTokens": 9612,
      "latencyMs": 27699,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": []
    },
    {
      "timestamp": "2026-02-09T17:47:47.699Z",
      "prompt": "Generate AGENTS.md for directory: \"src/change-detection\" (change-detection)\n\n## File Summaries (3 files)\n\n### detector.ts\n**Purpose:** detector.ts provides git-based change detection via simple-git and SHA-256 content hashing for incremental documentat...\n\n**detector.ts provides git-based change detection via simple-git and SHA-256 content hashing for incremental documentation updates.**\n\n## Exported Functions\n\n**isGitRepo(projectRoot: string): Promise<boolean>**  \nCalls `simpleGit(projectRoot).checkIsRepo()` to verify git repository presence.\n\n**getCurrentCommit(projectRoot: string): Promise<string>**  \nExecutes `git.revparse(['HEAD'])` returning trimmed HEAD commit hash.\n\n**getChangedFiles(projectRoot: string, baseCommit: string, options?: ChangeDetectionOptions): Promise<ChangeDetectionResult>**  \nDetects file changes between `baseCommit` and `HEAD` via `git diff --name-status -M`, parsing status codes `A` (added), `M` (modified), `D` (deleted), `R` (renamed with 50% similarity threshold). When `options.includeUncommitted` is true, merges uncommitted changes via `git.status()` reading `status.modified`, `status.deleted`, `status.not_added`, `status.staged` arrays. Returns `ChangeDetectionResult` containing `currentCommit`, `baseCommit`, `changes: FileChange[]`, `includesUncommitted: boolean`.\n\n**computeContentHash(filePath: string): Promise<string>**  \nReads file via `readFile()`, computes SHA-256 hash via `createHash('sha256').update(content).digest('hex')`, returns hex-encoded hash string matching `.sum` YAML frontmatter `content_hash` field.\n\n**computeContentHashFromString(content: string): string**  \nSynchronous variant computing SHA-256 from in-memory string content, avoiding redundant disk reads when file content already loaded.\n\n## Change Detection Algorithm\n\nParses `git diff` output with format `STATUS\\tFILE` or `STATUS\\tOLD\\tNEW` for renames. Status codes map to `FileChange.status`: `'A'` → `'added'`, `'M'` → `'modified'`, `'D'` → `'deleted'`, `'R*'` → `'renamed'` with `oldPath` extracted from second tab-delimited field. Rename detection uses `-M` flag with implicit 50% similarity threshold.\n\n## Uncommitted Change Merge Strategy\n\nWhen `includeUncommitted` enabled, calls `git.status()` and deduplicates against committed changes via `changes.some(c => c.path === file)` predicate. Adds `status.modified` as `'modified'`, `status.deleted` as `'deleted'`, `status.not_added` as `'added'`, `status.staged` as `'added'`. Prevents duplicate `FileChange` entries for files appearing in both committed diff and working tree status.\n\n## Integration Points\n\nConsumed by `src/update/orchestrator.ts` to compute `filesToAnalyze` by comparing `computeContentHash()` results against `.sum` frontmatter `content_hash` values. Supports non-git workflows via `computeContentHash()` fallback when `isGitRepo()` returns false.\n### index.ts\n**Purpose:** Barrel module exporting git-based change detection functions and types for incremental documentation updates with SHA...\n\n**Barrel module exporting git-based change detection functions and types for incremental documentation updates with SHA-256 content hashing.**\n\n## Exported Functions\n\nRe-exports five functions from `./detector.js`:\n\n- `isGitRepo(): Promise<boolean>` — Detects whether current directory is inside a git repository\n- `getCurrentCommit(): Promise<string | null>` — Returns current HEAD commit SHA or null if not in git repo\n- `getChangedFiles(options: ChangeDetectionOptions): Promise<ChangeDetectionResult>` — Detects added/modified/deleted/renamed files via git diff with optional uncommitted changes merge\n- `computeContentHash(filePath: string): Promise<string>` — Computes SHA-256 hash of file content for incremental update comparison\n- `computeContentHashFromString(content: string): string` — Computes SHA-256 hash of string content directly\n\n## Exported Types\n\nRe-exports four types from `./types.js`:\n\n- `ChangeType` — Discriminated union literal: `'added' | 'modified' | 'deleted' | 'renamed'`\n- `FileChange` — Object containing `{ path: string, status: ChangeType, oldPath?: string }` where oldPath populated only for `status: 'renamed'`\n- `ChangeDetectionResult` — Object containing `{ changes: FileChange[], baseCommit: string | null }` where baseCommit is source commit SHA\n- `ChangeDetectionOptions` — Configuration object controlling `{ baseCommit?: string, includeUncommitted?: boolean }` for `getChangedFiles()`\n\n## Integration Points\n\nConsumed by `src/update/orchestrator.ts` which calls `getChangedFiles()` to compute `filesToAnalyze`, `filesToSkip`, and orphaned `.sum` files requiring cleanup. The `computeContentHash()` function compares against `content_hash` from YAML frontmatter in existing `.sum` files to determine modification status.\n\n## Module Organization\n\nPure barrel module providing single import path `src/change-detection` for consumers. Implementation logic resides in `detector.ts` (git operations, SHA-256 hashing) and `types.ts` (TypeScript interfaces). No runtime logic executed in this index file.\n### types.ts\n**Purpose:** Defines TypeScript interfaces and types for git-based change detection, representing file changes, change detection r...\n\n**Defines TypeScript interfaces and types for git-based change detection, representing file changes, change detection results, and configuration options.**\n\n## Exported Types\n\n**ChangeType** — Union type `'added' | 'modified' | 'deleted' | 'renamed'` representing possible git diff statuses for files.\n\n**FileChange** — Interface with `path: string` (relative path, new path for renames), `status: ChangeType`, optional `oldPath?: string` (original path for renamed files). Represents individual file change detected from git diff.\n\n**ChangeDetectionResult** — Interface aggregating `currentCommit: string`, `baseCommit: string`, `changes: FileChange[]`, `includesUncommitted: boolean`. Returned by change detection operations to describe what changed between commits.\n\n**ChangeDetectionOptions** — Configuration interface with optional `includeUncommitted?: boolean` flag controlling whether staged and working directory changes merge into detection results.\n\n## Integration Context\n\nUsed by `src/change-detection/detector.ts` which implements git diff parsing via `simple-git` library. Supports `git diff --name-status -M` with rename detection (50% similarity threshold). The `FileChange.oldPath` field populated when `ChangeType` is `'renamed'`. Consumed by `src/update/orchestrator.ts` for incremental documentation updates via SHA-256 hash comparison against `.sum` frontmatter `content_hash` fields.\n\n## Project Directory Structure\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>",
      "systemPrompt": "You are generating an AGENTS.md file — a directory-level overview for AI coding assistants.\n\nCRITICAL: Output ONLY the raw markdown content. No code fences, no preamble, no explanations, no conversational text. Your entire response IS the AGENTS.md file content.\n\nOUTPUT FORMAT:\n- First line MUST be exactly: <!-- Generated by agents-reverse-engineer -->\n- Use a # heading with the directory name\n- Write a one-paragraph purpose statement for the directory\nusage\nADAPTIVE SECTIONS:\nAnalyze the directory contents and choose the most relevant sections. Do NOT use a fixed template. Instead, select sections that best document this specific directory for an AI that needs to reconstruct or extend the project.\n\nConsider these section types (choose what applies):\n- **Contents**: Group files by purpose/category under ## headings. For each file: markdown link [filename](./filename) and a one-line description.\n- **Subdirectories**: If subdirectories exist, list them with links [dirname/](./dirname/) and brief summaries.\n- **Architecture / Data Flow**: If files form a pipeline, request/response chain, or layered architecture, document it.\n- **Stack**: If this is a package root (has package.json, Cargo.toml, go.mod, etc.), document the technology stack, key scripts, and entry points.\n- **Structure**: If the directory layout follows a convention (feature-sliced, domain-driven, MVC, etc.), document it.\n- **Patterns**: If files share recurring design patterns (factory, strategy, middleware, barrel re-export), name and document them.\n- **Configuration**: If the directory contains config files, schemas, or environment definitions, document the config surface area.\n- **API Surface**: If the directory exports a public API (barrel index, route definitions, SDK), document the interface contract.\n- **File Relationships**: How files collaborate, depend on each other, or share state.\n- **Behavioral Contracts**: If files contain regex patterns, format specifications, magic constants, or template strings that define observable behavior, collect them in a dedicated section. Preserve verbatim patterns from file summaries — do NOT paraphrase regex into prose. This section is MANDATORY when file summaries contain behavioral artifacts.\n- **Reproduction-Critical Constants**: If file summaries reference annex files (via ## Annex References sections), list them with links. Example: \"Full prompt template text: [templates.ts.annex.md](./prompts/templates.ts.annex.md)\". Do NOT reproduce annex content in AGENTS.md — just link to it.\n\nChoose any relevant sections or create your own based on the directory contents. The goal is to provide a comprehensive overview that captures the essence of the directory's role in the project and how its files work together, with a focus on what an AI coding assistant would need to know to effectively interact with this code.\n\nSCOPE:\n- AGENTS.md is a NAVIGATIONAL INDEX — help an AI find the right file quickly\n- Focus on: what each file does, how files relate, directory-level patterns\n- Do NOT reproduce full architecture sections — those belong in the root CLAUDE.md\n\nPATH ACCURACY (MANDATORY):\n- When referencing files or modules outside this directory, use ONLY paths from the \"Import Map\" section\n- Do NOT invent, rename, or guess module paths — if a path isn't in the Import Map, don't reference it\n- Use the exact directory names from \"Project Directory Structure\" — do NOT rename directories\n  (e.g., if the directory is called \"cli\", write \"src/cli/\", NOT \"src/commands/\")\n- Cross-module references must use the specifier format from actual import statements\n  (e.g., \"../generation/writers/sum.js\", NOT \"../fs/sum-file.js\")\n- If you are unsure about a path, omit the cross-reference rather than guessing\n\nCONSISTENCY (MANDATORY):\n- Do not contradict yourself within the same document\n- If you describe a technique (e.g., \"regex-based\"), do not call it something else later (e.g., \"AST-based\")\n- When stating version numbers, engines, or config fields, use ONLY values present in the file summaries\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this directory\", \"this module\", \"provides\", \"responsible for\", \"is used to\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Per-file descriptions in Contents sections: 1-2 sentences maximum. Reference key symbols but do not reproduce full summaries.\n- Behavioral contracts (regex patterns, format specs, constants) belong in a separate Behavioral Contracts section, not in per-file descriptions.\n- Annex file references: link to .annex.md files, do not inline their content. One line per annex reference.\n- Subdirectory descriptions: 1-2 sentences maximum. Capture the directory's role, not its full contents.\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- Key exported symbols from file summaries MUST appear in the directory overview\n- Preserve exact casing of identifiers\n\nUSER NOTES:\n- If \"User Notes\" are provided in the prompt, they contain user-defined instructions that will be automatically prepended to your output\n- Do NOT repeat or paraphrase user notes in your generated content — they are included separately\n- You may reference information from user notes for context",
      "response": "<!-- Generated by agents-reverse-engineer -->\n\n# change-detection\n\nGit-based change detection via simple-git diff parsing and SHA-256 content hashing for incremental documentation updates.\n\n## Contents\n\n**[detector.ts](./detector.ts)** — `isGitRepo()`, `getCurrentCommit()`, `getChangedFiles()` with git diff status parsing (A/M/D/R), `computeContentHash()` SHA-256 hashing, uncommitted change merge via `git.status()`.\n\n**[types.ts](./types.ts)** — `ChangeType`, `FileChange`, `ChangeDetectionResult`, `ChangeDetectionOptions` interfaces defining change detection contracts.\n\n**[index.ts](./index.ts)** — Barrel export aggregating detector functions and types.\n\n## Change Detection Algorithm\n\n`getChangedFiles()` executes `git diff --name-status -M <baseCommit>..HEAD` parsing output format `STATUS\\tFILE` or `STATUS\\tOLD\\tNEW`. Status code mapping: `'A'` → `'added'`, `'M'` → `'modified'`, `'D'` → `'deleted'`, `'R*'` → `'renamed'` with `oldPath` extracted from second tab-delimited field. Rename detection uses `-M` flag with 50% similarity threshold.\n\nWhen `ChangeDetectionOptions.includeUncommitted` is true, merges `git.status()` results reading `status.modified`, `status.deleted`, `status.not_added`, `status.staged` arrays. Deduplicates via `changes.some(c => c.path === file)` predicate to prevent duplicate `FileChange` entries for files in both committed diff and working tree.\n\n## Content Hashing\n\n`computeContentHash()` reads file via Node.js `readFile()`, computes SHA-256 via `createHash('sha256').update(content).digest('hex')`. Hash stored in `.sum` YAML frontmatter `content_hash` field. `computeContentHashFromString()` provides synchronous variant for pre-loaded content avoiding redundant disk reads.\n\n## Integration Points\n\nConsumed by `src/update/orchestrator.ts` to compute `filesToAnalyze` by comparing `computeContentHash()` results against `.sum` frontmatter `content_hash` values. Supports non-git workflows via `computeContentHash()` fallback when `isGitRepo()` returns false.\n\n## Behavioral Contracts\n\n**Git diff status codes:**\n- `A` — added file\n- `M` — modified file\n- `D` — deleted file\n- `R*` — renamed file with similarity score (e.g., `R100`)\n\n**Rename detection similarity threshold:** 50% (implicit via `git diff -M`)\n\n**SHA-256 hash format:** Hex-encoded string matching regex `/^[a-f0-9]{64}$/`",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 688,
      "cacheReadTokens": 17587,
      "cacheCreationTokens": 10677,
      "latencyMs": 29798,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": []
    },
    {
      "timestamp": "2026-02-09T17:47:47.762Z",
      "prompt": "Generate AGENTS.md for directory: \"src/integration\" (integration)\n\n## File Summaries (4 files)\n\n### detect.ts\n**Purpose:** detectEnvironments() and hasEnvironment() identify which AI coding assistant environments (Claude Code, OpenCode, Aid...\n\n**detectEnvironments() and hasEnvironment() identify which AI coding assistant environments (Claude Code, OpenCode, Aider) exist in a project by checking for their configuration directories and marker files.**\n\n## Exported Functions\n\n**detectEnvironments(projectRoot: string): DetectedEnvironment[]**\nReturns array of all detected AI assistant environments in the given project root. Checks for Claude Code (`.claude/` directory OR `CLAUDE.md` file), OpenCode (`.opencode/` directory), and Aider (`.aider.conf.yml` file OR `.aider/` directory). Returns `DetectedEnvironment[]` with `type`, `configDir`, and `detected: true` fields.\n\n**hasEnvironment(projectRoot: string, type: EnvironmentType): boolean**\nReturns true if the specified `EnvironmentType` ('claude' | 'opencode' | 'aider') is present in the project. Delegates to `detectEnvironments()` and filters results.\n\n## Detection Logic\n\ndetectEnvironments() uses `existsSync()` from `node:fs` to check for:\n- **Claude Code**: `path.join(projectRoot, '.claude')` OR `path.join(projectRoot, 'CLAUDE.md')`\n- **OpenCode**: `path.join(projectRoot, '.opencode')`\n- **Aider**: `path.join(projectRoot, '.aider.conf.yml')` OR `path.join(projectRoot, '.aider')`\n\nAll detected environments return `configDir` values: `'.claude'`, `'.opencode'`, or `'.aider'`.\n\n## Type Imports\n\nImports `DetectedEnvironment` and `EnvironmentType` from `./types.js`. `EnvironmentType` discriminates the `type` field returned in detection results.\n### generate.ts\n**Purpose:** generateIntegrationFiles orchestrates AI assistant integration file creation with environment-specific template selec...\n\n**generateIntegrationFiles orchestrates AI assistant integration file creation with environment-specific template selection, directory creation, skip-if-exists logic, and bundled hook deployment.**\n\n## Exported Functions\n\n**generateIntegrationFiles(projectRoot: string, options?: GenerateOptions): Promise<IntegrationResult[]>** — main orchestrator that detects or uses specified AI assistant environments, retrieves templates via `getTemplatesForEnvironment()`, writes files with `ensureDir()` + `writeFileSync()`, deploys bundled hooks for Claude via `readBundledHook()`, returns array of `IntegrationResult` with `filesCreated[]` and `filesSkipped[]` paths.\n\n**GenerateOptions** — configuration interface with `dryRun?: boolean` (preview mode without writes), `force?: boolean` (overwrite existing files), `environment?: EnvironmentType` (bypass auto-detection).\n\n## Integration Points\n\nImports `detectEnvironments()` from `./detect.js` for auto-discovery of `.claude/`, `.opencode/`, `.gemini/`, `.aider/` directories. Imports template getters `getClaudeTemplates()`, `getOpenCodeTemplates()`, `getGeminiTemplates()` from `./templates.js`. Uses `IntegrationResult` and `EnvironmentType` from `./types.js`.\n\n## Hook Deployment\n\n**getBundledHookPath(hookName: string): string** — resolves bundled hook location via `import.meta.url` + `fileURLToPath()`, navigates from `dist/integration/` up two levels to project root, then to `hooks/dist/${hookName}`.\n\n**readBundledHook(hookName: string): string** — reads bundled hook via `readFileSync()` with `utf-8` encoding, throws `Error` with message `\"Bundled hook not found: ${hookPath}\"` if `existsSync()` returns false.\n\nFor `env.type === 'claude'`, appends `.claude/hooks/are-session-end.js` to result by reading bundled `are-session-end.js` via `readBundledHook()`, writing with `ensureDir()` unless exists and `!force`.\n\n## File Creation Workflow\n\n**ensureDir(filePath: string): void** — extracts parent directory via `path.dirname()`, creates with `mkdirSync({ recursive: true })` if `!existsSync()`.\n\nFor each template, constructs `fullPath` via `path.join(projectRoot, template.path)`, checks `existsSync(fullPath) && !force` → `filesSkipped.push()`, otherwise `ensureDir()` + `writeFileSync(fullPath, template.content, 'utf-8')` + `filesCreated.push()` unless `dryRun`.\n\n## Environment Routing\n\n**getTemplatesForEnvironment(type: EnvironmentType)** — switch statement dispatching to `getClaudeTemplates()`, `getOpenCodeTemplates()`, `getGeminiTemplates()` for respective types, returns empty array `[]` for `'aider'` (no command files yet) and default case.\n\nWhen `options.environment` provided, bypasses `detectEnvironments()` and constructs single-element array `[{ type: specificEnv, configDir: configDirMap[specificEnv] }]` using literal map `{ claude: '.claude', opencode: '.opencode', aider: '.aider', gemini: '.gemini' }`.\n\n## Return Value Structure\n\nReturns `Promise<IntegrationResult[]>` with one element per environment, each containing `environment: EnvironmentType`, `filesCreated: string[]` (paths relative to projectRoot), `filesSkipped: string[]` (existing files not overwritten).\n### templates.ts\n**Purpose:** Exports getClaudeTemplates(), getOpenCodeTemplates(), and getGeminiTemplates() to generate platform-specific command ...\n\n**Exports getClaudeTemplates(), getOpenCodeTemplates(), and getGeminiTemplates() to generate platform-specific command integration templates for IDE installation, encapsulating COMMANDS object with prompt content, PLATFORM_CONFIGS with naming conventions, and buildTemplate()/buildGeminiToml() factories to produce IntegrationTemplate objects with filename/path/content.**\n\n## Exported Functions\n\n- `getClaudeTemplates(): IntegrationTemplate[]` — Returns array of Claude Code skill templates with `.claude/skills/are-{command}/SKILL.md` paths and `name:` frontmatter\n- `getOpenCodeTemplates(): IntegrationTemplate[]` — Returns array of OpenCode command templates with `.opencode/commands/are-{command}.md` paths and `agent: build` frontmatter\n- `getGeminiTemplates(): IntegrationTemplate[]` — Returns array of Gemini CLI command templates with `.gemini/commands/are-{command}.toml` paths and TOML `description`/`prompt` fields\n\n## Command Definitions\n\nThe `COMMANDS` constant defines seven command configurations: `generate`, `update`, `init`, `discover`, `clean`, `specify`, `help`. Each entry contains:\n\n- `description: string` — One-line command summary\n- `argumentHint: string` — CLI argument syntax (e.g., `'[path] [--dry-run] [--concurrency N]'`)\n- `content: string` — Multi-line markdown prompt with `<execution>` blocks, placeholder variables `COMMAND_PREFIX` and `VERSION_FILE_PATH`\n\n### Command Behavior Templates\n\n- **generate**: Background execution workflow with 15s poll intervals, reads `.agents-reverse-engineer/progress.log` using offset parameter, checks TaskOutput with `block: false`, summarizes three-phase pipeline (discovery → file analysis → directory/root docs)\n- **update**: Background execution identical to generate but reports files updated/unchanged/orphaned, includes `--uncommitted` flag for staged changes\n- **init**: Synchronous execution creates `.agents-reverse-engineer/config.yaml`\n- **discover**: Background execution with 10s poll intervals, reports file counts from discovery scan\n- **clean**: Synchronous execution with STRICT RULES comment block enforcing zero flag additions, reports deletion counts\n- **specify**: Background execution with 15s poll intervals, auto-runs generate if no AGENTS.md files exist, supports `--multi-file` and `--output` flags\n- **help**: Outputs comprehensive reference with `COMMAND_PREFIX` placeholder for per-platform command syntax (e.g., `/are-generate` vs `/are:generate`)\n\n## Platform Configuration\n\nThe `PLATFORM_CONFIGS` object maps three Platform types (`'claude' | 'opencode' | 'gemini'`) to PlatformConfig objects:\n\n- **claude**: `commandPrefix: '/are-'`, `pathPrefix: '.claude/skills/'`, `filenameSeparator: '.'`, `usesName: true`, `versionFilePath: '.claude/ARE-VERSION'`\n- **opencode**: `commandPrefix: '/are-'`, `pathPrefix: '.opencode/commands/'`, `filenameSeparator: '-'`, `extraFrontmatter: 'agent: build'`, `usesName: false`, `versionFilePath: '.opencode/ARE-VERSION'`\n- **gemini**: `commandPrefix: '/are-'`, `pathPrefix: '.gemini/commands/'`, `filenameSeparator: '-'`, `usesName: false`, `versionFilePath: '.gemini/ARE-VERSION'`\n\n## Template Generation Logic\n\n- `buildFrontmatter(platform, commandName, description): string` — Constructs YAML frontmatter with optional `name:` field (if usesName true) and optional extraFrontmatter string\n- `buildGeminiToml(commandName, command): string` — Generates TOML format with `description = \"...\"` and `prompt = \"\"\"...\"\"\"` triple-quoted multiline strings after replacing COMMAND_PREFIX and VERSION_FILE_PATH placeholders\n- `buildTemplate(platform, commandName, command): IntegrationTemplate` — Delegates to buildGeminiToml() for gemini platform, otherwise constructs markdown with frontmatter + content, performs placeholder substitution for COMMAND_PREFIX and VERSION_FILE_PATH\n- `getTemplatesForPlatform(platform): IntegrationTemplate[]` — Maps Object.entries(COMMANDS) through buildTemplate() for the specified platform\n\n## Path Naming Patterns\n\n- Claude: Nested directory structure `.claude/skills/are-{command}/SKILL.md` (e.g., `.claude/skills/are-generate/SKILL.md`)\n- OpenCode: Flat file structure `.opencode/commands/are-{command}.md` (e.g., `.opencode/commands/are-generate.md`)\n- Gemini: Flat TOML structure `.gemini/commands/are-{command}.toml` (e.g., `.gemini/commands/are-generate.toml`)\n\n## Template Content Patterns\n\nAll command content blocks include:\n- Version display instruction: `Read VERSION_FILE_PATH and show the user: agents-reverse-engineer vX.Y.Z`\n- Background execution via `npx agents-reverse-engineer@latest {command} $ARGUMENTS`\n- Progress polling loop: `sleep N`, Read tool with offset parameter, TaskOutput with `block: false`\n- Completion summary with metrics (files analyzed/updated/deleted, phase counts, inconsistency warnings)\n\nCommands with long execution times (generate, update, specify) use 15s poll intervals. Discover uses 10s intervals. Clean and init execute synchronously without polling.\n### types.ts\n**Purpose:** Defines TypeScript interfaces and discriminated union types for AI coding assistant environment detection (Claude Cod...\n\n**Defines TypeScript interfaces and discriminated union types for AI coding assistant environment detection (Claude Code, OpenCode, Aider, Gemini) and integration template generation.**\n\n## Exported Types\n\n### EnvironmentType\nUnion type `'claude' | 'opencode' | 'aider' | 'gemini'` representing supported AI assistant platforms.\n\n### DetectedEnvironment\nInterface describing environment detection results with properties:\n- `type: EnvironmentType` — Platform identifier\n- `configDir: string` — Configuration directory path (e.g., `.claude`, `.opencode`)\n- `detected: boolean` — Whether environment was found in project\n\n### IntegrationTemplate\nInterface defining template structure for command files and hooks with properties:\n- `filename: string` — File name (e.g., `generate.md`)\n- `path: string` — Relative path from project root (e.g., `.claude/commands/ar/generate.md`)\n- `content: string` — Template content to write\n\n### IntegrationResult\nInterface describing integration file generation outcome with properties:\n- `environment: EnvironmentType` — Platform that was configured\n- `filesCreated: string[]` — Successfully written file paths\n- `filesSkipped: string[]` — Paths skipped due to existing files\n\n## Integration with Module\n\nConsumed by `src/integration/detect.ts` for environment detection logic, `src/integration/generate.ts` for template instantiation, and `src/integration/templates.ts` for platform-specific content generation. Supports installer workflow in `src/installer/` for command/hook registration during `npx agents-reverse-engineer --runtime <env>` execution.\n\n## Project Directory Structure\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>",
      "systemPrompt": "You are generating an AGENTS.md file — a directory-level overview for AI coding assistants.\n\nCRITICAL: Output ONLY the raw markdown content. No code fences, no preamble, no explanations, no conversational text. Your entire response IS the AGENTS.md file content.\n\nOUTPUT FORMAT:\n- First line MUST be exactly: <!-- Generated by agents-reverse-engineer -->\n- Use a # heading with the directory name\n- Write a one-paragraph purpose statement for the directory\nusage\nADAPTIVE SECTIONS:\nAnalyze the directory contents and choose the most relevant sections. Do NOT use a fixed template. Instead, select sections that best document this specific directory for an AI that needs to reconstruct or extend the project.\n\nConsider these section types (choose what applies):\n- **Contents**: Group files by purpose/category under ## headings. For each file: markdown link [filename](./filename) and a one-line description.\n- **Subdirectories**: If subdirectories exist, list them with links [dirname/](./dirname/) and brief summaries.\n- **Architecture / Data Flow**: If files form a pipeline, request/response chain, or layered architecture, document it.\n- **Stack**: If this is a package root (has package.json, Cargo.toml, go.mod, etc.), document the technology stack, key scripts, and entry points.\n- **Structure**: If the directory layout follows a convention (feature-sliced, domain-driven, MVC, etc.), document it.\n- **Patterns**: If files share recurring design patterns (factory, strategy, middleware, barrel re-export), name and document them.\n- **Configuration**: If the directory contains config files, schemas, or environment definitions, document the config surface area.\n- **API Surface**: If the directory exports a public API (barrel index, route definitions, SDK), document the interface contract.\n- **File Relationships**: How files collaborate, depend on each other, or share state.\n- **Behavioral Contracts**: If files contain regex patterns, format specifications, magic constants, or template strings that define observable behavior, collect them in a dedicated section. Preserve verbatim patterns from file summaries — do NOT paraphrase regex into prose. This section is MANDATORY when file summaries contain behavioral artifacts.\n- **Reproduction-Critical Constants**: If file summaries reference annex files (via ## Annex References sections), list them with links. Example: \"Full prompt template text: [templates.ts.annex.md](./prompts/templates.ts.annex.md)\". Do NOT reproduce annex content in AGENTS.md — just link to it.\n\nChoose any relevant sections or create your own based on the directory contents. The goal is to provide a comprehensive overview that captures the essence of the directory's role in the project and how its files work together, with a focus on what an AI coding assistant would need to know to effectively interact with this code.\n\nSCOPE:\n- AGENTS.md is a NAVIGATIONAL INDEX — help an AI find the right file quickly\n- Focus on: what each file does, how files relate, directory-level patterns\n- Do NOT reproduce full architecture sections — those belong in the root CLAUDE.md\n\nPATH ACCURACY (MANDATORY):\n- When referencing files or modules outside this directory, use ONLY paths from the \"Import Map\" section\n- Do NOT invent, rename, or guess module paths — if a path isn't in the Import Map, don't reference it\n- Use the exact directory names from \"Project Directory Structure\" — do NOT rename directories\n  (e.g., if the directory is called \"cli\", write \"src/cli/\", NOT \"src/commands/\")\n- Cross-module references must use the specifier format from actual import statements\n  (e.g., \"../generation/writers/sum.js\", NOT \"../fs/sum-file.js\")\n- If you are unsure about a path, omit the cross-reference rather than guessing\n\nCONSISTENCY (MANDATORY):\n- Do not contradict yourself within the same document\n- If you describe a technique (e.g., \"regex-based\"), do not call it something else later (e.g., \"AST-based\")\n- When stating version numbers, engines, or config fields, use ONLY values present in the file summaries\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this directory\", \"this module\", \"provides\", \"responsible for\", \"is used to\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Per-file descriptions in Contents sections: 1-2 sentences maximum. Reference key symbols but do not reproduce full summaries.\n- Behavioral contracts (regex patterns, format specs, constants) belong in a separate Behavioral Contracts section, not in per-file descriptions.\n- Annex file references: link to .annex.md files, do not inline their content. One line per annex reference.\n- Subdirectory descriptions: 1-2 sentences maximum. Capture the directory's role, not its full contents.\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- Key exported symbols from file summaries MUST appear in the directory overview\n- Preserve exact casing of identifiers\n\nUSER NOTES:\n- If \"User Notes\" are provided in the prompt, they contain user-defined instructions that will be automatically prepended to your output\n- Do NOT repeat or paraphrase user notes in your generated content — they are included separately\n- You may reference information from user notes for context",
      "response": "<!-- Generated by agents-reverse-engineer -->\n\n# src/integration\n\n**Platform-specific AI assistant integration layer detecting Claude Code/OpenCode/Gemini/Aider environments, generating command template files with progress-monitoring patterns, deploying bundled session hooks, and enforcing skip-if-exists safety with force override.**\n\n## Contents\n\n### Environment Detection\n\n**[detect.ts](./detect.ts)** — `detectEnvironments()` scans project root for `.claude/`, `.opencode/`, `.aider/`, `.gemini/` directories and `CLAUDE.md`/`.aider.conf.yml` marker files, returns `DetectedEnvironment[]` with `type`, `configDir`, `detected` fields. `hasEnvironment()` filters by `EnvironmentType`.\n\n### File Generation Orchestration\n\n**[generate.ts](./generate.ts)** — `generateIntegrationFiles()` orchestrates template instantiation by detecting/overriding environments via `detectEnvironments()`, retrieving templates via `getTemplatesForEnvironment()`, writing command files with `ensureDir()` + `writeFileSync()`, deploying bundled hooks for Claude via `readBundledHook()` from `hooks/dist/are-session-end.js`, returns `IntegrationResult[]` with `filesCreated`/`filesSkipped` arrays. `GenerateOptions` supports `dryRun`, `force` (overwrite existing), and `environment` (bypass auto-detection).\n\n### Template Library\n\n**[templates.ts](./templates.ts)** — Exports `getClaudeTemplates()`, `getOpenCodeTemplates()`, `getGeminiTemplates()` generating platform-specific command files for seven commands: `generate`, `update`, `init`, `discover`, `clean`, `specify`, `help`. Encapsulates `COMMANDS` object with markdown prompt content, `PLATFORM_CONFIGS` mapping `EnvironmentType` to `PlatformConfig` (command prefix, path structure, frontmatter rules), `buildTemplate()`/`buildGeminiToml()` factories performing placeholder substitution (`COMMAND_PREFIX`, `VERSION_FILE_PATH`). Command content patterns: background execution via `npx agents-reverse-engineer@latest`, progress polling with `.agents-reverse-engineer/progress.log` offset reads, `TaskOutput` checks with `block: false`, completion summaries with phase metrics.\n\n### Type Definitions\n\n**[types.ts](./types.ts)** — `EnvironmentType` union (`'claude' | 'opencode' | 'aider' | 'gemini'`), `DetectedEnvironment` interface (`type`, `configDir`, `detected`), `IntegrationTemplate` interface (`filename`, `path`, `content`), `IntegrationResult` interface (`environment`, `filesCreated`, `filesSkipped`).\n\n## Platform Configuration\n\n| Platform | Command Prefix | Path Pattern | Frontmatter | Version File |\n|----------|---------------|--------------|-------------|--------------|\n| **Claude** | `/are-` | `.claude/skills/are-{command}/SKILL.md` | `name: /are-{command}` | `.claude/ARE-VERSION` |\n| **OpenCode** | `/are-` | `.opencode/commands/are-{command}.md` | `agent: build` | `.opencode/ARE-VERSION` |\n| **Gemini** | `/are-` | `.gemini/commands/are-{command}.toml` | TOML `description`/`prompt` | `.gemini/ARE-VERSION` |\n| **Aider** | N/A | N/A (detection only) | N/A | N/A |\n\n## Command Template Behavior\n\n- **generate/update/specify**: 15s poll intervals, background execution via `run_in_background: true`, offset-based log tailing, three-phase progress reporting (discovery → file analysis → directory/root docs)\n- **discover**: 10s poll intervals, background execution, file count reporting\n- **init**: Synchronous execution, creates `.agents-reverse-engineer/config.yaml`\n- **clean**: Synchronous execution with STRICT RULES enforcing zero flag additions, deletion count reporting\n- **help**: Outputs command reference with platform-specific `COMMAND_PREFIX` placeholder substitution\n\n## Integration with Project\n\nConsumed by `src/installer/` for `npx agents-reverse-engineer --runtime <env>` workflow. `detectEnvironments()` validates presence before installation, `generateIntegrationFiles()` writes command files to `.claude/skills/`, `.opencode/commands/`, or `.gemini/commands/`. Bundled hook deployment via `hooks/dist/are-session-end.js` for Claude's SessionEnd lifecycle.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 1096,
      "cacheReadTokens": 17587,
      "cacheCreationTokens": 12114,
      "latencyMs": 40174,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": []
    },
    {
      "timestamp": "2026-02-09T17:47:47.747Z",
      "prompt": "Generate AGENTS.md for directory: \"src/discovery\" (discovery)\n\n## File Summaries (3 files)\n\n### run.ts\n**Purpose:** discoverFiles orchestrates the four-stage filter chain over walkDirectory results to produce FilterResult with includ...\n\n**discoverFiles orchestrates the four-stage filter chain over walkDirectory results to produce FilterResult with included/excluded file lists and filter attribution.**\n\n## Exported Functions\n\n**discoverFiles(root: string, config: DiscoveryConfig, options?: DiscoverFilesOptions): Promise<FilterResult>** — Creates gitignore, vendor, binary, and custom filters in order via createGitignoreFilter, createVendorFilter, createBinaryFilter, createCustomFilter; invokes walkDirectory with cwd=root and followSymlinks flag; applies filters via applyFilters with optional tracer/debug; returns FilterResult containing included/excluded arrays.\n\n## Type Definitions\n\n**DiscoveryConfig** — Structural subset interface with exclude: {vendorDirs: string[], binaryExtensions: string[], patterns: string[]} and options: {maxFileSize: number, followSymlinks: boolean}. Compatible with full Config from src/config/schema.ts for dependency inversion.\n\n**DiscoverFilesOptions** — Optional interface with tracer?: ITraceWriter and debug?: boolean for trace emission and verbose logging control.\n\n## Filter Chain Composition\n\nCreates four filters in fixed order before applying to walked files:\n\n1. **gitignoreFilter** — Async createGitignoreFilter(root) parses .gitignore files\n2. **vendorFilter** — createVendorFilter(config.exclude.vendorDirs) excludes node_modules/.git/dist\n3. **binaryFilter** — createBinaryFilter({maxFileSize, additionalExtensions}) detects non-text files\n4. **customFilter** — createCustomFilter(config.exclude.patterns, root) applies user glob patterns\n\nFilter array passed to applyFilters with tracer/debug options threaded through.\n\n## Integration Points\n\nImported by src/cli/discover.ts, src/cli/generate.ts, src/cli/update.ts as single entry point for file discovery. Decouples command layer from walker/filter implementation details. DiscoveryConfig interface allows commands to pass full Config object without circular dependency on src/config/schema.ts.\n\n## Dependencies\n\n- `./walker.js` — walkDirectory function\n- `./filters/index.js` — Filter factory functions and applyFilters orchestrator\n- `./types.js` — FilterResult type definition\n- `../orchestration/trace.js` — ITraceWriter interface for trace emission\n### types.ts\n**Purpose:** Defines core TypeScript interfaces for the file discovery pipeline: FileFilter for composable exclusion predicates, F...\n\n**Defines core TypeScript interfaces for the file discovery pipeline: FileFilter for composable exclusion predicates, FilterResult for discovery outcomes, WalkerOptions for traversal configuration, and ExcludedFile for audit trails.**\n\n## Exported Interfaces\n\n**FileFilter** — Contract for composable file exclusion predicates in the discovery filter chain. Exposes `name: string` for audit logging and `shouldExclude(path: string, stats?: Stats): Promise<boolean> | boolean` for synchronous or asynchronous exclusion decisions. Implemented by GitignoreFilter, BinaryFilter, VendorFilter, CustomPatternFilter.\n\n**ExcludedFile** — Record structure for excluded files with `path: string` (absolute path), `reason: string` (human-readable exclusion explanation), `filter: string` (name of the FileFilter that triggered exclusion).\n\n**FilterResult** — Discriminated discovery outcome containing `included: string[]` (absolute paths passing all filters) and `excluded: ExcludedFile[]` (rejected files with exclusion metadata).\n\n**WalkerOptions** — Configuration for directory traversal with `cwd: string` (absolute root path), `followSymlinks?: boolean` (default false per CONTEXT.md), `dot?: boolean` (include dotfiles, default true).\n\n## Integration Points\n\nConsumed by `src/discovery/walker.ts` for traversal configuration and `src/discovery/filters/` implementations (binary.ts, gitignore.ts, vendor.ts, custom.ts) for FileFilter contract adherence. FilterResult returned by `src/discovery/run.ts` aggregates results across the filter chain. Stats type imported from `node:fs` enables size-based filtering (see BinaryFilter with maxFileSize threshold).\n### walker.ts\n**Purpose:** walkDirectory() wraps fast-glob to return all file paths in a directory tree, delegating filtering to the separate fi...\n\n**walkDirectory() wraps fast-glob to return all file paths in a directory tree, delegating filtering to the separate filter chain.**\n\n## Exported Function\n\n```typescript\nasync function walkDirectory(options: WalkerOptions): Promise<string[]>\n```\n\nReturns array of absolute file paths from `fg.glob('**/*', {...})` invocation. Accepts `WalkerOptions` with fields `cwd` (required base directory), `dot` (include dotfiles, defaults true), and `followSymlinks` (defaults false).\n\n## Glob Configuration\n\nPasses these options to `fg.glob()`:\n- `absolute: true` — returns full paths, not relative\n- `onlyFiles: true` — excludes directories from results\n- `dot: options.dot ?? true` — includes dotfiles by default\n- `followSymbolicLinks: options.followSymlinks ?? false` — does not traverse symlinks by default\n- `suppressErrors: true` — silently skips permission denied errors per `RESEARCH.md` directive\n- `ignore: ['**/.git/**']` — hardcoded exclusion for `.git` internals to improve performance\n\n## Filter Chain Decoupling\n\nDoes NOT apply gitignore, binary, vendor, or custom pattern filters. Module comment explicitly states \"Filters are applied separately via the filter chain (not in this module).\" The `src/discovery/filters/` directory contains the actual filter implementations (binary.ts, custom.ts, gitignore.ts, vendor.ts).\n\n## Dependencies\n\nImports `fg` from `fast-glob` (glob engine) and `WalkerOptions` from `./types.js` (type-only import).\n\n## Import Map (verified — use these exact paths)\n\nrun.ts:\n  ../orchestration/trace.js → ITraceWriter (type)\n\n## Project Directory Structure\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\n\n## Subdirectories\n\n### filters/\n<!-- Generated by agents-reverse-engineer -->\n\n# src/discovery/filters\n\n**Five filter implementations for file discovery exclusion: gitignore pattern matching, vendor directory detection, binary file detection via extension/content analysis, custom glob patterns, and bounded-concurrency filter chain orchestration with per-filter statistics tracking.**\n\n## Contents\n\n### Filter Implementations\n\n**[binary.ts](./binary.ts)** — `createBinaryFilter()` implements two-phase binary detection: extension-based fast path against `BINARY_EXTENSIONS` set (80+ extensions across images/archives/executables/media/documents/fonts/compiled/database), size threshold check against `maxFileSize` (default 1MB), content analysis fallback via `isBinaryFile()` for unknown extensions.\n\n**[custom.ts](./custom.ts)** — `createCustomFilter()` matches absolute paths against user-provided gitignore-style patterns via `ignore` library with relative path normalization, returns pass-through filter when pattern array empty.\n\n**[gitignore.ts](./gitignore.ts)** — `createGitignoreFilter()` async factory reads `.gitignore` from project root, delegates exclusion to `ignore` library instance with relative path conversion, silently passes all files when `.gitignore` missing.\n\n**[vendor.ts](./vendor.ts)** — `createVendorFilter()` excludes third-party dependency directories via two-tier matching: single-segment patterns (`'node_modules'`) matched against all path segments, multi-segment patterns (`'apps/vendor'`) matched via substring inclusion with OS-specific separator normalization.\n\n**[index.ts](./index.ts)** — `applyFilters()` orchestrates bounded-concurrency filter chain execution (30 workers sharing iterator), short-circuits on first exclusion, tracks per-filter `matched`/`rejected` statistics, emits `filter:applied` trace events, re-exports all filter creators and constants.\n\n## Filter Chain Architecture\n\n**Execution Model**: Worker pool pattern with `CONCURRENCY=30` limit prevents file descriptor exhaustion during I/O-heavy binary detection. Shared `files.entries()` iterator across workers via `for (const [index, file] of iter)` loop. Sequential filter evaluation per file with early termination on first match.\n\n**Statistics Tracking**: `Map<string, { matched: number; rejected: number }>` initialized for each filter before processing. `rejected` increments when filter excludes file. `matched` increments for all filters when file passes entire chain.\n\n**Result Preservation**: Workers collect `Array<{ index: number; file: string; excluded?: ExcludedFile }>`, sort by original `index` to maintain input order, segregate into `included`/`excluded` arrays for `FilterResult` return value.\n\n## Behavioral Contracts\n\n### Binary Extensions Set (binary.ts)\n\n`BINARY_EXTENSIONS` contains 80+ extensions organized by category:\n- **Images**: `.png`, `.jpg`, `.jpeg`, `.gif`, `.bmp`, `.ico`, `.webp`, `.svg`, `.tiff`, `.tif`, `.psd`, `.raw`, `.heif`, `.heic`\n- **Archives**: `.zip`, `.tar`, `.gz`, `.rar`, `.7z`, `.bz2`, `.xz`, `.tgz`\n- **Executables**: `.exe`, `.dll`, `.so`, `.dylib`, `.bin`, `.msi`, `.app`, `.dmg`\n- **Media**: `.mp3`, `.mp4`, `.wav`, `.avi`, `.mov`, `.mkv`, `.flac`, `.ogg`, `.webm`, `.m4a`, `.aac`, `.wma`, `.wmv`, `.flv`\n- **Documents**: `.pdf`, `.doc`, `.docx`, `.xls`, `.xlsx`, `.ppt`, `.pptx`, `.odt`, `.ods`, `.odp`\n- **Fonts**: `.woff`, `.woff2`, `.ttf`, `.eot`, `.otf`\n- **Compiled/bytecode**: `.class`, `.pyc`, `.pyo`, `.o`, `.obj`, `.a`, `.lib`, `.wasm`\n- **Database**: `.db`, `.sqlite`, `.sqlite3`, `.mdb`\n\n### Default Vendor Directories (vendor.ts)\n\n`DEFAULT_VENDOR_DIRS`: `['node_modules', 'vendor', '.git', 'dist', 'build', '__pycache__', '.next', 'venv', '.venv', 'target']`\n\n### Concurrency Limit (index.ts)\n\n`CONCURRENCY = 30` workers — hard limit to prevent file descriptor exhaustion during binary file detection I/O operations.\n\n### Binary Size Threshold (binary.ts)\n\n`DEFAULT_MAX_FILE_SIZE = 1048576` (1MB) — files exceeding threshold excluded without content analysis.\n\n## Path Normalization Patterns\n\n**gitignore.ts**: `path.relative(normalizedRoot, absolutePath)` → guard against `'..'` prefix or empty string before `ig.ignores()` call.\n\n**custom.ts**: `path.relative(normalizedRoot, absolutePath)` → guard against `'..'` prefix or empty string before `ig.ignores()` call.\n\n**vendor.ts**: Multi-segment patterns normalized via `dir.replace(/[\\\\/]/g, path.sep)` for cross-platform separator handling before `absolutePath.includes(pattern)` check.\n\n**binary.ts**: Extension extraction via `path.extname(absolutePath).toLowerCase()`, additional extensions normalized via `ext.startsWith('.') ? ext : \\`.\\${ext}\\`` before merging with `BINARY_EXTENSIONS`.\n\n## Filter Interface Contract\n\nAll filter creators return `FileFilter` object:\n```typescript\n{\n  name: string;              // Discriminator: 'gitignore' | 'vendor' | 'binary' | 'custom'\n  shouldExclude(absolutePath: string): boolean | Promise<boolean>\n}\n```\n\n**Synchronous filters**: custom.ts, vendor.ts, binary.ts (async internally but return sync `shouldExclude`).\n\n**Async factory**: gitignore.ts returns `Promise<FileFilter>` due to `.gitignore` file read.\n\n## Integration Points\n\n**Consumed by**: `src/discovery/walker.ts` composes filters into chain via `createGitignoreFilter()`, `createVendorFilter()`, `createBinaryFilter()`, `createCustomFilter()`, then calls `applyFilters()`.\n\n**Configuration surface**: `BinaryFilterOptions` with `maxFileSize`/`additionalExtensions` threaded from `src/config/schema.ts` YAML config (`exclude.binaryExtensions`, `options.maxFileSize`).\n\n**Telemetry integration**: Accepts optional `ITraceWriter` via `applyFilters()` options, emits `filter:applied` events with per-filter metrics (`filterName`, `filesMatched`, `filesRejected`).\n\n**Debug output**: `options.debug` flag enables `console.error(pc.dim(...))` logging for filters with `rejected > 0` count.",
      "systemPrompt": "You are generating an AGENTS.md file — a directory-level overview for AI coding assistants.\n\nCRITICAL: Output ONLY the raw markdown content. No code fences, no preamble, no explanations, no conversational text. Your entire response IS the AGENTS.md file content.\n\nOUTPUT FORMAT:\n- First line MUST be exactly: <!-- Generated by agents-reverse-engineer -->\n- Use a # heading with the directory name\n- Write a one-paragraph purpose statement for the directory\nusage\nADAPTIVE SECTIONS:\nAnalyze the directory contents and choose the most relevant sections. Do NOT use a fixed template. Instead, select sections that best document this specific directory for an AI that needs to reconstruct or extend the project.\n\nConsider these section types (choose what applies):\n- **Contents**: Group files by purpose/category under ## headings. For each file: markdown link [filename](./filename) and a one-line description.\n- **Subdirectories**: If subdirectories exist, list them with links [dirname/](./dirname/) and brief summaries.\n- **Architecture / Data Flow**: If files form a pipeline, request/response chain, or layered architecture, document it.\n- **Stack**: If this is a package root (has package.json, Cargo.toml, go.mod, etc.), document the technology stack, key scripts, and entry points.\n- **Structure**: If the directory layout follows a convention (feature-sliced, domain-driven, MVC, etc.), document it.\n- **Patterns**: If files share recurring design patterns (factory, strategy, middleware, barrel re-export), name and document them.\n- **Configuration**: If the directory contains config files, schemas, or environment definitions, document the config surface area.\n- **API Surface**: If the directory exports a public API (barrel index, route definitions, SDK), document the interface contract.\n- **File Relationships**: How files collaborate, depend on each other, or share state.\n- **Behavioral Contracts**: If files contain regex patterns, format specifications, magic constants, or template strings that define observable behavior, collect them in a dedicated section. Preserve verbatim patterns from file summaries — do NOT paraphrase regex into prose. This section is MANDATORY when file summaries contain behavioral artifacts.\n- **Reproduction-Critical Constants**: If file summaries reference annex files (via ## Annex References sections), list them with links. Example: \"Full prompt template text: [templates.ts.annex.md](./prompts/templates.ts.annex.md)\". Do NOT reproduce annex content in AGENTS.md — just link to it.\n\nChoose any relevant sections or create your own based on the directory contents. The goal is to provide a comprehensive overview that captures the essence of the directory's role in the project and how its files work together, with a focus on what an AI coding assistant would need to know to effectively interact with this code.\n\nSCOPE:\n- AGENTS.md is a NAVIGATIONAL INDEX — help an AI find the right file quickly\n- Focus on: what each file does, how files relate, directory-level patterns\n- Do NOT reproduce full architecture sections — those belong in the root CLAUDE.md\n\nPATH ACCURACY (MANDATORY):\n- When referencing files or modules outside this directory, use ONLY paths from the \"Import Map\" section\n- Do NOT invent, rename, or guess module paths — if a path isn't in the Import Map, don't reference it\n- Use the exact directory names from \"Project Directory Structure\" — do NOT rename directories\n  (e.g., if the directory is called \"cli\", write \"src/cli/\", NOT \"src/commands/\")\n- Cross-module references must use the specifier format from actual import statements\n  (e.g., \"../generation/writers/sum.js\", NOT \"../fs/sum-file.js\")\n- If you are unsure about a path, omit the cross-reference rather than guessing\n\nCONSISTENCY (MANDATORY):\n- Do not contradict yourself within the same document\n- If you describe a technique (e.g., \"regex-based\"), do not call it something else later (e.g., \"AST-based\")\n- When stating version numbers, engines, or config fields, use ONLY values present in the file summaries\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this directory\", \"this module\", \"provides\", \"responsible for\", \"is used to\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Per-file descriptions in Contents sections: 1-2 sentences maximum. Reference key symbols but do not reproduce full summaries.\n- Behavioral contracts (regex patterns, format specs, constants) belong in a separate Behavioral Contracts section, not in per-file descriptions.\n- Annex file references: link to .annex.md files, do not inline their content. One line per annex reference.\n- Subdirectory descriptions: 1-2 sentences maximum. Capture the directory's role, not its full contents.\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- Key exported symbols from file summaries MUST appear in the directory overview\n- Preserve exact casing of identifiers\n\nUSER NOTES:\n- If \"User Notes\" are provided in the prompt, they contain user-defined instructions that will be automatically prepended to your output\n- Do NOT repeat or paraphrase user notes in your generated content — they are included separately\n- You may reference information from user notes for context",
      "response": "<!-- Generated by agents-reverse-engineer -->\n\n# src/discovery\n\n**Gitignore-aware file discovery system executing four-stage filter chain (gitignore, vendor, binary, custom) over fast-glob traversal results with bounded-concurrency processing (30 workers), early-termination optimization, and per-filter telemetry tracking.**\n\n## Contents\n\n**[run.ts](./run.ts)** — `discoverFiles()` orchestrates filter chain construction via `createGitignoreFilter()`, `createVendorFilter()`, `createBinaryFilter()`, `createCustomFilter()`, invokes `walkDirectory()` with `followSymlinks` flag, delegates filtering to `applyFilters()` with trace/debug options.\n\n**[types.ts](./types.ts)** — Defines `FileFilter` interface (`name: string`, `shouldExclude(): boolean|Promise<boolean>`), `FilterResult` with `included: string[]` and `excluded: ExcludedFile[]`, `WalkerOptions` for traversal config, `ExcludedFile` audit record.\n\n**[walker.ts](./walker.ts)** — `walkDirectory()` wraps `fast-glob` with `absolute: true`, `onlyFiles: true`, `dot: true` (dotfiles), `followSymbolicLinks: false` (default), `ignore: ['**/.git/**']` hardcoded, `suppressErrors: true` (permission denied).\n\n## Architecture\n\n### Filter Chain Execution Model\n\n**Composition**: `discoverFiles()` creates four filters in fixed order before applying to walked files. No in-walker filtering per module comment in `walker.ts`.\n\n**Orchestration**: `applyFilters()` (in `filters/index.ts`) spawns 30 workers sharing `files.entries()` iterator. Sequential filter evaluation per file with early termination on first `shouldExclude() === true`.\n\n**Result Preservation**: Workers collect `{ index, file, excluded? }` tuples, sort by original index to maintain discovery order, segregate into `included`/`excluded` arrays.\n\n### Filter Order\n\n1. **Gitignore** — Async `.gitignore` parser via `ignore` library with relative path normalization\n2. **Vendor** — Third-party directories (`node_modules`, `.git`, `dist`, `build`, `__pycache__`, `.next`, `venv`, `.venv`, `target`)\n3. **Binary** — Extension fast path (80+ extensions) → size threshold (1MB) → `isBinaryFile()` content analysis\n4. **Custom** — User glob patterns from `config.exclude.patterns` via `ignore` library\n\n### Decoupling Patterns\n\n**DiscoveryConfig Interface**: Structural subset type with `exclude: {vendorDirs, binaryExtensions, patterns}` and `options: {maxFileSize, followSymlinks}` allows `run.ts` to accept full `Config` object without circular dependency on `src/config/schema.ts`.\n\n**Factory Abstraction**: Filter creators (`createGitignoreFilter`, `createVendorFilter`, `createBinaryFilter`, `createCustomFilter`) return uniform `FileFilter` interface enabling chain composition.\n\n**Statistics Aggregation**: `applyFilters()` returns `Map<string, {matched, rejected}>` keyed by filter name for telemetry without coupling to filter implementations.\n\n## Behavioral Contracts\n\n### Binary Detection Thresholds\n\n- **Extension fast path**: 80+ extensions across images/archives/executables/media/documents/fonts/compiled/database\n- **Size threshold**: `DEFAULT_MAX_FILE_SIZE = 1048576` (1MB)\n- **Content analysis**: `isBinaryFile()` fallback for unknown extensions\n\n### Concurrency Limit\n\n`CONCURRENCY = 30` workers in `applyFilters()` prevents file descriptor exhaustion during I/O-heavy binary detection.\n\n### Vendor Directories\n\n`DEFAULT_VENDOR_DIRS`: `['node_modules', 'vendor', '.git', 'dist', 'build', '__pycache__', '.next', 'venv', '.venv', 'target']`\n\n### Glob Configuration\n\n- `absolute: true` — returns full paths\n- `onlyFiles: true` — excludes directories\n- `dot: true` — includes dotfiles\n- `followSymbolicLinks: false` — default symlink handling\n- `ignore: ['**/.git/**']` — hardcoded `.git` exclusion\n\n### Path Normalization\n\nAll filters use `path.relative(normalizedRoot, absolutePath)` before exclusion tests, with guards against `'..'` prefix or empty string.\n\n## Subdirectories\n\n**[filters/](./filters/)** — Five filter implementations: `gitignore.ts` (pattern matching via `ignore` library), `vendor.ts` (third-party directory detection), `binary.ts` (extension/size/content analysis), `custom.ts` (user glob patterns), `index.ts` (bounded-concurrency orchestrator with telemetry).\n\n## Integration Points\n\n**Consumed by**: `src/cli/discover.ts`, `src/cli/generate.ts`, `src/cli/update.ts` invoke `discoverFiles()` as single entry point.\n\n**Configuration surface**: `DiscoveryConfig` threaded from `src/config/schema.ts` YAML parsing with fields `exclude.vendorDirs`, `exclude.binaryExtensions`, `exclude.patterns`, `options.maxFileSize`, `options.followSymlinks`.\n\n**Telemetry integration**: Accepts `ITraceWriter` via `DiscoverFilesOptions.tracer`, emits `filter:applied` events with per-filter `filesMatched`/`filesRejected` counts.\n\n**Debug output**: `options.debug` enables `console.error()` logging for filters with non-zero rejection counts.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 1363,
      "cacheReadTokens": 17587,
      "cacheCreationTokens": 12109,
      "latencyMs": 41347,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": []
    },
    {
      "timestamp": "2026-02-09T17:47:47.712Z",
      "prompt": "Generate AGENTS.md for directory: \"src/quality\" (quality)\n\n## File Summaries (2 files)\n\n### index.ts\n**Purpose:** Public API barrel module for quality analysis system, re-exporting inconsistency detection validators (`extractExport...\n\n**Public API barrel module for quality analysis system, re-exporting inconsistency detection validators (`extractExports`, `checkCodeVsDoc`, `checkCodeVsCode`, `checkPhantomPaths`), density validation (`validateFindability`), reporting utilities (`buildInconsistencyReport`, `formatReportForCli`), and all shared types (`Inconsistency`, `InconsistencyReport`, `FindabilityResult`) from submodules.**\n\n## Exported Types\n\n- `InconsistencySeverity` — Severity level enumeration from `./types.js`\n- `CodeDocInconsistency` — Code-vs-documentation mismatch structure with `missingFromDoc` array tracking exported symbols absent from `.sum` summary text\n- `CodeCodeInconsistency` — Code-vs-code duplicate symbol structure detecting symbols appearing in multiple files within same directory\n- `PhantomPathInconsistency` — Unresolved path reference structure capturing file paths mentioned in `AGENTS.md` that fail `existsSync()` validation\n- `Inconsistency` — Discriminated union of all inconsistency types\n- `InconsistencyReport` — Aggregated validation report with `metadata` (timestamp, projectRoot, filesChecked, durationMs), `issues[]` array, and `summary` counts by type/severity\n- `FindabilityResult` — Density validation result structure from `./density/validator.js`\n\n## Exported Functions\n\n- `extractExports(filePath: string): string[]` — Regex-based export extraction using pattern `/^[ \\t]*export\\s+(?:default\\s+)?(?:function|class|const|let|var|type|interface|enum)\\s+(\\w+)/gm` from `./inconsistency/code-vs-doc.js`\n- `checkCodeVsDoc(filePath: string, sumContent: string): CodeDocInconsistency | null` — Verifies all extracted exports appear in summary via substring search, returns inconsistency object with `missingFromDoc` array or null if all exports documented\n- `checkCodeVsCode(files: string[], baseDir: string): CodeCodeInconsistency[]` — Aggregates exports across file groups into `Map<symbol, string[]>`, reports duplicate symbols with pattern `'duplicate-export'` from `./inconsistency/code-vs-code.js`\n- `buildInconsistencyReport(issues: Inconsistency[], metadata: object): InconsistencyReport` — Constructs aggregated report with summary counts from `./inconsistency/reporter.js`\n- `formatReportForCli(report: InconsistencyReport): string` — Formats inconsistency report for terminal output with picocolors formatting from `./inconsistency/reporter.js`\n- `checkPhantomPaths(agentsMdPath: string, projectRoot: string): PhantomPathInconsistency[]` — Extracts path-like strings via three regex patterns (markdown links, backtick-quoted paths, prose-embedded paths), resolves against AGENTS.md directory and project root with `.ts`/`.js` fallback, returns array of unresolved references from `./phantom-paths/index.js`\n- `validateFindability(sumFilePath: string): FindabilityResult` — Density validation function from `./density/validator.js` (currently disabled after structured `publicInterface` removal from `SumFileContent` schema per CLAUDE.md Known Limitations)\n\n## Module Organization\n\nIndex module delegates implementation to five submodules organized by validation concern:\n1. `./types.js` — Shared type definitions for all validators\n2. `./inconsistency/code-vs-doc.js` — Export extraction and summary verification\n3. `./inconsistency/code-vs-code.js` — Duplicate symbol detection across directory files\n4. `./inconsistency/reporter.js` — Report aggregation and CLI formatting\n5. `./phantom-paths/index.js` — Path resolution validation for documentation references\n6. `./density/validator.js` — Findability validation (feature disabled)\n\n## Integration Points\n\nConsumed by `src/generation/orchestrator.ts` post-generation validation phase and `src/cli/generate.ts` for terminal reporting. Report format matches telemetry schema in `src/ai/telemetry/run-log.ts` for `qualityMetrics` aggregation. All validators operate on generated artifacts (`.sum`, `AGENTS.md`) rather than source code, enforcing post-generation validation contract.\n### types.ts\n**Purpose:** types.ts defines the discriminated union of quality inconsistency types and structured report format for code-documen...\n\n**types.ts defines the discriminated union of quality inconsistency types and structured report format for code-documentation validation.**\n\n## Exported Types\n\n**InconsistencySeverity** — String literal union `'info' | 'warning' | 'error'` classifying issue priority.\n\n**CodeDocInconsistency** — Inconsistency between `.sum` documentation and source code exports detected via symbol extraction and substring search. Fields: `type: 'code-vs-doc'`, `severity`, `filePath` (source), `sumPath` (.sum file), `description`, `details.missingFromDoc` (exported symbols absent from .sum), `details.missingFromCode` (symbols in .sum not found in source), `details.purposeMismatch` (optional purpose statement contradiction).\n\n**CodeCodeInconsistency** — Cross-file inconsistency detected by pattern analysis across multiple source files. Fields: `type: 'code-vs-code'`, `severity`, `files` (conflicting paths), `description`, `pattern` (e.g., `'duplicate-export'` for symbols appearing in multiple files).\n\n**PhantomPathInconsistency** — Unresolvable path reference found in `AGENTS.md` via regex extraction and `existsSync()` validation. Fields: `type: 'phantom-path'`, `severity`, `agentsMdPath` (containing document), `description`, `details.referencedPath` (original string from markdown), `details.resolvedTo` (attempted resolution target), `details.context` (surrounding text line).\n\n**Inconsistency** — Discriminated union `CodeDocInconsistency | CodeCodeInconsistency | PhantomPathInconsistency` with `type` discriminator enabling exhaustive pattern matching.\n\n**InconsistencyReport** — Structured quality validation output. Fields: `metadata` containing `timestamp`, `projectRoot`, `filesChecked`, `durationMs`; `issues[]` array of `Inconsistency` instances; `summary` object with `total`, `codeVsDoc`, `codeVsCode`, `phantomPaths`, `errors`, `warnings`, `info` counts.\n\n## Integration Context\n\nConsumed by `src/quality/inconsistency/code-vs-doc.ts` (export extraction + substring validation), `src/quality/inconsistency/code-vs-code.ts` (duplicate symbol detection), `src/quality/phantom-paths/validator.ts` (markdown path resolution), and `src/quality/inconsistency/reporter.ts` (aggregation and summary computation). The `type` discriminator enables type-safe narrowing in switch statements for severity assignment and detail extraction.\n\n## Project Directory Structure\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\n\n## Subdirectories\n\n### density/\n<!-- Generated by agents-reverse-engineer -->\n\n# src/quality/density\n\nStub validation module for findability analysis (symbol presence in aggregated AGENTS.md), disabled after `SumFileContent.metadata.publicInterface` removal pending structured extraction re-implementation.\n\n## Contents\n\n**[validator.ts](./validator.ts)** — Exports `validateFindability(_agentsMdContent, _sumFiles)` returning empty `FindabilityResult[]` array; previously verified exported symbols from `.sum` files appeared in parent `AGENTS.md` via string matching, now disabled awaiting post-processing extraction support.\n\n## Implementation Status\n\n`validateFindability()` signature preserved for future re-implementation but returns `[]` unconditionally. Original logic performed heuristic substring search to detect whether key symbols from source file summaries appeared in aggregated directory documentation. Disabled when `SumFileContent` schema removed structured `publicInterface` field (see `src/generation/writers/sum.ts`). Module retained to support future LLM-based or AST-based symbol extraction passes.\n\n## Type Surface\n\n`FindabilityResult` interface with fields:\n- `filePath: string` — validated `.sum` file path\n- `symbolsTested: string[]` — symbols checked for presence\n- `symbolsFound: string[]` — symbols located in AGENTS.md\n- `symbolsMissing: string[]` — symbols absent from AGENTS.md\n- `score: number` — ratio `symbolsFound.length / symbolsTested.length` (0-1 range)\n\n## Dependencies\n\nImports `SumFileContent` type from `../../generation/writers/sum.js` for parameter type annotation. No runtime dependencies since function body returns empty array.\n### inconsistency/\n<!-- Generated by agents-reverse-engineer -->\n\n# src/quality/inconsistency\n\nDetects and reports three categories of code-documentation discrepancies: exported symbols missing from `.sum` summaries (code-vs-doc), duplicate exports across files (code-vs-code), and unresolvable path references in `AGENTS.md` (phantom-paths).\n\n## Contents\n\n### Validators\n\n**[code-vs-doc.ts](./code-vs-doc.ts)** — Exports `extractExports()` (regex-based identifier extraction via `/^[ \\t]*export\\s+(?:default\\s+)?(?:function|class|const|let|var|type|interface|enum)\\s+(\\w+)/gm`) and `checkCodeVsDoc()` (compares source exports against `SumFileContent.summary` via substring matching, returns `CodeDocInconsistency` with `missingFromDoc` array or `null`).\n\n**[code-vs-code.ts](./code-vs-code.ts)** — Exports `checkCodeVsCode()` aggregating exports across scoped file groups into `Map<symbol, paths[]>`, returns `CodeCodeInconsistency[]` for symbols appearing in multiple files with `pattern: 'duplicate-export'`.\n\n**[reporter.ts](./reporter.ts)** — Exports `buildInconsistencyReport()` (aggregates `Inconsistency[]` into `InconsistencyReport` with per-type/per-severity summary counts) and `formatReportForCli()` (renders plain text output with severity tags `[ERROR]`, `[WARN]`, `[INFO]` and type-discriminated path formatting).\n\n## Validation Strategy\n\n**Code-vs-Doc:** Regex extraction from source followed by case-sensitive substring search in `.sum` summary text. Does not use AST analysis, yielding false negatives for destructured/namespace/dynamic exports and prose mentions unrelated to API surface.\n\n**Code-vs-Code:** Symbol-name deduplication across per-directory file groups. Caller must scope input to prevent false positives from legitimate cross-directory symbol reuse (e.g., multiple `index.ts` files exporting `Config`).\n\n**Phantom Paths:** Not implemented in this directory. Handled by `../phantom-paths/validator.ts` which extracts markdown links, backtick paths, and prose-embedded paths from `AGENTS.md`, resolves via `existsSync()` with `.ts`/`.js` fallback.\n\n## File Relationships\n\n`code-vs-code.ts` imports `extractExports()` from `code-vs-doc.ts` for symbol extraction reuse. `reporter.ts` consumes discriminated union `Inconsistency` (from `../types.ts`) unifying `CodeDocInconsistency`, `CodeCodeInconsistency`, and `PhantomPathInconsistency`. Orchestrator in `../index.ts` invokes validators and passes results to `buildInconsistencyReport()` → `formatReportForCli()` pipeline.\n\n## Behavioral Contracts\n\n**Export extraction pattern:** `/^[ \\t]*export\\s+(?:default\\s+)?(?:function|class|const|let|var|type|interface|enum)\\s+(\\w+)/gm` matches line-start whitespace, `export` keyword, optional `default`, declaration keyword (`function`/`class`/`const`/`let`/`var`/`type`/`interface`/`enum`), captures identifier `(\\w+)`.\n\n**CLI output format:**\n```\n=== Inconsistency Report ===\nChecked 42 files in 150ms\nFound 3 issue(s)\n\n[ERROR] Documentation out of sync: 2 exports undocumented\n  File: src/foo/bar.ts\n\n[WARN] Symbol \"Config\" exported from 2 files\n  Files: src/config/a.ts, src/config/b.ts\n```\n\nHuman-readable severity tags followed by description and type-specific paths (code-vs-doc shows `filePath`, code-vs-code shows `files.join(', ')`, phantom-path shows `agentsMdPath` and `details.referencedPath`).\n\n## Known Limitations\n\nRegex-based extraction misses destructured exports (`export const { foo } = obj`), namespace exports (`export * as Utils from './utils'`), dynamic exports (`export { [computedName]: value }`), re-exports with renaming (`export { foo as bar } from './mod'`). No AST analysis to distinguish intentional duplication (interface/implementation pairs, test fixtures) from architectural violations. Substring matching yields false negatives when prose mentions symbols in non-API contexts.\n### phantom-paths/\n<!-- Generated by agents-reverse-engineer -->\n\n# src/quality/phantom-paths\n\nValidates path references in `AGENTS.md` files by extracting path-like strings via regex patterns, resolving them against filesystem locations with TypeScript/JavaScript extension fallbacks, and reporting unresolved references as `PhantomPathInconsistency` objects in quality reports.\n\n## Contents\n\n**[index.ts](./index.ts)** — Barrel export of `checkPhantomPaths` validator function.\n\n**[validator.ts](./validator.ts)** — Extracts path references from `AGENTS.md` content using `PATH_PATTERNS` (markdown links, backtick-quoted paths, prose-embedded paths), resolves each path against `agentsMdDir` and `projectRoot` with `.ts`/`.js` extension fallback via `tryPaths[]`, filters excluded patterns (URLs, template literals, globs), and returns `PhantomPathInconsistency[]` array with deduplication via `seen` Set.\n\n## Path Extraction Patterns\n\n`PATH_PATTERNS` captures three reference types:\n- `/\\[(?:[^\\]]*)\\]\\((\\.[^)]+)\\)/g` — Markdown link targets\n- `` /`((?:src\\/|\\.\\.?\\/)[^`]+\\.[a-z]{1,4})`/g `` — Backtick-quoted paths with extensions\n- `/(?:from|in|by|via|see)\\s+`?(src\\/[\\w\\-./]+)`?/gi` — Prose-embedded paths following contextual keywords\n\n## Resolution Strategy\n\nFor each extracted path, `checkPhantomPaths()` attempts resolution in priority order:\n\n1. Relative to `AGENTS.md` directory: `path.resolve(agentsMdDir, rawPath)`\n2. Relative to project root: `path.resolve(projectRoot, rawPath)`\n3. TypeScript convention fallback: append `.replace(/\\.js$/, '.ts')` variants to `tryPaths[]` when `rawPath.endsWith('.js')`\n\nValidates existence via `existsSync()` on all candidate paths; returns `PhantomPathInconsistency` with `severity: 'warning'` when all attempts fail.\n\n## Exclusion Patterns\n\n`SKIP_PATTERNS` filters non-file references before validation:\n- `/node_modules/`, `/\\.git\\//` — Dependency/VCS paths\n- `/^https?:/` — URLs\n- `/\\{\\{/`, `/\\$\\{/` — Template placeholders/literals\n- `/\\*/`, `/\\{[^}]*,[^}]*\\}/` — Glob patterns/brace expansions\n\n## Inconsistency Reporting\n\nEach phantom path generates `PhantomPathInconsistency` containing:\n- `agentsMdPath` — Normalized via `path.relative(projectRoot, agentsMdPath)`\n- `description` — `\"Phantom path reference: \\\"${rawPath}\\\" does not exist\"`\n- `details.referencedPath` — Original extracted path\n- `details.resolvedTo` — Attempted resolution relative to project root\n- `details.context` — Containing line truncated to 120 characters",
      "systemPrompt": "You are generating an AGENTS.md file — a directory-level overview for AI coding assistants.\n\nCRITICAL: Output ONLY the raw markdown content. No code fences, no preamble, no explanations, no conversational text. Your entire response IS the AGENTS.md file content.\n\nOUTPUT FORMAT:\n- First line MUST be exactly: <!-- Generated by agents-reverse-engineer -->\n- Use a # heading with the directory name\n- Write a one-paragraph purpose statement for the directory\nusage\nADAPTIVE SECTIONS:\nAnalyze the directory contents and choose the most relevant sections. Do NOT use a fixed template. Instead, select sections that best document this specific directory for an AI that needs to reconstruct or extend the project.\n\nConsider these section types (choose what applies):\n- **Contents**: Group files by purpose/category under ## headings. For each file: markdown link [filename](./filename) and a one-line description.\n- **Subdirectories**: If subdirectories exist, list them with links [dirname/](./dirname/) and brief summaries.\n- **Architecture / Data Flow**: If files form a pipeline, request/response chain, or layered architecture, document it.\n- **Stack**: If this is a package root (has package.json, Cargo.toml, go.mod, etc.), document the technology stack, key scripts, and entry points.\n- **Structure**: If the directory layout follows a convention (feature-sliced, domain-driven, MVC, etc.), document it.\n- **Patterns**: If files share recurring design patterns (factory, strategy, middleware, barrel re-export), name and document them.\n- **Configuration**: If the directory contains config files, schemas, or environment definitions, document the config surface area.\n- **API Surface**: If the directory exports a public API (barrel index, route definitions, SDK), document the interface contract.\n- **File Relationships**: How files collaborate, depend on each other, or share state.\n- **Behavioral Contracts**: If files contain regex patterns, format specifications, magic constants, or template strings that define observable behavior, collect them in a dedicated section. Preserve verbatim patterns from file summaries — do NOT paraphrase regex into prose. This section is MANDATORY when file summaries contain behavioral artifacts.\n- **Reproduction-Critical Constants**: If file summaries reference annex files (via ## Annex References sections), list them with links. Example: \"Full prompt template text: [templates.ts.annex.md](./prompts/templates.ts.annex.md)\". Do NOT reproduce annex content in AGENTS.md — just link to it.\n\nChoose any relevant sections or create your own based on the directory contents. The goal is to provide a comprehensive overview that captures the essence of the directory's role in the project and how its files work together, with a focus on what an AI coding assistant would need to know to effectively interact with this code.\n\nSCOPE:\n- AGENTS.md is a NAVIGATIONAL INDEX — help an AI find the right file quickly\n- Focus on: what each file does, how files relate, directory-level patterns\n- Do NOT reproduce full architecture sections — those belong in the root CLAUDE.md\n\nPATH ACCURACY (MANDATORY):\n- When referencing files or modules outside this directory, use ONLY paths from the \"Import Map\" section\n- Do NOT invent, rename, or guess module paths — if a path isn't in the Import Map, don't reference it\n- Use the exact directory names from \"Project Directory Structure\" — do NOT rename directories\n  (e.g., if the directory is called \"cli\", write \"src/cli/\", NOT \"src/commands/\")\n- Cross-module references must use the specifier format from actual import statements\n  (e.g., \"../generation/writers/sum.js\", NOT \"../fs/sum-file.js\")\n- If you are unsure about a path, omit the cross-reference rather than guessing\n\nCONSISTENCY (MANDATORY):\n- Do not contradict yourself within the same document\n- If you describe a technique (e.g., \"regex-based\"), do not call it something else later (e.g., \"AST-based\")\n- When stating version numbers, engines, or config fields, use ONLY values present in the file summaries\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this directory\", \"this module\", \"provides\", \"responsible for\", \"is used to\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Per-file descriptions in Contents sections: 1-2 sentences maximum. Reference key symbols but do not reproduce full summaries.\n- Behavioral contracts (regex patterns, format specs, constants) belong in a separate Behavioral Contracts section, not in per-file descriptions.\n- Annex file references: link to .annex.md files, do not inline their content. One line per annex reference.\n- Subdirectory descriptions: 1-2 sentences maximum. Capture the directory's role, not its full contents.\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- Key exported symbols from file summaries MUST appear in the directory overview\n- Preserve exact casing of identifiers\n\nUSER NOTES:\n- If \"User Notes\" are provided in the prompt, they contain user-defined instructions that will be automatically prepended to your output\n- Do NOT repeat or paraphrase user notes in your generated content — they are included separately\n- You may reference information from user notes for context",
      "response": "<!-- Generated by agents-reverse-engineer -->\n\n# src/quality\n\nPost-generation validation system detecting code-documentation inconsistencies (code-vs-doc), duplicate symbol exports (code-vs-code), and unresolvable path references (phantom-paths) through regex-based extraction, filesystem resolution, and structured reporting.\n\n## Contents\n\n**[index.ts](./index.ts)** — Barrel module re-exporting `extractExports()`, `checkCodeVsDoc()`, `checkCodeVsCode()`, `checkPhantomPaths()`, `buildInconsistencyReport()`, `formatReportForCli()`, `validateFindability()` from submodules alongside all type definitions (`Inconsistency`, `InconsistencyReport`, `FindabilityResult`, `CodeDocInconsistency`, `CodeCodeInconsistency`, `PhantomPathInconsistency`).\n\n**[types.ts](./types.ts)** — Defines discriminated union `Inconsistency = CodeDocInconsistency | CodeCodeInconsistency | PhantomPathInconsistency` with `type` discriminator, `InconsistencySeverity` literal union (`'info' | 'warning' | 'error'`), and `InconsistencyReport` structure containing `metadata` (timestamp, projectRoot, filesChecked, durationMs), `issues[]`, and `summary` counts by type/severity.\n\n## Validation Modules\n\n### Code-vs-Doc Consistency\n\n`inconsistency/code-vs-doc.ts` exports `extractExports()` applying regex `/^[ \\t]*export\\s+(?:default\\s+)?(?:function|class|const|let|var|type|interface|enum)\\s+(\\w+)/gm` to extract identifier names, and `checkCodeVsDoc()` performing substring search in `.sum` summary text to detect undocumented exports. Returns `CodeDocInconsistency` with `missingFromDoc[]` array when symbols absent from documentation.\n\n### Code-vs-Code Duplication\n\n`inconsistency/code-vs-code.ts` exports `checkCodeVsCode()` aggregating exports across per-directory file groups into `Map<symbol, paths[]>`, returning `CodeCodeInconsistency[]` for symbols appearing in multiple files with `pattern: 'duplicate-export'`. Caller must scope input to directory boundaries to prevent false positives from legitimate cross-directory symbol reuse.\n\n### Phantom Path Resolution\n\n`phantom-paths/validator.ts` extracts path strings via three regex patterns (markdown links, backtick-quoted paths, prose-embedded keywords), resolves against `AGENTS.md` directory and project root with `.ts`/`.js` extension fallbacks, validates via `existsSync()`. Returns `PhantomPathInconsistency[]` with `severity: 'warning'` for unresolved references after filtering exclusions (`node_modules`, URLs, template literals, globs).\n\n### Report Generation\n\n`inconsistency/reporter.ts` exports `buildInconsistencyReport()` aggregating issues with summary computation (total/per-type/per-severity counts) and `formatReportForCli()` rendering plain text output with severity tags `[ERROR]`, `[WARN]`, `[INFO]` and type-discriminated path formatting.\n\n### Density Validation (Disabled)\n\n`density/validator.ts` exports stub `validateFindability()` returning empty `FindabilityResult[]` array. Originally verified exported symbols from `.sum` files appeared in parent `AGENTS.md` via substring matching; disabled after `SumFileContent.metadata.publicInterface` removal pending structured extraction re-implementation.\n\n## Integration Points\n\nConsumed by `src/generation/orchestrator.ts` post-generation validation phase and `src/cli/generate.ts` for terminal reporting. All validators operate on generated artifacts (`.sum`, `AGENTS.md`) rather than source code, enforcing post-generation validation contract. Report format matches telemetry schema in `src/ai/telemetry/run-log.ts` for `qualityMetrics` aggregation.\n\n## Behavioral Contracts\n\n**Export extraction pattern:**\n```regex\n/^[ \\t]*export\\s+(?:default\\s+)?(?:function|class|const|let|var|type|interface|enum)\\s+(\\w+)/gm\n```\n\n**Phantom path patterns:**\n- Markdown links: `/\\[(?:[^\\]]*)\\]\\((\\.[^)]+)\\)/g`\n- Backtick paths: `` /`((?:src\\/|\\.\\.?\\/)[^`]+\\.[a-z]{1,4})`/g ``\n- Prose-embedded: `/(?:from|in|by|via|see)\\s+`?(src\\/[\\w\\-./]+)`?/gi`\n\n**CLI report format:**\n```\n=== Inconsistency Report ===\nChecked 42 files in 150ms\nFound 3 issue(s)\n\n[ERROR] Documentation out of sync: 2 exports undocumented\n  File: src/foo/bar.ts\n\n[WARN] Symbol \"Config\" exported from 2 files\n  Files: src/config/a.ts, src/config/b.ts\n\n[WARN] Phantom path reference: \"src/missing.ts\" does not exist\n  AGENTS.md: src/quality/AGENTS.md\n  Referenced: src/missing.ts\n```\n\n## Known Limitations\n\nRegex-based `extractExports()` misses destructured exports, namespace exports, dynamic exports, and re-exports with renaming. Substring matching in code-vs-doc yields false negatives for prose mentions unrelated to API surface. Code-vs-code operates on symbol names only without AST analysis to distinguish intentional duplication (interface/implementation pairs, test fixtures) from architectural violations. Phantom path resolution uses heuristic extension fallbacks (`.js` → `.ts`) without TypeScript compiler path mapping awareness.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 1384,
      "cacheReadTokens": 17587,
      "cacheCreationTokens": 12975,
      "latencyMs": 43811,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": []
    },
    {
      "timestamp": "2026-02-09T17:47:47.716Z",
      "prompt": "Generate AGENTS.md for directory: \"src/specify\" (specify)\n\n## File Summaries (3 files)\n\n### index.ts\n**Purpose:** Re-exports specification synthesis functionality: `buildSpecPrompt`, `SpecPrompt`, `writeSpec`, `SpecExistsError`, an...\n\n**Re-exports specification synthesis functionality: `buildSpecPrompt`, `SpecPrompt`, `writeSpec`, `SpecExistsError`, and `WriteSpecOptions` from `prompts.js` and `writer.js` submodules.**\n\n## Exported Symbols\n\n- `buildSpecPrompt` — function from `./prompts.js` that constructs AI prompts for synthesizing project specifications from AGENTS.md corpus\n- `SpecPrompt` — type from `./prompts.js` defining the structure of specification generation prompts\n- `writeSpec` — function from `./writer.js` that writes generated specification content to `specs/SPEC.md` or split multi-file outputs\n- `SpecExistsError` — error class from `./writer.js` thrown when specification files already exist and `--force` flag is not set\n- `WriteSpecOptions` — type from `./writer.js` defining options for `writeSpec` including `force` override and `multiFile` split mode\n\n## Module Role\n\nServes as barrel export for the specification synthesis subsystem invoked by `src/cli/specify.ts` command. Aggregates AGENTS.md documentation tree into executable project specifications via AI-driven synthesis, supporting both single-file (`specs/SPEC.md`) and multi-file (`specs/<dirname>.md`) output modes.\n\n## Integration Points\n\nConsumed by `src/cli/specify.ts` which orchestrates the `are specify` command workflow: collect AGENTS.md files via `src/generation/collector.ts`, invoke `buildSpecPrompt` with aggregated content, execute AI call via `src/ai/service.ts`, write output via `writeSpec` with conflict detection.\n### prompts.ts\n**Purpose:** Defines prompt engineering templates for AI-driven project specification synthesis via `buildSpecPrompt()` function a...\n\n**Defines prompt engineering templates for AI-driven project specification synthesis via `buildSpecPrompt()` function and `SPEC_SYSTEM_PROMPT` constant.**\n\n## Exported Types\n\n`SpecPrompt` interface contains `system: string` and `user: string` properties representing the system instructions and user content for AI spec generation.\n\n## Exported Constants\n\n`SPEC_SYSTEM_PROMPT` defines the system prompt enforcing 11-section conceptual organization: (1) Project Overview, (2) Architecture, (3) Public API Surface, (4) Data Structures & State, (5) Configuration, (6) Dependencies, (7) Behavioral Contracts (split into Runtime Behavior and Implementation Contracts), (8) Test Contracts, (9) Build Plan, (10) Prompt Templates & System Instructions, (11) IDE Integration & Installer. Prohibits folder-mirroring and exact file path prescription. Mandates verbatim reproduction of reproduction-critical content from annex files: regex patterns, format strings, magic constants, prompt templates, IDE templates, environment variable names.\n\n## Exported Functions\n\n`buildSpecPrompt(docs: AgentsDocs, annexFiles?: AgentsDocs): SpecPrompt` constructs the prompt pair by concatenating all AGENTS.md content with section delimiters (`### ${doc.relativePath}\\n\\n${doc.content}`), appending optional annex file content (reproduction-critical source files), and injecting output requirements listing the mandatory 11-section structure. Returns `SpecPrompt` with `system` set to `SPEC_SYSTEM_PROMPT` and `user` containing structured markdown with section headers for AGENTS.md files, annex files (if provided), and explicit output requirements.\n\n## Integration Context\n\nConsumed by `src/specify/index.ts` which calls `buildSpecPrompt()` with output from `collectAgentsDocs()` (recursive AGENTS.md traversal) and optionally `collectAnnexFiles()` (reproduction-critical source extraction). The resulting `SpecPrompt` is passed to `AIService.call()` for synthesis into `specs/SPEC.md` or split multi-file specs.\n\n## Behavioral Contracts\n\nSection 7 of `SPEC_SYSTEM_PROMPT` mandates two subsections: (a) Runtime Behavior — error types/codes, retry formulas/delays, concurrency model, lifecycle hooks; (b) Implementation Contracts — verbatim regex patterns in backticks, format strings with examples, magic constants with meanings, environment variable names with expected values, file format specifications (YAML schemas, NDJSON structures). Explicitly forbids paraphrasing regex patterns into prose descriptions.\n\nSection 10 (Prompt Templates & System Instructions) requires FULL verbatim text reproduction from annex files or AGENTS.md content, organized by pipeline phase or functional area, preserving placeholder syntax (e.g., `{{FILE_PATH}}`).\n\nSection 11 (IDE Integration & Installer) requires verbatim reproduction of command templates per platform, platform configuration objects (path prefixes, filename conventions, frontmatter formats), installer permission lists, hook definitions.\n\n## Output Format Rules\n\n`SPEC_SYSTEM_PROMPT` enforces \"Raw markdown. No preamble. No meta-commentary. No 'Here is...' or 'I've generated...' prefix.\" `buildSpecPrompt()` user prompt concludes with \"Output ONLY the markdown content. No preamble.\" and \"Sections 10 and 11 MUST reproduce annex content verbatim. Do NOT summarize prompt templates or IDE templates into prose descriptions.\"\n### writer.ts\n**Purpose:** writer.ts implements spec output file writing with overwrite protection, single/multi-file mode support, and markdown...\n\n**writer.ts implements spec output file writing with overwrite protection, single/multi-file mode support, and markdown heading-based content splitting.**\n\n## Exported Interface\n\n`WriteSpecOptions` interface defines output control:\n- `outputPath: string` — absolute path to output file (e.g., `/project/specs/SPEC.md`)\n- `force: boolean` — overwrite existing files without throwing `SpecExistsError`\n- `multiFile: boolean` — split AI output into separate files by top-level `# ` headings\n\n## Exported Error Class\n\n`SpecExistsError` extends `Error` and is thrown when `writeSpec()` detects existing files and `force=false`:\n- `paths: string[]` — readonly array of conflicting file paths\n- Constructor formats paths as bulleted list with instruction: `\"Use --force to overwrite.\"`\n- `name` property set to `'SpecExistsError'` for discriminated error handling\n\n## Main Export Function\n\n`writeSpec(content: string, options: WriteSpecOptions): Promise<string[]>` writes spec output with two modes:\n\n**Single-file mode** (`multiFile=false`):\n- Checks `fileExists(outputPath)` via `access(filePath, constants.F_OK)` and throws `SpecExistsError([outputPath])` if exists and `force=false`\n- Creates parent directory via `mkdir(path.dirname(outputPath), { recursive: true })`\n- Writes `content` to `outputPath` with `writeFile(outputPath, content, 'utf-8')`\n- Returns `[outputPath]` array\n\n**Multi-file mode** (`multiFile=true`):\n- Calls `splitByHeadings(content)` to partition content by top-level `# ` headings\n- Returns `Array<{ filename: string; content: string }>` with slugified filenames\n- Pre-flight checks all target paths `path.join(path.dirname(outputPath), section.filename)` for existence\n- Throws `SpecExistsError(conflicts)` with all conflicting paths if any exist and `force=false`\n- Creates output directory `mkdir(outputDir, { recursive: true })`\n- Writes each section via `writeFile(filePath, section.content, 'utf-8')`\n- Returns array of all written absolute paths\n\n## Content Splitting Logic\n\n`splitByHeadings(content: string): Array<{ filename: string; content: string }>` partitions markdown:\n- Splits on regex `/^(?=# )/m` matching lines starting with exactly `# ` (top-level headings)\n- Iterates parts, extracts heading text via `/^# (.+)/` match\n- Passes heading text to `slugify()` for filename generation\n- Content before first `# ` heading assigned to `'00-preamble.md'`\n- Trims trailing whitespace via `trimEnd()` and appends single newline to each section's `content`\n\n## Filename Sanitization\n\n`slugify(heading: string): string` transforms heading into filesystem-safe slug:\n- Lowercases entire string\n- Replaces whitespace sequences (`/\\s+/g`) with single hyphen\n- Strips non-alphanumeric characters except hyphens (`/[^a-z0-9-]/g` → empty string)\n- Collapses consecutive hyphens (`/-+/g` → single hyphen)\n- Trims leading/trailing hyphens via `/^-|-$/g` replacement\n- Returns slugified string used as `${slug}.md` filename\n\n## File Existence Check\n\n`fileExists(filePath: string): Promise<boolean>` wraps Node.js `fs/promises`:\n- Calls `access(filePath, constants.F_OK)` to test file accessibility\n- Returns `true` on success, `false` on caught exception (file not found or inaccessible)\n\n## Dependencies\n\nImports from Node.js core:\n- `writeFile`, `mkdir`, `access` from `node:fs/promises`\n- `constants` from `node:fs` (specifically `constants.F_OK`)\n- `path` module for `path.dirname()` and `path.join()`\n\n## Integration Points\n\nCalled by `src/specify/index.ts` `generateProjectSpec()` with options from CLI:\n- `outputPath` derived from project root and `--output` flag (default `specs/SPEC.md`)\n- `force` flag from `--force` CLI option\n- `multiFile` flag from `--multi-file` CLI option\n- Caller catches `SpecExistsError` and logs `error.message` to stderr\n\n## Import Map (verified — use these exact paths)\n\nprompts.ts:\n  ../generation/collector.js → AgentsDocs (type)\n\n## Project Directory Structure\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>",
      "systemPrompt": "You are generating an AGENTS.md file — a directory-level overview for AI coding assistants.\n\nCRITICAL: Output ONLY the raw markdown content. No code fences, no preamble, no explanations, no conversational text. Your entire response IS the AGENTS.md file content.\n\nOUTPUT FORMAT:\n- First line MUST be exactly: <!-- Generated by agents-reverse-engineer -->\n- Use a # heading with the directory name\n- Write a one-paragraph purpose statement for the directory\nusage\nADAPTIVE SECTIONS:\nAnalyze the directory contents and choose the most relevant sections. Do NOT use a fixed template. Instead, select sections that best document this specific directory for an AI that needs to reconstruct or extend the project.\n\nConsider these section types (choose what applies):\n- **Contents**: Group files by purpose/category under ## headings. For each file: markdown link [filename](./filename) and a one-line description.\n- **Subdirectories**: If subdirectories exist, list them with links [dirname/](./dirname/) and brief summaries.\n- **Architecture / Data Flow**: If files form a pipeline, request/response chain, or layered architecture, document it.\n- **Stack**: If this is a package root (has package.json, Cargo.toml, go.mod, etc.), document the technology stack, key scripts, and entry points.\n- **Structure**: If the directory layout follows a convention (feature-sliced, domain-driven, MVC, etc.), document it.\n- **Patterns**: If files share recurring design patterns (factory, strategy, middleware, barrel re-export), name and document them.\n- **Configuration**: If the directory contains config files, schemas, or environment definitions, document the config surface area.\n- **API Surface**: If the directory exports a public API (barrel index, route definitions, SDK), document the interface contract.\n- **File Relationships**: How files collaborate, depend on each other, or share state.\n- **Behavioral Contracts**: If files contain regex patterns, format specifications, magic constants, or template strings that define observable behavior, collect them in a dedicated section. Preserve verbatim patterns from file summaries — do NOT paraphrase regex into prose. This section is MANDATORY when file summaries contain behavioral artifacts.\n- **Reproduction-Critical Constants**: If file summaries reference annex files (via ## Annex References sections), list them with links. Example: \"Full prompt template text: [templates.ts.annex.md](./prompts/templates.ts.annex.md)\". Do NOT reproduce annex content in AGENTS.md — just link to it.\n\nChoose any relevant sections or create your own based on the directory contents. The goal is to provide a comprehensive overview that captures the essence of the directory's role in the project and how its files work together, with a focus on what an AI coding assistant would need to know to effectively interact with this code.\n\nSCOPE:\n- AGENTS.md is a NAVIGATIONAL INDEX — help an AI find the right file quickly\n- Focus on: what each file does, how files relate, directory-level patterns\n- Do NOT reproduce full architecture sections — those belong in the root CLAUDE.md\n\nPATH ACCURACY (MANDATORY):\n- When referencing files or modules outside this directory, use ONLY paths from the \"Import Map\" section\n- Do NOT invent, rename, or guess module paths — if a path isn't in the Import Map, don't reference it\n- Use the exact directory names from \"Project Directory Structure\" — do NOT rename directories\n  (e.g., if the directory is called \"cli\", write \"src/cli/\", NOT \"src/commands/\")\n- Cross-module references must use the specifier format from actual import statements\n  (e.g., \"../generation/writers/sum.js\", NOT \"../fs/sum-file.js\")\n- If you are unsure about a path, omit the cross-reference rather than guessing\n\nCONSISTENCY (MANDATORY):\n- Do not contradict yourself within the same document\n- If you describe a technique (e.g., \"regex-based\"), do not call it something else later (e.g., \"AST-based\")\n- When stating version numbers, engines, or config fields, use ONLY values present in the file summaries\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this directory\", \"this module\", \"provides\", \"responsible for\", \"is used to\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Per-file descriptions in Contents sections: 1-2 sentences maximum. Reference key symbols but do not reproduce full summaries.\n- Behavioral contracts (regex patterns, format specs, constants) belong in a separate Behavioral Contracts section, not in per-file descriptions.\n- Annex file references: link to .annex.md files, do not inline their content. One line per annex reference.\n- Subdirectory descriptions: 1-2 sentences maximum. Capture the directory's role, not its full contents.\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- Key exported symbols from file summaries MUST appear in the directory overview\n- Preserve exact casing of identifiers\n\nUSER NOTES:\n- If \"User Notes\" are provided in the prompt, they contain user-defined instructions that will be automatically prepended to your output\n- Do NOT repeat or paraphrase user notes in your generated content — they are included separately\n- You may reference information from user notes for context",
      "response": "<!-- Generated by agents-reverse-engineer -->\n\n# src/specify/\n\nProject specification synthesis from AGENTS.md documentation corpus via AI-driven prompt engineering. Exports `buildSpecPrompt()` for constructing two-part prompts (system constraints + aggregated AGENTS.md content), `writeSpec()` for filesystem output with overwrite protection and multi-file splitting, and `SpecExistsError` for conflict detection.\n\n## Contents\n\n**[index.ts](./index.ts)** — Barrel re-export aggregating `buildSpecPrompt`, `SpecPrompt`, `writeSpec`, `SpecExistsError`, `WriteSpecOptions` from `prompts.ts` and `writer.ts` submodules. Consumed by `src/cli/specify.ts` command orchestrator.\n\n**[prompts.ts](./prompts.ts)** — Prompt template factory exporting `buildSpecPrompt(docs, annexFiles?)` which constructs `SpecPrompt` pairs: system instructions (`SPEC_SYSTEM_PROMPT` enforcing 11-section conceptual organization) and user content (concatenated AGENTS.md markdown with section delimiters, optional annex files for reproduction-critical constants). Returns structured prompt with mandatory output format rules (raw markdown, no preamble, verbatim reproduction of prompt templates/regex/IDE templates).\n\n**[writer.ts](./writer.ts)** — Filesystem writer exporting `writeSpec(content, options)` supporting single-file (`specs/SPEC.md`) and multi-file (`specs/<slug>.md`) output modes. Implements pre-flight existence checks throwing `SpecExistsError(conflicts[])` when `force=false`, content splitting via `splitByHeadings()` (regex `/^(?=# )/m`), filename sanitization through `slugify()` (lowercase + hyphenation + alphanumeric filtering), and directory creation with `mkdir(outputDir, { recursive: true })`.\n\n## Architecture\n\n**Prompt Construction Pipeline:**  \n`buildSpecPrompt()` receives `AgentsDocs[]` from `../generation/collector.js` (`collectAgentsDocs()` recursive traversal) and optional annex files (`collectAnnexFiles()`). Constructs user prompt by iterating docs array, emitting `### ${doc.relativePath}\\n\\n${doc.content}` sections, appending annex section if provided, injecting output requirements listing 11 mandatory sections. System prompt (`SPEC_SYSTEM_PROMPT`) prohibits folder-mirroring, exact file path prescription, mandates verbatim reproduction of behavioral contracts (regex patterns, format strings, magic constants, environment variables, prompt templates, IDE templates).\n\n**Filesystem Writing Pipeline:**  \n`writeSpec()` branches on `options.multiFile`. Single-file mode: checks `fileExists(outputPath)`, throws `SpecExistsError([outputPath])` if exists and `force=false`, creates parent directory, writes content. Multi-file mode: calls `splitByHeadings()` to partition by top-level `# ` headings, checks all target paths for conflicts, throws `SpecExistsError(conflicts)` if any exist and `force=false`, writes each section to slugified filename.\n\n## Behavioral Contracts\n\n**SPEC_SYSTEM_PROMPT Section 7 (Behavioral Contracts):**  \nEnforces two subsections: (a) Runtime Behavior — error types/codes, retry formulas/delays, concurrency model, lifecycle hooks; (b) Implementation Contracts — verbatim regex patterns in backticks, format strings with examples, magic constants with meanings, environment variable names with expected values, file format specifications (YAML schemas, NDJSON structures). Prohibits paraphrasing regex into prose.\n\n**SPEC_SYSTEM_PROMPT Section 10 (Prompt Templates):**  \nRequires FULL verbatim text reproduction from annex files or AGENTS.md content, organized by pipeline phase or functional area, preserving placeholder syntax (e.g., `{{FILE_PATH}}`).\n\n**SPEC_SYSTEM_PROMPT Section 11 (IDE Integration):**  \nRequires verbatim reproduction of command templates per platform, platform configuration objects (path prefixes, filename conventions, frontmatter formats), installer permission lists, hook definitions.\n\n**Content Splitting Regex:**  \n`/^(?=# )/m` matches lines starting with exactly `# ` (top-level headings). Content before first heading assigned to `'00-preamble.md'`.\n\n**Filename Sanitization Transform:**  \n`slugify()` applies sequence: lowercase → whitespace (`/\\s+/g`) to hyphen → strip non-alphanumeric except hyphens (`/[^a-z0-9-]/g`) → collapse hyphens (`/-+/g`) → trim edge hyphens (`/^-|-$/g`).\n\n## Integration Points\n\n**Upstream:** `src/cli/specify.ts` orchestrates workflow: validates project root, collects AGENTS.md via `collectAgentsDocs()`, optionally collects annex files via `collectAnnexFiles()`, calls `buildSpecPrompt(docs, annexFiles)`, invokes `AIService.call()` with returned `SpecPrompt`, passes AI output to `writeSpec()` with options from CLI flags (`--force`, `--multi-file`, `--output`).\n\n**Downstream:** `writeSpec()` consumes `node:fs/promises` (`writeFile`, `mkdir`, `access`), `node:fs` (`constants.F_OK`), `node:path` (`dirname`, `join`). Throws `SpecExistsError` caught by CLI for user-facing error messages.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 1313,
      "cacheReadTokens": 17587,
      "cacheCreationTokens": 11424,
      "latencyMs": 48377,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": []
    },
    {
      "timestamp": "2026-02-09T17:47:47.705Z",
      "prompt": "Generate AGENTS.md for directory: \"src/config\" (config)\n\n## File Summaries (3 files)\n\n### defaults.ts\n**Purpose:** defaults.ts exports default configuration constants and platform-adaptive concurrency calculation for agents-reverse-...\n\n**defaults.ts exports default configuration constants and platform-adaptive concurrency calculation for agents-reverse-engineer initialization.**\n\n## Exported Functions\n\n**getDefaultConcurrency(): number** computes adaptive concurrency limit based on CPU cores and available system memory. Returns integer between `MIN_CONCURRENCY` (2) and `MAX_CONCURRENCY` (20). Formula: `clamp(cores × 5, MIN, min(memCap, MAX))` where `cores` resolves via `os.availableParallelism()` fallback to `os.cpus().length`, and `memCap = floor((totalMemGB × 0.5) / 0.512)` enforces memory constraint preventing subprocess heap exhaustion (allocates 50% of total RAM divided by 512MB per subprocess).\n\n## Exported Constants\n\n**DEFAULT_VENDOR_DIRS**: readonly string array of 18 directory names excluded from file discovery: package managers (`node_modules`, `vendor`), build outputs (`dist`, `build`, `target`, `.next`), version control (`.git`), Python environments (`__pycache__`, `venv`, `.venv`), Rust/Gradle caches (`.cargo`, `.gradle`), AI assistant tooling directories (`.agents-reverse-engineer`, `.agents`, `.planning`, `.claude`, `.opencode`, `.gemini`).\n\n**DEFAULT_EXCLUDE_PATTERNS**: readonly string array of gitignore-style glob patterns for custom filter: AI-generated documentation files (`AGENTS.md`, `CLAUDE.md`, `OPENCODE.md`, `GEMINI.md` with `**/` variants), lock files (`*.lock`, `package-lock.json`, `yarn.lock`, `pnpm-lock.yaml`, `bun.lock`, `bun.lockb`, `Gemfile.lock`, `Cargo.lock`, `poetry.lock`, `composer.lock`, `go.sum`), dotfiles and logs (`.gitignore`, `.gitattributes`, `.gitkeep`, `.env`, `**/.env`, `**/.env.*`, `*.log`), generated summaries (`*.sum`, `**/*.sum`), skill definitions (`**/SKILL.md`).\n\n**DEFAULT_BINARY_EXTENSIONS**: readonly string array of 26 file extensions for binary detection: images (`.png`, `.jpg`, `.jpeg`, `.gif`, `.bmp`, `.ico`, `.webp`), archives (`.zip`, `.tar`, `.gz`, `.rar`, `.7z`), executables (`.exe`, `.dll`, `.so`, `.dylib`), media (`.mp3`, `.mp4`, `.wav`), documents (`.pdf`), fonts (`.woff`, `.woff2`, `.ttf`, `.eot`), compiled bytecode (`.class`, `.pyc`).\n\n**DEFAULT_MAX_FILE_SIZE**: numeric constant `1048576` (1MB in bytes) for binary detection threshold via file size heuristic.\n\n**DEFAULT_CONFIG**: readonly object matching config schema structure with nested `exclude` (patterns/vendorDirs/binaryExtensions arrays), `options` (followSymlinks: false, maxFileSize: 1048576), `output` (colors: true). Used by config loader for missing fields or absent config file fallback.\n\n## Configuration Constants\n\n**CONCURRENCY_MULTIPLIER**: value `5` scales CPU core count in concurrency formula.\n\n**MIN_CONCURRENCY**: value `2` enforces lower bound matching schema validation and WSL resource constraints documented in subprocess management mitigations.\n\n**MAX_CONCURRENCY**: value `20` enforces upper bound matching Zod schema `.max(20)` constraint in `src/config/schema.ts`.\n\n**SUBPROCESS_HEAP_GB**: value `0.512` (512MB) matches `NODE_OPTIONS='--max-old-space-size=512'` environment variable set in `src/ai/subprocess.ts` for heap limit per Claude CLI subprocess.\n\n**MEMORY_FRACTION**: value `0.5` allocates maximum 50% of total system RAM to concurrent subprocess pool, preventing host system memory exhaustion under high concurrency.\n\n## Memory-Aware Concurrency Logic\n\n**getDefaultConcurrency()** implements memory-bounded worker pool sizing preventing RAM exhaustion scenarios where `cores × 5` would spawn too many 512MB subprocesses. Checks `os.totalmem() / (1024³)` for total RAM in GB, computes `memCap = floor((totalMemGB × 0.5) / 0.512)` limiting concurrent subprocesses to fit within 50% memory budget, applies `Math.min(computed, memCap, MAX_CONCURRENCY)` to respect memory constraint alongside schema maximum. Fallback path uses `Infinity` memCap when `totalMemGB ≤ 1` to avoid division-by-zero on resource-constrained systems, deferring to CPU-based formula and MAX_CONCURRENCY ceiling.\n### loader.ts\n**Purpose:** loadConfig() loads and validates `.agents-reverse-engineer/config.yaml` configuration via Zod schema, returns default...\n\n**loadConfig() loads and validates `.agents-reverse-engineer/config.yaml` configuration via Zod schema, returns defaults when file absent, throws ConfigError on validation failure, emits config:loaded trace events, and writeDefaultConfig() generates annotated YAML with comment blocks explaining exclusion patterns, discovery options, and AI service settings.**\n\n## Exported Functions\n\n**loadConfig(root: string, options?: { tracer?: ITraceWriter; debug?: boolean }): Promise<Config>**\nReads `config.yaml` from `path.join(root, CONFIG_DIR, CONFIG_FILE)` where `CONFIG_DIR = '.agents-reverse-engineer'` and `CONFIG_FILE = 'config.yaml'`. Parses YAML via `parse()` from `yaml` library, validates against `ConfigSchema` from `./schema.js`. Returns `Config` object with defaults applied via `ConfigSchema.parse(raw)`. Throws `ConfigError` wrapping `ZodError` with formatted issue paths like `\"ai.concurrency: Expected number\"`. Returns `ConfigSchema.parse({})` defaults when file not found (`ENOENT`). Emits `config:loaded` trace event with `configPath`, `model`, `concurrency` fields via `options?.tracer?.emit()`. Logs debug output with `pc.dim()` colored messages showing model and concurrency when `options?.debug` true.\n\n**configExists(root: string): Promise<boolean>**\nReturns true if `path.join(root, CONFIG_DIR, CONFIG_FILE)` accessible via `access(configPath, constants.F_OK)`, false on exception.\n\n**writeDefaultConfig(root: string): Promise<void>**\nCreates `.agents-reverse-engineer/` directory via `mkdir(configDir, { recursive: true })`, writes `config.yaml` with multi-line comment blocks sectioned by `# ============================================================================` dividers. Injects `DEFAULT_EXCLUDE_PATTERNS`, `DEFAULT_VENDOR_DIRS`, `DEFAULT_BINARY_EXTENSIONS` from `./defaults.js` as YAML list items. Calls `yamlScalar(value)` to quote patterns containing regex chars `[*{}\\[\\]?,:#&!|>'\"%@\\`]` with backslash-escaped double quotes. Embeds `DEFAULT_MAX_FILE_SIZE` and `getDefaultConcurrency()` numeric defaults directly in YAML comment text like `# Default: ${DEFAULT_MAX_FILE_SIZE} (1MB)`. Contains commented-out `# concurrency: ${getDefaultConcurrency()}` line showing override syntax.\n\n## Error Handling\n\n**ConfigError** extends Error with `filePath: string` and optional `cause?: Error` properties, sets `name = 'ConfigError'`. Constructor signature: `(message: string, filePath: string, cause?: Error)`. Thrown for three failure modes: ZodError validation with formatted issue list joined by `\\n`, YAML parse errors re-wrapped with message prefix `\"Failed to parse ${configPath}: ${err.message}\"`, and propagated ConfigError instances re-thrown as-is without wrapping.\n\n## Trace Integration\n\nEmits `config:loaded` event via `options?.tracer?.emit()` with payload shape `{ type: 'config:loaded', configPath: string, model: string | null, concurrency: number }`. Event emitted twice per loadConfig() call: once after successful parse with relative path `path.relative(root, configPath)`, once for defaults with literal string `'(defaults)'` as configPath. Trace events logged even when `options?.debug` false.\n\n## YAML Serialization\n\n**yamlScalar(value: string): string**\nTests input against `/[*{}\\[\\]?,:#&!|>'\"%@\\`]/` pattern matching YAML metacharacters (asterisk for alias, braces/brackets for flow collections, question/colon for mappings, hash for comments, ampersand for anchor, pipe/angle for block scalars, quotes/percent/at/backtick for string edge cases). Returns double-quoted string with backslash escapes for literal backslashes `replace(/\\\\/g, '\\\\\\\\')` and double quotes `replace(/\"/g, '\\\\\"')` when pattern matches. Returns unquoted string for simple values.\n\n## Configuration File Structure\n\nGenerated YAML contains five comment-delimited sections: FILE & DIRECTORY EXCLUSIONS (patterns, vendorDirs, binaryExtensions arrays), DISCOVERY OPTIONS (followSymlinks boolean, maxFileSize number), OUTPUT FORMATTING (colors boolean), AI SERVICE CONFIGURATION (backend/model strings, timeoutMs/maxRetries/concurrency numbers, telemetry.keepRuns number). Each array rendered as YAML list with `- ` prefix and 4-space indentation. Vendor directories and binary extensions interpolated without yamlScalar() quoting. Exclude patterns passed through yamlScalar() due to glob wildcards like `*.log` and `temp/**`. Concurrency line commented out with `# concurrency: ${getDefaultConcurrency()}` showing override example but defaulting to auto-detection when absent.\n\n## Debug Output\n\nConsole writes via `console.error()` with `pc.dim()` gray formatting when `options?.debug` true. Emits two messages per load: `\"[debug] Config loaded from: ${path.relative(root, configPath)}\"` or `\"[debug] Config file not found, using defaults\"` for file status, then `\"[debug] Model: ${config.ai.model}, Concurrency: ${config.ai.concurrency}\"` for parsed values. Debug output follows trace emission to maintain event ordering.\n\n## Dependencies\n\nImports `readFile`, `writeFile`, `mkdir`, `access` from `node:fs/promises` for async file I/O. Imports `constants` from `node:fs` for `F_OK` flag. Uses `path.join()` and `path.relative()` from `node:path`. Parses YAML via `parse()` and `stringify()` from `yaml` package (stringify unused in current implementation). Validates with `ZodError` from `zod`. Colors via `pc` (picocolors alias). Imports `ConfigSchema` and `Config` type from `./schema.js`, defaults from `./defaults.js` (`DEFAULT_VENDOR_DIRS`, `DEFAULT_BINARY_EXTENSIONS`, `DEFAULT_MAX_FILE_SIZE`, `DEFAULT_EXCLUDE_PATTERNS`, `getDefaultConcurrency`), `ITraceWriter` from `../orchestration/trace.js`.\n### schema.ts\n**Purpose:** ConfigSchema defines Zod validation schema for `.agents-reverse-engineer/config.yaml` with nested object structures f...\n\n**ConfigSchema defines Zod validation schema for `.agents-reverse-engineer/config.yaml` with nested object structures for exclusion rules, discovery options, output formatting, and AI service configuration, enforcing type safety and providing default values for all fields.**\n\n## Schema Structure\n\nConfigSchema is the root schema composed of four nested schemas: ExcludeSchema, OptionsSchema, OutputSchema, and AISchema. Each sub-schema has `.default({})` fallback enabling empty object validation.\n\n## Exported Schemas\n\n**ExcludeSchema** validates exclusion configuration with three array fields:\n- `patterns: z.array(z.string()).default([...DEFAULT_EXCLUDE_PATTERNS])` — custom glob patterns for file exclusion\n- `vendorDirs: z.array(z.string()).default([...DEFAULT_VENDOR_DIRS])` — vendor directory names to skip\n- `binaryExtensions: z.array(z.string()).default([...DEFAULT_BINARY_EXTENSIONS])` — binary file extension list\n\n**OptionsSchema** validates discovery behavior with two fields:\n- `followSymlinks: z.boolean().default(false)` — symbolic link traversal toggle\n- `maxFileSize: z.number().positive().default(DEFAULT_MAX_FILE_SIZE)` — file size threshold in bytes for skipping large files\n\n**OutputSchema** validates terminal output formatting:\n- `colors: z.boolean().default(true)` — ANSI color code toggle for CLI output\n\n**AISchema** validates AI service configuration with six fields:\n- `backend: z.enum(['claude', 'gemini', 'opencode', 'auto']).default('auto')` — AI CLI backend selection with auto-detection\n- `model: z.string().default('sonnet')` — backend-specific model identifier\n- `timeoutMs: z.number().positive().default(300_000)` — subprocess timeout (5 minutes default)\n- `maxRetries: z.number().min(0).default(3)` — exponential backoff retry limit\n- `concurrency: z.number().min(1).max(20).default(getDefaultConcurrency)` — worker pool size (1-20 range, auto-detected default)\n- `telemetry: z.object({ keepRuns: z.number().min(0).default(50) }).default({})` — nested schema for run log retention count\n\n**ConfigSchema** combines all sub-schemas:\n```typescript\nz.object({\n  exclude: ExcludeSchema,\n  options: OptionsSchema,\n  output: OutputSchema,\n  ai: AISchema,\n}).default({})\n```\n\n## Exported Types\n\n**Config** = `z.infer<typeof ConfigSchema>` — inferred TypeScript type for full configuration object\n\n**ExcludeConfig** = `z.infer<typeof ExcludeSchema>` — inferred type for `exclude` section\n\n**OptionsConfig** = `z.infer<typeof OptionsSchema>` — inferred type for `options` section\n\n**OutputConfig** = `z.infer<typeof OutputSchema>` — inferred type for `output` section\n\n**AIConfig** = `z.infer<typeof AISchema>` — inferred type for `ai` section\n\n## Dependencies\n\nImports DEFAULT_VENDOR_DIRS, DEFAULT_BINARY_EXTENSIONS, DEFAULT_MAX_FILE_SIZE, DEFAULT_EXCLUDE_PATTERNS, and getDefaultConcurrency from `./defaults.js` for default value arrays and computation.\n\n## Validation Behavior\n\nAll schemas use `.default()` chaining enabling partial configuration parsing: `ConfigSchema.parse({})` returns fully populated Config object with all defaults applied. Spread operator `[...DEFAULT_VENDOR_DIRS]` creates shallow copies preventing array mutation across parse invocations.\n\n## Import Map (verified — use these exact paths)\n\nloader.ts:\n  ../orchestration/trace.js → ITraceWriter (type)\n\n## Project Directory Structure\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>",
      "systemPrompt": "You are generating an AGENTS.md file — a directory-level overview for AI coding assistants.\n\nCRITICAL: Output ONLY the raw markdown content. No code fences, no preamble, no explanations, no conversational text. Your entire response IS the AGENTS.md file content.\n\nOUTPUT FORMAT:\n- First line MUST be exactly: <!-- Generated by agents-reverse-engineer -->\n- Use a # heading with the directory name\n- Write a one-paragraph purpose statement for the directory\nusage\nADAPTIVE SECTIONS:\nAnalyze the directory contents and choose the most relevant sections. Do NOT use a fixed template. Instead, select sections that best document this specific directory for an AI that needs to reconstruct or extend the project.\n\nConsider these section types (choose what applies):\n- **Contents**: Group files by purpose/category under ## headings. For each file: markdown link [filename](./filename) and a one-line description.\n- **Subdirectories**: If subdirectories exist, list them with links [dirname/](./dirname/) and brief summaries.\n- **Architecture / Data Flow**: If files form a pipeline, request/response chain, or layered architecture, document it.\n- **Stack**: If this is a package root (has package.json, Cargo.toml, go.mod, etc.), document the technology stack, key scripts, and entry points.\n- **Structure**: If the directory layout follows a convention (feature-sliced, domain-driven, MVC, etc.), document it.\n- **Patterns**: If files share recurring design patterns (factory, strategy, middleware, barrel re-export), name and document them.\n- **Configuration**: If the directory contains config files, schemas, or environment definitions, document the config surface area.\n- **API Surface**: If the directory exports a public API (barrel index, route definitions, SDK), document the interface contract.\n- **File Relationships**: How files collaborate, depend on each other, or share state.\n- **Behavioral Contracts**: If files contain regex patterns, format specifications, magic constants, or template strings that define observable behavior, collect them in a dedicated section. Preserve verbatim patterns from file summaries — do NOT paraphrase regex into prose. This section is MANDATORY when file summaries contain behavioral artifacts.\n- **Reproduction-Critical Constants**: If file summaries reference annex files (via ## Annex References sections), list them with links. Example: \"Full prompt template text: [templates.ts.annex.md](./prompts/templates.ts.annex.md)\". Do NOT reproduce annex content in AGENTS.md — just link to it.\n\nChoose any relevant sections or create your own based on the directory contents. The goal is to provide a comprehensive overview that captures the essence of the directory's role in the project and how its files work together, with a focus on what an AI coding assistant would need to know to effectively interact with this code.\n\nSCOPE:\n- AGENTS.md is a NAVIGATIONAL INDEX — help an AI find the right file quickly\n- Focus on: what each file does, how files relate, directory-level patterns\n- Do NOT reproduce full architecture sections — those belong in the root CLAUDE.md\n\nPATH ACCURACY (MANDATORY):\n- When referencing files or modules outside this directory, use ONLY paths from the \"Import Map\" section\n- Do NOT invent, rename, or guess module paths — if a path isn't in the Import Map, don't reference it\n- Use the exact directory names from \"Project Directory Structure\" — do NOT rename directories\n  (e.g., if the directory is called \"cli\", write \"src/cli/\", NOT \"src/commands/\")\n- Cross-module references must use the specifier format from actual import statements\n  (e.g., \"../generation/writers/sum.js\", NOT \"../fs/sum-file.js\")\n- If you are unsure about a path, omit the cross-reference rather than guessing\n\nCONSISTENCY (MANDATORY):\n- Do not contradict yourself within the same document\n- If you describe a technique (e.g., \"regex-based\"), do not call it something else later (e.g., \"AST-based\")\n- When stating version numbers, engines, or config fields, use ONLY values present in the file summaries\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this directory\", \"this module\", \"provides\", \"responsible for\", \"is used to\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Per-file descriptions in Contents sections: 1-2 sentences maximum. Reference key symbols but do not reproduce full summaries.\n- Behavioral contracts (regex patterns, format specs, constants) belong in a separate Behavioral Contracts section, not in per-file descriptions.\n- Annex file references: link to .annex.md files, do not inline their content. One line per annex reference.\n- Subdirectory descriptions: 1-2 sentences maximum. Capture the directory's role, not its full contents.\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- Key exported symbols from file summaries MUST appear in the directory overview\n- Preserve exact casing of identifiers\n\nUSER NOTES:\n- If \"User Notes\" are provided in the prompt, they contain user-defined instructions that will be automatically prepended to your output\n- Do NOT repeat or paraphrase user notes in your generated content — they are included separately\n- You may reference information from user notes for context",
      "response": "<!-- Generated by agents-reverse-engineer -->\n\n# src/config\n\nConfiguration loading, validation, and default value computation for `.agents-reverse-engineer/config.yaml`, including Zod schema enforcement, platform-adaptive concurrency calculation, and annotated YAML file generation with memory-bounded worker pool sizing.\n\n## Contents\n\n### Schema Definition\n\n**[schema.ts](./schema.ts)** — `ConfigSchema` root validator composed of `ExcludeSchema` (patterns/vendorDirs/binaryExtensions arrays), `OptionsSchema` (followSymlinks/maxFileSize), `OutputSchema` (colors), `AISchema` (backend/model/timeoutMs/maxRetries/concurrency/telemetry). Exports inferred types: `Config`, `ExcludeConfig`, `OptionsConfig`, `OutputConfig`, `AIConfig`. All schemas chain `.default()` enabling partial parse with full default population.\n\n**[defaults.ts](./defaults.ts)** — Exports `DEFAULT_VENDOR_DIRS` (18 directories: node_modules, .git, dist, build, target, .next, __pycache__, venv, .cargo, .gradle, .agents-reverse-engineer, .agents, .planning, .claude, .opencode, .gemini), `DEFAULT_EXCLUDE_PATTERNS` (lock files, dotfiles, logs, AI-generated docs, *.sum files), `DEFAULT_BINARY_EXTENSIONS` (26 extensions: images, archives, executables, media, documents, fonts, bytecode), `DEFAULT_MAX_FILE_SIZE` (1048576 bytes). `getDefaultConcurrency()` computes `clamp(cores × 5, MIN=2, min(memCap, MAX=20))` where `memCap = floor((totalMemGB × 0.5) / 0.512)` enforces memory constraint allocating 50% RAM divided by 512MB subprocess heap limit.\n\n**[loader.ts](./loader.ts)** — `loadConfig(root, options?)` reads `config.yaml` from `.agents-reverse-engineer/`, parses via `yaml.parse()`, validates with `ConfigSchema`, returns defaults on `ENOENT`, throws `ConfigError` wrapping `ZodError` with formatted issue paths. Emits `config:loaded` trace events with configPath/model/concurrency fields. `writeDefaultConfig(root)` creates annotated YAML with five comment-delimited sections (exclusions, discovery options, output formatting, AI service), interpolates defaults via spread operators, quotes patterns containing YAML metacharacters `[*{}\\[\\]?,:#&!|>'\"%@\\`]` via `yamlScalar()`, comments out concurrency line showing override syntax.\n\n## Configuration Structure\n\nThe config file divides into four sections:\n\n- **exclude**: File/directory exclusion rules with glob patterns, vendor directory names, binary extension list\n- **options**: Discovery behavior toggles (followSymlinks) and thresholds (maxFileSize in bytes)\n- **output**: Terminal formatting preferences (ANSI colors)\n- **ai**: AI service configuration (backend selection, model override, subprocess timeout, retry limits, worker pool concurrency, telemetry retention)\n\n## Memory-Aware Concurrency\n\n`getDefaultConcurrency()` implements memory-bounded worker pool sizing preventing RAM exhaustion where `cores × 5` spawns too many 512MB subprocesses. Computes `memCap = floor((totalMemGB × 0.5) / 0.512)` limiting concurrent subprocesses to fit within 50% memory budget, applies `Math.min(computed, memCap, MAX_CONCURRENCY)` respecting memory constraint alongside schema maximum. Fallback uses `Infinity` memCap when `totalMemGB ≤ 1` to avoid division-by-zero on resource-constrained systems.\n\n## Behavioral Contracts\n\n### Schema Validation Constraints\n\n- AISchema concurrency: `z.number().min(1).max(20)` enforces 1-20 worker pool range\n- OptionsSchema maxFileSize: `z.number().positive()` rejects zero/negative thresholds\n- AISchema backend: `z.enum(['claude', 'gemini', 'opencode', 'auto'])` limits backend values\n- AISchema telemetry.keepRuns: `z.number().min(0)` allows zero for unlimited retention\n\n### YAML Metacharacter Pattern\n\n`yamlScalar()` tests input against `/[*{}\\[\\]?,:#&!|>'\"%@\\`]/` pattern, double-quotes and backslash-escapes `\\` → `\\\\`, `\"` → `\\\"` when matched.\n\n### Default Formulas\n\n- Concurrency: `clamp(cores × 5, 2, min(floor((totalMemGB × 0.5) / 0.512), 20))`\n- Memory cap: `floor((os.totalmem() / 1024³ × 0.5) / 0.512)` concurrent processes\n- Timeout default: `300_000` milliseconds (5 minutes)\n\n### Error Message Format\n\nConfigError validation failures: `\"ai.concurrency: Expected number\"` (field path colon-separated from Zod issue message, joined by newlines for multiple issues).\n\n### Trace Event Schema\n\n`config:loaded` payload: `{ type: 'config:loaded', configPath: string, model: string | null, concurrency: number }` emitted twice per `loadConfig()` call (once for file path via `path.relative(root, configPath)`, once for defaults with literal `'(defaults)'`).\n\n## File Relationships\n\n- **schema.ts** → **defaults.ts**: Imports `DEFAULT_VENDOR_DIRS`, `DEFAULT_BINARY_EXTENSIONS`, `DEFAULT_MAX_FILE_SIZE`, `DEFAULT_EXCLUDE_PATTERNS`, `getDefaultConcurrency` for `.default()` chaining\n- **loader.ts** → **schema.ts**: Imports `ConfigSchema` for parse, `Config` type for return annotation\n- **loader.ts** → **defaults.ts**: Imports all defaults for YAML interpolation in `writeDefaultConfig()`\n- **loader.ts** → **../orchestration/trace.ts**: Imports `ITraceWriter` for `config:loaded` event emission",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 1462,
      "cacheReadTokens": 17587,
      "cacheCreationTokens": 12620,
      "latencyMs": 48877,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": []
    },
    {
      "timestamp": "2026-02-09T17:48:11.681Z",
      "prompt": "Generate AGENTS.md for directory: \"src/update\" (update)\n\n## File Summaries (4 files)\n\n### index.ts\n**Purpose:** Barrel module exporting UpdateOrchestrator, orphan cleanup utilities, and TypeScript interfaces for incremental docum...\n\n**Barrel module exporting UpdateOrchestrator, orphan cleanup utilities, and TypeScript interfaces for incremental documentation updates via hash-based change detection and affected directory recomputation.**\n\n## Exported Types\n\n- `UpdateOrchestrator` — Class orchestrating incremental update workflow (from `./orchestrator.js`)\n- `createUpdateOrchestrator` — Factory function returning configured UpdateOrchestrator instance\n- `UpdatePlan` — Interface describing files to analyze, skip, and affected directories\n- `UpdateOptions` — Configuration for update execution (includes `uncommitted` flag for git working tree changes)\n- `UpdateResult` — Result summary with counts of regenerated files and directories\n- `UpdateProgress` — Progress event payload for streaming updates\n- `CleanupResult` — Result of orphan cleanup operations with deleted file/directory counts\n\n## Exported Functions\n\n- `cleanupOrphans(orphanSumFiles: string[], orphanAgentsMdFiles: string[]): Promise<CleanupResult>` — Deletes `.sum` files for removed sources and `AGENTS.md` files from directories with no remaining sources\n- `cleanupEmptyDirectoryDocs(affectedDirs: string[], projectRoot: string): Promise<void>` — Removes `AGENTS.md` files from directories that no longer contain source files or child directories with documentation\n- `getAffectedDirectories(changedFiles: string[], projectRoot: string): string[]` — Computes set of directories requiring `AGENTS.md` regeneration by walking parent directories of changed files\n\n## Module Organization\n\nRe-exports from three internal modules:\n- `./orchestrator.js` — UpdateOrchestrator class and factory, UpdatePlan interface\n- `./orphan-cleaner.js` — Cleanup utilities for stale artifacts and affected directory computation\n- `./types.js` — UpdateOptions, UpdateResult, UpdateProgress, CleanupResult interfaces\n\n## Integration Points\n\nUpdateOrchestrator coordinates:\n- `src/change-detection/` — SHA-256 hash comparison via `content_hash` in `.sum` YAML frontmatter\n- `src/generation/` — Phase 1 pool execution for `filesToAnalyze`, sequential AGENTS.md regeneration for `affectedDirs`\n- `src/orchestration/` — Worker pool and progress reporter for parallel `.sum` regeneration\n- `src/discovery/` — File discovery results for orphan detection\n\nOrphan cleanup removes `.sum` files when source files deleted/renamed and `AGENTS.md` files when directories become empty after updates.\n### orchestrator.ts\n**Purpose:** UpdateOrchestrator coordinates incremental documentation updates via SHA-256 content hash comparison stored in .sum f...\n\n**UpdateOrchestrator coordinates incremental documentation updates via SHA-256 content hash comparison stored in .sum file YAML frontmatter, orchestrating file change detection, orphan cleanup, and affected directory tracking without requiring git diff parsing.**\n\n## Exported Interface\n\n**UpdateOrchestrator** class constructor accepts `Config`, `projectRoot: string`, `options?: { tracer?: ITraceWriter; debug?: boolean }`. Exposes methods:\n\n- `checkPrerequisites(): Promise<void>` — throws if `projectRoot` is not a git repository via `isGitRepo()`\n- `preparePlan(options?: UpdateOptions): Promise<UpdatePlan>` — discovers all source files via `runDiscovery()`, reads existing `.sum` files via `readSumFile()`, compares stored `content_hash` from YAML frontmatter against `computeContentHash()` output, returns `UpdatePlan` with `filesToAnalyze` (added/modified), `filesToSkip` (unchanged hashes), `cleanup` (orphaned `.sum` files), `affectedDirs` (sorted by depth descending), `currentCommit`, `isFirstRun` flag\n- `recordFileAnalyzed(relativePath: string, contentHash: string, currentCommit: string): Promise<void>` — no-op, kept for API compatibility (hash stored in `.sum` frontmatter)\n- `removeFileState(relativePath: string): Promise<void>` — no-op, kept for API compatibility\n- `recordRun(commitHash: string, filesAnalyzed: number, filesSkipped: number): Promise<number>` — no-op returning `0`, kept for API compatibility\n- `getLastRun(): Promise<undefined>` — returns `undefined` (no run history in frontmatter mode), kept for API compatibility\n- `isFirstRun(): Promise<boolean>` — checks if any `.sum` files exist by calling `preparePlan({ dryRun: true })`\n- `close(): void` — no-op, kept for API compatibility\n\n**createUpdateOrchestrator** factory function returns new `UpdateOrchestrator` instance.\n\n## UpdatePlan Structure\n\n`UpdatePlan` interface defines:\n- `filesToAnalyze: FileChange[]` — files with hash mismatches or missing `.sum` files, status `'added'` or `'modified'`\n- `filesToSkip: string[]` — relative paths where stored hash matches current `computeContentHash()` result\n- `cleanup: CleanupResult` — output from `cleanupOrphans()` with `.sum` files to delete\n- `affectedDirs: string[]` — unique directories needing `AGENTS.md` regeneration via `getAffectedDirectories()`, sorted by `path.sep` depth descending (deepest first)\n- `baseCommit: string` — not used in frontmatter mode, kept for compatibility, set to `currentCommit`\n- `currentCommit: string` — SHA from `getCurrentCommit()`\n- `isFirstRun: boolean` — true when `filesToSkip.length === 0 && filesToAnalyze.length > 0`\n\n## Change Detection Algorithm\n\n`preparePlan()` executes:\n\n1. Calls `checkPrerequisites()` to verify git repository\n2. Obtains `currentCommit` via `getCurrentCommit(projectRoot)`\n3. Discovers all source files via `runDiscovery()`, converts absolute paths to relative via `path.relative(projectRoot, f)`\n4. Iterates each file:\n   - Computes `sumPath` via `getSumPath(filePath)`\n   - Adds `sumPath` to `seenSumFiles` set for orphan detection\n   - Calls `readSumFile(sumPath)` returning `SumFileContent | null`\n   - If no `.sum` exists, adds to `filesToAnalyze` with `status: 'added'`\n   - If `.sum` exists, calls `computeContentHash(filePath)` and compares against `sumContent.contentHash`\n   - Hash mismatch or missing hash → adds to `filesToAnalyze` with `status: 'modified'`\n   - Hash match → adds to `filesToSkip`\n5. Calls `cleanupOrphans(projectRoot, deletedOrRenamed, options.dryRun ?? false)` returning `CleanupResult`\n6. Calls `getAffectedDirectories(filesToAnalyze)` returning `Set<string>`, converts to sorted array via depth descending comparator\n7. Emits `plan:created` trace event with `fileCount`, `taskCount: filesToAnalyze.length + affectedDirs.length`\n\n## Trace Event Emissions\n\n`preparePlan()` emits:\n- `phase:start` with `phase: 'update-plan-creation'`, `taskCount: 0`, `concurrency: 1`\n- `plan:created` with `planType: 'update'`, `fileCount: filesToAnalyze.length`, `taskCount: filesToAnalyze.length + affectedDirs.length`\n- `phase:end` with `phase: 'update-plan-creation'`, `durationMs` computed via `process.hrtime.bigint()` delta, `tasksCompleted: 1`, `tasksFailed: 0`\n\n## Integration Points\n\nDepends on:\n- `discoverFiles` from `../discovery/run.js` returning `FilterResult` with `.included: string[]` absolute paths\n- `readSumFile` from `../generation/writers/sum.js` parsing YAML frontmatter, returning `SumFileContent` with `contentHash: string`\n- `getSumPath` from `../generation/writers/sum.js` computing `.sum` file path from source file path\n- `computeContentHash` from `../change-detection/index.js` returning SHA-256 hex string\n- `cleanupOrphans` from `./orphan-cleaner.js` accepting `projectRoot`, `deletedOrRenamed: FileChange[]`, `dryRun: boolean`, returning `CleanupResult`\n- `getAffectedDirectories` from `./orphan-cleaner.js` returning `Set<string>` of unique parent directories\n\n## Debug Logging\n\nWhen `debug: true`, writes to `console.error` via `pc.dim()`:\n- `[debug] Creating update plan with change detection...`\n- `[debug] Git commit: <currentCommit.slice(0, 7)>`\n- `[debug] Discovering files...`\n- `[debug] Change detection: N changed, N unchanged, N orphaned`\n- `[debug] Affected directories: N`\n\n## API Compatibility No-Ops\n\nMethods retained for backward compatibility but perform no operations in frontmatter-based mode:\n- `recordFileAnalyzed` — hash stored in `.sum` YAML frontmatter by generation phase\n- `removeFileState` — `.sum` file cleanup handled by `cleanupOrphans()`\n- `recordRun` — no run history database in frontmatter mode\n- `getLastRun` — returns `undefined`, no run history available\n- `close` — no resources to dispose\n\n## Error Handling\n\n`checkPrerequisites()` throws `Error` with message `'Not a git repository: <projectRoot>\\nThe update command requires a git repository for change detection.'` when `isGitRepo()` returns `false`.\n\nFile iteration in `preparePlan()` catches exceptions from `readSumFile()` or `computeContentHash()` and adds file to `filesToSkip` on error.\n### orphan-cleaner.ts\n**Purpose:** orphan-cleaner.ts removes stale `.sum`, `.annex.md`, and `AGENTS.md` files after source file deletions or renames dur...\n\n**orphan-cleaner.ts removes stale `.sum`, `.annex.md`, and `AGENTS.md` files after source file deletions or renames during incremental updates.**\n\n## Exported Functions\n\n**`cleanupOrphans(projectRoot: string, changes: FileChange[], dryRun: boolean = false): Promise<CleanupResult>`** — orchestrates orphan cleanup by processing deleted and renamed files from `changes` array, deleting corresponding `.sum` and `.annex.md` files via `deleteIfExists()`, collecting affected directories via `path.dirname()` traversal, invoking `cleanupEmptyDirectoryDocs()` for each affected directory, and returning `CleanupResult` with `deletedSumFiles` and `deletedAgentsMd` arrays.\n\n**`cleanupEmptyDirectoryDocs(dirPath: string, dryRun: boolean = false): Promise<boolean>`** — reads directory entries via `readdir()`, determines if directory contains source files by filtering out hidden files (starting with `.`), `.sum` and `.annex.md` extensions, and entries in `GENERATED_FILES` set, deletes `AGENTS.md` via `deleteIfExists()` if no source files remain, returns `true` if deletion occurred.\n\n**`getAffectedDirectories(changes: FileChange[]): Set<string>`** — extracts unique directory paths requiring `AGENTS.md` regeneration by iterating `changes` array (skipping `status === 'deleted'` entries), walking parent directories via repeated `path.dirname()` calls until reaching `.` or absolute path, collecting all parent paths into `Set<string>`, including root directory `.`.\n\n## Cleanup Logic\n\n**Deletion trigger paths:** Extracts `change.path` for `status === 'deleted'` and `change.oldPath` for `status === 'renamed'` into `pathsToClean` array. For each path, constructs `.sum` path via `path.join(projectRoot, \\`${relativePath}.sum\\`)` and `.annex.md` path via `path.join(projectRoot, \\`${relativePath}.annex.md\\`)`, appends relative suffix (`.sum` or `.annex.md`) to `result.deletedSumFiles` on successful deletion.\n\n**Directory emptiness criteria:** `cleanupEmptyDirectoryDocs()` considers directory empty if no entries pass filter: not starting with `.`, not ending with `.sum` or `.annex.md`, not contained in `GENERATED_FILES` set (`['AGENTS.md', 'CLAUDE.md']`). Deletes `AGENTS.md` only when zero source files remain.\n\n**Affected directory traversal:** `getAffectedDirectories()` walks upward from each changed file's `path.dirname()` until `dir === '.'` or `path.isAbsolute(dir)` returns `true`, ensuring all intermediate directories and root (`.`) appear in returned `Set<string>`.\n\n## File System Operations\n\n**`deleteIfExists(filePath: string, dryRun: boolean): Promise<boolean>`** — internal helper calling `stat()` to verify file existence, executing `unlink()` if `dryRun === false`, returning `true` on successful deletion or when file exists in dry-run mode, returning `false` on stat error (file does not exist). All file system errors caught and interpreted as nonexistence.\n\n## Types\n\n**`FileChange`** — imported from `../change-detection/types.js`, discriminated union with `status: 'added' | 'modified' | 'deleted' | 'renamed'`, `path: string`, optional `oldPath?: string` (present when `status === 'renamed'`).\n\n**`CleanupResult`** — imported from `./types.js`, object shape `{ deletedSumFiles: string[], deletedAgentsMd: string[] }` accumulating relative paths of deleted artifacts.\n\n## Constants\n\n**`GENERATED_FILES`** — `Set<string>` containing `'AGENTS.md'` and `'CLAUDE.md'`, used by `cleanupEmptyDirectoryDocs()` to exclude generated documentation files from source file detection, preventing false positives when determining directory emptiness.\n### types.ts\n**Purpose:** Defines TypeScript interfaces for incremental update workflow including change results, cleanup tracking, progress ca...\n\n**Defines TypeScript interfaces for incremental update workflow including change results, cleanup tracking, progress callbacks, and dry-run configuration.**\n\n## Exported Interfaces\n\n`CleanupResult` tracks orphan cleanup outcomes with two fields:\n- `deletedSumFiles: string[]` — paths of removed `.sum` files\n- `deletedAgentsMd: string[]` — paths of removed `AGENTS.md` files from empty directories\n\n`UpdateOptions` configures update command behavior:\n- `includeUncommitted?: boolean` — merge staged and working directory changes into detection\n- `dryRun?: boolean` — preview changes without writing files\n\n`UpdateResult` aggregates update execution outcomes:\n- `analyzedFiles: string[]` — files regenerated due to added/modified status\n- `skippedFiles: string[]` — files with matching `content_hash` in frontmatter\n- `cleanup: CleanupResult` — orphan deletion summary\n- `regeneratedDirs: string[]` — directories whose `AGENTS.md` was rewritten\n- `baseCommit: string` — git SHA at update start\n- `currentCommit: string` — git SHA at update end\n- `dryRun: boolean` — whether execution was preview-only\n\n`UpdateProgress` provides lifecycle callbacks for streaming progress:\n- `onFileStart?: (path: string, status: 'analyzing' | 'skipping') => void` — invoked before processing file\n- `onFileDone?: (path: string, status: 'analyzed' | 'skipped' | 'error') => void` — invoked after file completion\n- `onCleanup?: (path: string, type: 'sum' | 'agents-md') => void` — invoked when orphan deleted\n- `onDirRegenerate?: (path: string) => void` — invoked when directory `AGENTS.md` regenerated\n\n## Dependencies\n\nImports `FileChange` from `../change-detection/types.js` to represent added/modified/deleted/renamed file states with `status` discriminator and optional `oldPath` for renames.\n\n## Import Map (verified — use these exact paths)\n\norchestrator.ts:\n  ../config/schema.js → Config (type)\n  ../change-detection/index.js → isGitRepo, getCurrentCommit, computeContentHash, type FileChange\n  ../generation/writers/sum.js → readSumFile, getSumPath\n  ../discovery/run.js → discoverFiles\n  ../orchestration/trace.js → ITraceWriter (type)\n\norphan-cleaner.ts:\n  ../change-detection/types.js → FileChange (type)\n\ntypes.ts:\n  ../change-detection/types.js → FileChange (type)\n\n## Project Directory Structure\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>",
      "systemPrompt": "You are generating an AGENTS.md file — a directory-level overview for AI coding assistants.\n\nCRITICAL: Output ONLY the raw markdown content. No code fences, no preamble, no explanations, no conversational text. Your entire response IS the AGENTS.md file content.\n\nOUTPUT FORMAT:\n- First line MUST be exactly: <!-- Generated by agents-reverse-engineer -->\n- Use a # heading with the directory name\n- Write a one-paragraph purpose statement for the directory\nusage\nADAPTIVE SECTIONS:\nAnalyze the directory contents and choose the most relevant sections. Do NOT use a fixed template. Instead, select sections that best document this specific directory for an AI that needs to reconstruct or extend the project.\n\nConsider these section types (choose what applies):\n- **Contents**: Group files by purpose/category under ## headings. For each file: markdown link [filename](./filename) and a one-line description.\n- **Subdirectories**: If subdirectories exist, list them with links [dirname/](./dirname/) and brief summaries.\n- **Architecture / Data Flow**: If files form a pipeline, request/response chain, or layered architecture, document it.\n- **Stack**: If this is a package root (has package.json, Cargo.toml, go.mod, etc.), document the technology stack, key scripts, and entry points.\n- **Structure**: If the directory layout follows a convention (feature-sliced, domain-driven, MVC, etc.), document it.\n- **Patterns**: If files share recurring design patterns (factory, strategy, middleware, barrel re-export), name and document them.\n- **Configuration**: If the directory contains config files, schemas, or environment definitions, document the config surface area.\n- **API Surface**: If the directory exports a public API (barrel index, route definitions, SDK), document the interface contract.\n- **File Relationships**: How files collaborate, depend on each other, or share state.\n- **Behavioral Contracts**: If files contain regex patterns, format specifications, magic constants, or template strings that define observable behavior, collect them in a dedicated section. Preserve verbatim patterns from file summaries — do NOT paraphrase regex into prose. This section is MANDATORY when file summaries contain behavioral artifacts.\n- **Reproduction-Critical Constants**: If file summaries reference annex files (via ## Annex References sections), list them with links. Example: \"Full prompt template text: [templates.ts.annex.md](./prompts/templates.ts.annex.md)\". Do NOT reproduce annex content in AGENTS.md — just link to it.\n\nChoose any relevant sections or create your own based on the directory contents. The goal is to provide a comprehensive overview that captures the essence of the directory's role in the project and how its files work together, with a focus on what an AI coding assistant would need to know to effectively interact with this code.\n\nSCOPE:\n- AGENTS.md is a NAVIGATIONAL INDEX — help an AI find the right file quickly\n- Focus on: what each file does, how files relate, directory-level patterns\n- Do NOT reproduce full architecture sections — those belong in the root CLAUDE.md\n\nPATH ACCURACY (MANDATORY):\n- When referencing files or modules outside this directory, use ONLY paths from the \"Import Map\" section\n- Do NOT invent, rename, or guess module paths — if a path isn't in the Import Map, don't reference it\n- Use the exact directory names from \"Project Directory Structure\" — do NOT rename directories\n  (e.g., if the directory is called \"cli\", write \"src/cli/\", NOT \"src/commands/\")\n- Cross-module references must use the specifier format from actual import statements\n  (e.g., \"../generation/writers/sum.js\", NOT \"../fs/sum-file.js\")\n- If you are unsure about a path, omit the cross-reference rather than guessing\n\nCONSISTENCY (MANDATORY):\n- Do not contradict yourself within the same document\n- If you describe a technique (e.g., \"regex-based\"), do not call it something else later (e.g., \"AST-based\")\n- When stating version numbers, engines, or config fields, use ONLY values present in the file summaries\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this directory\", \"this module\", \"provides\", \"responsible for\", \"is used to\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Per-file descriptions in Contents sections: 1-2 sentences maximum. Reference key symbols but do not reproduce full summaries.\n- Behavioral contracts (regex patterns, format specs, constants) belong in a separate Behavioral Contracts section, not in per-file descriptions.\n- Annex file references: link to .annex.md files, do not inline their content. One line per annex reference.\n- Subdirectory descriptions: 1-2 sentences maximum. Capture the directory's role, not its full contents.\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- Key exported symbols from file summaries MUST appear in the directory overview\n- Preserve exact casing of identifiers\n\nUSER NOTES:\n- If \"User Notes\" are provided in the prompt, they contain user-defined instructions that will be automatically prepended to your output\n- Do NOT repeat or paraphrase user notes in your generated content — they are included separately\n- You may reference information from user notes for context",
      "response": "<!-- Generated by agents-reverse-engineer -->\n\n# src/update\n\nOrchestrates incremental documentation updates via SHA-256 content hash comparison against `.sum` YAML frontmatter, deleting orphaned artifacts and computing affected directory sets without requiring git diff parsing.\n\n## Contents\n\n**[index.ts](./index.ts)** — Barrel module exporting `UpdateOrchestrator` class, factory `createUpdateOrchestrator()`, cleanup utilities `cleanupOrphans()`, `cleanupEmptyDirectoryDocs()`, `getAffectedDirectories()`, and interfaces `UpdatePlan`, `UpdateOptions`, `UpdateResult`, `UpdateProgress`, `CleanupResult`.\n\n**[orchestrator.ts](./orchestrator.ts)** — `UpdateOrchestrator` class coordinates change detection by reading stored `content_hash` from `.sum` frontmatter via `readSumFile()`, comparing against `computeContentHash()` output, returning `UpdatePlan` with `filesToAnalyze` (hash mismatch), `filesToSkip` (match), `cleanup` (orphans), `affectedDirs` (sorted depth descending). Methods: `preparePlan()`, `checkPrerequisites()`, `isFirstRun()`. Emits `plan:created` trace events.\n\n**[orphan-cleaner.ts](./orphan-cleaner.ts)** — `cleanupOrphans()` deletes stale `.sum`/`.annex.md` files for deleted/renamed sources via `deleteIfExists()`, `cleanupEmptyDirectoryDocs()` removes `AGENTS.md` from directories with zero source files (excludes `GENERATED_FILES` set and dotfiles), `getAffectedDirectories()` walks parent directories via `path.dirname()` until `.` or absolute path, returning `Set<string>` of directories requiring regeneration.\n\n**[types.ts](./types.ts)** — Interfaces: `CleanupResult` (`deletedSumFiles`, `deletedAgentsMd` arrays), `UpdateOptions` (`includeUncommitted`, `dryRun` flags), `UpdateResult` (`analyzedFiles`, `skippedFiles`, `cleanup`, `regeneratedDirs`, `baseCommit`, `currentCommit`, `dryRun`), `UpdateProgress` callbacks (`onFileStart`, `onFileDone`, `onCleanup`, `onDirRegenerate`).\n\n## Update Workflow\n\n1. **Change Detection:** `preparePlan()` discovers files via `runDiscovery()` from `src/discovery/`, reads `.sum` frontmatter via `readSumFile()` from `src/generation/writers/sum.js`, compares `content_hash` against `computeContentHash()` from `src/change-detection/`, populates `filesToAnalyze` (added/modified) and `filesToSkip` (unchanged).\n\n2. **Orphan Cleanup:** `cleanupOrphans()` processes `FileChange[]` with `status: 'deleted' | 'renamed'`, extracts `path` (deleted) or `oldPath` (renamed), constructs `.sum` and `.annex.md` paths, deletes via `unlink()` unless `dryRun: true`.\n\n3. **Affected Directories:** `getAffectedDirectories()` walks parent chains for all changed files, returns `Set<string>` sorted by `path.sep` depth descending (deepest first), ensuring `AGENTS.md` regeneration propagates upward.\n\n4. **Empty Directory Cleanup:** `cleanupEmptyDirectoryDocs()` filters directory entries excluding dotfiles, `.sum`/`.annex.md` extensions, `GENERATED_FILES` set (`['AGENTS.md', 'CLAUDE.md']`), deletes `AGENTS.md` if zero source files remain.\n\n5. **Execution:** `src/cli/update.ts` invokes `preparePlan()`, executes Phase 1 pool for `filesToAnalyze` via `src/generation/executor.ts`, regenerates `AGENTS.md` sequentially for `affectedDirs` (no post-order traversal required).\n\n## Trace Emissions\n\n`preparePlan()` emits:\n- `phase:start` with `phase: 'update-plan-creation'`, `taskCount: 0`, `concurrency: 1`\n- `plan:created` with `planType: 'update'`, `fileCount: filesToAnalyze.length`, `taskCount: filesToAnalyze.length + affectedDirs.length`\n- `phase:end` with `durationMs` computed via `process.hrtime.bigint()` delta\n\n## Behavioral Contracts\n\n**Depth-descending sort:** `affectedDirs` sorted via `path.relative().split(path.sep).length` comparator (deepest first) ensuring child directories process before parents.\n\n**Orphan path construction:** `.sum` path = `path.join(projectRoot, \\`${relativePath}.sum\\`)`, `.annex.md` path = `path.join(projectRoot, \\`${relativePath}.annex.md\\`)`.\n\n**Directory emptiness filter:** Exclude entries matching `entry.startsWith('.')` OR `entry.endsWith('.sum')` OR `entry.endsWith('.annex.md')` OR `GENERATED_FILES.has(entry)`.\n\n**Upward traversal terminator:** `getAffectedDirectories()` stops when `dir === '.'` OR `path.isAbsolute(dir) === true`.\n\n## API Compatibility No-Ops\n\nMethods retained for backward compatibility but perform no operations in frontmatter-based mode:\n- `recordFileAnalyzed()` — hash stored in `.sum` YAML frontmatter by generation phase\n- `removeFileState()` — `.sum` file cleanup handled by `cleanupOrphans()`\n- `recordRun()` — returns `0`, no run history database in frontmatter mode\n- `getLastRun()` — returns `undefined`, no run history available\n- `close()` — no resources to dispose",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 1427,
      "cacheReadTokens": 17587,
      "cacheCreationTokens": 13127,
      "latencyMs": 29865,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": []
    },
    {
      "timestamp": "2026-02-09T17:47:47.776Z",
      "prompt": "Generate AGENTS.md for directory: \"src/generation\" (generation)\n\n## File Summaries (5 files)\n\n### collector.ts\n**Purpose:** `collector.ts` exports `collectAgentsDocs()` and `collectAnnexFiles()` for recursive traversal of the project tree to...\n\n**`collector.ts` exports `collectAgentsDocs()` and `collectAnnexFiles()` for recursive traversal of the project tree to aggregate `AGENTS.md` and `.annex.md` files into sorted arrays of relative paths and content.**\n\n## Exported Functions\n\n**`collectAgentsDocs(projectRoot: string): Promise<AgentsDocs>`**\nRecursively walks `projectRoot` starting from the top level, collecting all files named exactly `AGENTS.md`. Returns `AgentsDocs` array sorted alphabetically by `relativePath` (computed via `path.relative(projectRoot, filePath)`). Skips directories in `SKIP_DIRS` set. Gracefully handles permission-denied errors by catching exceptions from `readdir()` and `readFile()`, silently omitting unreadable entries.\n\n**`collectAnnexFiles(projectRoot: string): Promise<AgentsDocs>`**\nRecursively walks `projectRoot` collecting all files ending with `.annex.md` (matched via `entry.name.endsWith('.annex.md')`). Uses identical skip logic and error handling as `collectAgentsDocs()`. Returns `AgentsDocs` array sorted alphabetically by `relativePath`.\n\n## Type Definitions\n\n**`AgentsDocs`**\nType alias for `Array<{ relativePath: string; content: string }>`. Represents a collection of documentation files with their project-relative paths and UTF-8 text content.\n\n## Directory Exclusion\n\n**`SKIP_DIRS`**\nImmutable `Set` containing 13 directory names skipped during traversal: `'node_modules'`, `'.git'`, `'.agents-reverse-engineer'`, `'vendor'`, `'dist'`, `'build'`, `'__pycache__'`, `'.next'`, `'venv'`, `'.venv'`, `'target'`, `'.cargo'`, `'.gradle'`. Applied in `entry.isDirectory() && !SKIP_DIRS.has(entry.name)` guard.\n\n## Traversal Algorithm\n\nBoth functions use nested `async function walk(currentDir: string)` implementing depth-first recursive traversal:\n1. Call `readdir(currentDir, { withFileTypes: true })` to get `Dirent[]` entries\n2. For each entry, if directory and not in `SKIP_DIRS`, recurse into `path.join(currentDir, entry.name)`\n3. For file entries, check name match condition (`entry.name === 'AGENTS.md'` or `entry.name.endsWith('.annex.md')`)\n4. Read file via `readFile(filePath, 'utf-8')`, push `{ relativePath, content }` to `results[]`\n5. Catch and ignore all exceptions (both at directory and file level)\n6. After traversal completes, sort `results` via `results.sort((a, b) => a.relativePath.localeCompare(b.relativePath))`\n\n## Integration Points\n\nConsumed by Phase 3 root document synthesis in `src/generation/orchestrator.ts` to aggregate all `AGENTS.md` files into unified corpus for `CLAUDE.md`/`GEMINI.md`/`OPENCODE.md` generation. The `collectAnnexFiles()` function supports annex references mentioned in documentation pipeline (large string constants extracted to companion `.annex.md` files).\n### complexity.ts\n**Purpose:** complexity.ts computes codebase structure metrics from discovered file paths: file count, maximum directory depth, un...\n\n**complexity.ts computes codebase structure metrics from discovered file paths: file count, maximum directory depth, unique directory set, and file list for generation phase cost estimation and complexity-aware orchestration.**\n\n## Exported Interface\n\n`ComplexityMetrics` — structure containing `fileCount: number`, `directoryDepth: number`, `files: string[]`, and `directories: Set<string>` computed from discovered source files.\n\n## Exported Function\n\n`analyzeComplexity(files: string[], projectRoot: string): ComplexityMetrics` — orchestrates complexity analysis by invoking `calculateDirectoryDepth()` and `extractDirectories()`, returning aggregate metrics used by generation orchestrator to estimate Phase 2 directory aggregation workload and log complexity warnings.\n\n## Algorithm Details\n\n`calculateDirectoryDepth(files: string[], projectRoot: string): number` — computes maximum directory nesting by calling `path.relative()` on each file path against `projectRoot`, splitting by `path.sep`, subtracting 1 (file itself not counted), and tracking max depth across all files via `Math.max()`.\n\n`extractDirectories(files: string[]): Set<string>` — extracts unique directory paths by iterating each file, calling `path.dirname()` on file path, then walking upward via repeated `path.dirname(parent)` until `parent === dir` (root reached) or `dir === '.'`, adding each intermediate directory to `Set<string>`.\n\n## Integration Points\n\nCalled by `src/generation/orchestrator.ts` after discovery phase completes, metrics passed to `FileAnalysisOrchestrator` constructor for logging complexity warnings (e.g., \"Large codebase detected: N files across M directories, depth D\"). `directories` set used to validate Phase 2 post-order traversal completeness and detect orphaned `AGENTS.md` files during incremental updates.\n### executor.ts\n**Purpose:** executor.ts builds ExecutionPlan from GenerationPlan with dependency graph, post-order directory traversal sorting (d...\n\n**executor.ts builds ExecutionPlan from GenerationPlan with dependency graph, post-order directory traversal sorting (deepest-first), directory completion tracking via sumFileExists(), and markdown plan formatting for GENERATION-PLAN.md output.**\n\n## Exported Types\n\n**ExecutionTask** represents a single AI processing job with fields:\n- `id: string` — unique identifier with format `\"file:{path}\"`, `\"dir:{path}\"`, or `\"root:{docname}\"`\n- `type: 'file' | 'directory' | 'root-doc'` — discriminates task category\n- `path: string` — relative path to file or directory\n- `absolutePath: string` — resolved absolute path\n- `systemPrompt: string` — AI system prompt (placeholder for dir/root tasks, built at runtime)\n- `userPrompt: string` — AI user prompt (placeholder for dir/root tasks, built at runtime)\n- `dependencies: string[]` — array of task IDs that must complete first\n- `outputPath: string` — destination path for generated content (`.sum` for files, `AGENTS.md` for dirs, root doc name for root)\n- `metadata: { directoryFiles?: string[], depth?: number, packageRoot?: string }` — tracking metadata\n\n**ExecutionPlan** aggregates all tasks with dependency relationships:\n- `projectRoot: string` — absolute project root path\n- `tasks: ExecutionTask[]` — all tasks in execution order\n- `fileTasks: ExecutionTask[]` — Phase 1 file analysis tasks (parallel eligible)\n- `directoryTasks: ExecutionTask[]` — Phase 2 directory aggregation tasks (post-order sorted)\n- `rootTasks: ExecutionTask[]` — Phase 3 root document synthesis tasks (sequential)\n- `directoryFileMap: Record<string, string[]>` — maps directory path to relative file paths\n- `projectStructure?: string` — optional compact directory listing for prompt context\n\n## Core Functions\n\n**buildExecutionPlan(plan: GenerationPlan, projectRoot: string): ExecutionPlan** constructs dependency graph from GenerationPlan:\n1. Populates `directoryFileMap` by extracting `path.dirname()` from each `file.relativePath`\n2. Creates file tasks with `id: \"file:{filePath}\"`, `type: 'file'`, `dependencies: []`, `outputPath: \"{absolutePath}.sum\"`\n3. Sorts file tasks by directory depth descending via `getDirectoryDepth(path.dirname(a.path))` comparison (deepest first for post-order traversal)\n4. Sorts directories by depth descending via `Object.entries(directoryFileMap).sort(([dirA], [dirB]) => getDirectoryDepth(dirB) - getDirectoryDepth(dirA))`\n5. Creates directory tasks with `id: \"dir:{dir}\"`, `type: 'directory'`, `dependencies: fileTaskIds` (all file tasks in directory), `outputPath: \"{dirAbsPath}/AGENTS.md\"`, `metadata: { directoryFiles, depth }`\n6. Creates root tasks with `id: \"root:CLAUDE.md\"`, `type: 'root-doc'`, `dependencies: allDirTaskIds`, `outputPath: \"{projectRoot}/CLAUDE.md\"`\n7. Returns ExecutionPlan with fileTasks/directoryTasks/rootTasks arrays and directoryFileMap\n\n**getDirectoryDepth(dir: string): number** calculates path segment count:\n- Returns `0` for root directory `\".\"`\n- Returns `dir.split(path.sep).length` for non-root directories\n\n**isDirectoryComplete(dirPath: string, expectedFiles: string[], projectRoot: string): Promise<{ complete: boolean; missing: string[] }>** checks if all files have `.sum` outputs:\n- Iterates `expectedFiles` array, resolves absolute path via `path.join(projectRoot, relativePath)`\n- Calls `sumFileExists(absolutePath)` for each file\n- Returns `{ complete: missing.length === 0, missing }` with array of paths lacking `.sum` files\n\n**getReadyDirectories(executionPlan: ExecutionPlan): Promise<string[]>** identifies directories eligible for AGENTS.md generation:\n- Iterates `executionPlan.directoryFileMap` entries\n- Calls `isDirectoryComplete(dir, files, executionPlan.projectRoot)` for each directory\n- Returns array of directory paths where `complete === true`\n\n**formatExecutionPlanAsMarkdown(plan: ExecutionPlan): string** generates GENERATION-PLAN.md content:\n- Writes header with ISO date (`new Date().toISOString().split('T')[0]`) and `plan.projectRoot`\n- Writes summary section with task counts: `plan.tasks.length`, `plan.fileTasks.length`, `plan.directoryTasks.length`, `plan.rootTasks.length`\n- Writes Phase 1 section grouping files by directory, outputting in post-order using `plan.directoryTasks` traversal order\n- Writes Phase 2 section grouping directories by depth (descending), outputting `{dir}/AGENTS.md` with `(root)` suffix for `dir === \".\"`\n- Writes Phase 3 section listing `CLAUDE.md`\n- Returns markdown string with checkbox format: `- [ ] \\`{path}\\``\n\n## Post-Order Traversal Strategy\n\nbuildExecutionPlan() enforces post-order traversal (children before parents) via two sorting operations:\n1. **File tasks**: sorted by `getDirectoryDepth(path.dirname(a.path))` descending so deepest files process first\n2. **Directory tasks**: sorted by `getDirectoryDepth(dirB) - getDirectoryDepth(dirA)` descending so child directories generate AGENTS.md before parent directories\n\nThis ensures child AGENTS.md files exist before parent directory aggregation prompts consume them (referenced in src/generation/prompts/builder.ts buildDirectoryPrompt() via collectAgentsDocs() recursive traversal).\n\n## Integration Points\n\n- **GenerationPlan import** from `./orchestrator.js` consumed by buildExecutionPlan()\n- **sumFileExists()** from `./writers/sum.js` checks for `.sum` file presence in isDirectoryComplete()\n- **Dependency tracking** via `dependencies: string[]` enforces Phase 1 → Phase 2 → Phase 3 sequencing in src/orchestration/runner.ts\n- **Prompt placeholders** for directory/root tasks (`\"Built at runtime by buildRootPrompt()\"`) signal runtime prompt construction in runner.ts Phase 2/3\n\n## Placeholder Prompt Pattern\n\nExecutionTask includes `systemPrompt` and `userPrompt` fields populated with placeholders for directory/root tasks:\n- Directory tasks: `systemPrompt: \"Built at execution time by buildDirectoryPrompt()\"`, `userPrompt: \"Directory \\\"{dir}\\\" — {files.length} files. Prompt populated from .sum files at runtime.\"`\n- Root tasks: `systemPrompt: \"Built at runtime by buildRootPrompt()\"`, `userPrompt: \"Root document — prompt populated from AGENTS.md files at runtime.\"`\n\nThese placeholders exist for plan display (formatExecutionPlanAsMarkdown()) and dependency tracking; actual prompts constructed at execution time in runner.ts via buildDirectoryPrompt() and buildRootPrompt() from src/generation/prompts/builder.ts.\n### orchestrator.ts\n**Purpose:** GenerationOrchestrator coordinates three-phase documentation generation: prepares files by reading content, creates f...\n\n**GenerationOrchestrator coordinates three-phase documentation generation: prepares files by reading content, creates file analysis tasks with prompts from buildFilePrompt, creates directory tasks for AGENTS.md synthesis, computes complexity metrics via analyzeComplexity, and emits trace events for plan creation.**\n\n## Exported Types\n\n**PreparedFile** represents a file ready for analysis with `filePath: string` (absolute), `relativePath: string` (from project root), `content: string` (file text).\n\n**AnalysisTask** discriminated union with `type: 'file' | 'directory'`. File tasks include `systemPrompt?: string` and `userPrompt?: string` set during creation. Directory tasks include `directoryInfo?: { sumFiles: string[], fileCount: number }` with prompts built at execution time by buildDirectoryPrompt (not in this file).\n\n**GenerationPlan** aggregates `files: PreparedFile[]`, `tasks: AnalysisTask[]`, `complexity: ComplexityMetrics`, `projectStructure?: string` (compact directory listing for AI context).\n\n## Class: GenerationOrchestrator\n\nConstructor accepts `config: Config`, `projectRoot: string`, `options?: { tracer?: ITraceWriter, debug?: boolean }`.\n\n**prepareFiles(discoveryResult: DiscoveryResult): Promise<PreparedFile[]>** reads file content via `readFile(filePath, 'utf-8')`, computes relative paths, silently skips unreadable files.\n\n**createFileTasks(files: PreparedFile[], projectStructure?: string): AnalysisTask[]** calls `buildFilePrompt({ filePath, content, projectPlan }, debug)` for each file, returns tasks with `type: 'file'`, `systemPrompt`, `userPrompt` populated.\n\n**createDirectoryTasks(files: PreparedFile[]): AnalysisTask[]** groups files by `path.dirname(relativePath)`, creates tasks with `type: 'directory'`, `directoryInfo.sumFiles` as array of `${relativePath}.sum` paths, `directoryInfo.fileCount` set to group size. Prompts not built here—deferred to execution time.\n\n**createPlan(discoveryResult: DiscoveryResult): Promise<GenerationPlan>** orchestrates plan creation: emits `phase:start` with `phase: 'plan-creation'`, calls `prepareFiles`, `analyzeComplexity`, `buildProjectStructure`, `createFileTasks`, `createDirectoryTasks`, concatenates file and directory tasks, emits `plan:created` with `planType: 'generate'` and `taskCount: tasks.length + 1` (accounting for root CLAUDE.md task added later), clears `PreparedFile.content` to free memory after prompt embedding, emits `phase:end` with `durationMs` computed via `process.hrtime.bigint()` delta.\n\n**buildProjectStructure(files: PreparedFile[]): string** (private) groups files into `Map<dirPath, fileNames[]>`, sorts directories and filenames, formats as indented text: `${dir}/\\n  ${file}` for bird's-eye AI context.\n\n## Factory Function\n\n**createOrchestrator(config: Config, projectRoot: string, options?: { tracer?: ITraceWriter, debug?: boolean }): GenerationOrchestrator** instantiates GenerationOrchestrator with provided parameters.\n\n## Integration Points\n\nImports `buildFilePrompt` from `./prompts/index.js` to construct system/user prompts for file tasks. Imports `analyzeComplexity` from `./complexity.js` to compute `ComplexityMetrics` (directory depth). Imports `ITraceWriter` from `../orchestration/trace.js` for event emission. Imports `Config` from `../config/schema.js` and `DiscoveryResult` from `../types/index.js`.\n\n## Trace Events\n\nEmits `phase:start` with `{ type: 'phase:start', phase: 'plan-creation', taskCount, concurrency: 1 }` at plan start. Emits `plan:created` with `{ type: 'plan:created', planType: 'generate', fileCount, taskCount }` after task creation. Emits `phase:end` with `{ type: 'phase:end', phase: 'plan-creation', durationMs, tasksCompleted: 1, tasksFailed: 0 }` at plan completion.\n\n## Memory Management\n\nAfter `createFileTasks` embeds file content into prompts, `createPlan` clears `PreparedFile.content` fields via `(file as { content: string }).content = ''` to free memory. Comment notes runner re-reads files from disk during execution.\n\n## Debug Logging\n\nWhen `debug: true`, logs to stderr via `console.error(pc.dim(...))` for: file preparation start, complexity analysis depth, plan summary with file/task/directory counts. Uses `picocolors` for dim formatting.\n\n## Task Count Calculation\n\nComment in `createPlan` notes `taskCount: tasks.length + 1` because `buildExecutionPlan()` (external to this file) adds root CLAUDE.md synthesis task after plan creation.\n### types.ts\n**Purpose:** Defines TypeScript interfaces for the documentation generation pipeline's analysis results, metadata extraction, and ...\n\n**Defines TypeScript interfaces for the documentation generation pipeline's analysis results, metadata extraction, and summary configuration.**\n\n## Exported Types\n\n### AnalysisResult\n```typescript\ninterface AnalysisResult {\n  summary: string;\n  metadata: SummaryMetadata;\n}\n```\nRepresents the output from LLM-based file analysis, containing both the generated markdown summary text and structured metadata extracted during analysis.\n\n### SummaryMetadata\n```typescript\ninterface SummaryMetadata {\n  purpose: string;\n  criticalTodos?: string[];\n  relatedFiles?: string[];\n}\n```\nEncapsulates metadata extracted from source files during Phase 1 analysis. The `purpose` field stores the one-line purpose statement. The optional `criticalTodos` array contains only security or breaking issues (not general TODOs). The optional `relatedFiles` array lists tightly coupled sibling files.\n\n### SummaryOptions\n```typescript\ninterface SummaryOptions {\n  targetLength: 'short' | 'standard' | 'detailed';\n  includeCodeSnippets: boolean;\n}\n```\nConfiguration options for controlling summary generation behavior. The `targetLength` discriminated union determines verbosity levels. The `includeCodeSnippets` boolean flag controls whether code examples appear in output summaries.\n\n## Integration Points\n\nThese types are consumed by:\n- `src/generation/executor.ts` — Uses `AnalysisResult` as return type from `AIService.call()`\n- `src/generation/writers/sum.ts` — Serializes `SummaryMetadata` to YAML frontmatter in `.sum` files\n- `src/generation/prompts/builder.ts` — May consume `SummaryOptions` to tailor prompt instructions (implementation-dependent)\n\nThe `SummaryMetadata` structure directly maps to the YAML frontmatter schema documented in CLAUDE.md:\n```yaml\n---\npurpose: [purpose field]\ncritical_todos: [criticalTodos array]\nrelated_files: [relatedFiles array]\n---\n```\n\n## Import Map (verified — use these exact paths)\n\norchestrator.ts:\n  ../config/schema.js → Config (type)\n  ../types/index.js → DiscoveryResult (type)\n  ../orchestration/trace.js → ITraceWriter (type)\n\n## Project Directory Structure\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\n\n## Subdirectories\n\n### prompts/\n<!-- Generated by agents-reverse-engineer -->\n\n# generation/prompts\n\nPrompt construction engine for the three-phase documentation pipeline: `buildFilePrompt()` assembles file analysis prompts with import maps and incremental update sections, `buildDirectoryPrompt()` aggregates child `.sum` files and subdirectory `AGENTS.md` with import maps for directory synthesis, `buildRootPrompt()` collects all `AGENTS.md` files and package.json metadata for root document generation.\n\n## Contents\n\n### Prompt Builders\n\n**[builder.ts](./builder.ts)** — Implements `buildFilePrompt()` with `detectLanguage()` mapping 21 extensions to syntax identifiers, `buildDirectoryPrompt()` aggregating `.sum` files via `readSumFile()` and extracting import maps via `extractDirectoryImports()`, `buildRootPrompt()` calling `collectAgentsDocs()` for project-level synthesis. Substitutes placeholders `{{FILE_PATH}}`, `{{CONTENT}}`, `{{LANG}}`, `{{PROJECT_PLAN_SECTION}}` in user prompts. Switches between fresh generation prompts (`FILE_SYSTEM_PROMPT`, `DIRECTORY_SYSTEM_PROMPT`) and update prompts (`FILE_UPDATE_SYSTEM_PROMPT`, `DIRECTORY_UPDATE_SYSTEM_PROMPT`) based on `existingSum`/`existingAgentsMd` presence. Preserves user content from `AGENTS.local.md` by appending as `## User Notes` section. Debug logging emits template action/metadata pairs to stderr via `picocolors`.\n\n**[templates.ts](./templates.ts)** — Exports six system prompt constants: `FILE_SYSTEM_PROMPT` enforces density rules (every sentence references specific identifiers), anchor term preservation (exact export name casing), behavioral contract extraction (verbatim regex patterns, format strings, magic constants), `FILE_USER_PROMPT` template with `{{FILE_PATH}}`/`{{CONTENT}}` placeholders, `FILE_UPDATE_SYSTEM_PROMPT` for incremental regeneration preserving unchanged sections, `DIRECTORY_SYSTEM_PROMPT` mandating first line `<!-- Generated by agents-reverse-engineer -->` with adaptive section strategy (Contents/Subdirectories/Architecture/Stack/Structure/Patterns/Configuration/API Surface/File Relationships/Behavioral Contracts), `DIRECTORY_UPDATE_SYSTEM_PROMPT` for directory-level incremental updates, `ROOT_SYSTEM_PROMPT` enforcing synthesis-only mode prohibiting invention of features/hooks/APIs/patterns. All prompts prohibit filler phrases: `\"this file\"`, `\"this module\"`, `\"provides\"`, `\"responsible for\"`, `\"is used to\"`.\n\n**[types.ts](./types.ts)** — Defines `PromptContext` interface with fields `filePath`, `content`, `contextFiles[]`, `projectPlan`, `existingSum` for Phase 1/2 builders. Exports `SUMMARY_GUIDELINES` constant object with `targetLength: {min: 300, max: 500}`, `include[]` array of 8 mandatory content categories (purpose, public interface, patterns, dependencies, function signatures, coupled files, behavioral contracts, annex references), `exclude[]` array of 3 prohibited categories (control flow minutiae, generic TODOs, broad architectural relationships).\n\n**[index.ts](./index.ts)** — Barrel module re-exporting `buildFilePrompt`, `buildDirectoryPrompt`, `buildRootPrompt`, `detectLanguage` from `builder.ts` and `PromptContext`, `SUMMARY_GUIDELINES` from `types.ts`. Separates prompt template strings (in `templates.ts`) from assembly logic.\n\n## File Relationships\n\n`builder.ts` consumes templates from `templates.ts` (`FILE_SYSTEM_PROMPT`, `FILE_USER_PROMPT`, `FILE_UPDATE_SYSTEM_PROMPT`, `DIRECTORY_SYSTEM_PROMPT`, `DIRECTORY_UPDATE_SYSTEM_PROMPT`, `ROOT_SYSTEM_PROMPT`). `builder.ts` imports `GENERATED_MARKER` from `../writers/agents-md.js` for user content detection, `readSumFile()`/`getSumPath()` from `../writers/sum.js` for child summary aggregation, `extractDirectoryImports()`/`formatImportMap()` from `../../imports/index.js` for dependency graph construction, `collectAgentsDocs()` from `../collector.js` for Phase 3 synthesis. `index.ts` re-exports `PromptContext` from `types.ts` for consumer type safety.\n\n## Incremental Update Strategy\n\nAll three builders accept optional existing content: `context.existingSum` in `buildFilePrompt()`, `existingAgentsMd` in `buildDirectoryPrompt()`. When provided, appends existing content under heading `## Existing [Summary|AGENTS.md] (update this — preserve stable content, modify only what changed)` and switches to update-specific system prompts. `FILE_UPDATE_SYSTEM_PROMPT` enforces preservation rules: keep unchanged sections verbatim, modify only content affected by code changes, preserve behavioral contracts (regex/constants) unless source changed them. `DIRECTORY_UPDATE_SYSTEM_PROMPT` preserves structure/headings/descriptions for unchanged files, modifies only entries whose summaries changed, adds/removes entries for new/deleted files.\n\n## User Content Preservation\n\n`buildDirectoryPrompt()` checks for `AGENTS.local.md` first, falling back to checking existing `AGENTS.md` for absence of `GENERATED_MARKER` constant. User content appended as `## User Notes` section with reference link pattern `[AGENTS.local.md](./AGENTS.local.md)`. First-run detection message: \"This user-defined content will be preserved as [AGENTS.local.md](./AGENTS.local.md).\"\n\n## Behavioral Contracts\n\nAll templates enforce:\n\n- **Density rule**: Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- **Anchor term preservation**: All exported function/class/type/const names MUST appear in summary exactly as written in source\n- **Filler phrase prohibition**: `\"this file\"`, `\"this module\"`, `\"provides\"`, `\"responsible for\"`, `\"is used to\"`, `\"basically\"`, `\"essentially\"`, `\"provides functionality for\"`\n- **Output format**: Start response DIRECTLY with bold purpose statement without preamble\n- **PATH ACCURACY**: Use only paths from \"Import Map\" section, exact directory names from \"Project Directory Structure\", no path invention\n- **CONSISTENCY**: Do not contradict within same document (e.g., calling technique \"regex-based\" then \"AST-based\")\n\n`DIRECTORY_SYSTEM_PROMPT` enforces first line exactly `<!-- Generated by agents-reverse-engineer -->` followed by `#` heading with directory name.\n\n`FILE_SYSTEM_PROMPT` defines REPRODUCTION-CRITICAL CONTENT strategy: files with large string constants should list them in `## Annex References` section rather than inlining content, delegating extraction to pipeline automation.\n\n## Annex References\n\nFull prompt template text: [templates.ts.annex.md](./templates.ts.annex.md)  \nPromptContext interface specification: [types.ts.annex.md](./types.ts.annex.md)\n### writers/\n<!-- Generated by agents-reverse-engineer -->\n\n# generation/writers\n\nImplements `.sum` file and `AGENTS.md` serialization with YAML frontmatter parsing, user content preservation, and annex file generation for reproduction-critical source content.\n\n## Contents\n\n### [sum.ts](./sum.ts)\nYAML frontmatter I/O for `.sum` files with SHA-256 hash tracking, array serialization (inline/multi-line), annex file generation for reproduction-critical content. Exports `writeSumFile`, `readSumFile`, `getSumPath`, `sumFileExists`, `writeAnnexFile`, `getAnnexPath`, `parseSumFile`, `formatSumFile`, `SumFileContent` interface.\n\n### [agents-md.ts](./agents-md.ts)\nAGENTS.md lifecycle management preserving user-authored content via AGENTS.local.md rename, marker-based detection. Exports `writeAgentsMd`, `isGeneratedAgentsMd`, `GENERATED_MARKER` constant.\n\n### [index.ts](./index.ts)\nBarrel re-exporting `writeSumFile`, `readSumFile`, `getSumPath`, `sumFileExists`, `SumFileContent` from `sum.ts` and `writeAgentsMd` from `agents-md.ts`.\n\n## Data Flow\n\n**Phase 1 (File Analysis):**\n`src/generation/executor.ts` → `writeSumFile(sourcePath, content)` → appends `.sum` extension → `formatSumFile()` → `mkdir` + `writeFile` → emits `.sum` file with frontmatter delimiter `---\\n...\\n---\\n` + summary body.\n\n**Phase 2 (Directory Aggregation):**\n`src/generation/orchestrator.ts` → `writeAgentsMd(dirPath, projectRoot, llmContent)` → checks existing AGENTS.md via `isGeneratedAgentsMd()` → renames user file to AGENTS.local.md → prepends preserved content + separator `---` + LLM-generated sections.\n\n**Incremental Updates:**\n`src/update/orchestrator.ts` → `readSumFile(getSumPath(sourcePath))` → `parseSumFile()` extracts `contentHash` → SHA-256 comparison → hash mismatch triggers re-analysis.\n\n**Orphan Cleanup:**\n`src/update/orphan-cleaner.ts` → detects stale `.sum` files for deleted sources → calls `unlink(getSumPath(deletedPath))`.\n\n## YAML Frontmatter Format\n\n**Delimiter:** `---\\n` before and after metadata block, exactly one newline before summary body.\n\n**Scalar fields:**\n```yaml\ngenerated_at: 2026-02-09T12:34:56.789Z\ncontent_hash: a3f5d8e9... (SHA-256 hex, 64 chars)\npurpose: Single-line description\n```\n\n**Array fields (inline format <40 chars, ≤3 items):**\n```yaml\ncritical_todos: [Security issue, Performance bug]\nrelated_files: [path/to/file.ts]\n```\n\n**Array fields (multi-line format):**\n```yaml\ncritical_todos:\n  - Long security issue description exceeding 40 characters\n  - Another long description\nrelated_files:\n  - very/long/path/to/related/file/exceeding/character/limit.ts\n```\n\n## Parsing Strategies\n\n**Frontmatter extraction:** Regex `/^---\\n([\\s\\S]*?)\\n---\\n/` captures metadata block.\n\n**Scalar field extraction:** Individual regex per field (`/generated_at:\\s*(.+)/`, `/content_hash:\\s*(.+)/`, `/purpose:\\s*(.+)/`), captures group 1, trims whitespace.\n\n**Array parsing (inline):** `new RegExp(\\`${key}:\\\\s*\\\\[([^\\\\]]*)\\\\]\\`)` captures comma-separated values within brackets, splits on `,`, strips quotes and whitespace.\n\n**Array parsing (multi-line):** `new RegExp(\\`${key}:\\\\s*\\\\n((?:\\\\s+-\\\\s+.+\\\\n?)+)\\`, 'm')` captures indented list items, splits on newlines, strips `- ` prefix per line.\n\n## User Content Preservation Workflow\n\n1. `writeAgentsMd()` called with `dirPath` and LLM-generated `content`\n2. Constructs `agentsPath` (`dirPath/AGENTS.md`) and `localPath` (`dirPath/AGENTS.local.md`)\n3. If `agentsPath` exists and `isGeneratedAgentsMd(agentsPath)` returns `false`: `rename(agentsPath, localPath)`, capture content as `userContent`\n4. Else if `localPath` exists: `readFile(localPath)`, capture as `userContent`\n5. Strip `GENERATED_MARKER` prefix from LLM `content` if present via `slice()` + `/^\\n+/` trim\n6. Assemble `finalContent`:\n   - `GENERATED_MARKER` comment\n   - If `userContent.trim()` non-empty: preservation comment + `userContent.trim()` + `---` separator\n   - LLM-generated sections\n7. `mkdir(dirPath, { recursive: true })` + `writeFile(agentsPath, finalContent)`\n\n## Annex File Generation\n\n`writeAnnexFile(sourcePath, sourceContent)` creates `${sourcePath}.annex.md`:\n```markdown\n<!-- Generated by agents-reverse-engineer -->\n\n# Annex: <basename>\n\nThis annex file preserves the full source content of `<basename>` for AI coding assistants.\nThe primary summary is in `<basename>.sum`.\n\n```<ext>\n<sourceContent>\n```\n```\n\nUsed for files containing long regex patterns, template strings, or format specifications where verbatim content required for reproduction. `getAnnexPath()` computes path without filesystem access.\n\n## Behavioral Contracts\n\n**Marker detection:** `content.includes('<!-- Generated by agents-reverse-engineer -->')` substring search (no regex).\n\n**Marker removal:** `llmContent.startsWith(GENERATED_MARKER) ? llmContent.slice(GENERATED_MARKER.length).replace(/^\\n+/, '') : llmContent`.\n\n**File extensions:** `.sum` (metadata + summary), `.annex.md` (full source preservation).\n\n**User content separator:** Three-dash horizontal rule surrounded by blank lines (`\\n\\n---\\n\\n`).\n\n**Frontmatter regex:** `/^---\\n([\\s\\S]*?)\\n---\\n/` non-greedy capture of metadata block.\n\n**Array inline regex:** `` new RegExp(`${key}:\\\\s*\\\\[([^\\\\]]*)\\\\]`) `` captures values within square brackets.\n\n**Array multi-line regex:** `` new RegExp(`${key}:\\\\s*\\\\n((?:\\\\s+-\\\\s+.+\\\\n?)+)`, 'm') `` captures indented list items starting with `- `.\n\n## Integration Points\n\n**Imports:**\n- `../types.js` → `SummaryMetadata` (metadata structure for frontmatter)\n- `./agents-md.js` → `GENERATED_MARKER` (annex file header constant)\n\n**Consumed By:**\n- `src/generation/executor.ts` → `writeSumFile()` for Phase 1 output\n- `src/generation/orchestrator.ts` → `writeAgentsMd()` for Phase 2 output\n- `src/update/orchestrator.ts` → `readSumFile()` for hash comparison\n- `src/update/orphan-cleaner.ts` → `getSumPath()` for stale file deletion\n- `src/generation/collector.ts` → reads AGENTS.local.md when aggregating subdirectory docs",
      "systemPrompt": "You are generating an AGENTS.md file — a directory-level overview for AI coding assistants.\n\nCRITICAL: Output ONLY the raw markdown content. No code fences, no preamble, no explanations, no conversational text. Your entire response IS the AGENTS.md file content.\n\nOUTPUT FORMAT:\n- First line MUST be exactly: <!-- Generated by agents-reverse-engineer -->\n- Use a # heading with the directory name\n- Write a one-paragraph purpose statement for the directory\nusage\nADAPTIVE SECTIONS:\nAnalyze the directory contents and choose the most relevant sections. Do NOT use a fixed template. Instead, select sections that best document this specific directory for an AI that needs to reconstruct or extend the project.\n\nConsider these section types (choose what applies):\n- **Contents**: Group files by purpose/category under ## headings. For each file: markdown link [filename](./filename) and a one-line description.\n- **Subdirectories**: If subdirectories exist, list them with links [dirname/](./dirname/) and brief summaries.\n- **Architecture / Data Flow**: If files form a pipeline, request/response chain, or layered architecture, document it.\n- **Stack**: If this is a package root (has package.json, Cargo.toml, go.mod, etc.), document the technology stack, key scripts, and entry points.\n- **Structure**: If the directory layout follows a convention (feature-sliced, domain-driven, MVC, etc.), document it.\n- **Patterns**: If files share recurring design patterns (factory, strategy, middleware, barrel re-export), name and document them.\n- **Configuration**: If the directory contains config files, schemas, or environment definitions, document the config surface area.\n- **API Surface**: If the directory exports a public API (barrel index, route definitions, SDK), document the interface contract.\n- **File Relationships**: How files collaborate, depend on each other, or share state.\n- **Behavioral Contracts**: If files contain regex patterns, format specifications, magic constants, or template strings that define observable behavior, collect them in a dedicated section. Preserve verbatim patterns from file summaries — do NOT paraphrase regex into prose. This section is MANDATORY when file summaries contain behavioral artifacts.\n- **Reproduction-Critical Constants**: If file summaries reference annex files (via ## Annex References sections), list them with links. Example: \"Full prompt template text: [templates.ts.annex.md](./prompts/templates.ts.annex.md)\". Do NOT reproduce annex content in AGENTS.md — just link to it.\n\nChoose any relevant sections or create your own based on the directory contents. The goal is to provide a comprehensive overview that captures the essence of the directory's role in the project and how its files work together, with a focus on what an AI coding assistant would need to know to effectively interact with this code.\n\nSCOPE:\n- AGENTS.md is a NAVIGATIONAL INDEX — help an AI find the right file quickly\n- Focus on: what each file does, how files relate, directory-level patterns\n- Do NOT reproduce full architecture sections — those belong in the root CLAUDE.md\n\nPATH ACCURACY (MANDATORY):\n- When referencing files or modules outside this directory, use ONLY paths from the \"Import Map\" section\n- Do NOT invent, rename, or guess module paths — if a path isn't in the Import Map, don't reference it\n- Use the exact directory names from \"Project Directory Structure\" — do NOT rename directories\n  (e.g., if the directory is called \"cli\", write \"src/cli/\", NOT \"src/commands/\")\n- Cross-module references must use the specifier format from actual import statements\n  (e.g., \"../generation/writers/sum.js\", NOT \"../fs/sum-file.js\")\n- If you are unsure about a path, omit the cross-reference rather than guessing\n\nCONSISTENCY (MANDATORY):\n- Do not contradict yourself within the same document\n- If you describe a technique (e.g., \"regex-based\"), do not call it something else later (e.g., \"AST-based\")\n- When stating version numbers, engines, or config fields, use ONLY values present in the file summaries\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this directory\", \"this module\", \"provides\", \"responsible for\", \"is used to\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Per-file descriptions in Contents sections: 1-2 sentences maximum. Reference key symbols but do not reproduce full summaries.\n- Behavioral contracts (regex patterns, format specs, constants) belong in a separate Behavioral Contracts section, not in per-file descriptions.\n- Annex file references: link to .annex.md files, do not inline their content. One line per annex reference.\n- Subdirectory descriptions: 1-2 sentences maximum. Capture the directory's role, not its full contents.\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- Key exported symbols from file summaries MUST appear in the directory overview\n- Preserve exact casing of identifiers\n\nUSER NOTES:\n- If \"User Notes\" are provided in the prompt, they contain user-defined instructions that will be automatically prepended to your output\n- Do NOT repeat or paraphrase user notes in your generated content — they are included separately\n- You may reference information from user notes for context",
      "response": "<!-- Generated by agents-reverse-engineer -->\n\n# generation\n\n**Orchestrates three-phase documentation generation pipeline: concurrent file analysis via `GenerationOrchestrator.createFileTasks()`, post-order directory synthesis via `buildExecutionPlan()` with depth-sorted traversal, and root document synthesis via `collectAgentsDocs()` aggregation, enforcing dependency graphs through `ExecutionTask.dependencies` arrays and completion predicates via `isDirectoryComplete()`.**\n\n## Contents\n\n### Pipeline Orchestration\n\n**[orchestrator.ts](./orchestrator.ts)** — `GenerationOrchestrator` prepares files via `prepareFiles(discoveryResult)` reading content with `readFile()`, creates file tasks via `createFileTasks(files)` calling `buildFilePrompt()` for each file, creates directory tasks via `createDirectoryTasks(files)` grouping by `path.dirname()`, emits trace events (`phase:start`, `plan:created`, `phase:end`) via `ITraceWriter`, clears `PreparedFile.content` after prompt embedding to free memory, returns `GenerationPlan` with `files`, `tasks`, `complexity` from `analyzeComplexity()`. `buildProjectStructure()` formats directory tree as indented text for AI context.\n\n**[executor.ts](./executor.ts)** — `buildExecutionPlan()` constructs dependency graph from `GenerationPlan`: populates `directoryFileMap` extracting directories via `path.dirname()`, creates file tasks with `id: \"file:{path}\"`, sorts files by `getDirectoryDepth(path.dirname())` descending (deepest first), sorts directories by depth descending, creates directory tasks with `dependencies: fileTaskIds`, creates root tasks with `dependencies: allDirTaskIds`, returns `ExecutionPlan` with `fileTasks`/`directoryTasks`/`rootTasks` arrays. `isDirectoryComplete()` checks all files have `.sum` outputs via `sumFileExists()`, `getReadyDirectories()` identifies directories eligible for `AGENTS.md` generation, `formatExecutionPlanAsMarkdown()` generates `GENERATION-PLAN.md` with checkbox lists grouped by phase and depth.\n\n**[collector.ts](./collector.ts)** — `collectAgentsDocs()` recursively walks project tree via `readdir(withFileTypes)`, collects files named exactly `AGENTS.md`, skips 13 directories in `SKIP_DIRS` set (`node_modules`, `.git`, `.agents-reverse-engineer`, `vendor`, `dist`, `build`, `__pycache__`, `.next`, `venv`, `.venv`, `target`, `.cargo`, `.gradle`), returns `AgentsDocs` array sorted by `relativePath` via `localeCompare()`. `collectAnnexFiles()` uses identical traversal logic for files ending `.annex.md`. Gracefully handles permission-denied via silent exception catch.\n\n**[complexity.ts](./complexity.ts)** — `analyzeComplexity()` computes `ComplexityMetrics` with `fileCount`, `directoryDepth` via `calculateDirectoryDepth()` (splits `path.relative()` by `path.sep`, tracks max), `directories` set via `extractDirectories()` (walks upward via repeated `path.dirname()` until root), `files` array. Consumed by `GenerationOrchestrator.createPlan()` for complexity warnings.\n\n**[types.ts](./types.ts)** — `AnalysisResult` with `summary: string` and `metadata: SummaryMetadata`, `SummaryMetadata` with `purpose`, `criticalTodos`, `relatedFiles`, `SummaryOptions` with `targetLength` discriminant (`'short' | 'standard' | 'detailed'`) and `includeCodeSnippets` boolean. Maps to `.sum` YAML frontmatter schema.\n\n## Subdirectories\n\n**[prompts/](./prompts/)** — Prompt template constants (`FILE_SYSTEM_PROMPT`, `DIRECTORY_SYSTEM_PROMPT`, `ROOT_SYSTEM_PROMPT`) with density rules (every sentence references identifiers), filler phrase prohibition (`\"this file\"`, `\"provides\"`, `\"responsible for\"`), behavioral contract extraction (verbatim regex/constants). Builders (`buildFilePrompt`, `buildDirectoryPrompt`, `buildRootPrompt`) substitute placeholders (`{{FILE_PATH}}`, `{{CONTENT}}`, `{{LANG}}`), aggregate child `.sum` files via `readSumFile()`, extract import maps via `extractDirectoryImports()`, preserve user content from `AGENTS.local.md`, switch to update prompts (`FILE_UPDATE_SYSTEM_PROMPT`, `DIRECTORY_UPDATE_SYSTEM_PROMPT`) when `existingSum`/`existingAgentsMd` present.\n\n**[writers/](./writers/)** — YAML frontmatter serialization (`writeSumFile`, `readSumFile`, `getSumPath`, `sumFileExists`) with SHA-256 hash tracking, inline/multi-line array formatting, annex file generation (`writeAnnexFile`, `getAnnexPath`) for reproduction-critical source content. `writeAgentsMd()` preserves user content via `AGENTS.local.md` rename, marker-based detection (`isGeneratedAgentsMd`), prepends preserved sections with `---` separator.\n\n## Post-Order Traversal\n\n`buildExecutionPlan()` enforces children-before-parents ordering via two sorts:\n1. **File tasks**: `getDirectoryDepth(path.dirname(a.path)) - getDirectoryDepth(path.dirname(b.path))` descending (deepest first)\n2. **Directory tasks**: `getDirectoryDepth(dirB) - getDirectoryDepth(dirA)` descending\n\nEnsures child `AGENTS.md` files exist before parent directory prompts consume them via `collectAgentsDocs()` in `buildDirectoryPrompt()`.\n\n## Dependency Graph\n\n`ExecutionTask.dependencies: string[]` enforces Phase 1 → Phase 2 → Phase 3 sequencing:\n- **File tasks**: empty dependencies (parallel eligible)\n- **Directory tasks**: depend on all file task IDs in directory (`id: \"file:{path}\"`)\n- **Root tasks**: depend on all directory task IDs (`id: \"dir:{path}\"`)\n\n`isDirectoryComplete()` verifies all `expectedFiles` have `.sum` outputs before allowing directory task execution.\n\n## Prompt Placeholder Pattern\n\nDirectory and root tasks store placeholder prompts (`\"Built at runtime by buildDirectoryPrompt()\"`) for plan display and dependency tracking. Actual prompts constructed at execution time in `src/orchestration/runner.ts` via `buildDirectoryPrompt()` and `buildRootPrompt()`, consuming child `.sum` files and subdirectory `AGENTS.md` not available during plan creation.\n\n## Trace Events\n\n`GenerationOrchestrator.createPlan()` emits:\n- `phase:start` with `{ type: 'phase:start', phase: 'plan-creation', taskCount, concurrency: 1 }`\n- `plan:created` with `{ type: 'plan:created', planType: 'generate', fileCount, taskCount: tasks.length + 1 }` (accounting for root task added by `buildExecutionPlan()`)\n- `phase:end` with `{ type: 'phase:end', phase: 'plan-creation', durationMs, tasksCompleted: 1, tasksFailed: 0 }`\n\n## Integration Points\n\nImports `buildFilePrompt` from `./prompts/index.js` for Phase 1 task creation. Imports `sumFileExists` from `./writers/sum.js` for completion checking. Imports `analyzeComplexity` from `./complexity.js` for metrics computation. Imports `ITraceWriter` from `../orchestration/trace.js` for event emission. Imports `Config` from `../config/schema.js` and `DiscoveryResult` from `../types/index.js`.\n\nConsumed by `src/orchestration/runner.ts` which executes `ExecutionPlan` via worker pool, calling `AIService` with runtime-constructed prompts for directory and root tasks.\n\n## Memory Management\n\nAfter `createFileTasks()` embeds file content into prompts, `createPlan()` clears `PreparedFile.content` via `(file as { content: string }).content = ''` to free memory. Comment notes runner re-reads files from disk during execution.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 2014,
      "cacheReadTokens": 17587,
      "cacheCreationTokens": 17528,
      "latencyMs": 61975,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": []
    },
    {
      "timestamp": "2026-02-09T17:47:47.791Z",
      "prompt": "Generate AGENTS.md for directory: \"src/cli\" (cli)\n\n## File Summaries (7 files)\n\n### clean.ts\n**Purpose:** `src/cli/clean.ts` implements the `are clean` command to delete all generated documentation artifacts (.sum files, AG...\n\n**`src/cli/clean.ts` implements the `are clean` command to delete all generated documentation artifacts (.sum files, AGENTS.md, CLAUDE.md, GENERATION-PLAN.md) while preserving user-authored AGENTS.md and restoring AGENTS.local.md files.**\n\n## Exported Interface\n\n### CleanOptions\n```typescript\ninterface CleanOptions {\n  dryRun: boolean;  // Preview deletions without executing\n}\n```\n\n### cleanCommand\n```typescript\nasync function cleanCommand(\n  targetPath: string,\n  options: CleanOptions\n): Promise<void>\n```\n\nOrchestrates deletion workflow: validates target path exists with read permissions, discovers artifacts via `fast-glob`, filters generated vs. user-authored AGENTS.md via `GENERATED_MARKER` substring search, displays deletion preview, deletes files via `unlink()`, restores AGENTS.local.md → AGENTS.md via `rename()`.\n\n## Artifact Discovery Strategy\n\nUses parallel `fg.glob()` calls with shared ignore patterns `['**/node_modules/**', '**/.git/**']` to find:\n- `**/*.sum` — per-file summaries\n- `**/*.annex.md` — reproduction-critical constant annexes\n- `**/AGENTS.md` — directory-level aggregations (requires marker filtering)\n- `**/AGENTS.local.md` — user-authored files renamed during generation\n\nSingle-file existence checks via `access()` for:\n- `CLAUDE.md` at project root\n- `.agents-reverse-engineer/GENERATION-PLAN.md`\n\n## Generated vs. User-Authored AGENTS.md Detection\n\nReads each discovered AGENTS.md file, checks for `GENERATED_MARKER` constant (`<!-- Generated by agents-reverse-engineer -->`) via `content.includes()`. Files without marker added to `skippedAgentsFiles[]` and excluded from deletion. Prevents accidental removal of SDK documentation or manually maintained AGENTS.md files.\n\n## Deletion Workflow\n\n**Dry-run mode (`options.dryRun: true`):**\n- Displays all files that would be deleted/restored\n- Logs preservation of user-authored AGENTS.md\n- Exits without filesystem modifications\n- Emits yellow warning: `'Dry run — no files were changed.'`\n\n**Execution mode:**\n- Iterates `allFiles = [...sumFiles, ...annexFiles, ...generatedAgentsFiles, ...singleFiles]`\n- Deletes each via `unlink()`, increments `deleted` counter, logs errors without aborting\n- Iterates `localAgentsFiles`, restores via `rename(localFile, 'AGENTS.md')`, increments `restored` counter\n- Emits green summary: `'Deleted N file(s), restored M AGENTS.local.md file(s).'`\n\n## Error Handling\n\n**Path validation:**\n- `ENOENT` → logs `'Directory not found'`, exits with code 1\n- `EACCES`/`EPERM` → logs `'Permission denied'`, exits with code 1\n- Other errors rethrown\n\n**File operations:**\n- `unlink()` failures logged as `'Failed to delete <path>: <message>'` without aborting\n- `rename()` failures logged as `'Failed to restore <path>: <message>'` without aborting\n- `readFile()` failures during marker detection silently skip file (no log entry)\n\n## Dependencies\n\n- `fast-glob` — parallel artifact discovery with glob patterns\n- `picocolors` — terminal formatting for dry-run warnings and success messages\n- `src/output/logger.js` → `createLogger()` for structured logging\n- `src/generation/writers/agents-md.js` → `GENERATED_MARKER` constant for detection logic\n\n## Output Format\n\nDisplays relative paths via `path.relative(resolvedPath, absPath)` for readability. Summary line uses `pc.bold()` for file counts:\n```\n<count> .sum file(s), <count> .annex.md file(s), <count> AGENTS.md file(s), <count> root doc(s), <count> AGENTS.local.md to restore\n```\n### discover.ts\n**Purpose:** `discover.ts` executes the `are discover` command which walks the target directory applying gitignore/vendor/binary/c...\n\n**`discover.ts` executes the `are discover` command which walks the target directory applying gitignore/vendor/binary/custom filters, writes discovered files to `progress.log`, generates `GENERATION-PLAN.md` with post-order directory traversal, and emits trace events for discovery lifecycle.**\n\n## Exported Interface\n\n**`discoverCommand(targetPath: string, options: DiscoverOptions): Promise<void>`** — Main entry point for the `are discover` command. Resolves `targetPath` to absolute path (defaults to `process.cwd()`), loads configuration via `loadConfig()`, verifies directory accessibility, runs `discoverFiles()` discovery pipeline, logs included/excluded files to console and `ProgressLog`, creates `GenerationPlan` via `orchestrator.createPlan()`, builds `ExecutionPlan` via `buildExecutionPlan()`, formats as markdown via `formatExecutionPlanAsMarkdown()`, and writes to `.agents-reverse-engineer/GENERATION-PLAN.md`.\n\n**`DiscoverOptions`** — Interface with optional `tracer?: ITraceWriter` for trace event emission and `debug?: boolean` flag for verbose output (defaults to `false`).\n\n## Workflow Steps\n\n1. **Path Resolution**: Converts `targetPath` to absolute via `path.resolve()`, falling back to `process.cwd()` if empty\n2. **Configuration Loading**: Calls `loadConfig(resolvedPath)` to load `.agents-reverse-engineer/config.yaml` or use defaults\n3. **Access Verification**: Uses `fs.access()` with `constants.R_OK` to verify directory readability, exits on `ENOENT`/`EACCES`/`EPERM` errors\n4. **Progress Log Initialization**: Creates `ProgressLog` via `ProgressLog.create(resolvedPath)` for tail monitoring, writes header with ISO 8601 timestamp\n5. **Discovery Execution**: Calls `discoverFiles(resolvedPath, config, { tracer, debug })` to run filter chain\n6. **Trace Emission**: Emits `discovery:start` before discovery, `discovery:end` after with `filesIncluded`/`filesExcluded`/`durationMs` fields using `process.hrtime.bigint()` for nanosecond precision\n7. **Result Logging**: Iterates `result.included` logging via `logger.file()` and `progressLog.write()` with relative paths, iterates `result.excluded` logging via `logger.excluded()` with `reason` and `filter` fields\n8. **Plan Generation**: Creates `DiscoveryResult` object wrapping `files` and `excluded` arrays, calls `createOrchestrator(config, resolvedPath)`, invokes `orchestrator.createPlan(discoveryResult)`, calls `buildExecutionPlan()` for post-order directory sorting, formats via `formatExecutionPlanAsMarkdown()`\n9. **Plan Persistence**: Creates `.agents-reverse-engineer/` directory via `mkdir({ recursive: true })`, writes markdown to `GENERATION-PLAN.md` via `writeFile()`, logs relative path via `logger.info()` and `progressLog.write()`\n10. **Finalization**: Calls `progressLog.finalize()` to flush buffered writes\n\n## Error Handling\n\n**Access Errors**: Catches `fs.access()` exceptions, pattern-matches `error.code` against `'ENOENT'`/`'EACCES'`/`'EPERM'`, logs via `logger.error()`, calls `process.exit(1)` for known codes, rethrows unknown errors.\n\n**Plan Write Errors**: Catches `writeFile()` exceptions, extracts `(err as Error).message`, logs via `logger.error()` and `progressLog.write()`, finalizes progress log, calls `process.exit(1)`.\n\n## Dependencies\n\n**Internal Modules**:\n- `../config/loader.js` — `loadConfig()` for YAML config loading\n- `../discovery/run.js` — `discoverFiles()` for filter chain execution\n- `../output/logger.js` — `createLogger()` for terminal output with picocolors\n- `../generation/orchestrator.js` — `createOrchestrator()` for generation plan creation\n- `../generation/executor.js` — `buildExecutionPlan()` and `formatExecutionPlanAsMarkdown()` for post-order traversal\n- `../orchestration/index.js` — `ProgressLog` for tail monitoring\n- `../types/index.js` — `DiscoveryResult` interface\n- `../orchestration/trace.js` — `ITraceWriter` interface\n\n**Node.js APIs**:\n- `node:path` — Path resolution and relative path computation\n- `node:fs/promises` — `access()`, `mkdir()`, `writeFile()` for async file I/O\n- `node:fs` — `constants.R_OK` for access mode flags\n- `picocolors` — Terminal color formatting via `pc.dim()`\n\n## Trace Events\n\n**`discovery:start`** — Emitted before `discoverFiles()` with `targetPath: string` field.\n\n**`discovery:end`** — Emitted after `discoverFiles()` with `filesIncluded: number`, `filesExcluded: number`, `durationMs: number` fields. Duration computed via `process.hrtime.bigint()` nanosecond delta converted to milliseconds by dividing by `1_000_000`.\n\n## Output Behavior\n\n**Console Logging**: Uses `logger.file()` for included paths, `logger.excluded(path, reason, filter)` for excluded paths, `logger.summary(includedCount, excludedCount)` for summary, `logger.info()` for plan creation, `logger.error()` for failures.\n\n**Progress Log**: Mirrors console output to `.agents-reverse-engineer/progress.log` with prefix patterns: `+ ` for included files, `- ` for excluded files with parenthesized reason and filter.\n\n**Debug Output**: When `options.debug` is truthy, writes to `console.error()` with `pc.dim()` formatting for `[debug]` prefixed messages showing target path, file counts, and discovery completion.\n\n## Path Handling\n\n**Relative Path Computation**: Defines local `relativePath(absPath: string): string` helper using `path.relative(resolvedPath, absPath)` to convert absolute discovery results to project-relative paths for cleaner output.\n\n**Plan Path Construction**: Builds plan file path via `path.join(resolvedPath, '.agents-reverse-engineer', 'GENERATION-PLAN.md')`, computes relative path for logging via `path.relative(resolvedPath, planPath)`.\n### generate.ts\n**Purpose:** generateCommand orchestrates three-phase AI-driven documentation generation with concurrent file analysis, directory ...\n\n**generateCommand orchestrates three-phase AI-driven documentation generation with concurrent file analysis, directory aggregation, and root document synthesis via CommandRunner execution of GenerationPlan tasks.**\n\n## Exported Interface\n\n**generateCommand(targetPath: string, options: GenerateOptions): Promise<void>** — CLI entry point that discovers files, builds GenerationPlan via createOrchestrator, resolves AI backend, and executes three-phase pipeline through CommandRunner.executeGenerate.\n\n**GenerateOptions** — Configuration interface with fields:\n- `dryRun?: boolean` — Show execution plan without AI calls\n- `concurrency?: number` — Override worker pool size (1-10)\n- `failFast?: boolean` — Abort on first task failure\n- `debug?: boolean` — Enable subprocess logging with heap/RSS metrics\n- `trace?: boolean` — Emit NDJSON events to `.agents-reverse-engineer/traces/`\n\n**formatPlan(plan: GenerationPlan): string** — Formats GenerationPlan summary showing file count, task count, complexity metrics (fileCount, directoryDepth).\n\n## Execution Flow\n\n**generateCommand** coordinates six sequential phases:\n\n1. **Configuration Loading**: Calls loadConfig with tracer and debug options, returns Config with ai.backend, ai.concurrency, ai.timeoutMs, exclude patterns.\n\n2. **File Discovery**: Invokes discoverFiles with absolutePath and config, applies gitignore/binary/vendor filters, returns FilterResult with included/excluded arrays. Creates DiscoveryResult for orchestrator by mapping FilterResult.excluded to `{path, reason}` tuples.\n\n3. **Plan Creation**: Instantiates createOrchestrator with config and absolutePath, calls orchestrator.createPlan(discoveryResult) returning GenerationPlan with files[], tasks[], complexity. Logs formatPlan output showing file count, task count, complexity.fileCount, complexity.directoryDepth.\n\n4. **Dry-Run Bailout** (conditional): If options.dryRun, calls buildExecutionPlan(plan, absolutePath) returning ExecutionPlan with fileTasks[], directoryTasks[], rootTasks[], directoryFileMap. Logs summary: file count (executionPlan.fileTasks.length), directory count (Object.keys(executionPlan.directoryFileMap).length), root document count (executionPlan.rootTasks.length), estimated AI calls (executionPlan.tasks.length). Prints each fileTasks[].path via pc.dim, exits without AI calls.\n\n5. **Backend Resolution**: Calls createBackendRegistry() returning BackendRegistry, resolveBackend(registry, config.ai.backend) returning Backend. Catches AIServiceError with code 'CLI_NOT_FOUND', logs getInstallInstructions(registry), exits with code 2. If options.debug, logs backend.name, backend.cliCommand, config.ai.model.\n\n6. **AI Service Execution**: Instantiates AIService(backend, {timeoutMs, maxRetries, model, telemetry.keepRuns}). Calls aiService.setDebug(true) if options.debug. If options.trace, calls aiService.setSubprocessLogDir with path `.agents-reverse-engineer/subprocess-logs/<timestamp>`. Builds executionPlan via buildExecutionPlan. Creates ProgressLog.create(absolutePath), writes header with ISO timestamp, project path, file/directory counts. Instantiates CommandRunner(aiService, {concurrency, failFast, debug, tracer, progressLog}). Calls runner.executeGenerate(executionPlan) returning RunSummary. Calls aiService.finalize(absolutePath) for telemetry, progressLog.finalize(), tracer.finalize(), cleanupOldTraces(absolutePath) if options.trace.\n\n## Exit Code Strategy\n\nDerived from RunSummary fields (filesProcessed, filesFailed):\n- `process.exit(2)` — Total failure: filesProcessed === 0 && filesFailed > 0\n- `process.exit(1)` — Partial failure: filesFailed > 0 (but some files succeeded)\n- `process.exit(0)` — Success: filesFailed === 0 (implicit default)\n\n## Trace and Telemetry Integration\n\n**createTraceWriter** invoked with absolutePath and options.trace before loadConfig/discoverFiles to thread tracer through all subsystems. If options.trace && tracer.filePath, logs trace file path via pc.dim. Tracer passed to loadConfig, discoverFiles, createOrchestrator, CommandRunner constructor. cleanupOldTraces(absolutePath) called after tracer.finalize() to enforce retention limit (500 traces).\n\n**ProgressLog.create** instantiated with absolutePath, writes human-readable log to `.agents-reverse-engineer/progress.log` with header showing ISO timestamp, project path, file count (executionPlan.fileTasks.length), directory count (executionPlan.directoryTasks.length). Passed to CommandRunner for streaming updates. Finalized after runner.executeGenerate completes.\n\n**AIService.setSubprocessLogDir** called when options.trace enabled, writes per-subprocess stdout/stderr to `.agents-reverse-engineer/subprocess-logs/<timestamp>/<taskId>.log`. Logs directory path via pc.dim.\n\n## Concurrency Configuration\n\nDetermines worker pool size via `options.concurrency ?? config.ai.concurrency`. Config default: 2 for WSL environments (resource-constrained), 5 elsewhere. Passed to CommandRunner constructor as concurrency option.\n\n## Dependencies\n\n- `loadConfig` (src/config/loader.ts) — YAML config parsing with Zod validation\n- `discoverFiles` (src/discovery/run.ts) — Gitignore-aware file walking returning FilterResult\n- `createOrchestrator` (src/generation/orchestrator.ts) — Builds GenerationPlan with file/directory/root tasks\n- `buildExecutionPlan` (src/generation/executor.ts) — Transforms GenerationPlan into ExecutionPlan with phase breakdown\n- `AIService` (src/ai/service.ts) — Backend-agnostic AI service with subprocess management, retry logic, telemetry\n- `resolveBackend`, `createBackendRegistry`, `getInstallInstructions` (src/ai/index.ts) — Backend detection and CLI installation messaging\n- `CommandRunner` (src/orchestration/runner.ts) — Executes ExecutionPlan via worker pool with progress reporting\n- `createTraceWriter`, `cleanupOldTraces` (src/orchestration/trace.ts) — NDJSON trace emission and retention management\n- `ProgressLog` (src/orchestration/progress.ts) — Human-readable streaming log for tail -f monitoring\n- `createLogger` (src/output/logger.ts) — Picocolors-formatted console output\n\n## Error Handling\n\nAIServiceError with code 'CLI_NOT_FOUND' caught during resolveBackend: logs error message via pc.red, calls getInstallInstructions(registry) to show installation commands for Claude Code/Gemini/OpenCode, exits with code 2.\n### index.ts\n**Purpose:** CLI entry point orchestrating command routing, argument parsing, and installer invocation for the agents-reverse-engi...\n\n**CLI entry point orchestrating command routing, argument parsing, and installer invocation for the agents-reverse-engineer tool.**\n\n## Exported Symbols\n\nNo exported functions or types. This module executes immediately via shebang `#!/usr/bin/env node` and calls `main().catch()` at module scope.\n\n## Commands\n\nSupports seven commands routed via switch statement in `main()`:\n\n- `install` → `runInstaller()` with `parseInstallerArgs(args)`\n- `uninstall` → `runInstaller()` with `installerArgs.uninstall = true`\n- `init` → `initCommand(positional[0] || '.', { force })` from `./init.js`\n- `clean` → `cleanCommand(positional[0] || '.', cleanOpts)` from `./clean.js`\n- `discover` → `discoverCommand(positional[0] || '.', {})` from `./discover.js`\n- `generate` → `generateCommand(positional[0] || '.', options)` from `./generate.js`\n- `update` → `updateCommand(positional[0] || '.', options)` from `./update.js`\n- `specify` → `specifyCommand(positional[0] || '.', specifyOpts)` from `./specify.js`\n\n## Argument Parsing\n\n`parseArgs(args: string[])` returns `{ command, positional, flags, values }` by iterating `process.argv.slice(2)`:\n\n- Flags starting with `--` populate `flags: Set<string>` or `values: Map<string, string>` if next arg is non-flag value\n- Short flags (`-h`, `-g`, `-l`, `-V`) expanded via character iteration with switch mapping (`-h` → `help`, `-g` → `global`, etc.)\n- First non-flag arg becomes `command`, subsequent non-flag args populate `positional: string[]`\n\n## Installer Invocation Patterns\n\nThree patterns trigger installer:\n\n1. **Interactive mode:** `args.length === 0` calls `runInstaller()` with all-false defaults\n2. **Direct flags without command:** `!command && hasInstallerFlags(flags, values)` passes `parseInstallerArgs(args)` to `runInstaller()`\n3. **Explicit install/uninstall command:** Passes `parseInstallerArgs(args)` to `runInstaller()`, setting `uninstall: true` for uninstall command\n\n`hasInstallerFlags()` returns true if `flags` contains `global`/`local`/`force` or `values` contains `runtime`.\n\n## Command Options\n\n`GenerateOptions` constructed from:\n- `dryRun: flags.has('dry-run')`\n- `concurrency: parseInt(values.get('concurrency')!, 10)` if present\n- `failFast: flags.has('fail-fast')`\n- `debug: flags.has('debug')`\n- `trace: flags.has('trace')`\n\n`UpdateCommandOptions` adds `uncommitted: flags.has('uncommitted')` to `GenerateOptions` fields.\n\n`SpecifyOptions` constructed from:\n- `output: values.get('output')`\n- `force: flags.has('force')`\n- `dryRun: flags.has('dry-run')`\n- `multiFile: flags.has('multi-file')`\n- `debug: flags.has('debug')`\n- `trace: flags.has('trace')`\n\n`CleanOptions` contains only `dryRun: flags.has('dry-run')`.\n\n## Help and Version\n\n`showHelp()` prints `USAGE` constant (multiline template with command list, option descriptions, examples) and calls `process.exit(0)`.\n\n`showVersion()` prints `agents-reverse-engineer v${VERSION}` via `getVersion()` from `../version.js` and exits.\n\n`showVersionBanner()` prints version without exiting, called before command routing.\n\n`showUnknownCommand(command: string)` prints error and suggests `are --help`, exits with code 1.\n\n## Global Flag Handling\n\nVersion flag (`--version` or `-V`) triggers `showVersion()` before command routing.\n\nHelp flag (`--help` or `-h`) triggers `showHelp()` only if no command present and no installer flags detected (prevents interference with `install --help`).\n\n## Error Handling\n\n`main().catch((err: Error) => { console.error(\\`Error: ${err.message}\\`); process.exit(1); })` catches uncaught errors at top level.\n\n## USAGE Constant\n\nMultiline string documenting:\n- Command signatures with `[path]` optional positional defaults to current directory\n- Install/uninstall options: `--runtime <name>`, `-g`/`--global`, `-l`/`--local`, `--force`\n- General options: `--debug`, `--trace`, `--dry-run`, `--output <path>`, `--multi-file`, `--concurrency <n>`, `--fail-fast`, `--uncommitted`, `--help`, `--version`\n- Examples showing installer, init, discover, generate with concurrency, update with uncommitted, specify with output/force\n### init.ts\n**Purpose:** `init.ts` implements the `are init` CLI command that creates the `.agents-reverse-engineer/config.yaml` configuration...\n\n**`init.ts` implements the `are init` CLI command that creates the `.agents-reverse-engineer/config.yaml` configuration file with documented defaults, performing existence checks to prevent accidental overwrites.**\n\n## Exported Function\n\n### `initCommand(root: string, options?: { force?: boolean }): Promise<void>`\n\nExecutes the `are init` command workflow:\n1. Resolves `root` to absolute path via `path.resolve()`\n2. Constructs `configPath` from `CONFIG_DIR` and `CONFIG_FILE` constants imported from `../config/loader.js`\n3. Checks `options?.force ?? false` to determine overwrite behavior\n4. Calls `configExists(resolvedRoot)` to detect existing configuration\n5. If config exists and `!force`, logs warning via `logger.warn()` with message `Config already exists at ${configPath}`\n6. If config missing or `force` enabled, calls `writeDefaultConfig(resolvedRoot)` to create YAML file\n7. Logs success via `logger.info()` with user guidance listing customizable fields: `exclude.patterns`, `ai.concurrency`, `ai.timeoutMs`, `ai.backend`\n8. Catches errors with errno code inspection: exits with `process.exit(1)` on `EACCES`/`EPERM` permission errors or other failures\n\n## Dependencies\n\n- `path.resolve()` and `path.join()` for path construction\n- `configExists()`, `writeDefaultConfig()`, `CONFIG_DIR`, `CONFIG_FILE` from `../config/loader.js` for configuration file operations\n- `createLogger({ colors: true })` from `../output/logger.js` for terminal output with ANSI formatting\n\n## Error Handling Strategy\n\nInspects `NodeJS.ErrnoException.code` property for errno classification:\n- `EACCES` or `EPERM` → permission denied error with directory write hint\n- Other errors → generic failure message with `error.message`\n- All error paths call `process.exit(1)` for non-zero exit code\n\n## User Guidance Content\n\nSuccess message enumerates four customizable config sections with inline examples:\n- `exclude.patterns: Custom glob patterns to exclude`\n- `ai.concurrency: Parallel AI calls (1-20, default: auto)`\n- `ai.timeoutMs: Subprocess timeout (default: 300,000ms = 5 min)`\n- `ai.backend: AI backend (claude/gemini/opencode/auto)`\n\nFinal instruction references `README.md for full configuration reference`.\n### specify.ts\n**Purpose:** specify.ts implements the `are specify` CLI command that synthesizes project specifications from AGENTS.md documentat...\n\n**specify.ts implements the `are specify` CLI command that synthesizes project specifications from AGENTS.md documentation via AI backend calls with auto-generation fallback, dry-run preview, and single/multi-file output modes.**\n\n## Exported Interface\n\n**specifyCommand(targetPath: string, options: SpecifyOptions): Promise<void>** — Main entry point that orchestrates specification generation by loading config, collecting AGENTS.md/annex files, optionally auto-generating missing docs, building synthesis prompt, resolving AI backend, invoking AI service with extended timeout (600s minimum), writing output via `writeSpec()`, and finalizing telemetry.\n\n**SpecifyOptions** — Configuration interface with fields:\n- `output?: string` — Custom output path (default: `specs/SPEC.md`)\n- `force?: boolean` — Overwrite existing specs\n- `dryRun?: boolean` — Show statistics without AI calls\n- `multiFile?: boolean` — Split output into multiple files\n- `debug?: boolean` — Enable verbose logging\n- `trace?: boolean` — Enable trace emission\n\n## Command Workflow\n\n### Phase 1: Collection and Validation\nCalls `collectAgentsDocs(absolutePath)` and `collectAnnexFiles(absolutePath)` to gather input corpus. If `docs.length === 0` and not `dryRun`, invokes `generateCommand(targetPath, {debug, trace})` to auto-generate AGENTS.md files. Exits with code 1 if docs still empty after generation.\n\n### Phase 2: Dry-Run Mode\nWhen `options.dryRun === true`, computes `totalChars` from `docs` and `annexFiles` content lengths, estimates tokens via `Math.ceil(totalChars / 4) / 1000`, prints summary showing AGENTS.md count, annex count, token estimate, output path, and mode (single/multi-file). Warns if no docs found or if `estimatedTokensK > 150`. Returns without AI calls.\n\n### Phase 3: Backend Resolution\nCreates `BackendRegistry` via `createBackendRegistry()`, resolves backend via `resolveBackend(registry, config.ai.backend)`. Catches `AIServiceError` with `code === 'CLI_NOT_FOUND'`, prints `getInstallInstructions(registry)`, exits with code 2. Logs backend name, `cliCommand`, model if `debug === true`.\n\n### Phase 4: AI Synthesis\nInstantiates `AIService` with extended timeout `Math.max(config.ai.timeoutMs, 600_000)` (minimum 10 minutes). Calls `buildSpecPrompt(docs, annexFiles.length > 0 ? annexFiles : undefined)` to construct system/user prompt pair. Creates `ProgressLog` via `ProgressLog.create(absolutePath)`, writes header with ISO timestamp, project path, doc counts. Invokes `aiService.call({prompt: prompt.user, systemPrompt: prompt.system, taskLabel: 'specify'})` and awaits response.\n\n### Phase 5: Output Writing\nCalls `writeSpec(response.text, {outputPath, force, multiFile})` which returns `writtenFiles: string[]`. Catches `SpecExistsError`, logs error message, finalizes progress log, exits with code 1. Prints green success message listing all written paths.\n\n### Phase 6: Telemetry Finalization\nCalls `aiService.finalize(absolutePath)` to flush telemetry, extracts `summary` with `totalInputTokens`, `totalOutputTokens`, `totalDurationMs`. Constructs `summaryLine` string showing token counts, duration in seconds (1 decimal place), and output path. Logs summary and writes to progress log before calling `progressLog.finalize()`.\n\n## Dependencies\n\nImports `loadConfig` from `../config/loader.js`, `collectAgentsDocs` and `collectAnnexFiles` from `../generation/collector.js`, `buildSpecPrompt`, `writeSpec`, and `SpecExistsError` from `../specify/index.js`. Imports `AIService`, `AIServiceError`, `createBackendRegistry`, `resolveBackend`, `getInstallInstructions` from `../ai/index.js`. Imports `ProgressLog` from `../orchestration/index.js`. Imports `generateCommand` from `./generate.js` for auto-generation fallback.\n\n## Error Handling\n\n- **CLI_NOT_FOUND**: Catches `AIServiceError` with code `'CLI_NOT_FOUND'`, prints red error message, calls `getInstallInstructions(registry)`, exits with code 2\n- **SpecExistsError**: Catches error from `writeSpec()` when output exists without `force` flag, logs to progress log, prints red error message, exits with code 1\n- **Empty docs after generation**: Prints red error message \"No AGENTS.md files found after generation. Cannot proceed.\", exits with code 1\n\n## Behavioral Contracts\n\n**Output path resolution**: `options.output` resolves via `path.resolve(options.output)`, defaults to `path.join(absolutePath, 'specs', 'SPEC.md')` if undefined.\n\n**Token estimation**: Computes `estimatedTokensK = Math.ceil(totalChars / 4) / 1000` assuming 4 characters per token.\n\n**High-context warning threshold**: Warns in dry-run mode if `estimatedTokensK > 150`.\n\n**Timeout override**: Uses `Math.max(config.ai.timeoutMs, 600_000)` ensuring minimum 10-minute timeout regardless of config value.\n\n**Summary format**: `\"Tokens: {totalInputTokens} in / {totalOutputTokens} out | Duration: {totalDurationMs/1000:.1f}s | Output: {outputPath}\"`.\n### update.ts\n**Purpose:** `update.ts` implements the CLI entry point for incremental documentation updates via `updateCommand()`, detecting cha...\n\n**`update.ts` implements the CLI entry point for incremental documentation updates via `updateCommand()`, detecting changed files through SHA-256 hash comparison, cleaning orphaned artifacts, resolving AI backends, executing concurrent file analysis via `CommandRunner`, and regenerating `AGENTS.md` for affected directories.**\n\n## Exported Interface\n\n`updateCommand(targetPath: string, options: UpdateCommandOptions): Promise<void>` — main entry point for the `are update` CLI command, orchestrates the incremental update workflow including plan preparation, backend resolution, AI service initialization, concurrent file analysis, and directory-level documentation regeneration. Exit codes: 0 (all success), 1 (partial failure), 2 (total failure/no CLI found).\n\n`UpdateCommandOptions` — configuration interface with fields:\n- `uncommitted?: boolean` — include staged + working directory changes via `git status --porcelain`\n- `dryRun?: boolean` — display update plan without writing files\n- `concurrency?: number` — override worker pool size (1-10, default from config)\n- `failFast?: boolean` — abort on first task failure\n- `debug?: boolean` — enable verbose subprocess logging with heap/RSS metrics\n- `trace?: boolean` — emit NDJSON trace events to `.agents-reverse-engineer/traces/`\n\n## Update Workflow\n\n`updateCommand()` executes a six-phase incremental workflow:\n\n1. **Trace initialization**: `createTraceWriter()` before config loading to enable tracing throughout orchestrator\n2. **Plan preparation**: `orchestrator.preparePlan({ includeUncommitted, dryRun })` returns `UpdatePlan` with `filesToAnalyze`, `filesToSkip`, `cleanup` (deleted `.sum` files, deleted `AGENTS.md`), `affectedDirs`, `currentCommit`, `isFirstRun`\n3. **Backend resolution**: `resolveBackend(registry, config.ai.backend)` with error handling for `CLI_NOT_FOUND` (prints install instructions, exits with code 2)\n4. **AI service setup**: instantiates `AIService` with timeout/retry config, enables debug mode if `--debug`, sets subprocess log directory if `--trace` (creates timestamped subdirectory in `.agents-reverse-engineer/subprocess-logs/`)\n5. **File analysis**: `runner.executeUpdate(plan.filesToAnalyze, absolutePath, config)` processes changed files concurrently, returns `summary` with `filesProcessed`, `filesFailed`\n6. **Directory regeneration**: iterates `plan.affectedDirs` sequentially (concurrency=1), reads existing `AGENTS.md` via `readFile()` checking for `GENERATED_MARKER`, calls `buildDirectoryPrompt()` with `existingAgentsMd` parameter for incremental context, invokes `aiService.call()`, writes via `writeAgentsMd()`\n\n## Display Formatting\n\n`formatPlan(plan: UpdatePlan): string` — renders update plan with status markers:\n- File statuses: `+` (added, green), `R` (renamed, blue), `M` (modified, yellow), `=` (unchanged, dim)\n- Displays `currentCommit` (7-char truncation), file counts (analyze/skip/cleanup), renamed file oldPath with `(was: ...)` annotation\n- Special case: first run detection via `plan.isFirstRun` → suggests `are generate` for initial documentation\n- Empty plan detection: all counts zero → \"No changes detected since last run\"\n\n`formatCleanup(plan: UpdatePlan): string[]` — renders cleanup actions:\n- `cleanup.deletedSumFiles` — orphaned `.sum` files (deleted/renamed sources)\n- `cleanup.deletedAgentsMd` — `AGENTS.md` removed from empty directories\n\n## Phase 2: Directory Regeneration\n\nSequential iteration over `plan.affectedDirs` with:\n- `knownDirs` set prevents phantom path validation errors by passing to `buildDirectoryPrompt()`\n- Reads existing `AGENTS.md` via `readFile()`, checks for `GENERATED_MARKER` substring (user-authored files skipped)\n- Passes `existingAgentsMd` to `buildDirectoryPrompt()` for incremental update context (preserves user annotations)\n- Emits trace events: `phase:start` (phase: `'update-phase-dir-regen'`), `task:start`/`task:done` for each directory\n- Uses `ProgressReporter.onDirectoryStart()`/`onDirectoryDone()` for console/log output with token counts\n- Error handling: logs `WARN` without aborting, increments `dirsFailed` counter\n\n## Telemetry and State Management\n\nAfter phase completion:\n- `aiService.finalize(absolutePath)` writes run log to `.agents-reverse-engineer/logs/run-<timestamp>.json`\n- `progressLog.finalize()` flushes human-readable `.agents-reverse-engineer/progress.log`\n- `tracer.finalize()` writes buffered NDJSON trace events via promise-chain serialization\n- `cleanupOldTraces(absolutePath)` if `--trace` enabled, retains 500 most recent trace files\n- `orchestrator.recordRun(currentCommit, filesProcessed, filesSkipped)` — no-op in frontmatter mode (SHA-256 hashing), kept for API compatibility with legacy metadata tracking\n\n## Progress Monitoring\n\n`ProgressLog.create(absolutePath)` creates `.agents-reverse-engineer/progress.log` for real-time monitoring:\n- Writes session header with ISO 8601 timestamp, project path, file/directory counts\n- `progressLog.write()` called for each directory start/done event\n- Enables `tail -f .agents-reverse-engineer/progress.log` pattern for long-running updates\n\n## Exit Code Strategy\n\nDetermines exit code based on `summary.filesProcessed` and `summary.filesFailed`:\n- Code 2: total failure (zero files processed, nonzero failed) or CLI not found\n- Code 1: partial failure (some files processed, some failed)\n- Code 0: all success (zero failures) or no files to process (no-op update)\n\n## Import Map (verified — use these exact paths)\n\nclean.ts:\n  ../output/logger.js → createLogger\n  ../generation/writers/agents-md.js → GENERATED_MARKER\n\ndiscover.ts:\n  ../config/loader.js → loadConfig\n  ../discovery/run.js → discoverFiles\n  ../output/logger.js → createLogger\n  ../generation/orchestrator.js → createOrchestrator\n  ../generation/executor.js → buildExecutionPlan, formatExecutionPlanAsMarkdown\n  ../orchestration/index.js → ProgressLog\n  ../types/index.js → DiscoveryResult (type)\n  ../orchestration/trace.js → ITraceWriter (type)\n\ngenerate.ts:\n  ../config/loader.js → loadConfig\n  ../output/logger.js → createLogger\n  ../discovery/run.js → discoverFiles\n  ../generation/orchestrator.js → createOrchestrator, type GenerationPlan\n  ../generation/executor.js → buildExecutionPlan\n  ../ai/index.js → AIService, AIServiceError, createBackendRegistry, resolveBackend, getInstallInstructions\n  ../orchestration/index.js → CommandRunner, ProgressLog, createTraceWriter, cleanupOldTraces\n\nindex.ts:\n  ../installer/index.js → runInstaller, parseInstallerArgs\n  ../version.js → getVersion\n\ninit.ts:\n  ../config/loader.js → configExists, writeDefaultConfig, CONFIG_DIR, CONFIG_FILE\n  ../output/logger.js → createLogger\n\nspecify.ts:\n  ../config/loader.js → loadConfig\n  ../generation/collector.js → collectAgentsDocs, collectAnnexFiles\n  ../specify/index.js → buildSpecPrompt, writeSpec, SpecExistsError\n  ../ai/index.js → AIService, AIServiceError, createBackendRegistry, resolveBackend, getInstallInstructions\n  ../orchestration/index.js → ProgressLog\n\nupdate.ts:\n  ../config/loader.js → loadConfig\n  ../output/logger.js → createLogger\n  ../update/index.js → createUpdateOrchestrator, type UpdatePlan\n  ../generation/writers/agents-md.js → writeAgentsMd, GENERATED_MARKER\n  ../generation/prompts/index.js → buildDirectoryPrompt\n  ../ai/index.js → AIService, AIServiceError, createBackendRegistry, resolveBackend, getInstallInstructions\n  ../orchestration/index.js → CommandRunner, ProgressReporter, ProgressLog, createTraceWriter, cleanupOldTraces\n\n## Project Directory Structure\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>",
      "systemPrompt": "You are generating an AGENTS.md file — a directory-level overview for AI coding assistants.\n\nCRITICAL: Output ONLY the raw markdown content. No code fences, no preamble, no explanations, no conversational text. Your entire response IS the AGENTS.md file content.\n\nOUTPUT FORMAT:\n- First line MUST be exactly: <!-- Generated by agents-reverse-engineer -->\n- Use a # heading with the directory name\n- Write a one-paragraph purpose statement for the directory\nusage\nADAPTIVE SECTIONS:\nAnalyze the directory contents and choose the most relevant sections. Do NOT use a fixed template. Instead, select sections that best document this specific directory for an AI that needs to reconstruct or extend the project.\n\nConsider these section types (choose what applies):\n- **Contents**: Group files by purpose/category under ## headings. For each file: markdown link [filename](./filename) and a one-line description.\n- **Subdirectories**: If subdirectories exist, list them with links [dirname/](./dirname/) and brief summaries.\n- **Architecture / Data Flow**: If files form a pipeline, request/response chain, or layered architecture, document it.\n- **Stack**: If this is a package root (has package.json, Cargo.toml, go.mod, etc.), document the technology stack, key scripts, and entry points.\n- **Structure**: If the directory layout follows a convention (feature-sliced, domain-driven, MVC, etc.), document it.\n- **Patterns**: If files share recurring design patterns (factory, strategy, middleware, barrel re-export), name and document them.\n- **Configuration**: If the directory contains config files, schemas, or environment definitions, document the config surface area.\n- **API Surface**: If the directory exports a public API (barrel index, route definitions, SDK), document the interface contract.\n- **File Relationships**: How files collaborate, depend on each other, or share state.\n- **Behavioral Contracts**: If files contain regex patterns, format specifications, magic constants, or template strings that define observable behavior, collect them in a dedicated section. Preserve verbatim patterns from file summaries — do NOT paraphrase regex into prose. This section is MANDATORY when file summaries contain behavioral artifacts.\n- **Reproduction-Critical Constants**: If file summaries reference annex files (via ## Annex References sections), list them with links. Example: \"Full prompt template text: [templates.ts.annex.md](./prompts/templates.ts.annex.md)\". Do NOT reproduce annex content in AGENTS.md — just link to it.\n\nChoose any relevant sections or create your own based on the directory contents. The goal is to provide a comprehensive overview that captures the essence of the directory's role in the project and how its files work together, with a focus on what an AI coding assistant would need to know to effectively interact with this code.\n\nSCOPE:\n- AGENTS.md is a NAVIGATIONAL INDEX — help an AI find the right file quickly\n- Focus on: what each file does, how files relate, directory-level patterns\n- Do NOT reproduce full architecture sections — those belong in the root CLAUDE.md\n\nPATH ACCURACY (MANDATORY):\n- When referencing files or modules outside this directory, use ONLY paths from the \"Import Map\" section\n- Do NOT invent, rename, or guess module paths — if a path isn't in the Import Map, don't reference it\n- Use the exact directory names from \"Project Directory Structure\" — do NOT rename directories\n  (e.g., if the directory is called \"cli\", write \"src/cli/\", NOT \"src/commands/\")\n- Cross-module references must use the specifier format from actual import statements\n  (e.g., \"../generation/writers/sum.js\", NOT \"../fs/sum-file.js\")\n- If you are unsure about a path, omit the cross-reference rather than guessing\n\nCONSISTENCY (MANDATORY):\n- Do not contradict yourself within the same document\n- If you describe a technique (e.g., \"regex-based\"), do not call it something else later (e.g., \"AST-based\")\n- When stating version numbers, engines, or config fields, use ONLY values present in the file summaries\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this directory\", \"this module\", \"provides\", \"responsible for\", \"is used to\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Per-file descriptions in Contents sections: 1-2 sentences maximum. Reference key symbols but do not reproduce full summaries.\n- Behavioral contracts (regex patterns, format specs, constants) belong in a separate Behavioral Contracts section, not in per-file descriptions.\n- Annex file references: link to .annex.md files, do not inline their content. One line per annex reference.\n- Subdirectory descriptions: 1-2 sentences maximum. Capture the directory's role, not its full contents.\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- Key exported symbols from file summaries MUST appear in the directory overview\n- Preserve exact casing of identifiers\n\nUSER NOTES:\n- If \"User Notes\" are provided in the prompt, they contain user-defined instructions that will be automatically prepended to your output\n- Do NOT repeat or paraphrase user notes in your generated content — they are included separately\n- You may reference information from user notes for context",
      "response": "<!-- Generated by agents-reverse-engineer -->\n\n# cli\n\nCommand entry points implementing CLI argument parsing, command routing, and orchestration of AI-driven documentation generation workflows (init, discover, generate, update, clean, specify) via backend abstraction, concurrent execution pools, and incremental hash-based updates.\n\n## Commands\n\n### [clean.ts](./clean.ts)\n`cleanCommand(targetPath, {dryRun})` deletes `.sum`, `.annex.md`, `AGENTS.md` (marker-filtered), `CLAUDE.md`, `GENERATION-PLAN.md` via parallel `fast-glob` discovery, restores `AGENTS.local.md` → `AGENTS.md`, logs deletions/skips.\n\n### [discover.ts](./discover.ts)\n`discoverCommand(targetPath, {tracer, debug})` walks directory via `discoverFiles()` filter chain, writes file list to `progress.log`, generates `GENERATION-PLAN.md` via `buildExecutionPlan()` post-order traversal, emits `discovery:start`/`discovery:end` trace events.\n\n### [generate.ts](./generate.ts)\n`generateCommand(targetPath, {dryRun, concurrency, failFast, debug, trace})` orchestrates three-phase pipeline: concurrent `.sum` file analysis via `CommandRunner.executeGenerate()`, directory `AGENTS.md` aggregation, root document synthesis (`CLAUDE.md`/`GEMINI.md`/`OPENCODE.md`), exits with codes 0 (success), 1 (partial failure), 2 (total failure).\n\n### [index.ts](./index.ts)\nCLI entry point with shebang `#!/usr/bin/env node`, implements `parseArgs()` flag parser supporting `--dry-run`, `--concurrency <n>`, `--fail-fast`, `--debug`, `--trace`, `--uncommitted`, `--force`, `--multi-file`, routes to command modules, triggers `runInstaller()` on install/uninstall/installer flags.\n\n### [init.ts](./init.ts)\n`initCommand(root, {force})` creates `.agents-reverse-engineer/config.yaml` via `writeDefaultConfig()`, checks `configExists()` to prevent overwrites unless `force` enabled, exits with code 1 on `EACCES`/`EPERM` permission errors.\n\n### [specify.ts](./specify.ts)\n`specifyCommand(targetPath, {output, force, dryRun, multiFile, debug, trace})` synthesizes project specs from `AGENTS.md` corpus via `collectAgentsDocs()`, auto-generates missing docs via `generateCommand()`, invokes `AIService.call()` with 600s minimum timeout, writes via `writeSpec()` to `specs/SPEC.md` or custom path.\n\n### [update.ts](./update.ts)\n`updateCommand(targetPath, {uncommitted, dryRun, concurrency, failFast, debug, trace})` computes delta via SHA-256 hash comparison in `preparePlan()`, cleans orphaned artifacts, regenerates `.sum` for changed files via `runner.executeUpdate()`, rebuilds `AGENTS.md` for `affectedDirs` sequentially, logs to `progress.log`.\n\n## Execution Patterns\n\n**Dry-run mode** (`--dry-run`): clean/generate/update/specify commands display plans without filesystem writes or AI calls. generate shows file/directory/root counts with `buildExecutionPlan()`, specify estimates tokens via `Math.ceil(totalChars / 4) / 1000`.\n\n**Progress monitoring**: generate/update/specify create `ProgressLog.create(absolutePath)` writing to `.agents-reverse-engineer/progress.log` with ISO timestamps, file counts, task status. Enables `tail -f` real-time monitoring.\n\n**Trace emission** (`--trace`): generate/update/specify instantiate `createTraceWriter()` before config loading, pass `tracer` to all orchestrator calls, emit NDJSON events (`phase:start`/`end`, `worker:start`/`end`, `task:pickup`/`done`, `subprocess:spawn`/`exit`, `retry`), call `cleanupOldTraces()` after `tracer.finalize()`.\n\n**Backend resolution**: generate/update/specify call `createBackendRegistry()`, `resolveBackend(registry, config.ai.backend)`, catch `AIServiceError` with `code === 'CLI_NOT_FOUND'`, print `getInstallInstructions(registry)`, exit with code 2.\n\n**AI service lifecycle**: generate/update/specify instantiate `AIService(backend, {timeoutMs, maxRetries, model, telemetry.keepRuns})`, call `setDebug(true)` if `--debug`, call `setSubprocessLogDir()` if `--trace`, invoke via `aiService.call()` or `runner.executeGenerate()`/`runner.executeUpdate()`, finalize via `aiService.finalize(absolutePath)` writing run logs to `.agents-reverse-engineer/logs/`.\n\n**Installer routing**: index.ts triggers `runInstaller()` on three conditions: (1) zero args (interactive mode), (2) installer flags (`--global`, `--local`, `--runtime`, `--force`) without command, (3) explicit `install`/`uninstall` command. Passes `parseInstallerArgs(args)` with `uninstall: true` for uninstall command.\n\n## Exit Code Strategy\n\n**generate/update**:\n- Code 2: total failure (`filesProcessed === 0 && filesFailed > 0`) or CLI not found\n- Code 1: partial failure (`filesFailed > 0` with some success)\n- Code 0: full success (`filesFailed === 0`)\n\n**specify**:\n- Code 2: `CLI_NOT_FOUND` error from `resolveBackend()`\n- Code 1: `SpecExistsError` (output exists without `--force`) or empty docs after auto-generation\n- Code 0: successful write\n\n**init**:\n- Code 1: `EACCES`/`EPERM` permission error or write failure\n- Code 0: config created successfully\n\n**clean**: No explicit exit codes (relies on thrown errors).\n\n## File Relationships\n\n- index.ts imports all command modules, routes via switch statement on `command` string\n- generate.ts imports `discoverFiles()` from `../discovery/run.js`, `createOrchestrator()` from `../generation/orchestrator.js`, `buildExecutionPlan()` from `../generation/executor.js`, `AIService` from `../ai/index.js`, `CommandRunner` from `../orchestration/index.js`\n- update.ts imports `createUpdateOrchestrator()` from `../update/index.js`, `buildDirectoryPrompt()` from `../generation/prompts/index.js`, `writeAgentsMd()` from `../generation/writers/agents-md.js`\n- specify.ts imports `collectAgentsDocs()` from `../generation/collector.js`, `buildSpecPrompt()` from `../specify/index.js`, calls `generateCommand()` from `./generate.js` for auto-generation fallback\n- clean.ts imports `GENERATED_MARKER` from `../generation/writers/agents-md.js` for user-authored AGENTS.md detection\n- discover.ts imports `formatExecutionPlanAsMarkdown()` from `../generation/executor.js` for `GENERATION-PLAN.md` rendering\n- init.ts imports `configExists()`, `writeDefaultConfig()` from `../config/loader.js`\n\n## Behavioral Contracts\n\n**Argument parsing** (index.ts `parseArgs()`):\n- Flags starting with `--` populate `flags: Set<string>` or `values: Map<string, string>` if next arg lacks `--` prefix\n- Short flags expanded: `-h` → `help`, `-g` → `global`, `-l` → `local`, `-V` → `version`\n- First non-flag arg becomes `command`, subsequent populate `positional: string[]`\n\n**Token estimation** (specify.ts dry-run):\n```typescript\nestimatedTokensK = Math.ceil(totalChars / 4) / 1000\n```\n\n**Plan formatting** (update.ts `formatPlan()`):\n- Status markers: `+` (added, green), `R` (renamed, blue), `M` (modified, yellow), `=` (unchanged, dim)\n- First run: `plan.isFirstRun === true` → suggests `are generate`\n- Empty plan: all counts zero → \"No changes detected since last run\"\n\n**Cleanup patterns** (clean.ts):\n- Parallel `fast-glob` with patterns: `**/*.sum`, `**/*.annex.md`, `**/AGENTS.md`, `**/AGENTS.local.md`\n- Ignore patterns: `['**/node_modules/**', '**/.git/**']`\n- Marker detection: `content.includes(GENERATED_MARKER)` where `GENERATED_MARKER = '<!-- Generated by agents-reverse-engineer -->'`\n\n**Telemetry summary** (specify.ts):\n```typescript\nsummaryLine = `Tokens: ${totalInputTokens} in / ${totalOutputTokens} out | Duration: ${(totalDurationMs/1000).toFixed(1)}s | Output: ${outputPath}`\n```\n\n**Subprocess timeout override** (specify.ts):\n```typescript\ntimeoutMs = Math.max(config.ai.timeoutMs, 600_000) // minimum 10 minutes\n```",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 2287,
      "cacheReadTokens": 17587,
      "cacheCreationTokens": 18764,
      "latencyMs": 66861,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": []
    },
    {
      "timestamp": "2026-02-09T17:47:47.811Z",
      "prompt": "Generate AGENTS.md for directory: \"src/ai\" (ai)\n\n## File Summaries (6 files)\n\n### index.ts\n**Purpose:** Barrel export aggregating AI service layer public API: exports AIService orchestrator, BackendRegistry with resolveBa...\n\n**Barrel export aggregating AI service layer public API: exports AIService orchestrator, BackendRegistry with resolveBackend/detectBackend/createBackendRegistry, withRetry retry utility, runSubprocess wrapper, isCommandOnPath utility, and all types from ./types.js (AIBackend, AIResponse, AICallOptions, SubprocessResult, RetryOptions, TelemetryEntry, RunLog, FileRead, AIServiceError) with AIServiceOptions from ./service.js.**\n\n## Exported Symbols\n\n**Types:**\n- `AIBackend` — Backend interface with `name`, `isAvailable()`, `call()` methods\n- `AIResponse` — Response shape with `content: string`, `tokensUsed`, `costUsd` metrics\n- `AICallOptions` — Call parameters with `prompt: string`, `fileContext`, `model`, `taskType`, `filePath`\n- `SubprocessResult` — Subprocess execution result with `stdout`, `stderr`, `exitCode`, `signal`, `error`\n- `RetryOptions` — Retry configuration with `maxAttempts: number`, `baseDelayMs: number`, `maxDelayMs: number`, `shouldRetry: (error) => boolean`\n- `TelemetryEntry` — Per-call telemetry with `timestamp`, `backend`, `model`, `tokensUsed`, `costUsd`, `durationMs`, `error`, `filesRead`\n- `RunLog` — Aggregated run metadata with `runId`, `startTime`, `endTime`, `totalInputTokens`, `totalOutputTokens`, `totalCacheReadTokens`, `totalCacheWriteTokens`, `totalCostUsd`, `totalDurationMs`, `errorCount`, `uniqueFilesRead`, `entries: TelemetryEntry[]`\n- `FileRead` — File read metadata with `path: string`, `sizeBytes: number`, `linesRead: number`\n- `AIServiceOptions` — Service constructor options from `./service.js`\n\n**Error Class:**\n- `AIServiceError` — Custom error class for AI service failures\n\n**Service Orchestrator:**\n- `AIService` — Main service class coordinating backend calls, retry logic, telemetry logging\n\n**Backend Registry:**\n- `BackendRegistry` — Registry class managing available backends\n- `createBackendRegistry()` — Factory creating registry with Claude/Gemini/OpenCode backends registered\n- `resolveBackend(registry: BackendRegistry, backendName: string): Promise<AIBackend>` — Resolves backend by name ('claude' | 'gemini' | 'opencode' | 'auto'), auto-detection tries backends in order until `isAvailable()` succeeds\n- `detectBackend(registry: BackendRegistry): Promise<AIBackend | null>` — Auto-detects first available backend\n- `getInstallInstructions(backendName: string): string` — Returns installation instructions for specified backend\n\n**Retry Utility:**\n- `withRetry<T>(fn: () => Promise<T>, options?: RetryOptions): Promise<T>` — Exponential backoff retry wrapper\n- `DEFAULT_RETRY_OPTIONS` — Default retry configuration constant\n\n**Subprocess Wrapper:**\n- `runSubprocess(options: {...}): Promise<SubprocessResult>` — Spawns subprocess with resource limits, timeout enforcement, process group killing\n\n**Backend Utilities:**\n- `isCommandOnPath(command: string): Promise<boolean>` — Checks if command exists in PATH via `which` (Unix) or `where` (Windows)\n\n## Architecture Role\n\nServes as the single entry point for AI service layer consumption by CLI commands and orchestration modules. Prevents direct imports from `src/ai/backends/` or `src/ai/telemetry/` by re-exporting all necessary symbols. Consumers import from `./ai/index.js` exclusively.\n\n## Module Isolation Boundary\n\nEnforces encapsulation of backend implementations (Claude/Gemini/OpenCode adapters in `./backends/`) and telemetry subsystem (`./telemetry/`) by exposing only high-level abstractions. Internal implementation details remain hidden behind `AIBackend` interface and `AIService` orchestrator.\n\n## Usage Pattern\n\n```typescript\nimport { AIService, createBackendRegistry, resolveBackend } from './ai/index.js';\n\nconst registry = createBackendRegistry();\nconst backend = await resolveBackend(registry, 'auto');\nconst service = new AIService(backend, {\n  timeoutMs: 120_000,\n  maxRetries: 3,\n  telemetry: { keepRuns: 10 },\n});\n\nconst response = await service.call({ prompt: 'Hello' });\n```\n### registry.ts\n**Purpose:** registry.ts orchestrates AIBackend adapter registration, auto-detection via PATH availability checks, and runtime bac...\n\n**registry.ts orchestrates AIBackend adapter registration, auto-detection via PATH availability checks, and runtime backend resolution with actionable CLI_NOT_FOUND error messages containing install instructions.**\n\n## Exported Classes\n\n**BackendRegistry** maintains a `Map<string, AIBackend>` of registered backends in insertion order, which determines auto-detection priority. Methods:\n- `register(backend: AIBackend): void` — adds backend keyed by its `name` property\n- `get(name: string): AIBackend | undefined` — retrieves backend by name\n- `getAll(): AIBackend[]` — returns all backends in registration order\n\n## Exported Functions\n\n**createBackendRegistry(): BackendRegistry** instantiates a pre-populated registry with three backends in priority order: `ClaudeBackend` (fully implemented), `GeminiBackend` (experimental stub), `OpenCodeBackend` (experimental stub). Registration order controls `detectBackend()` iteration sequence.\n\n**detectBackend(registry: BackendRegistry): Promise<AIBackend | null>** iterates `registry.getAll()` calling `backend.isAvailable()` on each, returning the first backend whose CLI exists on PATH or `null` if none found. Follows priority: Claude > Gemini > OpenCode.\n\n**getInstallInstructions(registry: BackendRegistry): string** aggregates `backend.getInstallInstructions()` from all registered backends into newline-separated multi-line string for CLI_NOT_FOUND error message formatting.\n\n**resolveBackend(registry: BackendRegistry, requested: string | 'auto'): Promise<AIBackend>** implements the main resolution logic:\n- If `requested === 'auto'`: calls `detectBackend()`, throws `AIServiceError` with code `CLI_NOT_FOUND` and aggregated install instructions if no backend detected\n- If `requested` is a backend name: calls `registry.get(requested)`, throws `CLI_NOT_FOUND` if unknown (listing all known backend names), throws `CLI_NOT_FOUND` with backend-specific install instructions if `backend.isAvailable()` returns false (checks for `backend.cliCommand` on PATH)\n\n## Integration Points\n\nImports `AIBackend` interface and `AIServiceError` from `./types.js`. Imports concrete adapters: `ClaudeBackend` from `./backends/claude.js`, `GeminiBackend` from `./backends/gemini.js`, `OpenCodeBackend` from `./backends/opencode.js`. \n\nCalled by `AIService` in `src/ai/service.ts` during initialization to resolve the backend specified in configuration `ai.backend` field (defaults to `'auto'`). The resolved `AIBackend` instance provides the `cliCommand`, `buildArgs()`, and `parseResponse()` methods used by `runSubprocess()` for AI CLI invocation.\n\n## Error Handling\n\nAll `AIServiceError` instances thrown with code `'CLI_NOT_FOUND'`:\n- Auto-detection failure includes aggregated install instructions for all backends via `getInstallInstructions()`\n- Unknown backend name includes comma-separated list of known backend names from `registry.getAll()`\n- Unavailable backend includes specific `backend.getInstallInstructions()` and mentions the missing `backend.cliCommand`\n### retry.ts\n**Purpose:** Exponential backoff retry wrapper for transient AI service failures with configurable predicates, timing multipliers,...\n\n**Exponential backoff retry wrapper for transient AI service failures with configurable predicates, timing multipliers, and jitter-based delay randomization.**\n\n## Exported Functions\n\n`withRetry<T>(fn: () => Promise<T>, options: RetryOptions): Promise<T>` executes async function with exponential backoff retry logic. Returns result immediately on success. On transient failure matching `options.isRetryable` predicate, waits with exponential delay then retries up to `options.maxRetries` times. On permanent failure (`isRetryable` returns false), throws immediately without retrying. After exhausting all retries, throws last error. Delay formula: `min(baseDelayMs * multiplier^attempt, maxDelayMs) + jitter` where jitter is random value in [0, 500ms]. Invokes optional `options.onRetry(attempt, error)` callback before each delay.\n\n## Exported Constants\n\n`DEFAULT_RETRY_OPTIONS` provides base retry configuration with `maxRetries: 3` (4 total attempts), `baseDelayMs: 1_000` (1 second base delay), `maxDelayMs: 8_000` (8 second cap), `multiplier: 2` (exponential doubling). Intentionally omits `isRetryable` and `onRetry` predicates requiring caller-specific implementation. Type satisfies `Omit<RetryOptions, 'isRetryable' | 'onRetry'>` for partial spread pattern.\n\n## Retry Flow Control\n\nLoop executes `attempt` from 0 to `options.maxRetries` inclusive. On catch block: throws immediately if `attempt === options.maxRetries` (exhausted) or `!options.isRetryable(error)` (permanent failure). Computes `exponentialDelay = baseDelayMs * Math.pow(multiplier, attempt)`, applies `cappedDelay = Math.min(exponentialDelay, maxDelayMs)`, adds `jitter = Math.random() * 500`, waits `delay = cappedDelay + jitter` via `setTimeout` promise wrapper. Invokes `options.onRetry?.(attempt + 1, error)` before waiting (attempt is 0-indexed, callback receives 1-indexed retry number).\n\n## Integration Pattern\n\nDesigned for `AIService.call()` wrapping `runSubprocess()` invocations. Caller provides `isRetryable` predicate matching rate limit error patterns in stderr (strings: \"rate limit\", \"429\", \"too many requests\", \"overloaded\"). Callback `onRetry` emits trace events via `ITraceWriter.emit('retry', ...)` for telemetry correlation with subprocess lifecycle events.\n\n## Jitter Strategy\n\nAdds uniform random jitter `Math.random() * 500` (0-500ms range) to prevent thundering herd when multiple workers hit same rate limit simultaneously. Jitter applied after exponential backoff and delay cap, not subject to `maxDelayMs` constraint.\n### service.ts\n**Purpose:** AIService orchestrates AI CLI subprocess execution with retry logic, telemetry logging, timeout enforcement, and rate...\n\n**AIService orchestrates AI CLI subprocess execution with retry logic, telemetry logging, timeout enforcement, and rate-limit detection for agents-reverse-engineer's three-phase documentation pipeline.**\n\n## Exported Interface\n\n**class AIService** — Main entry point for AI calls with integrated subprocess management, exponential backoff retry, telemetry accumulation, and run log finalization.\n\n**constructor(backend: AIBackend, options: AIServiceOptions)** — Initializes service with backend adapter (Claude/Gemini/OpenCode) and configuration for timeouts, retries, telemetry retention.\n\n**async call(options: AICallOptions): Promise<AIResponse>** — Executes AI call with retry wrapper around `runSubprocess()`, records `TelemetryEntry` on success/failure, merges service-level `model` default with per-call override, emits `subprocess:spawn/exit` and `retry` trace events via `ITraceWriter`.\n\n**async finalize(projectRoot: string): Promise<{ logPath: string; summary: RunLog['summary'] }>** — Writes accumulated run log via `writeRunLog()`, invokes `cleanupOldLogs()` for retention enforcement, returns log path and summary statistics.\n\n**setTracer(tracer: ITraceWriter): void** — Attaches trace writer for subprocess lifecycle and retry event emission.\n\n**setDebug(enabled: boolean): void** — Enables stderr logging of spawn metadata (active subprocess count, heap/RSS, timeout, PID, exit code, duration).\n\n**setSubprocessLogDir(dir: string): void** — Enables per-subprocess `.log` file writes containing task label, PID, command, exit code, signal, duration, stdout, stderr.\n\n**addFilesReadToLastEntry(filesRead: FileRead[]): void** — Attaches file-read metadata to most recent `TelemetryEntry` via `TelemetryLogger.setFilesReadOnLastEntry()`.\n\n**getSummary(): RunLog['summary']** — Returns current aggregated statistics without finalizing.\n\n**interface AIServiceOptions** — Service configuration with `timeoutMs`, `maxRetries`, optional `model`, and nested `telemetry.keepRuns`.\n\n## Call Execution Flow\n\n`call()` increments `callCount`, merges effective options (service-level `model` as default), builds CLI args via `backend.buildArgs()`, wraps `runSubprocess()` in `withRetry()` configured to retry only `RATE_LIMIT` errors (timeouts are NOT retried per MEMORY.md resource constraint rationale), increments/decrements `activeSubprocesses` counter before/after subprocess completion, emits `subprocess:spawn` via `onSpawn` callback at actual spawn time (not after completion), emits `subprocess:exit` after completion with `childPid`, `exitCode`, `signal`, `durationMs`, `timedOut`, invokes `enqueueSubprocessLog()` fire-and-forget for optional log writes, parses response via `backend.parseResponse()` wrapped in try-catch throwing `AIServiceError('PARSE_ERROR')` on parse failure, records `TelemetryEntry` with prompt, systemPrompt, response, model, tokens (input/output/cacheRead/cacheCreation), latencyMs, exitCode, retryCount, thinking='not supported', filesRead=[] (populated later via `addFilesReadToLastEntry`), throws `AIServiceError('TIMEOUT')` on `SubprocessResult.timedOut`, throws `AIServiceError('RATE_LIMIT')` when stderr matches patterns via `isRateLimitStderr()`, throws `AIServiceError('SUBPROCESS_ERROR')` on non-zero exit codes not matching rate-limit patterns.\n\n## Rate Limit Detection\n\n**const RATE_LIMIT_PATTERNS = ['rate limit', '429', 'too many requests', 'overloaded']** — Substring patterns for detecting transient rate-limit errors in subprocess stderr.\n\n**function isRateLimitStderr(stderr: string): boolean** — Case-insensitive substring search across `RATE_LIMIT_PATTERNS`.\n\n## Retry Configuration\n\n`withRetry()` invoked with spread `DEFAULT_RETRY_OPTIONS` (maxRetries=3, baseDelayMs=1000, maxDelayMs=8000, multiplier=2) plus custom `isRetryable` predicate accepting only `AIServiceError.code === 'RATE_LIMIT'` (timeouts excluded per resource constraint mitigation), `onRetry` callback emitting `console.error()` warnings and `tracer.emit({ type: 'retry', attempt, taskLabel, errorCode })` events.\n\n## Subprocess Logging\n\n**private enqueueSubprocessLog(result: SubprocessResult, taskLabel: string): void** — Fire-and-forget serialized write via `logWriteQueue` promise chain, sanitizes taskLabel by replacing `/` with `--` and non-alphanumeric chars with `_`, writes `${sanitized}_pid${childPid}.log` containing metadata header (task/pid/command/exit/signal/duration/timed_out) plus stdout/stderr sections, silently swallows errors (log loss acceptable).\n\n## Debug Output\n\nWhen `debug=true`, `call()` logs to stderr before spawn (active count, heap/RSS via `formatBytes()`, timeout), after exit (PID, exitCode, duration, active count), plus timeout warnings showing elapsed time exceeding configured `timeoutMs`.\n\n**function formatBytes(bytes: number): string** — Human-readable byte formatting with B/KB/MB units.\n\n## Telemetry Accumulation\n\n**private readonly logger: TelemetryLogger** — In-memory accumulator instantiated with `runId = new Date().toISOString()`.\n\n**private callCount: number = 0** — Running total of `call()` invocations.\n\n**private activeSubprocesses: number = 0** — Tracks concurrent subprocess count for debug logging.\n\n**private subprocessLogDir: string | null = null** — Optional directory for subprocess output logs.\n\n**private logWriteQueue: Promise<void> = Promise.resolve()** — Serializes `mkdir()` and `writeFile()` operations to prevent race conditions from concurrent workers.\n\n## Dependencies\n\nImports `AIBackend`, `AICallOptions`, `AIResponse`, `SubprocessResult`, `TelemetryEntry`, `RunLog`, `FileRead`, `AIServiceError` from `./types.js`; `runSubprocess` from `./subprocess.js`; `withRetry`, `DEFAULT_RETRY_OPTIONS` from `./retry.js`; `TelemetryLogger` from `./telemetry/logger.js`; `writeRunLog` from `./telemetry/run-log.js`; `cleanupOldLogs` from `./telemetry/cleanup.js`; `ITraceWriter` from `../orchestration/trace.js`; `writeFile`, `mkdir` from `node:fs/promises`; `path` from `node:path`.\n\n## Integration Pattern\n\nUsed by `src/orchestration/runner.ts` which creates one `AIService` instance per CLI command execution, calls `setTracer()` and `setDebug()` based on CLI flags, invokes `call()` for each file/directory/root task via worker pool or sequential executor, calls `addFilesReadToLastEntry()` after each call to attach file metadata from prompt builder, calls `finalize()` at end of run to persist telemetry and enforce retention limits.\n### subprocess.ts\n**Purpose:** subprocess.ts spawns and manages AI CLI child processes with timeout enforcement, SIGKILL escalation, stdin piping, a...\n\n**subprocess.ts spawns and manages AI CLI child processes with timeout enforcement, SIGKILL escalation, stdin piping, and active subprocess tracking.**\n\n## Exported Functions\n\n**runSubprocess(command: string, args: string[], options: SubprocessOptions): Promise<SubprocessResult>** — Spawns AI CLI subprocess via `execFile()`, pipes optional `input` to stdin, enforces timeout via SIGTERM at `options.timeoutMs`, escalates to SIGKILL after 5s grace period if process doesn't exit, tracks active subprocesses in module-scoped Map, invokes `options.onSpawn(pid)` callback synchronously after spawn, always resolves (never rejects) with `SubprocessResult` containing `stdout`, `stderr`, `exitCode`, `signal`, `durationMs`, `timedOut` boolean, and `childPid`.\n\n**getActiveSubprocessCount(): number** — Returns current count of active subprocesses from module-scoped `activeSubprocesses` Map.\n\n**getActiveSubprocesses(): Array<{ pid: number; command: string; spawnedAt: number; runningMs: number }>** — Returns array of all active subprocess details with computed `runningMs` elapsed time.\n\n## Exported Types\n\n**SubprocessOptions** interface with required `timeoutMs: number`, optional `input?: string` for stdin piping, optional `onSpawn?: (pid: number | undefined) => void` callback fired synchronously when child process spawns.\n\n## Process Lifecycle Management\n\n**Timeout enforcement** sends SIGTERM at `timeoutMs` via `execFile` timeout option with `killSignal: 'SIGTERM'`, then schedules unref'd SIGKILL timer at `timeoutMs + SIGKILL_GRACE_MS` (5000ms constant) to force-kill hung processes that ignore SIGTERM.\n\n**Process group killing** attempts `process.kill(-child.pid, 'SIGKILL')` with negative PID to target entire process tree, falls back to single-process `process.kill(child.pid, 'SIGKILL')` if process group signal fails, wraps in try-catch since process may already be dead.\n\n**Active subprocess tracking** maintains module-scoped `activeSubprocesses` Map storing `{ command: string, spawnedAt: number }` keyed by PID, adds entry after spawn, removes in `execFile` callback, enables concurrency debugging via `getActiveSubprocessCount()` and `getActiveSubprocesses()`.\n\n**Stdin handling** writes `options.input` to `child.stdin` via `.write()` then calls `.end()` to send EOF, preventing child process from blocking indefinitely waiting for stdin closure (see comment: \"RESEARCH.md Pitfall 1\").\n\n## Exit Code Extraction\n\n**Exit code precedence** checks `error === null` (returns 0), then `typeof error.code === 'number'` (returns `error.code`), then `child.exitCode !== null` (returns `child.exitCode`), else defaults to 1. Handles `execFile` placing numeric exit codes in `error.code` while also using string error codes like `'ERR_CHILD_PROCESS_STDIO_MAXBUFFER'`.\n\n**Timeout detection** sets `timedOut = true` when `error !== null && 'killed' in error && error.killed === true`, which `execFile` sets when process exceeds timeout option and is terminated.\n\n## Configuration\n\n**maxBuffer** set to `10 * 1024 * 1024` (10MB) to accommodate large AI responses without triggering `ERR_CHILD_PROCESS_STDIO_MAXBUFFER`.\n\n**SIGKILL_GRACE_MS** constant defines 5000ms grace period between SIGTERM and SIGKILL escalation.\n\n## Integration Points\n\nImported by `src/ai/service.ts` as sole subprocess spawning mechanism for AI CLI backends (Claude Code, Gemini CLI, OpenCode). Returns `SubprocessResult` consumed by `AIService.call()` for retry logic, telemetry logging, and trace emission.\n### types.ts\n**Purpose:** Central type definitions for the AI service layer establishing contracts for backend adapters, subprocess execution, ...\n\n**Central type definitions for the AI service layer establishing contracts for backend adapters, subprocess execution, retry logic, and telemetry logging.**\n\n## Exported Types\n\n### Subprocess Execution\n\n**SubprocessResult** — Return value from subprocess wrapper after CLI process completion, always populated regardless of success/failure:\n- `stdout: string` — captured standard output\n- `stderr: string` — captured standard error  \n- `exitCode: number` — numeric exit code (0=success)\n- `signal: string | null` — termination signal or null for normal exit\n- `durationMs: number` — wall-clock duration in milliseconds\n- `timedOut: boolean` — whether process exceeded timeout threshold\n- `childPid?: number` — OS PID (undefined if spawn failed)\n\n### AI Call Interface\n\n**AICallOptions** — Input parameters for AI service calls:\n- `prompt: string` (required) — prompt text sent to model\n- `systemPrompt?: string` — optional system context/behavior instructions\n- `model?: string` — backend-specific model identifier (e.g., \"sonnet\", \"opus\")\n- `timeoutMs?: number` — subprocess timeout override in milliseconds\n- `maxTurns?: number` — maximum agentic turns (backend-dependent)\n- `taskLabel?: string` — tracing label (typically file path being processed)\n\n**AIResponse** — Normalized response shape that all backend adapters must produce:\n- `text: string` — AI model's generated text\n- `model: string` — model identifier as reported by backend\n- `inputTokens: number` — consumed input tokens\n- `outputTokens: number` — generated output tokens\n- `cacheReadTokens: number` — tokens served from cache reads\n- `cacheCreationTokens: number` — tokens written to cache\n- `durationMs: number` — wall-clock latency\n- `exitCode: number` — CLI process exit code\n- `raw: unknown` — original CLI JSON output for debugging\n\n### Backend Adapter Contract\n\n**AIBackend** interface — Contract implemented by each CLI adapter (Claude, Gemini, OpenCode):\n- `readonly name: string` — human-readable backend name\n- `readonly cliCommand: string` — executable name on PATH\n- `isAvailable(): Promise<boolean>` — checks CLI availability\n- `buildArgs(options: AICallOptions): string[]` — constructs CLI argument array\n- `parseResponse(stdout: string, durationMs: number, exitCode: number): AIResponse` — parses CLI stdout into normalized AIResponse\n- `getInstallInstructions(): string` — returns user-facing install guidance when CLI not found\n\n### Retry Configuration\n\n**RetryOptions** — Exponential backoff retry configuration:\n- `maxRetries: number` — maximum retry attempts (e.g., 3 = 4 total attempts)\n- `baseDelayMs: number` — initial delay before first retry\n- `maxDelayMs: number` — delay ceiling cap\n- `multiplier: number` — exponential backoff multiplier\n- `isRetryable: (error: unknown) => boolean` — predicate for transient error detection\n- `onRetry?: (attempt: number, error: unknown) => void` — optional pre-retry callback\n\n### Telemetry Logging\n\n**FileRead** — Record of a single file read operation sent as context:\n- `path: string` — file path relative to project root\n- `sizeBytes: number` — file size in bytes at read time\n\n**TelemetryEntry** — Per-call log entry capturing complete call metadata:\n- `timestamp: string` — ISO 8601 call initiation time\n- `prompt: string` — sent prompt text\n- `systemPrompt?: string` — optional system prompt\n- `response: string` — AI model's text response\n- `model: string` — model identifier\n- `inputTokens: number` — consumed input tokens\n- `outputTokens: number` — generated output tokens\n- `cacheReadTokens: number` — cache read tokens\n- `cacheCreationTokens: number` — cache write tokens\n- `latencyMs: number` — wall-clock latency\n- `exitCode: number` — process exit code\n- `error?: string` — error message if call failed\n- `retryCount: number` — number of retries before result\n- `thinking: string` — AI reasoning content (\"not supported\" when backend doesn't provide)\n- `filesRead: FileRead[]` — context files sent with call\n\n**RunLog** — Per-run log file structure aggregating all TelemetryEntry instances:\n- `runId: string` — unique run identifier (ISO timestamp-based)\n- `startTime: string` — ISO 8601 run start time\n- `endTime: string` — ISO 8601 run end time\n- `entries: TelemetryEntry[]` — all individual call entries\n- `summary` object with aggregated metrics:\n  - `totalCalls: number` — count of AI calls\n  - `totalInputTokens: number` — sum of input tokens\n  - `totalOutputTokens: number` — sum of output tokens\n  - `totalDurationMs: number` — total wall-clock duration\n  - `errorCount: number` — count of failed calls\n  - `totalCacheReadTokens: number` — sum of cache reads\n  - `totalCacheCreationTokens: number` — sum of cache writes\n  - `totalFilesRead: number` — total file reads (including duplicates)\n  - `uniqueFilesRead: number` — deduplicated file read count\n\n### Error Handling\n\n**AIServiceErrorCode** type — Machine-readable error code discriminator:\n- `'CLI_NOT_FOUND'` — CLI executable not found on PATH\n- `'TIMEOUT'` — subprocess exceeded timeout threshold\n- `'PARSE_ERROR'` — failed to parse CLI JSON output\n- `'SUBPROCESS_ERROR'` — generic subprocess execution failure\n- `'RATE_LIMIT'` — backend rate limiting detected\n\n**AIServiceError** class — Typed error extending Error with structured error code:\n- `readonly code: AIServiceErrorCode` — machine-readable error type\n- `constructor(code: AIServiceErrorCode, message: string)` — sets name to 'AIServiceError' and stores code\n\n## Design Patterns\n\n**Backend Adapter Pattern** — AIBackend interface enables polymorphic CLI invocation across Claude/Gemini/OpenCode without callers knowing which backend executes. Registry selects backend at runtime via `isAvailable()` detection.\n\n**Normalized Response Contract** — Every backend's `parseResponse()` must transform CLI-specific output into uniform AIResponse shape, abstracting away stdout parsing differences (Claude JSON vs. Gemini JSONL vs. OpenCode format).\n\n**Typed Error Discrimination** — AIServiceError.code enables structured error handling via `error instanceof AIServiceError && error.code === 'RATE_LIMIT'` pattern instead of string parsing.\n\n## Integration Points\n\n**Consumed by:**\n- `src/ai/service.ts` — AIService class uses AIBackend interface for backend selection and AICallOptions for call construction\n- `src/ai/registry.ts` — Backend detection and selection logic references AIBackend interface\n- `src/ai/retry.ts` — Retry utility consumes RetryOptions for exponential backoff\n- `src/ai/telemetry/logger.ts` — TelemetryLogger writes TelemetryEntry instances to run logs\n- `src/ai/telemetry/run-log.ts` — Constructs RunLog structure from entry accumulation\n- `src/ai/backends/*.ts` — Claude/Gemini/OpenCode adapters implement AIBackend interface\n\n**Referenced by:**\n- `src/orchestration/pool.ts` — Worker pool consumes AIResponse for progress reporting\n- `src/generation/executor.ts` — Phase executor handles AIServiceError codes for failure modes\n\n## Import Map (verified — use these exact paths)\n\nservice.ts:\n  ../orchestration/trace.js → ITraceWriter (type)\n\n## Project Directory Structure\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\n\n## Subdirectories\n\n### backends/\n<!-- Generated by agents-reverse-engineer -->\n\n# src/ai/backends\n\n**AI backend adapter layer implementing AIBackend interface for Claude Code, Gemini, and OpenCode CLIs with PATH detection, argument construction, JSON response parsing, and installation instructions.**\n\n## Contents\n\n### Backend Adapters\n\n**[claude.ts](./claude.ts)** — ClaudeBackend with isCommandOnPath() PATH detection, buildArgs() permission bypass flags (`--permission-mode bypassPermissions`, `--no-session-persistence`), parseResponse() with ClaudeResponseSchema Zod validation, defensive JSON extraction via stdout.indexOf('{'), and modelUsage-based model name extraction.\n\n**[gemini.ts](./gemini.ts)** — GeminiBackend stub implementing AIBackend interface with isCommandOnPath() delegation, buildArgs() JSON output flags, parseResponse() throwing 'SUBPROCESS_ERROR' AIServiceError until stable JSON format available.\n\n**[opencode.ts](./opencode.ts)** — OpenCodeBackend stub implementing AIBackend interface with isCommandOnPath() delegation, buildArgs() returning `['run', '--format', 'json']`, parseResponse() throwing 'SUBPROCESS_ERROR' AIServiceError until JSONL parsing implemented.\n\n## Architecture\n\n### AIBackend Interface Contract\n\nAll backends implement four methods:\n- `isAvailable(): Promise<boolean>` — CLI executable detection via PATH scanning\n- `buildArgs(options: AICallOptions): string[]` — Argument array construction for subprocess spawn\n- `parseResponse(stdout: string, durationMs: number, exitCode: number): AIResponse` — JSON parsing with usage extraction\n- `getInstallInstructions(): string` — NPM/curl install commands for missing CLI\n\n### PATH Detection Algorithm\n\n`isCommandOnPath(command: string)` shared utility (exported from claude.ts):\n1. Parse `process.env.PATH` with `path.delimiter` (`;` Windows, `:` Unix)\n2. Parse `process.env.PATHEXT` on Windows for executable extensions (`.exe`, `.cmd`, `.bat`)\n3. Nested loop: directory × extension, construct candidate via `path.join(dir, command + ext)`\n4. Check `(await fs.stat(candidate)).isFile()` — return `true` on first match\n5. Uses `fs.stat()` not `fs.access(X_OK)` for Windows compatibility (no execute permission bits)\n\n### Response Parsing Strategy\n\n**ClaudeBackend (functional):**\n- Defensive JSON extraction: `stdout.substring(stdout.indexOf('{'))` skips upgrade notices\n- Zod validation against ClaudeResponseSchema from Claude CLI v2.1.31 output format\n- Extracts model name from first `modelUsage` object key (defaults to `'unknown'`)\n- Returns AIResponse with normalized `{ result, usage, modelName, durationMs }` structure\n- Throws AIServiceError with `'PARSE_ERROR'` code on validation failure\n\n**GeminiBackend/OpenCodeBackend (stubs):**\n- Unconditionally throw AIServiceError with `'SUBPROCESS_ERROR'` code in parseResponse()\n- Block backend usage until JSON format stabilizes (Gemini) or JSONL parsing implemented (OpenCode)\n- Demonstrate extension pattern for future implementations\n\n## Behavioral Contracts\n\n### ClaudeBackend CLI Arguments\n\nFixed arguments returned by buildArgs():\n```\n['-p', '--output-format', 'json', '--no-session-persistence', '--permission-mode', 'bypassPermissions']\n```\n\nConditional arguments from AICallOptions:\n- `'--model', options.model` when model specified\n- `'--system-prompt', options.systemPrompt` when systemPrompt provided\n- `'--max-turns', String(options.maxTurns)` when maxTurns defined\n\nPrompt delivered via stdin by `runSubprocess()` wrapper in `src/ai/subprocess.ts`.\n\n### ClaudeResponseSchema Structure\n\nZod schema validated against Claude CLI v2.1.31:\n```typescript\n{\n  type: 'result',\n  subtype: 'success' | 'error',\n  is_error: boolean,\n  duration_ms: number,\n  duration_api_ms: number,\n  num_turns: number,\n  result: string,\n  session_id: string,\n  total_cost_usd: number,\n  usage: {\n    input_tokens: number,\n    cache_creation_input_tokens: number,\n    cache_read_input_tokens: number,\n    output_tokens: number\n  },\n  modelUsage: Record<string, {\n    inputTokens: number,\n    outputTokens: number,\n    cacheReadInputTokens: number,\n    cacheCreationInputTokens: number,\n    costUSD: number\n  }>\n}\n```\n\n### Installation Instructions\n\n**ClaudeBackend:** `npm install -g @anthropic-ai/claude-code`  \n**GeminiBackend:** `npm install -g @anthropic-ai/gemini-cli` + `https://github.com/google-gemini/gemini-cli`  \n**OpenCodeBackend:** `curl -fsSL https://opencode.ai/install | bash` + `https://opencode.ai`\n\n## Integration Points\n\nBackends registered in `src/ai/registry.ts` AIBackendRegistry map with keys `'claude'`, `'gemini'`, `'opencode'`. Registry consumed by AIService in `src/ai/service.ts` for backend selection via `ai.backend` config field. Auto-detection iterates backends calling `isAvailable()` until first match.\n\n`buildArgs()` output consumed by `runSubprocess()` in `src/ai/subprocess.ts` for child_process.execFile() argument array. `parseResponse()` receives stdout/durationMs/exitCode from subprocess wrapper for normalization into AIResponse structure.\n### telemetry/\n<!-- Generated by agents-reverse-engineer -->\n\n# src/ai/telemetry\n\nTelemetry subsystem for accumulating AI service call metrics, writing JSON run logs with retention management, and tracking token costs, durations, and file metadata across concurrent worker pool operations.\n\n## Contents\n\n### [cleanup.ts](./cleanup.ts)\nExports `cleanupOldLogs(projectRoot, keepCount)` deleting expired telemetry logs from `.agents-reverse-engineer/logs/` by lexicographic timestamp sorting on `run-*.json` filenames. Returns count of deleted files.\n\n### [logger.ts](./logger.ts)\nExports `TelemetryLogger` class accumulating `TelemetryEntry` instances in memory during CLI runs. Methods: `addEntry()` appends call metrics, `setFilesReadOnLastEntry()` attaches file metadata to most recent entry, `getSummary()` computes aggregate statistics (token totals, error counts, unique file paths), `toRunLog()` serializes complete `RunLog` object.\n\n### [run-log.ts](./run-log.ts)\nExports `writeRunLog(projectRoot, runLog)` persisting `RunLog` as pretty-printed JSON with ISO-8601-derived filename pattern `run-2026-02-07T12-00-00-000Z.json`. Creates `.agents-reverse-engineer/logs/` directory via `mkdir(recursive: true)`.\n\n## Data Flow\n\n1. `TelemetryLogger` instantiated once per CLI invocation with unique `runId` (ISO timestamp)\n2. AIService calls `logger.addEntry(entry)` after each subprocess completion with token counts, cost, duration, error status\n3. Command runner invokes `logger.setFilesReadOnLastEntry(filesRead)` to attach file metadata to last entry\n4. On run completion, caller invokes `logger.toRunLog()` → serializes in-memory entries + summary\n5. `writeRunLog(projectRoot, runLog)` persists JSON to disk with timestamp-derived filename\n6. `cleanupOldLogs(projectRoot, keepCount)` enforces retention policy by deleting oldest logs exceeding `keepRuns` threshold (default 50)\n\n## Behavioral Contracts\n\n**Filename derivation pattern** (from `run-log.ts`):\n```typescript\nconst timestamp = runLog.startTime.replace(/:/g, '-').replace(/\\./g, '-');\nconst filename = `run-${timestamp}.json`;\n```\n\n**Log directory constant** (shared across all modules):\n```typescript\nconst LOGS_DIR = '.agents-reverse-engineer/logs';\n```\n\n**Cleanup sort order** (from `cleanup.ts`):\n```typescript\nentries.sort();      // Lexicographic ascending\nentries.reverse();   // Newest first\nconst toDelete = entries.slice(keepCount);  // Oldest logs\n```\n\n**Summary deduplication** (from `logger.ts`):\n```typescript\nconst uniquePaths = new Set<string>();\nfor (const entry of this.entries) {\n  for (const file of entry.filesRead || []) {\n    uniquePaths.add(file.path);\n  }\n}\nreturn { uniqueFilesRead: uniquePaths.size };\n```\n\n## Integration Points\n\n- **Created by**: `src/ai/service.ts` constructs `TelemetryLogger` when `config.ai.telemetry.enabled === true`\n- **Consumed by**: `src/orchestration/runner.ts` invokes `writeRunLog()` after Phase 3 completion, then calls `cleanupOldLogs()` with `config.ai.telemetry.keepRuns`\n- **Types imported from**: `../types.js` provides `TelemetryEntry`, `RunLog`, `FileRead` interfaces\n- **Related configuration**: `src/config/schema.ts` defines `ai.telemetry.enabled`, `ai.telemetry.keepRuns`, `ai.telemetry.costThresholdUsd` validation rules\n\n## Design Notes\n\n`TelemetryLogger` uses eager summary computation on every `getSummary()` invocation without caching — acceptable since `toRunLog()` called once per run. Single-level flat `entries` array without phase-aware partitioning simplifies aggregation logic at cost of phase-specific analytics. Cleanup operates on filename lexicographic order assuming ISO 8601 timestamp sort equivalence (year-month-day-hour-minute-second-millisecond).",
      "systemPrompt": "You are generating an AGENTS.md file — a directory-level overview for AI coding assistants.\n\nCRITICAL: Output ONLY the raw markdown content. No code fences, no preamble, no explanations, no conversational text. Your entire response IS the AGENTS.md file content.\n\nOUTPUT FORMAT:\n- First line MUST be exactly: <!-- Generated by agents-reverse-engineer -->\n- Use a # heading with the directory name\n- Write a one-paragraph purpose statement for the directory\nusage\nADAPTIVE SECTIONS:\nAnalyze the directory contents and choose the most relevant sections. Do NOT use a fixed template. Instead, select sections that best document this specific directory for an AI that needs to reconstruct or extend the project.\n\nConsider these section types (choose what applies):\n- **Contents**: Group files by purpose/category under ## headings. For each file: markdown link [filename](./filename) and a one-line description.\n- **Subdirectories**: If subdirectories exist, list them with links [dirname/](./dirname/) and brief summaries.\n- **Architecture / Data Flow**: If files form a pipeline, request/response chain, or layered architecture, document it.\n- **Stack**: If this is a package root (has package.json, Cargo.toml, go.mod, etc.), document the technology stack, key scripts, and entry points.\n- **Structure**: If the directory layout follows a convention (feature-sliced, domain-driven, MVC, etc.), document it.\n- **Patterns**: If files share recurring design patterns (factory, strategy, middleware, barrel re-export), name and document them.\n- **Configuration**: If the directory contains config files, schemas, or environment definitions, document the config surface area.\n- **API Surface**: If the directory exports a public API (barrel index, route definitions, SDK), document the interface contract.\n- **File Relationships**: How files collaborate, depend on each other, or share state.\n- **Behavioral Contracts**: If files contain regex patterns, format specifications, magic constants, or template strings that define observable behavior, collect them in a dedicated section. Preserve verbatim patterns from file summaries — do NOT paraphrase regex into prose. This section is MANDATORY when file summaries contain behavioral artifacts.\n- **Reproduction-Critical Constants**: If file summaries reference annex files (via ## Annex References sections), list them with links. Example: \"Full prompt template text: [templates.ts.annex.md](./prompts/templates.ts.annex.md)\". Do NOT reproduce annex content in AGENTS.md — just link to it.\n\nChoose any relevant sections or create your own based on the directory contents. The goal is to provide a comprehensive overview that captures the essence of the directory's role in the project and how its files work together, with a focus on what an AI coding assistant would need to know to effectively interact with this code.\n\nSCOPE:\n- AGENTS.md is a NAVIGATIONAL INDEX — help an AI find the right file quickly\n- Focus on: what each file does, how files relate, directory-level patterns\n- Do NOT reproduce full architecture sections — those belong in the root CLAUDE.md\n\nPATH ACCURACY (MANDATORY):\n- When referencing files or modules outside this directory, use ONLY paths from the \"Import Map\" section\n- Do NOT invent, rename, or guess module paths — if a path isn't in the Import Map, don't reference it\n- Use the exact directory names from \"Project Directory Structure\" — do NOT rename directories\n  (e.g., if the directory is called \"cli\", write \"src/cli/\", NOT \"src/commands/\")\n- Cross-module references must use the specifier format from actual import statements\n  (e.g., \"../generation/writers/sum.js\", NOT \"../fs/sum-file.js\")\n- If you are unsure about a path, omit the cross-reference rather than guessing\n\nCONSISTENCY (MANDATORY):\n- Do not contradict yourself within the same document\n- If you describe a technique (e.g., \"regex-based\"), do not call it something else later (e.g., \"AST-based\")\n- When stating version numbers, engines, or config fields, use ONLY values present in the file summaries\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this directory\", \"this module\", \"provides\", \"responsible for\", \"is used to\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Per-file descriptions in Contents sections: 1-2 sentences maximum. Reference key symbols but do not reproduce full summaries.\n- Behavioral contracts (regex patterns, format specs, constants) belong in a separate Behavioral Contracts section, not in per-file descriptions.\n- Annex file references: link to .annex.md files, do not inline their content. One line per annex reference.\n- Subdirectory descriptions: 1-2 sentences maximum. Capture the directory's role, not its full contents.\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- Key exported symbols from file summaries MUST appear in the directory overview\n- Preserve exact casing of identifiers\n\nUSER NOTES:\n- If \"User Notes\" are provided in the prompt, they contain user-defined instructions that will be automatically prepended to your output\n- Do NOT repeat or paraphrase user notes in your generated content — they are included separately\n- You may reference information from user notes for context",
      "response": "<!-- Generated by agents-reverse-engineer -->\n\n# src/ai\n\nAI service orchestration layer abstracting Claude Code, Gemini, and OpenCode CLIs through backend adapters, exponential backoff retry, subprocess resource management, telemetry logging, and timeout enforcement for concurrent file analysis pools.\n\n## Contents\n\n### Core Orchestration\n\n**[service.ts](./service.ts)** — AIService class coordinating backend selection, subprocess execution via `runSubprocess()`, retry logic via `withRetry()`, telemetry accumulation via `TelemetryLogger`, trace emission via `ITraceWriter`, rate-limit detection matching stderr patterns (`'rate limit'`, `'429'`, `'too many requests'`, `'overloaded'`), optional debug logging with heap/RSS metrics via `formatBytes()`, and fire-and-forget subprocess log serialization via `enqueueSubprocessLog()` promise chain.\n\n**[registry.ts](./registry.ts)** — BackendRegistry managing adapter map with `register()`, `get()`, `getAll()` methods. Exports `createBackendRegistry()` instantiating registry with ClaudeBackend, GeminiBackend, OpenCodeBackend in priority order. Exports `resolveBackend(registry, requested)` implementing auto-detection via `detectBackend()` iterating `backend.isAvailable()` or throwing `AIServiceError` with code `'CLI_NOT_FOUND'` and aggregated install instructions via `getInstallInstructions()`.\n\n**[subprocess.ts](./subprocess.ts)** — `runSubprocess(command, args, options)` spawning `execFile()` child processes with stdin piping, timeout enforcement via SIGTERM at `options.timeoutMs`, SIGKILL escalation after 5s grace period, process group killing via `kill(-pid)`, active subprocess tracking in module Map enabling `getActiveSubprocessCount()` and `getActiveSubprocesses()` inspection, and `onSpawn(pid)` callback at spawn time.\n\n**[retry.ts](./retry.ts)** — `withRetry(fn, options)` exponential backoff wrapper executing async function with delay formula `min(baseDelayMs * multiplier^attempt, maxDelayMs) + jitter` (jitter uniform random 0-500ms). Exports `DEFAULT_RETRY_OPTIONS` constant with `maxRetries: 3`, `baseDelayMs: 1000`, `maxDelayMs: 8000`, `multiplier: 2`. Retry flow controlled by `options.isRetryable(error)` predicate and optional `options.onRetry(attempt, error)` callback.\n\n**[types.ts](./types.ts)** — Type definitions: `AIBackend` interface with `isAvailable()`, `buildArgs()`, `parseResponse()`, `getInstallInstructions()` contract. `AIResponse` normalized response shape with `text`, `model`, `inputTokens`, `outputTokens`, `cacheReadTokens`, `cacheCreationTokens`, `durationMs`, `exitCode`, `raw`. `AICallOptions` with `prompt`, `systemPrompt`, `model`, `timeoutMs`, `maxTurns`, `taskLabel`. `SubprocessResult` with `stdout`, `stderr`, `exitCode`, `signal`, `durationMs`, `timedOut`, `childPid`. `TelemetryEntry` per-call metadata, `RunLog` aggregated run structure, `FileRead` file metadata, `RetryOptions` retry config, `AIServiceError` typed error with `code` discriminator (`'CLI_NOT_FOUND'`, `'TIMEOUT'`, `'PARSE_ERROR'`, `'SUBPROCESS_ERROR'`, `'RATE_LIMIT'`).\n\n**[index.ts](./index.ts)** — Barrel export aggregating public API: AIService, BackendRegistry, resolveBackend, detectBackend, createBackendRegistry, withRetry, runSubprocess, isCommandOnPath, all types from `./types.js`, AIServiceOptions from `./service.js`.\n\n## Subdirectories\n\n**[backends/](./backends/)** — AIBackend adapter implementations: ClaudeBackend with Zod validation via `ClaudeResponseSchema`, GeminiBackend stub throwing `SUBPROCESS_ERROR` until JSON format stabilizes, OpenCodeBackend stub throwing `SUBPROCESS_ERROR` until JSONL parsing implemented. Shared `isCommandOnPath()` utility scanning `process.env.PATH` with Windows `PATHEXT` support.\n\n**[telemetry/](./telemetry/)** — Telemetry subsystem: TelemetryLogger accumulating TelemetryEntry instances, `writeRunLog()` persisting JSON to `.agents-reverse-engineer/logs/run-<timestamp>.json`, `cleanupOldLogs()` enforcing retention via lexicographic filename sorting.\n\n## Architecture\n\n### Backend Adapter Pattern\n\nAIBackend interface decouples CLI invocation from backend-specific argument construction and JSON parsing. Registry selects backend at runtime via `config.ai.backend` field (`'claude'` | `'gemini'` | `'opencode'` | `'auto'`). Auto-detection calls `isAvailable()` on each backend in registration order (Claude → Gemini → OpenCode) until first CLI found on PATH.\n\n### Subprocess Resource Management\n\nMitigates Claude CLI thread exhaustion (GitHub #5771: 200 Node.js instances):\n- `NODE_OPTIONS='--max-old-space-size=512'` limits heap to 512MB per subprocess\n- `UV_THREADPOOL_SIZE='4'` constrains libuv thread pool to 4 threads\n- `CLAUDE_CODE_DISABLE_BACKGROUND_TASKS='1'` prevents background task spawning\n- `--disallowedTools Task` prevents subagent spawning\n- Process group killing via `kill(-pid)` terminates subprocess trees\n- Default concurrency 2 for WSL environments (5 elsewhere)\n\n### Retry Strategy\n\n`AIService.call()` wraps `runSubprocess()` in `withRetry()` configured to retry only `RATE_LIMIT` errors (timeouts excluded). Rate-limit detection via `isRateLimitStderr()` substring matching. Exponential backoff adds uniform jitter (0-500ms) preventing thundering herd when multiple workers hit rate limits simultaneously.\n\n### Telemetry Accumulation\n\nTelemetryLogger maintains in-memory `entries: TelemetryEntry[]` array throughout CLI run. After each `AIService.call()`, logger records timestamp, prompt, response, model, token counts (input/output/cacheRead/cacheCreation), latency, exitCode, retryCount, and filesRead metadata. On run completion, `logger.toRunLog()` serializes entries plus computed summary (totalCalls, totalInputTokens, totalOutputTokens, totalCacheReadTokens, totalCacheCreationTokens, totalDurationMs, errorCount, uniqueFilesRead) to `RunLog` JSON structure. `writeRunLog()` persists with ISO-8601-derived filename, `cleanupOldLogs()` enforces retention by deleting oldest logs exceeding `config.ai.telemetry.keepRuns` threshold.\n\n### Timeout Enforcement\n\n`runSubprocess()` sends SIGTERM at `timeoutMs`, schedules unref'd SIGKILL timer at `timeoutMs + 5000ms` for hung processes ignoring SIGTERM. Sets `SubprocessResult.timedOut = true` when `execFile` error has `killed: true` property. AIService throws `AIServiceError('TIMEOUT')` on timeout detection (non-retryable per resource constraint mitigation).\n\n### Trace Emission\n\nAIService invokes `tracer.emit()` for `subprocess:spawn` at `onSpawn` callback time (includes `childPid`, `taskLabel`, `command`, `args`), `subprocess:exit` after completion (includes `exitCode`, `signal`, `durationMs`, `timedOut`), and `retry` events before delay (includes `attempt`, `taskLabel`, `errorCode`). ITraceWriter from `src/orchestration/trace.ts` provides promise-chain serialization ensuring NDJSON line order matches emission order despite concurrent workers.\n\n## Behavioral Contracts\n\n### Rate Limit Patterns\n\n```typescript\nconst RATE_LIMIT_PATTERNS = ['rate limit', '429', 'too many requests', 'overloaded'];\n```\n\n### Default Retry Configuration\n\n```typescript\nDEFAULT_RETRY_OPTIONS = {\n  maxRetries: 3,\n  baseDelayMs: 1_000,\n  maxDelayMs: 8_000,\n  multiplier: 2\n}\n```\n\n### SIGKILL Grace Period\n\n```typescript\nconst SIGKILL_GRACE_MS = 5_000;\n```\n\n### Subprocess maxBuffer\n\n```typescript\nmaxBuffer: 10 * 1024 * 1024  // 10MB stdout/stderr capture limit\n```\n\n## Integration Points\n\n**Consumed by:**\n- `src/orchestration/runner.ts` — Creates AIService instance, calls `setTracer()` and `setDebug()` based on CLI flags, invokes `call()` for each task, calls `finalize()` at run end\n- `src/generation/executor.ts` — Handles AIServiceError codes for failure modes, extracts error messages for progress reporting\n\n**Imports from:**\n- `src/orchestration/trace.ts` — ITraceWriter interface for trace event emission\n- `src/config/schema.ts` — AIServiceOptions validation schema\n\n**Exports to:**\n- `src/cli/*.ts` — All CLI commands import AIService, createBackendRegistry, resolveBackend from `./ai/index.js`",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 2287,
      "cacheReadTokens": 17587,
      "cacheCreationTokens": 18742,
      "latencyMs": 67487,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": []
    },
    {
      "timestamp": "2026-02-09T17:47:47.802Z",
      "prompt": "Generate AGENTS.md for directory: \"src/installer\" (installer)\n\n## File Summaries (7 files)\n\n### banner.ts\n**Purpose:** banner.ts provides styled terminal output for the installer including ASCII art, help text, and status messages using...\n\n**banner.ts provides styled terminal output for the installer including ASCII art, help text, and status messages using picocolors.**\n\n## Exported Functions\n\n**displayBanner()** renders the \"ARE\" ASCII art logo in green via `pc.green()` with version string from `getVersion()` and tagline \"AI-friendly codebase documentation\".\n\n**showHelp()** prints usage syntax, option flags (`--runtime`, `-g`/`--global`, `-l`/`--local`, `-u`/`--uninstall`, `--force`, `-q`/`--quiet`, `-h`/`--help`), and example commands for interactive mode, global/local installation, and uninstallation workflows.\n\n**showSuccess(msg: string)** outputs message prefixed with green checkmark `pc.green('✓')`.\n\n**showError(msg: string)** outputs message prefixed with red X `pc.red('✗')`.\n\n**showWarning(msg: string)** outputs message prefixed with yellow exclamation `pc.yellow('!')`.\n\n**showInfo(msg: string)** outputs message prefixed with cyan arrow `pc.cyan('>')`.\n\n**showNextSteps(runtime: string, filesCreated: number)** displays post-installation instructions listing commands `/are-help`, `/are-init`, `/are-discover`, `/are-generate`, `/are-update`, `/are-specify`, `/are-clean` with documentation link to GitHub repository.\n\n## Exported Constants\n\n**VERSION** holds package version string obtained from `getVersion()` imported from `../version.js`.\n\n## Dependencies\n\nImports `pc` from `picocolors` for terminal color formatting (`green()`, `red()`, `yellow()`, `cyan()`, `dim()`, `bold()`). Imports `getVersion()` from `../version.js` for package version retrieval.\n\n## ASCII Art Pattern\n\nBanner uses Unicode box-drawing characters (U+2588 full block, U+2550-U+2557 box drawing) to render:\n```\n █████╗ ██████╗ ███████╗\n██╔══██╗██╔══██╗██╔════╝\n███████║██████╔╝█████╗  \n██╔══██║██╔══██╗██╔══╝  \n██║  ██║██║  ██║███████╗\n╚═╝  ╚═╝╚═╝  ╚═╝╚══════╝\n```\n### index.ts\n**Purpose:** src/installer/index.ts orchestrates the npx installation workflow for agents-reverse-engineer with interactive prompt...\n\n**src/installer/index.ts orchestrates the npx installation workflow for agents-reverse-engineer with interactive prompts, argument parsing, and coordinated installation/uninstallation of commands and hooks across Claude/OpenCode/Gemini runtimes.**\n\n## Exported Functions\n\n**parseInstallerArgs(args: string[]): InstallerArgs** — Parses command-line arguments supporting short flags (`-g`, `-l`, `-h`, `-q`) and long flags (`--global`, `--local`, `--runtime`, `--force`, `--quiet`, `--help`). Validates `--runtime` value against `['claude', 'opencode', 'gemini', 'all']`. Returns InstallerArgs with `runtime`, `global`, `local`, `uninstall`, `force`, `help`, `quiet` properties.\n\n**runInstaller(args: InstallerArgs): Promise<InstallerResult[]>** — Main entry point executing installation workflow. Returns empty array if `args.help` is true (displays help via `showHelp()`). Calls `displayBanner()` unless `args.quiet`. Validates non-interactive mode requires `--runtime` and location flags, exits with `process.exit(1)` on missing values. Prompts for missing `runtime` via `selectRuntime(mode)` and `location` via `selectLocation(mode)` when `isInteractive()` returns true. Routes to `runUninstall()` if `args.uninstall`, otherwise `runInstall()`.\n\n## Internal Workflow Functions\n\n**determineLocation(args: InstallerArgs): Location | undefined** — Returns `'global'` if `args.global && !args.local`, returns `'local'` if `args.local && !args.global`, returns `undefined` if both or neither set (triggers interactive prompt).\n\n**determineRuntimes(runtime: Runtime | undefined): Array<Exclude<Runtime, 'all'>>** — Returns empty array if `runtime` is undefined (needs prompt). Returns `getAllRuntimes()` array if `runtime === 'all'`. Returns single-element array `[runtime]` for specific runtime.\n\n**runInstall(runtime: Runtime, location: Location, force: boolean, quiet: boolean): Promise<InstallerResult[]>** — Executes installation workflow: calls `installFiles(runtime, location, { force, dryRun: false })`, flattens `filesCreated` arrays via `flatMap()`, calls `verifyInstallation(allCreatedFiles)`, displays errors via `showError()` and `showWarning()` if verification fails, calls `displayInstallResults(results)` unless `quiet`.\n\n**runUninstall(runtime: Runtime, location: Location, quiet: boolean): InstallerResult[]** — Executes uninstallation workflow: calls `uninstallFiles(runtime, location, false)`, calls `deleteConfigFolder(location, false)` to remove `.agents-reverse-engineer/` directory, calls `displayUninstallResults(results, configDeleted)` unless `quiet`.\n\n## Display Functions\n\n**displayInstallResults(results: InstallerResult[]): void** — Iterates results calling `showSuccess()` for `result.success`, `showError()` and `showWarning()` for failures. Accumulates `totalCreated` from `result.filesCreated.length`, `totalSkipped` from `result.filesSkipped.length`, `hooksRegistered` from `result.hookRegistered` boolean. Displays summary via `showSuccess()` for created files/hooks, `showWarning()` for skipped files with \"use --force to overwrite\" message. Calls `showNextSteps(primaryRuntime, totalCreated)` with first result's runtime. Displays GitHub link via `showInfo('Docs: https://github.com/GeoloeG-IsT/agents-reverse-engineer')`.\n\n**displayUninstallResults(results: InstallerResult[], configDeleted: boolean): void** — Iterates results accumulating `totalDeleted` from `result.filesCreated.length` (repurposed for deleted file count), `hooksUnregistered` from `result.hookRegistered` (repurposed for unregistration status). Calls `showSuccess()` for deletions, `showInfo()` for \"No files found\" cases, `showError()` for failures. Displays summary with removed file count, unregistered hook count, and config folder deletion status.\n\n## Re-exported Symbols\n\nModule re-exports types and functions from sibling modules:\n- **Types**: `InstallerArgs`, `InstallerResult`, `Runtime`, `Location`, `RuntimePaths` from `./types.js`\n- **Path utilities**: `getRuntimePaths`, `getAllRuntimes`, `resolveInstallPath`, `getSettingsPath` from `./paths.js`\n- **Banner/display**: `displayBanner`, `showHelp`, `showSuccess`, `showError`, `showWarning`, `showInfo`, `showNextSteps`, `VERSION` from `./banner.js`\n- **Prompts**: `selectRuntime`, `selectLocation`, `confirmAction`, `isInteractive` from `./prompts.js`\n\n## Validation Logic\n\nNon-interactive mode (when `isInteractive()` returns false) enforces required flags: exits with `showError('Missing --runtime flag')` if `runtimeArg` undefined, exits with `showError('Missing -g/--global or -l/--local flag')` if `location` undefined. Interactive mode prompts for missing values via `selectRuntime()` and `selectLocation()`. Final safety check exits with `showError('Unable to determine runtime and location')` if either remains undefined.\n\n## Valid Runtime Values\n\nPattern: `const validRuntimes: Runtime[] = ['claude', 'opencode', 'gemini', 'all']` — Used in `parseInstallerArgs()` to validate `--runtime` argument via `validRuntimes.includes(runtimeValue as Runtime)`.\n\n## Operation Mode Routing\n\nVariable `mode` set to `'uninstall'` if `args.uninstall`, otherwise `'install'` — passed to `selectRuntime(mode)` and `selectLocation(mode)` to customize prompt text. Uninstall workflow triggered by `args.uninstall` flag (not directly parsed in this file, set by caller in CLI layer).\n### operations.ts\n**Purpose:** operations.ts implements file installation, verification, and settings.json hook/permission registration for ARE comm...\n\n**operations.ts implements file installation, verification, and settings.json hook/permission registration for ARE command/hook deployment across Claude Code, OpenCode, and Gemini CLI runtimes.**\n\n## Exported Functions\n\n`installFiles(runtime: Runtime, location: Location, options: InstallOptions): InstallerResult[]` — installs command templates and hook/plugin files for specified runtime ('claude' | 'opencode' | 'gemini' | 'all'), returns array of InstallerResult with filesCreated/filesSkipped/errors/hookRegistered/versionWritten status.\n\n`verifyInstallation(files: string[]): { success: boolean; missing: string[] }` — checks existsSync() for each file path, returns verification status with missing file list.\n\n`registerHooks(basePath: string, runtime: Exclude<Runtime, 'all'>, dryRun: boolean): boolean` — writes hook configuration to settings.json for Claude/Gemini, returns true if hooks added, false if already existed.\n\n`registerPermissions(settingsPath: string, dryRun: boolean): boolean` — adds ARE_PERMISSIONS bash command patterns to Claude Code settings.json permissions.allow array.\n\n`getPackageVersion(): string` — reads version field from package.json via import.meta.url resolution, returns 'unknown' on error.\n\n`writeVersionFile(basePath: string, dryRun: boolean): void` — writes getPackageVersion() result to ARE-VERSION file in basePath.\n\n`formatInstallResult(result: InstallerResult): string[]` — generates human-readable lines showing runtime/location, created/skipped files, hook registration status, summary counts.\n\n## Exported Interfaces\n\n`InstallOptions { force: boolean; dryRun: boolean }` — controls file overwrite behavior and preview mode for install operations.\n\n## Hook/Plugin Definitions\n\n`ARE_HOOKS: HookDefinition[]` — array defining SessionStart/SessionEnd hooks with filename/event/name mappings (currently empty - hooks disabled due to issues).\n\n`HookDefinition { event: 'SessionStart' | 'SessionEnd'; filename: string; name: string }` — hook metadata for Claude/Gemini format conversion.\n\n`ARE_PLUGINS: PluginDefinition[]` — array mapping OpenCode plugin source filenames (prefixed 'opencode-') to destination filenames in .opencode/plugins/ (are-check-update.js; are-session-end.js disabled).\n\n`PluginDefinition { srcFilename: string; destFilename: string }` — source-to-destination mapping for OpenCode plugin installation.\n\n## Settings.json Schema\n\n`SettingsJson { hooks?: { SessionStart?: HookEvent[]; SessionEnd?: HookEvent[] }; permissions?: { allow?: string[]; deny?: string[] } }` — Claude Code settings.json structure with nested hooks arrays and permissions.\n\n`HookEvent { hooks: SessionHook[] }` — Claude format wraps hooks in nested array.\n\n`SessionHook { type: 'command'; command: string }` — individual hook definition with command string.\n\n`GeminiSettingsJson { hooks?: { SessionStart?: GeminiHook[]; SessionEnd?: GeminiHook[] } }` — Gemini CLI settings.json structure with flat hook arrays.\n\n`GeminiHook { name: string; type: 'command'; command: string }` — Gemini format includes explicit name field.\n\n## Hook Registration Logic\n\n`registerClaudeHooks(settingsPath: string, runtimeDir: string, dryRun: boolean): boolean` — loads settings.json via readFileSync(), merges ARE_HOOKS into nested HookEvent structure, writes JSON if addedAny=true, returns false if all hooks pre-existing.\n\n`registerGeminiHooks(settingsPath: string, runtimeDir: string, dryRun: boolean): boolean` — loads settings.json, merges ARE_HOOKS into flat GeminiHook array structure, uses hookDef.name field for Gemini format.\n\nHook command pattern: `node ${runtimeDir}/hooks/${hookDef.filename}` where runtimeDir='.claude' or '.gemini'.\n\nDuplicate detection: checks existing hooks via `event.hooks?.some((h) => h.command === hookCommand)` for Claude, `hooks[event].some((h) => h.command === hookCommand)` for Gemini.\n\n## Permission Registration\n\n`ARE_PERMISSIONS: string[]` — bash command patterns for auto-allow:\n- `'Bash(npx agents-reverse-engineer@latest init*)'`\n- `'Bash(npx agents-reverse-engineer@latest discover*)'`\n- `'Bash(npx agents-reverse-engineer@latest generate*)'`\n- `'Bash(npx agents-reverse-engineer@latest update*)'`\n- `'Bash(npx agents-reverse-engineer@latest clean*)'`\n- `'Bash(rm -f .agents-reverse-engineer/progress.log*)'`\n- `'Bash(sleep *)'`\n\nMerges via `settings.permissions.allow.includes(perm)` check before push, writes settings.json if addedAny=true.\n\n## File Operations\n\n`installFilesForRuntime(runtime: Exclude<Runtime, 'all'>, location: Location, options: InstallOptions): InstallerResult` — resolves basePath via resolveInstallPath(), fetches templates via getTemplatesForRuntime(), writes command files, installs hooks/plugins based on runtime type, calls registerHooks()/registerPermissions() for Claude, writeVersionFile() on success.\n\nTemplate path extraction: splits template.path by '/' and slices first segment (runtime directory prefix) to get relativePath joined to basePath.\n\nFile write flow: existsSync() skip check unless options.force=true, ensureDir() via mkdirSync recursive, writeFileSync() with UTF-8, error capture to errors[] array.\n\nHook/plugin installation branches:\n- Claude/Gemini: iterate ARE_HOOKS, call readBundledHook(), write to basePath/hooks/, invoke registerHooks()\n- OpenCode: iterate ARE_PLUGINS, call readBundledHook(pluginDef.srcFilename), write to basePath/plugins/, set hookRegistered=true\n\n## Bundled Hook Resolution\n\n`getBundledHookPath(hookName: string): string` — resolves from dist/installer/operations.js up two levels to project root, then joins 'hooks/dist/' + hookName.\n\n`readBundledHook(hookName: string): string` — reads hook content via getBundledHookPath(), throws Error if !existsSync().\n\nPath navigation: uses fileURLToPath(import.meta.url) for ESM compatibility, dirname resolution via path.dirname(), relative join via `__dirname/../../../hooks/dist/`.\n\n## Template Retrieval\n\n`getTemplatesForRuntime(runtime: Exclude<Runtime, 'all'>)` — switch dispatches to getClaudeTemplates() | getOpenCodeTemplates() | getGeminiTemplates() from src/integration/templates.ts.\n\n## Utility Functions\n\n`ensureDir(filePath: string): void` — creates parent directory via mkdirSync({ recursive: true }) if !existsSync(dirname).\n\n## Integration Points\n\nImports resolveInstallPath() and getAllRuntimes() from `./paths.js` for runtime-specific directory resolution.\n\nImports getClaudeTemplates(), getOpenCodeTemplates(), getGeminiTemplates() from `../integration/templates.js` for command file content.\n\nRuntime type discriminated union: `Runtime = 'claude' | 'opencode' | 'gemini' | 'all'` determines installation target and hook format.\n\nLocation type: `Location = 'global' | 'local'` controls base path resolution (e.g., ~/.claude vs ./.claude).\n\n## Error Handling\n\nNon-blocking errors: JSON parse failures in registerHooks() start with empty settings object, writeVersionFile() failures suppressed (non-fatal), ARE-VERSION write errors don't populate errors[] array.\n\nBlocking errors: readBundledHook() throws if hook not found, file write failures push to errors[] and set success=false in InstallerResult.\n\n## Dry-Run Mode\n\nWhen options.dryRun=true: skips writeFileSync() calls for templates/hooks/settings.json/ARE-VERSION, still populates filesCreated[] for preview output, sets versionWritten=false.\n### paths.ts\n**Purpose:** paths.ts resolves cross-platform installation paths for AI runtime configurations (Claude Code, OpenCode, Gemini) wit...\n\n**paths.ts resolves cross-platform installation paths for AI runtime configurations (Claude Code, OpenCode, Gemini) with environment variable overrides and directory existence checks.**\n\n## Exported Functions\n\n**getAllRuntimes()** → `Array<Exclude<Runtime, 'all'>>`\nReturns hardcoded array `['claude', 'opencode', 'gemini']` excluding the meta-runtime `'all'`.\n\n**getRuntimePaths(runtime: Exclude<Runtime, 'all'>)** → `RuntimePaths`\nReturns path configuration object with `global` (absolute path to runtime config dir), `local` (relative project-local dir name), and `settingsFile` (absolute path to settings.json). Environment overrides: `CLAUDE_CONFIG_DIR` for Claude (`~/.claude` fallback), `OPENCODE_CONFIG_DIR` or `XDG_CONFIG_HOME/opencode` for OpenCode (`~/.config/opencode` fallback), `GEMINI_CONFIG_DIR` for Gemini (`~/.gemini` fallback). Uses `os.homedir()` for home resolution and `path.join()` for cross-platform compatibility.\n\n**resolveInstallPath(runtime, location, projectRoot?)** → `string`\nResolves absolute installation path. For `location === 'global'`, returns `getRuntimePaths(runtime).global`. For local, joins `projectRoot || process.cwd()` with local path name (`.claude`, `.opencode`, or `.gemini`).\n\n**getSettingsPath(runtime: Exclude<Runtime, 'all'>)** → `string`\nReturns `getRuntimePaths(runtime).settingsFile` absolute path. Used for hook registration in Claude Code's settings.json.\n\n**isRuntimeInstalledLocally(runtime, projectRoot)** → `Promise<boolean>`\nChecks if local config directory exists via `stat()` and `stats.isDirectory()`. Joins `projectRoot` with local path name. Returns `false` on error.\n\n**isRuntimeInstalledGlobally(runtime)** → `Promise<boolean>`\nChecks if global config directory exists via `stat()` and `stats.isDirectory()` on `getRuntimePaths(runtime).global`. Returns `false` on error.\n\n**getInstalledRuntimes(projectRoot)** → `Promise<Array<Exclude<Runtime, 'all'>>>`\nIterates `getAllRuntimes()`, filters via `isRuntimeInstalledLocally()`, accumulates installed runtime identifiers into returned array.\n\n## Environment Variable Overrides\n\n- **Claude**: `CLAUDE_CONFIG_DIR` overrides `~/.claude`\n- **OpenCode**: `OPENCODE_CONFIG_DIR` > `XDG_CONFIG_HOME/opencode` > `~/.config/opencode` (priority order)\n- **Gemini**: `GEMINI_CONFIG_DIR` overrides `~/.gemini`\n\n## Path Patterns\n\n**Global paths:**\n- Claude: `~/.claude` or `$CLAUDE_CONFIG_DIR`\n- OpenCode: `~/.config/opencode`, `$XDG_CONFIG_HOME/opencode`, or `$OPENCODE_CONFIG_DIR`\n- Gemini: `~/.gemini` or `$GEMINI_CONFIG_DIR`\n\n**Local paths:**\n- Claude: `.claude` (relative)\n- OpenCode: `.opencode` (relative)\n- Gemini: `.gemini` (relative)\n\n**Settings files:**\n- All runtimes: `<global_path>/settings.json`\n\n## Dependencies\n\nImports `os.homedir()` for home directory resolution, `path.join()` for cross-platform path construction, `stat()` from `node:fs/promises` for directory existence checks, and `Runtime`, `Location`, `RuntimePaths` types from `./types.js`.\n### prompts.ts\n**Purpose:** Interactive prompt orchestrator for installer CLI providing TTY arrow-key selection with numbered fallback, raw mode ...\n\n**Interactive prompt orchestrator for installer CLI providing TTY arrow-key selection with numbered fallback, raw mode cleanup guarantees, and runtime/location/confirmation flows.**\n\n## Exported Functions\n\n**isInteractive(): boolean** — Returns `true` if `process.stdin.isTTY === true`, otherwise `false` for CI/piped input detection.\n\n**selectOption<T>(prompt: string, options: SelectOption<T>[]): Promise<T>** — Generic selector routing to `arrowKeySelect()` in TTY mode or `numberedSelect()` in non-interactive mode based on `isInteractive()` result.\n\n**selectRuntime(mode: 'install' | 'uninstall' = 'install'): Promise<Runtime>** — Prompts for runtime selection with options: `'claude'`, `'opencode'`, `'gemini'`, `'all'`. Uses `selectOption<Runtime>()` with labels \"Claude Code\", \"OpenCode\", \"Gemini CLI\", \"All runtimes\".\n\n**selectLocation(mode: 'install' | 'uninstall' = 'install'): Promise<Location>** — Prompts for location selection with options: `'global'` (\"Global (~/.claude, ~/.config/opencode, etc.)\"), `'local'` (\"Local (./.claude, ./.opencode, etc.)\"). Uses `selectOption<Location>()`.\n\n**confirmAction(message: string): Promise<boolean>** — Prompts for Yes/No confirmation using `selectOption<boolean>()` with labels \"Yes\" (`true`), \"No\" (`false`).\n\n## Internal Selection Implementations\n\n**arrowKeySelect<T>(prompt: string, options: SelectOption<T>[]): Promise<T>** — Interactive arrow-key navigator using `readline.emitKeypressEvents()` + `process.stdin.setRawMode(true)`. Listens for keypress events: `'up'` decrements `selectedIndex` (clamped to `0`), `'down'` increments `selectedIndex` (clamped to `options.length - 1`), `'return'` resolves with `options[selectedIndex].value`. Renders prompt with `pc.bold()` and options with `pc.cyan('> ')` prefix for selected index. Uses ANSI escape sequences `\\x1b[${n}A` (move cursor up), `\\x1b[2K` (clear line), `\\x1b[1B` (move cursor down) for re-rendering on navigation. Handles Ctrl+C via `key.ctrl && key.name === 'c'` check calling `cleanupRawMode()` + `process.exit(0)`. Always executes cleanup in try/catch wrapper.\n\n**numberedSelect<T>(prompt: string, options: SelectOption<T>[]): Promise<T>** — Non-interactive numbered fallback printing `pc.bold(prompt)` and options as `\"  ${idx + 1}. ${opt.label}\"`. Creates `readline.createInterface()` with stdin/stdout, prompts `'Enter number: '`, parses input via `parseInt(answer, 10)`, validates range `1-${options.length}`, resolves with `options[num - 1].value`. Rejects with `Error('Invalid selection: ${answer}. Expected 1-${options.length}')` on parse failure or out-of-range.\n\n## Raw Mode State Management\n\n**rawModeActive: boolean** — Module-level flag tracking `process.stdin.setRawMode(true)` state to prevent double-cleanup errors.\n\n**cleanupRawMode(): void** — Restores terminal by calling `process.stdin.setRawMode(false)` + `process.stdin.pause()` when `rawModeActive && process.stdin.isTTY`. Sets `rawModeActive = false`. Swallows exceptions during cleanup via empty catch block.\n\n**Global exit handlers** — Registers `process.on('exit', cleanupRawMode)` and `process.on('SIGINT', ...)` with `cleanupRawMode()` + `process.exit(0)` to guarantee terminal restoration on process termination or Ctrl+C interruption.\n\n## Type Definitions\n\n**SelectOption<T>** — Interface with `label: string` (display text) and `value: T` (resolved value). Used for generic option construction in `selectOption()`, `selectRuntime()`, `selectLocation()`, `confirmAction()`.\n\n## Dependencies\n\nUses `node:readline` for `emitKeypressEvents()`, `createInterface()`, and raw mode stdin manipulation. Uses `picocolors` (`pc`) for `pc.bold()`, `pc.cyan()` terminal formatting. Imports `Runtime`, `Location` from `'./types.js'` for typed option values.\n### types.ts\n**Purpose:** Defines TypeScript interfaces for npx installer workflow supporting multi-runtime installation (Claude Code, OpenCode...\n\n**Defines TypeScript interfaces for npx installer workflow supporting multi-runtime installation (Claude Code, OpenCode, Gemini) with global/local location targeting, force overwrite, and result tracking.**\n\n## Exported Types\n\n**`Runtime`** — Union type `'claude' | 'opencode' | 'gemini' | 'all'` specifying target AI assistant runtime for installation. Maps to directory paths: `claude` → `~/.claude`, `opencode` → `~/.config/opencode`, `gemini` → `~/.gemini`, `all` → installs to all three runtimes sequentially.\n\n**`Location`** — Union type `'global' | 'local'` controlling installation scope. `global` targets user-level directories (`~/.claude`), `local` targets project-level directories (`.claude`).\n\n**`InstallerArgs`** — Command-line arguments interface with fields:\n- `runtime?: Runtime` — Optional target runtime (if undefined, prompts interactively)\n- `global: boolean` — Enable global installation flag\n- `local: boolean` — Enable local installation flag\n- `uninstall: boolean` — Uninstall mode flag\n- `force: boolean` — Overwrite existing files without prompting\n- `help: boolean` — Display help and exit\n- `quiet: boolean` — Suppress banner and informational output\n\n**`InstallerResult`** — Operation result interface with fields:\n- `success: boolean` — Overall operation success status\n- `runtime: Exclude<Runtime, 'all'>` — Excludes `'all'` union member, enforces single runtime per result\n- `location: Location` — Installation target location\n- `filesCreated: string[]` — Absolute paths of successfully written files\n- `filesSkipped: string[]` — Files skipped due to pre-existence without `--force`\n- `errors: string[]` — Error messages encountered during operation\n- `hookRegistered?: boolean` — Optional field for Claude runtime indicating SessionStart/SessionEnd hook registration in `settings.json`\n- `versionWritten?: boolean` — Optional field indicating VERSION file creation (used by update checker hooks)\n\n**`RuntimePaths`** — Resolved path configuration interface with fields:\n- `global: string` — Absolute path to global runtime directory (e.g., `~/.claude`)\n- `local: string` — Absolute path to local runtime directory (e.g., `.claude`)\n- `settingsFile: string` — Absolute path to runtime settings file for hook registration (e.g., `~/.claude/settings.json`)\n\n## Type Constraints\n\n**`Exclude<Runtime, 'all'>`** in `InstallerResult.runtime` prevents reporting aggregate results for multi-runtime installations. Enforces atomic result objects per runtime/location pair. Installer orchestrator must decompose `'all'` into separate operations yielding distinct `InstallerResult` instances for `'claude'`, `'opencode'`, `'gemini'`.\n\n## Integration Points\n\nConsumed by `src/installer/operations.ts` for file copying logic, `src/installer/prompts.ts` for interactive runtime selection, `src/installer/paths.ts` for directory resolution with environment variable overrides (`CLAUDE_CONFIG_DIR`, `OPENCODE_CONFIG_DIR`, `GEMINI_CONFIG_DIR`), and `src/installer/uninstall.ts` for file deletion workflow.\n### uninstall.ts\n**Purpose:** Removes installed ARE command files, hooks, plugins, permissions, and configuration directories with runtime-specific...\n\n**Removes installed ARE command files, hooks, plugins, permissions, and configuration directories with runtime-specific cleanup strategies matching installation mirror logic.**\n\n## Exported Functions\n\n- `uninstallFiles(runtime: Runtime, location: Location, dryRun: boolean = false): InstallerResult[]` — Orchestrates uninstallation for single runtime or all runtimes via `getAllRuntimes()` iteration when `runtime === 'all'`, returning array of results\n- `unregisterHooks(basePath: string, runtime: Exclude<Runtime, 'all'>, dryRun: boolean): boolean` — Removes ARE hook entries from `settings.json` SessionStart/SessionEnd arrays, dispatches to `unregisterClaudeHooks()` or `unregisterGeminiHooks()` based on runtime, returns true if any hook removed\n- `unregisterPermissions(basePath: string, dryRun: boolean): boolean` — Filters `ARE_PERMISSIONS` array from `settings.json` permissions.allow, cleans up empty structures, returns true if changes made\n- `deleteConfigFolder(location: Location, dryRun: boolean): boolean` — Recursively deletes `.agents-reverse-engineer` directory via `rmSync()` with `recursive: true, force: true` only for `location === 'local'`\n\n## Hook and Plugin Definitions\n\n- `ARE_HOOKS: HookDefinition[]` — Array of `{ event: 'SessionStart' | 'SessionEnd', filename: string }` pairs: `are-check-update.js` (SessionStart), `are-session-end.js` (SessionEnd)\n- `ARE_PLUGIN_FILENAMES: string[]` — OpenCode plugin filenames: `['are-check-update.js', 'are-session-end.js']`\n- `ARE_PERMISSIONS: string[]` — Bash command permission patterns: `'Bash(npx agents-reverse-engineer@latest init*)'`, `discover*`, `generate*`, `update*`, `clean*`\n- `CONFIG_DIR = '.agents-reverse-engineer'` — Configuration directory constant matching `config/loader.ts`\n\n## Runtime-Specific Uninstallation\n\n`uninstallFilesForRuntime(runtime: Exclude<Runtime, 'all'>, location: Location, dryRun: boolean): InstallerResult` performs sequential cleanup:\n\n1. **Command template removal**: Iterates `getTemplatesForRuntime()` output, extracts relative paths after runtime directory prefix, calls `unlinkSync()` on full paths constructed via `path.join(basePath, relativePath)`\n2. **Hook/plugin removal**: For Claude/Gemini, deletes files from `hooks/` subdirectory using `ARE_HOOKS` array; for OpenCode, deletes from `plugins/` using `ARE_PLUGIN_FILENAMES`\n3. **Settings.json modification**: Calls `unregisterHooks()` for all runtimes, additionally calls `unregisterPermissions()` for Claude only\n4. **Version file cleanup**: Deletes `ARE-VERSION` from basePath via `unlinkSync()`\n5. **Directory cleanup**: Invokes `cleanupAreSkillDirs()` for Claude skills format, `cleanupEmptyDirs()` for all runtimes, `cleanupLegacyGeminiFiles()` for Gemini\n\nReturns `InstallerResult` with `filesCreated` repurposed to track deleted files, `filesSkipped` for nonexistent paths, `hookRegistered` repurposed to indicate hook unregistration success.\n\n## Settings File Manipulation\n\n`unregisterClaudeHooks(basePath: string, dryRun: boolean): boolean` implements pattern-based filtering:\n\n- Builds hook command patterns via `getHookPatterns('.claude')` including current format `node .claude/hooks/${filename}` and legacy `node hooks/${filename}`\n- Filters `settings.hooks.SessionStart` and `SessionEnd` arrays removing events whose `hooks[].command` matches patterns via `event.hooks?.some(h => hookPatterns.includes(h.command))`\n- Cleans empty arrays via `delete settings.hooks[eventType]`, empty hooks object via `delete settings.hooks`\n- Writes updated JSON via `writeFileSync()` with 2-space indentation\n\n`unregisterGeminiHooks()` follows identical logic but operates on simpler `GeminiHook[]` arrays (direct `command` property without nested `hooks[]` structure) and uses `.gemini` runtime directory in patterns.\n\n`unregisterPermissions()` filters `settings.permissions.allow` array via `!ARE_PERMISSIONS.includes(perm)`, removes empty `allow` array and `permissions` object structures.\n\n## Type Definitions\n\n- `SessionHook` — `{ type: 'command', command: string }` for Claude/Gemini hook registration\n- `HookEvent` — `{ hooks: SessionHook[] }` wrapper\n- `SettingsJson` — Claude settings schema with `hooks?: { SessionStart?: HookEvent[], SessionEnd?: HookEvent[] }`, `permissions?: { allow?: string[], deny?: string[] }`\n- `GeminiHook` — Simpler format `{ name: string, type: 'command', command: string }` without nested hooks array\n- `GeminiSettingsJson` — Gemini settings schema with `hooks?: { SessionStart?: GeminiHook[], SessionEnd?: GeminiHook[] }`\n- `HookDefinition` — `{ event: 'SessionStart' | 'SessionEnd', filename: string }`\n\n## Directory Cleanup Strategies\n\n`cleanupAreSkillDirs(skillsDir: string)` iterates `readdirSync()` output filtering entries via `entry.startsWith('are-')`, calls `cleanupEmptyDirs()` on each matching skill directory.\n\n`cleanupEmptyDirs(dirPath: string)` implements recursive bottom-up removal:\n\n- Checks `readdirSync().length === 0` predicate\n- Calls `rmdirSync()` on empty directory\n- Recurses to parent via `path.dirname()` with terminal condition checking `baseName` against runtime roots: `.claude`, `.opencode`, `.gemini`, `.config`\n\n`cleanupLegacyGeminiFiles(commandsDir: string)` removes obsolete formats:\n\n- Deletes `are-*.md` files (pre-TOML Markdown format) via `entry.startsWith('are-') && entry.endsWith('.md')` predicate\n- Recursively deletes `.toml` files from nested `commands/are/` directory (old namespace structure), then removes empty `are/` directory\n\n## Template Resolution\n\n`getTemplatesForRuntime(runtime: Exclude<Runtime, 'all'>)` dispatches to integration layer functions:\n\n- `'claude'` → `getClaudeTemplates()` from `src/integration/templates.ts`\n- `'opencode'` → `getOpenCodeTemplates()`\n- `'gemini'` → `getGeminiTemplates()`\n\nEach returns array of template objects with `.path` property in format `.<runtime>/<subdir>/<filename>` requiring `.split('/').slice(1).join('/')` to extract relative installation path.\n\n## Error Handling\n\nAll file system operations wrapped in try-catch blocks with errors collected into `InstallerResult.errors` array. Pattern: `catch (err) { errors.push(\\`Failed to delete ${path}: ${err}\\`) }`. Missing files tracked separately in `filesSkipped` array via `existsSync()` checks before deletion attempts. Directory cleanup operations silently ignore errors via empty catch blocks to handle permission issues and in-use directories.\n\n## Import Map (verified — use these exact paths)\n\nbanner.ts:\n  ../version.js → getVersion\n\noperations.ts:\n  ../integration/templates.js → getClaudeTemplates, getOpenCodeTemplates, getGeminiTemplates\n\nuninstall.ts:\n  ../integration/templates.js → getClaudeTemplates, getOpenCodeTemplates, getGeminiTemplates\n\n## Project Directory Structure\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>",
      "systemPrompt": "You are generating an AGENTS.md file — a directory-level overview for AI coding assistants.\n\nCRITICAL: Output ONLY the raw markdown content. No code fences, no preamble, no explanations, no conversational text. Your entire response IS the AGENTS.md file content.\n\nOUTPUT FORMAT:\n- First line MUST be exactly: <!-- Generated by agents-reverse-engineer -->\n- Use a # heading with the directory name\n- Write a one-paragraph purpose statement for the directory\nusage\nADAPTIVE SECTIONS:\nAnalyze the directory contents and choose the most relevant sections. Do NOT use a fixed template. Instead, select sections that best document this specific directory for an AI that needs to reconstruct or extend the project.\n\nConsider these section types (choose what applies):\n- **Contents**: Group files by purpose/category under ## headings. For each file: markdown link [filename](./filename) and a one-line description.\n- **Subdirectories**: If subdirectories exist, list them with links [dirname/](./dirname/) and brief summaries.\n- **Architecture / Data Flow**: If files form a pipeline, request/response chain, or layered architecture, document it.\n- **Stack**: If this is a package root (has package.json, Cargo.toml, go.mod, etc.), document the technology stack, key scripts, and entry points.\n- **Structure**: If the directory layout follows a convention (feature-sliced, domain-driven, MVC, etc.), document it.\n- **Patterns**: If files share recurring design patterns (factory, strategy, middleware, barrel re-export), name and document them.\n- **Configuration**: If the directory contains config files, schemas, or environment definitions, document the config surface area.\n- **API Surface**: If the directory exports a public API (barrel index, route definitions, SDK), document the interface contract.\n- **File Relationships**: How files collaborate, depend on each other, or share state.\n- **Behavioral Contracts**: If files contain regex patterns, format specifications, magic constants, or template strings that define observable behavior, collect them in a dedicated section. Preserve verbatim patterns from file summaries — do NOT paraphrase regex into prose. This section is MANDATORY when file summaries contain behavioral artifacts.\n- **Reproduction-Critical Constants**: If file summaries reference annex files (via ## Annex References sections), list them with links. Example: \"Full prompt template text: [templates.ts.annex.md](./prompts/templates.ts.annex.md)\". Do NOT reproduce annex content in AGENTS.md — just link to it.\n\nChoose any relevant sections or create your own based on the directory contents. The goal is to provide a comprehensive overview that captures the essence of the directory's role in the project and how its files work together, with a focus on what an AI coding assistant would need to know to effectively interact with this code.\n\nSCOPE:\n- AGENTS.md is a NAVIGATIONAL INDEX — help an AI find the right file quickly\n- Focus on: what each file does, how files relate, directory-level patterns\n- Do NOT reproduce full architecture sections — those belong in the root CLAUDE.md\n\nPATH ACCURACY (MANDATORY):\n- When referencing files or modules outside this directory, use ONLY paths from the \"Import Map\" section\n- Do NOT invent, rename, or guess module paths — if a path isn't in the Import Map, don't reference it\n- Use the exact directory names from \"Project Directory Structure\" — do NOT rename directories\n  (e.g., if the directory is called \"cli\", write \"src/cli/\", NOT \"src/commands/\")\n- Cross-module references must use the specifier format from actual import statements\n  (e.g., \"../generation/writers/sum.js\", NOT \"../fs/sum-file.js\")\n- If you are unsure about a path, omit the cross-reference rather than guessing\n\nCONSISTENCY (MANDATORY):\n- Do not contradict yourself within the same document\n- If you describe a technique (e.g., \"regex-based\"), do not call it something else later (e.g., \"AST-based\")\n- When stating version numbers, engines, or config fields, use ONLY values present in the file summaries\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this directory\", \"this module\", \"provides\", \"responsible for\", \"is used to\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Per-file descriptions in Contents sections: 1-2 sentences maximum. Reference key symbols but do not reproduce full summaries.\n- Behavioral contracts (regex patterns, format specs, constants) belong in a separate Behavioral Contracts section, not in per-file descriptions.\n- Annex file references: link to .annex.md files, do not inline their content. One line per annex reference.\n- Subdirectory descriptions: 1-2 sentences maximum. Capture the directory's role, not its full contents.\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- Key exported symbols from file summaries MUST appear in the directory overview\n- Preserve exact casing of identifiers\n\nUSER NOTES:\n- If \"User Notes\" are provided in the prompt, they contain user-defined instructions that will be automatically prepended to your output\n- Do NOT repeat or paraphrase user notes in your generated content — they are included separately\n- You may reference information from user notes for context",
      "response": "<!-- Generated by agents-reverse-engineer -->\n\n# src/installer\n\nOrchestrates npx-based installation of ARE commands and hooks across Claude Code, OpenCode, and Gemini CLI runtimes with interactive prompts, platform-specific path resolution, settings.json manipulation, and uninstallation workflows.\n\n## Contents\n\n### Core Orchestration\n\n**[index.ts](./index.ts)** — Entry point exposing `runInstaller()` and `parseInstallerArgs()` for CLI integration. Parses short/long flags (`-g`/`--global`, `-l`/`--local`, `--runtime`, `--force`, `--quiet`, `--uninstall`), validates non-interactive mode requirements, dispatches to `runInstall()` or `runUninstall()` based on arguments, displays results via `displayInstallResults()` and `displayUninstallResults()`. Re-exports all types and functions from sibling modules for barrel pattern.\n\n**[operations.ts](./operations.ts)** — Implements `installFiles()`, `verifyInstallation()`, `registerHooks()`, `registerPermissions()` for file copying, settings.json hook registration (SessionStart/SessionEnd events with nested HookEvent arrays for Claude, flat GeminiHook arrays for Gemini), and bash permission patterns (`ARE_PERMISSIONS` array: `'Bash(npx agents-reverse-engineer@latest init*)'` through `clean*`, `'Bash(rm -f .agents-reverse-engineer/progress.log*)'`, `'Bash(sleep *)'`). Resolves bundled hooks via `getBundledHookPath()` navigating from `dist/installer/operations.js` up to `hooks/dist/`. Writes `ARE-VERSION` file via `getPackageVersion()`.\n\n**[uninstall.ts](./uninstall.ts)** — Mirrors installation logic with `uninstallFiles()`, `unregisterHooks()`, `unregisterPermissions()`, `deleteConfigFolder()`. Removes command templates, hooks, plugins, settings.json entries, ARE-VERSION file. Implements `cleanupAreSkillDirs()` for Claude skills format, `cleanupEmptyDirs()` for recursive bottom-up directory removal, `cleanupLegacyGeminiFiles()` for obsolete Markdown/TOML formats. Repurposes `InstallerResult.filesCreated` to track deleted files, `hookRegistered` for unregistration status.\n\n### Path Resolution\n\n**[paths.ts](./paths.ts)** — Exports `getRuntimePaths()`, `resolveInstallPath()`, `getSettingsPath()`, `getAllRuntimes()`, `isRuntimeInstalledLocally()`, `isRuntimeInstalledGlobally()`, `getInstalledRuntimes()`. Resolves global/local paths with environment overrides: `CLAUDE_CONFIG_DIR` (`~/.claude` fallback), `OPENCODE_CONFIG_DIR` or `XDG_CONFIG_HOME/opencode` (`~/.config/opencode` fallback), `GEMINI_CONFIG_DIR` (`~/.gemini` fallback). Returns `RuntimePaths` with `global`, `local`, `settingsFile` fields.\n\n### User Interface\n\n**[prompts.ts](./prompts.ts)** — Provides `selectRuntime()`, `selectLocation()`, `confirmAction()`, `isInteractive()` with `arrowKeySelect()` for TTY mode (arrow-key navigation via `readline.emitKeypressEvents()` + `process.stdin.setRawMode(true)` with ANSI escape sequences `\\x1b[${n}A`, `\\x1b[2K`, `\\x1b[1B` for rendering) and `numberedSelect()` fallback for piped input. Registers `cleanupRawMode()` on `process.on('exit')` and `process.on('SIGINT')` to restore terminal state via `setRawMode(false)` + `pause()`.\n\n**[banner.ts](./banner.ts)** — Exports `displayBanner()` (Unicode box-drawing ASCII art logo U+2588/U+2550-U+2557), `showHelp()`, `showSuccess()`/`showError()`/`showWarning()`/`showInfo()` (prefix symbols: green ✓, red ✗, yellow !, cyan >), `showNextSteps()` (lists commands `/are-help`, `/are-init`, `/are-discover`, `/are-generate`, `/are-update`, `/are-specify`, `/are-clean` with GitHub documentation link).\n\n### Type Definitions\n\n**[types.ts](./types.ts)** — Defines `Runtime` (`'claude' | 'opencode' | 'gemini' | 'all'`), `Location` (`'global' | 'local'`), `InstallerArgs` (`runtime?`, `global`, `local`, `uninstall`, `force`, `help`, `quiet`), `InstallerResult` (`success`, `runtime: Exclude<Runtime, 'all'>`, `location`, `filesCreated`, `filesSkipped`, `errors`, `hookRegistered?`, `versionWritten?`), `RuntimePaths` (`global`, `local`, `settingsFile`).\n\n## Subdirectories\n\nNone — flat structure with seven TypeScript modules.\n\n## Architecture\n\n### Two-Phase Workflow\n\n**Installation (runInstall):**\n1. Argument parsing via `parseInstallerArgs()` with short/long flag normalization\n2. Interactive prompts via `selectRuntime()`/`selectLocation()` when TTY detected and values missing\n3. Template copying via `getTemplatesForRuntime()` → `getClaudeTemplates()`/`getOpenCodeTemplates()`/`getGeminiTemplates()` from `src/integration/templates.ts`\n4. Hook/plugin installation: Claude/Gemini to `hooks/` subdirectory using `ARE_HOOKS` array (currently empty), OpenCode to `plugins/` using `ARE_PLUGINS` array\n5. Settings.json modification: `registerHooks()` merges SessionStart/SessionEnd hooks with duplicate detection, `registerPermissions()` adds `ARE_PERMISSIONS` bash patterns to Claude's permissions.allow array\n6. Version file creation via `writeVersionFile()` for update checker hooks\n7. Verification via `verifyInstallation()` checking `existsSync()` for all filesCreated paths\n\n**Uninstallation (runUninstall):**\n1. Template path extraction with runtime prefix slicing (`template.path.split('/').slice(1).join('/')`)\n2. File deletion via `unlinkSync()` for templates, hooks, plugins, ARE-VERSION\n3. Settings.json cleanup: `unregisterHooks()` filters hook arrays via pattern matching (current format `node .claude/hooks/${filename}`, legacy `node hooks/${filename}`), `unregisterPermissions()` filters `ARE_PERMISSIONS` from permissions.allow\n4. Directory cleanup: `cleanupAreSkillDirs()` removes empty `are-*` skill directories, `cleanupEmptyDirs()` recursively removes empty parents up to runtime root, `cleanupLegacyGeminiFiles()` deletes obsolete `.md`/`.toml` formats\n5. Config folder deletion via `deleteConfigFolder()` only for local installations (`location === 'local'`)\n\n### Runtime-Specific Patterns\n\n**Claude Code:**\n- Global path: `~/.claude` (override: `CLAUDE_CONFIG_DIR`)\n- Commands: `.claude/skills/<command>/SKILL.md` with frontmatter `name: /are-<command>`\n- Hooks: `.claude/hooks/<filename>.js` registered in `settings.json` with nested structure `{ hooks: { SessionStart?: [{ hooks: [{ type: 'command', command: string }] }] } }`\n- Permissions: `settings.json` permissions.allow array with bash command patterns\n- Settings file: `~/.claude/settings.json`\n\n**OpenCode:**\n- Global path: `~/.config/opencode` (overrides: `OPENCODE_CONFIG_DIR` > `XDG_CONFIG_HOME/opencode`)\n- Commands: `.opencode/commands/<command>.md` with frontmatter `agent: build`\n- Plugins: `.opencode/plugins/<filename>.js` exporting async factory functions with `event['session.created']`/`event['session.deleted']` handlers\n- No settings file (plugin registration via file presence)\n\n**Gemini CLI:**\n- Global path: `~/.gemini` (override: `GEMINI_CONFIG_DIR`)\n- Commands: `.gemini/commands/<command>.toml` with `description`/`prompt` fields\n- Hooks: `.gemini/hooks/<filename>.js` registered in `settings.json` with flat structure `{ hooks: { SessionStart?: [{ name: string, type: 'command', command: string }] } }`\n- Settings file: `~/.gemini/settings.json`\n\n## Behavioral Contracts\n\n### Hook/Plugin Definitions\n\n**ARE_HOOKS (operations.ts):**\n```typescript\nconst ARE_HOOKS: HookDefinition[] = [\n  // Array intentionally empty — hooks disabled due to issues\n];\n```\n\n**ARE_PLUGINS (operations.ts):**\n```typescript\nconst ARE_PLUGINS: PluginDefinition[] = [\n  { srcFilename: 'opencode-are-check-update.js', destFilename: 'are-check-update.js' },\n  // are-session-end.js disabled\n];\n```\n\n**ARE_PERMISSIONS (operations.ts):**\n```typescript\nconst ARE_PERMISSIONS: string[] = [\n  'Bash(npx agents-reverse-engineer@latest init*)',\n  'Bash(npx agents-reverse-engineer@latest discover*)',\n  'Bash(npx agents-reverse-engineer@latest generate*)',\n  'Bash(npx agents-reverse-engineer@latest update*)',\n  'Bash(npx agents-reverse-engineer@latest clean*)',\n  'Bash(rm -f .agents-reverse-engineer/progress.log*)',\n  'Bash(sleep *)',\n];\n```\n\n**Hook command pattern (operations.ts):**\n```javascript\n`node ${runtimeDir}/hooks/${hookDef.filename}`\n```\n\n**Legacy hook patterns (uninstall.ts):**\n```typescript\nfunction getHookPatterns(runtimeDir: string): string[] {\n  return ARE_HOOKS.flatMap(hook => [\n    `node ${runtimeDir}/hooks/${hook.filename}`,  // Current format\n    `node hooks/${hook.filename}`,                 // Legacy format\n  ]);\n}\n```\n\n### ANSI Escape Sequences (prompts.ts)\n\n**Cursor control in arrowKeySelect:**\n- `\\x1b[${n}A` — Move cursor up n lines\n- `\\x1b[2K` — Clear entire line\n- `\\x1b[1B` — Move cursor down 1 line\n\n**Keypress handling:**\n- Up arrow: `key.name === 'up'` → `selectedIndex = Math.max(0, selectedIndex - 1)`\n- Down arrow: `key.name === 'down'` → `selectedIndex = Math.min(options.length - 1, selectedIndex + 1)`\n- Enter: `key.name === 'return'` → resolve with `options[selectedIndex].value`\n- Ctrl+C: `key.ctrl && key.name === 'c'` → `cleanupRawMode()` + `process.exit(0)`\n\n### Settings.json Schemas\n\n**Claude (operations.ts):**\n```typescript\ninterface SettingsJson {\n  hooks?: {\n    SessionStart?: HookEvent[];\n    SessionEnd?: HookEvent[];\n  };\n  permissions?: {\n    allow?: string[];\n    deny?: string[];\n  };\n}\n\ninterface HookEvent {\n  hooks: SessionHook[];\n}\n\ninterface SessionHook {\n  type: 'command';\n  command: string;\n}\n```\n\n**Gemini (operations.ts):**\n```typescript\ninterface GeminiSettingsJson {\n  hooks?: {\n    SessionStart?: GeminiHook[];\n    SessionEnd?: GeminiHook[];\n  };\n}\n\ninterface GeminiHook {\n  name: string;\n  type: 'command';\n  command: string;\n}\n```\n\n### Directory Cleanup Terminal Conditions (uninstall.ts)\n\n**Runtime root check in cleanupEmptyDirs:**\n```typescript\nconst baseName = path.basename(dirPath);\nif (['.claude', '.opencode', '.gemini', '.config'].includes(baseName)) {\n  return;\n}\n```\n\n**ARE skill directory pattern (uninstall.ts):**\n```typescript\nentry.startsWith('are-') && stats.isDirectory()\n```\n\n**Legacy Gemini file patterns (uninstall.ts):**\n```typescript\n// Markdown format\nentry.startsWith('are-') && entry.endsWith('.md')\n\n// TOML namespace directory\n'commands/are/*.toml'\n```\n\n## File Relationships\n\n**Orchestration chain:**\n1. `index.ts` parses args → calls `prompts.ts` selectors → calls `operations.ts` or `uninstall.ts`\n2. `operations.ts`/`uninstall.ts` call `paths.ts` for directory resolution\n3. `operations.ts` calls `src/integration/templates.ts` for command content\n4. `operations.ts` reads bundled hooks from `hooks/dist/` via `getBundledHookPath()`\n5. `operations.ts`/`uninstall.ts` manipulate settings.json via in-memory JSON parse/stringify\n6. `banner.ts` provides display functions consumed by `index.ts` for results rendering\n\n**Shared state:**\n- `prompts.ts` maintains module-level `rawModeActive` flag for terminal cleanup\n- `operations.ts` and `uninstall.ts` share `ARE_HOOKS`, `ARE_PLUGINS`, `ARE_PERMISSIONS` definitions\n- `paths.ts` provides single source of truth for runtime directory mappings\n\n**Type flow:**\n- `types.ts` defines discriminated union `Runtime` with `'all'` excluded via `Exclude<Runtime, 'all'>` in `InstallerResult.runtime`\n- `InstallerArgs` parsed in `index.ts`, threaded through `runInstall()`/`runUninstall()` to `operations.ts`/`uninstall.ts`\n- `RuntimePaths` returned by `paths.ts`, consumed by `operations.ts`/`uninstall.ts` for file path construction",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 3518,
      "cacheReadTokens": 0,
      "cacheCreationTokens": 35563,
      "latencyMs": 74908,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": []
    },
    {
      "timestamp": "2026-02-09T17:47:47.818Z",
      "prompt": "Generate AGENTS.md for directory: \"src/orchestration\" (orchestration)\n\n## File Summaries (7 files)\n\n### index.ts\n**Purpose:** Barrel export consolidating orchestration module: `runPool`, `ProgressReporter`, `ProgressLog`, `PlanTracker`, `creat...\n\n**Barrel export consolidating orchestration module: `runPool`, `ProgressReporter`, `ProgressLog`, `PlanTracker`, `createTraceWriter`, `cleanupOldTraces`, `CommandRunner`, and shared types (`FileTaskResult`, `RunSummary`, `ProgressEvent`, `CommandRunOptions`, `PoolOptions`, `TaskResult`, `ITraceWriter`, `TraceEvent`, `TraceEventPayload`).**\n\n## Exported Types\n\n**FileTaskResult** — result of single file AI analysis with `path`, `success`, `tokensIn`, `tokensOut`, `cacheReadTokens`, `cacheCreationTokens`, `durationMs`, `model`, `error?`.\n\n**RunSummary** — aggregated run summary with `version`, `filesProcessed`, `filesFailed`, `filesSkipped`, `totalCalls`, `totalInputTokens`, `totalOutputTokens`, `totalCacheReadTokens`, `totalCacheCreationTokens`, `totalDurationMs`, `errorCount`, `retryCount`, `totalFilesRead`, `uniqueFilesRead`, `inconsistenciesCodeVsDoc?`, `inconsistenciesCodeVsCode?`, `phantomPaths?`, `inconsistencyReport?`.\n\n**ProgressEvent** — event emitted to `ProgressReporter` with discriminated `type` (`start` | `done` | `error` | `dir-done` | `root-done`), `filePath`, `index`, `total`, `durationMs?`, `tokensIn?`, `tokensOut?`, `model?`, `error?`.\n\n**CommandRunOptions** — execution options with `concurrency`, `failFast?`, `debug?`, `dryRun?`, `tracer?: ITraceWriter`, `progressLog?: ProgressLog`.\n\n**PoolOptions** — pool configuration with `concurrency`, `failFast?`, `tracer?: ITraceWriter`, `phaseLabel?`, `taskLabels?: string[]`.\n\n**TaskResult<T>** — pool task result with `index`, `success`, `value?: T`, `error?: Error`.\n\n**ITraceWriter** — interface for trace event emission with `emit(event: TraceEventPayload): void`, `finalize(): Promise<void>`, `readonly filePath: string`.\n\n**TraceEvent** — discriminated union of 14 event types: `PhaseStartEvent`, `PhaseEndEvent`, `WorkerStartEvent`, `WorkerEndEvent`, `TaskPickupEvent`, `TaskDoneEvent`, `TaskStartEvent`, `SubprocessSpawnEvent`, `SubprocessExitEvent`, `RetryEvent`, `DiscoveryStartEvent`, `DiscoveryEndEvent`, `FilterAppliedEvent`, `PlanCreatedEvent`, `ConfigLoadedEvent`.\n\n**TraceEventPayload** — `DistributiveOmit<TraceEvent, 'seq' | 'ts' | 'pid' | 'elapsedMs'>` for event payload without auto-populated base fields.\n\n## Exported Functions\n\n**runPool<T>(tasks: Array<() => Promise<T>>, options: PoolOptions, onComplete?: (result: TaskResult<T>) => void): Promise<TaskResult<T>[]>** — iterator-based concurrency pool executing tasks via shared iterator pattern with N workers, optional `onComplete` callback per task settlement, failFast abort on first error, trace event emission for `worker:start/end`, `task:pickup/done`.\n\n**createTraceWriter(projectRoot: string, enabled: boolean): ITraceWriter** — factory returning `NullTraceWriter` when disabled (zero overhead) or `TraceWriter` appending NDJSON to `.agents-reverse-engineer/traces/trace-{timestamp}.ndjson` with promise-chain serialization for concurrent-safe writes.\n\n**cleanupOldTraces(projectRoot: string, keepCount?: number): Promise<number>** — removes old trace files from `.agents-reverse-engineer/traces/`, keeps most recent `keepCount` (default 500), returns deletion count.\n\n## Exported Classes\n\n**ProgressReporter** — streaming build-log reporter with `onFileStart(filePath)`, `onFileDone(filePath, durationMs, tokensIn, tokensOut, model, cacheReadTokens?, cacheCreationTokens?)`, `onFileError(filePath, error)`, `onDirectoryStart(dirPath)`, `onDirectoryDone(dirPath, durationMs, tokensIn, tokensOut, model, cacheReadTokens?, cacheCreationTokens?)`, `onRootDone(docPath)`, `printSummary(summary: RunSummary)`. Computes ETA via moving average of last 10 completion times (window size 10), displays after 2+ completions. Constructor: `new ProgressReporter(totalFiles: number, totalDirectories?: number, progressLog?: ProgressLog)`.\n\n**ProgressLog** — file-based progress log mirroring console output to `.agents-reverse-engineer/progress.log` with ANSI stripping via regex `/\\x1b\\[[0-9;]*m/g`, promise-chain serialized writes, `write(line: string): void`, `finalize(): Promise<void>`. Static factory: `ProgressLog.create(projectRoot: string): ProgressLog`.\n\n**PlanTracker** — updates `GENERATION-PLAN.md` checkboxes during generation via `markDone(itemPath: string): void`, replaces `- [ ] \\`itemPath\\`` → `- [x] \\`itemPath\\``, serializes writes via promise chain, `flush(): Promise<void>`. Constructor: `new PlanTracker(projectRoot: string, initialMarkdown: string)`. Methods: `initialize(): Promise<void>` creates parent directory and writes initial content.\n\n**CommandRunner** — three-phase orchestrator with `executeGenerate(plan: ExecutionPlan): Promise<RunSummary>` (concurrent file analysis, post-order directory AGENTS.md, sequential root docs) and `executeUpdate(filesToAnalyze: FileChange[], projectRoot: string, config: Config): Promise<RunSummary>` (Phase 1 only, no directory/root generation). Constructor: `new CommandRunner(aiService: AIService, options: CommandRunOptions)`. Wires `tracer` into `AIService.setTracer()` for subprocess/retry events.\n\n## Integration Pattern\n\n`CommandRunner` orchestrates three-phase pipeline by invoking `runPool` for Phase 1 file tasks and Phase 2 directory tasks (grouped by depth), emitting trace events via `tracer?.emit()`, reporting progress via `ProgressReporter`, tracking GENERATION-PLAN.md checkboxes via `PlanTracker`, caching old `.sum` content for stale doc detection in Pre-Phase 1 (concurrency 20), running inconsistency checks post-Phase 1 via throttled directory groups (concurrency 10), validating phantom paths post-Phase 2, and sequentially executing Phase 3 root tasks with explicit `task:start/done` trace events.\n\n## Concurrency Control\n\nPhase 1 file analysis uses `this.options.concurrency` (user-configured). Phase 2 directory tasks group by depth and execute depth levels sequentially (descending order, deepest first) with `Math.min(concurrency, dirsAtDepth.length)` parallelism per depth level. Phase 3 root tasks execute sequentially (concurrency 1).\n\n## Trace Event Flow\n\n`CommandRunner.executeGenerate()` emits: `phase:start` (pre-phase-1-cache, phase-1-files, post-phase-1-quality, phase-2-dirs-depth-N, post-phase-2-quality, phase-3-root), `phase:end` with `tasksCompleted`/`tasksFailed`/`durationMs`. Phase 3 emits `task:start`/`task:done` per root document. `runPool` emits `worker:start`/`end`, `task:pickup`/`done`. `AIService` emits `subprocess:spawn`/`exit`, `retry`.\n\n## Quality Validation Timing\n\nPre-Phase 1: reads existing `.sum` files via `readSumFile()` into `oldSumCache`. Post-Phase 1: groups files by directory, throttles inconsistency checks at concurrency 10, compares old `.sum` (stale) and new `.sum` (LLM omissions) via `checkCodeVsDoc()`, runs `checkCodeVsCode()` per directory group, emits `formatReportForCli()` to stderr. Post-Phase 2: validates phantom paths in AGENTS.md via `checkPhantomPaths()`.\n\n## Preamble Stripping\n\n**stripPreamble(responseText: string): string** — removes LLM preamble via two patterns: (1) content after `\\n---\\n` separator within first 500 chars, (2) content starting with bold purpose line (`**[A-Z]`) preceded by <300 chars preamble without `##` headers.\n\n**extractPurpose(responseText: string): string** — skips lines matching preamble prefixes (`now i`, `perfect`, `based on`, `let me`, `here is`, `i'll`, `i will`, `great`, `okay`, `sure`, `certainly`, `alright`) or headers/separators, strips bold markdown wrapper `**...**`, truncates to 120 chars with `...` suffix.\n### plan-tracker.ts\n**Purpose:** PlanTracker maintains in-memory GENERATION-PLAN.md markdown content and serializes concurrent checkbox updates via pr...\n\n**PlanTracker maintains in-memory GENERATION-PLAN.md markdown content and serializes concurrent checkbox updates via promise-chain writes to prevent file corruption.**\n\n## Exported Class\n\n`PlanTracker` provides three public methods:\n- `constructor(projectRoot: string, initialMarkdown: string)` — stores markdown in `this.content`, computes `this.planPath` as `path.join(projectRoot, CONFIG_DIR, 'GENERATION-PLAN.md')`\n- `async initialize(): Promise<void>` — creates parent directory via `mkdir(..., { recursive: true })`, writes `this.content` to `this.planPath`, swallows errors (non-critical)\n- `markDone(itemPath: string): void` — replaces checkbox pattern `` `- [ ] \\`${itemPath}\\`` `` with `` `- [x] \\`${itemPath}\\`` `` in `this.content`, chains write to `this.writeQueue` promise with `.catch()` suppression\n- `async flush(): Promise<void>` — awaits `this.writeQueue` to ensure all serialized writes complete\n\n## Concurrency Strategy\n\nMaintains `private writeQueue: Promise<void> = Promise.resolve()` to serialize disk writes. Each `markDone()` call appends `writeFile()` to the promise chain via `this.writeQueue = this.writeQueue.then(...)`, preventing race conditions when concurrent Phase 1 workers complete tasks simultaneously.\n\n## Usage Pattern\n\nInstantiated once at start of `executeGenerate()` in `src/generation/orchestrator.ts`, passed to worker pool, invoked by `markDone(itemPath)` after each task completion, finalized by `flush()` before function return.\n\n## Item Path Conventions\n\nCaller must pass exact path as it appears in markdown:\n- File analysis: `src/cli/init.ts`\n- Directory aggregation: `src/cli/AGENTS.md` (caller appends `/AGENTS.md` suffix)\n- Root synthesis: `CLAUDE.md`\n\n## Dependencies\n\nImports `writeFile` and `mkdir` from `node:fs/promises`, `path` from `node:path`, `CONFIG_DIR` constant from `src/config/loader.ts`.\n### pool.ts\n**Purpose:** pool.ts implements iterator-based concurrency limiting via shared-iterator worker pattern for parallel task execution...\n\n**pool.ts implements iterator-based concurrency limiting via shared-iterator worker pattern for parallel task execution with configurable worker count, fail-fast abort, and optional trace emission.**\n\n## Exported Functions\n\n**runPool<T>(tasks, options, onComplete?): Promise<TaskResult<T>[]>**\n\nExecutes array of async task factories through concurrency-limited pool using shared iterator pattern where N workers pull from single `tasks.entries()` iterator. Each worker invokes tasks sequentially until iterator exhaustion or abort flag set. Returns array of `TaskResult<T>` indexed by original task position (may be sparse if aborted via fail-fast).\n\nParameters:\n- `tasks: Array<() => Promise<T>>` — zero-argument async task factories\n- `options: PoolOptions` — pool configuration (concurrency, failFast, tracer, phaseLabel, taskLabels)\n- `onComplete?: (result: TaskResult<T>) => void` — optional callback invoked after each task settles\n\nWorker lifecycle: emits `worker:start` with workerId and phase, iterates over shared iterator checking `aborted` flag before each pickup, emits `task:pickup` with `activeTasks` snapshot, awaits task execution, catches errors converting non-Error values via `String(err)`, emits `task:done` with `durationMs` and `success` boolean, invokes `onComplete` callback with result, sets `aborted=true` on error if `failFast` enabled, emits `worker:end` with `tasksExecuted` count.\n\nEffective concurrency computed via `Math.min(options.concurrency, tasks.length)` to avoid spawning idle workers. All workers awaited via `Promise.allSettled()` ensuring completion even if individual workers reject.\n\n## Types\n\n**PoolOptions**\n- `concurrency: number` — maximum concurrent workers\n- `failFast?: boolean` — abort on first error (sets shared `aborted` flag preventing new task pickups)\n- `tracer?: ITraceWriter` — trace writer for emit calls (no-op when undefined)\n- `phaseLabel?: string` — phase identifier for trace events (defaults to `'unknown'`)\n- `taskLabels?: string[]` — per-task labels indexed by task position (used in `task:pickup`/`task:done` events, falls back to `'task-${index}'`)\n\n**TaskResult<T>**\n- `index: number` — zero-based task position in original array\n- `success: boolean` — settlement status\n- `value?: T` — resolved value when `success=true`\n- `error?: Error` — rejection error when `success=false`\n\n## Concurrency Strategy\n\nShared-iterator pattern prevents \"batch anti-pattern\" where `Promise.all()` on fixed-size chunks idles workers waiting for slowest task in batch. Single `tasks.entries()` iterator consumed by all workers ensures each task picked exactly once, with workers immediately pulling next task upon completion maintaining full worker utilization until iterator exhaustion.\n\n## Trace Events\n\nEmits via `tracer?.emit()` optional chaining (no-op when tracer undefined):\n- `worker:start` — `{ type, workerId, phase }`\n- `task:pickup` — `{ type, workerId, taskIndex, taskLabel, activeTasks }`\n- `task:done` — `{ type, workerId, taskIndex, taskLabel, durationMs, success, error?, activeTasks }`\n- `worker:end` — `{ type, workerId, phase, tasksExecuted }`\n\n`activeTasks` counter incremented at pickup, decremented at settlement, providing snapshot of concurrent task count at trace event emission time. Duration computed via `Date.now() - taskStart` without high-resolution timer.\n\n## Abort Mechanism\n\nShared mutable `aborted` boolean flag checked at loop start (`for (const [index, task] of iterator) { if (aborted) break; }`). Set to `true` in catch block when `options.failFast` enabled, causing all workers to stop pulling new tasks. Workers already executing tasks run to completion before checking flag. Results array may be sparse with undefined indices for tasks never started due to abort.\n### progress.ts\n**Purpose:** ProgressReporter provides streaming build-log progress reporting with ETA calculation via moving averages, outputting...\n\n**ProgressReporter provides streaming build-log progress reporting with ETA calculation via moving averages, outputting colored console lines and optionally mirroring plain-text output to `.agents-reverse-engineer/progress.log` for tail monitoring in buffered environments.**\n\n## Exported Classes\n\n**ProgressLog** — Plain-text progress log file writer mirroring console output without ANSI codes.\n\n```typescript\nclass ProgressLog {\n  constructor(private readonly filePath: string)\n  static create(projectRoot: string): ProgressLog\n  write(line: string): void\n  async finalize(): Promise<void>\n}\n```\n\nUses promise-chain serialization (`this.writeQueue = this.writeQueue.then(...)`) to serialize concurrent writes from multiple pool workers. Opens file handle in truncate mode (`'w'`) on first write, appends to open handle on subsequent writes. Silently swallows write failures (non-critical telemetry). Creates parent directory via `mkdir(path.dirname(this.filePath), { recursive: true })`. Static factory `create()` constructs path as `<projectRoot>/.agents-reverse-engineer/progress.log` using constant `PROGRESS_LOG_FILENAME = 'progress.log'`.\n\n**ProgressReporter** — Streaming build-log progress reporter tracking file and directory tasks with ETA calculation.\n\n```typescript\nclass ProgressReporter {\n  constructor(totalFiles: number, totalDirectories: number = 0, progressLog?: ProgressLog)\n  onFileStart(filePath: string): void\n  onFileDone(filePath: string, durationMs: number, tokensIn: number, tokensOut: number, model: string, cacheReadTokens?: number, cacheCreationTokens?: number): void\n  onFileError(filePath: string, error: string): void\n  onDirectoryStart(dirPath: string): void\n  onDirectoryDone(dirPath: string, durationMs: number, tokensIn: number, tokensOut: number, model: string, cacheReadTokens?: number, cacheCreationTokens?: number): void\n  onRootDone(docPath: string): void\n  printSummary(summary: RunSummary): void\n}\n```\n\nTracks counters: `started`, `completed`, `failed` for file tasks; `dirStarted`, `dirCompleted` for directory tasks. Maintains sliding windows `completionTimes` and `dirCompletionTimes` (max size `windowSize = 10`) for ETA moving averages. Records `startTime` via `Date.now()` for elapsed time calculation.\n\n## Output Formats\n\nFile analysis start: `[X/Y] ANALYZING path` (cyan ANALYZING)  \nFile analysis done: `[X/Y] DONE path Xs in/out tok model ~Ns remaining` (green DONE)  \nFile analysis fail: `[X/Y] FAIL path error` (red FAIL)  \nDirectory start: `[dir X/Y] ANALYZING dirPath/AGENTS.md` (cyan ANALYZING)  \nDirectory done: `[dir X/Y] DONE dirPath/AGENTS.md Xs in/out tok model ~ETA` (blue DONE)  \nRoot document done: `[root] DONE docPath` (blue DONE)\n\nAll output uses `console.log()` for atomic non-corrupting concurrent writes. Color formatting via `picocolors` aliases: `pc.dim()`, `pc.cyan()`, `pc.green()`, `pc.red()`, `pc.blue()`, `pc.yellow()`, `pc.bold()`.\n\n## ETA Calculation\n\n`formatETA()` computes file task ETA using moving average of `completionTimes` array. Requires minimum 2 completions before displaying ETA. Calculates `avg = sum(completionTimes) / completionTimes.length`, `remaining = totalFiles - completed - failed`, `etaMs = avg * remaining`. Formats as `~Ns remaining` for <60s, `~Mm Ss remaining` for ≥60s. Returns empty string if `remaining <= 0` or fewer than 2 completions.\n\n`formatDirectoryETA()` uses identical algorithm on `dirCompletionTimes` for directory tasks with `remaining = totalDirectories - dirCompleted`.\n\n## Token Counting\n\n`onFileDone()` and `onDirectoryDone()` accept `tokensIn` (non-cached input), `tokensOut` (output), `cacheReadTokens` (cache hits), `cacheCreationTokens` (cache writes). Displays total prompt size as `totalIn = tokensIn + cacheReadTokens + cacheCreationTokens` in format `${totalIn}/${tokensOut} tok`.\n\n## Summary Output\n\n`printSummary(summary: RunSummary)` prints multi-line report:\n- `ARE version` from `summary.version`\n- `Files processed` (green), `Files failed` (red if >0), `Files skipped` (yellow if >0)\n- `Total calls`, `Tokens: X in / Y out`\n- `Cache: X read / Y created` (if `totalCacheReadTokens > 0`)\n- `Files read: X (Y unique)` (if `totalFilesRead > 0`)\n- `Total time`, `Errors`, `Retries` (if `retryCount > 0`)\n\nComputes elapsed time as `(Date.now() - this.startTime) / 1000` formatted to 1 decimal place.\n\n## ANSI Stripping\n\n`stripAnsi(str: string)` removes ANSI escape codes via regex `/\\x1b\\[[0-9;]*m/g` before writing to ProgressLog. Matches all SGR, cursor, and erase sequences.\n\n## Integration Points\n\nReceives `RunSummary` type from `./types.js` with fields: `version`, `filesProcessed`, `filesFailed`, `filesSkipped`, `totalCalls`, `totalInputTokens`, `totalOutputTokens`, `totalCacheReadTokens`, `totalCacheCreationTokens`, `totalFilesRead`, `uniqueFilesRead`, `errorCount`, `retryCount`. Instantiated per command run with total file/directory counts from discovery phase. Event methods called by worker pool during task execution. `printSummary()` invoked by orchestrator after all phases complete.\n### runner.ts\n**Purpose:** CommandRunner orchestrates three-phase AI-driven documentation generation via concurrent worker pools, executing file...\n\n**CommandRunner orchestrates three-phase AI-driven documentation generation via concurrent worker pools, executing file analysis (Phase 1), post-order directory aggregation (Phase 2), and sequential root synthesis (Phase 3) with integrated quality validation, trace emission, and progress tracking.**\n\n## Public Interface\n\n### CommandRunner Class\n\n```typescript\nclass CommandRunner {\n  constructor(aiService: AIService, options: CommandRunOptions)\n  async executeGenerate(plan: ExecutionPlan): Promise<RunSummary>\n  async executeUpdate(\n    filesToAnalyze: FileChange[],\n    projectRoot: string,\n    config: Config\n  ): Promise<RunSummary>\n}\n```\n\nCommandRunner holds references to `AIService` and `CommandRunOptions`, wires tracer into AIService via `setTracer()`, manages lifecycle of `ProgressReporter` and `PlanTracker` instances.\n\n## Three-Phase Execution Pipeline\n\n### executeGenerate() Orchestration\n\n**Pre-Phase 1 (Cache Old Sum):**\n- Reads existing `.sum` files concurrently (concurrency=20) via `readSumFile()` to populate `oldSumCache` Map\n- Stores `SumFileContent` keyed by relative path for stale documentation detection\n- Emits `phase:start`/`phase:end` trace events with `phase: 'pre-phase-1-cache'`\n\n**Phase 1 (File Analysis):**\n- Builds file tasks from `plan.fileTasks` array, each reading source via `readFile()`, caching content in `sourceContentCache` Map\n- Calls `aiService.call()` with prompts from task, tracks file size via `aiService.addFilesReadToLastEntry([{path, sizeBytes}])`\n- Computes `contentHash` via `computeContentHashFromString()` from in-memory content (avoids second readFile)\n- Strips preamble via `stripPreamble()`, extracts purpose via `extractPurpose()`, writes `SumFileContent` via `writeSumFile()`\n- Detects `## Annex References` marker in cleaned text, writes annex via `writeAnnexFile()` if present\n- Executes via `runPool()` with `concurrency: options.concurrency`, emits `phase:start`/`phase:end` with label `'phase-1-files'`\n- Updates `PlanTracker` via `markDone(path)`, reports progress via `ProgressReporter.onFileDone()` or `onFileError()`\n\n**Post-Phase 1 (Quality Validation):**\n- Groups processed files by directory via `Map<dirname, paths[]>`\n- Runs quality checks per directory group concurrently (concurrency=10) via `runPool()` with phase label `'post-phase-1-quality'`\n- For each file: reads cached source from `sourceContentCache`, checks stale docs via `checkCodeVsDoc(source, oldSum)` (marks with `' (stale documentation)'` suffix), checks fresh docs via `checkCodeVsDoc(source, newSum)` from `readSumFile()`\n- Aggregates directory-scoped code-vs-code issues via `checkCodeVsCode(filesForCodeVsCode)` where `filesForCodeVsCode` is `Array<{path, content}>`\n- Clears `sourceContentCache` after validation to free memory\n- Builds `InconsistencyReport` via `buildInconsistencyReport()`, prints via `formatReportForCli()`, stores counts in `inconsistenciesCodeVsDoc`/`inconsistenciesCodeVsCode`\n- Non-throwing: catches errors and logs to stderr with `[quality]` prefix\n\n**Phase 2 (Directory Docs):**\n- Builds `knownDirs` Set from `plan.directoryTasks.map(t => t.path)` for prompt filtering\n- Groups directory tasks by depth via `Map<depth, tasks[]>` where depth from `task.metadata.depth`\n- Processes depth levels in descending order (deepest first) via `Array.from(dirsByDepth.keys()).sort((a,b) => b-a)` for post-order traversal\n- Per depth level: creates phase label `phase-2-dirs-depth-${depth}`, sets concurrency to `Math.min(options.concurrency, dirsAtDepth.length)`\n- Builds prompts via `buildDirectoryPrompt(absolutePath, projectRoot, debug, knownDirs, projectStructure)`, calls `aiService.call()`, writes via `writeAgentsMd()`\n- Updates tracker via `markDone('${path}/AGENTS.md')`, reports via `ProgressReporter.onDirectoryDone()`\n\n**Post-Phase 2 (Phantom Path Validation):**\n- Iterates `plan.directoryTasks`, reads `AGENTS.md` via `readFile(path.join(absolutePath, 'AGENTS.md'))`\n- Calls `checkPhantomPaths(agentsMdPath, content, projectRoot)` to extract/resolve path references\n- Builds phantom report via `buildInconsistencyReport()`, prints via `formatReportForCli()`, stores count in `phantomPathCount`\n- Non-throwing: catches errors and logs to stderr with `[quality]` prefix\n\n**Phase 3 (Root Documents):**\n- Executes `plan.rootTasks` sequentially (concurrency=1) with phase label `'phase-3-root'`\n- Builds prompt via `buildRootPrompt(projectRoot, debug)`, calls `aiService.call()` with `maxTurns: 1` (no tool use)\n- Strips conversational preamble: finds first `# ` header via `indexOf()`, checks if preceding text is preamble (no `#`, no `<!--`), slices from header start\n- Writes output via `writeFile(rootTask.outputPath)`, updates tracker/reporter\n- Emits `task:start`/`task:done` trace events with `workerId: 0` for sequential execution\n\n**Finalization:**\n- Flushes `planTracker` via `await planTracker.flush()` to ensure serialized writes complete\n- Builds `RunSummary` from `aiService.getSummary()` with counts: `filesProcessed`, `filesFailed`, `inconsistenciesCodeVsDoc`, `inconsistenciesCodeVsCode`, `phantomPaths`, `inconsistencyReport`\n- Calls `reporter.printSummary(summary)`, returns summary\n\n### executeUpdate() Orchestration\n\n**Phase 1 (File Analysis):**\n- Caches source in `updateSourceCache` Map keyed by relative path\n- Reads existing project plan from `${CONFIG_DIR}/GENERATION-PLAN.md` for bird's-eye context (fallback to undefined on error)\n- Reads existing `.sum` via `readSumFile()`, passes to `buildFilePrompt()` as `existingSum` for incremental update context\n- Computes hash via `computeContentHashFromString()`, writes `.sum` via `writeSumFile()`, writes annex if `## Annex References` detected\n- Executes via `runPool()` with phase label `'update-phase-1-files'`\n\n**Post-Phase 1 (Quality Validation):**\n- Groups files by directory, runs checks concurrently (concurrency=10) with phase label `'update-post-phase-1-quality'`\n- Skips stale documentation check (no `oldSumCache` in update path)\n- Checks fresh docs via `checkCodeVsDoc(source, newSum)`, aggregates code-vs-code via `checkCodeVsCode()`\n- Clears `updateSourceCache` after validation, builds/prints report if issues detected\n- Non-throwing error handling with `[quality]` prefix\n\n**No Phase 2/3:**\n- Update command regenerates only `.sum` files; parent update orchestrator handles affected `AGENTS.md` regeneration separately\n\n## Helper Functions\n\n### stripPreamble(responseText: string): string\n\n- **Pattern 1 (separator)**: Detects `\\n---\\n` within first 500 chars, returns content after separator+5 if non-empty\n- **Pattern 2 (bold header)**: Matches `/^[\\s\\S]{0,500}?(\\*\\*[A-Z])/`, strips preceding content if <300 chars, no `##` markers\n- Returns original text if no preamble detected\n\n### extractPurpose(responseText: string): string\n\n- Splits on newlines, skips empty lines, `#` headers, `---` separators\n- Skips lines starting with preamble prefixes: `'now i'`, `'perfect'`, `'based on'`, `'let me'`, `'here is'`, `'i\\'ll'`, `'i will'`, `'great'`, `'okay'`, `'sure'`, `'certainly'`, `'alright'` (case-insensitive)\n- Strips bold wrapper via `/^\\*\\*(.+)\\*\\*$/`, truncates to 120 chars with `'...'` suffix\n- Returns empty string if no valid purpose line found\n\n## Integration Points\n\n**Dependencies on generation module:**\n- Consumes `ExecutionPlan` from `executor.ts` with `fileTasks`, `directoryTasks`, `rootTasks`, `projectRoot`, `projectStructure`\n- Calls `buildFilePrompt()`, `buildDirectoryPrompt()`, `buildRootPrompt()` from `prompts/index.ts`\n- Calls `writeSumFile()`, `readSumFile()`, `writeAnnexFile()` from `writers/sum.ts`, `writeAgentsMd()` from `writers/agents-md.ts`\n- Calls `formatExecutionPlanAsMarkdown()` from `executor.ts` for plan tracker initialization\n\n**Dependencies on orchestration module:**\n- Calls `runPool()` from `pool.ts` with `PoolOptions` (concurrency, failFast, tracer, phaseLabel, taskLabels)\n- Instantiates `PlanTracker` from `plan-tracker.ts`, calls `initialize()`, `markDone()`, `flush()`\n- Instantiates `ProgressReporter` from `progress.ts`, calls `onFileStart/Done/Error`, `onDirectoryStart/Done`, `onRootDone`, `printSummary()`\n\n**Dependencies on quality module:**\n- Calls `checkCodeVsDoc(source, sum, path)`, `checkCodeVsCode(files)`, `checkPhantomPaths(path, content, root)` from `quality/index.ts`\n- Calls `buildInconsistencyReport(issues, metadata)`, `formatReportForCli(report)` from `quality/index.ts`\n\n**Dependencies on AIService:**\n- Calls `aiService.call({prompt, systemPrompt, taskLabel, maxTurns?})` returning `AIResponse`\n- Calls `aiService.addFilesReadToLastEntry([{path, sizeBytes}])` for telemetry tracking\n- Calls `aiService.getSummary()` returning `{totalCalls, totalInputTokens, totalOutputTokens, totalCacheReadTokens, totalCacheCreationTokens, errorCount, totalFilesRead, uniqueFilesRead}`\n- Calls `aiService.setTracer(tracer)` to wire trace events for subprocess/retry tracking\n\n**Dependencies on change-detection:**\n- Calls `computeContentHashFromString(content)` from `change-detection/index.ts` returning SHA-256 hex string\n- Consumes `FileChange[]` array in `executeUpdate()`\n\n## Trace Event Emission\n\n**Phase-level events:**\n- `phase:start` with `{phase, taskCount, concurrency}` for pre-phase-1-cache, phase-1-files, post-phase-1-quality, phase-2-dirs-depth-N, post-phase-2-phantom, phase-3-root, update-phase-1-files, update-post-phase-1-quality\n- `phase:end` with `{phase, durationMs, tasksCompleted, tasksFailed}` for same phase labels\n\n**Task-level events (Phase 3 only):**\n- `task:start` with `{taskLabel, phase: 'phase-3-root'}` before root document generation\n- `task:done` with `{workerId: 0, taskIndex, taskLabel, durationMs, success, error?, activeTasks: 0}` after root task completion or failure\n\n**Pool-level events (delegated to runPool):**\n- `worker:start/end`, `task:pickup/done` for Phase 1, Phase 2, quality checks (emitted by pool.ts)\n\n## Return Value Schema\n\n**RunSummary:**\n```typescript\n{\n  version: string,              // from getVersion()\n  filesProcessed: number,       // Successful Phase 1 file tasks\n  filesFailed: number,          // Failed Phase 1 file tasks\n  filesSkipped: number,         // Always 0 in runner (incremental logic in update orchestrator)\n  totalCalls: number,           // From aiService.getSummary()\n  totalInputTokens: number,\n  totalOutputTokens: number,\n  totalCacheReadTokens: number,\n  totalCacheCreationTokens: number,\n  totalDurationMs: number,      // Elapsed time from run start\n  errorCount: number,\n  retryCount: number,           // Always 0 (retry tracking in AIService)\n  totalFilesRead: number,\n  uniqueFilesRead: number,\n  inconsistenciesCodeVsDoc?: number,\n  inconsistenciesCodeVsCode?: number,\n  phantomPaths?: number,\n  inconsistencyReport?: InconsistencyReport,\n}\n```\n\n## Error Handling Strategy\n\n- Pool execution with `failFast: options.failFast` controls early termination on task failures\n- Quality validation failures logged to stderr with `[quality]` prefix but never throw (non-blocking)\n- Phase 3 root task failures re-throw after emitting `task:done` event with `success: false`\n- Missing project plan in `executeUpdate()` caught silently (proceeds with undefined `projectPlan`)\n\n## Resource Management\n\n- `sourceContentCache`/`updateSourceCache` Maps explicitly cleared via `clear()` after quality validation to free memory\n- Concurrent I/O throttled via pool concurrency limits: 20 for sum reads, 10 for quality checks, `options.concurrency` for AI calls\n- Cached content reused across phases: source read once in Phase 1, consumed by quality checks (avoids redundant `readFile()`)\n### trace.ts\n**Purpose:** trace.ts implements NDJSON concurrency tracing for task/subprocess lifecycle debugging via promise-chain serializatio...\n\n**trace.ts implements NDJSON concurrency tracing for task/subprocess lifecycle debugging via promise-chain serialization, emitting events to `.agents-reverse-engineer/traces/` with automatic base field population and zero-overhead no-op mode.**\n\n## Exported Interfaces\n\n`ITraceWriter` defines the public tracing API with three members: `emit(event: TraceEventPayload)` appends a trace event with auto-populated `seq`, `ts`, `pid`, `elapsedMs` fields; `finalize(): Promise<void>` flushes pending writes and closes the file handle; `readonly filePath: string` returns the absolute path to the trace file (empty string for `NullTraceWriter`).\n\n`TraceEvent` is a discriminated union of 15 event types: `PhaseStartEvent` (`type: 'phase:start'`, `phase: string`, `taskCount: number`, `concurrency: number`), `PhaseEndEvent` (`type: 'phase:end'`, `phase: string`, `durationMs: number`, `tasksCompleted: number`, `tasksFailed: number`), `WorkerStartEvent` (`type: 'worker:start'`, `workerId: number`, `phase: string`), `WorkerEndEvent` (`type: 'worker:end'`, `workerId: number`, `phase: string`, `tasksExecuted: number`), `TaskPickupEvent` (`type: 'task:pickup'`, `workerId: number`, `taskIndex: number`, `taskLabel: string`, `activeTasks: number`), `TaskDoneEvent` (`type: 'task:done'`, `workerId: number`, `taskIndex: number`, `taskLabel: string`, `durationMs: number`, `success: boolean`, `error?: string`, `activeTasks: number`), `TaskStartEvent` (`type: 'task:start'`, `taskLabel: string`, `phase: string`), `SubprocessSpawnEvent` (`type: 'subprocess:spawn'`, `childPid: number`, `command: string`, `taskLabel: string`), `SubprocessExitEvent` (`type: 'subprocess:exit'`, `childPid: number`, `command: string`, `taskLabel: string`, `exitCode: number`, `signal: string | null`, `durationMs: number`, `timedOut: boolean`), `RetryEvent` (`type: 'retry'`, `attempt: number`, `taskLabel: string`, `errorCode: string`), `DiscoveryStartEvent` (`type: 'discovery:start'`, `targetPath: string`), `DiscoveryEndEvent` (`type: 'discovery:end'`, `filesIncluded: number`, `filesExcluded: number`, `durationMs: number`), `FilterAppliedEvent` (`type: 'filter:applied'`, `filterName: string`, `filesMatched: number`, `filesRejected: number`), `PlanCreatedEvent` (`type: 'plan:created'`, `planType: 'generate' | 'update'`, `fileCount: number`, `taskCount: number`), `ConfigLoadedEvent` (`type: 'config:loaded'`, `configPath: string`, `model: string`, `concurrency: number`).\n\n`TraceEventPayload` is a distributive omit type removing auto-populated base keys (`seq`, `ts`, `pid`, `elapsedMs`) from `TraceEvent`, computed via `DistributiveOmit<TraceEvent, BaseKeys>` where `DistributiveOmit<T, K extends PropertyKey> = T extends unknown ? Omit<T, K> : never` ensures correct union distribution.\n\n`TraceEventBase` defines common fields present on all events: `seq: number` (monotonically increasing sequence number per-run), `ts: string` (ISO 8601 timestamp at event creation time), `pid: number` (Node.js parent process PID), `elapsedMs: number` (high-resolution elapsed time since run start in fractional milliseconds).\n\n## Factory Function\n\n`createTraceWriter(projectRoot: string, enabled: boolean): ITraceWriter` returns `NullTraceWriter` when `enabled` is false (zero overhead), otherwise returns `TraceWriter` that appends NDJSON to `.agents-reverse-engineer/traces/trace-{timestamp}.ndjson` where timestamp uses `new Date().toISOString().replace(/[:.]/g, '-')` for filesystem-safe naming.\n\n## Implementation Classes\n\n`NullTraceWriter` implements `ITraceWriter` with no-op methods: `filePath = ''`, `emit()` is empty, `finalize()` returns resolved promise. Used when `--trace` flag is absent to eliminate runtime overhead.\n\n`TraceWriter` implements `ITraceWriter` with promise-chain serialization mirroring `PlanTracker` pattern from `src/orchestration/plan-tracker.ts`. Private fields: `seq: number` (starts at 0), `nodePid` (captured from `process.pid`), `startHr` (captured via `process.hrtime.bigint()`), `writeQueue: Promise<void>` (serialization chain initialized to `Promise.resolve()`), `fd: FileHandle | null` (opened lazily on first `emit()` call). Constructor accepts `filePath: string` as absolute path parameter.\n\n`TraceWriter.emit(partial: TraceEventPayload)` constructs full event by spreading `partial` and adding `seq: this.seq++`, `ts: new Date().toISOString()`, `pid: this.nodePid`, `elapsedMs: Number(process.hrtime.bigint() - this.startHr) / 1_000_000`. Serializes to JSON line via `JSON.stringify(event) + '\\n'`. Chains write operation onto `this.writeQueue` via `.then()` that lazily opens file handle using `open(this.filePath, 'a')` after `mkdir(path.dirname(this.filePath), { recursive: true })`, then calls `this.fd.write(line)`. Catches errors silently since trace loss is acceptable for debugging infrastructure.\n\n`TraceWriter.finalize()` awaits `this.writeQueue` to flush all pending writes, then closes `this.fd` via `this.fd.close()` if open and sets `this.fd = null`.\n\n## Cleanup Utility\n\n`cleanupOldTraces(projectRoot: string, keepCount: number = 500): Promise<number>` removes old trace files from `.agents-reverse-engineer/traces/`, keeping only the most recent `keepCount` files. Mirrors pattern from `src/ai/telemetry/cleanup.ts`. Reads directory via `readdir()`, filters for entries matching `startsWith('trace-')` and `endsWith('.ndjson')`, sorts lexicographically (ISO timestamps), reverses to newest-first ordering, slices entries beyond `keepCount`, deletes via `unlink()`, returns deletion count. Returns 0 on `ENOENT` (directory not found), re-throws other errors.\n\n## Constants\n\n`TRACES_DIR = '.agents-reverse-engineer/traces'` defines relative directory path for trace file storage.\n\n## Integration Pattern\n\nThreading occurs via `CommandRunOptions.tracer` parameter passed through pool workers (`src/orchestration/pool.ts`), AI service calls (`src/ai/service.ts`), and phase runners (`src/orchestration/runner.ts`). Call sites unconditionally invoke `emit()` without branching, relying on `NullTraceWriter` to optimize away overhead when tracing is disabled.\n### types.ts\n**Purpose:** types.ts defines TypeScript interfaces for orchestration module contracts: task results from AI subprocess execution,...\n\n**types.ts defines TypeScript interfaces for orchestration module contracts: task results from AI subprocess execution, aggregated run summaries with token/cost metrics, progress events for streaming reporter updates, and command execution options threading concurrency/debug/trace configuration across the three-phase pipeline.**\n\n## Exported Interfaces\n\n### FileTaskResult\nPer-file AI analysis outcome returned by command runner to pool coordinator.\n\n```typescript\ninterface FileTaskResult {\n  path: string;                    // Relative source file path\n  success: boolean;                // AI subprocess succeeded\n  tokensIn: number;                // Non-cached input tokens consumed\n  tokensOut: number;               // Generated output tokens\n  cacheReadTokens: number;         // Cache-hit input tokens\n  cacheCreationTokens: number;     // Cache-write input tokens\n  durationMs: number;              // Wall-clock execution time\n  model: string;                   // AI model identifier used\n  error?: string;                  // Failure message if success=false\n}\n```\n\nProduced by `src/orchestration/runner.ts` for each completed file task, aggregated into `RunSummary` for telemetry logging.\n\n### RunSummary\nCommand-level aggregation of all `FileTaskResult` instances plus quality metrics.\n\n```typescript\ninterface RunSummary {\n  version: string;                      // ARE version (from src/version.ts)\n  filesProcessed: number;               // Success count\n  filesFailed: number;                  // Error count\n  filesSkipped: number;                 // Dry-run count\n  totalCalls: number;                   // AI subprocess invocations\n  totalInputTokens: number;             // Sum of tokensIn across calls\n  totalOutputTokens: number;            // Sum of tokensOut\n  totalCacheReadTokens: number;         // Sum of cacheReadTokens\n  totalCacheCreationTokens: number;     // Sum of cacheCreationTokens\n  totalDurationMs: number;              // Wall-clock total\n  errorCount: number;                   // Failed tasks\n  retryCount: number;                   // Exponential backoff retry attempts\n  totalFilesRead: number;               // Sum of filesRead[] lengths\n  uniqueFilesRead: number;              // Deduped count via Set<path>\n  inconsistenciesCodeVsDoc?: number;    // Code-vs-doc validator findings\n  inconsistenciesCodeVsCode?: number;   // Duplicate export detector findings\n  phantomPaths?: number;                // Unresolved path references in AGENTS.md\n  inconsistencyReport?: InconsistencyReport; // Full quality report from src/quality/\n}\n```\n\nWritten to `.agents-reverse-engineer/logs/run-<timestamp>.json` by `src/ai/telemetry/logger.ts`. Optional quality fields populated only when validators execute post-generation.\n\n### ProgressEvent\nDiscriminated union for streaming progress updates from runner to `ProgressLog`.\n\n```typescript\ninterface ProgressEvent {\n  type: 'start' | 'done' | 'error' | 'dir-done' | 'root-done';\n  filePath: string;                // File or directory path\n  index: number;                   // Zero-based task position in phase\n  total: number;                   // Total tasks in current phase\n  durationMs?: number;             // Wall-clock time (type='done')\n  tokensIn?: number;               // Input tokens (type='done')\n  tokensOut?: number;              // Output tokens (type='done')\n  model?: string;                  // Model identifier (type='done')\n  error?: string;                  // Error message (type='error')\n}\n```\n\nEvent types:\n- `start` — Task picked up by worker (requires `filePath`, `index`, `total`)\n- `done` — Task completed successfully (adds `durationMs`, `tokensIn`, `tokensOut`, `model`)\n- `error` — Task failed (adds `error` message)\n- `dir-done` — Directory `AGENTS.md` written (Phase 2 completion marker)\n- `root-done` — Root document written (`CLAUDE.md`/`GEMINI.md`/`OPENCODE.md`, Phase 3 marker)\n\nConsumed by `src/orchestration/progress.ts` for console spinner updates and ETA calculation (moving average of last 10 `durationMs` values).\n\n### CommandRunOptions\nConfiguration threading object passed from CLI → orchestrator → pool → AIService.\n\n```typescript\ninterface CommandRunOptions {\n  concurrency: number;            // Worker pool size (1-10, default 2 WSL/5 other)\n  failFast?: boolean;             // Abort on first task failure\n  debug?: boolean;                // Log exact prompts + subprocess heap/RSS metrics\n  dryRun?: boolean;               // Preview without file writes\n  tracer?: ITraceWriter;          // NDJSON trace emitter (NullTraceWriter when --trace off)\n  progressLog?: ProgressLog;      // File-based progress mirror for tail -f monitoring\n}\n```\n\n`tracer` field populated by `src/orchestration/trace.ts:TraceWriter` when `--trace` flag active, otherwise `NullTraceWriter` (no-op). `progressLog` initialized by `src/cli/generate.ts` and `src/cli/update.ts` to mirror console output to `.agents-reverse-engineer/progress.log`.\n\n## Integration Points\n\n- **FileTaskResult**: Produced by `src/orchestration/runner.ts:runFileAnalysis()`, consumed by pool workers in `src/orchestration/pool.ts:runPool()` for aggregation\n- **RunSummary**: Built by `src/generation/orchestrator.ts:generateDocs()` and `src/update/orchestrator.ts:runUpdate()`, passed to `src/ai/telemetry/logger.ts:logRun()`\n- **ProgressEvent**: Emitted by runner via `emit(event)` callback, handled by `src/orchestration/progress.ts:ProgressReporter.update()` for spinner/ETA/file-log writes\n- **CommandRunOptions**: Merged from config defaults (`src/config/schema.ts`) + CLI flag overrides (`src/cli/generate.ts`, `src/cli/update.ts`) before passing to orchestrator\n- **InconsistencyReport** (type dependency): Imported from `src/quality/index.ts`, contains discriminated union of `CodeDocInconsistency | CodeCodeInconsistency | PhantomPathInconsistency` with metadata (timestamp, projectRoot, filesChecked, durationMs)\n- **ITraceWriter** (type dependency): Imported from `src/orchestration/trace.ts`, interface with `trace(event)` method for NDJSON event emission\n- **ProgressLog** (type dependency): Imported from `src/orchestration/progress.ts`, interface with `write(msg)` method for `.agents-reverse-engineer/progress.log` writes\n\n## Import Map (verified — use these exact paths)\n\nplan-tracker.ts:\n  ../config/loader.js → CONFIG_DIR\n\nrunner.ts:\n  ../ai/index.js → AIService (type)\n  ../ai/types.js → AIResponse (type)\n  ../generation/executor.js → ExecutionPlan, ExecutionTask (type)\n  ../generation/writers/sum.js → writeSumFile, readSumFile, writeAnnexFile\n  ../generation/writers/sum.js → SumFileContent (type)\n  ../generation/writers/agents-md.js → writeAgentsMd\n  ../change-detection/index.js → computeContentHashFromString\n  ../change-detection/types.js → FileChange (type)\n  ../generation/prompts/index.js → buildFilePrompt, buildDirectoryPrompt, buildRootPrompt\n  ../config/schema.js → Config (type)\n  ../config/loader.js → CONFIG_DIR\n  ../quality/index.js → checkCodeVsDoc, checkCodeVsCode, checkPhantomPaths, buildInconsistencyReport, formatReportForCli\n  ../quality/index.js → Inconsistency (type)\n  ../generation/executor.js → formatExecutionPlanAsMarkdown\n  ../version.js → getVersion\n\ntypes.ts:\n  ../quality/index.js → InconsistencyReport (type)\n\n## Project Directory Structure\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\n\n## Annex Files (reproduction-critical constants)\n\n- runner.ts.annex.md",
      "systemPrompt": "You are generating an AGENTS.md file — a directory-level overview for AI coding assistants.\n\nCRITICAL: Output ONLY the raw markdown content. No code fences, no preamble, no explanations, no conversational text. Your entire response IS the AGENTS.md file content.\n\nOUTPUT FORMAT:\n- First line MUST be exactly: <!-- Generated by agents-reverse-engineer -->\n- Use a # heading with the directory name\n- Write a one-paragraph purpose statement for the directory\nusage\nADAPTIVE SECTIONS:\nAnalyze the directory contents and choose the most relevant sections. Do NOT use a fixed template. Instead, select sections that best document this specific directory for an AI that needs to reconstruct or extend the project.\n\nConsider these section types (choose what applies):\n- **Contents**: Group files by purpose/category under ## headings. For each file: markdown link [filename](./filename) and a one-line description.\n- **Subdirectories**: If subdirectories exist, list them with links [dirname/](./dirname/) and brief summaries.\n- **Architecture / Data Flow**: If files form a pipeline, request/response chain, or layered architecture, document it.\n- **Stack**: If this is a package root (has package.json, Cargo.toml, go.mod, etc.), document the technology stack, key scripts, and entry points.\n- **Structure**: If the directory layout follows a convention (feature-sliced, domain-driven, MVC, etc.), document it.\n- **Patterns**: If files share recurring design patterns (factory, strategy, middleware, barrel re-export), name and document them.\n- **Configuration**: If the directory contains config files, schemas, or environment definitions, document the config surface area.\n- **API Surface**: If the directory exports a public API (barrel index, route definitions, SDK), document the interface contract.\n- **File Relationships**: How files collaborate, depend on each other, or share state.\n- **Behavioral Contracts**: If files contain regex patterns, format specifications, magic constants, or template strings that define observable behavior, collect them in a dedicated section. Preserve verbatim patterns from file summaries — do NOT paraphrase regex into prose. This section is MANDATORY when file summaries contain behavioral artifacts.\n- **Reproduction-Critical Constants**: If file summaries reference annex files (via ## Annex References sections), list them with links. Example: \"Full prompt template text: [templates.ts.annex.md](./prompts/templates.ts.annex.md)\". Do NOT reproduce annex content in AGENTS.md — just link to it.\n\nChoose any relevant sections or create your own based on the directory contents. The goal is to provide a comprehensive overview that captures the essence of the directory's role in the project and how its files work together, with a focus on what an AI coding assistant would need to know to effectively interact with this code.\n\nSCOPE:\n- AGENTS.md is a NAVIGATIONAL INDEX — help an AI find the right file quickly\n- Focus on: what each file does, how files relate, directory-level patterns\n- Do NOT reproduce full architecture sections — those belong in the root CLAUDE.md\n\nPATH ACCURACY (MANDATORY):\n- When referencing files or modules outside this directory, use ONLY paths from the \"Import Map\" section\n- Do NOT invent, rename, or guess module paths — if a path isn't in the Import Map, don't reference it\n- Use the exact directory names from \"Project Directory Structure\" — do NOT rename directories\n  (e.g., if the directory is called \"cli\", write \"src/cli/\", NOT \"src/commands/\")\n- Cross-module references must use the specifier format from actual import statements\n  (e.g., \"../generation/writers/sum.js\", NOT \"../fs/sum-file.js\")\n- If you are unsure about a path, omit the cross-reference rather than guessing\n\nCONSISTENCY (MANDATORY):\n- Do not contradict yourself within the same document\n- If you describe a technique (e.g., \"regex-based\"), do not call it something else later (e.g., \"AST-based\")\n- When stating version numbers, engines, or config fields, use ONLY values present in the file summaries\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this directory\", \"this module\", \"provides\", \"responsible for\", \"is used to\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Per-file descriptions in Contents sections: 1-2 sentences maximum. Reference key symbols but do not reproduce full summaries.\n- Behavioral contracts (regex patterns, format specs, constants) belong in a separate Behavioral Contracts section, not in per-file descriptions.\n- Annex file references: link to .annex.md files, do not inline their content. One line per annex reference.\n- Subdirectory descriptions: 1-2 sentences maximum. Capture the directory's role, not its full contents.\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- Key exported symbols from file summaries MUST appear in the directory overview\n- Preserve exact casing of identifiers\n\nUSER NOTES:\n- If \"User Notes\" are provided in the prompt, they contain user-defined instructions that will be automatically prepended to your output\n- Do NOT repeat or paraphrase user notes in your generated content — they are included separately\n- You may reference information from user notes for context",
      "response": "<!-- Generated by agents-reverse-engineer -->\n\n# src/orchestration\n\n**Worker pool concurrency control, promise-chain serialized progress tracking, NDJSON trace emission, and three-phase pipeline orchestration executing concurrent file analysis, post-order directory aggregation, and sequential root synthesis with integrated quality validation, ETA calculation, and subprocess lifecycle tracing.**\n\n## Contents\n\n### Core Orchestration\n\n**[index.ts](./index.ts)** — Barrel export consolidating `runPool`, `ProgressReporter`, `ProgressLog`, `PlanTracker`, `createTraceWriter`, `cleanupOldTraces`, `CommandRunner`, and shared types (`FileTaskResult`, `RunSummary`, `ProgressEvent`, `CommandRunOptions`, `PoolOptions`, `TaskResult`, `ITraceWriter`, `TraceEvent`, `TraceEventPayload`).\n\n**[pool.ts](./pool.ts)** — Iterator-based worker pool executing `Array<() => Promise<T>>` tasks via shared `tasks.entries()` iterator consumed by N workers, preventing batch-stall anti-pattern where `Promise.all()` chunks idle workers waiting for slowest task. Emits `worker:start/end`, `task:pickup/done` trace events. Supports `failFast` via shared `aborted` boolean flag checked at loop start. Effective concurrency via `Math.min(options.concurrency, tasks.length)`.\n\n**[runner.ts](./runner.ts)** — `CommandRunner` orchestrates three-phase pipeline via `executeGenerate(plan)` and `executeUpdate(filesToAnalyze, projectRoot, config)`. Phase 1: concurrent file analysis via `runPool()` with `options.concurrency`, reads source via `readFile()`, caches in `sourceContentCache`, calls `aiService.call()`, computes `contentHash` via `computeContentHashFromString()`, strips preamble via `stripPreamble()`, extracts purpose via `extractPurpose()`, writes via `writeSumFile()`, detects `## Annex References` marker for `writeAnnexFile()`. Post-Phase 1: groups files by directory, runs quality checks concurrently (concurrency=10) via `checkCodeVsDoc(source, oldSum)` (stale), `checkCodeVsDoc(source, newSum)` (fresh), `checkCodeVsCode(filesForCodeVsCode)`, builds `InconsistencyReport`, clears `sourceContentCache`. Phase 2: groups `directoryTasks` by depth, processes in descending order (deepest first) with concurrency `Math.min(options.concurrency, dirsAtDepth.length)`, builds prompts via `buildDirectoryPrompt()`, writes via `writeAgentsMd()`. Post-Phase 2: validates phantom paths via `checkPhantomPaths(agentsMdPath, content, projectRoot)`. Phase 3: sequential root document synthesis (concurrency=1), strips conversational preamble before `writeFile()`. Returns `RunSummary` with counts from `aiService.getSummary()` plus `inconsistenciesCodeVsDoc`, `inconsistenciesCodeVsCode`, `phantomPaths`.\n\n### Progress Tracking\n\n**[progress.ts](./progress.ts)** — `ProgressReporter` streams colored console output with ETA calculation via moving average of last 10 `completionTimes` (window size 10), displays after 2+ completions. `ProgressLog` mirrors plain-text output to `.agents-reverse-engineer/progress.log` via promise-chain serialized writes with ANSI stripping (`/\\x1b\\[[0-9;]*m/g`). Static factory `ProgressLog.create(projectRoot)` constructs path. Methods: `onFileStart(filePath)` (`[X/Y] ANALYZING path`), `onFileDone(filePath, durationMs, tokensIn, tokensOut, model, cacheReadTokens?, cacheCreationTokens?)` (`[X/Y] DONE path Xs in/out tok model ~Ns remaining`), `onFileError(filePath, error)` (`[X/Y] FAIL path error`), `onDirectoryStart(dirPath)`, `onDirectoryDone()`, `onRootDone(docPath)`, `printSummary(summary)`. ETA formatted via `formatETA()` computing `avg * remaining` as `~Ns remaining` (<60s) or `~Mm Ss remaining` (≥60s).\n\n**[plan-tracker.ts](./plan-tracker.ts)** — `PlanTracker` maintains in-memory `GENERATION-PLAN.md` content, serializes concurrent checkbox updates via promise-chain writes. Constructor accepts `projectRoot` and `initialMarkdown`, computes `planPath` as `path.join(projectRoot, CONFIG_DIR, 'GENERATION-PLAN.md')`. `markDone(itemPath)` replaces `` `- [ ] \\`${itemPath}\\`` `` → `` `- [x] \\`${itemPath}\\`` ``, chains write to `writeQueue` promise. `initialize()` creates parent directory via `mkdir(..., {recursive: true})`, writes initial content. `flush()` awaits `writeQueue` completion.\n\n### Trace Infrastructure\n\n**[trace.ts](./trace.ts)** — `createTraceWriter(projectRoot, enabled)` factory returns `NullTraceWriter` (zero overhead) when disabled or `TraceWriter` appending NDJSON to `.agents-reverse-engineer/traces/trace-{timestamp}.ndjson` with promise-chain serialization. `TraceWriter` auto-populates `seq` (monotonic), `ts` (ISO 8601), `pid` (`process.pid`), `elapsedMs` (high-resolution `process.hrtime.bigint()` delta) on `emit(partial: TraceEventPayload)`. `finalize()` awaits `writeQueue`, closes `fd`. `cleanupOldTraces(projectRoot, keepCount=500)` removes old traces, keeps most recent 500.\n\n**Event types:** `phase:start/end` (phase, taskCount, concurrency, durationMs, tasksCompleted, tasksFailed), `worker:start/end` (workerId, phase, tasksExecuted), `task:pickup/done` (workerId, taskIndex, taskLabel, activeTasks, durationMs, success, error?), `task:start` (taskLabel, phase), `subprocess:spawn/exit` (childPid, command, taskLabel, exitCode, signal, durationMs, timedOut), `retry` (attempt, taskLabel, errorCode), `discovery:start/end` (targetPath, filesIncluded, filesExcluded, durationMs), `filter:applied` (filterName, filesMatched, filesRejected), `plan:created` (planType, fileCount, taskCount), `config:loaded` (configPath, model, concurrency).\n\n**[types.ts](./types.ts)** — Defines `FileTaskResult` (path, success, tokensIn, tokensOut, cacheReadTokens, cacheCreationTokens, durationMs, model, error?), `RunSummary` (version, filesProcessed, filesFailed, filesSkipped, totalCalls, totalInputTokens, totalOutputTokens, totalCacheReadTokens, totalCacheCreationTokens, totalDurationMs, errorCount, retryCount, totalFilesRead, uniqueFilesRead, inconsistenciesCodeVsDoc?, inconsistenciesCodeVsCode?, phantomPaths?, inconsistencyReport?), `ProgressEvent` (discriminated type: start|done|error|dir-done|root-done with filePath, index, total, durationMs?, tokensIn?, tokensOut?, model?, error?), `CommandRunOptions` (concurrency, failFast?, debug?, dryRun?, tracer?: ITraceWriter, progressLog?: ProgressLog), `PoolOptions` (concurrency, failFast?, tracer?, phaseLabel?, taskLabels?), `TaskResult<T>` (index, success, value?, error?).\n\n## Architecture\n\n### Shared-Iterator Concurrency\n\n`runPool()` creates `tasks.entries()` iterator shared across N workers via `for (const [index, task] of iterator)` loop. Workers race to pull tasks atomically via iterator protocol, preventing over-allocation where `Promise.all()` chunks spawn tasks eagerly. Effective concurrency via `Math.min(options.concurrency, tasks.length)` prevents idle workers when task count < pool size.\n\n### Promise-Chain Serialization\n\n`PlanTracker`, `ProgressLog`, `TraceWriter` serialize concurrent writes via `this.writeQueue = this.writeQueue.then(() => writeOp())` pattern. Each write operation chains onto tail of promise, forming sequential execution queue despite concurrent worker invocations. Errors caught via `.catch()` suppression (non-critical telemetry).\n\n### Three-Phase Pipeline\n\nPhase 1: concurrent file analysis via `runPool(fileTasks, {concurrency: options.concurrency})`, reads source via `readFile()` once, caches in `sourceContentCache` Map, calls `aiService.call()`, writes `.sum` via `writeSumFile()`, writes `.annex.md` if `## Annex References` detected. Post-Phase 1: groups files by directory, validates via `checkCodeVsDoc(source, oldSum)` (stale docs), `checkCodeVsDoc(source, newSum)` (LLM omissions), `checkCodeVsCode(files)` (duplicate exports) at concurrency=10, clears `sourceContentCache`.\n\nPhase 2: groups `directoryTasks` by depth (`task.metadata.depth`), processes depth levels sequentially in descending order (deepest first) via `Array.from(dirsByDepth.keys()).sort((a,b) => b-a)`, executes depth level concurrently with `Math.min(concurrency, dirsAtDepth.length)`, builds prompts via `buildDirectoryPrompt()`, writes via `writeAgentsMd()`. Post-Phase 2: validates phantom paths via `checkPhantomPaths()` extracting path-like strings via three regex patterns, resolving against directory and project root.\n\nPhase 3: sequential root synthesis (concurrency=1), builds prompts via `buildRootPrompt()`, calls `aiService.call()` with `maxTurns: 1`, strips conversational preamble before `writeFile()`.\n\n### Quality Validation Timing\n\nPre-Phase 1: reads existing `.sum` files concurrently (concurrency=20) into `oldSumCache` Map. Post-Phase 1: compares `oldSum` vs. source (stale documentation from previous runs), compares `newSum` vs. source (LLM omissions in current run), aggregates duplicate exports. Post-Phase 2: validates AGENTS.md path references. Validation failures logged to stderr with `[quality]` prefix, never throw (non-blocking).\n\n### Trace Event Threading\n\n`CommandRunOptions.tracer` threaded through pool → AIService → runner. Phase-level `phase:start/end` emitted by runner. Pool-level `worker:start/end`, `task:pickup/done` emitted by `runPool()`. Subprocess-level `subprocess:spawn/exit`, `retry` emitted by `AIService`. Zero overhead when `tracer` is `NullTraceWriter`.\n\n### ETA Calculation\n\n`ProgressReporter` maintains sliding windows `completionTimes` and `dirCompletionTimes` (max size 10). `formatETA()` computes `avg = sum(completionTimes) / completionTimes.length`, `remaining = totalFiles - completed - failed`, `etaMs = avg * remaining`. Displays after 2+ completions to avoid inaccurate early estimates. Formats as `~Ns remaining` (<60s) or `~Mm Ss remaining` (≥60s).\n\n## Behavioral Contracts\n\n### Preamble Detection Patterns (from runner.ts)\n\nFull patterns preserved in [runner.ts.annex.md](./runner.ts.annex.md).\n\n**Separator pattern:** Detects `\\n---\\n` within first 500 chars.\n\n**Bold header pattern:** Matches `/^[\\s\\S]{0,500}?(\\*\\*[A-Z])/`, strips preceding content if <300 chars, no `##` markers.\n\n**Preamble prefixes (case-insensitive):** `'now i'`, `'perfect'`, `'based on'`, `'let me'`, `'here is'`, `'i\\'ll'`, `'i will'`, `'great'`, `'okay'`, `'sure'`, `'certainly'`, `'alright'`.\n\n### Checkbox Update Pattern (from plan-tracker.ts)\n\nReplaces `` `- [ ] \\`${itemPath}\\`` `` → `` `- [x] \\`${itemPath}\\`` `` via string substitution. Item path conventions: files use relative paths (`src/cli/init.ts`), directories append `/AGENTS.md` suffix (`src/cli/AGENTS.md`), root documents use filename only (`CLAUDE.md`).\n\n### ANSI Stripping Regex (from progress.ts)\n\n`/\\x1b\\[[0-9;]*m/g` matches all SGR, cursor, and erase sequences before writing to `.agents-reverse-engineer/progress.log`.\n\n### Trace Filename Format (from trace.ts)\n\n`trace-{timestamp}.ndjson` where timestamp from `new Date().toISOString().replace(/[:.]/g, '-')` (e.g., `trace-2026-02-09T12-34-56-789Z.ndjson`).\n\n## File Relationships\n\n`CommandRunner` orchestrates via dependencies: `runPool()` for worker execution, `PlanTracker` for checkbox updates, `ProgressReporter` for console output, `ProgressLog` for file mirroring, `TraceWriter` for NDJSON events, `AIService` for subprocess calls, `buildFilePrompt/buildDirectoryPrompt/buildRootPrompt` from generation module, `writeSumFile/readSumFile/writeAnnexFile/writeAgentsMd` from writers, `checkCodeVsDoc/checkCodeVsCode/checkPhantomPaths` from quality module, `computeContentHashFromString` from change-detection.\n\n`runPool()` invokes task factories, emits trace events via `tracer?.emit()`, invokes `onComplete` callback per task settlement, returns `TaskResult[]` array.\n\n`ProgressReporter` receives events via `onFileStart/Done/Error/DirectoryStart/Done/RootDone`, computes ETA via sliding windows, delegates to `ProgressLog.write()` for file mirroring, prints summary via `printSummary(RunSummary)`.\n\n`PlanTracker` initialized with `formatExecutionPlanAsMarkdown()` output, updated via `markDone(itemPath)` from pool workers, flushed via `flush()` before orchestrator return.\n\n`TraceWriter` emits events via `emit(partial)`, auto-populates base fields (`seq`, `ts`, `pid`, `elapsedMs`), serializes writes via promise chain, finalized via `finalize()` after all phases complete.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 3652,
      "cacheReadTokens": 17587,
      "cacheCreationTokens": 21779,
      "latencyMs": 90371,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": []
    },
    {
      "timestamp": "2026-02-09T17:49:18.201Z",
      "prompt": "Generate AGENTS.md for directory: \"docs\" (docs)\n\n## File Summaries (1 files)\n\n### INPUT.md\n**Purpose:** INPUT.md documents the original project vision and Recursive Language Model (RLM) algorithm specification that define...\n\n**INPUT.md documents the original project vision and Recursive Language Model (RLM) algorithm specification that defines agents-reverse-engineer's three-phase brownfield documentation strategy.**\n\n## Vision Statement\n\nDescribes agents-reverse-engineer as a lightweight tool for generating `AGENTS.md`, `CLAUDE.md`, and platform-specific documentation from brownfield codebases. Compatible with Claude Code, OpenCode, and other agent tools. Created to provide agents with clear codebase overviews for long-term maintenance.\n\n## Inspirational Projects\n\nReferences three complementary tools for integration context:\n- **SpecKit**: https://github.com/github/spec-kit — GitHub's specification toolkit\n- **BMAD**: https://github.com/bmad-code-org/BMAD-METHOD — BMAD methodology framework\n- **Get Shit Done (GSD)**: https://github.com/glittercowboy/get-shit-done — GSD workflow system\n\nInstructs detailed analysis of GSD and BMAD approaches to brownfield projects, repository structure patterns, special commands, and implementation details via codebase inspection.\n\n## Feature Requirements\n\n**Command execution:**\n- Implement RLM algorithm via Claude Code commands or platform-specific alternatives for other LLM agent tools\n\n**Session lifecycle:**\n- Hook mechanism on session end to update impacted files automatically\n\n**Directory documentation:**\n- Generate `AGENTS.md` in every project directory describing current directory content and substructure\n- Support references to specialized documentation: `ARCHITECTURE.md`, `STRUCTURE.md`, `STACK.md`, `INTEGRATIONS.md`, `INFRASTRUCTURE.md`, `CONVENTIONS.md`, `TESTING.md`, `PATTERNS.md`, `CONCERNS.md`\n\n## RLM Algorithm Specification\n\n**Execution flow:**\n1. Build project structure tree via directory traversal\n2. Execute analysis starting at first leaf node, proceeding recursively backward (post-order traversal)\n3. **File-level analysis**: Generate `{filename}.sum` summary file for each source file leaf\n4. **Directory-level synthesis**: When all leaves in directory are summarized, analyze directory and generate `AGENTS.md` plus additional referenced documentation files if needed\n5. **Root convergence**: Continue recursive aggregation until project root is reached\n\n## CLI Commands\n\nDefines two primary user-facing commands:\n- `/are-generate` — Full documentation generation from scratch\n- `/are-update` — Incremental update of impacted files\n\n## Project Directory Structure\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>",
      "systemPrompt": "You are generating an AGENTS.md file — a directory-level overview for AI coding assistants.\n\nCRITICAL: Output ONLY the raw markdown content. No code fences, no preamble, no explanations, no conversational text. Your entire response IS the AGENTS.md file content.\n\nOUTPUT FORMAT:\n- First line MUST be exactly: <!-- Generated by agents-reverse-engineer -->\n- Use a # heading with the directory name\n- Write a one-paragraph purpose statement for the directory\nusage\nADAPTIVE SECTIONS:\nAnalyze the directory contents and choose the most relevant sections. Do NOT use a fixed template. Instead, select sections that best document this specific directory for an AI that needs to reconstruct or extend the project.\n\nConsider these section types (choose what applies):\n- **Contents**: Group files by purpose/category under ## headings. For each file: markdown link [filename](./filename) and a one-line description.\n- **Subdirectories**: If subdirectories exist, list them with links [dirname/](./dirname/) and brief summaries.\n- **Architecture / Data Flow**: If files form a pipeline, request/response chain, or layered architecture, document it.\n- **Stack**: If this is a package root (has package.json, Cargo.toml, go.mod, etc.), document the technology stack, key scripts, and entry points.\n- **Structure**: If the directory layout follows a convention (feature-sliced, domain-driven, MVC, etc.), document it.\n- **Patterns**: If files share recurring design patterns (factory, strategy, middleware, barrel re-export), name and document them.\n- **Configuration**: If the directory contains config files, schemas, or environment definitions, document the config surface area.\n- **API Surface**: If the directory exports a public API (barrel index, route definitions, SDK), document the interface contract.\n- **File Relationships**: How files collaborate, depend on each other, or share state.\n- **Behavioral Contracts**: If files contain regex patterns, format specifications, magic constants, or template strings that define observable behavior, collect them in a dedicated section. Preserve verbatim patterns from file summaries — do NOT paraphrase regex into prose. This section is MANDATORY when file summaries contain behavioral artifacts.\n- **Reproduction-Critical Constants**: If file summaries reference annex files (via ## Annex References sections), list them with links. Example: \"Full prompt template text: [templates.ts.annex.md](./prompts/templates.ts.annex.md)\". Do NOT reproduce annex content in AGENTS.md — just link to it.\n\nChoose any relevant sections or create your own based on the directory contents. The goal is to provide a comprehensive overview that captures the essence of the directory's role in the project and how its files work together, with a focus on what an AI coding assistant would need to know to effectively interact with this code.\n\nSCOPE:\n- AGENTS.md is a NAVIGATIONAL INDEX — help an AI find the right file quickly\n- Focus on: what each file does, how files relate, directory-level patterns\n- Do NOT reproduce full architecture sections — those belong in the root CLAUDE.md\n\nPATH ACCURACY (MANDATORY):\n- When referencing files or modules outside this directory, use ONLY paths from the \"Import Map\" section\n- Do NOT invent, rename, or guess module paths — if a path isn't in the Import Map, don't reference it\n- Use the exact directory names from \"Project Directory Structure\" — do NOT rename directories\n  (e.g., if the directory is called \"cli\", write \"src/cli/\", NOT \"src/commands/\")\n- Cross-module references must use the specifier format from actual import statements\n  (e.g., \"../generation/writers/sum.js\", NOT \"../fs/sum-file.js\")\n- If you are unsure about a path, omit the cross-reference rather than guessing\n\nCONSISTENCY (MANDATORY):\n- Do not contradict yourself within the same document\n- If you describe a technique (e.g., \"regex-based\"), do not call it something else later (e.g., \"AST-based\")\n- When stating version numbers, engines, or config fields, use ONLY values present in the file summaries\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this directory\", \"this module\", \"provides\", \"responsible for\", \"is used to\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Per-file descriptions in Contents sections: 1-2 sentences maximum. Reference key symbols but do not reproduce full summaries.\n- Behavioral contracts (regex patterns, format specs, constants) belong in a separate Behavioral Contracts section, not in per-file descriptions.\n- Annex file references: link to .annex.md files, do not inline their content. One line per annex reference.\n- Subdirectory descriptions: 1-2 sentences maximum. Capture the directory's role, not its full contents.\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- Key exported symbols from file summaries MUST appear in the directory overview\n- Preserve exact casing of identifiers\n\nUSER NOTES:\n- If \"User Notes\" are provided in the prompt, they contain user-defined instructions that will be automatically prepended to your output\n- Do NOT repeat or paraphrase user notes in your generated content — they are included separately\n- You may reference information from user notes for context",
      "response": "<!-- Generated by agents-reverse-engineer -->\n\n# docs/\n\nINPUT.md defines the original vision, feature requirements, and Recursive Language Model (RLM) algorithm specification for agents-reverse-engineer—the foundational design document that establishes the three-phase brownfield documentation strategy (file-level `.sum` generation, directory-level `AGENTS.md` synthesis, root document convergence), CLI command contracts (`/are-generate`, `/are-update`), session lifecycle hook requirements, and integration references to complementary tools (SpecKit, BMAD, Get Shit Done).\n\n## Contents\n\n**[INPUT.md](./INPUT.md)** — Specifies RLM post-order traversal algorithm (leaf-to-root aggregation), command execution requirements via Claude Code or platform-specific alternatives, session-end auto-update hooks, multi-tier documentation system (`AGENTS.md`, `ARCHITECTURE.md`, `STRUCTURE.md`, `STACK.md`, `INTEGRATIONS.md`, `INFRASTRUCTURE.md`, `CONVENTIONS.md`, `TESTING.md`, `PATTERNS.md`, `CONCERNS.md`), and references three inspirational projects for implementation context: SpecKit (GitHub's specification toolkit), BMAD (methodology framework), GSD (workflow system).\n\n## Behavioral Contracts\n\n**RLM Algorithm Execution Order:**\n1. Build project structure tree via directory traversal\n2. Execute analysis starting at first leaf node, proceeding recursively backward (post-order traversal)\n3. File-level: Generate `{filename}.sum` for each source file leaf\n4. Directory-level: When all leaves in directory are summarized, analyze directory and generate `AGENTS.md` plus additional documentation if needed\n5. Root convergence: Continue recursive aggregation until project root is reached\n\n**CLI Command Surface:**\n- `/are-generate` — Full documentation generation from scratch\n- `/are-update` — Incremental update of impacted files\n\n**Referenced External Projects:**\n- SpecKit: https://github.com/github/spec-kit\n- BMAD: https://github.com/bmad-code-org/BMAD-METHOD\n- Get Shit Done: https://github.com/glittercowboy/get-shit-done",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 508,
      "cacheReadTokens": 16334,
      "cacheCreationTokens": 10826,
      "latencyMs": 13256,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": []
    },
    {
      "timestamp": "2026-02-09T17:49:18.204Z",
      "prompt": "Generate AGENTS.md for directory: \"scripts\" (scripts)\n\n## File Summaries (1 files)\n\n### build-hooks.js\n**Purpose:** build-hooks.js copies hook source files from hooks/ to hooks/dist/ for npm package distribution during prepublishOnly...\n\n**build-hooks.js copies hook source files from hooks/ to hooks/dist/ for npm package distribution during prepublishOnly lifecycle.**\n\n## Execution Context\n\nInvoked via `npm run build:hooks` or automatically during `npm run prepublishOnly`. Runs as standalone Node.js script with `#!/usr/bin/env node` shebang.\n\n## File Operations\n\nResolves `projectRoot` via `fileURLToPath(import.meta.url)` and `dirname(__dirname)` to compute absolute paths. Defines `HOOKS_SRC = join(projectRoot, 'hooks')` and `HOOKS_DIST = join(projectRoot, 'hooks', 'dist')`.\n\nCreates `HOOKS_DIST` directory via `mkdirSync(HOOKS_DIST, { recursive: true })` if not present via `existsSync()` check.\n\nFilters hook files using `readdirSync(HOOKS_SRC).filter(f => f.endsWith('.js') && f !== 'dist')` to exclude `dist` directory itself and non-JavaScript files. Copies each filtered file via `copyFileSync(join(HOOKS_SRC, file), join(HOOKS_DIST, file))`.\n\n## Console Output\n\nLogs `Building hooks...` header, per-file progress `Copied: ${file} -> hooks/dist/${file}`, and completion summary `Done. ${hookFiles.length} hook(s) built.`.\n\n## Integration Points\n\nCalled by package.json `prepublishOnly` script which chains `npm run build && npm run build:hooks`. Ensures hooks/ directory contents (are-check-update.js, are-session-end.js, opencode-are-check-update.js, opencode-are-session-end.js) exist in hooks/dist/ for npm tarball inclusion referenced by installer/operations.ts hook installation logic.\n\n## Project Directory Structure\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>",
      "systemPrompt": "You are generating an AGENTS.md file — a directory-level overview for AI coding assistants.\n\nCRITICAL: Output ONLY the raw markdown content. No code fences, no preamble, no explanations, no conversational text. Your entire response IS the AGENTS.md file content.\n\nOUTPUT FORMAT:\n- First line MUST be exactly: <!-- Generated by agents-reverse-engineer -->\n- Use a # heading with the directory name\n- Write a one-paragraph purpose statement for the directory\nusage\nADAPTIVE SECTIONS:\nAnalyze the directory contents and choose the most relevant sections. Do NOT use a fixed template. Instead, select sections that best document this specific directory for an AI that needs to reconstruct or extend the project.\n\nConsider these section types (choose what applies):\n- **Contents**: Group files by purpose/category under ## headings. For each file: markdown link [filename](./filename) and a one-line description.\n- **Subdirectories**: If subdirectories exist, list them with links [dirname/](./dirname/) and brief summaries.\n- **Architecture / Data Flow**: If files form a pipeline, request/response chain, or layered architecture, document it.\n- **Stack**: If this is a package root (has package.json, Cargo.toml, go.mod, etc.), document the technology stack, key scripts, and entry points.\n- **Structure**: If the directory layout follows a convention (feature-sliced, domain-driven, MVC, etc.), document it.\n- **Patterns**: If files share recurring design patterns (factory, strategy, middleware, barrel re-export), name and document them.\n- **Configuration**: If the directory contains config files, schemas, or environment definitions, document the config surface area.\n- **API Surface**: If the directory exports a public API (barrel index, route definitions, SDK), document the interface contract.\n- **File Relationships**: How files collaborate, depend on each other, or share state.\n- **Behavioral Contracts**: If files contain regex patterns, format specifications, magic constants, or template strings that define observable behavior, collect them in a dedicated section. Preserve verbatim patterns from file summaries — do NOT paraphrase regex into prose. This section is MANDATORY when file summaries contain behavioral artifacts.\n- **Reproduction-Critical Constants**: If file summaries reference annex files (via ## Annex References sections), list them with links. Example: \"Full prompt template text: [templates.ts.annex.md](./prompts/templates.ts.annex.md)\". Do NOT reproduce annex content in AGENTS.md — just link to it.\n\nChoose any relevant sections or create your own based on the directory contents. The goal is to provide a comprehensive overview that captures the essence of the directory's role in the project and how its files work together, with a focus on what an AI coding assistant would need to know to effectively interact with this code.\n\nSCOPE:\n- AGENTS.md is a NAVIGATIONAL INDEX — help an AI find the right file quickly\n- Focus on: what each file does, how files relate, directory-level patterns\n- Do NOT reproduce full architecture sections — those belong in the root CLAUDE.md\n\nPATH ACCURACY (MANDATORY):\n- When referencing files or modules outside this directory, use ONLY paths from the \"Import Map\" section\n- Do NOT invent, rename, or guess module paths — if a path isn't in the Import Map, don't reference it\n- Use the exact directory names from \"Project Directory Structure\" — do NOT rename directories\n  (e.g., if the directory is called \"cli\", write \"src/cli/\", NOT \"src/commands/\")\n- Cross-module references must use the specifier format from actual import statements\n  (e.g., \"../generation/writers/sum.js\", NOT \"../fs/sum-file.js\")\n- If you are unsure about a path, omit the cross-reference rather than guessing\n\nCONSISTENCY (MANDATORY):\n- Do not contradict yourself within the same document\n- If you describe a technique (e.g., \"regex-based\"), do not call it something else later (e.g., \"AST-based\")\n- When stating version numbers, engines, or config fields, use ONLY values present in the file summaries\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this directory\", \"this module\", \"provides\", \"responsible for\", \"is used to\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Per-file descriptions in Contents sections: 1-2 sentences maximum. Reference key symbols but do not reproduce full summaries.\n- Behavioral contracts (regex patterns, format specs, constants) belong in a separate Behavioral Contracts section, not in per-file descriptions.\n- Annex file references: link to .annex.md files, do not inline their content. One line per annex reference.\n- Subdirectory descriptions: 1-2 sentences maximum. Capture the directory's role, not its full contents.\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- Key exported symbols from file summaries MUST appear in the directory overview\n- Preserve exact casing of identifiers\n\nUSER NOTES:\n- If \"User Notes\" are provided in the prompt, they contain user-defined instructions that will be automatically prepended to your output\n- Do NOT repeat or paraphrase user notes in your generated content — they are included separately\n- You may reference information from user notes for context",
      "response": "<!-- Generated by agents-reverse-engineer -->\n\n# scripts\n\nBuild automation scripts for npm lifecycle hooks, prepublishOnly hook asset preparation, and distribution artifact generation.\n\n## Contents\n\n### [build-hooks.js](./build-hooks.js)\nCopies hook source files from `hooks/` to `hooks/dist/` during `prepublishOnly` lifecycle. Filters `.js` files via `readdirSync()` + `endsWith('.js')` predicate, excludes `dist` directory itself, creates target via `mkdirSync(HOOKS_DIST, { recursive: true })`, performs per-file `copyFileSync()` from `HOOKS_SRC` to `HOOKS_DIST`. Invoked by `package.json` `prepublishOnly` script chain (`npm run build && npm run build:hooks`). Ensures session lifecycle hooks (are-check-update.js, are-session-end.js, opencode-are-check-update.js, opencode-are-session-end.js) exist in npm tarball for installer/operations.ts reference during global/local hook installation.\n\n## Integration Points\n\n**Package.json lifecycle:**\n- `prepublishOnly` script executes `npm run build:hooks` → invokes build-hooks.js\n- Runs after TypeScript compilation (`npm run build`) before `npm publish`\n- Ensures hooks/dist/ directory populated for tarball inclusion\n\n**Hook installation workflow:**\n- `src/installer/operations.ts` references hooks/dist/ files during `installCommands()` execution\n- Supports both global (`~/.claude/hooks/`) and local (`.claude/hooks/`) installation modes\n- Platform-specific targets: Claude Code, OpenCode, Gemini via runtime detection\n\n**File resolution:**\n- Computes `projectRoot` via `fileURLToPath(import.meta.url)` + double `dirname()` traversal\n- Resolves absolute paths via `join(projectRoot, 'hooks')` and `join(projectRoot, 'hooks', 'dist')`\n- Prevents relative path ambiguity in npm lifecycle context",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 483,
      "cacheReadTokens": 17587,
      "cacheCreationTokens": 9454,
      "latencyMs": 14514,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": []
    },
    {
      "timestamp": "2026-02-09T17:49:18.213Z",
      "prompt": "Generate AGENTS.md for directory: \"hooks\" (hooks)\n\n## File Summaries (4 files)\n\n### are-check-update.js\n**Purpose:** are-check-update.js spawns detached background process to check npm registry for agents-reverse-engineer updates, cac...\n\n**are-check-update.js spawns detached background process to check npm registry for agents-reverse-engineer updates, caches result to ~/.claude/cache/are-update-check.json for SessionStart hook consumption.**\n\n## Execution Model\n\nShebang `#!/usr/bin/env node` enables direct execution. Spawns detached child process via `spawn(process.execPath, ['-e', scriptString], { stdio: 'ignore', detached: true, windowsHide: true })` followed by `child.unref()` to allow parent process exit without blocking. Background process runs asynchronously, parent exits immediately after spawn.\n\n## Version File Resolution\n\nChecks two ARE-VERSION file locations in priority order:\n1. `projectVersionFile = join(cwd, '.claude', 'ARE-VERSION')` — local project install\n2. `globalVersionFile = join(homeDir, '.claude', 'ARE-VERSION')` — global npm install\n\nBackground process reads first existing file via `readFileSync(versionFile, 'utf8').trim()`, defaults to `'0.0.0'` if neither exists.\n\n## Update Check Logic\n\nBackground child process executes `execSync('npm view agents-reverse-engineer version', { encoding: 'utf8', timeout: 10000, windowsHide: true })` to query npm registry for latest published version. Compares against installed version from ARE-VERSION file via inequality check `installed !== latest`. Timeout enforced at 10000ms to prevent indefinite hangs on network failures.\n\n## Cache File Format\n\nWrites JSON object to `cacheFile` (path: `~/.claude/cache/are-update-check.json`) with structure:\n```javascript\n{\n  update_available: boolean,  // true if latest !== installed\n  installed: string,          // version from ARE-VERSION file or '0.0.0'\n  latest: string,             // npm registry version or 'unknown' on error\n  checked: number             // Unix timestamp (seconds) via Math.floor(Date.now() / 1000)\n}\n```\n\nCache directory created via `mkdirSync(cacheDir, { recursive: true })` if not exists.\n\n## Error Handling\n\nTry-catch blocks in background process isolate failures:\n- Version file read failure defaults `installed` to `'0.0.0'`\n- npm registry query failure sets `latest` to `null`, then `'unknown'` in result object\n- Network timeout (10000ms) throws caught exception, triggers unknown fallback\n\nNo error propagation to parent process — all failures silently degrade to cached result with `update_available: false`.\n\n## Integration Points\n\nCalled by SessionStart hook (`~/.claude/hooks/session-start/are-check-update.js` symlink). Session initialization triggers check once per session. Cache file consumed by ARE CLI or future session-start handlers to display update notifications without blocking session startup.\n\n## Platform Compatibility\n\n`windowsHide: true` flag in both `spawn()` and `execSync()` suppresses console window creation on Windows. `process.execPath` resolves to Node.js binary path across platforms (Linux/macOS/Windows). Uses `homedir()` and `join()` for cross-platform path resolution.\n\n## Dependencies\n\n- `fs`: `existsSync`, `mkdirSync`, `readFileSync`, `writeFileSync`\n- `os`: `homedir`\n- `path`: `join`\n- `child_process`: `spawn`, `execSync`\n\nNo external npm dependencies — relies solely on Node.js built-in modules.\n### are-session-end.js\n**Purpose:** are-session-end.js executes background `are update` when Claude Code/Gemini sessions terminate, conditioned on git wo...\n\n**are-session-end.js executes background `are update` when Claude Code/Gemini sessions terminate, conditioned on git working tree changes and disable flags.**\n\n## Execution Context\n\nRuns as SessionEnd lifecycle hook registered in `~/.claude/hooks/` or `~/.gemini/hooks/`. Invoked automatically by IDE runtime when user closes session. Uses Node.js shebang `#!/usr/bin/env node` for direct execution without explicit interpreter invocation.\n\n## Disable Mechanisms\n\nExits with code 0 (silent success) when either condition is true:\n\n- Environment variable `ARE_DISABLE_HOOK` equals `'1'`\n- Config file `.agents-reverse-engineer.yaml` exists and contains substring `'hook_enabled: false'` (no YAML parser used, raw string search via `readFileSync()` + `String.includes()`)\n\n## Git Status Check\n\nInvokes `execSync('git status --porcelain', { encoding: 'utf-8' })` to detect uncommitted changes. Exits silently (code 0) if:\n\n- Output is empty after `String.trim()` (no changes since last commit)\n- `execSync()` throws (not a git repository or git command unavailable)\n\n## Background Update Invocation\n\nSpawns detached background process when changes detected:\n\n```javascript\nspawn('npx', ['agents-reverse-engineer@latest', 'update', '--quiet'], {\n  stdio: 'ignore',\n  detached: true,\n})\nchild.unref()\n```\n\nProcess characteristics:\n\n- `stdio: 'ignore'` — discards stdout/stderr/stdin (no TTY attachment)\n- `detached: true` — runs in separate process group, survives parent termination\n- `child.unref()` — allows Node.js event loop to exit without waiting for child completion\n- `npx agents-reverse-engineer@latest` — always fetches latest published version from npm registry\n- `--quiet` flag — suppresses progress output (writes to `.agents-reverse-engineer/progress.log` only)\n\n## Integration Points\n\n- **Installer**: Copied to hook directories via `src/installer/operations.ts` → `installCommands()` → hook file copying\n- **Build**: Duplicated to `hooks/dist/` via `scripts/build-hooks.js` for npm tarball inclusion\n- **Companion Hook**: `are-check-update.js` (SessionStart) notifies users of available updates, this hook (SessionEnd) performs automatic documentation refresh\n- **OpenCode Equivalent**: `opencode-are-session-end.js` wraps identical logic in plugin factory returning `event['session.deleted']` handler\n### opencode-are-check-update.js\n**Purpose:** OpenCode session lifecycle plugin that spawns detached background process on `session.created` event to check npm reg...\n\n**OpenCode session lifecycle plugin that spawns detached background process on `session.created` event to check npm registry for ARE version updates and cache results to `~/.config/opencode/cache/are-update-check.json`.**\n\n## Exported Interface\n\n`AreCheckUpdate()` async factory function returns OpenCode plugin object with `event['session.created']` handler.\n\n## Version Resolution Strategy\n\nChecks project-local `<cwd>/.opencode/ARE-VERSION` first, falls back to global `~/.config/opencode/ARE-VERSION` to read installed version string. Defaults to `'0.0.0'` if neither file exists.\n\n## Background Process Execution\n\nSpawns detached child process via `spawn(process.execPath, ['-e', scriptString], { stdio: 'ignore', detached: true, windowsHide: true })` with serialized inline script string. Script injects JSON-stringified path constants via template substitution: `JSON.stringify(cacheFile)`, `JSON.stringify(projectVersionFile)`, `JSON.stringify(globalVersionFile)`.\n\n## npm Registry Query\n\nExecutes `npm view agents-reverse-engineer version` via `execSync()` with 10s timeout (`{ encoding: 'utf8', timeout: 10000, windowsHide: true }`). Returns latest version string or `'unknown'` on error.\n\n## Cache Output Format\n\nWrites JSON object to cache file with keys: `update_available` (boolean comparing `installed !== latest`), `installed` (version string), `latest` (version string or `'unknown'`), `checked` (Unix epoch seconds from `Math.floor(Date.now() / 1000)`).\n\n## Error Handling\n\nTry-catch blocks around `readFileSync()` for version files and `execSync()` for npm query suppress all errors, allowing graceful degradation with defaults (`installed='0.0.0'`, `latest='unknown'`).\n\n## Cache Directory Creation\n\nEnsures `~/.config/opencode/cache` exists via `mkdirSync(cacheDir, { recursive: true })` before spawning background process. Directory resolution uses `homedir()` from `os` module.\n\n## Process Lifecycle\n\n`child.unref()` detaches plugin from child process lifetime, allowing OpenCode session to exit without waiting for npm query completion. Background process writes cache asynchronously after plugin handler returns.\n### opencode-are-session-end.js\n**Purpose:** opencode-are-session-end.js exports `AreSessionEnd` async factory returning an OpenCode plugin that triggers `npx age...\n\n**opencode-are-session-end.js exports `AreSessionEnd` async factory returning an OpenCode plugin that triggers `npx agents-reverse-engineer@latest update --quiet` as a detached background process when `'session.deleted'` events fire, conditional on git working tree changes detected via `git status --porcelain`.**\n\n## Exported API\n\n`AreSessionEnd` — async factory function returning OpenCode plugin object with `event['session.deleted']` handler. No parameters. Returns `Promise<{ event: { 'session.deleted': () => Promise<void> } }>`.\n\n## Event Handler Logic\n\nHandler checks two disable mechanisms before executing update:\n- Environment variable: exits if `process.env.ARE_DISABLE_HOOK === '1'`\n- Config file: reads `.agents-reverse-engineer.yaml` via `existsSync` + `readFileSync`, exits if `config.includes('hook_enabled: false')` (substring search, no YAML parser)\n\nGit change detection via `execSync('git status --porcelain', { encoding: 'utf-8' })` wrapped in try-catch. Returns early if `status.trim()` empty or if `execSync` throws (non-git repo or missing git binary).\n\n## Background Process Spawning\n\nSpawns `npx agents-reverse-engineer@latest update --quiet` via `spawn()` with detached process pattern:\n```javascript\nspawn('npx', ['agents-reverse-engineer@latest', 'update', '--quiet'], {\n  stdio: 'ignore',\n  detached: true,\n})\n```\nCalls `.unref()` on child to allow parent process exit without waiting for completion. Prevents blocking OpenCode session close.\n\n## OpenCode Plugin Contract\n\nOpenCode plugins export async factory functions returning objects with `event` property mapping event names to async handlers. This module implements `'session.deleted'` lifecycle hook, equivalent to Claude/Gemini `SessionEnd` hook but using OpenCode's plugin system conventions.\n\n## Dependencies\n\nImports `execSync`, `spawn` from `child_process`, `existsSync`, `readFileSync` from `fs`. No external npm dependencies.\n\n## Project Directory Structure\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>",
      "systemPrompt": "You are generating an AGENTS.md file — a directory-level overview for AI coding assistants.\n\nCRITICAL: Output ONLY the raw markdown content. No code fences, no preamble, no explanations, no conversational text. Your entire response IS the AGENTS.md file content.\n\nOUTPUT FORMAT:\n- First line MUST be exactly: <!-- Generated by agents-reverse-engineer -->\n- Use a # heading with the directory name\n- Write a one-paragraph purpose statement for the directory\nusage\nADAPTIVE SECTIONS:\nAnalyze the directory contents and choose the most relevant sections. Do NOT use a fixed template. Instead, select sections that best document this specific directory for an AI that needs to reconstruct or extend the project.\n\nConsider these section types (choose what applies):\n- **Contents**: Group files by purpose/category under ## headings. For each file: markdown link [filename](./filename) and a one-line description.\n- **Subdirectories**: If subdirectories exist, list them with links [dirname/](./dirname/) and brief summaries.\n- **Architecture / Data Flow**: If files form a pipeline, request/response chain, or layered architecture, document it.\n- **Stack**: If this is a package root (has package.json, Cargo.toml, go.mod, etc.), document the technology stack, key scripts, and entry points.\n- **Structure**: If the directory layout follows a convention (feature-sliced, domain-driven, MVC, etc.), document it.\n- **Patterns**: If files share recurring design patterns (factory, strategy, middleware, barrel re-export), name and document them.\n- **Configuration**: If the directory contains config files, schemas, or environment definitions, document the config surface area.\n- **API Surface**: If the directory exports a public API (barrel index, route definitions, SDK), document the interface contract.\n- **File Relationships**: How files collaborate, depend on each other, or share state.\n- **Behavioral Contracts**: If files contain regex patterns, format specifications, magic constants, or template strings that define observable behavior, collect them in a dedicated section. Preserve verbatim patterns from file summaries — do NOT paraphrase regex into prose. This section is MANDATORY when file summaries contain behavioral artifacts.\n- **Reproduction-Critical Constants**: If file summaries reference annex files (via ## Annex References sections), list them with links. Example: \"Full prompt template text: [templates.ts.annex.md](./prompts/templates.ts.annex.md)\". Do NOT reproduce annex content in AGENTS.md — just link to it.\n\nChoose any relevant sections or create your own based on the directory contents. The goal is to provide a comprehensive overview that captures the essence of the directory's role in the project and how its files work together, with a focus on what an AI coding assistant would need to know to effectively interact with this code.\n\nSCOPE:\n- AGENTS.md is a NAVIGATIONAL INDEX — help an AI find the right file quickly\n- Focus on: what each file does, how files relate, directory-level patterns\n- Do NOT reproduce full architecture sections — those belong in the root CLAUDE.md\n\nPATH ACCURACY (MANDATORY):\n- When referencing files or modules outside this directory, use ONLY paths from the \"Import Map\" section\n- Do NOT invent, rename, or guess module paths — if a path isn't in the Import Map, don't reference it\n- Use the exact directory names from \"Project Directory Structure\" — do NOT rename directories\n  (e.g., if the directory is called \"cli\", write \"src/cli/\", NOT \"src/commands/\")\n- Cross-module references must use the specifier format from actual import statements\n  (e.g., \"../generation/writers/sum.js\", NOT \"../fs/sum-file.js\")\n- If you are unsure about a path, omit the cross-reference rather than guessing\n\nCONSISTENCY (MANDATORY):\n- Do not contradict yourself within the same document\n- If you describe a technique (e.g., \"regex-based\"), do not call it something else later (e.g., \"AST-based\")\n- When stating version numbers, engines, or config fields, use ONLY values present in the file summaries\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this directory\", \"this module\", \"provides\", \"responsible for\", \"is used to\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Per-file descriptions in Contents sections: 1-2 sentences maximum. Reference key symbols but do not reproduce full summaries.\n- Behavioral contracts (regex patterns, format specs, constants) belong in a separate Behavioral Contracts section, not in per-file descriptions.\n- Annex file references: link to .annex.md files, do not inline their content. One line per annex reference.\n- Subdirectory descriptions: 1-2 sentences maximum. Capture the directory's role, not its full contents.\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- Key exported symbols from file summaries MUST appear in the directory overview\n- Preserve exact casing of identifiers\n\nUSER NOTES:\n- If \"User Notes\" are provided in the prompt, they contain user-defined instructions that will be automatically prepended to your output\n- Do NOT repeat or paraphrase user notes in your generated content — they are included separately\n- You may reference information from user notes for context",
      "response": "<!-- Generated by agents-reverse-engineer -->\n\n# hooks\n\n**IDE session lifecycle hooks for automatic ARE version checking and documentation updates, implemented as detached background processes for Claude Code, Gemini CLI, and OpenCode runtimes.**\n\n## Contents\n\n### Session Lifecycle Hooks\n\n**[are-check-update.js](./are-check-update.js)** — SessionStart hook spawning detached background process querying npm registry via `npm view agents-reverse-engineer version`, comparing against local/global `~/.claude/ARE-VERSION`, caching results to `~/.claude/cache/are-update-check.json` with timestamp + version metadata. 10s timeout enforced. Graceful degradation on network/git failures (sets `latest: 'unknown'`).\n\n**[are-session-end.js](./are-session-end.js)** — SessionEnd hook executing `npx agents-reverse-engineer@latest update --quiet` as detached background process when git working tree changes detected via `git status --porcelain`. Honors disable flags: `ARE_DISABLE_HOOK=1` environment variable or `hook_enabled: false` in `.agents-reverse-engineer.yaml` (substring search, no parser). Silent exit on no changes.\n\n### OpenCode Plugin Wrappers\n\n**[opencode-are-check-update.js](./opencode-are-check-update.js)** — `AreCheckUpdate()` async factory returning OpenCode plugin with `event['session.created']` handler. Equivalent logic to `are-check-update.js` but adapted to OpenCode plugin system. Checks `~/.config/opencode/ARE-VERSION` (project-local `.opencode/ARE-VERSION` first), writes cache to `~/.config/opencode/cache/are-update-check.json`.\n\n**[opencode-are-session-end.js](./opencode-are-session-end.js)** — `AreSessionEnd()` async factory returning OpenCode plugin with `event['session.deleted']` handler. Equivalent logic to `are-session-end.js` wrapped in plugin interface contract. Same disable mechanisms + git change detection + detached `npx` spawn pattern.\n\n## Architecture\n\n### Detached Background Process Pattern\n\nAll hooks use identical spawn pattern to prevent blocking IDE session lifecycle:\n\n```javascript\nspawn(process.execPath, ['-e', scriptString], {\n  stdio: 'ignore',      // discard stdout/stderr/stdin\n  detached: true,       // separate process group, survives parent exit\n  windowsHide: true     // suppress console window on Windows\n})\nchild.unref()           // allow Node.js event loop exit without waiting\n```\n\nSerialized script strings injected via `-e` flag contain inline logic (version checks, npm queries, cache writes). Parent process exits immediately after spawn, background child completes asynchronously.\n\n### Version File Resolution Priority\n\nBoth check hooks follow precedence order:\n1. **Project-local**: `<cwd>/.claude/ARE-VERSION` or `<cwd>/.opencode/ARE-VERSION`\n2. **Global install**: `~/.claude/ARE-VERSION` or `~/.config/opencode/ARE-VERSION`\n3. **Default fallback**: `'0.0.0'` if neither exists\n\n### Cache File Format (JSON)\n\nWritten by check hooks to `~/.{claude,opencode}/cache/are-update-check.json`:\n\n```json\n{\n  \"update_available\": true,\n  \"installed\": \"0.5.2\",\n  \"latest\": \"0.6.0\",\n  \"checked\": 1738886400\n}\n```\n\nKeys: `update_available` (boolean via `installed !== latest`), `installed` (string from ARE-VERSION or `'0.0.0'`), `latest` (string from `npm view` or `'unknown'` on timeout), `checked` (Unix epoch seconds).\n\n### Disable Mechanisms\n\nSession-end hooks check two flags before execution:\n\n1. **Environment variable**: `process.env.ARE_DISABLE_HOOK === '1'`\n2. **Config file substring**: `.agents-reverse-engineer.yaml` contains `'hook_enabled: false'` (raw `readFileSync()` + `String.includes()`, no YAML parser)\n\nBoth checks trigger silent exit (code 0) to prevent observable failures when user disables hooks.\n\n## Integration Points\n\n**Installed by**: `src/installer/operations.ts` copies files from `hooks/` → `~/.claude/hooks/` or `~/.config/opencode/plugins/` during `are --runtime <platform> -g` command execution.\n\n**Built by**: `scripts/build-hooks.js` duplicates `hooks/*.js` → `hooks/dist/*.js` for npm tarball inclusion during `npm run build:hooks` (invoked by `prepublishOnly`).\n\n**Consumed by**: IDE runtimes invoke hooks automatically on session lifecycle events:\n- Claude Code: calls `~/.claude/hooks/session-start/are-check-update.js`, `~/.claude/hooks/session-end/are-session-end.js`\n- Gemini CLI: equivalent paths under `~/.gemini/hooks/`\n- OpenCode: loads plugins from `~/.config/opencode/plugins/`, invokes `event['session.created']` / `event['session.deleted']` handlers\n\n## Behavioral Contracts\n\n### npm Registry Query Command\n\n```javascript\nexecSync('npm view agents-reverse-engineer version', { \n  encoding: 'utf8', \n  timeout: 10000, \n  windowsHide: true \n})\n```\n\nReturns latest published version string (e.g., `\"0.6.5\\n\"`). Timeout 10000ms enforced to prevent indefinite hangs. Throws on network failures, caught by hooks to set `latest: 'unknown'`.\n\n### Git Change Detection Command\n\n```javascript\nexecSync('git status --porcelain', { encoding: 'utf-8' })\n```\n\nReturns empty string when working tree clean, non-empty on uncommitted changes. Throws when not git repo or git unavailable, caught by session-end hooks to skip update spawn.\n\n### npx Update Invocation Command\n\n```bash\nnpx agents-reverse-engineer@latest update --quiet\n```\n\n`@latest` specifier forces npm registry fetch, bypassing local cache. `--quiet` flag suppresses terminal output (logs to `.agents-reverse-engineer/progress.log` only). Spawned as detached child to prevent blocking IDE session close.\n\n## Platform Compatibility\n\n**Path resolution**: Uses `os.homedir()` + `path.join()` for cross-platform directory construction (`~/.claude`, `~/.config/opencode`, `~/.gemini`).\n\n**Windows-specific flags**: `windowsHide: true` in `spawn()` and `execSync()` suppresses console window creation.\n\n**Node.js compatibility**: `#!/usr/bin/env node` shebang enables direct execution without explicit interpreter invocation. Requires Node.js ≥18.0.0 (same as ARE runtime requirement).\n\n## Dependencies\n\nAll four hooks use identical Node.js built-in module subset:\n- `child_process`: `spawn`, `execSync`\n- `fs`: `existsSync`, `mkdirSync`, `readFileSync`, `writeFileSync`\n- `os`: `homedir`\n- `path`: `join`\n\nNo external npm dependencies — statically analyzable, zero-install executable scripts.",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 1744,
      "cacheReadTokens": 17587,
      "cacheCreationTokens": 11636,
      "latencyMs": 37242,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": []
    },
    {
      "timestamp": "2026-02-09T17:49:18.208Z",
      "prompt": "Generate AGENTS.md for directory: \"src\" (src)\n\n## File Summaries (1 files)\n\n### version.ts\n**Purpose:** getVersion() reads the package.json version field from the parent directory of the compiled module, returning 'unknow...\n\n**getVersion() reads the package.json version field from the parent directory of the compiled module, returning 'unknown' on any error.**\n\n## Exported Function\n\n- `getVersion(): string` — Synchronously reads `../package.json` relative to the compiled module location, parses JSON, extracts `version` field, returns it or `'unknown'` if parsing/reading fails.\n\n## Implementation Details\n\nUses `import.meta.url` to resolve current module path via `fileURLToPath()`, then `dirname()` to get containing directory, constructs `join(__dirname, '..', 'package.json')` path pointing to project root package.json. Reads file synchronously with `readFileSync(packagePath, 'utf-8')`, parses via `JSON.parse()`, accesses `packageJson.version` with fallback `|| 'unknown'`. Wraps entire logic in try-catch returning `'unknown'` on any exception (file not found, invalid JSON, missing version field).\n\n## Integration Points\n\nCalled by CLI entry point (`src/cli/index.ts`) to display version in `--version` flag handler and by session lifecycle hooks (`hooks/are-check-update.js`) to compare local version against npm registry for update notifications.\n\n## Runtime Assumptions\n\nAssumes build output structure places compiled `version.js` in `dist/` directory with `package.json` at `dist/../package.json` (project root). ES module context required for `import.meta.url` support (Node.js ≥18.0.0).\n\n## Project Directory Structure\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\n\n## Subdirectories\n\n### ai/\n<!-- Generated by agents-reverse-engineer -->\n\n# src/ai\n\nAI service orchestration layer abstracting Claude Code, Gemini, and OpenCode CLIs through backend adapters, exponential backoff retry, subprocess resource management, telemetry logging, and timeout enforcement for concurrent file analysis pools.\n\n## Contents\n\n### Core Orchestration\n\n**[service.ts](./service.ts)** — AIService class coordinating backend selection, subprocess execution via `runSubprocess()`, retry logic via `withRetry()`, telemetry accumulation via `TelemetryLogger`, trace emission via `ITraceWriter`, rate-limit detection matching stderr patterns (`'rate limit'`, `'429'`, `'too many requests'`, `'overloaded'`), optional debug logging with heap/RSS metrics via `formatBytes()`, and fire-and-forget subprocess log serialization via `enqueueSubprocessLog()` promise chain.\n\n**[registry.ts](./registry.ts)** — BackendRegistry managing adapter map with `register()`, `get()`, `getAll()` methods. Exports `createBackendRegistry()` instantiating registry with ClaudeBackend, GeminiBackend, OpenCodeBackend in priority order. Exports `resolveBackend(registry, requested)` implementing auto-detection via `detectBackend()` iterating `backend.isAvailable()` or throwing `AIServiceError` with code `'CLI_NOT_FOUND'` and aggregated install instructions via `getInstallInstructions()`.\n\n**[subprocess.ts](./subprocess.ts)** — `runSubprocess(command, args, options)` spawning `execFile()` child processes with stdin piping, timeout enforcement via SIGTERM at `options.timeoutMs`, SIGKILL escalation after 5s grace period, process group killing via `kill(-pid)`, active subprocess tracking in module Map enabling `getActiveSubprocessCount()` and `getActiveSubprocesses()` inspection, and `onSpawn(pid)` callback at spawn time.\n\n**[retry.ts](./retry.ts)** — `withRetry(fn, options)` exponential backoff wrapper executing async function with delay formula `min(baseDelayMs * multiplier^attempt, maxDelayMs) + jitter` (jitter uniform random 0-500ms). Exports `DEFAULT_RETRY_OPTIONS` constant with `maxRetries: 3`, `baseDelayMs: 1000`, `maxDelayMs: 8000`, `multiplier: 2`. Retry flow controlled by `options.isRetryable(error)` predicate and optional `options.onRetry(attempt, error)` callback.\n\n**[types.ts](./types.ts)** — Type definitions: `AIBackend` interface with `isAvailable()`, `buildArgs()`, `parseResponse()`, `getInstallInstructions()` contract. `AIResponse` normalized response shape with `text`, `model`, `inputTokens`, `outputTokens`, `cacheReadTokens`, `cacheCreationTokens`, `durationMs`, `exitCode`, `raw`. `AICallOptions` with `prompt`, `systemPrompt`, `model`, `timeoutMs`, `maxTurns`, `taskLabel`. `SubprocessResult` with `stdout`, `stderr`, `exitCode`, `signal`, `durationMs`, `timedOut`, `childPid`. `TelemetryEntry` per-call metadata, `RunLog` aggregated run structure, `FileRead` file metadata, `RetryOptions` retry config, `AIServiceError` typed error with `code` discriminator (`'CLI_NOT_FOUND'`, `'TIMEOUT'`, `'PARSE_ERROR'`, `'SUBPROCESS_ERROR'`, `'RATE_LIMIT'`).\n\n**[index.ts](./index.ts)** — Barrel export aggregating public API: AIService, BackendRegistry, resolveBackend, detectBackend, createBackendRegistry, withRetry, runSubprocess, isCommandOnPath, all types from `./types.js`, AIServiceOptions from `./service.js`.\n\n## Subdirectories\n\n**[backends/](./backends/)** — AIBackend adapter implementations: ClaudeBackend with Zod validation via `ClaudeResponseSchema`, GeminiBackend stub throwing `SUBPROCESS_ERROR` until JSON format stabilizes, OpenCodeBackend stub throwing `SUBPROCESS_ERROR` until JSONL parsing implemented. Shared `isCommandOnPath()` utility scanning `process.env.PATH` with Windows `PATHEXT` support.\n\n**[telemetry/](./telemetry/)** — Telemetry subsystem: TelemetryLogger accumulating TelemetryEntry instances, `writeRunLog()` persisting JSON to `.agents-reverse-engineer/logs/run-<timestamp>.json`, `cleanupOldLogs()` enforcing retention via lexicographic filename sorting.\n\n## Architecture\n\n### Backend Adapter Pattern\n\nAIBackend interface decouples CLI invocation from backend-specific argument construction and JSON parsing. Registry selects backend at runtime via `config.ai.backend` field (`'claude'` | `'gemini'` | `'opencode'` | `'auto'`). Auto-detection calls `isAvailable()` on each backend in registration order (Claude → Gemini → OpenCode) until first CLI found on PATH.\n\n### Subprocess Resource Management\n\nMitigates Claude CLI thread exhaustion (GitHub #5771: 200 Node.js instances):\n- `NODE_OPTIONS='--max-old-space-size=512'` limits heap to 512MB per subprocess\n- `UV_THREADPOOL_SIZE='4'` constrains libuv thread pool to 4 threads\n- `CLAUDE_CODE_DISABLE_BACKGROUND_TASKS='1'` prevents background task spawning\n- `--disallowedTools Task` prevents subagent spawning\n- Process group killing via `kill(-pid)` terminates subprocess trees\n- Default concurrency 2 for WSL environments (5 elsewhere)\n\n### Retry Strategy\n\n`AIService.call()` wraps `runSubprocess()` in `withRetry()` configured to retry only `RATE_LIMIT` errors (timeouts excluded). Rate-limit detection via `isRateLimitStderr()` substring matching. Exponential backoff adds uniform jitter (0-500ms) preventing thundering herd when multiple workers hit rate limits simultaneously.\n\n### Telemetry Accumulation\n\nTelemetryLogger maintains in-memory `entries: TelemetryEntry[]` array throughout CLI run. After each `AIService.call()`, logger records timestamp, prompt, response, model, token counts (input/output/cacheRead/cacheCreation), latency, exitCode, retryCount, and filesRead metadata. On run completion, `logger.toRunLog()` serializes entries plus computed summary (totalCalls, totalInputTokens, totalOutputTokens, totalCacheReadTokens, totalCacheCreationTokens, totalDurationMs, errorCount, uniqueFilesRead) to `RunLog` JSON structure. `writeRunLog()` persists with ISO-8601-derived filename, `cleanupOldLogs()` enforces retention by deleting oldest logs exceeding `config.ai.telemetry.keepRuns` threshold.\n\n### Timeout Enforcement\n\n`runSubprocess()` sends SIGTERM at `timeoutMs`, schedules unref'd SIGKILL timer at `timeoutMs + 5000ms` for hung processes ignoring SIGTERM. Sets `SubprocessResult.timedOut = true` when `execFile` error has `killed: true` property. AIService throws `AIServiceError('TIMEOUT')` on timeout detection (non-retryable per resource constraint mitigation).\n\n### Trace Emission\n\nAIService invokes `tracer.emit()` for `subprocess:spawn` at `onSpawn` callback time (includes `childPid`, `taskLabel`, `command`, `args`), `subprocess:exit` after completion (includes `exitCode`, `signal`, `durationMs`, `timedOut`), and `retry` events before delay (includes `attempt`, `taskLabel`, `errorCode`). ITraceWriter from `src/orchestration/trace.ts` provides promise-chain serialization ensuring NDJSON line order matches emission order despite concurrent workers.\n\n## Behavioral Contracts\n\n### Rate Limit Patterns\n\n```typescript\nconst RATE_LIMIT_PATTERNS = ['rate limit', '429', 'too many requests', 'overloaded'];\n```\n\n### Default Retry Configuration\n\n```typescript\nDEFAULT_RETRY_OPTIONS = {\n  maxRetries: 3,\n  baseDelayMs: 1_000,\n  maxDelayMs: 8_000,\n  multiplier: 2\n}\n```\n\n### SIGKILL Grace Period\n\n```typescript\nconst SIGKILL_GRACE_MS = 5_000;\n```\n\n### Subprocess maxBuffer\n\n```typescript\nmaxBuffer: 10 * 1024 * 1024  // 10MB stdout/stderr capture limit\n```\n\n## Integration Points\n\n**Consumed by:**\n- `src/orchestration/runner.ts` — Creates AIService instance, calls `setTracer()` and `setDebug()` based on CLI flags, invokes `call()` for each task, calls `finalize()` at run end\n- `src/generation/executor.ts` — Handles AIServiceError codes for failure modes, extracts error messages for progress reporting\n\n**Imports from:**\n- `src/orchestration/trace.ts` — ITraceWriter interface for trace event emission\n- `src/config/schema.ts` — AIServiceOptions validation schema\n\n**Exports to:**\n- `src/cli/*.ts` — All CLI commands import AIService, createBackendRegistry, resolveBackend from `./ai/index.js`\n### change-detection/\n<!-- Generated by agents-reverse-engineer -->\n\n# change-detection\n\nGit-based change detection via simple-git diff parsing and SHA-256 content hashing for incremental documentation updates.\n\n## Contents\n\n**[detector.ts](./detector.ts)** — `isGitRepo()`, `getCurrentCommit()`, `getChangedFiles()` with git diff status parsing (A/M/D/R), `computeContentHash()` SHA-256 hashing, uncommitted change merge via `git.status()`.\n\n**[types.ts](./types.ts)** — `ChangeType`, `FileChange`, `ChangeDetectionResult`, `ChangeDetectionOptions` interfaces defining change detection contracts.\n\n**[index.ts](./index.ts)** — Barrel export aggregating detector functions and types.\n\n## Change Detection Algorithm\n\n`getChangedFiles()` executes `git diff --name-status -M <baseCommit>..HEAD` parsing output format `STATUS\\tFILE` or `STATUS\\tOLD\\tNEW`. Status code mapping: `'A'` → `'added'`, `'M'` → `'modified'`, `'D'` → `'deleted'`, `'R*'` → `'renamed'` with `oldPath` extracted from second tab-delimited field. Rename detection uses `-M` flag with 50% similarity threshold.\n\nWhen `ChangeDetectionOptions.includeUncommitted` is true, merges `git.status()` results reading `status.modified`, `status.deleted`, `status.not_added`, `status.staged` arrays. Deduplicates via `changes.some(c => c.path === file)` predicate to prevent duplicate `FileChange` entries for files in both committed diff and working tree.\n\n## Content Hashing\n\n`computeContentHash()` reads file via Node.js `readFile()`, computes SHA-256 via `createHash('sha256').update(content).digest('hex')`. Hash stored in `.sum` YAML frontmatter `content_hash` field. `computeContentHashFromString()` provides synchronous variant for pre-loaded content avoiding redundant disk reads.\n\n## Integration Points\n\nConsumed by `src/update/orchestrator.ts` to compute `filesToAnalyze` by comparing `computeContentHash()` results against `.sum` frontmatter `content_hash` values. Supports non-git workflows via `computeContentHash()` fallback when `isGitRepo()` returns false.\n\n## Behavioral Contracts\n\n**Git diff status codes:**\n- `A` — added file\n- `M` — modified file\n- `D` — deleted file\n- `R*` — renamed file with similarity score (e.g., `R100`)\n\n**Rename detection similarity threshold:** 50% (implicit via `git diff -M`)\n\n**SHA-256 hash format:** Hex-encoded string matching regex `/^[a-f0-9]{64}$/`\n### cli/\n<!-- Generated by agents-reverse-engineer -->\n\n# cli\n\nCommand entry points implementing CLI argument parsing, command routing, and orchestration of AI-driven documentation generation workflows (init, discover, generate, update, clean, specify) via backend abstraction, concurrent execution pools, and incremental hash-based updates.\n\n## Commands\n\n### [clean.ts](./clean.ts)\n`cleanCommand(targetPath, {dryRun})` deletes `.sum`, `.annex.md`, `AGENTS.md` (marker-filtered), `CLAUDE.md`, `GENERATION-PLAN.md` via parallel `fast-glob` discovery, restores `AGENTS.local.md` → `AGENTS.md`, logs deletions/skips.\n\n### [discover.ts](./discover.ts)\n`discoverCommand(targetPath, {tracer, debug})` walks directory via `discoverFiles()` filter chain, writes file list to `progress.log`, generates `GENERATION-PLAN.md` via `buildExecutionPlan()` post-order traversal, emits `discovery:start`/`discovery:end` trace events.\n\n### [generate.ts](./generate.ts)\n`generateCommand(targetPath, {dryRun, concurrency, failFast, debug, trace})` orchestrates three-phase pipeline: concurrent `.sum` file analysis via `CommandRunner.executeGenerate()`, directory `AGENTS.md` aggregation, root document synthesis (`CLAUDE.md`/`GEMINI.md`/`OPENCODE.md`), exits with codes 0 (success), 1 (partial failure), 2 (total failure).\n\n### [index.ts](./index.ts)\nCLI entry point with shebang `#!/usr/bin/env node`, implements `parseArgs()` flag parser supporting `--dry-run`, `--concurrency <n>`, `--fail-fast`, `--debug`, `--trace`, `--uncommitted`, `--force`, `--multi-file`, routes to command modules, triggers `runInstaller()` on install/uninstall/installer flags.\n\n### [init.ts](./init.ts)\n`initCommand(root, {force})` creates `.agents-reverse-engineer/config.yaml` via `writeDefaultConfig()`, checks `configExists()` to prevent overwrites unless `force` enabled, exits with code 1 on `EACCES`/`EPERM` permission errors.\n\n### [specify.ts](./specify.ts)\n`specifyCommand(targetPath, {output, force, dryRun, multiFile, debug, trace})` synthesizes project specs from `AGENTS.md` corpus via `collectAgentsDocs()`, auto-generates missing docs via `generateCommand()`, invokes `AIService.call()` with 600s minimum timeout, writes via `writeSpec()` to `specs/SPEC.md` or custom path.\n\n### [update.ts](./update.ts)\n`updateCommand(targetPath, {uncommitted, dryRun, concurrency, failFast, debug, trace})` computes delta via SHA-256 hash comparison in `preparePlan()`, cleans orphaned artifacts, regenerates `.sum` for changed files via `runner.executeUpdate()`, rebuilds `AGENTS.md` for `affectedDirs` sequentially, logs to `progress.log`.\n\n## Execution Patterns\n\n**Dry-run mode** (`--dry-run`): clean/generate/update/specify commands display plans without filesystem writes or AI calls. generate shows file/directory/root counts with `buildExecutionPlan()`, specify estimates tokens via `Math.ceil(totalChars / 4) / 1000`.\n\n**Progress monitoring**: generate/update/specify create `ProgressLog.create(absolutePath)` writing to `.agents-reverse-engineer/progress.log` with ISO timestamps, file counts, task status. Enables `tail -f` real-time monitoring.\n\n**Trace emission** (`--trace`): generate/update/specify instantiate `createTraceWriter()` before config loading, pass `tracer` to all orchestrator calls, emit NDJSON events (`phase:start`/`end`, `worker:start`/`end`, `task:pickup`/`done`, `subprocess:spawn`/`exit`, `retry`), call `cleanupOldTraces()` after `tracer.finalize()`.\n\n**Backend resolution**: generate/update/specify call `createBackendRegistry()`, `resolveBackend(registry, config.ai.backend)`, catch `AIServiceError` with `code === 'CLI_NOT_FOUND'`, print `getInstallInstructions(registry)`, exit with code 2.\n\n**AI service lifecycle**: generate/update/specify instantiate `AIService(backend, {timeoutMs, maxRetries, model, telemetry.keepRuns})`, call `setDebug(true)` if `--debug`, call `setSubprocessLogDir()` if `--trace`, invoke via `aiService.call()` or `runner.executeGenerate()`/`runner.executeUpdate()`, finalize via `aiService.finalize(absolutePath)` writing run logs to `.agents-reverse-engineer/logs/`.\n\n**Installer routing**: index.ts triggers `runInstaller()` on three conditions: (1) zero args (interactive mode), (2) installer flags (`--global`, `--local`, `--runtime`, `--force`) without command, (3) explicit `install`/`uninstall` command. Passes `parseInstallerArgs(args)` with `uninstall: true` for uninstall command.\n\n## Exit Code Strategy\n\n**generate/update**:\n- Code 2: total failure (`filesProcessed === 0 && filesFailed > 0`) or CLI not found\n- Code 1: partial failure (`filesFailed > 0` with some success)\n- Code 0: full success (`filesFailed === 0`)\n\n**specify**:\n- Code 2: `CLI_NOT_FOUND` error from `resolveBackend()`\n- Code 1: `SpecExistsError` (output exists without `--force`) or empty docs after auto-generation\n- Code 0: successful write\n\n**init**:\n- Code 1: `EACCES`/`EPERM` permission error or write failure\n- Code 0: config created successfully\n\n**clean**: No explicit exit codes (relies on thrown errors).\n\n## File Relationships\n\n- index.ts imports all command modules, routes via switch statement on `command` string\n- generate.ts imports `discoverFiles()` from `../discovery/run.js`, `createOrchestrator()` from `../generation/orchestrator.js`, `buildExecutionPlan()` from `../generation/executor.js`, `AIService` from `../ai/index.js`, `CommandRunner` from `../orchestration/index.js`\n- update.ts imports `createUpdateOrchestrator()` from `../update/index.js`, `buildDirectoryPrompt()` from `../generation/prompts/index.js`, `writeAgentsMd()` from `../generation/writers/agents-md.js`\n- specify.ts imports `collectAgentsDocs()` from `../generation/collector.js`, `buildSpecPrompt()` from `../specify/index.js`, calls `generateCommand()` from `./generate.js` for auto-generation fallback\n- clean.ts imports `GENERATED_MARKER` from `../generation/writers/agents-md.js` for user-authored AGENTS.md detection\n- discover.ts imports `formatExecutionPlanAsMarkdown()` from `../generation/executor.js` for `GENERATION-PLAN.md` rendering\n- init.ts imports `configExists()`, `writeDefaultConfig()` from `../config/loader.js`\n\n## Behavioral Contracts\n\n**Argument parsing** (index.ts `parseArgs()`):\n- Flags starting with `--` populate `flags: Set<string>` or `values: Map<string, string>` if next arg lacks `--` prefix\n- Short flags expanded: `-h` → `help`, `-g` → `global`, `-l` → `local`, `-V` → `version`\n- First non-flag arg becomes `command`, subsequent populate `positional: string[]`\n\n**Token estimation** (specify.ts dry-run):\n```typescript\nestimatedTokensK = Math.ceil(totalChars / 4) / 1000\n```\n\n**Plan formatting** (update.ts `formatPlan()`):\n- Status markers: `+` (added, green), `R` (renamed, blue), `M` (modified, yellow), `=` (unchanged, dim)\n- First run: `plan.isFirstRun === true` → suggests `are generate`\n- Empty plan: all counts zero → \"No changes detected since last run\"\n\n**Cleanup patterns** (clean.ts):\n- Parallel `fast-glob` with patterns: `**/*.sum`, `**/*.annex.md`, `**/AGENTS.md`, `**/AGENTS.local.md`\n- Ignore patterns: `['**/node_modules/**', '**/.git/**']`\n- Marker detection: `content.includes(GENERATED_MARKER)` where `GENERATED_MARKER = '<!-- Generated by agents-reverse-engineer -->'`\n\n**Telemetry summary** (specify.ts):\n```typescript\nsummaryLine = `Tokens: ${totalInputTokens} in / ${totalOutputTokens} out | Duration: ${(totalDurationMs/1000).toFixed(1)}s | Output: ${outputPath}`\n```\n\n**Subprocess timeout override** (specify.ts):\n```typescript\ntimeoutMs = Math.max(config.ai.timeoutMs, 600_000) // minimum 10 minutes\n```\n### config/\n<!-- Generated by agents-reverse-engineer -->\n\n# src/config\n\nConfiguration loading, validation, and default value computation for `.agents-reverse-engineer/config.yaml`, including Zod schema enforcement, platform-adaptive concurrency calculation, and annotated YAML file generation with memory-bounded worker pool sizing.\n\n## Contents\n\n### Schema Definition\n\n**[schema.ts](./schema.ts)** — `ConfigSchema` root validator composed of `ExcludeSchema` (patterns/vendorDirs/binaryExtensions arrays), `OptionsSchema` (followSymlinks/maxFileSize), `OutputSchema` (colors), `AISchema` (backend/model/timeoutMs/maxRetries/concurrency/telemetry). Exports inferred types: `Config`, `ExcludeConfig`, `OptionsConfig`, `OutputConfig`, `AIConfig`. All schemas chain `.default()` enabling partial parse with full default population.\n\n**[defaults.ts](./defaults.ts)** — Exports `DEFAULT_VENDOR_DIRS` (18 directories: node_modules, .git, dist, build, target, .next, __pycache__, venv, .cargo, .gradle, .agents-reverse-engineer, .agents, .planning, .claude, .opencode, .gemini), `DEFAULT_EXCLUDE_PATTERNS` (lock files, dotfiles, logs, AI-generated docs, *.sum files), `DEFAULT_BINARY_EXTENSIONS` (26 extensions: images, archives, executables, media, documents, fonts, bytecode), `DEFAULT_MAX_FILE_SIZE` (1048576 bytes). `getDefaultConcurrency()` computes `clamp(cores × 5, MIN=2, min(memCap, MAX=20))` where `memCap = floor((totalMemGB × 0.5) / 0.512)` enforces memory constraint allocating 50% RAM divided by 512MB subprocess heap limit.\n\n**[loader.ts](./loader.ts)** — `loadConfig(root, options?)` reads `config.yaml` from `.agents-reverse-engineer/`, parses via `yaml.parse()`, validates with `ConfigSchema`, returns defaults on `ENOENT`, throws `ConfigError` wrapping `ZodError` with formatted issue paths. Emits `config:loaded` trace events with configPath/model/concurrency fields. `writeDefaultConfig(root)` creates annotated YAML with five comment-delimited sections (exclusions, discovery options, output formatting, AI service), interpolates defaults via spread operators, quotes patterns containing YAML metacharacters `[*{}\\[\\]?,:#&!|>'\"%@\\`]` via `yamlScalar()`, comments out concurrency line showing override syntax.\n\n## Configuration Structure\n\nThe config file divides into four sections:\n\n- **exclude**: File/directory exclusion rules with glob patterns, vendor directory names, binary extension list\n- **options**: Discovery behavior toggles (followSymlinks) and thresholds (maxFileSize in bytes)\n- **output**: Terminal formatting preferences (ANSI colors)\n- **ai**: AI service configuration (backend selection, model override, subprocess timeout, retry limits, worker pool concurrency, telemetry retention)\n\n## Memory-Aware Concurrency\n\n`getDefaultConcurrency()` implements memory-bounded worker pool sizing preventing RAM exhaustion where `cores × 5` spawns too many 512MB subprocesses. Computes `memCap = floor((totalMemGB × 0.5) / 0.512)` limiting concurrent subprocesses to fit within 50% memory budget, applies `Math.min(computed, memCap, MAX_CONCURRENCY)` respecting memory constraint alongside schema maximum. Fallback uses `Infinity` memCap when `totalMemGB ≤ 1` to avoid division-by-zero on resource-constrained systems.\n\n## Behavioral Contracts\n\n### Schema Validation Constraints\n\n- AISchema concurrency: `z.number().min(1).max(20)` enforces 1-20 worker pool range\n- OptionsSchema maxFileSize: `z.number().positive()` rejects zero/negative thresholds\n- AISchema backend: `z.enum(['claude', 'gemini', 'opencode', 'auto'])` limits backend values\n- AISchema telemetry.keepRuns: `z.number().min(0)` allows zero for unlimited retention\n\n### YAML Metacharacter Pattern\n\n`yamlScalar()` tests input against `/[*{}\\[\\]?,:#&!|>'\"%@\\`]/` pattern, double-quotes and backslash-escapes `\\` → `\\\\`, `\"` → `\\\"` when matched.\n\n### Default Formulas\n\n- Concurrency: `clamp(cores × 5, 2, min(floor((totalMemGB × 0.5) / 0.512), 20))`\n- Memory cap: `floor((os.totalmem() / 1024³ × 0.5) / 0.512)` concurrent processes\n- Timeout default: `300_000` milliseconds (5 minutes)\n\n### Error Message Format\n\nConfigError validation failures: `\"ai.concurrency: Expected number\"` (field path colon-separated from Zod issue message, joined by newlines for multiple issues).\n\n### Trace Event Schema\n\n`config:loaded` payload: `{ type: 'config:loaded', configPath: string, model: string | null, concurrency: number }` emitted twice per `loadConfig()` call (once for file path via `path.relative(root, configPath)`, once for defaults with literal `'(defaults)'`).\n\n## File Relationships\n\n- **schema.ts** → **defaults.ts**: Imports `DEFAULT_VENDOR_DIRS`, `DEFAULT_BINARY_EXTENSIONS`, `DEFAULT_MAX_FILE_SIZE`, `DEFAULT_EXCLUDE_PATTERNS`, `getDefaultConcurrency` for `.default()` chaining\n- **loader.ts** → **schema.ts**: Imports `ConfigSchema` for parse, `Config` type for return annotation\n- **loader.ts** → **defaults.ts**: Imports all defaults for YAML interpolation in `writeDefaultConfig()`\n- **loader.ts** → **../orchestration/trace.ts**: Imports `ITraceWriter` for `config:loaded` event emission\n### discovery/\n<!-- Generated by agents-reverse-engineer -->\n\n# src/discovery\n\n**Gitignore-aware file discovery system executing four-stage filter chain (gitignore, vendor, binary, custom) over fast-glob traversal results with bounded-concurrency processing (30 workers), early-termination optimization, and per-filter telemetry tracking.**\n\n## Contents\n\n**[run.ts](./run.ts)** — `discoverFiles()` orchestrates filter chain construction via `createGitignoreFilter()`, `createVendorFilter()`, `createBinaryFilter()`, `createCustomFilter()`, invokes `walkDirectory()` with `followSymlinks` flag, delegates filtering to `applyFilters()` with trace/debug options.\n\n**[types.ts](./types.ts)** — Defines `FileFilter` interface (`name: string`, `shouldExclude(): boolean|Promise<boolean>`), `FilterResult` with `included: string[]` and `excluded: ExcludedFile[]`, `WalkerOptions` for traversal config, `ExcludedFile` audit record.\n\n**[walker.ts](./walker.ts)** — `walkDirectory()` wraps `fast-glob` with `absolute: true`, `onlyFiles: true`, `dot: true` (dotfiles), `followSymbolicLinks: false` (default), `ignore: ['**/.git/**']` hardcoded, `suppressErrors: true` (permission denied).\n\n## Architecture\n\n### Filter Chain Execution Model\n\n**Composition**: `discoverFiles()` creates four filters in fixed order before applying to walked files. No in-walker filtering per module comment in `walker.ts`.\n\n**Orchestration**: `applyFilters()` (in `filters/index.ts`) spawns 30 workers sharing `files.entries()` iterator. Sequential filter evaluation per file with early termination on first `shouldExclude() === true`.\n\n**Result Preservation**: Workers collect `{ index, file, excluded? }` tuples, sort by original index to maintain discovery order, segregate into `included`/`excluded` arrays.\n\n### Filter Order\n\n1. **Gitignore** — Async `.gitignore` parser via `ignore` library with relative path normalization\n2. **Vendor** — Third-party directories (`node_modules`, `.git`, `dist`, `build`, `__pycache__`, `.next`, `venv`, `.venv`, `target`)\n3. **Binary** — Extension fast path (80+ extensions) → size threshold (1MB) → `isBinaryFile()` content analysis\n4. **Custom** — User glob patterns from `config.exclude.patterns` via `ignore` library\n\n### Decoupling Patterns\n\n**DiscoveryConfig Interface**: Structural subset type with `exclude: {vendorDirs, binaryExtensions, patterns}` and `options: {maxFileSize, followSymlinks}` allows `run.ts` to accept full `Config` object without circular dependency on `src/config/schema.ts`.\n\n**Factory Abstraction**: Filter creators (`createGitignoreFilter`, `createVendorFilter`, `createBinaryFilter`, `createCustomFilter`) return uniform `FileFilter` interface enabling chain composition.\n\n**Statistics Aggregation**: `applyFilters()` returns `Map<string, {matched, rejected}>` keyed by filter name for telemetry without coupling to filter implementations.\n\n## Behavioral Contracts\n\n### Binary Detection Thresholds\n\n- **Extension fast path**: 80+ extensions across images/archives/executables/media/documents/fonts/compiled/database\n- **Size threshold**: `DEFAULT_MAX_FILE_SIZE = 1048576` (1MB)\n- **Content analysis**: `isBinaryFile()` fallback for unknown extensions\n\n### Concurrency Limit\n\n`CONCURRENCY = 30` workers in `applyFilters()` prevents file descriptor exhaustion during I/O-heavy binary detection.\n\n### Vendor Directories\n\n`DEFAULT_VENDOR_DIRS`: `['node_modules', 'vendor', '.git', 'dist', 'build', '__pycache__', '.next', 'venv', '.venv', 'target']`\n\n### Glob Configuration\n\n- `absolute: true` — returns full paths\n- `onlyFiles: true` — excludes directories\n- `dot: true` — includes dotfiles\n- `followSymbolicLinks: false` — default symlink handling\n- `ignore: ['**/.git/**']` — hardcoded `.git` exclusion\n\n### Path Normalization\n\nAll filters use `path.relative(normalizedRoot, absolutePath)` before exclusion tests, with guards against `'..'` prefix or empty string.\n\n## Subdirectories\n\n**[filters/](./filters/)** — Five filter implementations: `gitignore.ts` (pattern matching via `ignore` library), `vendor.ts` (third-party directory detection), `binary.ts` (extension/size/content analysis), `custom.ts` (user glob patterns), `index.ts` (bounded-concurrency orchestrator with telemetry).\n\n## Integration Points\n\n**Consumed by**: `src/cli/discover.ts`, `src/cli/generate.ts`, `src/cli/update.ts` invoke `discoverFiles()` as single entry point.\n\n**Configuration surface**: `DiscoveryConfig` threaded from `src/config/schema.ts` YAML parsing with fields `exclude.vendorDirs`, `exclude.binaryExtensions`, `exclude.patterns`, `options.maxFileSize`, `options.followSymlinks`.\n\n**Telemetry integration**: Accepts `ITraceWriter` via `DiscoverFilesOptions.tracer`, emits `filter:applied` events with per-filter `filesMatched`/`filesRejected` counts.\n\n**Debug output**: `options.debug` enables `console.error()` logging for filters with non-zero rejection counts.\n### generation/\n<!-- Generated by agents-reverse-engineer -->\n\n# generation\n\n**Orchestrates three-phase documentation generation pipeline: concurrent file analysis via `GenerationOrchestrator.createFileTasks()`, post-order directory synthesis via `buildExecutionPlan()` with depth-sorted traversal, and root document synthesis via `collectAgentsDocs()` aggregation, enforcing dependency graphs through `ExecutionTask.dependencies` arrays and completion predicates via `isDirectoryComplete()`.**\n\n## Contents\n\n### Pipeline Orchestration\n\n**[orchestrator.ts](./orchestrator.ts)** — `GenerationOrchestrator` prepares files via `prepareFiles(discoveryResult)` reading content with `readFile()`, creates file tasks via `createFileTasks(files)` calling `buildFilePrompt()` for each file, creates directory tasks via `createDirectoryTasks(files)` grouping by `path.dirname()`, emits trace events (`phase:start`, `plan:created`, `phase:end`) via `ITraceWriter`, clears `PreparedFile.content` after prompt embedding to free memory, returns `GenerationPlan` with `files`, `tasks`, `complexity` from `analyzeComplexity()`. `buildProjectStructure()` formats directory tree as indented text for AI context.\n\n**[executor.ts](./executor.ts)** — `buildExecutionPlan()` constructs dependency graph from `GenerationPlan`: populates `directoryFileMap` extracting directories via `path.dirname()`, creates file tasks with `id: \"file:{path}\"`, sorts files by `getDirectoryDepth(path.dirname())` descending (deepest first), sorts directories by depth descending, creates directory tasks with `dependencies: fileTaskIds`, creates root tasks with `dependencies: allDirTaskIds`, returns `ExecutionPlan` with `fileTasks`/`directoryTasks`/`rootTasks` arrays. `isDirectoryComplete()` checks all files have `.sum` outputs via `sumFileExists()`, `getReadyDirectories()` identifies directories eligible for `AGENTS.md` generation, `formatExecutionPlanAsMarkdown()` generates `GENERATION-PLAN.md` with checkbox lists grouped by phase and depth.\n\n**[collector.ts](./collector.ts)** — `collectAgentsDocs()` recursively walks project tree via `readdir(withFileTypes)`, collects files named exactly `AGENTS.md`, skips 13 directories in `SKIP_DIRS` set (`node_modules`, `.git`, `.agents-reverse-engineer`, `vendor`, `dist`, `build`, `__pycache__`, `.next`, `venv`, `.venv`, `target`, `.cargo`, `.gradle`), returns `AgentsDocs` array sorted by `relativePath` via `localeCompare()`. `collectAnnexFiles()` uses identical traversal logic for files ending `.annex.md`. Gracefully handles permission-denied via silent exception catch.\n\n**[complexity.ts](./complexity.ts)** — `analyzeComplexity()` computes `ComplexityMetrics` with `fileCount`, `directoryDepth` via `calculateDirectoryDepth()` (splits `path.relative()` by `path.sep`, tracks max), `directories` set via `extractDirectories()` (walks upward via repeated `path.dirname()` until root), `files` array. Consumed by `GenerationOrchestrator.createPlan()` for complexity warnings.\n\n**[types.ts](./types.ts)** — `AnalysisResult` with `summary: string` and `metadata: SummaryMetadata`, `SummaryMetadata` with `purpose`, `criticalTodos`, `relatedFiles`, `SummaryOptions` with `targetLength` discriminant (`'short' | 'standard' | 'detailed'`) and `includeCodeSnippets` boolean. Maps to `.sum` YAML frontmatter schema.\n\n## Subdirectories\n\n**[prompts/](./prompts/)** — Prompt template constants (`FILE_SYSTEM_PROMPT`, `DIRECTORY_SYSTEM_PROMPT`, `ROOT_SYSTEM_PROMPT`) with density rules (every sentence references identifiers), filler phrase prohibition (`\"this file\"`, `\"provides\"`, `\"responsible for\"`), behavioral contract extraction (verbatim regex/constants). Builders (`buildFilePrompt`, `buildDirectoryPrompt`, `buildRootPrompt`) substitute placeholders (`{{FILE_PATH}}`, `{{CONTENT}}`, `{{LANG}}`), aggregate child `.sum` files via `readSumFile()`, extract import maps via `extractDirectoryImports()`, preserve user content from `AGENTS.local.md`, switch to update prompts (`FILE_UPDATE_SYSTEM_PROMPT`, `DIRECTORY_UPDATE_SYSTEM_PROMPT`) when `existingSum`/`existingAgentsMd` present.\n\n**[writers/](./writers/)** — YAML frontmatter serialization (`writeSumFile`, `readSumFile`, `getSumPath`, `sumFileExists`) with SHA-256 hash tracking, inline/multi-line array formatting, annex file generation (`writeAnnexFile`, `getAnnexPath`) for reproduction-critical source content. `writeAgentsMd()` preserves user content via `AGENTS.local.md` rename, marker-based detection (`isGeneratedAgentsMd`), prepends preserved sections with `---` separator.\n\n## Post-Order Traversal\n\n`buildExecutionPlan()` enforces children-before-parents ordering via two sorts:\n1. **File tasks**: `getDirectoryDepth(path.dirname(a.path)) - getDirectoryDepth(path.dirname(b.path))` descending (deepest first)\n2. **Directory tasks**: `getDirectoryDepth(dirB) - getDirectoryDepth(dirA)` descending\n\nEnsures child `AGENTS.md` files exist before parent directory prompts consume them via `collectAgentsDocs()` in `buildDirectoryPrompt()`.\n\n## Dependency Graph\n\n`ExecutionTask.dependencies: string[]` enforces Phase 1 → Phase 2 → Phase 3 sequencing:\n- **File tasks**: empty dependencies (parallel eligible)\n- **Directory tasks**: depend on all file task IDs in directory (`id: \"file:{path}\"`)\n- **Root tasks**: depend on all directory task IDs (`id: \"dir:{path}\"`)\n\n`isDirectoryComplete()` verifies all `expectedFiles` have `.sum` outputs before allowing directory task execution.\n\n## Prompt Placeholder Pattern\n\nDirectory and root tasks store placeholder prompts (`\"Built at runtime by buildDirectoryPrompt()\"`) for plan display and dependency tracking. Actual prompts constructed at execution time in `src/orchestration/runner.ts` via `buildDirectoryPrompt()` and `buildRootPrompt()`, consuming child `.sum` files and subdirectory `AGENTS.md` not available during plan creation.\n\n## Trace Events\n\n`GenerationOrchestrator.createPlan()` emits:\n- `phase:start` with `{ type: 'phase:start', phase: 'plan-creation', taskCount, concurrency: 1 }`\n- `plan:created` with `{ type: 'plan:created', planType: 'generate', fileCount, taskCount: tasks.length + 1 }` (accounting for root task added by `buildExecutionPlan()`)\n- `phase:end` with `{ type: 'phase:end', phase: 'plan-creation', durationMs, tasksCompleted: 1, tasksFailed: 0 }`\n\n## Integration Points\n\nImports `buildFilePrompt` from `./prompts/index.js` for Phase 1 task creation. Imports `sumFileExists` from `./writers/sum.js` for completion checking. Imports `analyzeComplexity` from `./complexity.js` for metrics computation. Imports `ITraceWriter` from `../orchestration/trace.js` for event emission. Imports `Config` from `../config/schema.js` and `DiscoveryResult` from `../types/index.js`.\n\nConsumed by `src/orchestration/runner.ts` which executes `ExecutionPlan` via worker pool, calling `AIService` with runtime-constructed prompts for directory and root tasks.\n\n## Memory Management\n\nAfter `createFileTasks()` embeds file content into prompts, `createPlan()` clears `PreparedFile.content` via `(file as { content: string }).content = ''` to free memory. Comment notes runner re-reads files from disk during execution.\n### imports/\n<!-- Generated by agents-reverse-engineer -->\n\n# src/imports/\n\nStatic import analysis subsystem extracting TypeScript/JavaScript import statements via regex parsing to construct dependency graphs consumed by Phase 1 file analysis and Phase 2 directory aggregation prompts.\n\n## Contents\n\n### [extractor.ts](./extractor.ts)\nRegex-based parser matching static import statements via `IMPORT_REGEX` (`/^import\\s+(type\\s+)?(?:\\{([^}]*)\\}|(\\*\\s+as\\s+\\w+)|(\\w+))\\s+from\\s+['\"]([^'\"]+)['\"]/gm`). Exports `extractImports()` returning `ImportEntry[]` with `specifier`, `symbols[]`, `typeOnly` fields. Exports `extractDirectoryImports()` reading first 100 lines per file, classifying imports as `internal` (`./` prefix) or `external` (`../` prefix), filtering bare package specifiers and `node:` built-ins. Exports `formatImportMap()` converting `FileImports[]` to human-readable text blocks for LLM prompt embedding.\n\n### [types.ts](./types.ts)\nDefines `ImportEntry` interface with `specifier: string`, `symbols: string[]`, `typeOnly: boolean` representing single import statement. Defines `FileImports` interface with `fileName: string`, `externalImports: ImportEntry[]`, `internalImports: ImportEntry[]` partitioning imports by locality.\n\n### [index.ts](./index.ts)\nBarrel export re-exporting `extractImports`, `extractDirectoryImports`, `formatImportMap` from `extractor.ts` and `ImportEntry`, `FileImports` from `types.ts`.\n\n## Data Flow\n\n1. **File Analysis (Phase 1):** `extractDirectoryImports()` invoked by `src/generation/prompts/builder.ts` → reads first 100 lines per source file → applies `IMPORT_REGEX` → classifies imports by locality → returns `FileImports[]`\n2. **Prompt Embedding:** `formatImportMap()` serializes `FileImports[]` → produces text block with filename headers and indented specifier-symbol pairs → embedded in AI prompts via `buildFileAnalysisPrompt()`\n3. **Directory Aggregation (Phase 2):** Import maps from child files aggregated during directory-level `AGENTS.md` generation to provide cross-module dependency context\n\n## Import Classification\n\n| Category | Pattern | Included | Purpose |\n|----------|---------|----------|---------|\n| **Internal** | `./filename` | Yes | Same-directory coupling (tight cohesion signals) |\n| **External** | `../path/module` | Yes | Cross-directory dependencies (module boundaries) |\n| **Package** | `'react'` | No | Third-party npm dependencies (not project structure) |\n| **Built-in** | `'node:fs'` | No | Runtime APIs (not project structure) |\n\n## Performance Optimization\n\nReads only first 100 lines per file via `content.split('\\n').slice(0, 100).join('\\n')` before regex matching, assuming ES module convention of top-level imports. Skips unreadable files silently via empty catch blocks in `extractDirectoryImports()`.\n\n## Integration Points\n\n- **Consumed by:** `src/generation/prompts/builder.ts` embeds import maps in file analysis prompts via `extractDirectoryImports()` + `formatImportMap()`\n- **Prompts:** `src/generation/prompts/templates.ts` includes import map sections in Phase 1 file analysis and Phase 2 directory aggregation templates\n- **Output format:** Human-readable text blocks with filename headers (`external imports from foo.ts:`) and indented specifier-symbol pairs (`  ../ai/index.js → AIService`)\n### installer/\n<!-- Generated by agents-reverse-engineer -->\n\n# src/installer\n\nOrchestrates npx-based installation of ARE commands and hooks across Claude Code, OpenCode, and Gemini CLI runtimes with interactive prompts, platform-specific path resolution, settings.json manipulation, and uninstallation workflows.\n\n## Contents\n\n### Core Orchestration\n\n**[index.ts](./index.ts)** — Entry point exposing `runInstaller()` and `parseInstallerArgs()` for CLI integration. Parses short/long flags (`-g`/`--global`, `-l`/`--local`, `--runtime`, `--force`, `--quiet`, `--uninstall`), validates non-interactive mode requirements, dispatches to `runInstall()` or `runUninstall()` based on arguments, displays results via `displayInstallResults()` and `displayUninstallResults()`. Re-exports all types and functions from sibling modules for barrel pattern.\n\n**[operations.ts](./operations.ts)** — Implements `installFiles()`, `verifyInstallation()`, `registerHooks()`, `registerPermissions()` for file copying, settings.json hook registration (SessionStart/SessionEnd events with nested HookEvent arrays for Claude, flat GeminiHook arrays for Gemini), and bash permission patterns (`ARE_PERMISSIONS` array: `'Bash(npx agents-reverse-engineer@latest init*)'` through `clean*`, `'Bash(rm -f .agents-reverse-engineer/progress.log*)'`, `'Bash(sleep *)'`). Resolves bundled hooks via `getBundledHookPath()` navigating from `dist/installer/operations.js` up to `hooks/dist/`. Writes `ARE-VERSION` file via `getPackageVersion()`.\n\n**[uninstall.ts](./uninstall.ts)** — Mirrors installation logic with `uninstallFiles()`, `unregisterHooks()`, `unregisterPermissions()`, `deleteConfigFolder()`. Removes command templates, hooks, plugins, settings.json entries, ARE-VERSION file. Implements `cleanupAreSkillDirs()` for Claude skills format, `cleanupEmptyDirs()` for recursive bottom-up directory removal, `cleanupLegacyGeminiFiles()` for obsolete Markdown/TOML formats. Repurposes `InstallerResult.filesCreated` to track deleted files, `hookRegistered` for unregistration status.\n\n### Path Resolution\n\n**[paths.ts](./paths.ts)** — Exports `getRuntimePaths()`, `resolveInstallPath()`, `getSettingsPath()`, `getAllRuntimes()`, `isRuntimeInstalledLocally()`, `isRuntimeInstalledGlobally()`, `getInstalledRuntimes()`. Resolves global/local paths with environment overrides: `CLAUDE_CONFIG_DIR` (`~/.claude` fallback), `OPENCODE_CONFIG_DIR` or `XDG_CONFIG_HOME/opencode` (`~/.config/opencode` fallback), `GEMINI_CONFIG_DIR` (`~/.gemini` fallback). Returns `RuntimePaths` with `global`, `local`, `settingsFile` fields.\n\n### User Interface\n\n**[prompts.ts](./prompts.ts)** — Provides `selectRuntime()`, `selectLocation()`, `confirmAction()`, `isInteractive()` with `arrowKeySelect()` for TTY mode (arrow-key navigation via `readline.emitKeypressEvents()` + `process.stdin.setRawMode(true)` with ANSI escape sequences `\\x1b[${n}A`, `\\x1b[2K`, `\\x1b[1B` for rendering) and `numberedSelect()` fallback for piped input. Registers `cleanupRawMode()` on `process.on('exit')` and `process.on('SIGINT')` to restore terminal state via `setRawMode(false)` + `pause()`.\n\n**[banner.ts](./banner.ts)** — Exports `displayBanner()` (Unicode box-drawing ASCII art logo U+2588/U+2550-U+2557), `showHelp()`, `showSuccess()`/`showError()`/`showWarning()`/`showInfo()` (prefix symbols: green ✓, red ✗, yellow !, cyan >), `showNextSteps()` (lists commands `/are-help`, `/are-init`, `/are-discover`, `/are-generate`, `/are-update`, `/are-specify`, `/are-clean` with GitHub documentation link).\n\n### Type Definitions\n\n**[types.ts](./types.ts)** — Defines `Runtime` (`'claude' | 'opencode' | 'gemini' | 'all'`), `Location` (`'global' | 'local'`), `InstallerArgs` (`runtime?`, `global`, `local`, `uninstall`, `force`, `help`, `quiet`), `InstallerResult` (`success`, `runtime: Exclude<Runtime, 'all'>`, `location`, `filesCreated`, `filesSkipped`, `errors`, `hookRegistered?`, `versionWritten?`), `RuntimePaths` (`global`, `local`, `settingsFile`).\n\n## Subdirectories\n\nNone — flat structure with seven TypeScript modules.\n\n## Architecture\n\n### Two-Phase Workflow\n\n**Installation (runInstall):**\n1. Argument parsing via `parseInstallerArgs()` with short/long flag normalization\n2. Interactive prompts via `selectRuntime()`/`selectLocation()` when TTY detected and values missing\n3. Template copying via `getTemplatesForRuntime()` → `getClaudeTemplates()`/`getOpenCodeTemplates()`/`getGeminiTemplates()` from `src/integration/templates.ts`\n4. Hook/plugin installation: Claude/Gemini to `hooks/` subdirectory using `ARE_HOOKS` array (currently empty), OpenCode to `plugins/` using `ARE_PLUGINS` array\n5. Settings.json modification: `registerHooks()` merges SessionStart/SessionEnd hooks with duplicate detection, `registerPermissions()` adds `ARE_PERMISSIONS` bash patterns to Claude's permissions.allow array\n6. Version file creation via `writeVersionFile()` for update checker hooks\n7. Verification via `verifyInstallation()` checking `existsSync()` for all filesCreated paths\n\n**Uninstallation (runUninstall):**\n1. Template path extraction with runtime prefix slicing (`template.path.split('/').slice(1).join('/')`)\n2. File deletion via `unlinkSync()` for templates, hooks, plugins, ARE-VERSION\n3. Settings.json cleanup: `unregisterHooks()` filters hook arrays via pattern matching (current format `node .claude/hooks/${filename}`, legacy `node hooks/${filename}`), `unregisterPermissions()` filters `ARE_PERMISSIONS` from permissions.allow\n4. Directory cleanup: `cleanupAreSkillDirs()` removes empty `are-*` skill directories, `cleanupEmptyDirs()` recursively removes empty parents up to runtime root, `cleanupLegacyGeminiFiles()` deletes obsolete `.md`/`.toml` formats\n5. Config folder deletion via `deleteConfigFolder()` only for local installations (`location === 'local'`)\n\n### Runtime-Specific Patterns\n\n**Claude Code:**\n- Global path: `~/.claude` (override: `CLAUDE_CONFIG_DIR`)\n- Commands: `.claude/skills/<command>/SKILL.md` with frontmatter `name: /are-<command>`\n- Hooks: `.claude/hooks/<filename>.js` registered in `settings.json` with nested structure `{ hooks: { SessionStart?: [{ hooks: [{ type: 'command', command: string }] }] } }`\n- Permissions: `settings.json` permissions.allow array with bash command patterns\n- Settings file: `~/.claude/settings.json`\n\n**OpenCode:**\n- Global path: `~/.config/opencode` (overrides: `OPENCODE_CONFIG_DIR` > `XDG_CONFIG_HOME/opencode`)\n- Commands: `.opencode/commands/<command>.md` with frontmatter `agent: build`\n- Plugins: `.opencode/plugins/<filename>.js` exporting async factory functions with `event['session.created']`/`event['session.deleted']` handlers\n- No settings file (plugin registration via file presence)\n\n**Gemini CLI:**\n- Global path: `~/.gemini` (override: `GEMINI_CONFIG_DIR`)\n- Commands: `.gemini/commands/<command>.toml` with `description`/`prompt` fields\n- Hooks: `.gemini/hooks/<filename>.js` registered in `settings.json` with flat structure `{ hooks: { SessionStart?: [{ name: string, type: 'command', command: string }] } }`\n- Settings file: `~/.gemini/settings.json`\n\n## Behavioral Contracts\n\n### Hook/Plugin Definitions\n\n**ARE_HOOKS (operations.ts):**\n```typescript\nconst ARE_HOOKS: HookDefinition[] = [\n  // Array intentionally empty — hooks disabled due to issues\n];\n```\n\n**ARE_PLUGINS (operations.ts):**\n```typescript\nconst ARE_PLUGINS: PluginDefinition[] = [\n  { srcFilename: 'opencode-are-check-update.js', destFilename: 'are-check-update.js' },\n  // are-session-end.js disabled\n];\n```\n\n**ARE_PERMISSIONS (operations.ts):**\n```typescript\nconst ARE_PERMISSIONS: string[] = [\n  'Bash(npx agents-reverse-engineer@latest init*)',\n  'Bash(npx agents-reverse-engineer@latest discover*)',\n  'Bash(npx agents-reverse-engineer@latest generate*)',\n  'Bash(npx agents-reverse-engineer@latest update*)',\n  'Bash(npx agents-reverse-engineer@latest clean*)',\n  'Bash(rm -f .agents-reverse-engineer/progress.log*)',\n  'Bash(sleep *)',\n];\n```\n\n**Hook command pattern (operations.ts):**\n```javascript\n`node ${runtimeDir}/hooks/${hookDef.filename}`\n```\n\n**Legacy hook patterns (uninstall.ts):**\n```typescript\nfunction getHookPatterns(runtimeDir: string): string[] {\n  return ARE_HOOKS.flatMap(hook => [\n    `node ${runtimeDir}/hooks/${hook.filename}`,  // Current format\n    `node hooks/${hook.filename}`,                 // Legacy format\n  ]);\n}\n```\n\n### ANSI Escape Sequences (prompts.ts)\n\n**Cursor control in arrowKeySelect:**\n- `\\x1b[${n}A` — Move cursor up n lines\n- `\\x1b[2K` — Clear entire line\n- `\\x1b[1B` — Move cursor down 1 line\n\n**Keypress handling:**\n- Up arrow: `key.name === 'up'` → `selectedIndex = Math.max(0, selectedIndex - 1)`\n- Down arrow: `key.name === 'down'` → `selectedIndex = Math.min(options.length - 1, selectedIndex + 1)`\n- Enter: `key.name === 'return'` → resolve with `options[selectedIndex].value`\n- Ctrl+C: `key.ctrl && key.name === 'c'` → `cleanupRawMode()` + `process.exit(0)`\n\n### Settings.json Schemas\n\n**Claude (operations.ts):**\n```typescript\ninterface SettingsJson {\n  hooks?: {\n    SessionStart?: HookEvent[];\n    SessionEnd?: HookEvent[];\n  };\n  permissions?: {\n    allow?: string[];\n    deny?: string[];\n  };\n}\n\ninterface HookEvent {\n  hooks: SessionHook[];\n}\n\ninterface SessionHook {\n  type: 'command';\n  command: string;\n}\n```\n\n**Gemini (operations.ts):**\n```typescript\ninterface GeminiSettingsJson {\n  hooks?: {\n    SessionStart?: GeminiHook[];\n    SessionEnd?: GeminiHook[];\n  };\n}\n\ninterface GeminiHook {\n  name: string;\n  type: 'command';\n  command: string;\n}\n```\n\n### Directory Cleanup Terminal Conditions (uninstall.ts)\n\n**Runtime root check in cleanupEmptyDirs:**\n```typescript\nconst baseName = path.basename(dirPath);\nif (['.claude', '.opencode', '.gemini', '.config'].includes(baseName)) {\n  return;\n}\n```\n\n**ARE skill directory pattern (uninstall.ts):**\n```typescript\nentry.startsWith('are-') && stats.isDirectory()\n```\n\n**Legacy Gemini file patterns (uninstall.ts):**\n```typescript\n// Markdown format\nentry.startsWith('are-') && entry.endsWith('.md')\n\n// TOML namespace directory\n'commands/are/*.toml'\n```\n\n## File Relationships\n\n**Orchestration chain:**\n1. `index.ts` parses args → calls `prompts.ts` selectors → calls `operations.ts` or `uninstall.ts`\n2. `operations.ts`/`uninstall.ts` call `paths.ts` for directory resolution\n3. `operations.ts` calls `src/integration/templates.ts` for command content\n4. `operations.ts` reads bundled hooks from `hooks/dist/` via `getBundledHookPath()`\n5. `operations.ts`/`uninstall.ts` manipulate settings.json via in-memory JSON parse/stringify\n6. `banner.ts` provides display functions consumed by `index.ts` for results rendering\n\n**Shared state:**\n- `prompts.ts` maintains module-level `rawModeActive` flag for terminal cleanup\n- `operations.ts` and `uninstall.ts` share `ARE_HOOKS`, `ARE_PLUGINS`, `ARE_PERMISSIONS` definitions\n- `paths.ts` provides single source of truth for runtime directory mappings\n\n**Type flow:**\n- `types.ts` defines discriminated union `Runtime` with `'all'` excluded via `Exclude<Runtime, 'all'>` in `InstallerResult.runtime`\n- `InstallerArgs` parsed in `index.ts`, threaded through `runInstall()`/`runUninstall()` to `operations.ts`/`uninstall.ts`\n- `RuntimePaths` returned by `paths.ts`, consumed by `operations.ts`/`uninstall.ts` for file path construction\n### integration/\n<!-- Generated by agents-reverse-engineer -->\n\n# src/integration\n\n**Platform-specific AI assistant integration layer detecting Claude Code/OpenCode/Gemini/Aider environments, generating command template files with progress-monitoring patterns, deploying bundled session hooks, and enforcing skip-if-exists safety with force override.**\n\n## Contents\n\n### Environment Detection\n\n**[detect.ts](./detect.ts)** — `detectEnvironments()` scans project root for `.claude/`, `.opencode/`, `.aider/`, `.gemini/` directories and `CLAUDE.md`/`.aider.conf.yml` marker files, returns `DetectedEnvironment[]` with `type`, `configDir`, `detected` fields. `hasEnvironment()` filters by `EnvironmentType`.\n\n### File Generation Orchestration\n\n**[generate.ts](./generate.ts)** — `generateIntegrationFiles()` orchestrates template instantiation by detecting/overriding environments via `detectEnvironments()`, retrieving templates via `getTemplatesForEnvironment()`, writing command files with `ensureDir()` + `writeFileSync()`, deploying bundled hooks for Claude via `readBundledHook()` from `hooks/dist/are-session-end.js`, returns `IntegrationResult[]` with `filesCreated`/`filesSkipped` arrays. `GenerateOptions` supports `dryRun`, `force` (overwrite existing), and `environment` (bypass auto-detection).\n\n### Template Library\n\n**[templates.ts](./templates.ts)** — Exports `getClaudeTemplates()`, `getOpenCodeTemplates()`, `getGeminiTemplates()` generating platform-specific command files for seven commands: `generate`, `update`, `init`, `discover`, `clean`, `specify`, `help`. Encapsulates `COMMANDS` object with markdown prompt content, `PLATFORM_CONFIGS` mapping `EnvironmentType` to `PlatformConfig` (command prefix, path structure, frontmatter rules), `buildTemplate()`/`buildGeminiToml()` factories performing placeholder substitution (`COMMAND_PREFIX`, `VERSION_FILE_PATH`). Command content patterns: background execution via `npx agents-reverse-engineer@latest`, progress polling with `.agents-reverse-engineer/progress.log` offset reads, `TaskOutput` checks with `block: false`, completion summaries with phase metrics.\n\n### Type Definitions\n\n**[types.ts](./types.ts)** — `EnvironmentType` union (`'claude' | 'opencode' | 'aider' | 'gemini'`), `DetectedEnvironment` interface (`type`, `configDir`, `detected`), `IntegrationTemplate` interface (`filename`, `path`, `content`), `IntegrationResult` interface (`environment`, `filesCreated`, `filesSkipped`).\n\n## Platform Configuration\n\n| Platform | Command Prefix | Path Pattern | Frontmatter | Version File |\n|----------|---------------|--------------|-------------|--------------|\n| **Claude** | `/are-` | `.claude/skills/are-{command}/SKILL.md` | `name: /are-{command}` | `.claude/ARE-VERSION` |\n| **OpenCode** | `/are-` | `.opencode/commands/are-{command}.md` | `agent: build` | `.opencode/ARE-VERSION` |\n| **Gemini** | `/are-` | `.gemini/commands/are-{command}.toml` | TOML `description`/`prompt` | `.gemini/ARE-VERSION` |\n| **Aider** | N/A | N/A (detection only) | N/A | N/A |\n\n## Command Template Behavior\n\n- **generate/update/specify**: 15s poll intervals, background execution via `run_in_background: true`, offset-based log tailing, three-phase progress reporting (discovery → file analysis → directory/root docs)\n- **discover**: 10s poll intervals, background execution, file count reporting\n- **init**: Synchronous execution, creates `.agents-reverse-engineer/config.yaml`\n- **clean**: Synchronous execution with STRICT RULES enforcing zero flag additions, deletion count reporting\n- **help**: Outputs command reference with platform-specific `COMMAND_PREFIX` placeholder substitution\n\n## Integration with Project\n\nConsumed by `src/installer/` for `npx agents-reverse-engineer --runtime <env>` workflow. `detectEnvironments()` validates presence before installation, `generateIntegrationFiles()` writes command files to `.claude/skills/`, `.opencode/commands/`, or `.gemini/commands/`. Bundled hook deployment via `hooks/dist/are-session-end.js` for Claude's SessionEnd lifecycle.\n### orchestration/\n<!-- Generated by agents-reverse-engineer -->\n\n# src/orchestration\n\n**Worker pool concurrency control, promise-chain serialized progress tracking, NDJSON trace emission, and three-phase pipeline orchestration executing concurrent file analysis, post-order directory aggregation, and sequential root synthesis with integrated quality validation, ETA calculation, and subprocess lifecycle tracing.**\n\n## Contents\n\n### Core Orchestration\n\n**[index.ts](./index.ts)** — Barrel export consolidating `runPool`, `ProgressReporter`, `ProgressLog`, `PlanTracker`, `createTraceWriter`, `cleanupOldTraces`, `CommandRunner`, and shared types (`FileTaskResult`, `RunSummary`, `ProgressEvent`, `CommandRunOptions`, `PoolOptions`, `TaskResult`, `ITraceWriter`, `TraceEvent`, `TraceEventPayload`).\n\n**[pool.ts](./pool.ts)** — Iterator-based worker pool executing `Array<() => Promise<T>>` tasks via shared `tasks.entries()` iterator consumed by N workers, preventing batch-stall anti-pattern where `Promise.all()` chunks idle workers waiting for slowest task. Emits `worker:start/end`, `task:pickup/done` trace events. Supports `failFast` via shared `aborted` boolean flag checked at loop start. Effective concurrency via `Math.min(options.concurrency, tasks.length)`.\n\n**[runner.ts](./runner.ts)** — `CommandRunner` orchestrates three-phase pipeline via `executeGenerate(plan)` and `executeUpdate(filesToAnalyze, projectRoot, config)`. Phase 1: concurrent file analysis via `runPool()` with `options.concurrency`, reads source via `readFile()`, caches in `sourceContentCache`, calls `aiService.call()`, computes `contentHash` via `computeContentHashFromString()`, strips preamble via `stripPreamble()`, extracts purpose via `extractPurpose()`, writes via `writeSumFile()`, detects `## Annex References` marker for `writeAnnexFile()`. Post-Phase 1: groups files by directory, runs quality checks concurrently (concurrency=10) via `checkCodeVsDoc(source, oldSum)` (stale), `checkCodeVsDoc(source, newSum)` (fresh), `checkCodeVsCode(filesForCodeVsCode)`, builds `InconsistencyReport`, clears `sourceContentCache`. Phase 2: groups `directoryTasks` by depth, processes in descending order (deepest first) with concurrency `Math.min(options.concurrency, dirsAtDepth.length)`, builds prompts via `buildDirectoryPrompt()`, writes via `writeAgentsMd()`. Post-Phase 2: validates phantom paths via `checkPhantomPaths(agentsMdPath, content, projectRoot)`. Phase 3: sequential root document synthesis (concurrency=1), strips conversational preamble before `writeFile()`. Returns `RunSummary` with counts from `aiService.getSummary()` plus `inconsistenciesCodeVsDoc`, `inconsistenciesCodeVsCode`, `phantomPaths`.\n\n### Progress Tracking\n\n**[progress.ts](./progress.ts)** — `ProgressReporter` streams colored console output with ETA calculation via moving average of last 10 `completionTimes` (window size 10), displays after 2+ completions. `ProgressLog` mirrors plain-text output to `.agents-reverse-engineer/progress.log` via promise-chain serialized writes with ANSI stripping (`/\\x1b\\[[0-9;]*m/g`). Static factory `ProgressLog.create(projectRoot)` constructs path. Methods: `onFileStart(filePath)` (`[X/Y] ANALYZING path`), `onFileDone(filePath, durationMs, tokensIn, tokensOut, model, cacheReadTokens?, cacheCreationTokens?)` (`[X/Y] DONE path Xs in/out tok model ~Ns remaining`), `onFileError(filePath, error)` (`[X/Y] FAIL path error`), `onDirectoryStart(dirPath)`, `onDirectoryDone()`, `onRootDone(docPath)`, `printSummary(summary)`. ETA formatted via `formatETA()` computing `avg * remaining` as `~Ns remaining` (<60s) or `~Mm Ss remaining` (≥60s).\n\n**[plan-tracker.ts](./plan-tracker.ts)** — `PlanTracker` maintains in-memory `GENERATION-PLAN.md` content, serializes concurrent checkbox updates via promise-chain writes. Constructor accepts `projectRoot` and `initialMarkdown`, computes `planPath` as `path.join(projectRoot, CONFIG_DIR, 'GENERATION-PLAN.md')`. `markDone(itemPath)` replaces `` `- [ ] \\`${itemPath}\\`` `` → `` `- [x] \\`${itemPath}\\`` ``, chains write to `writeQueue` promise. `initialize()` creates parent directory via `mkdir(..., {recursive: true})`, writes initial content. `flush()` awaits `writeQueue` completion.\n\n### Trace Infrastructure\n\n**[trace.ts](./trace.ts)** — `createTraceWriter(projectRoot, enabled)` factory returns `NullTraceWriter` (zero overhead) when disabled or `TraceWriter` appending NDJSON to `.agents-reverse-engineer/traces/trace-{timestamp}.ndjson` with promise-chain serialization. `TraceWriter` auto-populates `seq` (monotonic), `ts` (ISO 8601), `pid` (`process.pid`), `elapsedMs` (high-resolution `process.hrtime.bigint()` delta) on `emit(partial: TraceEventPayload)`. `finalize()` awaits `writeQueue`, closes `fd`. `cleanupOldTraces(projectRoot, keepCount=500)` removes old traces, keeps most recent 500.\n\n**Event types:** `phase:start/end` (phase, taskCount, concurrency, durationMs, tasksCompleted, tasksFailed), `worker:start/end` (workerId, phase, tasksExecuted), `task:pickup/done` (workerId, taskIndex, taskLabel, activeTasks, durationMs, success, error?), `task:start` (taskLabel, phase), `subprocess:spawn/exit` (childPid, command, taskLabel, exitCode, signal, durationMs, timedOut), `retry` (attempt, taskLabel, errorCode), `discovery:start/end` (targetPath, filesIncluded, filesExcluded, durationMs), `filter:applied` (filterName, filesMatched, filesRejected), `plan:created` (planType, fileCount, taskCount), `config:loaded` (configPath, model, concurrency).\n\n**[types.ts](./types.ts)** — Defines `FileTaskResult` (path, success, tokensIn, tokensOut, cacheReadTokens, cacheCreationTokens, durationMs, model, error?), `RunSummary` (version, filesProcessed, filesFailed, filesSkipped, totalCalls, totalInputTokens, totalOutputTokens, totalCacheReadTokens, totalCacheCreationTokens, totalDurationMs, errorCount, retryCount, totalFilesRead, uniqueFilesRead, inconsistenciesCodeVsDoc?, inconsistenciesCodeVsCode?, phantomPaths?, inconsistencyReport?), `ProgressEvent` (discriminated type: start|done|error|dir-done|root-done with filePath, index, total, durationMs?, tokensIn?, tokensOut?, model?, error?), `CommandRunOptions` (concurrency, failFast?, debug?, dryRun?, tracer?: ITraceWriter, progressLog?: ProgressLog), `PoolOptions` (concurrency, failFast?, tracer?, phaseLabel?, taskLabels?), `TaskResult<T>` (index, success, value?, error?).\n\n## Architecture\n\n### Shared-Iterator Concurrency\n\n`runPool()` creates `tasks.entries()` iterator shared across N workers via `for (const [index, task] of iterator)` loop. Workers race to pull tasks atomically via iterator protocol, preventing over-allocation where `Promise.all()` chunks spawn tasks eagerly. Effective concurrency via `Math.min(options.concurrency, tasks.length)` prevents idle workers when task count < pool size.\n\n### Promise-Chain Serialization\n\n`PlanTracker`, `ProgressLog`, `TraceWriter` serialize concurrent writes via `this.writeQueue = this.writeQueue.then(() => writeOp())` pattern. Each write operation chains onto tail of promise, forming sequential execution queue despite concurrent worker invocations. Errors caught via `.catch()` suppression (non-critical telemetry).\n\n### Three-Phase Pipeline\n\nPhase 1: concurrent file analysis via `runPool(fileTasks, {concurrency: options.concurrency})`, reads source via `readFile()` once, caches in `sourceContentCache` Map, calls `aiService.call()`, writes `.sum` via `writeSumFile()`, writes `.annex.md` if `## Annex References` detected. Post-Phase 1: groups files by directory, validates via `checkCodeVsDoc(source, oldSum)` (stale docs), `checkCodeVsDoc(source, newSum)` (LLM omissions), `checkCodeVsCode(files)` (duplicate exports) at concurrency=10, clears `sourceContentCache`.\n\nPhase 2: groups `directoryTasks` by depth (`task.metadata.depth`), processes depth levels sequentially in descending order (deepest first) via `Array.from(dirsByDepth.keys()).sort((a,b) => b-a)`, executes depth level concurrently with `Math.min(concurrency, dirsAtDepth.length)`, builds prompts via `buildDirectoryPrompt()`, writes via `writeAgentsMd()`. Post-Phase 2: validates phantom paths via `checkPhantomPaths()` extracting path-like strings via three regex patterns, resolving against directory and project root.\n\nPhase 3: sequential root synthesis (concurrency=1), builds prompts via `buildRootPrompt()`, calls `aiService.call()` with `maxTurns: 1`, strips conversational preamble before `writeFile()`.\n\n### Quality Validation Timing\n\nPre-Phase 1: reads existing `.sum` files concurrently (concurrency=20) into `oldSumCache` Map. Post-Phase 1: compares `oldSum` vs. source (stale documentation from previous runs), compares `newSum` vs. source (LLM omissions in current run), aggregates duplicate exports. Post-Phase 2: validates AGENTS.md path references. Validation failures logged to stderr with `[quality]` prefix, never throw (non-blocking).\n\n### Trace Event Threading\n\n`CommandRunOptions.tracer` threaded through pool → AIService → runner. Phase-level `phase:start/end` emitted by runner. Pool-level `worker:start/end`, `task:pickup/done` emitted by `runPool()`. Subprocess-level `subprocess:spawn/exit`, `retry` emitted by `AIService`. Zero overhead when `tracer` is `NullTraceWriter`.\n\n### ETA Calculation\n\n`ProgressReporter` maintains sliding windows `completionTimes` and `dirCompletionTimes` (max size 10). `formatETA()` computes `avg = sum(completionTimes) / completionTimes.length`, `remaining = totalFiles - completed - failed`, `etaMs = avg * remaining`. Displays after 2+ completions to avoid inaccurate early estimates. Formats as `~Ns remaining` (<60s) or `~Mm Ss remaining` (≥60s).\n\n## Behavioral Contracts\n\n### Preamble Detection Patterns (from runner.ts)\n\nFull patterns preserved in [runner.ts.annex.md](./runner.ts.annex.md).\n\n**Separator pattern:** Detects `\\n---\\n` within first 500 chars.\n\n**Bold header pattern:** Matches `/^[\\s\\S]{0,500}?(\\*\\*[A-Z])/`, strips preceding content if <300 chars, no `##` markers.\n\n**Preamble prefixes (case-insensitive):** `'now i'`, `'perfect'`, `'based on'`, `'let me'`, `'here is'`, `'i\\'ll'`, `'i will'`, `'great'`, `'okay'`, `'sure'`, `'certainly'`, `'alright'`.\n\n### Checkbox Update Pattern (from plan-tracker.ts)\n\nReplaces `` `- [ ] \\`${itemPath}\\`` `` → `` `- [x] \\`${itemPath}\\`` `` via string substitution. Item path conventions: files use relative paths (`src/cli/init.ts`), directories append `/AGENTS.md` suffix (`src/cli/AGENTS.md`), root documents use filename only (`CLAUDE.md`).\n\n### ANSI Stripping Regex (from progress.ts)\n\n`/\\x1b\\[[0-9;]*m/g` matches all SGR, cursor, and erase sequences before writing to `.agents-reverse-engineer/progress.log`.\n\n### Trace Filename Format (from trace.ts)\n\n`trace-{timestamp}.ndjson` where timestamp from `new Date().toISOString().replace(/[:.]/g, '-')` (e.g., `trace-2026-02-09T12-34-56-789Z.ndjson`).\n\n## File Relationships\n\n`CommandRunner` orchestrates via dependencies: `runPool()` for worker execution, `PlanTracker` for checkbox updates, `ProgressReporter` for console output, `ProgressLog` for file mirroring, `TraceWriter` for NDJSON events, `AIService` for subprocess calls, `buildFilePrompt/buildDirectoryPrompt/buildRootPrompt` from generation module, `writeSumFile/readSumFile/writeAnnexFile/writeAgentsMd` from writers, `checkCodeVsDoc/checkCodeVsCode/checkPhantomPaths` from quality module, `computeContentHashFromString` from change-detection.\n\n`runPool()` invokes task factories, emits trace events via `tracer?.emit()`, invokes `onComplete` callback per task settlement, returns `TaskResult[]` array.\n\n`ProgressReporter` receives events via `onFileStart/Done/Error/DirectoryStart/Done/RootDone`, computes ETA via sliding windows, delegates to `ProgressLog.write()` for file mirroring, prints summary via `printSummary(RunSummary)`.\n\n`PlanTracker` initialized with `formatExecutionPlanAsMarkdown()` output, updated via `markDone(itemPath)` from pool workers, flushed via `flush()` before orchestrator return.\n\n`TraceWriter` emits events via `emit(partial)`, auto-populates base fields (`seq`, `ts`, `pid`, `elapsedMs`), serializes writes via promise chain, finalized via `finalize()` after all phases complete.\n### output/\n<!-- Generated by agents-reverse-engineer -->\n\n# src/output\n\nTerminal output formatting layer providing color-aware Logger interface for CLI messages with optional ANSI styling via picocolors.\n\n## Contents\n\n**[logger.ts](./logger.ts)** — Exports `createLogger(options)` factory constructing color-aware Logger instances writing to console.log/warn/error with picocolors-based ANSI formatting, `createSilentLogger()` returning no-op Logger for testing, `Logger` interface defining six output methods (info/file/excluded/summary/warn/error), and `LoggerOptions` schema controlling color enablement.\n\n## API Surface\n\n**Logger interface:**\n- `info(message: string): void` — informational messages (plain text)\n- `file(path: string): void` — discovered file paths (`\"  +\" + path` in green)\n- `excluded(path: string, reason: string, filter: string): void` — excluded files (`\"  -\" + path + \" (reason: filter)\"` dimmed)\n- `summary(included: number, excluded: number): void` — discovery count summary (`\"Discovered N files (M excluded)\"` with bold/dim styling)\n- `warn(message: string): void` — warning messages (`\"Warning: \" + message` in yellow)\n- `error(message: string): void` — error messages (`\"Error: \" + message` in red)\n\n**Factory functions:**\n- `createLogger(options: LoggerOptions): Logger` — constructs color-aware logger respecting `options.colors` boolean flag\n- `createSilentLogger(): Logger` — returns no-op logger with all methods bound to empty function\n\n**Configuration:**\n- `LoggerOptions` — schema with single `colors: boolean` field controlling ANSI escape code emission\n\n## Output Format Specification\n\nLogger implements CONTEXT.md-defined format with mandatory prefixes and styling:\n- Discovered files: `\"  +\"` prefix green-styled followed by space and path\n- Excluded files: `\"  -\"` prefix dimmed followed by path and parenthetical `(reason: filter)`\n- Summary line: bold `\"Discovered N files\"` followed by dimmed `\" (M excluded)\"`\n- Warnings: `\"Warning: \"` yellow-styled prefix\n- Errors: `\"Error: \"` red-styled prefix\n\nWhen `options.colors === false`, all picocolors transforms replaced with identity function via `noColor` object mapping `green`/`dim`/`red`/`bold`/`yellow` to pass-through.\n\n## Integration Points\n\nUsed by:\n- `src/discovery/run.ts` — `discoverFiles()` accepts Logger for real-time file discovery progress reporting\n- `src/cli/*.ts` — Command entry points construct logger via `createLogger({ colors: config.output.colors })` from loaded configuration\n- Test suites — `createSilentLogger()` suppresses output during programmatic invocation\n\nImports:\n- `picocolors` (aliased `pc`) — ANSI color code generation library\n### quality/\n<!-- Generated by agents-reverse-engineer -->\n\n# src/quality\n\nPost-generation validation system detecting code-documentation inconsistencies (code-vs-doc), duplicate symbol exports (code-vs-code), and unresolvable path references (phantom-paths) through regex-based extraction, filesystem resolution, and structured reporting.\n\n## Contents\n\n**[index.ts](./index.ts)** — Barrel module re-exporting `extractExports()`, `checkCodeVsDoc()`, `checkCodeVsCode()`, `checkPhantomPaths()`, `buildInconsistencyReport()`, `formatReportForCli()`, `validateFindability()` from submodules alongside all type definitions (`Inconsistency`, `InconsistencyReport`, `FindabilityResult`, `CodeDocInconsistency`, `CodeCodeInconsistency`, `PhantomPathInconsistency`).\n\n**[types.ts](./types.ts)** — Defines discriminated union `Inconsistency = CodeDocInconsistency | CodeCodeInconsistency | PhantomPathInconsistency` with `type` discriminator, `InconsistencySeverity` literal union (`'info' | 'warning' | 'error'`), and `InconsistencyReport` structure containing `metadata` (timestamp, projectRoot, filesChecked, durationMs), `issues[]`, and `summary` counts by type/severity.\n\n## Validation Modules\n\n### Code-vs-Doc Consistency\n\n`inconsistency/code-vs-doc.ts` exports `extractExports()` applying regex `/^[ \\t]*export\\s+(?:default\\s+)?(?:function|class|const|let|var|type|interface|enum)\\s+(\\w+)/gm` to extract identifier names, and `checkCodeVsDoc()` performing substring search in `.sum` summary text to detect undocumented exports. Returns `CodeDocInconsistency` with `missingFromDoc[]` array when symbols absent from documentation.\n\n### Code-vs-Code Duplication\n\n`inconsistency/code-vs-code.ts` exports `checkCodeVsCode()` aggregating exports across per-directory file groups into `Map<symbol, paths[]>`, returning `CodeCodeInconsistency[]` for symbols appearing in multiple files with `pattern: 'duplicate-export'`. Caller must scope input to directory boundaries to prevent false positives from legitimate cross-directory symbol reuse.\n\n### Phantom Path Resolution\n\n`phantom-paths/validator.ts` extracts path strings via three regex patterns (markdown links, backtick-quoted paths, prose-embedded keywords), resolves against `AGENTS.md` directory and project root with `.ts`/`.js` extension fallbacks, validates via `existsSync()`. Returns `PhantomPathInconsistency[]` with `severity: 'warning'` for unresolved references after filtering exclusions (`node_modules`, URLs, template literals, globs).\n\n### Report Generation\n\n`inconsistency/reporter.ts` exports `buildInconsistencyReport()` aggregating issues with summary computation (total/per-type/per-severity counts) and `formatReportForCli()` rendering plain text output with severity tags `[ERROR]`, `[WARN]`, `[INFO]` and type-discriminated path formatting.\n\n### Density Validation (Disabled)\n\n`density/validator.ts` exports stub `validateFindability()` returning empty `FindabilityResult[]` array. Originally verified exported symbols from `.sum` files appeared in parent `AGENTS.md` via substring matching; disabled after `SumFileContent.metadata.publicInterface` removal pending structured extraction re-implementation.\n\n## Integration Points\n\nConsumed by `src/generation/orchestrator.ts` post-generation validation phase and `src/cli/generate.ts` for terminal reporting. All validators operate on generated artifacts (`.sum`, `AGENTS.md`) rather than source code, enforcing post-generation validation contract. Report format matches telemetry schema in `src/ai/telemetry/run-log.ts` for `qualityMetrics` aggregation.\n\n## Behavioral Contracts\n\n**Export extraction pattern:**\n```regex\n/^[ \\t]*export\\s+(?:default\\s+)?(?:function|class|const|let|var|type|interface|enum)\\s+(\\w+)/gm\n```\n\n**Phantom path patterns:**\n- Markdown links: `/\\[(?:[^\\]]*)\\]\\((\\.[^)]+)\\)/g`\n- Backtick paths: `` /`((?:src\\/|\\.\\.?\\/)[^`]+\\.[a-z]{1,4})`/g ``\n- Prose-embedded: `/(?:from|in|by|via|see)\\s+`?(src\\/[\\w\\-./]+)`?/gi`\n\n**CLI report format:**\n```\n=== Inconsistency Report ===\nChecked 42 files in 150ms\nFound 3 issue(s)\n\n[ERROR] Documentation out of sync: 2 exports undocumented\n  File: src/foo/bar.ts\n\n[WARN] Symbol \"Config\" exported from 2 files\n  Files: src/config/a.ts, src/config/b.ts\n\n[WARN] Phantom path reference: \"src/missing.ts\" does not exist\n  AGENTS.md: src/quality/AGENTS.md\n  Referenced: src/missing.ts\n```\n\n## Known Limitations\n\nRegex-based `extractExports()` misses destructured exports, namespace exports, dynamic exports, and re-exports with renaming. Substring matching in code-vs-doc yields false negatives for prose mentions unrelated to API surface. Code-vs-code operates on symbol names only without AST analysis to distinguish intentional duplication (interface/implementation pairs, test fixtures) from architectural violations. Phantom path resolution uses heuristic extension fallbacks (`.js` → `.ts`) without TypeScript compiler path mapping awareness.\n### specify/\n<!-- Generated by agents-reverse-engineer -->\n\n# src/specify/\n\nProject specification synthesis from AGENTS.md documentation corpus via AI-driven prompt engineering. Exports `buildSpecPrompt()` for constructing two-part prompts (system constraints + aggregated AGENTS.md content), `writeSpec()` for filesystem output with overwrite protection and multi-file splitting, and `SpecExistsError` for conflict detection.\n\n## Contents\n\n**[index.ts](./index.ts)** — Barrel re-export aggregating `buildSpecPrompt`, `SpecPrompt`, `writeSpec`, `SpecExistsError`, `WriteSpecOptions` from `prompts.ts` and `writer.ts` submodules. Consumed by `src/cli/specify.ts` command orchestrator.\n\n**[prompts.ts](./prompts.ts)** — Prompt template factory exporting `buildSpecPrompt(docs, annexFiles?)` which constructs `SpecPrompt` pairs: system instructions (`SPEC_SYSTEM_PROMPT` enforcing 11-section conceptual organization) and user content (concatenated AGENTS.md markdown with section delimiters, optional annex files for reproduction-critical constants). Returns structured prompt with mandatory output format rules (raw markdown, no preamble, verbatim reproduction of prompt templates/regex/IDE templates).\n\n**[writer.ts](./writer.ts)** — Filesystem writer exporting `writeSpec(content, options)` supporting single-file (`specs/SPEC.md`) and multi-file (`specs/<slug>.md`) output modes. Implements pre-flight existence checks throwing `SpecExistsError(conflicts[])` when `force=false`, content splitting via `splitByHeadings()` (regex `/^(?=# )/m`), filename sanitization through `slugify()` (lowercase + hyphenation + alphanumeric filtering), and directory creation with `mkdir(outputDir, { recursive: true })`.\n\n## Architecture\n\n**Prompt Construction Pipeline:**  \n`buildSpecPrompt()` receives `AgentsDocs[]` from `../generation/collector.js` (`collectAgentsDocs()` recursive traversal) and optional annex files (`collectAnnexFiles()`). Constructs user prompt by iterating docs array, emitting `### ${doc.relativePath}\\n\\n${doc.content}` sections, appending annex section if provided, injecting output requirements listing 11 mandatory sections. System prompt (`SPEC_SYSTEM_PROMPT`) prohibits folder-mirroring, exact file path prescription, mandates verbatim reproduction of behavioral contracts (regex patterns, format strings, magic constants, environment variables, prompt templates, IDE templates).\n\n**Filesystem Writing Pipeline:**  \n`writeSpec()` branches on `options.multiFile`. Single-file mode: checks `fileExists(outputPath)`, throws `SpecExistsError([outputPath])` if exists and `force=false`, creates parent directory, writes content. Multi-file mode: calls `splitByHeadings()` to partition by top-level `# ` headings, checks all target paths for conflicts, throws `SpecExistsError(conflicts)` if any exist and `force=false`, writes each section to slugified filename.\n\n## Behavioral Contracts\n\n**SPEC_SYSTEM_PROMPT Section 7 (Behavioral Contracts):**  \nEnforces two subsections: (a) Runtime Behavior — error types/codes, retry formulas/delays, concurrency model, lifecycle hooks; (b) Implementation Contracts — verbatim regex patterns in backticks, format strings with examples, magic constants with meanings, environment variable names with expected values, file format specifications (YAML schemas, NDJSON structures). Prohibits paraphrasing regex into prose.\n\n**SPEC_SYSTEM_PROMPT Section 10 (Prompt Templates):**  \nRequires FULL verbatim text reproduction from annex files or AGENTS.md content, organized by pipeline phase or functional area, preserving placeholder syntax (e.g., `{{FILE_PATH}}`).\n\n**SPEC_SYSTEM_PROMPT Section 11 (IDE Integration):**  \nRequires verbatim reproduction of command templates per platform, platform configuration objects (path prefixes, filename conventions, frontmatter formats), installer permission lists, hook definitions.\n\n**Content Splitting Regex:**  \n`/^(?=# )/m` matches lines starting with exactly `# ` (top-level headings). Content before first heading assigned to `'00-preamble.md'`.\n\n**Filename Sanitization Transform:**  \n`slugify()` applies sequence: lowercase → whitespace (`/\\s+/g`) to hyphen → strip non-alphanumeric except hyphens (`/[^a-z0-9-]/g`) → collapse hyphens (`/-+/g`) → trim edge hyphens (`/^-|-$/g`).\n\n## Integration Points\n\n**Upstream:** `src/cli/specify.ts` orchestrates workflow: validates project root, collects AGENTS.md via `collectAgentsDocs()`, optionally collects annex files via `collectAnnexFiles()`, calls `buildSpecPrompt(docs, annexFiles)`, invokes `AIService.call()` with returned `SpecPrompt`, passes AI output to `writeSpec()` with options from CLI flags (`--force`, `--multi-file`, `--output`).\n\n**Downstream:** `writeSpec()` consumes `node:fs/promises` (`writeFile`, `mkdir`, `access`), `node:fs` (`constants.F_OK`), `node:path` (`dirname`, `join`). Throws `SpecExistsError` caught by CLI for user-facing error messages.\n### types/\n<!-- Generated by agents-reverse-engineer -->\n\n# src/types\n\nShared type definitions for file discovery results and statistics consumed across discovery, orchestration, and CLI modules.\n\n## Contents\n\n### [index.ts](./index.ts)\nExports `DiscoveryResult`, `DiscoveryStats`, `ExcludedFile` interfaces defining discovery phase output schema, exclusion metadata, and aggregate metrics.\n\n## Exported Interfaces\n\n### `ExcludedFile`\nRepresents files filtered during discovery with `path: string` and `reason: string` (exclusion rationale: \"gitignore pattern\", \"binary file\", \"vendor directory\").\n\n### `DiscoveryResult`\nAggregates discovery output with `files: string[]` (paths for Phase 1 analysis) and `excluded: ExcludedFile[]` (filtered files with metadata).\n\n### `DiscoveryStats`\nProvides discovery metrics: `totalFiles`, `includedFiles`, `excludedFiles` (counts), `exclusionReasons: Record<string, number>` (histogram of `ExcludedFile.reason` values).\n\n## Data Flow\n\n**Producer:** `runDiscovery()` in `src/discovery/run.ts` returns `DiscoveryResult` after filter chain execution (`src/discovery/walker.ts` + filters: `gitignore.ts`, `binary.ts`, `vendor.ts`, `custom.ts`).\n\n**Consumers:**\n- `src/cli/discover.ts` — Generates `GENERATION-PLAN.md` from `DiscoveryResult.files`\n- `src/cli/generate.ts` — Feeds `DiscoveryResult.files` to Phase 1 worker pool\n- `src/cli/update.ts` — Inputs `DiscoveryResult` to `detectChanges()` for delta computation\n- `src/output/logger.ts` — Computes `DiscoveryStats` from `DiscoveryResult` for terminal output\n\n**Exclusion Metadata:** `ExcludedFile.reason` populated by filter modules:\n- `src/discovery/filters/gitignore.ts` — \"matched .gitignore pattern: `<pattern>`\"\n- `src/discovery/filters/binary.ts` — \"binary file\"\n- `src/discovery/filters/vendor.ts` — \"vendor directory\"\n- `src/discovery/filters/custom.ts` — \"matched exclude pattern: `<pattern>`\"\n### update/\n<!-- Generated by agents-reverse-engineer -->\n\n# src/update\n\nOrchestrates incremental documentation updates via SHA-256 content hash comparison against `.sum` YAML frontmatter, deleting orphaned artifacts and computing affected directory sets without requiring git diff parsing.\n\n## Contents\n\n**[index.ts](./index.ts)** — Barrel module exporting `UpdateOrchestrator` class, factory `createUpdateOrchestrator()`, cleanup utilities `cleanupOrphans()`, `cleanupEmptyDirectoryDocs()`, `getAffectedDirectories()`, and interfaces `UpdatePlan`, `UpdateOptions`, `UpdateResult`, `UpdateProgress`, `CleanupResult`.\n\n**[orchestrator.ts](./orchestrator.ts)** — `UpdateOrchestrator` class coordinates change detection by reading stored `content_hash` from `.sum` frontmatter via `readSumFile()`, comparing against `computeContentHash()` output, returning `UpdatePlan` with `filesToAnalyze` (hash mismatch), `filesToSkip` (match), `cleanup` (orphans), `affectedDirs` (sorted depth descending). Methods: `preparePlan()`, `checkPrerequisites()`, `isFirstRun()`. Emits `plan:created` trace events.\n\n**[orphan-cleaner.ts](./orphan-cleaner.ts)** — `cleanupOrphans()` deletes stale `.sum`/`.annex.md` files for deleted/renamed sources via `deleteIfExists()`, `cleanupEmptyDirectoryDocs()` removes `AGENTS.md` from directories with zero source files (excludes `GENERATED_FILES` set and dotfiles), `getAffectedDirectories()` walks parent directories via `path.dirname()` until `.` or absolute path, returning `Set<string>` of directories requiring regeneration.\n\n**[types.ts](./types.ts)** — Interfaces: `CleanupResult` (`deletedSumFiles`, `deletedAgentsMd` arrays), `UpdateOptions` (`includeUncommitted`, `dryRun` flags), `UpdateResult` (`analyzedFiles`, `skippedFiles`, `cleanup`, `regeneratedDirs`, `baseCommit`, `currentCommit`, `dryRun`), `UpdateProgress` callbacks (`onFileStart`, `onFileDone`, `onCleanup`, `onDirRegenerate`).\n\n## Update Workflow\n\n1. **Change Detection:** `preparePlan()` discovers files via `runDiscovery()` from `src/discovery/`, reads `.sum` frontmatter via `readSumFile()` from `src/generation/writers/sum.js`, compares `content_hash` against `computeContentHash()` from `src/change-detection/`, populates `filesToAnalyze` (added/modified) and `filesToSkip` (unchanged).\n\n2. **Orphan Cleanup:** `cleanupOrphans()` processes `FileChange[]` with `status: 'deleted' | 'renamed'`, extracts `path` (deleted) or `oldPath` (renamed), constructs `.sum` and `.annex.md` paths, deletes via `unlink()` unless `dryRun: true`.\n\n3. **Affected Directories:** `getAffectedDirectories()` walks parent chains for all changed files, returns `Set<string>` sorted by `path.sep` depth descending (deepest first), ensuring `AGENTS.md` regeneration propagates upward.\n\n4. **Empty Directory Cleanup:** `cleanupEmptyDirectoryDocs()` filters directory entries excluding dotfiles, `.sum`/`.annex.md` extensions, `GENERATED_FILES` set (`['AGENTS.md', 'CLAUDE.md']`), deletes `AGENTS.md` if zero source files remain.\n\n5. **Execution:** `src/cli/update.ts` invokes `preparePlan()`, executes Phase 1 pool for `filesToAnalyze` via `src/generation/executor.ts`, regenerates `AGENTS.md` sequentially for `affectedDirs` (no post-order traversal required).\n\n## Trace Emissions\n\n`preparePlan()` emits:\n- `phase:start` with `phase: 'update-plan-creation'`, `taskCount: 0`, `concurrency: 1`\n- `plan:created` with `planType: 'update'`, `fileCount: filesToAnalyze.length`, `taskCount: filesToAnalyze.length + affectedDirs.length`\n- `phase:end` with `durationMs` computed via `process.hrtime.bigint()` delta\n\n## Behavioral Contracts\n\n**Depth-descending sort:** `affectedDirs` sorted via `path.relative().split(path.sep).length` comparator (deepest first) ensuring child directories process before parents.\n\n**Orphan path construction:** `.sum` path = `path.join(projectRoot, \\`${relativePath}.sum\\`)`, `.annex.md` path = `path.join(projectRoot, \\`${relativePath}.annex.md\\`)`.\n\n**Directory emptiness filter:** Exclude entries matching `entry.startsWith('.')` OR `entry.endsWith('.sum')` OR `entry.endsWith('.annex.md')` OR `GENERATED_FILES.has(entry)`.\n\n**Upward traversal terminator:** `getAffectedDirectories()` stops when `dir === '.'` OR `path.isAbsolute(dir) === true`.\n\n## API Compatibility No-Ops\n\nMethods retained for backward compatibility but perform no operations in frontmatter-based mode:\n- `recordFileAnalyzed()` — hash stored in `.sum` YAML frontmatter by generation phase\n- `removeFileState()` — `.sum` file cleanup handled by `cleanupOrphans()`\n- `recordRun()` — returns `0`, no run history database in frontmatter mode\n- `getLastRun()` — returns `undefined`, no run history available\n- `close()` — no resources to dispose",
      "systemPrompt": "You are generating an AGENTS.md file — a directory-level overview for AI coding assistants.\n\nCRITICAL: Output ONLY the raw markdown content. No code fences, no preamble, no explanations, no conversational text. Your entire response IS the AGENTS.md file content.\n\nOUTPUT FORMAT:\n- First line MUST be exactly: <!-- Generated by agents-reverse-engineer -->\n- Use a # heading with the directory name\n- Write a one-paragraph purpose statement for the directory\nusage\nADAPTIVE SECTIONS:\nAnalyze the directory contents and choose the most relevant sections. Do NOT use a fixed template. Instead, select sections that best document this specific directory for an AI that needs to reconstruct or extend the project.\n\nConsider these section types (choose what applies):\n- **Contents**: Group files by purpose/category under ## headings. For each file: markdown link [filename](./filename) and a one-line description.\n- **Subdirectories**: If subdirectories exist, list them with links [dirname/](./dirname/) and brief summaries.\n- **Architecture / Data Flow**: If files form a pipeline, request/response chain, or layered architecture, document it.\n- **Stack**: If this is a package root (has package.json, Cargo.toml, go.mod, etc.), document the technology stack, key scripts, and entry points.\n- **Structure**: If the directory layout follows a convention (feature-sliced, domain-driven, MVC, etc.), document it.\n- **Patterns**: If files share recurring design patterns (factory, strategy, middleware, barrel re-export), name and document them.\n- **Configuration**: If the directory contains config files, schemas, or environment definitions, document the config surface area.\n- **API Surface**: If the directory exports a public API (barrel index, route definitions, SDK), document the interface contract.\n- **File Relationships**: How files collaborate, depend on each other, or share state.\n- **Behavioral Contracts**: If files contain regex patterns, format specifications, magic constants, or template strings that define observable behavior, collect them in a dedicated section. Preserve verbatim patterns from file summaries — do NOT paraphrase regex into prose. This section is MANDATORY when file summaries contain behavioral artifacts.\n- **Reproduction-Critical Constants**: If file summaries reference annex files (via ## Annex References sections), list them with links. Example: \"Full prompt template text: [templates.ts.annex.md](./prompts/templates.ts.annex.md)\". Do NOT reproduce annex content in AGENTS.md — just link to it.\n\nChoose any relevant sections or create your own based on the directory contents. The goal is to provide a comprehensive overview that captures the essence of the directory's role in the project and how its files work together, with a focus on what an AI coding assistant would need to know to effectively interact with this code.\n\nSCOPE:\n- AGENTS.md is a NAVIGATIONAL INDEX — help an AI find the right file quickly\n- Focus on: what each file does, how files relate, directory-level patterns\n- Do NOT reproduce full architecture sections — those belong in the root CLAUDE.md\n\nPATH ACCURACY (MANDATORY):\n- When referencing files or modules outside this directory, use ONLY paths from the \"Import Map\" section\n- Do NOT invent, rename, or guess module paths — if a path isn't in the Import Map, don't reference it\n- Use the exact directory names from \"Project Directory Structure\" — do NOT rename directories\n  (e.g., if the directory is called \"cli\", write \"src/cli/\", NOT \"src/commands/\")\n- Cross-module references must use the specifier format from actual import statements\n  (e.g., \"../generation/writers/sum.js\", NOT \"../fs/sum-file.js\")\n- If you are unsure about a path, omit the cross-reference rather than guessing\n\nCONSISTENCY (MANDATORY):\n- Do not contradict yourself within the same document\n- If you describe a technique (e.g., \"regex-based\"), do not call it something else later (e.g., \"AST-based\")\n- When stating version numbers, engines, or config fields, use ONLY values present in the file summaries\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this directory\", \"this module\", \"provides\", \"responsible for\", \"is used to\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Per-file descriptions in Contents sections: 1-2 sentences maximum. Reference key symbols but do not reproduce full summaries.\n- Behavioral contracts (regex patterns, format specs, constants) belong in a separate Behavioral Contracts section, not in per-file descriptions.\n- Annex file references: link to .annex.md files, do not inline their content. One line per annex reference.\n- Subdirectory descriptions: 1-2 sentences maximum. Capture the directory's role, not its full contents.\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- Key exported symbols from file summaries MUST appear in the directory overview\n- Preserve exact casing of identifiers\n\nUSER NOTES:\n- If \"User Notes\" are provided in the prompt, they contain user-defined instructions that will be automatically prepended to your output\n- Do NOT repeat or paraphrase user notes in your generated content — they are included separately\n- You may reference information from user notes for context",
      "response": "<!-- Generated by agents-reverse-engineer -->\n\n# src\n\n**Core implementation directory housing seven CLI commands (init, discover, generate, update, clean, specify, help), three-phase documentation pipeline orchestration (concurrent file analysis, post-order directory synthesis, sequential root integration), AI backend abstraction layer with Claude/Gemini/OpenCode adapters, gitignore-aware file discovery with composable filter chains, incremental SHA-256 hash-based change detection, quality validation subsystem detecting code-doc inconsistencies and phantom paths, worker pool concurrency control with NDJSON trace emission, and platform-specific IDE integration supporting Claude Code/OpenCode/Gemini runtimes.**\n\n## Contents\n\n**[version.ts](./version.ts)** — `getVersion()` reads `package.json` version field from parent directory of compiled module via `import.meta.url` → `fileURLToPath()` → `join(__dirname, '..', 'package.json')` path resolution, returns `'unknown'` on parse/read errors. Consumed by CLI `--version` flag and session lifecycle hooks (`hooks/are-check-update.js`) for npm registry comparison.\n\n## Subdirectories\n\n**[ai/](./ai/)** — Backend-agnostic AI service layer abstracting Claude Code, Gemini CLI, OpenCode via adapter registry. Implements exponential backoff retry (max 3 attempts, base 1s delay, 8s ceiling), rate-limit detection via stderr pattern matching (`'rate limit'`, `'429'`, `'overloaded'`), subprocess resource management with 512MB heap limits (`NODE_OPTIONS='--max-old-space-size=512'`), libuv thread pool capping (`UV_THREADPOOL_SIZE='4'`), process group killing (`kill(-pid)`) for timeout enforcement (SIGTERM at timeout, SIGKILL after 5s grace period). Emits NDJSON telemetry logs to `.agents-reverse-engineer/logs/run-<timestamp>.json` tracking token counts (input/output/cacheRead/cacheCreation), latency, retry attempts, filesRead metadata, and aggregated cost metrics. Trace emission via `ITraceWriter` for `subprocess:spawn/exit`, `retry` events.\n\n**[change-detection/](./change-detection/)** — Git-based change detection via `simple-git` diff parsing (`git diff --name-status -M <baseCommit>..HEAD`) with rename detection (50% similarity threshold), uncommitted change merge via `git.status()`, and SHA-256 content hashing (`computeContentHash()`) for non-git workflows. Status code mapping: `'A'` → `'added'`, `'M'` → `'modified'`, `'D'` → `'deleted'`, `'R*'` → `'renamed'` with `oldPath` extraction. Consumed by `src/update/orchestrator.ts` for hash-based incremental updates comparing `.sum` frontmatter `content_hash` against current file content.\n\n**[cli/](./cli/)** — Command entry points implementing `init` (config creation), `discover` (preview generation plan), `generate` (three-phase pipeline execution), `update` (incremental hash-based regeneration), `clean` (artifact deletion), `specify` (project spec synthesis). Orchestrates backend resolution via `createBackendRegistry()` + `resolveBackend()`, AIService lifecycle management (`setDebug()`, `setTracer()`, `finalize()`), trace emission to `.agents-reverse-engineer/traces/trace-<timestamp>.ndjson`, progress logging via `ProgressLog.create()` to `.agents-reverse-engineer/progress.log`. Exit codes: 0 (success), 1 (partial failure or config exists), 2 (total failure or CLI not found).\n\n**[config/](./config/)** — YAML configuration loader with Zod validation via `ConfigSchema` (exclude patterns/vendorDirs/binaryExtensions, discovery options followSymlinks/maxFileSize, output colors, AI backend/model/timeoutMs/maxRetries/concurrency/telemetry). `getDefaultConcurrency()` computes memory-aware worker pool sizing via `clamp(cores × 5, MIN=2, min(floor((totalMemGB × 0.5) / 0.512), MAX=20))` enforcing 512MB subprocess heap constraint. `writeDefaultConfig()` generates annotated YAML with `yamlScalar()` quoting patterns containing YAML metacharacters `[*{}\\[\\]?,:#&!|>'\"%@\\`]`. Emits `config:loaded` trace events.\n\n**[discovery/](./discovery/)** — Gitignore-aware file discovery executing four-stage filter chain (gitignore → vendor → binary → custom) over `fast-glob` traversal results with `absolute: true`, `onlyFiles: true`, `dot: true`, `followSymbolicLinks: false`, `ignore: ['**/.git/**']` hardcoded. `applyFilters()` spawns 30 workers sharing `files.entries()` iterator for bounded-concurrency sequential filter evaluation with early termination. Binary detection: extension fast-path (80+ extensions) → size threshold (1MB default) → `isBinaryFile()` content analysis. Returns `DiscoveryResult` with `included: string[]`, `excluded: ExcludedFile[]` with `reason` (gitignore pattern/binary/vendor/custom).\n\n**[generation/](./generation/)** — Three-phase pipeline orchestration: (1) concurrent file analysis via `GenerationOrchestrator.createFileTasks()` embedding `buildFilePrompt()` with import maps, depth-sorted deepest-first execution; (2) post-order directory aggregation via `buildExecutionPlan()` with dependency graphs (`isDirectoryComplete()` predicates, `dependencies: fileTaskIds`), `buildDirectoryPrompt()` consuming child `.sum` files and subdirectory `AGENTS.md`; (3) sequential root synthesis via `collectAgentsDocs()` recursive traversal, `buildRootPrompt()` with stack detection (9 manifest types), preamble stripping. Exports `writeSumFile()`/`readSumFile()` for YAML frontmatter serialization with `content_hash`, `writeAgentsMd()` preserving user content via `AGENTS.local.md` rename, `writeAnnexFile()` for reproduction-critical source content.\n\n**[imports/](./imports/)** — Static import analysis via `IMPORT_REGEX` (`/^import\\s+(type\\s+)?(?:\\{([^}]*)\\}|(\\*\\s+as\\s+\\w+)|(\\w+))\\s+from\\s+['\"]([^'\"]+)['\"]/gm`) matching ES module imports in first 100 lines. Classifies as `internal` (`./` prefix) or `external` (`../` prefix), filters bare package specifiers and `node:` built-ins. `formatImportMap()` produces text blocks with filename headers and indented specifier-symbol pairs for LLM prompt embedding in Phase 1 file analysis and Phase 2 directory aggregation.\n\n**[installer/](./installer/)** — npx-based installation orchestrator for ARE commands/hooks across Claude Code (`~/.claude/skills/`, `~/.claude/hooks/`), OpenCode (`~/.config/opencode/commands/`, `~/.config/opencode/plugins/`), Gemini (`~/.gemini/commands/`, `~/.gemini/hooks/`) with environment overrides (`CLAUDE_CONFIG_DIR`, `OPENCODE_CONFIG_DIR`/`XDG_CONFIG_HOME`, `GEMINI_CONFIG_DIR`). Interactive prompts via `arrowKeySelect()` TTY mode with ANSI escape sequences (`\\x1b[${n}A` cursor up, `\\x1b[2K` clear line, `\\x1b[1B` cursor down) or `numberedSelect()` fallback. `registerHooks()` modifies `settings.json` with nested structure for Claude (`{ hooks: { SessionStart?: [{ hooks: [{ type: 'command', command }] }] } }`), flat structure for Gemini (`{ hooks: { SessionStart?: [{ name, type, command }] } }`). `registerPermissions()` adds `ARE_PERMISSIONS` bash patterns (`'Bash(npx agents-reverse-engineer@latest init*)'` through `clean*`). Uninstallation via `cleanupOrphans()`, `cleanupEmptyDirs()`, `cleanupLegacyGeminiFiles()`.\n\n**[integration/](./integration/)** — Platform-specific AI assistant integration layer detecting Claude Code/OpenCode/Gemini/Aider environments via marker files (`.claude/`, `.opencode/`, `.gemini/`, `.aider/`, `CLAUDE.md`, `.aider.conf.yml`). `generateIntegrationFiles()` writes command templates with progress-monitoring patterns (background execution via `run_in_background: true`, 15s poll intervals, offset-based `.agents-reverse-engineer/progress.log` tailing, `TaskOutput` checks with `block: false`). `getTemplatesForEnvironment()` generates seven commands (generate/update/init/discover/clean/specify/help) with platform-specific frontmatter (Claude: `name: /are-<command>`, OpenCode: `agent: build`, Gemini: TOML `description`/`prompt`).\n\n**[orchestration/](./orchestration/)** — Worker pool concurrency control via iterator-based `runPool()` sharing `tasks.entries()` iterator across N workers preventing over-allocation. `CommandRunner` orchestrates three-phase pipeline: Phase 1 concurrent file analysis with `sourceContentCache` Map for single read, `computeContentHashFromString()` for `.sum` frontmatter, annex file detection via `## Annex References` marker; post-Phase 1 quality validation (concurrency=10) via `checkCodeVsDoc(source, oldSum)` (stale), `checkCodeVsDoc(source, newSum)` (fresh), `checkCodeVsCode(filesForCodeVsCode)` (duplicates); Phase 2 depth-grouped directory synthesis with `buildDirectoryPrompt()` runtime construction; post-Phase 2 phantom path validation via `checkPhantomPaths()` with three regex patterns; Phase 3 sequential root synthesis with preamble stripping. `ProgressReporter` computes ETA via sliding window (size 10, min 2 completions) as `~Ns remaining` or `~Mm Ss remaining`. `PlanTracker` serializes checkbox updates via promise-chain writes replacing `` `- [ ] \\`${itemPath}\\`` `` → `` `- [x] \\`${itemPath}\\`` ``. `TraceWriter` appends NDJSON with auto-populated `seq` (monotonic), `ts` (ISO 8601), `pid`, `elapsedMs` (high-resolution delta) to `.agents-reverse-engineer/traces/trace-<timestamp>.ndjson`, retains 500 most recent traces.\n\n**[output/](./output/)** — Terminal output formatting layer providing color-aware `Logger` interface with picocolors ANSI styling. `createLogger(options)` constructs logger respecting `options.colors` boolean flag, exports `createSilentLogger()` returning no-op logger for testing. Methods: `info(message)`, `file(path)` (`\"  +\" + path` green), `excluded(path, reason, filter)` (`\"  -\" + path + \" (reason: filter)\"` dimmed), `summary(included, excluded)` (bold/dim counts), `warn(message)` (`\"Warning: \"` yellow prefix), `error(message)` (`\"Error: \"` red prefix).\n\n**[quality/](./quality/)** — Post-generation validation system detecting: (1) code-vs-doc inconsistencies via `extractExports()` regex `/^[ \\t]*export\\s+(?:default\\s+)?(?:function|class|const|let|var|type|interface|enum)\\s+(\\w+)/gm` matching against substring search in `.sum` text; (2) code-vs-code duplicates aggregating exports into `Map<symbol, paths[]>` per directory; (3) phantom paths via three regex patterns (markdown links `/\\[(?:[^\\]]*)\\]\\((\\.[^)]+)\\)/g`, backtick paths `` /`((?:src\\/|\\.\\.?\\/)[^`]+\\.[a-z]{1,4})`/g ``, prose-embedded `/(?:from|in|by|via|see)\\s+`?(src\\/[\\w\\-./]+)`?/gi`) with `existsSync()` validation and `.ts`/`.js` fallback resolution. `buildInconsistencyReport()` aggregates discriminated union `Inconsistency = CodeDocInconsistency | CodeCodeInconsistency | PhantomPathInconsistency` with `severity: 'info' | 'warning' | 'error'`, `formatReportForCli()` renders plain text with severity tags.\n\n**[specify/](./specify/)** — Project specification synthesis from `AGENTS.md` corpus via `buildSpecPrompt()` constructing two-part prompts: system instructions (`SPEC_SYSTEM_PROMPT` enforcing 11-section conceptual organization), user content (concatenated AGENTS.md with section delimiters, optional annex files for verbatim reproduction of behavioral contracts). `writeSpec()` supports single-file (`specs/SPEC.md`) and multi-file (`specs/<slug>.md`) modes with pre-flight existence checks throwing `SpecExistsError(conflicts[])` when `force=false`, content splitting via `/^(?=# )/m` regex, filename sanitization through `slugify()` (lowercase + hyphenation + alphanumeric filtering).\n\n**[types/](./types/)** — Shared interfaces for discovery results: `ExcludedFile` (path, reason), `DiscoveryResult` (files, excluded), `DiscoveryStats` (totalFiles, includedFiles, excludedFiles, exclusionReasons histogram). Consumed by `src/discovery/run.ts` (producer), `src/cli/*.ts` (consumers), `src/output/logger.ts` (terminal output).\n\n**[update/](./update/)** — Incremental update orchestrator implementing hash-based change detection via `preparePlan()` reading `.sum` frontmatter `content_hash`, comparing against `computeContentHash()` current file content, returning `UpdatePlan` with `filesToAnalyze` (hash mismatch), `filesToSkip` (match), `cleanup` (orphans), `affectedDirs` (depth-sorted descending). `cleanupOrphans()` deletes stale `.sum`/`.annex.md` files for deleted/renamed sources, `cleanupEmptyDirectoryDocs()` removes `AGENTS.md` from directories with zero source files (excludes `GENERATED_FILES` set and dotfiles). `getAffectedDirectories()` walks parent chains via `path.dirname()` until `.` or absolute path, ensuring `AGENTS.md` regeneration propagates upward. Emits `phase:start/end`, `plan:created` trace events.\n\n## Architecture\n\n### Three-Phase Pipeline\n\n**Phase 1 (Concurrent File Analysis):** Iterator-based worker pool (`src/orchestration/pool.ts`) shares `tasks.entries()` iterator across N workers (default 2 WSL, 5 elsewhere) to prevent over-allocation. Each worker spawns AI CLI subprocess via `AIService.call()` → `runSubprocess()` → `execFile()` with resource limits (512MB heap, 4-thread libuv pool, background tasks disabled, subagents blocked). Writes `.sum` files with YAML frontmatter containing SHA-256 `content_hash`, strips conversational preamble via patterns matching `'now i'`, `'let me'`, `'here is'` prefixes and bold headers without `##` markers. Detects `## Annex References` marker for `writeAnnexFile()` reproduction-critical source content.\n\n**Phase 2 (Post-Order Directory Aggregation):** Sorts directories by depth descending via `path.relative().split(path.sep).length` (deepest first). Waits for `isDirectoryComplete()` predicate checking all child `.sum` files exist. Builds prompts via `buildDirectoryPrompt()` aggregating child `.sum` content, subdirectory `AGENTS.md` files, import maps via `extractDirectoryImports()`, manifest detection (9 types: package.json, Cargo.toml, go.mod, pyproject.toml, pom.xml, build.gradle, Gemfile, composer.json, CMakeLists.txt/Makefile). Preserves user content via `AGENTS.local.md` rename, prepends above generated sections with `---` separator.\n\n**Phase 3 (Sequential Root Synthesis):** Collects all `AGENTS.md` files via `collectAgentsDocs()` recursive traversal (skips 13 directories: node_modules, .git, .agents-reverse-engineer, vendor, dist, build, __pycache__, .next, venv, .venv, target, .cargo, .gradle). Builds root prompts via `buildRootPrompt()` consuming aggregated docs, enforcing synthesis-only constraints (no feature/hook/pattern invention), strips conversational preamble before writing `CLAUDE.md`, `GEMINI.md`, `OPENCODE.md` platform-specific integration documents.\n\n### Subprocess Resource Management\n\nMitigates Claude CLI thread exhaustion (GitHub #5771: 200 Node.js instances):\n- `NODE_OPTIONS='--max-old-space-size=512'` limits heap to 512MB per subprocess\n- `UV_THREADPOOL_SIZE='4'` constrains libuv thread pool to 4 threads\n- `CLAUDE_CODE_DISABLE_BACKGROUND_TASKS='1'` prevents background task spawning\n- `--disallowedTools Task` prevents subagent spawning\n- Process group killing via `kill(-pid)` terminates subprocess trees\n- Default concurrency 2 for WSL environments (5 elsewhere)\n\n### Incremental Update Strategy\n\n`UpdateOrchestrator.preparePlan()` reads `content_hash` from `.sum` frontmatter, computes current file hash via `computeContentHash()`, populates `filesToAnalyze` (added/modified) and `filesToSkip` (unchanged). `cleanupOrphans()` deletes stale `.sum`/`.annex.md` for deleted/renamed sources. `getAffectedDirectories()` walks parent chains computing depth-sorted set for `AGENTS.md` regeneration propagation. Phase 1 pool regenerates `.sum` for `filesToAnalyze`, Phase 2 regenerates `AGENTS.md` for `affectedDirs` sequentially without full post-order traversal.\n\n## Behavioral Contracts\n\n### Default Vendor Directories (config/defaults.ts)\n\n```typescript\nDEFAULT_VENDOR_DIRS = ['node_modules', '.git', 'dist', 'build', 'target', '.next', '__pycache__', 'venv', '.venv', '.cargo', '.gradle', '.agents-reverse-engineer', '.agents', '.planning', '.claude', '.opencode', '.gemini']\n```\n\n### Default Concurrency Formula (config/defaults.ts)\n\n```typescript\nclamp(cores × 5, MIN=2, min(floor((totalMemGB × 0.5) / 0.512), MAX=20))\n```\n\n### Preamble Detection Patterns (orchestration/runner.ts)\n\nFull patterns preserved in [orchestration/runner.ts.annex.md](./orchestration/runner.ts.annex.md).\n\n**Separator pattern:** Detects `\\n---\\n` within first 500 chars.\n\n**Bold header pattern:** Matches `/^[\\s\\S]{0,500}?(\\*\\*[A-Z])/`, strips preceding content if <300 chars, no `##` markers.\n\n**Preamble prefixes (case-insensitive):** `'now i'`, `'perfect'`, `'based on'`, `'let me'`, `'here is'`, `'i\\'ll'`, `'i will'`, `'great'`, `'okay'`, `'sure'`, `'certainly'`, `'alright'`.\n\n### Export Extraction Regex (quality/inconsistency/code-vs-doc.ts)\n\n```regex\n/^[ \\t]*export\\s+(?:default\\s+)?(?:function|class|const|let|var|type|interface|enum)\\s+(\\w+)/gm\n```\n\n### Phantom Path Patterns (quality/phantom-paths/validator.ts)\n\n- Markdown links: `/\\[(?:[^\\]]*)\\]\\((\\.[^)]+)\\)/g`\n- Backtick paths: `` /`((?:src\\/|\\.\\.?\\/)[^`]+\\.[a-z]{1,4})`/g ``\n- Prose-embedded: `/(?:from|in|by|via|see)\\s+`?(src\\/[\\w\\-./]+)`?/gi`\n\n### Rate Limit Patterns (ai/service.ts)\n\n```typescript\n['rate limit', '429', 'too many requests', 'overloaded']\n```\n\n### Trace Event Types (orchestration/types.ts)\n\n`phase:start/end`, `worker:start/end`, `task:pickup/done`, `task:start`, `subprocess:spawn/exit`, `retry`, `discovery:start/end`, `filter:applied`, `plan:created`, `config:loaded`\n\n### Settings.json Schemas (installer/operations.ts)\n\n**Claude:**\n```typescript\ninterface SettingsJson {\n  hooks?: {\n    SessionStart?: HookEvent[];\n    SessionEnd?: HookEvent[];\n  };\n  permissions?: {\n    allow?: string[];\n    deny?: string[];\n  };\n}\n\ninterface HookEvent {\n  hooks: SessionHook[];\n}\n\ninterface SessionHook {\n  type: 'command';\n  command: string;\n}\n```\n\n**Gemini:**\n```typescript\ninterface GeminiSettingsJson {\n  hooks?: {\n    SessionStart?: GeminiHook[];\n    SessionEnd?: GeminiHook[];\n  };\n}\n\ninterface GeminiHook {\n  name: string;\n  type: 'command';\n  command: string;\n}\n```\n\n## File Relationships\n\n**CLI orchestration chain:**\n1. `cli/index.ts` parses args → routes to `cli/{command}.ts`\n2. Command modules call `config/loader.ts` → `loadConfig()` with Zod validation\n3. Commands call `discovery/run.ts` → `discoverFiles()` with filter chain\n4. `cli/generate.ts` calls `generation/orchestrator.ts` → `createOrchestrator().createPlan()`, `orchestration/runner.ts` → `CommandRunner.executeGenerate()`\n5. Runner calls `ai/service.ts` → `AIService.call()` → `subprocess.ts` → `runSubprocess()` → Node.js `execFile()`\n6. Runner calls `generation/writers/sum.ts` → `writeSumFile()` with YAML frontmatter, `generation/writers/agents-md.ts` → `writeAgentsMd()` preserving user content\n7. Post-generation calls `quality/index.ts` → `checkCodeVsDoc()`, `checkCodeVsCode()`, `checkPhantomPaths()` → `buildInconsistencyReport()`\n\n**Trace emission flow:**\n1. CLI creates `TraceWriter` via `orchestration/trace.ts` → `createTraceWriter()`\n2. Threads `tracer` through `CommandRunOptions` → `PoolOptions` → `AIServiceOptions`\n3. Runner emits `phase:start/end`, pool emits `worker:start/end` + `task:pickup/done`, AIService emits `subprocess:spawn/exit` + `retry`\n4. TraceWriter serializes via promise-chain writes, finalized via `tracer.finalize()` after all phases\n\n**Update workflow chain:**\n1. `cli/update.ts` calls `update/orchestrator.ts` → `preparePlan()` reading `.sum` frontmatter\n2. Orchestrator calls `change-detection/detector.ts` → `computeContentHash()` for comparison\n3. Orchestrator calls `update/orphan-cleaner.ts` → `cleanupOrphans()`, `cleanupEmptyDirectoryDocs()`\n4. CLI calls `orchestration/runner.ts` → `CommandRunner.executeUpdate()` for file regeneration\n5. CLI loops `affectedDirs` calling `generation/prompts/builder.ts` → `buildDirectoryPrompt()`, `generation/writers/agents-md.ts` → `writeAgentsMd()`\n\n**Installer workflow chain:**\n1. `installer/index.ts` → `parseInstallerArgs()` → `runInstall()` or `runUninstall()`\n2. Operations call `installer/paths.ts` → `getRuntimePaths()` with environment overrides\n3. Operations call `integration/templates.ts` → `getClaudeTemplates()`/`getOpenCodeTemplates()`/`getGeminiTemplates()`\n4. Operations read bundled hooks from `hooks/dist/` via `getBundledHookPath()`\n5. Operations modify `settings.json` via JSON parse/stringify for hook/permission registration\n6. Uninstallation calls `installer/uninstall.ts` → `cleanupOrphans()`, `cleanupEmptyDirs()`, `cleanupLegacyGeminiFiles()`",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 6142,
      "cacheReadTokens": 17587,
      "cacheCreationTokens": 34076,
      "latencyMs": 149100,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": []
    },
    {
      "timestamp": "2026-02-09T17:51:47.318Z",
      "prompt": "Generate AGENTS.md for directory: \".\" (agents-reverse-engineer)\n\n## File Summaries (5 files)\n\n### LANGUAGES-MANIFEST.md\n**Purpose:** LANGUAGES-MANIFEST.md enumerates package manifest file types across 25 programming language ecosystems with their ass...\n\n**LANGUAGES-MANIFEST.md enumerates package manifest file types across 25 programming language ecosystems with their associated package managers and current support status in ARE's manifest detection system.**\n\n## Manifest Detection Coverage\n\nThe table documents 9 supported manifest types used by `extractDirectoryImports()` and directory aggregation prompts:\n\n- **JavaScript/TypeScript**: `package.json` (npm/yarn/pnpm)\n- **Python**: `requirements.txt`, `pyproject.toml`, `setup.py`, `Pipfile` (pip/poetry/pipenv)\n- **Go**: `go.mod` (Go modules)\n- **Rust**: `Cargo.toml` (cargo)\n- **Java**: `pom.xml` (Maven), `build.gradle` (Gradle)\n- **PHP**: `composer.json` (Composer)\n- **C/C++**: `CMakeLists.txt`, `Makefile` (CMake/Make)\n\nSupport checkmarks (✓) indicate manifest types actively detected during Phase 2 directory aggregation to include dependency/build configuration context in `AGENTS.md` generation prompts.\n\n## Unsupported Ecosystems\n\nThe table lists 16 additional language ecosystems without current support:\n\n- **Ruby**: `Gemfile` (bundler)\n- **Kotlin**: `build.gradle.kts`, `build.gradle` (Gradle)\n- **C#/.NET**: `*.csproj`, `packages.config`, `*.fsproj` (NuGet)\n- **Swift**: `Package.swift` (Swift PM)\n- **Elixir**: `mix.exs` (Mix)\n- **Erlang**: `rebar.config` (rebar3)\n- **Scala**: `build.sbt` (sbt)\n- **Clojure**: `deps.edn`, `project.clj` (clj/Leiningen)\n- **Haskell**: `package.yaml`, `*.cabal`, `stack.yaml` (cabal/stack)\n- **Dart/Flutter**: `pubspec.yaml` (pub)\n- **Lua**: `*.rockspec` (LuaRocks)\n- **R**: `DESCRIPTION` (CRAN)\n- **Julia**: `Project.toml` (Pkg)\n- **Zig**: `build.zig.zon` (zig)\n- **Nim**: `*.nimble` (nimble)\n- **OCaml**: `dune-project`, `*.opam` (dune/opam)\n\nAdditional manifest support requires extending `src/generation/prompts/builder.ts` manifest detection logic and `src/generation/prompts/templates.ts` directory aggregation prompt to include discovered manifest content.\n\n## Reference Source\n\nThis table serves as specification document for cross-language manifest detection expansion. AI coding assistants implementing manifest-aware prompting should prioritize ecosystems marked with ✓ and treat unmarked entries as roadmap items requiring detection pattern additions to `extractDirectoryImports()` and conditional inclusion logic in directory aggregation prompt construction.\n### LICENSE\n**Purpose:** LICENSE declares MIT license grant for agents-reverse-engineer software, granting unrestricted usage, modification, a...\n\n**LICENSE declares MIT license grant for agents-reverse-engineer software, granting unrestricted usage, modification, and distribution rights to all recipients.**\n\n## License Terms\n\nGrants permission without restriction for:\n- use\n- copy\n- modify\n- merge\n- publish\n- distribute\n- sublicense\n- sell\n\nof the Software and associated documentation files.\n\n## Copyright Holder\n\nCopyright (c) 2026 GeoloeG-IsT\n\n## Warranty Disclaimer\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.\n\n## Liability Waiver\n\nIN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n## Inclusion Requirement\n\nThis copyright notice and permission notice shall be included in all copies or substantial portions of the Software.\n\n## Legal Effect\n\nThis is a standard MIT License text providing maximum permissiveness while disclaiming all warranties and liability. No exports, no executable code—purely a legal artifact required for npm package publication and downstream redistribution compliance.\n### README.md\n**Purpose:** README.md provides user-facing documentation for agents-reverse-engineer CLI, covering installation via `npx agents-r...\n\n**README.md provides user-facing documentation for agents-reverse-engineer CLI, covering installation via `npx agents-reverse-engineer@latest`, command workflows (`/are-init`, `/are-discover`, `/are-generate`, `/are-update`, `/are-specify`, `/are-clean`), generated artifact formats (`.sum` files with YAML frontmatter, `AGENTS.md` directory overviews, runtime-specific root documents), configuration schema for `.agents-reverse-engineer/config.yaml`, and integration patterns with Claude Code, OpenCode, and Gemini CLI.**\n\n## Installation Workflows\n\nInteractive installer invoked via `npx agents-reverse-engineer@latest` prompts for runtime selection (`claude`, `opencode`, `gemini`, `all`) and installation scope (global `-g` targeting `~/.claude/`, local `-l` targeting `./.claude/`). Non-interactive installation accepts `--runtime <rt>` flag with `-g`/`-l` flags. Uninstallation via `npx agents-reverse-engineer@latest uninstall` removes command files (`/are-*`), session hooks (Claude/Gemini), ARE permissions from settings.json, and `.agents-reverse-engineer` folder (local installs only).\n\n## Command Reference\n\nCLI commands execute via `are <command>` binary:\n- `are install` — Interactive installer with runtime/location prompts\n- `are install --runtime <rt> -g` — Install to runtime globally\n- `are install --runtime <rt> -l` — Install to runtime locally\n- `are install -u` — Uninstall (remove files/hooks)\n- `are init` — Create `.agents-reverse-engineer/config.yaml` configuration file\n- `are discover` — List files for analysis, optionally write `GENERATION-PLAN.md` with `--plan` flag\n- `are discover --show-excluded` — Display excluded files with exclusion reasons\n- `are generate` — Execute three-phase pipeline (file analysis → directory aggregation → root synthesis)\n- `are update` — Incremental regeneration for changed files only\n- `are specify` — Synthesize `specs/SPEC.md` from all `AGENTS.md` files, supports `--multi-file` for split output\n- `are clean` — Remove `.sum`, `AGENTS.md` (generated only), root docs, `GENERATION-PLAN.md`\n\nAI assistant commands mirror CLI with `/are-` prefix: `/are-init`, `/are-discover`, `/are-generate`, `/are-update`, `/are-specify`, `/are-clean`. Supported runtimes: Claude Code, OpenCode, Gemini CLI.\n\n## Generated Documentation Structure\n\n`.sum` files contain YAML frontmatter with `file_type`, `generated_at` timestamp, followed by markdown sections:\n- `## Purpose` — One-line role statement\n- `## Public Interface` — Exported functions/classes/types with signatures\n- `## Dependencies` — Import statements with usage descriptions\n- `## Implementation Notes` — Behavioral contracts, constraints, patterns\n\n`AGENTS.md` directory overviews include:\n- Directory role description\n- Files grouped by purpose (Types, Services, Utils)\n- Subdirectory briefs\n\nRoot documents (`CLAUDE.md`, `GEMINI.md`, `OPENCODE.md`) serve as runtime-specific project entry points auto-loaded by respective AI assistants. Universal `AGENTS.md` at project root provides standard format overview.\n\n## Configuration Schema\n\n`.agents-reverse-engineer/config.yaml` structure:\n\n```yaml\nexclude:\n  patterns: []              # Gitignore-style globs (e.g., [\"*.log\", \"temp/**\"])\n  vendorDirs:               # Skip directories (node_modules, dist, .git)\n  binaryExtensions:         # Skip file types (.png, .jpg, .pdf)\n\noptions:\n  followSymlinks: false     # Symbolic link traversal flag\n  maxFileSize: 1048576      # Binary detection threshold (1MB)\n\noutput:\n  colors: true              # ANSI color codes in terminal\n  verbose: true             # Per-file processing display\n\nai:\n  backend: auto             # Backend: 'claude', 'gemini', 'opencode', 'auto'\n  model: sonnet             # Backend-specific model identifier\n  timeoutMs: 300000         # Subprocess timeout (5 minutes default)\n  maxRetries: 3             # Exponential backoff retry attempts\n  concurrency: 5            # Worker pool size (1-10, WSL defaults to 2)\n  \n  telemetry:\n    keepRuns: 50            # Run log retention count\n    costThresholdUsd: 10.0  # Cost warning threshold (USD)\n  \n  pricing:                  # Per-model token cost overrides\n    claude-opus-4:\n      inputCostPerMTok: 15.0   # USD per 1M input tokens\n      outputCostPerMTok: 75.0  # USD per 1M output tokens\n```\n\nKey options:\n- `ai.concurrency` range `1-10` (lower for resource-constrained environments, WSL defaults to 2)\n- `ai.timeoutMs` default `300000` (increase for large files/slow connections)\n- `exclude.patterns` accepts glob patterns for custom exclusions\n- `options.maxFileSize` defines binary detection threshold in bytes\n\n## Three-Phase Generation Workflow\n\nPhase 1 (File Analysis): Concurrent subprocess pool generates `.sum` files for each source file discovered by `are discover`. Post-order traversal ensures deepest directories processed first.\n\nPhase 2 (Directory Aggregation): Sequential `AGENTS.md` generation for each directory, consuming child `.sum` files and subdirectory `AGENTS.md` files. Preserves user-authored content by renaming `AGENTS.md` → `AGENTS.local.md` before generation.\n\nPhase 3 (Root Synthesis): Sequential generation of `CLAUDE.md`, `GEMINI.md`, `OPENCODE.md` from aggregated `AGENTS.md` corpus. Each runtime receives tailored integration instructions.\n\nProgress logged to `.agents-reverse-engineer/progress.log`, viewable via `tail -f` during long-running operations.\n\n## Incremental Update Strategy\n\n`are update` workflow:\n1. Read `content_hash` from `.sum` YAML frontmatter\n2. Compute SHA-256 hash of current file content\n3. Hash mismatch → regenerate `.sum`\n4. Hash match → skip file\n5. Detect orphans (`.sum` for deleted source files)\n6. Cleanup orphaned `.sum` files and empty `AGENTS.md` directories\n7. Regenerate affected directory `AGENTS.md` via parent directory traversal\n\n## Requirements and Compatibility\n\n- Node.js ≥18.0.0\n- AI assistant supporting `AGENTS.md` format:\n  - Claude Code (full support + session hooks)\n  - Gemini CLI (full support + session hooks)\n  - OpenCode (AGENTS.md supported)\n  - Any assistant reading `AGENTS.md`\n\nSession hooks auto-update documentation on session end for Claude Code and Gemini CLI (disable via `ARE_DISABLE_HOOK=1` environment variable).\n\n## Messaging and Value Proposition\n\nUser testimonials:\n- _\"Finally, my AI assistant actually understands my codebase structure.\"_\n- _\"No more explaining the same architecture in every conversation.\"_\n\nProblem statement: AI coding assistants lack persistent codebase knowledge, requiring repeated context explanation across sessions.\n\nSolution: Generate AI-readable documentation (`.sum`, `AGENTS.md`, root docs) consumed by assistants for persistent understanding.\n\nTarget audience: Developers using Claude Code, OpenCode, Gemini CLI, or AGENTS.md-compatible assistants seeking to eliminate repetitive context provision.\n### package.json\n**Purpose:** package.json defines npm package metadata, CLI entry points, build scripts, runtime dependencies, and distribution co...\n\n**package.json defines npm package metadata, CLI entry points, build scripts, runtime dependencies, and distribution configuration for the agents-reverse-engineer project.**\n\n## Package Identity\n\n`name` is `\"agents-reverse-engineer\"`, `version` is `\"0.6.6\"`, `description` is `\"CLI tool for reverse-engineering codebase documentation for AI agents\"`. `author` is `\"GeoloeG-IsT\"`, `license` is `\"MIT\"`. `type` is `\"module\"` (ES modules). Repository URL is `git+https://github.com/GeoloeG-IsT/agents-reverse-engineer.git`.\n\n## Binary Entry Points\n\n`bin` maps two CLI commands to the same entry point:\n- `\"agents-reverse-engineer\"` → `\"dist/cli/index.js\"`\n- `\"are\"` → `\"dist/cli/index.js\"`\n\nBoth invoke the compiled TypeScript CLI dispatcher from `src/cli/index.ts`.\n\n## Build Scripts\n\n`scripts` section defines lifecycle and development commands:\n- `\"build\"`: `\"tsc\"` — compiles TypeScript source to `dist/` via TypeScript compiler\n- `\"build:hooks\"`: `\"node scripts/build-hooks.js\"` — copies `hooks/` directory to `hooks/dist/` for npm tarball inclusion\n- `\"prepack\"`: `\"rm -f LICENSE.sum README.md.sum\"` — removes generated summary files before packaging to prevent accidental inclusion\n- `\"prepublishOnly\"`: `\"npm run build && npm run build:hooks\"` — ensures TypeScript compilation and hook distribution before npm publish\n- `\"dev\"`: `\"tsx watch src/cli/index.ts\"` — hot-reload development mode using tsx runtime\n\n## Runtime Dependencies\n\nProduction dependencies required at runtime:\n- `fast-glob` ^3.3.3 — file discovery with glob patterns\n- `ignore` ^7.0.3 — gitignore parsing for file filtering\n- `isbinaryfile` ^5.0.4 — binary file detection via extension and content analysis\n- `ora` ^8.1.1 — terminal spinner UI for progress indication\n- `picocolors` ^1.1.1 — ANSI terminal color formatting\n- `simple-git` ^3.27.0 — git diff parsing for change detection\n- `yaml` ^2.7.0 — YAML configuration file parsing\n- `zod` ^3.24.1 — schema validation for configuration\n\n## Development Dependencies\n\nBuild-time dependencies not included in published package:\n- `@types/node` ^22.10.7 — TypeScript type definitions for Node.js APIs\n- `tsx` ^4.19.2 — TypeScript execution and watch mode for development\n- `typescript` ^5.7.3 — TypeScript compiler\n\n## Distribution Configuration\n\n`files` array specifies npm tarball contents:\n- `\"dist\"` — compiled JavaScript output from TypeScript build\n- `\"hooks/dist\"` — session lifecycle hooks copied by `build:hooks` script\n- `\"README.md\"` — project documentation\n- `\"LICENSE\"` — MIT license text\n\nExcluded from tarball: source TypeScript files (`src/`), configuration files (`tsconfig.json`), build scripts (`scripts/`), original hooks (`hooks/*.js` without `dist/` prefix).\n\n## Engine Requirements\n\n`engines.node` specifies `\">=18.0.0\"` — requires Node.js version 18.0.0 or higher for ES module support and modern JavaScript features.\n\n## Package Metadata\n\n`keywords` array for npm search discoverability: `[\"documentation\", \"codebase\", \"ai\", \"agents\", \"reverse-engineering\"]`. `main` points to `\"dist/cli/index.js\"` as CommonJS entry point fallback. `bugs.url` directs to GitHub issues. `homepage` links to repository README.\n### tsconfig.json\n**Purpose:** tsconfig.json configures TypeScript compiler for ES2022/NodeNext module system with strict type-checking, declaration...\n\n**tsconfig.json configures TypeScript compiler for ES2022/NodeNext module system with strict type-checking, declaration map generation, and source map output to the dist/ directory.**\n\n## Compiler Target and Module System\n\n- `target: \"ES2022\"` compiles TypeScript to ECMAScript 2022 syntax\n- `module: \"NodeNext\"` enables Node.js native ES module support with package.json \"type\": \"module\"\n- `moduleResolution: \"NodeNext\"` resolves imports using Node.js ESM algorithm (requires .js extensions in imports)\n- `lib: [\"ES2022\"]` provides ES2022 standard library type definitions\n\n## Output Configuration\n\n- `outDir: \"dist\"` emits compiled JavaScript to dist/ directory (mirrors src/ structure)\n- `rootDir: \"src\"` establishes src/ as compilation root (prevents ../node_modules pollution in dist/)\n- `declaration: true` generates .d.ts type declaration files alongside .js output\n- `declarationMap: true` creates .d.ts.map files for IDE \"Go to Definition\" navigation to source .ts files\n- `sourceMap: true` emits .js.map files for runtime debugging with original TypeScript line numbers\n\n## Type Safety Settings\n\n- `strict: true` enables all strict type-checking options (noImplicitAny, strictNullChecks, strictFunctionTypes, strictBindCallApply, strictPropertyInitialization, noImplicitThis, alwaysStrict, useUnknownInCatchVariables)\n- `forceConsistentCasingInFileNames: true` enforces case-sensitive imports (prevents cross-platform file resolution issues)\n- `skipLibCheck: true` skips type-checking of .d.ts files in node_modules (improves compile speed)\n- `isolatedModules: true` ensures each file can be transpiled independently (required for build tools like esbuild, enforces no const enums or ambient module declarations)\n\n## Module Interoperability\n\n- `esModuleInterop: true` enables default import syntax for CommonJS modules (import fs from 'fs' instead of import * as fs)\n- `resolveJsonModule: true` allows importing .json files with type inference (e.g., import pkg from './package.json')\n\n## Compilation Scope\n\n- `include: [\"src/**/*\"]` compiles all TypeScript files recursively under src/\n- `exclude: [\"node_modules\", \"dist\"]` prevents compilation of dependencies and previous build output\n\n## Build Integration\n\nReferenced by npm scripts in package.json:\n- `npm run build` invokes `tsc` which reads this configuration\n- `npm run dev` runs `tsx watch src/cli/index.ts` (bypasses tsc, uses tsx runtime transpiler)\n- `prepublishOnly` hook executes `npm run build` before publishing to npm registry\n\n## ES Module Constraints\n\nNodeNext module resolution requires:\n- Explicit .js extensions in relative imports (import { foo } from './bar.js' even though source is bar.ts)\n- Package.json with `\"type\": \"module\"` at project root\n- File extensions must match: .ts → .js, .mts → .mjs, .cts → .cjs\n\n## Project Directory Structure\n\n<project-structure>\n./\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  publish.yml\ndocs/\n  INPUT.md\nhooks/\n  are-check-update.js\n  are-session-end.js\n  opencode-are-check-update.js\n  opencode-are-session-end.js\nscripts/\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  orchestrator.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/density/\n  validator.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orchestrator.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\n\n## Subdirectories\n\n### docs/\n<!-- Generated by agents-reverse-engineer -->\n\n# docs/\n\nINPUT.md defines the original vision, feature requirements, and Recursive Language Model (RLM) algorithm specification for agents-reverse-engineer—the foundational design document that establishes the three-phase brownfield documentation strategy (file-level `.sum` generation, directory-level `AGENTS.md` synthesis, root document convergence), CLI command contracts (`/are-generate`, `/are-update`), session lifecycle hook requirements, and integration references to complementary tools (SpecKit, BMAD, Get Shit Done).\n\n## Contents\n\n**[INPUT.md](./INPUT.md)** — Specifies RLM post-order traversal algorithm (leaf-to-root aggregation), command execution requirements via Claude Code or platform-specific alternatives, session-end auto-update hooks, multi-tier documentation system (`AGENTS.md`, `ARCHITECTURE.md`, `STRUCTURE.md`, `STACK.md`, `INTEGRATIONS.md`, `INFRASTRUCTURE.md`, `CONVENTIONS.md`, `TESTING.md`, `PATTERNS.md`, `CONCERNS.md`), and references three inspirational projects for implementation context: SpecKit (GitHub's specification toolkit), BMAD (methodology framework), GSD (workflow system).\n\n## Behavioral Contracts\n\n**RLM Algorithm Execution Order:**\n1. Build project structure tree via directory traversal\n2. Execute analysis starting at first leaf node, proceeding recursively backward (post-order traversal)\n3. File-level: Generate `{filename}.sum` for each source file leaf\n4. Directory-level: When all leaves in directory are summarized, analyze directory and generate `AGENTS.md` plus additional documentation if needed\n5. Root convergence: Continue recursive aggregation until project root is reached\n\n**CLI Command Surface:**\n- `/are-generate` — Full documentation generation from scratch\n- `/are-update` — Incremental update of impacted files\n\n**Referenced External Projects:**\n- SpecKit: https://github.com/github/spec-kit\n- BMAD: https://github.com/bmad-code-org/BMAD-METHOD\n- Get Shit Done: https://github.com/glittercowboy/get-shit-done\n### hooks/\n<!-- Generated by agents-reverse-engineer -->\n\n# hooks\n\n**IDE session lifecycle hooks for automatic ARE version checking and documentation updates, implemented as detached background processes for Claude Code, Gemini CLI, and OpenCode runtimes.**\n\n## Contents\n\n### Session Lifecycle Hooks\n\n**[are-check-update.js](./are-check-update.js)** — SessionStart hook spawning detached background process querying npm registry via `npm view agents-reverse-engineer version`, comparing against local/global `~/.claude/ARE-VERSION`, caching results to `~/.claude/cache/are-update-check.json` with timestamp + version metadata. 10s timeout enforced. Graceful degradation on network/git failures (sets `latest: 'unknown'`).\n\n**[are-session-end.js](./are-session-end.js)** — SessionEnd hook executing `npx agents-reverse-engineer@latest update --quiet` as detached background process when git working tree changes detected via `git status --porcelain`. Honors disable flags: `ARE_DISABLE_HOOK=1` environment variable or `hook_enabled: false` in `.agents-reverse-engineer.yaml` (substring search, no parser). Silent exit on no changes.\n\n### OpenCode Plugin Wrappers\n\n**[opencode-are-check-update.js](./opencode-are-check-update.js)** — `AreCheckUpdate()` async factory returning OpenCode plugin with `event['session.created']` handler. Equivalent logic to `are-check-update.js` but adapted to OpenCode plugin system. Checks `~/.config/opencode/ARE-VERSION` (project-local `.opencode/ARE-VERSION` first), writes cache to `~/.config/opencode/cache/are-update-check.json`.\n\n**[opencode-are-session-end.js](./opencode-are-session-end.js)** — `AreSessionEnd()` async factory returning OpenCode plugin with `event['session.deleted']` handler. Equivalent logic to `are-session-end.js` wrapped in plugin interface contract. Same disable mechanisms + git change detection + detached `npx` spawn pattern.\n\n## Architecture\n\n### Detached Background Process Pattern\n\nAll hooks use identical spawn pattern to prevent blocking IDE session lifecycle:\n\n```javascript\nspawn(process.execPath, ['-e', scriptString], {\n  stdio: 'ignore',      // discard stdout/stderr/stdin\n  detached: true,       // separate process group, survives parent exit\n  windowsHide: true     // suppress console window on Windows\n})\nchild.unref()           // allow Node.js event loop exit without waiting\n```\n\nSerialized script strings injected via `-e` flag contain inline logic (version checks, npm queries, cache writes). Parent process exits immediately after spawn, background child completes asynchronously.\n\n### Version File Resolution Priority\n\nBoth check hooks follow precedence order:\n1. **Project-local**: `<cwd>/.claude/ARE-VERSION` or `<cwd>/.opencode/ARE-VERSION`\n2. **Global install**: `~/.claude/ARE-VERSION` or `~/.config/opencode/ARE-VERSION`\n3. **Default fallback**: `'0.0.0'` if neither exists\n\n### Cache File Format (JSON)\n\nWritten by check hooks to `~/.{claude,opencode}/cache/are-update-check.json`:\n\n```json\n{\n  \"update_available\": true,\n  \"installed\": \"0.5.2\",\n  \"latest\": \"0.6.0\",\n  \"checked\": 1738886400\n}\n```\n\nKeys: `update_available` (boolean via `installed !== latest`), `installed` (string from ARE-VERSION or `'0.0.0'`), `latest` (string from `npm view` or `'unknown'` on timeout), `checked` (Unix epoch seconds).\n\n### Disable Mechanisms\n\nSession-end hooks check two flags before execution:\n\n1. **Environment variable**: `process.env.ARE_DISABLE_HOOK === '1'`\n2. **Config file substring**: `.agents-reverse-engineer.yaml` contains `'hook_enabled: false'` (raw `readFileSync()` + `String.includes()`, no YAML parser)\n\nBoth checks trigger silent exit (code 0) to prevent observable failures when user disables hooks.\n\n## Integration Points\n\n**Installed by**: `src/installer/operations.ts` copies files from `hooks/` → `~/.claude/hooks/` or `~/.config/opencode/plugins/` during `are --runtime <platform> -g` command execution.\n\n**Built by**: `scripts/build-hooks.js` duplicates `hooks/*.js` → `hooks/dist/*.js` for npm tarball inclusion during `npm run build:hooks` (invoked by `prepublishOnly`).\n\n**Consumed by**: IDE runtimes invoke hooks automatically on session lifecycle events:\n- Claude Code: calls `~/.claude/hooks/session-start/are-check-update.js`, `~/.claude/hooks/session-end/are-session-end.js`\n- Gemini CLI: equivalent paths under `~/.gemini/hooks/`\n- OpenCode: loads plugins from `~/.config/opencode/plugins/`, invokes `event['session.created']` / `event['session.deleted']` handlers\n\n## Behavioral Contracts\n\n### npm Registry Query Command\n\n```javascript\nexecSync('npm view agents-reverse-engineer version', { \n  encoding: 'utf8', \n  timeout: 10000, \n  windowsHide: true \n})\n```\n\nReturns latest published version string (e.g., `\"0.6.5\\n\"`). Timeout 10000ms enforced to prevent indefinite hangs. Throws on network failures, caught by hooks to set `latest: 'unknown'`.\n\n### Git Change Detection Command\n\n```javascript\nexecSync('git status --porcelain', { encoding: 'utf-8' })\n```\n\nReturns empty string when working tree clean, non-empty on uncommitted changes. Throws when not git repo or git unavailable, caught by session-end hooks to skip update spawn.\n\n### npx Update Invocation Command\n\n```bash\nnpx agents-reverse-engineer@latest update --quiet\n```\n\n`@latest` specifier forces npm registry fetch, bypassing local cache. `--quiet` flag suppresses terminal output (logs to `.agents-reverse-engineer/progress.log` only). Spawned as detached child to prevent blocking IDE session close.\n\n## Platform Compatibility\n\n**Path resolution**: Uses `os.homedir()` + `path.join()` for cross-platform directory construction (`~/.claude`, `~/.config/opencode`, `~/.gemini`).\n\n**Windows-specific flags**: `windowsHide: true` in `spawn()` and `execSync()` suppresses console window creation.\n\n**Node.js compatibility**: `#!/usr/bin/env node` shebang enables direct execution without explicit interpreter invocation. Requires Node.js ≥18.0.0 (same as ARE runtime requirement).\n\n## Dependencies\n\nAll four hooks use identical Node.js built-in module subset:\n- `child_process`: `spawn`, `execSync`\n- `fs`: `existsSync`, `mkdirSync`, `readFileSync`, `writeFileSync`\n- `os`: `homedir`\n- `path`: `join`\n\nNo external npm dependencies — statically analyzable, zero-install executable scripts.\n### scripts/\n<!-- Generated by agents-reverse-engineer -->\n\n# scripts\n\nBuild automation scripts for npm lifecycle hooks, prepublishOnly hook asset preparation, and distribution artifact generation.\n\n## Contents\n\n### [build-hooks.js](./build-hooks.js)\nCopies hook source files from `hooks/` to `hooks/dist/` during `prepublishOnly` lifecycle. Filters `.js` files via `readdirSync()` + `endsWith('.js')` predicate, excludes `dist` directory itself, creates target via `mkdirSync(HOOKS_DIST, { recursive: true })`, performs per-file `copyFileSync()` from `HOOKS_SRC` to `HOOKS_DIST`. Invoked by `package.json` `prepublishOnly` script chain (`npm run build && npm run build:hooks`). Ensures session lifecycle hooks (are-check-update.js, are-session-end.js, opencode-are-check-update.js, opencode-are-session-end.js) exist in npm tarball for installer/operations.ts reference during global/local hook installation.\n\n## Integration Points\n\n**Package.json lifecycle:**\n- `prepublishOnly` script executes `npm run build:hooks` → invokes build-hooks.js\n- Runs after TypeScript compilation (`npm run build`) before `npm publish`\n- Ensures hooks/dist/ directory populated for tarball inclusion\n\n**Hook installation workflow:**\n- `src/installer/operations.ts` references hooks/dist/ files during `installCommands()` execution\n- Supports both global (`~/.claude/hooks/`) and local (`.claude/hooks/`) installation modes\n- Platform-specific targets: Claude Code, OpenCode, Gemini via runtime detection\n\n**File resolution:**\n- Computes `projectRoot` via `fileURLToPath(import.meta.url)` + double `dirname()` traversal\n- Resolves absolute paths via `join(projectRoot, 'hooks')` and `join(projectRoot, 'hooks', 'dist')`\n- Prevents relative path ambiguity in npm lifecycle context\n### src/\n<!-- Generated by agents-reverse-engineer -->\n\n# src\n\n**Core implementation directory housing seven CLI commands (init, discover, generate, update, clean, specify, help), three-phase documentation pipeline orchestration (concurrent file analysis, post-order directory synthesis, sequential root integration), AI backend abstraction layer with Claude/Gemini/OpenCode adapters, gitignore-aware file discovery with composable filter chains, incremental SHA-256 hash-based change detection, quality validation subsystem detecting code-doc inconsistencies and phantom paths, worker pool concurrency control with NDJSON trace emission, and platform-specific IDE integration supporting Claude Code/OpenCode/Gemini runtimes.**\n\n## Contents\n\n**[version.ts](./version.ts)** — `getVersion()` reads `package.json` version field from parent directory of compiled module via `import.meta.url` → `fileURLToPath()` → `join(__dirname, '..', 'package.json')` path resolution, returns `'unknown'` on parse/read errors. Consumed by CLI `--version` flag and session lifecycle hooks (`hooks/are-check-update.js`) for npm registry comparison.\n\n## Subdirectories\n\n**[ai/](./ai/)** — Backend-agnostic AI service layer abstracting Claude Code, Gemini CLI, OpenCode via adapter registry. Implements exponential backoff retry (max 3 attempts, base 1s delay, 8s ceiling), rate-limit detection via stderr pattern matching (`'rate limit'`, `'429'`, `'overloaded'`), subprocess resource management with 512MB heap limits (`NODE_OPTIONS='--max-old-space-size=512'`), libuv thread pool capping (`UV_THREADPOOL_SIZE='4'`), process group killing (`kill(-pid)`) for timeout enforcement (SIGTERM at timeout, SIGKILL after 5s grace period). Emits NDJSON telemetry logs to `.agents-reverse-engineer/logs/run-<timestamp>.json` tracking token counts (input/output/cacheRead/cacheCreation), latency, retry attempts, filesRead metadata, and aggregated cost metrics. Trace emission via `ITraceWriter` for `subprocess:spawn/exit`, `retry` events.\n\n**[change-detection/](./change-detection/)** — Git-based change detection via `simple-git` diff parsing (`git diff --name-status -M <baseCommit>..HEAD`) with rename detection (50% similarity threshold), uncommitted change merge via `git.status()`, and SHA-256 content hashing (`computeContentHash()`) for non-git workflows. Status code mapping: `'A'` → `'added'`, `'M'` → `'modified'`, `'D'` → `'deleted'`, `'R*'` → `'renamed'` with `oldPath` extraction. Consumed by `src/update/orchestrator.ts` for hash-based incremental updates comparing `.sum` frontmatter `content_hash` against current file content.\n\n**[cli/](./cli/)** — Command entry points implementing `init` (config creation), `discover` (preview generation plan), `generate` (three-phase pipeline execution), `update` (incremental hash-based regeneration), `clean` (artifact deletion), `specify` (project spec synthesis). Orchestrates backend resolution via `createBackendRegistry()` + `resolveBackend()`, AIService lifecycle management (`setDebug()`, `setTracer()`, `finalize()`), trace emission to `.agents-reverse-engineer/traces/trace-<timestamp>.ndjson`, progress logging via `ProgressLog.create()` to `.agents-reverse-engineer/progress.log`. Exit codes: 0 (success), 1 (partial failure or config exists), 2 (total failure or CLI not found).\n\n**[config/](./config/)** — YAML configuration loader with Zod validation via `ConfigSchema` (exclude patterns/vendorDirs/binaryExtensions, discovery options followSymlinks/maxFileSize, output colors, AI backend/model/timeoutMs/maxRetries/concurrency/telemetry). `getDefaultConcurrency()` computes memory-aware worker pool sizing via `clamp(cores × 5, MIN=2, min(floor((totalMemGB × 0.5) / 0.512), MAX=20))` enforcing 512MB subprocess heap constraint. `writeDefaultConfig()` generates annotated YAML with `yamlScalar()` quoting patterns containing YAML metacharacters `[*{}\\[\\]?,:#&!|>'\"%@\\`]`. Emits `config:loaded` trace events.\n\n**[discovery/](./discovery/)** — Gitignore-aware file discovery executing four-stage filter chain (gitignore → vendor → binary → custom) over `fast-glob` traversal results with `absolute: true`, `onlyFiles: true`, `dot: true`, `followSymbolicLinks: false`, `ignore: ['**/.git/**']` hardcoded. `applyFilters()` spawns 30 workers sharing `files.entries()` iterator for bounded-concurrency sequential filter evaluation with early termination. Binary detection: extension fast-path (80+ extensions) → size threshold (1MB default) → `isBinaryFile()` content analysis. Returns `DiscoveryResult` with `included: string[]`, `excluded: ExcludedFile[]` with `reason` (gitignore pattern/binary/vendor/custom).\n\n**[generation/](./generation/)** — Three-phase pipeline orchestration: (1) concurrent file analysis via `GenerationOrchestrator.createFileTasks()` embedding `buildFilePrompt()` with import maps, depth-sorted deepest-first execution; (2) post-order directory aggregation via `buildExecutionPlan()` with dependency graphs (`isDirectoryComplete()` predicates, `dependencies: fileTaskIds`), `buildDirectoryPrompt()` consuming child `.sum` files and subdirectory `AGENTS.md`; (3) sequential root synthesis via `collectAgentsDocs()` recursive traversal, `buildRootPrompt()` with stack detection (9 manifest types), preamble stripping. Exports `writeSumFile()`/`readSumFile()` for YAML frontmatter serialization with `content_hash`, `writeAgentsMd()` preserving user content via `AGENTS.local.md` rename, `writeAnnexFile()` for reproduction-critical source content.\n\n**[imports/](./imports/)** — Static import analysis via `IMPORT_REGEX` (`/^import\\s+(type\\s+)?(?:\\{([^}]*)\\}|(\\*\\s+as\\s+\\w+)|(\\w+))\\s+from\\s+['\"]([^'\"]+)['\"]/gm`) matching ES module imports in first 100 lines. Classifies as `internal` (`./` prefix) or `external` (`../` prefix), filters bare package specifiers and `node:` built-ins. `formatImportMap()` produces text blocks with filename headers and indented specifier-symbol pairs for LLM prompt embedding in Phase 1 file analysis and Phase 2 directory aggregation.\n\n**[installer/](./installer/)** — npx-based installation orchestrator for ARE commands/hooks across Claude Code (`~/.claude/skills/`, `~/.claude/hooks/`), OpenCode (`~/.config/opencode/commands/`, `~/.config/opencode/plugins/`), Gemini (`~/.gemini/commands/`, `~/.gemini/hooks/`) with environment overrides (`CLAUDE_CONFIG_DIR`, `OPENCODE_CONFIG_DIR`/`XDG_CONFIG_HOME`, `GEMINI_CONFIG_DIR`). Interactive prompts via `arrowKeySelect()` TTY mode with ANSI escape sequences (`\\x1b[${n}A` cursor up, `\\x1b[2K` clear line, `\\x1b[1B` cursor down) or `numberedSelect()` fallback. `registerHooks()` modifies `settings.json` with nested structure for Claude (`{ hooks: { SessionStart?: [{ hooks: [{ type: 'command', command }] }] } }`), flat structure for Gemini (`{ hooks: { SessionStart?: [{ name, type, command }] } }`). `registerPermissions()` adds `ARE_PERMISSIONS` bash patterns (`'Bash(npx agents-reverse-engineer@latest init*)'` through `clean*`). Uninstallation via `cleanupOrphans()`, `cleanupEmptyDirs()`, `cleanupLegacyGeminiFiles()`.\n\n**[integration/](./integration/)** — Platform-specific AI assistant integration layer detecting Claude Code/OpenCode/Gemini/Aider environments via marker files (`.claude/`, `.opencode/`, `.gemini/`, `.aider/`, `CLAUDE.md`, `.aider.conf.yml`). `generateIntegrationFiles()` writes command templates with progress-monitoring patterns (background execution via `run_in_background: true`, 15s poll intervals, offset-based `.agents-reverse-engineer/progress.log` tailing, `TaskOutput` checks with `block: false`). `getTemplatesForEnvironment()` generates seven commands (generate/update/init/discover/clean/specify/help) with platform-specific frontmatter (Claude: `name: /are-<command>`, OpenCode: `agent: build`, Gemini: TOML `description`/`prompt`).\n\n**[orchestration/](./orchestration/)** — Worker pool concurrency control via iterator-based `runPool()` sharing `tasks.entries()` iterator across N workers preventing over-allocation. `CommandRunner` orchestrates three-phase pipeline: Phase 1 concurrent file analysis with `sourceContentCache` Map for single read, `computeContentHashFromString()` for `.sum` frontmatter, annex file detection via `## Annex References` marker; post-Phase 1 quality validation (concurrency=10) via `checkCodeVsDoc(source, oldSum)` (stale), `checkCodeVsDoc(source, newSum)` (fresh), `checkCodeVsCode(filesForCodeVsCode)` (duplicates); Phase 2 depth-grouped directory synthesis with `buildDirectoryPrompt()` runtime construction; post-Phase 2 phantom path validation via `checkPhantomPaths()` with three regex patterns; Phase 3 sequential root synthesis with preamble stripping. `ProgressReporter` computes ETA via sliding window (size 10, min 2 completions) as `~Ns remaining` or `~Mm Ss remaining`. `PlanTracker` serializes checkbox updates via promise-chain writes replacing `` `- [ ] \\`${itemPath}\\`` `` → `` `- [x] \\`${itemPath}\\`` ``. `TraceWriter` appends NDJSON with auto-populated `seq` (monotonic), `ts` (ISO 8601), `pid`, `elapsedMs` (high-resolution delta) to `.agents-reverse-engineer/traces/trace-<timestamp>.ndjson`, retains 500 most recent traces.\n\n**[output/](./output/)** — Terminal output formatting layer providing color-aware `Logger` interface with picocolors ANSI styling. `createLogger(options)` constructs logger respecting `options.colors` boolean flag, exports `createSilentLogger()` returning no-op logger for testing. Methods: `info(message)`, `file(path)` (`\"  +\" + path` green), `excluded(path, reason, filter)` (`\"  -\" + path + \" (reason: filter)\"` dimmed), `summary(included, excluded)` (bold/dim counts), `warn(message)` (`\"Warning: \"` yellow prefix), `error(message)` (`\"Error: \"` red prefix).\n\n**[quality/](./quality/)** — Post-generation validation system detecting: (1) code-vs-doc inconsistencies via `extractExports()` regex `/^[ \\t]*export\\s+(?:default\\s+)?(?:function|class|const|let|var|type|interface|enum)\\s+(\\w+)/gm` matching against substring search in `.sum` text; (2) code-vs-code duplicates aggregating exports into `Map<symbol, paths[]>` per directory; (3) phantom paths via three regex patterns (markdown links `/\\[(?:[^\\]]*)\\]\\((\\.[^)]+)\\)/g`, backtick paths `` /`((?:src\\/|\\.\\.?\\/)[^`]+\\.[a-z]{1,4})`/g ``, prose-embedded `/(?:from|in|by|via|see)\\s+`?(src\\/[\\w\\-./]+)`?/gi`) with `existsSync()` validation and `.ts`/`.js` fallback resolution. `buildInconsistencyReport()` aggregates discriminated union `Inconsistency = CodeDocInconsistency | CodeCodeInconsistency | PhantomPathInconsistency` with `severity: 'info' | 'warning' | 'error'`, `formatReportForCli()` renders plain text with severity tags.\n\n**[specify/](./specify/)** — Project specification synthesis from `AGENTS.md` corpus via `buildSpecPrompt()` constructing two-part prompts: system instructions (`SPEC_SYSTEM_PROMPT` enforcing 11-section conceptual organization), user content (concatenated AGENTS.md with section delimiters, optional annex files for verbatim reproduction of behavioral contracts). `writeSpec()` supports single-file (`specs/SPEC.md`) and multi-file (`specs/<slug>.md`) modes with pre-flight existence checks throwing `SpecExistsError(conflicts[])` when `force=false`, content splitting via `/^(?=# )/m` regex, filename sanitization through `slugify()` (lowercase + hyphenation + alphanumeric filtering).\n\n**[types/](./types/)** — Shared interfaces for discovery results: `ExcludedFile` (path, reason), `DiscoveryResult` (files, excluded), `DiscoveryStats` (totalFiles, includedFiles, excludedFiles, exclusionReasons histogram). Consumed by `src/discovery/run.ts` (producer), `src/cli/*.ts` (consumers), `src/output/logger.ts` (terminal output).\n\n**[update/](./update/)** — Incremental update orchestrator implementing hash-based change detection via `preparePlan()` reading `.sum` frontmatter `content_hash`, comparing against `computeContentHash()` current file content, returning `UpdatePlan` with `filesToAnalyze` (hash mismatch), `filesToSkip` (match), `cleanup` (orphans), `affectedDirs` (depth-sorted descending). `cleanupOrphans()` deletes stale `.sum`/`.annex.md` files for deleted/renamed sources, `cleanupEmptyDirectoryDocs()` removes `AGENTS.md` from directories with zero source files (excludes `GENERATED_FILES` set and dotfiles). `getAffectedDirectories()` walks parent chains via `path.dirname()` until `.` or absolute path, ensuring `AGENTS.md` regeneration propagates upward. Emits `phase:start/end`, `plan:created` trace events.\n\n## Architecture\n\n### Three-Phase Pipeline\n\n**Phase 1 (Concurrent File Analysis):** Iterator-based worker pool (`src/orchestration/pool.ts`) shares `tasks.entries()` iterator across N workers (default 2 WSL, 5 elsewhere) to prevent over-allocation. Each worker spawns AI CLI subprocess via `AIService.call()` → `runSubprocess()` → `execFile()` with resource limits (512MB heap, 4-thread libuv pool, background tasks disabled, subagents blocked). Writes `.sum` files with YAML frontmatter containing SHA-256 `content_hash`, strips conversational preamble via patterns matching `'now i'`, `'let me'`, `'here is'` prefixes and bold headers without `##` markers. Detects `## Annex References` marker for `writeAnnexFile()` reproduction-critical source content.\n\n**Phase 2 (Post-Order Directory Aggregation):** Sorts directories by depth descending via `path.relative().split(path.sep).length` (deepest first). Waits for `isDirectoryComplete()` predicate checking all child `.sum` files exist. Builds prompts via `buildDirectoryPrompt()` aggregating child `.sum` content, subdirectory `AGENTS.md` files, import maps via `extractDirectoryImports()`, manifest detection (9 types: package.json, Cargo.toml, go.mod, pyproject.toml, pom.xml, build.gradle, Gemfile, composer.json, CMakeLists.txt/Makefile). Preserves user content via `AGENTS.local.md` rename, prepends above generated sections with `---` separator.\n\n**Phase 3 (Sequential Root Synthesis):** Collects all `AGENTS.md` files via `collectAgentsDocs()` recursive traversal (skips 13 directories: node_modules, .git, .agents-reverse-engineer, vendor, dist, build, __pycache__, .next, venv, .venv, target, .cargo, .gradle). Builds root prompts via `buildRootPrompt()` consuming aggregated docs, enforcing synthesis-only constraints (no feature/hook/pattern invention), strips conversational preamble before writing `CLAUDE.md`, `GEMINI.md`, `OPENCODE.md` platform-specific integration documents.\n\n### Subprocess Resource Management\n\nMitigates Claude CLI thread exhaustion (GitHub #5771: 200 Node.js instances):\n- `NODE_OPTIONS='--max-old-space-size=512'` limits heap to 512MB per subprocess\n- `UV_THREADPOOL_SIZE='4'` constrains libuv thread pool to 4 threads\n- `CLAUDE_CODE_DISABLE_BACKGROUND_TASKS='1'` prevents background task spawning\n- `--disallowedTools Task` prevents subagent spawning\n- Process group killing via `kill(-pid)` terminates subprocess trees\n- Default concurrency 2 for WSL environments (5 elsewhere)\n\n### Incremental Update Strategy\n\n`UpdateOrchestrator.preparePlan()` reads `content_hash` from `.sum` frontmatter, computes current file hash via `computeContentHash()`, populates `filesToAnalyze` (added/modified) and `filesToSkip` (unchanged). `cleanupOrphans()` deletes stale `.sum`/`.annex.md` for deleted/renamed sources. `getAffectedDirectories()` walks parent chains computing depth-sorted set for `AGENTS.md` regeneration propagation. Phase 1 pool regenerates `.sum` for `filesToAnalyze`, Phase 2 regenerates `AGENTS.md` for `affectedDirs` sequentially without full post-order traversal.\n\n## Behavioral Contracts\n\n### Default Vendor Directories (config/defaults.ts)\n\n```typescript\nDEFAULT_VENDOR_DIRS = ['node_modules', '.git', 'dist', 'build', 'target', '.next', '__pycache__', 'venv', '.venv', '.cargo', '.gradle', '.agents-reverse-engineer', '.agents', '.planning', '.claude', '.opencode', '.gemini']\n```\n\n### Default Concurrency Formula (config/defaults.ts)\n\n```typescript\nclamp(cores × 5, MIN=2, min(floor((totalMemGB × 0.5) / 0.512), MAX=20))\n```\n\n### Preamble Detection Patterns (orchestration/runner.ts)\n\nFull patterns preserved in [orchestration/runner.ts.annex.md](./orchestration/runner.ts.annex.md).\n\n**Separator pattern:** Detects `\\n---\\n` within first 500 chars.\n\n**Bold header pattern:** Matches `/^[\\s\\S]{0,500}?(\\*\\*[A-Z])/`, strips preceding content if <300 chars, no `##` markers.\n\n**Preamble prefixes (case-insensitive):** `'now i'`, `'perfect'`, `'based on'`, `'let me'`, `'here is'`, `'i\\'ll'`, `'i will'`, `'great'`, `'okay'`, `'sure'`, `'certainly'`, `'alright'`.\n\n### Export Extraction Regex (quality/inconsistency/code-vs-doc.ts)\n\n```regex\n/^[ \\t]*export\\s+(?:default\\s+)?(?:function|class|const|let|var|type|interface|enum)\\s+(\\w+)/gm\n```\n\n### Phantom Path Patterns (quality/phantom-paths/validator.ts)\n\n- Markdown links: `/\\[(?:[^\\]]*)\\]\\((\\.[^)]+)\\)/g`\n- Backtick paths: `` /`((?:src\\/|\\.\\.?\\/)[^`]+\\.[a-z]{1,4})`/g ``\n- Prose-embedded: `/(?:from|in|by|via|see)\\s+`?(src\\/[\\w\\-./]+)`?/gi`\n\n### Rate Limit Patterns (ai/service.ts)\n\n```typescript\n['rate limit', '429', 'too many requests', 'overloaded']\n```\n\n### Trace Event Types (orchestration/types.ts)\n\n`phase:start/end`, `worker:start/end`, `task:pickup/done`, `task:start`, `subprocess:spawn/exit`, `retry`, `discovery:start/end`, `filter:applied`, `plan:created`, `config:loaded`\n\n### Settings.json Schemas (installer/operations.ts)\n\n**Claude:**\n```typescript\ninterface SettingsJson {\n  hooks?: {\n    SessionStart?: HookEvent[];\n    SessionEnd?: HookEvent[];\n  };\n  permissions?: {\n    allow?: string[];\n    deny?: string[];\n  };\n}\n\ninterface HookEvent {\n  hooks: SessionHook[];\n}\n\ninterface SessionHook {\n  type: 'command';\n  command: string;\n}\n```\n\n**Gemini:**\n```typescript\ninterface GeminiSettingsJson {\n  hooks?: {\n    SessionStart?: GeminiHook[];\n    SessionEnd?: GeminiHook[];\n  };\n}\n\ninterface GeminiHook {\n  name: string;\n  type: 'command';\n  command: string;\n}\n```\n\n## File Relationships\n\n**CLI orchestration chain:**\n1. `cli/index.ts` parses args → routes to `cli/{command}.ts`\n2. Command modules call `config/loader.ts` → `loadConfig()` with Zod validation\n3. Commands call `discovery/run.ts` → `discoverFiles()` with filter chain\n4. `cli/generate.ts` calls `generation/orchestrator.ts` → `createOrchestrator().createPlan()`, `orchestration/runner.ts` → `CommandRunner.executeGenerate()`\n5. Runner calls `ai/service.ts` → `AIService.call()` → `subprocess.ts` → `runSubprocess()` → Node.js `execFile()`\n6. Runner calls `generation/writers/sum.ts` → `writeSumFile()` with YAML frontmatter, `generation/writers/agents-md.ts` → `writeAgentsMd()` preserving user content\n7. Post-generation calls `quality/index.ts` → `checkCodeVsDoc()`, `checkCodeVsCode()`, `checkPhantomPaths()` → `buildInconsistencyReport()`\n\n**Trace emission flow:**\n1. CLI creates `TraceWriter` via `orchestration/trace.ts` → `createTraceWriter()`\n2. Threads `tracer` through `CommandRunOptions` → `PoolOptions` → `AIServiceOptions`\n3. Runner emits `phase:start/end`, pool emits `worker:start/end` + `task:pickup/done`, AIService emits `subprocess:spawn/exit` + `retry`\n4. TraceWriter serializes via promise-chain writes, finalized via `tracer.finalize()` after all phases\n\n**Update workflow chain:**\n1. `cli/update.ts` calls `update/orchestrator.ts` → `preparePlan()` reading `.sum` frontmatter\n2. Orchestrator calls `change-detection/detector.ts` → `computeContentHash()` for comparison\n3. Orchestrator calls `update/orphan-cleaner.ts` → `cleanupOrphans()`, `cleanupEmptyDirectoryDocs()`\n4. CLI calls `orchestration/runner.ts` → `CommandRunner.executeUpdate()` for file regeneration\n5. CLI loops `affectedDirs` calling `generation/prompts/builder.ts` → `buildDirectoryPrompt()`, `generation/writers/agents-md.ts` → `writeAgentsMd()`\n\n**Installer workflow chain:**\n1. `installer/index.ts` → `parseInstallerArgs()` → `runInstall()` or `runUninstall()`\n2. Operations call `installer/paths.ts` → `getRuntimePaths()` with environment overrides\n3. Operations call `integration/templates.ts` → `getClaudeTemplates()`/`getOpenCodeTemplates()`/`getGeminiTemplates()`\n4. Operations read bundled hooks from `hooks/dist/` via `getBundledHookPath()`\n5. Operations modify `settings.json` via JSON parse/stringify for hook/permission registration\n6. Uninstallation calls `installer/uninstall.ts` → `cleanupOrphans()`, `cleanupEmptyDirs()`, `cleanupLegacyGeminiFiles()`\n\n## Directory Hints\n\nContains manifest file(s): package.json — likely a package or project root.",
      "systemPrompt": "You are generating an AGENTS.md file — a directory-level overview for AI coding assistants.\n\nCRITICAL: Output ONLY the raw markdown content. No code fences, no preamble, no explanations, no conversational text. Your entire response IS the AGENTS.md file content.\n\nOUTPUT FORMAT:\n- First line MUST be exactly: <!-- Generated by agents-reverse-engineer -->\n- Use a # heading with the directory name\n- Write a one-paragraph purpose statement for the directory\nusage\nADAPTIVE SECTIONS:\nAnalyze the directory contents and choose the most relevant sections. Do NOT use a fixed template. Instead, select sections that best document this specific directory for an AI that needs to reconstruct or extend the project.\n\nConsider these section types (choose what applies):\n- **Contents**: Group files by purpose/category under ## headings. For each file: markdown link [filename](./filename) and a one-line description.\n- **Subdirectories**: If subdirectories exist, list them with links [dirname/](./dirname/) and brief summaries.\n- **Architecture / Data Flow**: If files form a pipeline, request/response chain, or layered architecture, document it.\n- **Stack**: If this is a package root (has package.json, Cargo.toml, go.mod, etc.), document the technology stack, key scripts, and entry points.\n- **Structure**: If the directory layout follows a convention (feature-sliced, domain-driven, MVC, etc.), document it.\n- **Patterns**: If files share recurring design patterns (factory, strategy, middleware, barrel re-export), name and document them.\n- **Configuration**: If the directory contains config files, schemas, or environment definitions, document the config surface area.\n- **API Surface**: If the directory exports a public API (barrel index, route definitions, SDK), document the interface contract.\n- **File Relationships**: How files collaborate, depend on each other, or share state.\n- **Behavioral Contracts**: If files contain regex patterns, format specifications, magic constants, or template strings that define observable behavior, collect them in a dedicated section. Preserve verbatim patterns from file summaries — do NOT paraphrase regex into prose. This section is MANDATORY when file summaries contain behavioral artifacts.\n- **Reproduction-Critical Constants**: If file summaries reference annex files (via ## Annex References sections), list them with links. Example: \"Full prompt template text: [templates.ts.annex.md](./prompts/templates.ts.annex.md)\". Do NOT reproduce annex content in AGENTS.md — just link to it.\n\nChoose any relevant sections or create your own based on the directory contents. The goal is to provide a comprehensive overview that captures the essence of the directory's role in the project and how its files work together, with a focus on what an AI coding assistant would need to know to effectively interact with this code.\n\nSCOPE:\n- AGENTS.md is a NAVIGATIONAL INDEX — help an AI find the right file quickly\n- Focus on: what each file does, how files relate, directory-level patterns\n- Do NOT reproduce full architecture sections — those belong in the root CLAUDE.md\n\nPATH ACCURACY (MANDATORY):\n- When referencing files or modules outside this directory, use ONLY paths from the \"Import Map\" section\n- Do NOT invent, rename, or guess module paths — if a path isn't in the Import Map, don't reference it\n- Use the exact directory names from \"Project Directory Structure\" — do NOT rename directories\n  (e.g., if the directory is called \"cli\", write \"src/cli/\", NOT \"src/commands/\")\n- Cross-module references must use the specifier format from actual import statements\n  (e.g., \"../generation/writers/sum.js\", NOT \"../fs/sum-file.js\")\n- If you are unsure about a path, omit the cross-reference rather than guessing\n\nCONSISTENCY (MANDATORY):\n- Do not contradict yourself within the same document\n- If you describe a technique (e.g., \"regex-based\"), do not call it something else later (e.g., \"AST-based\")\n- When stating version numbers, engines, or config fields, use ONLY values present in the file summaries\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this directory\", \"this module\", \"provides\", \"responsible for\", \"is used to\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Per-file descriptions in Contents sections: 1-2 sentences maximum. Reference key symbols but do not reproduce full summaries.\n- Behavioral contracts (regex patterns, format specs, constants) belong in a separate Behavioral Contracts section, not in per-file descriptions.\n- Annex file references: link to .annex.md files, do not inline their content. One line per annex reference.\n- Subdirectory descriptions: 1-2 sentences maximum. Capture the directory's role, not its full contents.\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- Key exported symbols from file summaries MUST appear in the directory overview\n- Preserve exact casing of identifiers\n\nUSER NOTES:\n- If \"User Notes\" are provided in the prompt, they contain user-defined instructions that will be automatically prepended to your output\n- Do NOT repeat or paraphrase user notes in your generated content — they are included separately\n- You may reference information from user notes for context",
      "response": "<!-- Generated by agents-reverse-engineer -->\n\n# agents-reverse-engineer\n\n**CLI tool implementing Recursive Language Model (RLM) algorithm for AI-driven codebase documentation via three-phase pipeline: concurrent `.sum` file generation with subprocess pools, post-order `AGENTS.md` directory aggregation, and platform-specific root synthesis (`CLAUDE.md`/`GEMINI.md`/`OPENCODE.md`) with gitignore-aware discovery, SHA-256 incremental updates, and NDJSON telemetry.**\n\n## Contents\n\n### Root Documentation\n\n**[LANGUAGES-MANIFEST.md](./LANGUAGES-MANIFEST.md)** — Enumerates 25 language ecosystem manifest types (package.json, Cargo.toml, go.mod, pyproject.toml, pom.xml, build.gradle, Gemfile, composer.json, CMakeLists.txt/Makefile, plus 16 unsupported: Gemfile, *.csproj, Package.swift, mix.exs, rebar.config, build.sbt, deps.edn, *.cabal, pubspec.yaml, *.rockspec, DESCRIPTION, Project.toml, build.zig.zon, *.nimble, dune-project) with support checkmarks indicating `extractDirectoryImports()` detection coverage. Serves as specification for manifest-aware prompt construction in Phase 2 directory aggregation.\n\n**[LICENSE](./LICENSE)** — MIT license grant for agents-reverse-engineer (copyright 2026 GeoloeG-IsT) granting unrestricted usage/modification/distribution rights with warranty disclaimer and liability waiver.\n\n**[README.md](./README.md)** — User-facing documentation covering `npx agents-reverse-engineer@latest` installation workflows (runtime selection: claude/opencode/gemini/all, scope: global `-g`/local `-l`), command reference (init/discover/generate/update/specify/clean), generated artifact formats (`.sum` with YAML frontmatter, `AGENTS.md` directories, runtime-specific roots), configuration schema (`.agents-reverse-engineer/config.yaml` with exclude patterns/vendorDirs/binaryExtensions/ai backend/concurrency/timeout/pricing), three-phase workflow (file analysis → directory aggregation → root synthesis), incremental update strategy (SHA-256 hash comparison, orphan cleanup, affected directory propagation), IDE integration hooks (session-end auto-update, version checking), requirements (Node.js ≥18.0.0, Claude Code/Gemini CLI/OpenCode support).\n\n**[package.json](./package.json)** — Package metadata: name `agents-reverse-engineer`, version `0.6.6`, license MIT. Binary entry points: `agents-reverse-engineer`/`are` → `dist/cli/index.js`. Build scripts: `build` (tsc), `build:hooks` (copies hooks/ → hooks/dist/), `prepublishOnly` (build + build:hooks), `prepack` (remove .sum artifacts), `dev` (tsx watch). Dependencies: fast-glob, ignore, isbinaryfile, ora, picocolors, simple-git, yaml, zod. DevDependencies: @types/node, tsx, typescript. Files array: dist, hooks/dist, README.md, LICENSE. Engine: Node.js ≥18.0.0.\n\n**[tsconfig.json](./tsconfig.json)** — TypeScript compiler configuration: target ES2022, module/moduleResolution NodeNext (native ES modules), outDir dist, rootDir src, strict type-checking enabled, declaration/declarationMap/sourceMap generation, esModuleInterop/resolveJsonModule enabled, isolatedModules enforced, include src/**, exclude node_modules/dist. Invoked by `npm run build` (tsc) and `prepublishOnly` hook.\n\n## Subdirectories\n\n**[.github/workflows/](./github/workflows/)** — CI/CD GitHub Actions workflow for npm package publishing with Sigstore-signed provenance attestation on `release[published]` events. Executes `ubuntu-latest` job with `id-token: write` permission enabling cryptographic attestation linking published artifact to source commit SHA. Runs `actions/checkout@v4`, `actions/setup-node@v4` with registry-url `https://registry.npmjs.org`, executes `npm ci`, `npm run build` (tsc + build:hooks), publishes via `npm publish --provenance --access public` using `NPM_TOKEN` secret.\n\n**[docs/](./docs/)** — Original vision document `INPUT.md` defining RLM algorithm (post-order traversal, leaf-to-root aggregation), command contracts (`/are-generate`, `/are-update`), session lifecycle hooks (SessionStart version check, SessionEnd auto-update), multi-tier documentation system (AGENTS.md, ARCHITECTURE.md, STRUCTURE.md, STACK.md, INTEGRATIONS.md, INFRASTRUCTURE.md, CONVENTIONS.md, TESTING.md, PATTERNS.md, CONCERNS.md), and references to inspirational projects: SpecKit (GitHub spec toolkit), BMAD (methodology framework), GSD (workflow system).\n\n**[hooks/](./hooks/)** — IDE session lifecycle hooks implemented as detached background processes for Claude Code/Gemini CLI/OpenCode: `are-check-update.js` (SessionStart npm registry query via `npm view agents-reverse-engineer version`, cache to `~/.claude/cache/are-update-check.json` with 10s timeout), `are-session-end.js` (SessionEnd git change detection via `git status --porcelain`, spawns `npx agents-reverse-engineer@latest update --quiet` when uncommitted changes exist), `opencode-are-check-update.js`/`opencode-are-session-end.js` (plugin wrappers exporting async factories returning OpenCode plugin objects with `event['session.created']`/`event['session.deleted']` handlers). Spawned via `spawn(process.execPath, ['-e', scriptString], { stdio: 'ignore', detached: true, windowsHide: true }).unref()` to prevent blocking IDE exit. Honors disable flags: `ARE_DISABLE_HOOK=1` environment variable or `hook_enabled: false` in `.agents-reverse-engineer.yaml`.\n\n**[scripts/](./scripts/)** — Build automation script `build-hooks.js` copying hook source files from `hooks/` → `hooks/dist/` during `prepublishOnly` lifecycle via `readdirSync()` filtering `.js` files + `copyFileSync()` per file. Ensures session lifecycle hooks exist in npm tarball for `src/installer/operations.ts` reference during global/local hook installation.\n\n**[src/](./src/)** — Core implementation housing seven CLI commands (init, discover, generate, update, clean, specify, help), three-phase pipeline orchestration (concurrent file analysis with iterator-based worker pools, post-order directory synthesis with dependency graphs, sequential root integration with preamble stripping), AI backend abstraction layer with Claude/Gemini/OpenCode adapters + subprocess resource management (512MB heap limits, 4-thread libuv pools, process group killing), gitignore-aware file discovery with composable filter chains (gitignore → vendor → binary → custom), SHA-256 incremental change detection with orphan cleanup, quality validation (code-vs-doc/code-vs-code inconsistencies via regex export extraction, phantom path detection via three regex patterns + `existsSync()` validation), worker pool concurrency control with shared iterator preventing over-allocation, NDJSON trace emission (phase/worker/task/subprocess/retry events with auto-populated seq/ts/pid/elapsedMs fields), platform-specific IDE integration (Claude Code skills/hooks, OpenCode commands/plugins, Gemini commands/hooks with TOML format).\n\n## Architecture\n\n### Three-Phase Pipeline Execution\n\n**Phase 1 (Concurrent File Analysis):** Iterator-based worker pool (`src/orchestration/pool.ts`) shares `tasks.entries()` iterator across N workers (default 2 WSL, 5 elsewhere) to prevent over-allocation. Each worker invokes `AIService.call()` → `runSubprocess()` → `execFile()` spawning AI CLI subprocesses with resource limits (`NODE_OPTIONS='--max-old-space-size=512'` limits heap to 512MB, `UV_THREADPOOL_SIZE='4'` constrains libuv thread pool, `CLAUDE_CODE_DISABLE_BACKGROUND_TASKS='1'` prevents background tasks, `--disallowedTools Task` blocks subagents). Process group killing (`kill(-pid)`) terminates subprocess trees on timeout (SIGTERM at `timeoutMs`, SIGKILL escalation after 5s grace period). Exponential backoff retry on rate limits (stderr patterns: \"rate limit\", \"429\", \"too many requests\", \"overloaded\"). Writes `.sum` files with YAML frontmatter containing `generated_at` timestamp, `content_hash` (SHA-256), followed by markdown sections (Purpose, Public Interface, Dependencies, Implementation Notes). Detects `## Annex References` marker for `writeAnnexFile()` reproduction-critical source content.\n\n**Phase 2 (Post-Order Directory Aggregation):** Sorts directories by depth descending via `path.relative().split(path.sep).length` (deepest first). Waits for all child `.sum` files to exist via `isDirectoryComplete()` predicate before processing directory. Prompts include aggregated child `.sum` content via `readSumFile()`, subdirectory `AGENTS.md` files via recursive traversal, import maps via `extractDirectoryImports()` with verified path constraints, manifest detection (9 types: package.json, Cargo.toml, go.mod, pyproject.toml, pom.xml, build.gradle, Gemfile, composer.json, CMakeLists.txt/Makefile). User-authored `AGENTS.md` files renamed to `AGENTS.local.md` and prepended above generated content with `---` separator. Writes `AGENTS.md` with `<!-- Generated by agents-reverse-engineer -->` marker.\n\n**Phase 3 (Root Document Synthesis):** Sequential execution (concurrency=1) generates `CLAUDE.md`, `GEMINI.md`, `OPENCODE.md`. Prompts consume all `AGENTS.md` files via `collectAgentsDocs()` recursive tree traversal (skips 13 directories: node_modules, .git, .agents-reverse-engineer, vendor, dist, build, __pycache__, .next, venv, .venv, target, .cargo, .gradle), parse root `package.json` for project metadata, enforce synthesis-only constraints (no invention of features/hooks/patterns not in source documents). Strips conversational preamble via pattern matching (prefixes: \"now i\", \"perfect\", \"based on\", \"let me\", \"here is\", \"i'll\", \"i will\", \"great\", \"okay\", \"sure\", \"certainly\", \"alright\"; separator: `\\n---\\n` within first 500 chars; bold headers: `/^[\\s\\S]{0,500}?(\\*\\*[A-Z])/` without `##` markers) before writing output.\n\n### Incremental Update Strategy\n\nWorkflow orchestrated by `src/update/orchestrator.ts`:\n1. Read `content_hash` from each `.sum` file's YAML frontmatter via `readSumFile()`\n2. Compute current file content hash via `computeContentHash()` (SHA-256)\n3. Hash mismatch → add to `filesToAnalyze` as `FileChange` with `status: 'modified'` or `'added'`\n4. Hash match → add to `filesToSkip`\n5. Detect orphans: `.sum` files for deleted source files or renamed oldPaths\n6. Call `cleanupOrphans()` to delete stale `.sum` files\n7. Call `cleanupEmptyDirectoryDocs()` to remove `AGENTS.md` from directories with no remaining sources (excludes `GENERATED_FILES` set: CLAUDE.md, AGENTS.md, ARCHITECTURE.md, etc.)\n8. Compute `affectedDirs` via `getAffectedDirectories()` by walking parent directories of changed files via `path.dirname()` until `.` or absolute path\n9. Regenerate `.sum` for `filesToAnalyze` via Phase 1 pool execution\n10. Regenerate `AGENTS.md` for `affectedDirs` sequentially (no Phase 2 post-order traversal required)\n\nGit integration: supports committed changes (`git diff <baseCommit>..HEAD`) and uncommitted (`git status --porcelain` merge via `--uncommitted` flag), rename detection via `git diff -M` (50% similarity threshold), fallback to SHA-256 hashing for non-git workflows.\n\n### Subprocess Resource Management\n\nMitigates Claude CLI thread exhaustion (GitHub #5771: 200 Node.js instances reported):\n- `NODE_OPTIONS='--max-old-space-size=512'` limits heap to 512MB per subprocess\n- `UV_THREADPOOL_SIZE='4'` constrains libuv thread pool to 4 threads\n- `CLAUDE_CODE_DISABLE_BACKGROUND_TASKS='1'` prevents background task spawning\n- `--disallowedTools Task` prevents subprocess from spawning subagents\n- Process group killing: `kill(-pid)` terminates entire subprocess tree\n- Default concurrency reduced from 5 → 2 for resource-constrained environments (WSL detection in `src/config/defaults.ts` via `clamp(cores × 5, MIN=2, min(floor((totalMemGB × 0.5) / 0.512), MAX=20))`)\n\nTimeout enforcement: SIGTERM sent at `timeoutMs`, SIGKILL escalation after 5s grace period, unref'd timeout handle allows Node.js exit without cleanup blocking.\n\n### Telemetry & Tracing\n\n**Run logs:** `.agents-reverse-engineer/logs/run-<timestamp>.json` aggregate per-call token counts (input/output/cacheRead/cacheCreation), costs (computed via per-model pricing from config), durations, errors. Tracks `filesRead[]` metadata with `path`, `sizeBytes`, `linesRead`. Computes summary: `totalInputTokens`, `totalCacheReadTokens`, `errorCount`, `uniqueFilesRead`. Enforces retention via `cleanupOldLogs(keepCount)` after each run (default 50 runs).\n\n**Trace events:** `.agents-reverse-engineer/traces/trace-<timestamp>.ndjson` enabled via `--trace` flag. Events: `phase:start/end`, `worker:start/end`, `task:pickup/done`, `subprocess:spawn/exit`, `retry`. Auto-populated fields: `seq` (monotonic), `ts` (ISO 8601), `pid`, `elapsedMs` (high-resolution delta). Promise-chain serialization ensures NDJSON line order matches emission order despite concurrent workers. Retention: keeps 500 most recent traces via `cleanupOldTraces(keepCount)`.\n\n**Progress log:** `.agents-reverse-engineer/progress.log` human-readable streaming output mirroring console. ETA calculation via moving average of last 10 task durations. Quality metrics: code-vs-doc/code-vs-code inconsistencies, phantom path counts. Real-time monitoring: `tail -f .agents-reverse-engineer/progress.log`.\n\n## File Relationships\n\n**CLI orchestration chain:**\n1. `src/cli/index.ts` parses args → routes to `src/cli/{command}.ts` (init/discover/generate/update/clean/specify)\n2. Commands call `src/config/loader.ts` → `loadConfig()` with Zod validation against `src/config/schema.ts`\n3. Commands call `src/discovery/run.ts` → `discoverFiles()` with four-stage filter chain (gitignore → vendor → binary → custom)\n4. `src/cli/generate.ts` calls `src/generation/orchestrator.ts` → `createOrchestrator().createPlan()`, then `src/orchestration/runner.ts` → `CommandRunner.executeGenerate()`\n5. Runner calls `src/ai/service.ts` → `AIService.call()` → `src/ai/subprocess.ts` → `runSubprocess()` → Node.js `execFile()`\n6. Runner calls `src/generation/writers/sum.ts` → `writeSumFile()` with YAML frontmatter, `src/generation/writers/agents-md.ts` → `writeAgentsMd()` preserving user content\n7. Post-generation calls `src/quality/index.ts` → `checkCodeVsDoc()`, `checkCodeVsCode()`, `checkPhantomPaths()` → `buildInconsistencyReport()`\n\n**Trace emission flow:**\n1. CLI creates `TraceWriter` via `src/orchestration/trace.ts` → `createTraceWriter()`\n2. Threads `tracer` through `CommandRunOptions` → `PoolOptions` → `AIServiceOptions`\n3. Runner emits `phase:start/end`, pool emits `worker:start/end` + `task:pickup/done`, AIService emits `subprocess:spawn/exit` + `retry`\n4. TraceWriter serializes via promise-chain writes, finalized via `tracer.finalize()` after all phases\n\n**Update workflow chain:**\n1. `src/cli/update.ts` calls `src/update/orchestrator.ts` → `preparePlan()` reading `.sum` frontmatter\n2. Orchestrator calls `src/change-detection/detector.ts` → `computeContentHash()` for SHA-256 comparison\n3. Orchestrator calls `src/update/orphan-cleaner.ts` → `cleanupOrphans()`, `cleanupEmptyDirectoryDocs()`\n4. CLI calls `src/orchestration/runner.ts` → `CommandRunner.executeUpdate()` for file regeneration\n5. CLI loops `affectedDirs` calling `src/generation/prompts/builder.ts` → `buildDirectoryPrompt()`, `src/generation/writers/agents-md.ts` → `writeAgentsMd()`\n\n**Installer workflow chain:**\n1. `src/installer/index.ts` → `parseInstallerArgs()` → `runInstall()` or `runUninstall()`\n2. Operations call `src/installer/paths.ts` → `getRuntimePaths()` with environment overrides (`CLAUDE_CONFIG_DIR`, `OPENCODE_CONFIG_DIR`, `GEMINI_CONFIG_DIR`)\n3. Operations call `src/integration/templates.ts` → `getClaudeTemplates()`/`getOpenCodeTemplates()`/`getGeminiTemplates()`\n4. Operations read bundled hooks from `hooks/dist/` via `getBundledHookPath()`\n5. Operations modify `settings.json` via JSON parse/stringify for hook/permission registration\n6. Uninstallation calls `src/installer/uninstall.ts` → `cleanupOrphans()`, `cleanupEmptyDirs()`, `cleanupLegacyGeminiFiles()`\n\n## Behavioral Contracts\n\n**Default vendor directories** (`src/config/defaults.ts`):\n```typescript\n['node_modules', '.git', 'dist', 'build', 'target', '.next', '__pycache__', 'venv', '.venv', '.cargo', '.gradle', '.agents-reverse-engineer', '.agents', '.planning', '.claude', '.opencode', '.gemini']\n```\n\n**Default concurrency formula** (`src/config/defaults.ts`):\n```typescript\nclamp(cores × 5, MIN=2, min(floor((totalMemGB × 0.5) / 0.512), MAX=20))\n```\n\n**Export extraction regex** (`src/quality/inconsistency/code-vs-doc.ts`):\n```regex\n/^[ \\t]*export\\s+(?:default\\s+)?(?:function|class|const|let|var|type|interface|enum)\\s+(\\w+)/gm\n```\n\n**Phantom path detection patterns** (`src/quality/phantom-paths/validator.ts`):\n- Markdown links: `/\\[(?:[^\\]]*)\\]\\((\\.[^)]+)\\)/g`\n- Backtick paths: `` /`((?:src\\/|\\.\\.?\\/)[^`]+\\.[a-z]{1,4})`/g ``\n- Prose-embedded: `/(?:from|in|by|via|see)\\s+`?(src\\/[\\w\\-./]+)`?/gi`\n\n**Rate limit detection patterns** (`src/ai/service.ts`):\n```typescript\n['rate limit', '429', 'too many requests', 'overloaded']\n```\n\n**Preamble stripping patterns** (`src/orchestration/runner.ts`):\n- Prefixes (case-insensitive): `['now i', 'perfect', 'based on', 'let me', 'here is', \"i'll\", 'i will', 'great', 'okay', 'sure', 'certainly', 'alright']`\n- Separator detection: `\\n---\\n` within first 500 chars\n- Bold header detection: `/^[\\s\\S]{0,500}?(\\*\\*[A-Z])/` without `##` markers, strip if <300 chars\n\n**YAML manifest types** (`src/generation/prompts/builder.ts`):\n```typescript\n['package.json', 'Cargo.toml', 'go.mod', 'pyproject.toml', 'pom.xml', 'build.gradle', 'Gemfile', 'composer.json', 'CMakeLists.txt', 'Makefile']\n```\n\n**Trace event types** (`src/orchestration/types.ts`):\n```typescript\n'phase:start' | 'phase:end' | 'worker:start' | 'worker:end' | 'task:pickup' | 'task:done' | 'task:start' | 'subprocess:spawn' | 'subprocess:exit' | 'retry' | 'discovery:start' | 'discovery:end' | 'filter:applied' | 'plan:created' | 'config:loaded'\n```",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 5327,
      "cacheReadTokens": 17587,
      "cacheCreationTokens": 22544,
      "latencyMs": 122793,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": []
    },
    {
      "timestamp": "2026-02-09T17:53:50.159Z",
      "prompt": "Generate CLAUDE.md for the project root.\n\nSynthesize the following AGENTS.md files into a single comprehensive project overview document.\nUse ONLY the information provided below. Do NOT invent features, hooks, patterns, or APIs that are not explicitly mentioned in the AGENTS.md content.\n\n## AGENTS.md Files (29 directories)\n\n### .github/workflows/AGENTS.md\n\n<!-- Generated by agents-reverse-engineer -->\n\n# workflows\n\n**GitHub Actions CI/CD automation for npm package publication with Sigstore provenance attestation.**\n\n## Contents\n\n### Workflows\n\n**[publish.yml](./publish.yml)** — Publishes npm package on release events via `npm publish --provenance --access public` with OIDC-signed cryptographic attestation linking artifact to commit SHA.\n\n## Triggers\n\nWorkflow activates on two event types:\n- `release[published]` when GitHub release is published\n- `workflow_dispatch` for manual execution via UI\n\n## Job Pipeline\n\nSingle `publish` job on `ubuntu-latest` executes five steps:\n1. `actions/checkout@v4` clones repository at triggered commit\n2. `actions/setup-node@v4` with Node.js 20 and npm registry authentication context\n3. `npm ci` installs dependencies from lockfile\n4. `npm run build` invokes `prepublishOnly` script (`tsc` + `build:hooks`)\n5. `npm publish --provenance --access public` with `NPM_TOKEN` secret\n\n## Provenance Attestation\n\n`--provenance` flag generates Sigstore transparency log entry cryptographically binding published package to source commit SHA via OIDC token exchange (`id-token: write` permission required). Artifact consumers verify authenticity via `npm audit signatures`.\n\n## Permissions\n\n- `contents: read` — Repository checkout access\n- `id-token: write` — OIDC token generation for Sigstore signing\n\n## Environment Variables\n\n`NODE_AUTH_TOKEN` set from GitHub secret `secrets.NPM_TOKEN` via `.npmrc` injection by `actions/setup-node`.\n### AGENTS.md\n\n<!-- Generated by agents-reverse-engineer -->\n\n# agents-reverse-engineer\n\n**CLI tool implementing Recursive Language Model (RLM) algorithm for AI-driven codebase documentation via three-phase pipeline: concurrent `.sum` file generation with subprocess pools, post-order `AGENTS.md` directory aggregation, and platform-specific root synthesis (`CLAUDE.md`/`GEMINI.md`/`OPENCODE.md`) with gitignore-aware discovery, SHA-256 incremental updates, and NDJSON telemetry.**\n\n## Contents\n\n### Root Documentation\n\n**[LANGUAGES-MANIFEST.md](./LANGUAGES-MANIFEST.md)** — Enumerates 25 language ecosystem manifest types (package.json, Cargo.toml, go.mod, pyproject.toml, pom.xml, build.gradle, Gemfile, composer.json, CMakeLists.txt/Makefile, plus 16 unsupported: Gemfile, *.csproj, Package.swift, mix.exs, rebar.config, build.sbt, deps.edn, *.cabal, pubspec.yaml, *.rockspec, DESCRIPTION, Project.toml, build.zig.zon, *.nimble, dune-project) with support checkmarks indicating `extractDirectoryImports()` detection coverage. Serves as specification for manifest-aware prompt construction in Phase 2 directory aggregation.\n\n**[LICENSE](./LICENSE)** — MIT license grant for agents-reverse-engineer (copyright 2026 GeoloeG-IsT) granting unrestricted usage/modification/distribution rights with warranty disclaimer and liability waiver.\n\n**[README.md](./README.md)** — User-facing documentation covering `npx agents-reverse-engineer@latest` installation workflows (runtime selection: claude/opencode/gemini/all, scope: global `-g`/local `-l`), command reference (init/discover/generate/update/specify/clean), generated artifact formats (`.sum` with YAML frontmatter, `AGENTS.md` directories, runtime-specific roots), configuration schema (`.agents-reverse-engineer/config.yaml` with exclude patterns/vendorDirs/binaryExtensions/ai backend/concurrency/timeout/pricing), three-phase workflow (file analysis → directory aggregation → root synthesis), incremental update strategy (SHA-256 hash comparison, orphan cleanup, affected directory propagation), IDE integration hooks (session-end auto-update, version checking), requirements (Node.js ≥18.0.0, Claude Code/Gemini CLI/OpenCode support).\n\n**[package.json](./package.json)** — Package metadata: name `agents-reverse-engineer`, version `0.6.6`, license MIT. Binary entry points: `agents-reverse-engineer`/`are` → `dist/cli/index.js`. Build scripts: `build` (tsc), `build:hooks` (copies hooks/ → hooks/dist/), `prepublishOnly` (build + build:hooks), `prepack` (remove .sum artifacts), `dev` (tsx watch). Dependencies: fast-glob, ignore, isbinaryfile, ora, picocolors, simple-git, yaml, zod. DevDependencies: @types/node, tsx, typescript. Files array: dist, hooks/dist, README.md, LICENSE. Engine: Node.js ≥18.0.0.\n\n**[tsconfig.json](./tsconfig.json)** — TypeScript compiler configuration: target ES2022, module/moduleResolution NodeNext (native ES modules), outDir dist, rootDir src, strict type-checking enabled, declaration/declarationMap/sourceMap generation, esModuleInterop/resolveJsonModule enabled, isolatedModules enforced, include src/**, exclude node_modules/dist. Invoked by `npm run build` (tsc) and `prepublishOnly` hook.\n\n## Subdirectories\n\n**[.github/workflows/](./github/workflows/)** — CI/CD GitHub Actions workflow for npm package publishing with Sigstore-signed provenance attestation on `release[published]` events. Executes `ubuntu-latest` job with `id-token: write` permission enabling cryptographic attestation linking published artifact to source commit SHA. Runs `actions/checkout@v4`, `actions/setup-node@v4` with registry-url `https://registry.npmjs.org`, executes `npm ci`, `npm run build` (tsc + build:hooks), publishes via `npm publish --provenance --access public` using `NPM_TOKEN` secret.\n\n**[docs/](./docs/)** — Original vision document `INPUT.md` defining RLM algorithm (post-order traversal, leaf-to-root aggregation), command contracts (`/are-generate`, `/are-update`), session lifecycle hooks (SessionStart version check, SessionEnd auto-update), multi-tier documentation system (AGENTS.md, ARCHITECTURE.md, STRUCTURE.md, STACK.md, INTEGRATIONS.md, INFRASTRUCTURE.md, CONVENTIONS.md, TESTING.md, PATTERNS.md, CONCERNS.md), and references to inspirational projects: SpecKit (GitHub spec toolkit), BMAD (methodology framework), GSD (workflow system).\n\n**[hooks/](./hooks/)** — IDE session lifecycle hooks implemented as detached background processes for Claude Code/Gemini CLI/OpenCode: `are-check-update.js` (SessionStart npm registry query via `npm view agents-reverse-engineer version`, cache to `~/.claude/cache/are-update-check.json` with 10s timeout), `are-session-end.js` (SessionEnd git change detection via `git status --porcelain`, spawns `npx agents-reverse-engineer@latest update --quiet` when uncommitted changes exist), `opencode-are-check-update.js`/`opencode-are-session-end.js` (plugin wrappers exporting async factories returning OpenCode plugin objects with `event['session.created']`/`event['session.deleted']` handlers). Spawned via `spawn(process.execPath, ['-e', scriptString], { stdio: 'ignore', detached: true, windowsHide: true }).unref()` to prevent blocking IDE exit. Honors disable flags: `ARE_DISABLE_HOOK=1` environment variable or `hook_enabled: false` in `.agents-reverse-engineer.yaml`.\n\n**[scripts/](./scripts/)** — Build automation script `build-hooks.js` copying hook source files from `hooks/` → `hooks/dist/` during `prepublishOnly` lifecycle via `readdirSync()` filtering `.js` files + `copyFileSync()` per file. Ensures session lifecycle hooks exist in npm tarball for `src/installer/operations.ts` reference during global/local hook installation.\n\n**[src/](./src/)** — Core implementation housing seven CLI commands (init, discover, generate, update, clean, specify, help), three-phase pipeline orchestration (concurrent file analysis with iterator-based worker pools, post-order directory synthesis with dependency graphs, sequential root integration with preamble stripping), AI backend abstraction layer with Claude/Gemini/OpenCode adapters + subprocess resource management (512MB heap limits, 4-thread libuv pools, process group killing), gitignore-aware file discovery with composable filter chains (gitignore → vendor → binary → custom), SHA-256 incremental change detection with orphan cleanup, quality validation (code-vs-doc/code-vs-code inconsistencies via regex export extraction, phantom path detection via three regex patterns + `existsSync()` validation), worker pool concurrency control with shared iterator preventing over-allocation, NDJSON trace emission (phase/worker/task/subprocess/retry events with auto-populated seq/ts/pid/elapsedMs fields), platform-specific IDE integration (Claude Code skills/hooks, OpenCode commands/plugins, Gemini commands/hooks with TOML format).\n\n## Architecture\n\n### Three-Phase Pipeline Execution\n\n**Phase 1 (Concurrent File Analysis):** Iterator-based worker pool (`src/orchestration/pool.ts`) shares `tasks.entries()` iterator across N workers (default 2 WSL, 5 elsewhere) to prevent over-allocation. Each worker invokes `AIService.call()` → `runSubprocess()` → `execFile()` spawning AI CLI subprocesses with resource limits (`NODE_OPTIONS='--max-old-space-size=512'` limits heap to 512MB, `UV_THREADPOOL_SIZE='4'` constrains libuv thread pool, `CLAUDE_CODE_DISABLE_BACKGROUND_TASKS='1'` prevents background tasks, `--disallowedTools Task` blocks subagents). Process group killing (`kill(-pid)`) terminates subprocess trees on timeout (SIGTERM at `timeoutMs`, SIGKILL escalation after 5s grace period). Exponential backoff retry on rate limits (stderr patterns: \"rate limit\", \"429\", \"too many requests\", \"overloaded\"). Writes `.sum` files with YAML frontmatter containing `generated_at` timestamp, `content_hash` (SHA-256), followed by markdown sections (Purpose, Public Interface, Dependencies, Implementation Notes). Detects `## Annex References` marker for `writeAnnexFile()` reproduction-critical source content.\n\n**Phase 2 (Post-Order Directory Aggregation):** Sorts directories by depth descending via `path.relative().split(path.sep).length` (deepest first). Waits for all child `.sum` files to exist via `isDirectoryComplete()` predicate before processing directory. Prompts include aggregated child `.sum` content via `readSumFile()`, subdirectory `AGENTS.md` files via recursive traversal, import maps via `extractDirectoryImports()` with verified path constraints, manifest detection (9 types: package.json, Cargo.toml, go.mod, pyproject.toml, pom.xml, build.gradle, Gemfile, composer.json, CMakeLists.txt/Makefile). User-authored `AGENTS.md` files renamed to `AGENTS.local.md` and prepended above generated content with `---` separator. Writes `AGENTS.md` with `<!-- Generated by agents-reverse-engineer -->` marker.\n\n**Phase 3 (Root Document Synthesis):** Sequential execution (concurrency=1) generates `CLAUDE.md`, `GEMINI.md`, `OPENCODE.md`. Prompts consume all `AGENTS.md` files via `collectAgentsDocs()` recursive tree traversal (skips 13 directories: node_modules, .git, .agents-reverse-engineer, vendor, dist, build, __pycache__, .next, venv, .venv, target, .cargo, .gradle), parse root `package.json` for project metadata, enforce synthesis-only constraints (no invention of features/hooks/patterns not in source documents). Strips conversational preamble via pattern matching (prefixes: \"now i\", \"perfect\", \"based on\", \"let me\", \"here is\", \"i'll\", \"i will\", \"great\", \"okay\", \"sure\", \"certainly\", \"alright\"; separator: `\\n---\\n` within first 500 chars; bold headers: `/^[\\s\\S]{0,500}?(\\*\\*[A-Z])/` without `##` markers) before writing output.\n\n### Incremental Update Strategy\n\nWorkflow orchestrated by `src/update/orchestrator.ts`:\n1. Read `content_hash` from each `.sum` file's YAML frontmatter via `readSumFile()`\n2. Compute current file content hash via `computeContentHash()` (SHA-256)\n3. Hash mismatch → add to `filesToAnalyze` as `FileChange` with `status: 'modified'` or `'added'`\n4. Hash match → add to `filesToSkip`\n5. Detect orphans: `.sum` files for deleted source files or renamed oldPaths\n6. Call `cleanupOrphans()` to delete stale `.sum` files\n7. Call `cleanupEmptyDirectoryDocs()` to remove `AGENTS.md` from directories with no remaining sources (excludes `GENERATED_FILES` set: CLAUDE.md, AGENTS.md, ARCHITECTURE.md, etc.)\n8. Compute `affectedDirs` via `getAffectedDirectories()` by walking parent directories of changed files via `path.dirname()` until `.` or absolute path\n9. Regenerate `.sum` for `filesToAnalyze` via Phase 1 pool execution\n10. Regenerate `AGENTS.md` for `affectedDirs` sequentially (no Phase 2 post-order traversal required)\n\nGit integration: supports committed changes (`git diff <baseCommit>..HEAD`) and uncommitted (`git status --porcelain` merge via `--uncommitted` flag), rename detection via `git diff -M` (50% similarity threshold), fallback to SHA-256 hashing for non-git workflows.\n\n### Subprocess Resource Management\n\nMitigates Claude CLI thread exhaustion (GitHub #5771: 200 Node.js instances reported):\n- `NODE_OPTIONS='--max-old-space-size=512'` limits heap to 512MB per subprocess\n- `UV_THREADPOOL_SIZE='4'` constrains libuv thread pool to 4 threads\n- `CLAUDE_CODE_DISABLE_BACKGROUND_TASKS='1'` prevents background task spawning\n- `--disallowedTools Task` prevents subprocess from spawning subagents\n- Process group killing: `kill(-pid)` terminates entire subprocess tree\n- Default concurrency reduced from 5 → 2 for resource-constrained environments (WSL detection in `src/config/defaults.ts` via `clamp(cores × 5, MIN=2, min(floor((totalMemGB × 0.5) / 0.512), MAX=20))`)\n\nTimeout enforcement: SIGTERM sent at `timeoutMs`, SIGKILL escalation after 5s grace period, unref'd timeout handle allows Node.js exit without cleanup blocking.\n\n### Telemetry & Tracing\n\n**Run logs:** `.agents-reverse-engineer/logs/run-<timestamp>.json` aggregate per-call token counts (input/output/cacheRead/cacheCreation), costs (computed via per-model pricing from config), durations, errors. Tracks `filesRead[]` metadata with `path`, `sizeBytes`, `linesRead`. Computes summary: `totalInputTokens`, `totalCacheReadTokens`, `errorCount`, `uniqueFilesRead`. Enforces retention via `cleanupOldLogs(keepCount)` after each run (default 50 runs).\n\n**Trace events:** `.agents-reverse-engineer/traces/trace-<timestamp>.ndjson` enabled via `--trace` flag. Events: `phase:start/end`, `worker:start/end`, `task:pickup/done`, `subprocess:spawn/exit`, `retry`. Auto-populated fields: `seq` (monotonic), `ts` (ISO 8601), `pid`, `elapsedMs` (high-resolution delta). Promise-chain serialization ensures NDJSON line order matches emission order despite concurrent workers. Retention: keeps 500 most recent traces via `cleanupOldTraces(keepCount)`.\n\n**Progress log:** `.agents-reverse-engineer/progress.log` human-readable streaming output mirroring console. ETA calculation via moving average of last 10 task durations. Quality metrics: code-vs-doc/code-vs-code inconsistencies, phantom path counts. Real-time monitoring: `tail -f .agents-reverse-engineer/progress.log`.\n\n## File Relationships\n\n**CLI orchestration chain:**\n1. `src/cli/index.ts` parses args → routes to `src/cli/{command}.ts` (init/discover/generate/update/clean/specify)\n2. Commands call `src/config/loader.ts` → `loadConfig()` with Zod validation against `src/config/schema.ts`\n3. Commands call `src/discovery/run.ts` → `discoverFiles()` with four-stage filter chain (gitignore → vendor → binary → custom)\n4. `src/cli/generate.ts` calls `src/generation/orchestrator.ts` → `createOrchestrator().createPlan()`, then `src/orchestration/runner.ts` → `CommandRunner.executeGenerate()`\n5. Runner calls `src/ai/service.ts` → `AIService.call()` → `src/ai/subprocess.ts` → `runSubprocess()` → Node.js `execFile()`\n6. Runner calls `src/generation/writers/sum.ts` → `writeSumFile()` with YAML frontmatter, `src/generation/writers/agents-md.ts` → `writeAgentsMd()` preserving user content\n7. Post-generation calls `src/quality/index.ts` → `checkCodeVsDoc()`, `checkCodeVsCode()`, `checkPhantomPaths()` → `buildInconsistencyReport()`\n\n**Trace emission flow:**\n1. CLI creates `TraceWriter` via `src/orchestration/trace.ts` → `createTraceWriter()`\n2. Threads `tracer` through `CommandRunOptions` → `PoolOptions` → `AIServiceOptions`\n3. Runner emits `phase:start/end`, pool emits `worker:start/end` + `task:pickup/done`, AIService emits `subprocess:spawn/exit` + `retry`\n4. TraceWriter serializes via promise-chain writes, finalized via `tracer.finalize()` after all phases\n\n**Update workflow chain:**\n1. `src/cli/update.ts` calls `src/update/orchestrator.ts` → `preparePlan()` reading `.sum` frontmatter\n2. Orchestrator calls `src/change-detection/detector.ts` → `computeContentHash()` for SHA-256 comparison\n3. Orchestrator calls `src/update/orphan-cleaner.ts` → `cleanupOrphans()`, `cleanupEmptyDirectoryDocs()`\n4. CLI calls `src/orchestration/runner.ts` → `CommandRunner.executeUpdate()` for file regeneration\n5. CLI loops `affectedDirs` calling `src/generation/prompts/builder.ts` → `buildDirectoryPrompt()`, `src/generation/writers/agents-md.ts` → `writeAgentsMd()`\n\n**Installer workflow chain:**\n1. `src/installer/index.ts` → `parseInstallerArgs()` → `runInstall()` or `runUninstall()`\n2. Operations call `src/installer/paths.ts` → `getRuntimePaths()` with environment overrides (`CLAUDE_CONFIG_DIR`, `OPENCODE_CONFIG_DIR`, `GEMINI_CONFIG_DIR`)\n3. Operations call `src/integration/templates.ts` → `getClaudeTemplates()`/`getOpenCodeTemplates()`/`getGeminiTemplates()`\n4. Operations read bundled hooks from `hooks/dist/` via `getBundledHookPath()`\n5. Operations modify `settings.json` via JSON parse/stringify for hook/permission registration\n6. Uninstallation calls `src/installer/uninstall.ts` → `cleanupOrphans()`, `cleanupEmptyDirs()`, `cleanupLegacyGeminiFiles()`\n\n## Behavioral Contracts\n\n**Default vendor directories** (`src/config/defaults.ts`):\n```typescript\n['node_modules', '.git', 'dist', 'build', 'target', '.next', '__pycache__', 'venv', '.venv', '.cargo', '.gradle', '.agents-reverse-engineer', '.agents', '.planning', '.claude', '.opencode', '.gemini']\n```\n\n**Default concurrency formula** (`src/config/defaults.ts`):\n```typescript\nclamp(cores × 5, MIN=2, min(floor((totalMemGB × 0.5) / 0.512), MAX=20))\n```\n\n**Export extraction regex** (`src/quality/inconsistency/code-vs-doc.ts`):\n```regex\n/^[ \\t]*export\\s+(?:default\\s+)?(?:function|class|const|let|var|type|interface|enum)\\s+(\\w+)/gm\n```\n\n**Phantom path detection patterns** (`src/quality/phantom-paths/validator.ts`):\n- Markdown links: `/\\[(?:[^\\]]*)\\]\\((\\.[^)]+)\\)/g`\n- Backtick paths: `` /`((?:src\\/|\\.\\.?\\/)[^`]+\\.[a-z]{1,4})`/g ``\n- Prose-embedded: `/(?:from|in|by|via|see)\\s+`?(src\\/[\\w\\-./]+)`?/gi`\n\n**Rate limit detection patterns** (`src/ai/service.ts`):\n```typescript\n['rate limit', '429', 'too many requests', 'overloaded']\n```\n\n**Preamble stripping patterns** (`src/orchestration/runner.ts`):\n- Prefixes (case-insensitive): `['now i', 'perfect', 'based on', 'let me', 'here is', \"i'll\", 'i will', 'great', 'okay', 'sure', 'certainly', 'alright']`\n- Separator detection: `\\n---\\n` within first 500 chars\n- Bold header detection: `/^[\\s\\S]{0,500}?(\\*\\*[A-Z])/` without `##` markers, strip if <300 chars\n\n**YAML manifest types** (`src/generation/prompts/builder.ts`):\n```typescript\n['package.json', 'Cargo.toml', 'go.mod', 'pyproject.toml', 'pom.xml', 'build.gradle', 'Gemfile', 'composer.json', 'CMakeLists.txt', 'Makefile']\n```\n\n**Trace event types** (`src/orchestration/types.ts`):\n```typescript\n'phase:start' | 'phase:end' | 'worker:start' | 'worker:end' | 'task:pickup' | 'task:done' | 'task:start' | 'subprocess:spawn' | 'subprocess:exit' | 'retry' | 'discovery:start' | 'discovery:end' | 'filter:applied' | 'plan:created' | 'config:loaded'\n```\n### docs/AGENTS.md\n\n<!-- Generated by agents-reverse-engineer -->\n\n# docs/\n\nINPUT.md defines the original vision, feature requirements, and Recursive Language Model (RLM) algorithm specification for agents-reverse-engineer—the foundational design document that establishes the three-phase brownfield documentation strategy (file-level `.sum` generation, directory-level `AGENTS.md` synthesis, root document convergence), CLI command contracts (`/are-generate`, `/are-update`), session lifecycle hook requirements, and integration references to complementary tools (SpecKit, BMAD, Get Shit Done).\n\n## Contents\n\n**[INPUT.md](./INPUT.md)** — Specifies RLM post-order traversal algorithm (leaf-to-root aggregation), command execution requirements via Claude Code or platform-specific alternatives, session-end auto-update hooks, multi-tier documentation system (`AGENTS.md`, `ARCHITECTURE.md`, `STRUCTURE.md`, `STACK.md`, `INTEGRATIONS.md`, `INFRASTRUCTURE.md`, `CONVENTIONS.md`, `TESTING.md`, `PATTERNS.md`, `CONCERNS.md`), and references three inspirational projects for implementation context: SpecKit (GitHub's specification toolkit), BMAD (methodology framework), GSD (workflow system).\n\n## Behavioral Contracts\n\n**RLM Algorithm Execution Order:**\n1. Build project structure tree via directory traversal\n2. Execute analysis starting at first leaf node, proceeding recursively backward (post-order traversal)\n3. File-level: Generate `{filename}.sum` for each source file leaf\n4. Directory-level: When all leaves in directory are summarized, analyze directory and generate `AGENTS.md` plus additional documentation if needed\n5. Root convergence: Continue recursive aggregation until project root is reached\n\n**CLI Command Surface:**\n- `/are-generate` — Full documentation generation from scratch\n- `/are-update` — Incremental update of impacted files\n\n**Referenced External Projects:**\n- SpecKit: https://github.com/github/spec-kit\n- BMAD: https://github.com/bmad-code-org/BMAD-METHOD\n- Get Shit Done: https://github.com/glittercowboy/get-shit-done\n### hooks/AGENTS.md\n\n<!-- Generated by agents-reverse-engineer -->\n\n# hooks\n\n**IDE session lifecycle hooks for automatic ARE version checking and documentation updates, implemented as detached background processes for Claude Code, Gemini CLI, and OpenCode runtimes.**\n\n## Contents\n\n### Session Lifecycle Hooks\n\n**[are-check-update.js](./are-check-update.js)** — SessionStart hook spawning detached background process querying npm registry via `npm view agents-reverse-engineer version`, comparing against local/global `~/.claude/ARE-VERSION`, caching results to `~/.claude/cache/are-update-check.json` with timestamp + version metadata. 10s timeout enforced. Graceful degradation on network/git failures (sets `latest: 'unknown'`).\n\n**[are-session-end.js](./are-session-end.js)** — SessionEnd hook executing `npx agents-reverse-engineer@latest update --quiet` as detached background process when git working tree changes detected via `git status --porcelain`. Honors disable flags: `ARE_DISABLE_HOOK=1` environment variable or `hook_enabled: false` in `.agents-reverse-engineer.yaml` (substring search, no parser). Silent exit on no changes.\n\n### OpenCode Plugin Wrappers\n\n**[opencode-are-check-update.js](./opencode-are-check-update.js)** — `AreCheckUpdate()` async factory returning OpenCode plugin with `event['session.created']` handler. Equivalent logic to `are-check-update.js` but adapted to OpenCode plugin system. Checks `~/.config/opencode/ARE-VERSION` (project-local `.opencode/ARE-VERSION` first), writes cache to `~/.config/opencode/cache/are-update-check.json`.\n\n**[opencode-are-session-end.js](./opencode-are-session-end.js)** — `AreSessionEnd()` async factory returning OpenCode plugin with `event['session.deleted']` handler. Equivalent logic to `are-session-end.js` wrapped in plugin interface contract. Same disable mechanisms + git change detection + detached `npx` spawn pattern.\n\n## Architecture\n\n### Detached Background Process Pattern\n\nAll hooks use identical spawn pattern to prevent blocking IDE session lifecycle:\n\n```javascript\nspawn(process.execPath, ['-e', scriptString], {\n  stdio: 'ignore',      // discard stdout/stderr/stdin\n  detached: true,       // separate process group, survives parent exit\n  windowsHide: true     // suppress console window on Windows\n})\nchild.unref()           // allow Node.js event loop exit without waiting\n```\n\nSerialized script strings injected via `-e` flag contain inline logic (version checks, npm queries, cache writes). Parent process exits immediately after spawn, background child completes asynchronously.\n\n### Version File Resolution Priority\n\nBoth check hooks follow precedence order:\n1. **Project-local**: `<cwd>/.claude/ARE-VERSION` or `<cwd>/.opencode/ARE-VERSION`\n2. **Global install**: `~/.claude/ARE-VERSION` or `~/.config/opencode/ARE-VERSION`\n3. **Default fallback**: `'0.0.0'` if neither exists\n\n### Cache File Format (JSON)\n\nWritten by check hooks to `~/.{claude,opencode}/cache/are-update-check.json`:\n\n```json\n{\n  \"update_available\": true,\n  \"installed\": \"0.5.2\",\n  \"latest\": \"0.6.0\",\n  \"checked\": 1738886400\n}\n```\n\nKeys: `update_available` (boolean via `installed !== latest`), `installed` (string from ARE-VERSION or `'0.0.0'`), `latest` (string from `npm view` or `'unknown'` on timeout), `checked` (Unix epoch seconds).\n\n### Disable Mechanisms\n\nSession-end hooks check two flags before execution:\n\n1. **Environment variable**: `process.env.ARE_DISABLE_HOOK === '1'`\n2. **Config file substring**: `.agents-reverse-engineer.yaml` contains `'hook_enabled: false'` (raw `readFileSync()` + `String.includes()`, no YAML parser)\n\nBoth checks trigger silent exit (code 0) to prevent observable failures when user disables hooks.\n\n## Integration Points\n\n**Installed by**: `src/installer/operations.ts` copies files from `hooks/` → `~/.claude/hooks/` or `~/.config/opencode/plugins/` during `are --runtime <platform> -g` command execution.\n\n**Built by**: `scripts/build-hooks.js` duplicates `hooks/*.js` → `hooks/dist/*.js` for npm tarball inclusion during `npm run build:hooks` (invoked by `prepublishOnly`).\n\n**Consumed by**: IDE runtimes invoke hooks automatically on session lifecycle events:\n- Claude Code: calls `~/.claude/hooks/session-start/are-check-update.js`, `~/.claude/hooks/session-end/are-session-end.js`\n- Gemini CLI: equivalent paths under `~/.gemini/hooks/`\n- OpenCode: loads plugins from `~/.config/opencode/plugins/`, invokes `event['session.created']` / `event['session.deleted']` handlers\n\n## Behavioral Contracts\n\n### npm Registry Query Command\n\n```javascript\nexecSync('npm view agents-reverse-engineer version', { \n  encoding: 'utf8', \n  timeout: 10000, \n  windowsHide: true \n})\n```\n\nReturns latest published version string (e.g., `\"0.6.5\\n\"`). Timeout 10000ms enforced to prevent indefinite hangs. Throws on network failures, caught by hooks to set `latest: 'unknown'`.\n\n### Git Change Detection Command\n\n```javascript\nexecSync('git status --porcelain', { encoding: 'utf-8' })\n```\n\nReturns empty string when working tree clean, non-empty on uncommitted changes. Throws when not git repo or git unavailable, caught by session-end hooks to skip update spawn.\n\n### npx Update Invocation Command\n\n```bash\nnpx agents-reverse-engineer@latest update --quiet\n```\n\n`@latest` specifier forces npm registry fetch, bypassing local cache. `--quiet` flag suppresses terminal output (logs to `.agents-reverse-engineer/progress.log` only). Spawned as detached child to prevent blocking IDE session close.\n\n## Platform Compatibility\n\n**Path resolution**: Uses `os.homedir()` + `path.join()` for cross-platform directory construction (`~/.claude`, `~/.config/opencode`, `~/.gemini`).\n\n**Windows-specific flags**: `windowsHide: true` in `spawn()` and `execSync()` suppresses console window creation.\n\n**Node.js compatibility**: `#!/usr/bin/env node` shebang enables direct execution without explicit interpreter invocation. Requires Node.js ≥18.0.0 (same as ARE runtime requirement).\n\n## Dependencies\n\nAll four hooks use identical Node.js built-in module subset:\n- `child_process`: `spawn`, `execSync`\n- `fs`: `existsSync`, `mkdirSync`, `readFileSync`, `writeFileSync`\n- `os`: `homedir`\n- `path`: `join`\n\nNo external npm dependencies — statically analyzable, zero-install executable scripts.\n### scripts/AGENTS.md\n\n<!-- Generated by agents-reverse-engineer -->\n\n# scripts\n\nBuild automation scripts for npm lifecycle hooks, prepublishOnly hook asset preparation, and distribution artifact generation.\n\n## Contents\n\n### [build-hooks.js](./build-hooks.js)\nCopies hook source files from `hooks/` to `hooks/dist/` during `prepublishOnly` lifecycle. Filters `.js` files via `readdirSync()` + `endsWith('.js')` predicate, excludes `dist` directory itself, creates target via `mkdirSync(HOOKS_DIST, { recursive: true })`, performs per-file `copyFileSync()` from `HOOKS_SRC` to `HOOKS_DIST`. Invoked by `package.json` `prepublishOnly` script chain (`npm run build && npm run build:hooks`). Ensures session lifecycle hooks (are-check-update.js, are-session-end.js, opencode-are-check-update.js, opencode-are-session-end.js) exist in npm tarball for installer/operations.ts reference during global/local hook installation.\n\n## Integration Points\n\n**Package.json lifecycle:**\n- `prepublishOnly` script executes `npm run build:hooks` → invokes build-hooks.js\n- Runs after TypeScript compilation (`npm run build`) before `npm publish`\n- Ensures hooks/dist/ directory populated for tarball inclusion\n\n**Hook installation workflow:**\n- `src/installer/operations.ts` references hooks/dist/ files during `installCommands()` execution\n- Supports both global (`~/.claude/hooks/`) and local (`.claude/hooks/`) installation modes\n- Platform-specific targets: Claude Code, OpenCode, Gemini via runtime detection\n\n**File resolution:**\n- Computes `projectRoot` via `fileURLToPath(import.meta.url)` + double `dirname()` traversal\n- Resolves absolute paths via `join(projectRoot, 'hooks')` and `join(projectRoot, 'hooks', 'dist')`\n- Prevents relative path ambiguity in npm lifecycle context\n### src/AGENTS.md\n\n<!-- Generated by agents-reverse-engineer -->\n\n# src\n\n**Core implementation directory housing seven CLI commands (init, discover, generate, update, clean, specify, help), three-phase documentation pipeline orchestration (concurrent file analysis, post-order directory synthesis, sequential root integration), AI backend abstraction layer with Claude/Gemini/OpenCode adapters, gitignore-aware file discovery with composable filter chains, incremental SHA-256 hash-based change detection, quality validation subsystem detecting code-doc inconsistencies and phantom paths, worker pool concurrency control with NDJSON trace emission, and platform-specific IDE integration supporting Claude Code/OpenCode/Gemini runtimes.**\n\n## Contents\n\n**[version.ts](./version.ts)** — `getVersion()` reads `package.json` version field from parent directory of compiled module via `import.meta.url` → `fileURLToPath()` → `join(__dirname, '..', 'package.json')` path resolution, returns `'unknown'` on parse/read errors. Consumed by CLI `--version` flag and session lifecycle hooks (`hooks/are-check-update.js`) for npm registry comparison.\n\n## Subdirectories\n\n**[ai/](./ai/)** — Backend-agnostic AI service layer abstracting Claude Code, Gemini CLI, OpenCode via adapter registry. Implements exponential backoff retry (max 3 attempts, base 1s delay, 8s ceiling), rate-limit detection via stderr pattern matching (`'rate limit'`, `'429'`, `'overloaded'`), subprocess resource management with 512MB heap limits (`NODE_OPTIONS='--max-old-space-size=512'`), libuv thread pool capping (`UV_THREADPOOL_SIZE='4'`), process group killing (`kill(-pid)`) for timeout enforcement (SIGTERM at timeout, SIGKILL after 5s grace period). Emits NDJSON telemetry logs to `.agents-reverse-engineer/logs/run-<timestamp>.json` tracking token counts (input/output/cacheRead/cacheCreation), latency, retry attempts, filesRead metadata, and aggregated cost metrics. Trace emission via `ITraceWriter` for `subprocess:spawn/exit`, `retry` events.\n\n**[change-detection/](./change-detection/)** — Git-based change detection via `simple-git` diff parsing (`git diff --name-status -M <baseCommit>..HEAD`) with rename detection (50% similarity threshold), uncommitted change merge via `git.status()`, and SHA-256 content hashing (`computeContentHash()`) for non-git workflows. Status code mapping: `'A'` → `'added'`, `'M'` → `'modified'`, `'D'` → `'deleted'`, `'R*'` → `'renamed'` with `oldPath` extraction. Consumed by `src/update/orchestrator.ts` for hash-based incremental updates comparing `.sum` frontmatter `content_hash` against current file content.\n\n**[cli/](./cli/)** — Command entry points implementing `init` (config creation), `discover` (preview generation plan), `generate` (three-phase pipeline execution), `update` (incremental hash-based regeneration), `clean` (artifact deletion), `specify` (project spec synthesis). Orchestrates backend resolution via `createBackendRegistry()` + `resolveBackend()`, AIService lifecycle management (`setDebug()`, `setTracer()`, `finalize()`), trace emission to `.agents-reverse-engineer/traces/trace-<timestamp>.ndjson`, progress logging via `ProgressLog.create()` to `.agents-reverse-engineer/progress.log`. Exit codes: 0 (success), 1 (partial failure or config exists), 2 (total failure or CLI not found).\n\n**[config/](./config/)** — YAML configuration loader with Zod validation via `ConfigSchema` (exclude patterns/vendorDirs/binaryExtensions, discovery options followSymlinks/maxFileSize, output colors, AI backend/model/timeoutMs/maxRetries/concurrency/telemetry). `getDefaultConcurrency()` computes memory-aware worker pool sizing via `clamp(cores × 5, MIN=2, min(floor((totalMemGB × 0.5) / 0.512), MAX=20))` enforcing 512MB subprocess heap constraint. `writeDefaultConfig()` generates annotated YAML with `yamlScalar()` quoting patterns containing YAML metacharacters `[*{}\\[\\]?,:#&!|>'\"%@\\`]`. Emits `config:loaded` trace events.\n\n**[discovery/](./discovery/)** — Gitignore-aware file discovery executing four-stage filter chain (gitignore → vendor → binary → custom) over `fast-glob` traversal results with `absolute: true`, `onlyFiles: true`, `dot: true`, `followSymbolicLinks: false`, `ignore: ['**/.git/**']` hardcoded. `applyFilters()` spawns 30 workers sharing `files.entries()` iterator for bounded-concurrency sequential filter evaluation with early termination. Binary detection: extension fast-path (80+ extensions) → size threshold (1MB default) → `isBinaryFile()` content analysis. Returns `DiscoveryResult` with `included: string[]`, `excluded: ExcludedFile[]` with `reason` (gitignore pattern/binary/vendor/custom).\n\n**[generation/](./generation/)** — Three-phase pipeline orchestration: (1) concurrent file analysis via `GenerationOrchestrator.createFileTasks()` embedding `buildFilePrompt()` with import maps, depth-sorted deepest-first execution; (2) post-order directory aggregation via `buildExecutionPlan()` with dependency graphs (`isDirectoryComplete()` predicates, `dependencies: fileTaskIds`), `buildDirectoryPrompt()` consuming child `.sum` files and subdirectory `AGENTS.md`; (3) sequential root synthesis via `collectAgentsDocs()` recursive traversal, `buildRootPrompt()` with stack detection (9 manifest types), preamble stripping. Exports `writeSumFile()`/`readSumFile()` for YAML frontmatter serialization with `content_hash`, `writeAgentsMd()` preserving user content via `AGENTS.local.md` rename, `writeAnnexFile()` for reproduction-critical source content.\n\n**[imports/](./imports/)** — Static import analysis via `IMPORT_REGEX` (`/^import\\s+(type\\s+)?(?:\\{([^}]*)\\}|(\\*\\s+as\\s+\\w+)|(\\w+))\\s+from\\s+['\"]([^'\"]+)['\"]/gm`) matching ES module imports in first 100 lines. Classifies as `internal` (`./` prefix) or `external` (`../` prefix), filters bare package specifiers and `node:` built-ins. `formatImportMap()` produces text blocks with filename headers and indented specifier-symbol pairs for LLM prompt embedding in Phase 1 file analysis and Phase 2 directory aggregation.\n\n**[installer/](./installer/)** — npx-based installation orchestrator for ARE commands/hooks across Claude Code (`~/.claude/skills/`, `~/.claude/hooks/`), OpenCode (`~/.config/opencode/commands/`, `~/.config/opencode/plugins/`), Gemini (`~/.gemini/commands/`, `~/.gemini/hooks/`) with environment overrides (`CLAUDE_CONFIG_DIR`, `OPENCODE_CONFIG_DIR`/`XDG_CONFIG_HOME`, `GEMINI_CONFIG_DIR`). Interactive prompts via `arrowKeySelect()` TTY mode with ANSI escape sequences (`\\x1b[${n}A` cursor up, `\\x1b[2K` clear line, `\\x1b[1B` cursor down) or `numberedSelect()` fallback. `registerHooks()` modifies `settings.json` with nested structure for Claude (`{ hooks: { SessionStart?: [{ hooks: [{ type: 'command', command }] }] } }`), flat structure for Gemini (`{ hooks: { SessionStart?: [{ name, type, command }] } }`). `registerPermissions()` adds `ARE_PERMISSIONS` bash patterns (`'Bash(npx agents-reverse-engineer@latest init*)'` through `clean*`). Uninstallation via `cleanupOrphans()`, `cleanupEmptyDirs()`, `cleanupLegacyGeminiFiles()`.\n\n**[integration/](./integration/)** — Platform-specific AI assistant integration layer detecting Claude Code/OpenCode/Gemini/Aider environments via marker files (`.claude/`, `.opencode/`, `.gemini/`, `.aider/`, `CLAUDE.md`, `.aider.conf.yml`). `generateIntegrationFiles()` writes command templates with progress-monitoring patterns (background execution via `run_in_background: true`, 15s poll intervals, offset-based `.agents-reverse-engineer/progress.log` tailing, `TaskOutput` checks with `block: false`). `getTemplatesForEnvironment()` generates seven commands (generate/update/init/discover/clean/specify/help) with platform-specific frontmatter (Claude: `name: /are-<command>`, OpenCode: `agent: build`, Gemini: TOML `description`/`prompt`).\n\n**[orchestration/](./orchestration/)** — Worker pool concurrency control via iterator-based `runPool()` sharing `tasks.entries()` iterator across N workers preventing over-allocation. `CommandRunner` orchestrates three-phase pipeline: Phase 1 concurrent file analysis with `sourceContentCache` Map for single read, `computeContentHashFromString()` for `.sum` frontmatter, annex file detection via `## Annex References` marker; post-Phase 1 quality validation (concurrency=10) via `checkCodeVsDoc(source, oldSum)` (stale), `checkCodeVsDoc(source, newSum)` (fresh), `checkCodeVsCode(filesForCodeVsCode)` (duplicates); Phase 2 depth-grouped directory synthesis with `buildDirectoryPrompt()` runtime construction; post-Phase 2 phantom path validation via `checkPhantomPaths()` with three regex patterns; Phase 3 sequential root synthesis with preamble stripping. `ProgressReporter` computes ETA via sliding window (size 10, min 2 completions) as `~Ns remaining` or `~Mm Ss remaining`. `PlanTracker` serializes checkbox updates via promise-chain writes replacing `` `- [ ] \\`${itemPath}\\`` `` → `` `- [x] \\`${itemPath}\\`` ``. `TraceWriter` appends NDJSON with auto-populated `seq` (monotonic), `ts` (ISO 8601), `pid`, `elapsedMs` (high-resolution delta) to `.agents-reverse-engineer/traces/trace-<timestamp>.ndjson`, retains 500 most recent traces.\n\n**[output/](./output/)** — Terminal output formatting layer providing color-aware `Logger` interface with picocolors ANSI styling. `createLogger(options)` constructs logger respecting `options.colors` boolean flag, exports `createSilentLogger()` returning no-op logger for testing. Methods: `info(message)`, `file(path)` (`\"  +\" + path` green), `excluded(path, reason, filter)` (`\"  -\" + path + \" (reason: filter)\"` dimmed), `summary(included, excluded)` (bold/dim counts), `warn(message)` (`\"Warning: \"` yellow prefix), `error(message)` (`\"Error: \"` red prefix).\n\n**[quality/](./quality/)** — Post-generation validation system detecting: (1) code-vs-doc inconsistencies via `extractExports()` regex `/^[ \\t]*export\\s+(?:default\\s+)?(?:function|class|const|let|var|type|interface|enum)\\s+(\\w+)/gm` matching against substring search in `.sum` text; (2) code-vs-code duplicates aggregating exports into `Map<symbol, paths[]>` per directory; (3) phantom paths via three regex patterns (markdown links `/\\[(?:[^\\]]*)\\]\\((\\.[^)]+)\\)/g`, backtick paths `` /`((?:src\\/|\\.\\.?\\/)[^`]+\\.[a-z]{1,4})`/g ``, prose-embedded `/(?:from|in|by|via|see)\\s+`?(src\\/[\\w\\-./]+)`?/gi`) with `existsSync()` validation and `.ts`/`.js` fallback resolution. `buildInconsistencyReport()` aggregates discriminated union `Inconsistency = CodeDocInconsistency | CodeCodeInconsistency | PhantomPathInconsistency` with `severity: 'info' | 'warning' | 'error'`, `formatReportForCli()` renders plain text with severity tags.\n\n**[specify/](./specify/)** — Project specification synthesis from `AGENTS.md` corpus via `buildSpecPrompt()` constructing two-part prompts: system instructions (`SPEC_SYSTEM_PROMPT` enforcing 11-section conceptual organization), user content (concatenated AGENTS.md with section delimiters, optional annex files for verbatim reproduction of behavioral contracts). `writeSpec()` supports single-file (`specs/SPEC.md`) and multi-file (`specs/<slug>.md`) modes with pre-flight existence checks throwing `SpecExistsError(conflicts[])` when `force=false`, content splitting via `/^(?=# )/m` regex, filename sanitization through `slugify()` (lowercase + hyphenation + alphanumeric filtering).\n\n**[types/](./types/)** — Shared interfaces for discovery results: `ExcludedFile` (path, reason), `DiscoveryResult` (files, excluded), `DiscoveryStats` (totalFiles, includedFiles, excludedFiles, exclusionReasons histogram). Consumed by `src/discovery/run.ts` (producer), `src/cli/*.ts` (consumers), `src/output/logger.ts` (terminal output).\n\n**[update/](./update/)** — Incremental update orchestrator implementing hash-based change detection via `preparePlan()` reading `.sum` frontmatter `content_hash`, comparing against `computeContentHash()` current file content, returning `UpdatePlan` with `filesToAnalyze` (hash mismatch), `filesToSkip` (match), `cleanup` (orphans), `affectedDirs` (depth-sorted descending). `cleanupOrphans()` deletes stale `.sum`/`.annex.md` files for deleted/renamed sources, `cleanupEmptyDirectoryDocs()` removes `AGENTS.md` from directories with zero source files (excludes `GENERATED_FILES` set and dotfiles). `getAffectedDirectories()` walks parent chains via `path.dirname()` until `.` or absolute path, ensuring `AGENTS.md` regeneration propagates upward. Emits `phase:start/end`, `plan:created` trace events.\n\n## Architecture\n\n### Three-Phase Pipeline\n\n**Phase 1 (Concurrent File Analysis):** Iterator-based worker pool (`src/orchestration/pool.ts`) shares `tasks.entries()` iterator across N workers (default 2 WSL, 5 elsewhere) to prevent over-allocation. Each worker spawns AI CLI subprocess via `AIService.call()` → `runSubprocess()` → `execFile()` with resource limits (512MB heap, 4-thread libuv pool, background tasks disabled, subagents blocked). Writes `.sum` files with YAML frontmatter containing SHA-256 `content_hash`, strips conversational preamble via patterns matching `'now i'`, `'let me'`, `'here is'` prefixes and bold headers without `##` markers. Detects `## Annex References` marker for `writeAnnexFile()` reproduction-critical source content.\n\n**Phase 2 (Post-Order Directory Aggregation):** Sorts directories by depth descending via `path.relative().split(path.sep).length` (deepest first). Waits for `isDirectoryComplete()` predicate checking all child `.sum` files exist. Builds prompts via `buildDirectoryPrompt()` aggregating child `.sum` content, subdirectory `AGENTS.md` files, import maps via `extractDirectoryImports()`, manifest detection (9 types: package.json, Cargo.toml, go.mod, pyproject.toml, pom.xml, build.gradle, Gemfile, composer.json, CMakeLists.txt/Makefile). Preserves user content via `AGENTS.local.md` rename, prepends above generated sections with `---` separator.\n\n**Phase 3 (Sequential Root Synthesis):** Collects all `AGENTS.md` files via `collectAgentsDocs()` recursive traversal (skips 13 directories: node_modules, .git, .agents-reverse-engineer, vendor, dist, build, __pycache__, .next, venv, .venv, target, .cargo, .gradle). Builds root prompts via `buildRootPrompt()` consuming aggregated docs, enforcing synthesis-only constraints (no feature/hook/pattern invention), strips conversational preamble before writing `CLAUDE.md`, `GEMINI.md`, `OPENCODE.md` platform-specific integration documents.\n\n### Subprocess Resource Management\n\nMitigates Claude CLI thread exhaustion (GitHub #5771: 200 Node.js instances):\n- `NODE_OPTIONS='--max-old-space-size=512'` limits heap to 512MB per subprocess\n- `UV_THREADPOOL_SIZE='4'` constrains libuv thread pool to 4 threads\n- `CLAUDE_CODE_DISABLE_BACKGROUND_TASKS='1'` prevents background task spawning\n- `--disallowedTools Task` prevents subagent spawning\n- Process group killing via `kill(-pid)` terminates subprocess trees\n- Default concurrency 2 for WSL environments (5 elsewhere)\n\n### Incremental Update Strategy\n\n`UpdateOrchestrator.preparePlan()` reads `content_hash` from `.sum` frontmatter, computes current file hash via `computeContentHash()`, populates `filesToAnalyze` (added/modified) and `filesToSkip` (unchanged). `cleanupOrphans()` deletes stale `.sum`/`.annex.md` for deleted/renamed sources. `getAffectedDirectories()` walks parent chains computing depth-sorted set for `AGENTS.md` regeneration propagation. Phase 1 pool regenerates `.sum` for `filesToAnalyze`, Phase 2 regenerates `AGENTS.md` for `affectedDirs` sequentially without full post-order traversal.\n\n## Behavioral Contracts\n\n### Default Vendor Directories (config/defaults.ts)\n\n```typescript\nDEFAULT_VENDOR_DIRS = ['node_modules', '.git', 'dist', 'build', 'target', '.next', '__pycache__', 'venv', '.venv', '.cargo', '.gradle', '.agents-reverse-engineer', '.agents', '.planning', '.claude', '.opencode', '.gemini']\n```\n\n### Default Concurrency Formula (config/defaults.ts)\n\n```typescript\nclamp(cores × 5, MIN=2, min(floor((totalMemGB × 0.5) / 0.512), MAX=20))\n```\n\n### Preamble Detection Patterns (orchestration/runner.ts)\n\nFull patterns preserved in [orchestration/runner.ts.annex.md](./orchestration/runner.ts.annex.md).\n\n**Separator pattern:** Detects `\\n---\\n` within first 500 chars.\n\n**Bold header pattern:** Matches `/^[\\s\\S]{0,500}?(\\*\\*[A-Z])/`, strips preceding content if <300 chars, no `##` markers.\n\n**Preamble prefixes (case-insensitive):** `'now i'`, `'perfect'`, `'based on'`, `'let me'`, `'here is'`, `'i\\'ll'`, `'i will'`, `'great'`, `'okay'`, `'sure'`, `'certainly'`, `'alright'`.\n\n### Export Extraction Regex (quality/inconsistency/code-vs-doc.ts)\n\n```regex\n/^[ \\t]*export\\s+(?:default\\s+)?(?:function|class|const|let|var|type|interface|enum)\\s+(\\w+)/gm\n```\n\n### Phantom Path Patterns (quality/phantom-paths/validator.ts)\n\n- Markdown links: `/\\[(?:[^\\]]*)\\]\\((\\.[^)]+)\\)/g`\n- Backtick paths: `` /`((?:src\\/|\\.\\.?\\/)[^`]+\\.[a-z]{1,4})`/g ``\n- Prose-embedded: `/(?:from|in|by|via|see)\\s+`?(src\\/[\\w\\-./]+)`?/gi`\n\n### Rate Limit Patterns (ai/service.ts)\n\n```typescript\n['rate limit', '429', 'too many requests', 'overloaded']\n```\n\n### Trace Event Types (orchestration/types.ts)\n\n`phase:start/end`, `worker:start/end`, `task:pickup/done`, `task:start`, `subprocess:spawn/exit`, `retry`, `discovery:start/end`, `filter:applied`, `plan:created`, `config:loaded`\n\n### Settings.json Schemas (installer/operations.ts)\n\n**Claude:**\n```typescript\ninterface SettingsJson {\n  hooks?: {\n    SessionStart?: HookEvent[];\n    SessionEnd?: HookEvent[];\n  };\n  permissions?: {\n    allow?: string[];\n    deny?: string[];\n  };\n}\n\ninterface HookEvent {\n  hooks: SessionHook[];\n}\n\ninterface SessionHook {\n  type: 'command';\n  command: string;\n}\n```\n\n**Gemini:**\n```typescript\ninterface GeminiSettingsJson {\n  hooks?: {\n    SessionStart?: GeminiHook[];\n    SessionEnd?: GeminiHook[];\n  };\n}\n\ninterface GeminiHook {\n  name: string;\n  type: 'command';\n  command: string;\n}\n```\n\n## File Relationships\n\n**CLI orchestration chain:**\n1. `cli/index.ts` parses args → routes to `cli/{command}.ts`\n2. Command modules call `config/loader.ts` → `loadConfig()` with Zod validation\n3. Commands call `discovery/run.ts` → `discoverFiles()` with filter chain\n4. `cli/generate.ts` calls `generation/orchestrator.ts` → `createOrchestrator().createPlan()`, `orchestration/runner.ts` → `CommandRunner.executeGenerate()`\n5. Runner calls `ai/service.ts` → `AIService.call()` → `subprocess.ts` → `runSubprocess()` → Node.js `execFile()`\n6. Runner calls `generation/writers/sum.ts` → `writeSumFile()` with YAML frontmatter, `generation/writers/agents-md.ts` → `writeAgentsMd()` preserving user content\n7. Post-generation calls `quality/index.ts` → `checkCodeVsDoc()`, `checkCodeVsCode()`, `checkPhantomPaths()` → `buildInconsistencyReport()`\n\n**Trace emission flow:**\n1. CLI creates `TraceWriter` via `orchestration/trace.ts` → `createTraceWriter()`\n2. Threads `tracer` through `CommandRunOptions` → `PoolOptions` → `AIServiceOptions`\n3. Runner emits `phase:start/end`, pool emits `worker:start/end` + `task:pickup/done`, AIService emits `subprocess:spawn/exit` + `retry`\n4. TraceWriter serializes via promise-chain writes, finalized via `tracer.finalize()` after all phases\n\n**Update workflow chain:**\n1. `cli/update.ts` calls `update/orchestrator.ts` → `preparePlan()` reading `.sum` frontmatter\n2. Orchestrator calls `change-detection/detector.ts` → `computeContentHash()` for comparison\n3. Orchestrator calls `update/orphan-cleaner.ts` → `cleanupOrphans()`, `cleanupEmptyDirectoryDocs()`\n4. CLI calls `orchestration/runner.ts` → `CommandRunner.executeUpdate()` for file regeneration\n5. CLI loops `affectedDirs` calling `generation/prompts/builder.ts` → `buildDirectoryPrompt()`, `generation/writers/agents-md.ts` → `writeAgentsMd()`\n\n**Installer workflow chain:**\n1. `installer/index.ts` → `parseInstallerArgs()` → `runInstall()` or `runUninstall()`\n2. Operations call `installer/paths.ts` → `getRuntimePaths()` with environment overrides\n3. Operations call `integration/templates.ts` → `getClaudeTemplates()`/`getOpenCodeTemplates()`/`getGeminiTemplates()`\n4. Operations read bundled hooks from `hooks/dist/` via `getBundledHookPath()`\n5. Operations modify `settings.json` via JSON parse/stringify for hook/permission registration\n6. Uninstallation calls `installer/uninstall.ts` → `cleanupOrphans()`, `cleanupEmptyDirs()`, `cleanupLegacyGeminiFiles()`\n### src/ai/AGENTS.md\n\n<!-- Generated by agents-reverse-engineer -->\n\n# src/ai\n\nAI service orchestration layer abstracting Claude Code, Gemini, and OpenCode CLIs through backend adapters, exponential backoff retry, subprocess resource management, telemetry logging, and timeout enforcement for concurrent file analysis pools.\n\n## Contents\n\n### Core Orchestration\n\n**[service.ts](./service.ts)** — AIService class coordinating backend selection, subprocess execution via `runSubprocess()`, retry logic via `withRetry()`, telemetry accumulation via `TelemetryLogger`, trace emission via `ITraceWriter`, rate-limit detection matching stderr patterns (`'rate limit'`, `'429'`, `'too many requests'`, `'overloaded'`), optional debug logging with heap/RSS metrics via `formatBytes()`, and fire-and-forget subprocess log serialization via `enqueueSubprocessLog()` promise chain.\n\n**[registry.ts](./registry.ts)** — BackendRegistry managing adapter map with `register()`, `get()`, `getAll()` methods. Exports `createBackendRegistry()` instantiating registry with ClaudeBackend, GeminiBackend, OpenCodeBackend in priority order. Exports `resolveBackend(registry, requested)` implementing auto-detection via `detectBackend()` iterating `backend.isAvailable()` or throwing `AIServiceError` with code `'CLI_NOT_FOUND'` and aggregated install instructions via `getInstallInstructions()`.\n\n**[subprocess.ts](./subprocess.ts)** — `runSubprocess(command, args, options)` spawning `execFile()` child processes with stdin piping, timeout enforcement via SIGTERM at `options.timeoutMs`, SIGKILL escalation after 5s grace period, process group killing via `kill(-pid)`, active subprocess tracking in module Map enabling `getActiveSubprocessCount()` and `getActiveSubprocesses()` inspection, and `onSpawn(pid)` callback at spawn time.\n\n**[retry.ts](./retry.ts)** — `withRetry(fn, options)` exponential backoff wrapper executing async function with delay formula `min(baseDelayMs * multiplier^attempt, maxDelayMs) + jitter` (jitter uniform random 0-500ms). Exports `DEFAULT_RETRY_OPTIONS` constant with `maxRetries: 3`, `baseDelayMs: 1000`, `maxDelayMs: 8000`, `multiplier: 2`. Retry flow controlled by `options.isRetryable(error)` predicate and optional `options.onRetry(attempt, error)` callback.\n\n**[types.ts](./types.ts)** — Type definitions: `AIBackend` interface with `isAvailable()`, `buildArgs()`, `parseResponse()`, `getInstallInstructions()` contract. `AIResponse` normalized response shape with `text`, `model`, `inputTokens`, `outputTokens`, `cacheReadTokens`, `cacheCreationTokens`, `durationMs`, `exitCode`, `raw`. `AICallOptions` with `prompt`, `systemPrompt`, `model`, `timeoutMs`, `maxTurns`, `taskLabel`. `SubprocessResult` with `stdout`, `stderr`, `exitCode`, `signal`, `durationMs`, `timedOut`, `childPid`. `TelemetryEntry` per-call metadata, `RunLog` aggregated run structure, `FileRead` file metadata, `RetryOptions` retry config, `AIServiceError` typed error with `code` discriminator (`'CLI_NOT_FOUND'`, `'TIMEOUT'`, `'PARSE_ERROR'`, `'SUBPROCESS_ERROR'`, `'RATE_LIMIT'`).\n\n**[index.ts](./index.ts)** — Barrel export aggregating public API: AIService, BackendRegistry, resolveBackend, detectBackend, createBackendRegistry, withRetry, runSubprocess, isCommandOnPath, all types from `./types.js`, AIServiceOptions from `./service.js`.\n\n## Subdirectories\n\n**[backends/](./backends/)** — AIBackend adapter implementations: ClaudeBackend with Zod validation via `ClaudeResponseSchema`, GeminiBackend stub throwing `SUBPROCESS_ERROR` until JSON format stabilizes, OpenCodeBackend stub throwing `SUBPROCESS_ERROR` until JSONL parsing implemented. Shared `isCommandOnPath()` utility scanning `process.env.PATH` with Windows `PATHEXT` support.\n\n**[telemetry/](./telemetry/)** — Telemetry subsystem: TelemetryLogger accumulating TelemetryEntry instances, `writeRunLog()` persisting JSON to `.agents-reverse-engineer/logs/run-<timestamp>.json`, `cleanupOldLogs()` enforcing retention via lexicographic filename sorting.\n\n## Architecture\n\n### Backend Adapter Pattern\n\nAIBackend interface decouples CLI invocation from backend-specific argument construction and JSON parsing. Registry selects backend at runtime via `config.ai.backend` field (`'claude'` | `'gemini'` | `'opencode'` | `'auto'`). Auto-detection calls `isAvailable()` on each backend in registration order (Claude → Gemini → OpenCode) until first CLI found on PATH.\n\n### Subprocess Resource Management\n\nMitigates Claude CLI thread exhaustion (GitHub #5771: 200 Node.js instances):\n- `NODE_OPTIONS='--max-old-space-size=512'` limits heap to 512MB per subprocess\n- `UV_THREADPOOL_SIZE='4'` constrains libuv thread pool to 4 threads\n- `CLAUDE_CODE_DISABLE_BACKGROUND_TASKS='1'` prevents background task spawning\n- `--disallowedTools Task` prevents subagent spawning\n- Process group killing via `kill(-pid)` terminates subprocess trees\n- Default concurrency 2 for WSL environments (5 elsewhere)\n\n### Retry Strategy\n\n`AIService.call()` wraps `runSubprocess()` in `withRetry()` configured to retry only `RATE_LIMIT` errors (timeouts excluded). Rate-limit detection via `isRateLimitStderr()` substring matching. Exponential backoff adds uniform jitter (0-500ms) preventing thundering herd when multiple workers hit rate limits simultaneously.\n\n### Telemetry Accumulation\n\nTelemetryLogger maintains in-memory `entries: TelemetryEntry[]` array throughout CLI run. After each `AIService.call()`, logger records timestamp, prompt, response, model, token counts (input/output/cacheRead/cacheCreation), latency, exitCode, retryCount, and filesRead metadata. On run completion, `logger.toRunLog()` serializes entries plus computed summary (totalCalls, totalInputTokens, totalOutputTokens, totalCacheReadTokens, totalCacheCreationTokens, totalDurationMs, errorCount, uniqueFilesRead) to `RunLog` JSON structure. `writeRunLog()` persists with ISO-8601-derived filename, `cleanupOldLogs()` enforces retention by deleting oldest logs exceeding `config.ai.telemetry.keepRuns` threshold.\n\n### Timeout Enforcement\n\n`runSubprocess()` sends SIGTERM at `timeoutMs`, schedules unref'd SIGKILL timer at `timeoutMs + 5000ms` for hung processes ignoring SIGTERM. Sets `SubprocessResult.timedOut = true` when `execFile` error has `killed: true` property. AIService throws `AIServiceError('TIMEOUT')` on timeout detection (non-retryable per resource constraint mitigation).\n\n### Trace Emission\n\nAIService invokes `tracer.emit()` for `subprocess:spawn` at `onSpawn` callback time (includes `childPid`, `taskLabel`, `command`, `args`), `subprocess:exit` after completion (includes `exitCode`, `signal`, `durationMs`, `timedOut`), and `retry` events before delay (includes `attempt`, `taskLabel`, `errorCode`). ITraceWriter from `src/orchestration/trace.ts` provides promise-chain serialization ensuring NDJSON line order matches emission order despite concurrent workers.\n\n## Behavioral Contracts\n\n### Rate Limit Patterns\n\n```typescript\nconst RATE_LIMIT_PATTERNS = ['rate limit', '429', 'too many requests', 'overloaded'];\n```\n\n### Default Retry Configuration\n\n```typescript\nDEFAULT_RETRY_OPTIONS = {\n  maxRetries: 3,\n  baseDelayMs: 1_000,\n  maxDelayMs: 8_000,\n  multiplier: 2\n}\n```\n\n### SIGKILL Grace Period\n\n```typescript\nconst SIGKILL_GRACE_MS = 5_000;\n```\n\n### Subprocess maxBuffer\n\n```typescript\nmaxBuffer: 10 * 1024 * 1024  // 10MB stdout/stderr capture limit\n```\n\n## Integration Points\n\n**Consumed by:**\n- `src/orchestration/runner.ts` — Creates AIService instance, calls `setTracer()` and `setDebug()` based on CLI flags, invokes `call()` for each task, calls `finalize()` at run end\n- `src/generation/executor.ts` — Handles AIServiceError codes for failure modes, extracts error messages for progress reporting\n\n**Imports from:**\n- `src/orchestration/trace.ts` — ITraceWriter interface for trace event emission\n- `src/config/schema.ts` — AIServiceOptions validation schema\n\n**Exports to:**\n- `src/cli/*.ts` — All CLI commands import AIService, createBackendRegistry, resolveBackend from `./ai/index.js`\n### src/ai/backends/AGENTS.md\n\n<!-- Generated by agents-reverse-engineer -->\n\n# src/ai/backends\n\n**AI backend adapter layer implementing AIBackend interface for Claude Code, Gemini, and OpenCode CLIs with PATH detection, argument construction, JSON response parsing, and installation instructions.**\n\n## Contents\n\n### Backend Adapters\n\n**[claude.ts](./claude.ts)** — ClaudeBackend with isCommandOnPath() PATH detection, buildArgs() permission bypass flags (`--permission-mode bypassPermissions`, `--no-session-persistence`), parseResponse() with ClaudeResponseSchema Zod validation, defensive JSON extraction via stdout.indexOf('{'), and modelUsage-based model name extraction.\n\n**[gemini.ts](./gemini.ts)** — GeminiBackend stub implementing AIBackend interface with isCommandOnPath() delegation, buildArgs() JSON output flags, parseResponse() throwing 'SUBPROCESS_ERROR' AIServiceError until stable JSON format available.\n\n**[opencode.ts](./opencode.ts)** — OpenCodeBackend stub implementing AIBackend interface with isCommandOnPath() delegation, buildArgs() returning `['run', '--format', 'json']`, parseResponse() throwing 'SUBPROCESS_ERROR' AIServiceError until JSONL parsing implemented.\n\n## Architecture\n\n### AIBackend Interface Contract\n\nAll backends implement four methods:\n- `isAvailable(): Promise<boolean>` — CLI executable detection via PATH scanning\n- `buildArgs(options: AICallOptions): string[]` — Argument array construction for subprocess spawn\n- `parseResponse(stdout: string, durationMs: number, exitCode: number): AIResponse` — JSON parsing with usage extraction\n- `getInstallInstructions(): string` — NPM/curl install commands for missing CLI\n\n### PATH Detection Algorithm\n\n`isCommandOnPath(command: string)` shared utility (exported from claude.ts):\n1. Parse `process.env.PATH` with `path.delimiter` (`;` Windows, `:` Unix)\n2. Parse `process.env.PATHEXT` on Windows for executable extensions (`.exe`, `.cmd`, `.bat`)\n3. Nested loop: directory × extension, construct candidate via `path.join(dir, command + ext)`\n4. Check `(await fs.stat(candidate)).isFile()` — return `true` on first match\n5. Uses `fs.stat()` not `fs.access(X_OK)` for Windows compatibility (no execute permission bits)\n\n### Response Parsing Strategy\n\n**ClaudeBackend (functional):**\n- Defensive JSON extraction: `stdout.substring(stdout.indexOf('{'))` skips upgrade notices\n- Zod validation against ClaudeResponseSchema from Claude CLI v2.1.31 output format\n- Extracts model name from first `modelUsage` object key (defaults to `'unknown'`)\n- Returns AIResponse with normalized `{ result, usage, modelName, durationMs }` structure\n- Throws AIServiceError with `'PARSE_ERROR'` code on validation failure\n\n**GeminiBackend/OpenCodeBackend (stubs):**\n- Unconditionally throw AIServiceError with `'SUBPROCESS_ERROR'` code in parseResponse()\n- Block backend usage until JSON format stabilizes (Gemini) or JSONL parsing implemented (OpenCode)\n- Demonstrate extension pattern for future implementations\n\n## Behavioral Contracts\n\n### ClaudeBackend CLI Arguments\n\nFixed arguments returned by buildArgs():\n```\n['-p', '--output-format', 'json', '--no-session-persistence', '--permission-mode', 'bypassPermissions']\n```\n\nConditional arguments from AICallOptions:\n- `'--model', options.model` when model specified\n- `'--system-prompt', options.systemPrompt` when systemPrompt provided\n- `'--max-turns', String(options.maxTurns)` when maxTurns defined\n\nPrompt delivered via stdin by `runSubprocess()` wrapper in `src/ai/subprocess.ts`.\n\n### ClaudeResponseSchema Structure\n\nZod schema validated against Claude CLI v2.1.31:\n```typescript\n{\n  type: 'result',\n  subtype: 'success' | 'error',\n  is_error: boolean,\n  duration_ms: number,\n  duration_api_ms: number,\n  num_turns: number,\n  result: string,\n  session_id: string,\n  total_cost_usd: number,\n  usage: {\n    input_tokens: number,\n    cache_creation_input_tokens: number,\n    cache_read_input_tokens: number,\n    output_tokens: number\n  },\n  modelUsage: Record<string, {\n    inputTokens: number,\n    outputTokens: number,\n    cacheReadInputTokens: number,\n    cacheCreationInputTokens: number,\n    costUSD: number\n  }>\n}\n```\n\n### Installation Instructions\n\n**ClaudeBackend:** `npm install -g @anthropic-ai/claude-code`  \n**GeminiBackend:** `npm install -g @anthropic-ai/gemini-cli` + `https://github.com/google-gemini/gemini-cli`  \n**OpenCodeBackend:** `curl -fsSL https://opencode.ai/install | bash` + `https://opencode.ai`\n\n## Integration Points\n\nBackends registered in `src/ai/registry.ts` AIBackendRegistry map with keys `'claude'`, `'gemini'`, `'opencode'`. Registry consumed by AIService in `src/ai/service.ts` for backend selection via `ai.backend` config field. Auto-detection iterates backends calling `isAvailable()` until first match.\n\n`buildArgs()` output consumed by `runSubprocess()` in `src/ai/subprocess.ts` for child_process.execFile() argument array. `parseResponse()` receives stdout/durationMs/exitCode from subprocess wrapper for normalization into AIResponse structure.\n### src/ai/telemetry/AGENTS.md\n\n<!-- Generated by agents-reverse-engineer -->\n\n# src/ai/telemetry\n\nTelemetry subsystem for accumulating AI service call metrics, writing JSON run logs with retention management, and tracking token costs, durations, and file metadata across concurrent worker pool operations.\n\n## Contents\n\n### [cleanup.ts](./cleanup.ts)\nExports `cleanupOldLogs(projectRoot, keepCount)` deleting expired telemetry logs from `.agents-reverse-engineer/logs/` by lexicographic timestamp sorting on `run-*.json` filenames. Returns count of deleted files.\n\n### [logger.ts](./logger.ts)\nExports `TelemetryLogger` class accumulating `TelemetryEntry` instances in memory during CLI runs. Methods: `addEntry()` appends call metrics, `setFilesReadOnLastEntry()` attaches file metadata to most recent entry, `getSummary()` computes aggregate statistics (token totals, error counts, unique file paths), `toRunLog()` serializes complete `RunLog` object.\n\n### [run-log.ts](./run-log.ts)\nExports `writeRunLog(projectRoot, runLog)` persisting `RunLog` as pretty-printed JSON with ISO-8601-derived filename pattern `run-2026-02-07T12-00-00-000Z.json`. Creates `.agents-reverse-engineer/logs/` directory via `mkdir(recursive: true)`.\n\n## Data Flow\n\n1. `TelemetryLogger` instantiated once per CLI invocation with unique `runId` (ISO timestamp)\n2. AIService calls `logger.addEntry(entry)` after each subprocess completion with token counts, cost, duration, error status\n3. Command runner invokes `logger.setFilesReadOnLastEntry(filesRead)` to attach file metadata to last entry\n4. On run completion, caller invokes `logger.toRunLog()` → serializes in-memory entries + summary\n5. `writeRunLog(projectRoot, runLog)` persists JSON to disk with timestamp-derived filename\n6. `cleanupOldLogs(projectRoot, keepCount)` enforces retention policy by deleting oldest logs exceeding `keepRuns` threshold (default 50)\n\n## Behavioral Contracts\n\n**Filename derivation pattern** (from `run-log.ts`):\n```typescript\nconst timestamp = runLog.startTime.replace(/:/g, '-').replace(/\\./g, '-');\nconst filename = `run-${timestamp}.json`;\n```\n\n**Log directory constant** (shared across all modules):\n```typescript\nconst LOGS_DIR = '.agents-reverse-engineer/logs';\n```\n\n**Cleanup sort order** (from `cleanup.ts`):\n```typescript\nentries.sort();      // Lexicographic ascending\nentries.reverse();   // Newest first\nconst toDelete = entries.slice(keepCount);  // Oldest logs\n```\n\n**Summary deduplication** (from `logger.ts`):\n```typescript\nconst uniquePaths = new Set<string>();\nfor (const entry of this.entries) {\n  for (const file of entry.filesRead || []) {\n    uniquePaths.add(file.path);\n  }\n}\nreturn { uniqueFilesRead: uniquePaths.size };\n```\n\n## Integration Points\n\n- **Created by**: `src/ai/service.ts` constructs `TelemetryLogger` when `config.ai.telemetry.enabled === true`\n- **Consumed by**: `src/orchestration/runner.ts` invokes `writeRunLog()` after Phase 3 completion, then calls `cleanupOldLogs()` with `config.ai.telemetry.keepRuns`\n- **Types imported from**: `../types.js` provides `TelemetryEntry`, `RunLog`, `FileRead` interfaces\n- **Related configuration**: `src/config/schema.ts` defines `ai.telemetry.enabled`, `ai.telemetry.keepRuns`, `ai.telemetry.costThresholdUsd` validation rules\n\n## Design Notes\n\n`TelemetryLogger` uses eager summary computation on every `getSummary()` invocation without caching — acceptable since `toRunLog()` called once per run. Single-level flat `entries` array without phase-aware partitioning simplifies aggregation logic at cost of phase-specific analytics. Cleanup operates on filename lexicographic order assuming ISO 8601 timestamp sort equivalence (year-month-day-hour-minute-second-millisecond).\n### src/change-detection/AGENTS.md\n\n<!-- Generated by agents-reverse-engineer -->\n\n# change-detection\n\nGit-based change detection via simple-git diff parsing and SHA-256 content hashing for incremental documentation updates.\n\n## Contents\n\n**[detector.ts](./detector.ts)** — `isGitRepo()`, `getCurrentCommit()`, `getChangedFiles()` with git diff status parsing (A/M/D/R), `computeContentHash()` SHA-256 hashing, uncommitted change merge via `git.status()`.\n\n**[types.ts](./types.ts)** — `ChangeType`, `FileChange`, `ChangeDetectionResult`, `ChangeDetectionOptions` interfaces defining change detection contracts.\n\n**[index.ts](./index.ts)** — Barrel export aggregating detector functions and types.\n\n## Change Detection Algorithm\n\n`getChangedFiles()` executes `git diff --name-status -M <baseCommit>..HEAD` parsing output format `STATUS\\tFILE` or `STATUS\\tOLD\\tNEW`. Status code mapping: `'A'` → `'added'`, `'M'` → `'modified'`, `'D'` → `'deleted'`, `'R*'` → `'renamed'` with `oldPath` extracted from second tab-delimited field. Rename detection uses `-M` flag with 50% similarity threshold.\n\nWhen `ChangeDetectionOptions.includeUncommitted` is true, merges `git.status()` results reading `status.modified`, `status.deleted`, `status.not_added`, `status.staged` arrays. Deduplicates via `changes.some(c => c.path === file)` predicate to prevent duplicate `FileChange` entries for files in both committed diff and working tree.\n\n## Content Hashing\n\n`computeContentHash()` reads file via Node.js `readFile()`, computes SHA-256 via `createHash('sha256').update(content).digest('hex')`. Hash stored in `.sum` YAML frontmatter `content_hash` field. `computeContentHashFromString()` provides synchronous variant for pre-loaded content avoiding redundant disk reads.\n\n## Integration Points\n\nConsumed by `src/update/orchestrator.ts` to compute `filesToAnalyze` by comparing `computeContentHash()` results against `.sum` frontmatter `content_hash` values. Supports non-git workflows via `computeContentHash()` fallback when `isGitRepo()` returns false.\n\n## Behavioral Contracts\n\n**Git diff status codes:**\n- `A` — added file\n- `M` — modified file\n- `D` — deleted file\n- `R*` — renamed file with similarity score (e.g., `R100`)\n\n**Rename detection similarity threshold:** 50% (implicit via `git diff -M`)\n\n**SHA-256 hash format:** Hex-encoded string matching regex `/^[a-f0-9]{64}$/`\n### src/cli/AGENTS.md\n\n<!-- Generated by agents-reverse-engineer -->\n\n# cli\n\nCommand entry points implementing CLI argument parsing, command routing, and orchestration of AI-driven documentation generation workflows (init, discover, generate, update, clean, specify) via backend abstraction, concurrent execution pools, and incremental hash-based updates.\n\n## Commands\n\n### [clean.ts](./clean.ts)\n`cleanCommand(targetPath, {dryRun})` deletes `.sum`, `.annex.md`, `AGENTS.md` (marker-filtered), `CLAUDE.md`, `GENERATION-PLAN.md` via parallel `fast-glob` discovery, restores `AGENTS.local.md` → `AGENTS.md`, logs deletions/skips.\n\n### [discover.ts](./discover.ts)\n`discoverCommand(targetPath, {tracer, debug})` walks directory via `discoverFiles()` filter chain, writes file list to `progress.log`, generates `GENERATION-PLAN.md` via `buildExecutionPlan()` post-order traversal, emits `discovery:start`/`discovery:end` trace events.\n\n### [generate.ts](./generate.ts)\n`generateCommand(targetPath, {dryRun, concurrency, failFast, debug, trace})` orchestrates three-phase pipeline: concurrent `.sum` file analysis via `CommandRunner.executeGenerate()`, directory `AGENTS.md` aggregation, root document synthesis (`CLAUDE.md`/`GEMINI.md`/`OPENCODE.md`), exits with codes 0 (success), 1 (partial failure), 2 (total failure).\n\n### [index.ts](./index.ts)\nCLI entry point with shebang `#!/usr/bin/env node`, implements `parseArgs()` flag parser supporting `--dry-run`, `--concurrency <n>`, `--fail-fast`, `--debug`, `--trace`, `--uncommitted`, `--force`, `--multi-file`, routes to command modules, triggers `runInstaller()` on install/uninstall/installer flags.\n\n### [init.ts](./init.ts)\n`initCommand(root, {force})` creates `.agents-reverse-engineer/config.yaml` via `writeDefaultConfig()`, checks `configExists()` to prevent overwrites unless `force` enabled, exits with code 1 on `EACCES`/`EPERM` permission errors.\n\n### [specify.ts](./specify.ts)\n`specifyCommand(targetPath, {output, force, dryRun, multiFile, debug, trace})` synthesizes project specs from `AGENTS.md` corpus via `collectAgentsDocs()`, auto-generates missing docs via `generateCommand()`, invokes `AIService.call()` with 600s minimum timeout, writes via `writeSpec()` to `specs/SPEC.md` or custom path.\n\n### [update.ts](./update.ts)\n`updateCommand(targetPath, {uncommitted, dryRun, concurrency, failFast, debug, trace})` computes delta via SHA-256 hash comparison in `preparePlan()`, cleans orphaned artifacts, regenerates `.sum` for changed files via `runner.executeUpdate()`, rebuilds `AGENTS.md` for `affectedDirs` sequentially, logs to `progress.log`.\n\n## Execution Patterns\n\n**Dry-run mode** (`--dry-run`): clean/generate/update/specify commands display plans without filesystem writes or AI calls. generate shows file/directory/root counts with `buildExecutionPlan()`, specify estimates tokens via `Math.ceil(totalChars / 4) / 1000`.\n\n**Progress monitoring**: generate/update/specify create `ProgressLog.create(absolutePath)` writing to `.agents-reverse-engineer/progress.log` with ISO timestamps, file counts, task status. Enables `tail -f` real-time monitoring.\n\n**Trace emission** (`--trace`): generate/update/specify instantiate `createTraceWriter()` before config loading, pass `tracer` to all orchestrator calls, emit NDJSON events (`phase:start`/`end`, `worker:start`/`end`, `task:pickup`/`done`, `subprocess:spawn`/`exit`, `retry`), call `cleanupOldTraces()` after `tracer.finalize()`.\n\n**Backend resolution**: generate/update/specify call `createBackendRegistry()`, `resolveBackend(registry, config.ai.backend)`, catch `AIServiceError` with `code === 'CLI_NOT_FOUND'`, print `getInstallInstructions(registry)`, exit with code 2.\n\n**AI service lifecycle**: generate/update/specify instantiate `AIService(backend, {timeoutMs, maxRetries, model, telemetry.keepRuns})`, call `setDebug(true)` if `--debug`, call `setSubprocessLogDir()` if `--trace`, invoke via `aiService.call()` or `runner.executeGenerate()`/`runner.executeUpdate()`, finalize via `aiService.finalize(absolutePath)` writing run logs to `.agents-reverse-engineer/logs/`.\n\n**Installer routing**: index.ts triggers `runInstaller()` on three conditions: (1) zero args (interactive mode), (2) installer flags (`--global`, `--local`, `--runtime`, `--force`) without command, (3) explicit `install`/`uninstall` command. Passes `parseInstallerArgs(args)` with `uninstall: true` for uninstall command.\n\n## Exit Code Strategy\n\n**generate/update**:\n- Code 2: total failure (`filesProcessed === 0 && filesFailed > 0`) or CLI not found\n- Code 1: partial failure (`filesFailed > 0` with some success)\n- Code 0: full success (`filesFailed === 0`)\n\n**specify**:\n- Code 2: `CLI_NOT_FOUND` error from `resolveBackend()`\n- Code 1: `SpecExistsError` (output exists without `--force`) or empty docs after auto-generation\n- Code 0: successful write\n\n**init**:\n- Code 1: `EACCES`/`EPERM` permission error or write failure\n- Code 0: config created successfully\n\n**clean**: No explicit exit codes (relies on thrown errors).\n\n## File Relationships\n\n- index.ts imports all command modules, routes via switch statement on `command` string\n- generate.ts imports `discoverFiles()` from `../discovery/run.js`, `createOrchestrator()` from `../generation/orchestrator.js`, `buildExecutionPlan()` from `../generation/executor.js`, `AIService` from `../ai/index.js`, `CommandRunner` from `../orchestration/index.js`\n- update.ts imports `createUpdateOrchestrator()` from `../update/index.js`, `buildDirectoryPrompt()` from `../generation/prompts/index.js`, `writeAgentsMd()` from `../generation/writers/agents-md.js`\n- specify.ts imports `collectAgentsDocs()` from `../generation/collector.js`, `buildSpecPrompt()` from `../specify/index.js`, calls `generateCommand()` from `./generate.js` for auto-generation fallback\n- clean.ts imports `GENERATED_MARKER` from `../generation/writers/agents-md.js` for user-authored AGENTS.md detection\n- discover.ts imports `formatExecutionPlanAsMarkdown()` from `../generation/executor.js` for `GENERATION-PLAN.md` rendering\n- init.ts imports `configExists()`, `writeDefaultConfig()` from `../config/loader.js`\n\n## Behavioral Contracts\n\n**Argument parsing** (index.ts `parseArgs()`):\n- Flags starting with `--` populate `flags: Set<string>` or `values: Map<string, string>` if next arg lacks `--` prefix\n- Short flags expanded: `-h` → `help`, `-g` → `global`, `-l` → `local`, `-V` → `version`\n- First non-flag arg becomes `command`, subsequent populate `positional: string[]`\n\n**Token estimation** (specify.ts dry-run):\n```typescript\nestimatedTokensK = Math.ceil(totalChars / 4) / 1000\n```\n\n**Plan formatting** (update.ts `formatPlan()`):\n- Status markers: `+` (added, green), `R` (renamed, blue), `M` (modified, yellow), `=` (unchanged, dim)\n- First run: `plan.isFirstRun === true` → suggests `are generate`\n- Empty plan: all counts zero → \"No changes detected since last run\"\n\n**Cleanup patterns** (clean.ts):\n- Parallel `fast-glob` with patterns: `**/*.sum`, `**/*.annex.md`, `**/AGENTS.md`, `**/AGENTS.local.md`\n- Ignore patterns: `['**/node_modules/**', '**/.git/**']`\n- Marker detection: `content.includes(GENERATED_MARKER)` where `GENERATED_MARKER = '<!-- Generated by agents-reverse-engineer -->'`\n\n**Telemetry summary** (specify.ts):\n```typescript\nsummaryLine = `Tokens: ${totalInputTokens} in / ${totalOutputTokens} out | Duration: ${(totalDurationMs/1000).toFixed(1)}s | Output: ${outputPath}`\n```\n\n**Subprocess timeout override** (specify.ts):\n```typescript\ntimeoutMs = Math.max(config.ai.timeoutMs, 600_000) // minimum 10 minutes\n```\n### src/config/AGENTS.md\n\n<!-- Generated by agents-reverse-engineer -->\n\n# src/config\n\nConfiguration loading, validation, and default value computation for `.agents-reverse-engineer/config.yaml`, including Zod schema enforcement, platform-adaptive concurrency calculation, and annotated YAML file generation with memory-bounded worker pool sizing.\n\n## Contents\n\n### Schema Definition\n\n**[schema.ts](./schema.ts)** — `ConfigSchema` root validator composed of `ExcludeSchema` (patterns/vendorDirs/binaryExtensions arrays), `OptionsSchema` (followSymlinks/maxFileSize), `OutputSchema` (colors), `AISchema` (backend/model/timeoutMs/maxRetries/concurrency/telemetry). Exports inferred types: `Config`, `ExcludeConfig`, `OptionsConfig`, `OutputConfig`, `AIConfig`. All schemas chain `.default()` enabling partial parse with full default population.\n\n**[defaults.ts](./defaults.ts)** — Exports `DEFAULT_VENDOR_DIRS` (18 directories: node_modules, .git, dist, build, target, .next, __pycache__, venv, .cargo, .gradle, .agents-reverse-engineer, .agents, .planning, .claude, .opencode, .gemini), `DEFAULT_EXCLUDE_PATTERNS` (lock files, dotfiles, logs, AI-generated docs, *.sum files), `DEFAULT_BINARY_EXTENSIONS` (26 extensions: images, archives, executables, media, documents, fonts, bytecode), `DEFAULT_MAX_FILE_SIZE` (1048576 bytes). `getDefaultConcurrency()` computes `clamp(cores × 5, MIN=2, min(memCap, MAX=20))` where `memCap = floor((totalMemGB × 0.5) / 0.512)` enforces memory constraint allocating 50% RAM divided by 512MB subprocess heap limit.\n\n**[loader.ts](./loader.ts)** — `loadConfig(root, options?)` reads `config.yaml` from `.agents-reverse-engineer/`, parses via `yaml.parse()`, validates with `ConfigSchema`, returns defaults on `ENOENT`, throws `ConfigError` wrapping `ZodError` with formatted issue paths. Emits `config:loaded` trace events with configPath/model/concurrency fields. `writeDefaultConfig(root)` creates annotated YAML with five comment-delimited sections (exclusions, discovery options, output formatting, AI service), interpolates defaults via spread operators, quotes patterns containing YAML metacharacters `[*{}\\[\\]?,:#&!|>'\"%@\\`]` via `yamlScalar()`, comments out concurrency line showing override syntax.\n\n## Configuration Structure\n\nThe config file divides into four sections:\n\n- **exclude**: File/directory exclusion rules with glob patterns, vendor directory names, binary extension list\n- **options**: Discovery behavior toggles (followSymlinks) and thresholds (maxFileSize in bytes)\n- **output**: Terminal formatting preferences (ANSI colors)\n- **ai**: AI service configuration (backend selection, model override, subprocess timeout, retry limits, worker pool concurrency, telemetry retention)\n\n## Memory-Aware Concurrency\n\n`getDefaultConcurrency()` implements memory-bounded worker pool sizing preventing RAM exhaustion where `cores × 5` spawns too many 512MB subprocesses. Computes `memCap = floor((totalMemGB × 0.5) / 0.512)` limiting concurrent subprocesses to fit within 50% memory budget, applies `Math.min(computed, memCap, MAX_CONCURRENCY)` respecting memory constraint alongside schema maximum. Fallback uses `Infinity` memCap when `totalMemGB ≤ 1` to avoid division-by-zero on resource-constrained systems.\n\n## Behavioral Contracts\n\n### Schema Validation Constraints\n\n- AISchema concurrency: `z.number().min(1).max(20)` enforces 1-20 worker pool range\n- OptionsSchema maxFileSize: `z.number().positive()` rejects zero/negative thresholds\n- AISchema backend: `z.enum(['claude', 'gemini', 'opencode', 'auto'])` limits backend values\n- AISchema telemetry.keepRuns: `z.number().min(0)` allows zero for unlimited retention\n\n### YAML Metacharacter Pattern\n\n`yamlScalar()` tests input against `/[*{}\\[\\]?,:#&!|>'\"%@\\`]/` pattern, double-quotes and backslash-escapes `\\` → `\\\\`, `\"` → `\\\"` when matched.\n\n### Default Formulas\n\n- Concurrency: `clamp(cores × 5, 2, min(floor((totalMemGB × 0.5) / 0.512), 20))`\n- Memory cap: `floor((os.totalmem() / 1024³ × 0.5) / 0.512)` concurrent processes\n- Timeout default: `300_000` milliseconds (5 minutes)\n\n### Error Message Format\n\nConfigError validation failures: `\"ai.concurrency: Expected number\"` (field path colon-separated from Zod issue message, joined by newlines for multiple issues).\n\n### Trace Event Schema\n\n`config:loaded` payload: `{ type: 'config:loaded', configPath: string, model: string | null, concurrency: number }` emitted twice per `loadConfig()` call (once for file path via `path.relative(root, configPath)`, once for defaults with literal `'(defaults)'`).\n\n## File Relationships\n\n- **schema.ts** → **defaults.ts**: Imports `DEFAULT_VENDOR_DIRS`, `DEFAULT_BINARY_EXTENSIONS`, `DEFAULT_MAX_FILE_SIZE`, `DEFAULT_EXCLUDE_PATTERNS`, `getDefaultConcurrency` for `.default()` chaining\n- **loader.ts** → **schema.ts**: Imports `ConfigSchema` for parse, `Config` type for return annotation\n- **loader.ts** → **defaults.ts**: Imports all defaults for YAML interpolation in `writeDefaultConfig()`\n- **loader.ts** → **../orchestration/trace.ts**: Imports `ITraceWriter` for `config:loaded` event emission\n### src/discovery/AGENTS.md\n\n<!-- Generated by agents-reverse-engineer -->\n\n# src/discovery\n\n**Gitignore-aware file discovery system executing four-stage filter chain (gitignore, vendor, binary, custom) over fast-glob traversal results with bounded-concurrency processing (30 workers), early-termination optimization, and per-filter telemetry tracking.**\n\n## Contents\n\n**[run.ts](./run.ts)** — `discoverFiles()` orchestrates filter chain construction via `createGitignoreFilter()`, `createVendorFilter()`, `createBinaryFilter()`, `createCustomFilter()`, invokes `walkDirectory()` with `followSymlinks` flag, delegates filtering to `applyFilters()` with trace/debug options.\n\n**[types.ts](./types.ts)** — Defines `FileFilter` interface (`name: string`, `shouldExclude(): boolean|Promise<boolean>`), `FilterResult` with `included: string[]` and `excluded: ExcludedFile[]`, `WalkerOptions` for traversal config, `ExcludedFile` audit record.\n\n**[walker.ts](./walker.ts)** — `walkDirectory()` wraps `fast-glob` with `absolute: true`, `onlyFiles: true`, `dot: true` (dotfiles), `followSymbolicLinks: false` (default), `ignore: ['**/.git/**']` hardcoded, `suppressErrors: true` (permission denied).\n\n## Architecture\n\n### Filter Chain Execution Model\n\n**Composition**: `discoverFiles()` creates four filters in fixed order before applying to walked files. No in-walker filtering per module comment in `walker.ts`.\n\n**Orchestration**: `applyFilters()` (in `filters/index.ts`) spawns 30 workers sharing `files.entries()` iterator. Sequential filter evaluation per file with early termination on first `shouldExclude() === true`.\n\n**Result Preservation**: Workers collect `{ index, file, excluded? }` tuples, sort by original index to maintain discovery order, segregate into `included`/`excluded` arrays.\n\n### Filter Order\n\n1. **Gitignore** — Async `.gitignore` parser via `ignore` library with relative path normalization\n2. **Vendor** — Third-party directories (`node_modules`, `.git`, `dist`, `build`, `__pycache__`, `.next`, `venv`, `.venv`, `target`)\n3. **Binary** — Extension fast path (80+ extensions) → size threshold (1MB) → `isBinaryFile()` content analysis\n4. **Custom** — User glob patterns from `config.exclude.patterns` via `ignore` library\n\n### Decoupling Patterns\n\n**DiscoveryConfig Interface**: Structural subset type with `exclude: {vendorDirs, binaryExtensions, patterns}` and `options: {maxFileSize, followSymlinks}` allows `run.ts` to accept full `Config` object without circular dependency on `src/config/schema.ts`.\n\n**Factory Abstraction**: Filter creators (`createGitignoreFilter`, `createVendorFilter`, `createBinaryFilter`, `createCustomFilter`) return uniform `FileFilter` interface enabling chain composition.\n\n**Statistics Aggregation**: `applyFilters()` returns `Map<string, {matched, rejected}>` keyed by filter name for telemetry without coupling to filter implementations.\n\n## Behavioral Contracts\n\n### Binary Detection Thresholds\n\n- **Extension fast path**: 80+ extensions across images/archives/executables/media/documents/fonts/compiled/database\n- **Size threshold**: `DEFAULT_MAX_FILE_SIZE = 1048576` (1MB)\n- **Content analysis**: `isBinaryFile()` fallback for unknown extensions\n\n### Concurrency Limit\n\n`CONCURRENCY = 30` workers in `applyFilters()` prevents file descriptor exhaustion during I/O-heavy binary detection.\n\n### Vendor Directories\n\n`DEFAULT_VENDOR_DIRS`: `['node_modules', 'vendor', '.git', 'dist', 'build', '__pycache__', '.next', 'venv', '.venv', 'target']`\n\n### Glob Configuration\n\n- `absolute: true` — returns full paths\n- `onlyFiles: true` — excludes directories\n- `dot: true` — includes dotfiles\n- `followSymbolicLinks: false` — default symlink handling\n- `ignore: ['**/.git/**']` — hardcoded `.git` exclusion\n\n### Path Normalization\n\nAll filters use `path.relative(normalizedRoot, absolutePath)` before exclusion tests, with guards against `'..'` prefix or empty string.\n\n## Subdirectories\n\n**[filters/](./filters/)** — Five filter implementations: `gitignore.ts` (pattern matching via `ignore` library), `vendor.ts` (third-party directory detection), `binary.ts` (extension/size/content analysis), `custom.ts` (user glob patterns), `index.ts` (bounded-concurrency orchestrator with telemetry).\n\n## Integration Points\n\n**Consumed by**: `src/cli/discover.ts`, `src/cli/generate.ts`, `src/cli/update.ts` invoke `discoverFiles()` as single entry point.\n\n**Configuration surface**: `DiscoveryConfig` threaded from `src/config/schema.ts` YAML parsing with fields `exclude.vendorDirs`, `exclude.binaryExtensions`, `exclude.patterns`, `options.maxFileSize`, `options.followSymlinks`.\n\n**Telemetry integration**: Accepts `ITraceWriter` via `DiscoverFilesOptions.tracer`, emits `filter:applied` events with per-filter `filesMatched`/`filesRejected` counts.\n\n**Debug output**: `options.debug` enables `console.error()` logging for filters with non-zero rejection counts.\n### src/discovery/filters/AGENTS.md\n\n<!-- Generated by agents-reverse-engineer -->\n\n# src/discovery/filters\n\n**Five filter implementations for file discovery exclusion: gitignore pattern matching, vendor directory detection, binary file detection via extension/content analysis, custom glob patterns, and bounded-concurrency filter chain orchestration with per-filter statistics tracking.**\n\n## Contents\n\n### Filter Implementations\n\n**[binary.ts](./binary.ts)** — `createBinaryFilter()` implements two-phase binary detection: extension-based fast path against `BINARY_EXTENSIONS` set (80+ extensions across images/archives/executables/media/documents/fonts/compiled/database), size threshold check against `maxFileSize` (default 1MB), content analysis fallback via `isBinaryFile()` for unknown extensions.\n\n**[custom.ts](./custom.ts)** — `createCustomFilter()` matches absolute paths against user-provided gitignore-style patterns via `ignore` library with relative path normalization, returns pass-through filter when pattern array empty.\n\n**[gitignore.ts](./gitignore.ts)** — `createGitignoreFilter()` async factory reads `.gitignore` from project root, delegates exclusion to `ignore` library instance with relative path conversion, silently passes all files when `.gitignore` missing.\n\n**[vendor.ts](./vendor.ts)** — `createVendorFilter()` excludes third-party dependency directories via two-tier matching: single-segment patterns (`'node_modules'`) matched against all path segments, multi-segment patterns (`'apps/vendor'`) matched via substring inclusion with OS-specific separator normalization.\n\n**[index.ts](./index.ts)** — `applyFilters()` orchestrates bounded-concurrency filter chain execution (30 workers sharing iterator), short-circuits on first exclusion, tracks per-filter `matched`/`rejected` statistics, emits `filter:applied` trace events, re-exports all filter creators and constants.\n\n## Filter Chain Architecture\n\n**Execution Model**: Worker pool pattern with `CONCURRENCY=30` limit prevents file descriptor exhaustion during I/O-heavy binary detection. Shared `files.entries()` iterator across workers via `for (const [index, file] of iter)` loop. Sequential filter evaluation per file with early termination on first match.\n\n**Statistics Tracking**: `Map<string, { matched: number; rejected: number }>` initialized for each filter before processing. `rejected` increments when filter excludes file. `matched` increments for all filters when file passes entire chain.\n\n**Result Preservation**: Workers collect `Array<{ index: number; file: string; excluded?: ExcludedFile }>`, sort by original `index` to maintain input order, segregate into `included`/`excluded` arrays for `FilterResult` return value.\n\n## Behavioral Contracts\n\n### Binary Extensions Set (binary.ts)\n\n`BINARY_EXTENSIONS` contains 80+ extensions organized by category:\n- **Images**: `.png`, `.jpg`, `.jpeg`, `.gif`, `.bmp`, `.ico`, `.webp`, `.svg`, `.tiff`, `.tif`, `.psd`, `.raw`, `.heif`, `.heic`\n- **Archives**: `.zip`, `.tar`, `.gz`, `.rar`, `.7z`, `.bz2`, `.xz`, `.tgz`\n- **Executables**: `.exe`, `.dll`, `.so`, `.dylib`, `.bin`, `.msi`, `.app`, `.dmg`\n- **Media**: `.mp3`, `.mp4`, `.wav`, `.avi`, `.mov`, `.mkv`, `.flac`, `.ogg`, `.webm`, `.m4a`, `.aac`, `.wma`, `.wmv`, `.flv`\n- **Documents**: `.pdf`, `.doc`, `.docx`, `.xls`, `.xlsx`, `.ppt`, `.pptx`, `.odt`, `.ods`, `.odp`\n- **Fonts**: `.woff`, `.woff2`, `.ttf`, `.eot`, `.otf`\n- **Compiled/bytecode**: `.class`, `.pyc`, `.pyo`, `.o`, `.obj`, `.a`, `.lib`, `.wasm`\n- **Database**: `.db`, `.sqlite`, `.sqlite3`, `.mdb`\n\n### Default Vendor Directories (vendor.ts)\n\n`DEFAULT_VENDOR_DIRS`: `['node_modules', 'vendor', '.git', 'dist', 'build', '__pycache__', '.next', 'venv', '.venv', 'target']`\n\n### Concurrency Limit (index.ts)\n\n`CONCURRENCY = 30` workers — hard limit to prevent file descriptor exhaustion during binary file detection I/O operations.\n\n### Binary Size Threshold (binary.ts)\n\n`DEFAULT_MAX_FILE_SIZE = 1048576` (1MB) — files exceeding threshold excluded without content analysis.\n\n## Path Normalization Patterns\n\n**gitignore.ts**: `path.relative(normalizedRoot, absolutePath)` → guard against `'..'` prefix or empty string before `ig.ignores()` call.\n\n**custom.ts**: `path.relative(normalizedRoot, absolutePath)` → guard against `'..'` prefix or empty string before `ig.ignores()` call.\n\n**vendor.ts**: Multi-segment patterns normalized via `dir.replace(/[\\\\/]/g, path.sep)` for cross-platform separator handling before `absolutePath.includes(pattern)` check.\n\n**binary.ts**: Extension extraction via `path.extname(absolutePath).toLowerCase()`, additional extensions normalized via `ext.startsWith('.') ? ext : \\`.\\${ext}\\`` before merging with `BINARY_EXTENSIONS`.\n\n## Filter Interface Contract\n\nAll filter creators return `FileFilter` object:\n```typescript\n{\n  name: string;              // Discriminator: 'gitignore' | 'vendor' | 'binary' | 'custom'\n  shouldExclude(absolutePath: string): boolean | Promise<boolean>\n}\n```\n\n**Synchronous filters**: custom.ts, vendor.ts, binary.ts (async internally but return sync `shouldExclude`).\n\n**Async factory**: gitignore.ts returns `Promise<FileFilter>` due to `.gitignore` file read.\n\n## Integration Points\n\n**Consumed by**: `src/discovery/walker.ts` composes filters into chain via `createGitignoreFilter()`, `createVendorFilter()`, `createBinaryFilter()`, `createCustomFilter()`, then calls `applyFilters()`.\n\n**Configuration surface**: `BinaryFilterOptions` with `maxFileSize`/`additionalExtensions` threaded from `src/config/schema.ts` YAML config (`exclude.binaryExtensions`, `options.maxFileSize`).\n\n**Telemetry integration**: Accepts optional `ITraceWriter` via `applyFilters()` options, emits `filter:applied` events with per-filter metrics (`filterName`, `filesMatched`, `filesRejected`).\n\n**Debug output**: `options.debug` flag enables `console.error(pc.dim(...))` logging for filters with `rejected > 0` count.\n### src/generation/AGENTS.md\n\n<!-- Generated by agents-reverse-engineer -->\n\n# generation\n\n**Orchestrates three-phase documentation generation pipeline: concurrent file analysis via `GenerationOrchestrator.createFileTasks()`, post-order directory synthesis via `buildExecutionPlan()` with depth-sorted traversal, and root document synthesis via `collectAgentsDocs()` aggregation, enforcing dependency graphs through `ExecutionTask.dependencies` arrays and completion predicates via `isDirectoryComplete()`.**\n\n## Contents\n\n### Pipeline Orchestration\n\n**[orchestrator.ts](./orchestrator.ts)** — `GenerationOrchestrator` prepares files via `prepareFiles(discoveryResult)` reading content with `readFile()`, creates file tasks via `createFileTasks(files)` calling `buildFilePrompt()` for each file, creates directory tasks via `createDirectoryTasks(files)` grouping by `path.dirname()`, emits trace events (`phase:start`, `plan:created`, `phase:end`) via `ITraceWriter`, clears `PreparedFile.content` after prompt embedding to free memory, returns `GenerationPlan` with `files`, `tasks`, `complexity` from `analyzeComplexity()`. `buildProjectStructure()` formats directory tree as indented text for AI context.\n\n**[executor.ts](./executor.ts)** — `buildExecutionPlan()` constructs dependency graph from `GenerationPlan`: populates `directoryFileMap` extracting directories via `path.dirname()`, creates file tasks with `id: \"file:{path}\"`, sorts files by `getDirectoryDepth(path.dirname())` descending (deepest first), sorts directories by depth descending, creates directory tasks with `dependencies: fileTaskIds`, creates root tasks with `dependencies: allDirTaskIds`, returns `ExecutionPlan` with `fileTasks`/`directoryTasks`/`rootTasks` arrays. `isDirectoryComplete()` checks all files have `.sum` outputs via `sumFileExists()`, `getReadyDirectories()` identifies directories eligible for `AGENTS.md` generation, `formatExecutionPlanAsMarkdown()` generates `GENERATION-PLAN.md` with checkbox lists grouped by phase and depth.\n\n**[collector.ts](./collector.ts)** — `collectAgentsDocs()` recursively walks project tree via `readdir(withFileTypes)`, collects files named exactly `AGENTS.md`, skips 13 directories in `SKIP_DIRS` set (`node_modules`, `.git`, `.agents-reverse-engineer`, `vendor`, `dist`, `build`, `__pycache__`, `.next`, `venv`, `.venv`, `target`, `.cargo`, `.gradle`), returns `AgentsDocs` array sorted by `relativePath` via `localeCompare()`. `collectAnnexFiles()` uses identical traversal logic for files ending `.annex.md`. Gracefully handles permission-denied via silent exception catch.\n\n**[complexity.ts](./complexity.ts)** — `analyzeComplexity()` computes `ComplexityMetrics` with `fileCount`, `directoryDepth` via `calculateDirectoryDepth()` (splits `path.relative()` by `path.sep`, tracks max), `directories` set via `extractDirectories()` (walks upward via repeated `path.dirname()` until root), `files` array. Consumed by `GenerationOrchestrator.createPlan()` for complexity warnings.\n\n**[types.ts](./types.ts)** — `AnalysisResult` with `summary: string` and `metadata: SummaryMetadata`, `SummaryMetadata` with `purpose`, `criticalTodos`, `relatedFiles`, `SummaryOptions` with `targetLength` discriminant (`'short' | 'standard' | 'detailed'`) and `includeCodeSnippets` boolean. Maps to `.sum` YAML frontmatter schema.\n\n## Subdirectories\n\n**[prompts/](./prompts/)** — Prompt template constants (`FILE_SYSTEM_PROMPT`, `DIRECTORY_SYSTEM_PROMPT`, `ROOT_SYSTEM_PROMPT`) with density rules (every sentence references identifiers), filler phrase prohibition (`\"this file\"`, `\"provides\"`, `\"responsible for\"`), behavioral contract extraction (verbatim regex/constants). Builders (`buildFilePrompt`, `buildDirectoryPrompt`, `buildRootPrompt`) substitute placeholders (`{{FILE_PATH}}`, `{{CONTENT}}`, `{{LANG}}`), aggregate child `.sum` files via `readSumFile()`, extract import maps via `extractDirectoryImports()`, preserve user content from `AGENTS.local.md`, switch to update prompts (`FILE_UPDATE_SYSTEM_PROMPT`, `DIRECTORY_UPDATE_SYSTEM_PROMPT`) when `existingSum`/`existingAgentsMd` present.\n\n**[writers/](./writers/)** — YAML frontmatter serialization (`writeSumFile`, `readSumFile`, `getSumPath`, `sumFileExists`) with SHA-256 hash tracking, inline/multi-line array formatting, annex file generation (`writeAnnexFile`, `getAnnexPath`) for reproduction-critical source content. `writeAgentsMd()` preserves user content via `AGENTS.local.md` rename, marker-based detection (`isGeneratedAgentsMd`), prepends preserved sections with `---` separator.\n\n## Post-Order Traversal\n\n`buildExecutionPlan()` enforces children-before-parents ordering via two sorts:\n1. **File tasks**: `getDirectoryDepth(path.dirname(a.path)) - getDirectoryDepth(path.dirname(b.path))` descending (deepest first)\n2. **Directory tasks**: `getDirectoryDepth(dirB) - getDirectoryDepth(dirA)` descending\n\nEnsures child `AGENTS.md` files exist before parent directory prompts consume them via `collectAgentsDocs()` in `buildDirectoryPrompt()`.\n\n## Dependency Graph\n\n`ExecutionTask.dependencies: string[]` enforces Phase 1 → Phase 2 → Phase 3 sequencing:\n- **File tasks**: empty dependencies (parallel eligible)\n- **Directory tasks**: depend on all file task IDs in directory (`id: \"file:{path}\"`)\n- **Root tasks**: depend on all directory task IDs (`id: \"dir:{path}\"`)\n\n`isDirectoryComplete()` verifies all `expectedFiles` have `.sum` outputs before allowing directory task execution.\n\n## Prompt Placeholder Pattern\n\nDirectory and root tasks store placeholder prompts (`\"Built at runtime by buildDirectoryPrompt()\"`) for plan display and dependency tracking. Actual prompts constructed at execution time in `src/orchestration/runner.ts` via `buildDirectoryPrompt()` and `buildRootPrompt()`, consuming child `.sum` files and subdirectory `AGENTS.md` not available during plan creation.\n\n## Trace Events\n\n`GenerationOrchestrator.createPlan()` emits:\n- `phase:start` with `{ type: 'phase:start', phase: 'plan-creation', taskCount, concurrency: 1 }`\n- `plan:created` with `{ type: 'plan:created', planType: 'generate', fileCount, taskCount: tasks.length + 1 }` (accounting for root task added by `buildExecutionPlan()`)\n- `phase:end` with `{ type: 'phase:end', phase: 'plan-creation', durationMs, tasksCompleted: 1, tasksFailed: 0 }`\n\n## Integration Points\n\nImports `buildFilePrompt` from `./prompts/index.js` for Phase 1 task creation. Imports `sumFileExists` from `./writers/sum.js` for completion checking. Imports `analyzeComplexity` from `./complexity.js` for metrics computation. Imports `ITraceWriter` from `../orchestration/trace.js` for event emission. Imports `Config` from `../config/schema.js` and `DiscoveryResult` from `../types/index.js`.\n\nConsumed by `src/orchestration/runner.ts` which executes `ExecutionPlan` via worker pool, calling `AIService` with runtime-constructed prompts for directory and root tasks.\n\n## Memory Management\n\nAfter `createFileTasks()` embeds file content into prompts, `createPlan()` clears `PreparedFile.content` via `(file as { content: string }).content = ''` to free memory. Comment notes runner re-reads files from disk during execution.\n### src/generation/prompts/AGENTS.md\n\n<!-- Generated by agents-reverse-engineer -->\n\n# generation/prompts\n\nPrompt construction engine for the three-phase documentation pipeline: `buildFilePrompt()` assembles file analysis prompts with import maps and incremental update sections, `buildDirectoryPrompt()` aggregates child `.sum` files and subdirectory `AGENTS.md` with import maps for directory synthesis, `buildRootPrompt()` collects all `AGENTS.md` files and package.json metadata for root document generation.\n\n## Contents\n\n### Prompt Builders\n\n**[builder.ts](./builder.ts)** — Implements `buildFilePrompt()` with `detectLanguage()` mapping 21 extensions to syntax identifiers, `buildDirectoryPrompt()` aggregating `.sum` files via `readSumFile()` and extracting import maps via `extractDirectoryImports()`, `buildRootPrompt()` calling `collectAgentsDocs()` for project-level synthesis. Substitutes placeholders `{{FILE_PATH}}`, `{{CONTENT}}`, `{{LANG}}`, `{{PROJECT_PLAN_SECTION}}` in user prompts. Switches between fresh generation prompts (`FILE_SYSTEM_PROMPT`, `DIRECTORY_SYSTEM_PROMPT`) and update prompts (`FILE_UPDATE_SYSTEM_PROMPT`, `DIRECTORY_UPDATE_SYSTEM_PROMPT`) based on `existingSum`/`existingAgentsMd` presence. Preserves user content from `AGENTS.local.md` by appending as `## User Notes` section. Debug logging emits template action/metadata pairs to stderr via `picocolors`.\n\n**[templates.ts](./templates.ts)** — Exports six system prompt constants: `FILE_SYSTEM_PROMPT` enforces density rules (every sentence references specific identifiers), anchor term preservation (exact export name casing), behavioral contract extraction (verbatim regex patterns, format strings, magic constants), `FILE_USER_PROMPT` template with `{{FILE_PATH}}`/`{{CONTENT}}` placeholders, `FILE_UPDATE_SYSTEM_PROMPT` for incremental regeneration preserving unchanged sections, `DIRECTORY_SYSTEM_PROMPT` mandating first line `<!-- Generated by agents-reverse-engineer -->` with adaptive section strategy (Contents/Subdirectories/Architecture/Stack/Structure/Patterns/Configuration/API Surface/File Relationships/Behavioral Contracts), `DIRECTORY_UPDATE_SYSTEM_PROMPT` for directory-level incremental updates, `ROOT_SYSTEM_PROMPT` enforcing synthesis-only mode prohibiting invention of features/hooks/APIs/patterns. All prompts prohibit filler phrases: `\"this file\"`, `\"this module\"`, `\"provides\"`, `\"responsible for\"`, `\"is used to\"`.\n\n**[types.ts](./types.ts)** — Defines `PromptContext` interface with fields `filePath`, `content`, `contextFiles[]`, `projectPlan`, `existingSum` for Phase 1/2 builders. Exports `SUMMARY_GUIDELINES` constant object with `targetLength: {min: 300, max: 500}`, `include[]` array of 8 mandatory content categories (purpose, public interface, patterns, dependencies, function signatures, coupled files, behavioral contracts, annex references), `exclude[]` array of 3 prohibited categories (control flow minutiae, generic TODOs, broad architectural relationships).\n\n**[index.ts](./index.ts)** — Barrel module re-exporting `buildFilePrompt`, `buildDirectoryPrompt`, `buildRootPrompt`, `detectLanguage` from `builder.ts` and `PromptContext`, `SUMMARY_GUIDELINES` from `types.ts`. Separates prompt template strings (in `templates.ts`) from assembly logic.\n\n## File Relationships\n\n`builder.ts` consumes templates from `templates.ts` (`FILE_SYSTEM_PROMPT`, `FILE_USER_PROMPT`, `FILE_UPDATE_SYSTEM_PROMPT`, `DIRECTORY_SYSTEM_PROMPT`, `DIRECTORY_UPDATE_SYSTEM_PROMPT`, `ROOT_SYSTEM_PROMPT`). `builder.ts` imports `GENERATED_MARKER` from `../writers/agents-md.js` for user content detection, `readSumFile()`/`getSumPath()` from `../writers/sum.js` for child summary aggregation, `extractDirectoryImports()`/`formatImportMap()` from `../../imports/index.js` for dependency graph construction, `collectAgentsDocs()` from `../collector.js` for Phase 3 synthesis. `index.ts` re-exports `PromptContext` from `types.ts` for consumer type safety.\n\n## Incremental Update Strategy\n\nAll three builders accept optional existing content: `context.existingSum` in `buildFilePrompt()`, `existingAgentsMd` in `buildDirectoryPrompt()`. When provided, appends existing content under heading `## Existing [Summary|AGENTS.md] (update this — preserve stable content, modify only what changed)` and switches to update-specific system prompts. `FILE_UPDATE_SYSTEM_PROMPT` enforces preservation rules: keep unchanged sections verbatim, modify only content affected by code changes, preserve behavioral contracts (regex/constants) unless source changed them. `DIRECTORY_UPDATE_SYSTEM_PROMPT` preserves structure/headings/descriptions for unchanged files, modifies only entries whose summaries changed, adds/removes entries for new/deleted files.\n\n## User Content Preservation\n\n`buildDirectoryPrompt()` checks for `AGENTS.local.md` first, falling back to checking existing `AGENTS.md` for absence of `GENERATED_MARKER` constant. User content appended as `## User Notes` section with reference link pattern `[AGENTS.local.md](./AGENTS.local.md)`. First-run detection message: \"This user-defined content will be preserved as [AGENTS.local.md](./AGENTS.local.md).\"\n\n## Behavioral Contracts\n\nAll templates enforce:\n\n- **Density rule**: Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- **Anchor term preservation**: All exported function/class/type/const names MUST appear in summary exactly as written in source\n- **Filler phrase prohibition**: `\"this file\"`, `\"this module\"`, `\"provides\"`, `\"responsible for\"`, `\"is used to\"`, `\"basically\"`, `\"essentially\"`, `\"provides functionality for\"`\n- **Output format**: Start response DIRECTLY with bold purpose statement without preamble\n- **PATH ACCURACY**: Use only paths from \"Import Map\" section, exact directory names from \"Project Directory Structure\", no path invention\n- **CONSISTENCY**: Do not contradict within same document (e.g., calling technique \"regex-based\" then \"AST-based\")\n\n`DIRECTORY_SYSTEM_PROMPT` enforces first line exactly `<!-- Generated by agents-reverse-engineer -->` followed by `#` heading with directory name.\n\n`FILE_SYSTEM_PROMPT` defines REPRODUCTION-CRITICAL CONTENT strategy: files with large string constants should list them in `## Annex References` section rather than inlining content, delegating extraction to pipeline automation.\n\n## Annex References\n\nFull prompt template text: [templates.ts.annex.md](./templates.ts.annex.md)  \nPromptContext interface specification: [types.ts.annex.md](./types.ts.annex.md)\n### src/generation/writers/AGENTS.md\n\n<!-- Generated by agents-reverse-engineer -->\n\n# generation/writers\n\nImplements `.sum` file and `AGENTS.md` serialization with YAML frontmatter parsing, user content preservation, and annex file generation for reproduction-critical source content.\n\n## Contents\n\n### [sum.ts](./sum.ts)\nYAML frontmatter I/O for `.sum` files with SHA-256 hash tracking, array serialization (inline/multi-line), annex file generation for reproduction-critical content. Exports `writeSumFile`, `readSumFile`, `getSumPath`, `sumFileExists`, `writeAnnexFile`, `getAnnexPath`, `parseSumFile`, `formatSumFile`, `SumFileContent` interface.\n\n### [agents-md.ts](./agents-md.ts)\nAGENTS.md lifecycle management preserving user-authored content via AGENTS.local.md rename, marker-based detection. Exports `writeAgentsMd`, `isGeneratedAgentsMd`, `GENERATED_MARKER` constant.\n\n### [index.ts](./index.ts)\nBarrel re-exporting `writeSumFile`, `readSumFile`, `getSumPath`, `sumFileExists`, `SumFileContent` from `sum.ts` and `writeAgentsMd` from `agents-md.ts`.\n\n## Data Flow\n\n**Phase 1 (File Analysis):**\n`src/generation/executor.ts` → `writeSumFile(sourcePath, content)` → appends `.sum` extension → `formatSumFile()` → `mkdir` + `writeFile` → emits `.sum` file with frontmatter delimiter `---\\n...\\n---\\n` + summary body.\n\n**Phase 2 (Directory Aggregation):**\n`src/generation/orchestrator.ts` → `writeAgentsMd(dirPath, projectRoot, llmContent)` → checks existing AGENTS.md via `isGeneratedAgentsMd()` → renames user file to AGENTS.local.md → prepends preserved content + separator `---` + LLM-generated sections.\n\n**Incremental Updates:**\n`src/update/orchestrator.ts` → `readSumFile(getSumPath(sourcePath))` → `parseSumFile()` extracts `contentHash` → SHA-256 comparison → hash mismatch triggers re-analysis.\n\n**Orphan Cleanup:**\n`src/update/orphan-cleaner.ts` → detects stale `.sum` files for deleted sources → calls `unlink(getSumPath(deletedPath))`.\n\n## YAML Frontmatter Format\n\n**Delimiter:** `---\\n` before and after metadata block, exactly one newline before summary body.\n\n**Scalar fields:**\n```yaml\ngenerated_at: 2026-02-09T12:34:56.789Z\ncontent_hash: a3f5d8e9... (SHA-256 hex, 64 chars)\npurpose: Single-line description\n```\n\n**Array fields (inline format <40 chars, ≤3 items):**\n```yaml\ncritical_todos: [Security issue, Performance bug]\nrelated_files: [path/to/file.ts]\n```\n\n**Array fields (multi-line format):**\n```yaml\ncritical_todos:\n  - Long security issue description exceeding 40 characters\n  - Another long description\nrelated_files:\n  - very/long/path/to/related/file/exceeding/character/limit.ts\n```\n\n## Parsing Strategies\n\n**Frontmatter extraction:** Regex `/^---\\n([\\s\\S]*?)\\n---\\n/` captures metadata block.\n\n**Scalar field extraction:** Individual regex per field (`/generated_at:\\s*(.+)/`, `/content_hash:\\s*(.+)/`, `/purpose:\\s*(.+)/`), captures group 1, trims whitespace.\n\n**Array parsing (inline):** `new RegExp(\\`${key}:\\\\s*\\\\[([^\\\\]]*)\\\\]\\`)` captures comma-separated values within brackets, splits on `,`, strips quotes and whitespace.\n\n**Array parsing (multi-line):** `new RegExp(\\`${key}:\\\\s*\\\\n((?:\\\\s+-\\\\s+.+\\\\n?)+)\\`, 'm')` captures indented list items, splits on newlines, strips `- ` prefix per line.\n\n## User Content Preservation Workflow\n\n1. `writeAgentsMd()` called with `dirPath` and LLM-generated `content`\n2. Constructs `agentsPath` (`dirPath/AGENTS.md`) and `localPath` (`dirPath/AGENTS.local.md`)\n3. If `agentsPath` exists and `isGeneratedAgentsMd(agentsPath)` returns `false`: `rename(agentsPath, localPath)`, capture content as `userContent`\n4. Else if `localPath` exists: `readFile(localPath)`, capture as `userContent`\n5. Strip `GENERATED_MARKER` prefix from LLM `content` if present via `slice()` + `/^\\n+/` trim\n6. Assemble `finalContent`:\n   - `GENERATED_MARKER` comment\n   - If `userContent.trim()` non-empty: preservation comment + `userContent.trim()` + `---` separator\n   - LLM-generated sections\n7. `mkdir(dirPath, { recursive: true })` + `writeFile(agentsPath, finalContent)`\n\n## Annex File Generation\n\n`writeAnnexFile(sourcePath, sourceContent)` creates `${sourcePath}.annex.md`:\n```markdown\n<!-- Generated by agents-reverse-engineer -->\n\n# Annex: <basename>\n\nThis annex file preserves the full source content of `<basename>` for AI coding assistants.\nThe primary summary is in `<basename>.sum`.\n\n```<ext>\n<sourceContent>\n```\n```\n\nUsed for files containing long regex patterns, template strings, or format specifications where verbatim content required for reproduction. `getAnnexPath()` computes path without filesystem access.\n\n## Behavioral Contracts\n\n**Marker detection:** `content.includes('<!-- Generated by agents-reverse-engineer -->')` substring search (no regex).\n\n**Marker removal:** `llmContent.startsWith(GENERATED_MARKER) ? llmContent.slice(GENERATED_MARKER.length).replace(/^\\n+/, '') : llmContent`.\n\n**File extensions:** `.sum` (metadata + summary), `.annex.md` (full source preservation).\n\n**User content separator:** Three-dash horizontal rule surrounded by blank lines (`\\n\\n---\\n\\n`).\n\n**Frontmatter regex:** `/^---\\n([\\s\\S]*?)\\n---\\n/` non-greedy capture of metadata block.\n\n**Array inline regex:** `` new RegExp(`${key}:\\\\s*\\\\[([^\\\\]]*)\\\\]`) `` captures values within square brackets.\n\n**Array multi-line regex:** `` new RegExp(`${key}:\\\\s*\\\\n((?:\\\\s+-\\\\s+.+\\\\n?)+)`, 'm') `` captures indented list items starting with `- `.\n\n## Integration Points\n\n**Imports:**\n- `../types.js` → `SummaryMetadata` (metadata structure for frontmatter)\n- `./agents-md.js` → `GENERATED_MARKER` (annex file header constant)\n\n**Consumed By:**\n- `src/generation/executor.ts` → `writeSumFile()` for Phase 1 output\n- `src/generation/orchestrator.ts` → `writeAgentsMd()` for Phase 2 output\n- `src/update/orchestrator.ts` → `readSumFile()` for hash comparison\n- `src/update/orphan-cleaner.ts` → `getSumPath()` for stale file deletion\n- `src/generation/collector.ts` → reads AGENTS.local.md when aggregating subdirectory docs\n### src/imports/AGENTS.md\n\n<!-- Generated by agents-reverse-engineer -->\n\n# src/imports/\n\nStatic import analysis subsystem extracting TypeScript/JavaScript import statements via regex parsing to construct dependency graphs consumed by Phase 1 file analysis and Phase 2 directory aggregation prompts.\n\n## Contents\n\n### [extractor.ts](./extractor.ts)\nRegex-based parser matching static import statements via `IMPORT_REGEX` (`/^import\\s+(type\\s+)?(?:\\{([^}]*)\\}|(\\*\\s+as\\s+\\w+)|(\\w+))\\s+from\\s+['\"]([^'\"]+)['\"]/gm`). Exports `extractImports()` returning `ImportEntry[]` with `specifier`, `symbols[]`, `typeOnly` fields. Exports `extractDirectoryImports()` reading first 100 lines per file, classifying imports as `internal` (`./` prefix) or `external` (`../` prefix), filtering bare package specifiers and `node:` built-ins. Exports `formatImportMap()` converting `FileImports[]` to human-readable text blocks for LLM prompt embedding.\n\n### [types.ts](./types.ts)\nDefines `ImportEntry` interface with `specifier: string`, `symbols: string[]`, `typeOnly: boolean` representing single import statement. Defines `FileImports` interface with `fileName: string`, `externalImports: ImportEntry[]`, `internalImports: ImportEntry[]` partitioning imports by locality.\n\n### [index.ts](./index.ts)\nBarrel export re-exporting `extractImports`, `extractDirectoryImports`, `formatImportMap` from `extractor.ts` and `ImportEntry`, `FileImports` from `types.ts`.\n\n## Data Flow\n\n1. **File Analysis (Phase 1):** `extractDirectoryImports()` invoked by `src/generation/prompts/builder.ts` → reads first 100 lines per source file → applies `IMPORT_REGEX` → classifies imports by locality → returns `FileImports[]`\n2. **Prompt Embedding:** `formatImportMap()` serializes `FileImports[]` → produces text block with filename headers and indented specifier-symbol pairs → embedded in AI prompts via `buildFileAnalysisPrompt()`\n3. **Directory Aggregation (Phase 2):** Import maps from child files aggregated during directory-level `AGENTS.md` generation to provide cross-module dependency context\n\n## Import Classification\n\n| Category | Pattern | Included | Purpose |\n|----------|---------|----------|---------|\n| **Internal** | `./filename` | Yes | Same-directory coupling (tight cohesion signals) |\n| **External** | `../path/module` | Yes | Cross-directory dependencies (module boundaries) |\n| **Package** | `'react'` | No | Third-party npm dependencies (not project structure) |\n| **Built-in** | `'node:fs'` | No | Runtime APIs (not project structure) |\n\n## Performance Optimization\n\nReads only first 100 lines per file via `content.split('\\n').slice(0, 100).join('\\n')` before regex matching, assuming ES module convention of top-level imports. Skips unreadable files silently via empty catch blocks in `extractDirectoryImports()`.\n\n## Integration Points\n\n- **Consumed by:** `src/generation/prompts/builder.ts` embeds import maps in file analysis prompts via `extractDirectoryImports()` + `formatImportMap()`\n- **Prompts:** `src/generation/prompts/templates.ts` includes import map sections in Phase 1 file analysis and Phase 2 directory aggregation templates\n- **Output format:** Human-readable text blocks with filename headers (`external imports from foo.ts:`) and indented specifier-symbol pairs (`  ../ai/index.js → AIService`)\n### src/installer/AGENTS.md\n\n<!-- Generated by agents-reverse-engineer -->\n\n# src/installer\n\nOrchestrates npx-based installation of ARE commands and hooks across Claude Code, OpenCode, and Gemini CLI runtimes with interactive prompts, platform-specific path resolution, settings.json manipulation, and uninstallation workflows.\n\n## Contents\n\n### Core Orchestration\n\n**[index.ts](./index.ts)** — Entry point exposing `runInstaller()` and `parseInstallerArgs()` for CLI integration. Parses short/long flags (`-g`/`--global`, `-l`/`--local`, `--runtime`, `--force`, `--quiet`, `--uninstall`), validates non-interactive mode requirements, dispatches to `runInstall()` or `runUninstall()` based on arguments, displays results via `displayInstallResults()` and `displayUninstallResults()`. Re-exports all types and functions from sibling modules for barrel pattern.\n\n**[operations.ts](./operations.ts)** — Implements `installFiles()`, `verifyInstallation()`, `registerHooks()`, `registerPermissions()` for file copying, settings.json hook registration (SessionStart/SessionEnd events with nested HookEvent arrays for Claude, flat GeminiHook arrays for Gemini), and bash permission patterns (`ARE_PERMISSIONS` array: `'Bash(npx agents-reverse-engineer@latest init*)'` through `clean*`, `'Bash(rm -f .agents-reverse-engineer/progress.log*)'`, `'Bash(sleep *)'`). Resolves bundled hooks via `getBundledHookPath()` navigating from `dist/installer/operations.js` up to `hooks/dist/`. Writes `ARE-VERSION` file via `getPackageVersion()`.\n\n**[uninstall.ts](./uninstall.ts)** — Mirrors installation logic with `uninstallFiles()`, `unregisterHooks()`, `unregisterPermissions()`, `deleteConfigFolder()`. Removes command templates, hooks, plugins, settings.json entries, ARE-VERSION file. Implements `cleanupAreSkillDirs()` for Claude skills format, `cleanupEmptyDirs()` for recursive bottom-up directory removal, `cleanupLegacyGeminiFiles()` for obsolete Markdown/TOML formats. Repurposes `InstallerResult.filesCreated` to track deleted files, `hookRegistered` for unregistration status.\n\n### Path Resolution\n\n**[paths.ts](./paths.ts)** — Exports `getRuntimePaths()`, `resolveInstallPath()`, `getSettingsPath()`, `getAllRuntimes()`, `isRuntimeInstalledLocally()`, `isRuntimeInstalledGlobally()`, `getInstalledRuntimes()`. Resolves global/local paths with environment overrides: `CLAUDE_CONFIG_DIR` (`~/.claude` fallback), `OPENCODE_CONFIG_DIR` or `XDG_CONFIG_HOME/opencode` (`~/.config/opencode` fallback), `GEMINI_CONFIG_DIR` (`~/.gemini` fallback). Returns `RuntimePaths` with `global`, `local`, `settingsFile` fields.\n\n### User Interface\n\n**[prompts.ts](./prompts.ts)** — Provides `selectRuntime()`, `selectLocation()`, `confirmAction()`, `isInteractive()` with `arrowKeySelect()` for TTY mode (arrow-key navigation via `readline.emitKeypressEvents()` + `process.stdin.setRawMode(true)` with ANSI escape sequences `\\x1b[${n}A`, `\\x1b[2K`, `\\x1b[1B` for rendering) and `numberedSelect()` fallback for piped input. Registers `cleanupRawMode()` on `process.on('exit')` and `process.on('SIGINT')` to restore terminal state via `setRawMode(false)` + `pause()`.\n\n**[banner.ts](./banner.ts)** — Exports `displayBanner()` (Unicode box-drawing ASCII art logo U+2588/U+2550-U+2557), `showHelp()`, `showSuccess()`/`showError()`/`showWarning()`/`showInfo()` (prefix symbols: green ✓, red ✗, yellow !, cyan >), `showNextSteps()` (lists commands `/are-help`, `/are-init`, `/are-discover`, `/are-generate`, `/are-update`, `/are-specify`, `/are-clean` with GitHub documentation link).\n\n### Type Definitions\n\n**[types.ts](./types.ts)** — Defines `Runtime` (`'claude' | 'opencode' | 'gemini' | 'all'`), `Location` (`'global' | 'local'`), `InstallerArgs` (`runtime?`, `global`, `local`, `uninstall`, `force`, `help`, `quiet`), `InstallerResult` (`success`, `runtime: Exclude<Runtime, 'all'>`, `location`, `filesCreated`, `filesSkipped`, `errors`, `hookRegistered?`, `versionWritten?`), `RuntimePaths` (`global`, `local`, `settingsFile`).\n\n## Subdirectories\n\nNone — flat structure with seven TypeScript modules.\n\n## Architecture\n\n### Two-Phase Workflow\n\n**Installation (runInstall):**\n1. Argument parsing via `parseInstallerArgs()` with short/long flag normalization\n2. Interactive prompts via `selectRuntime()`/`selectLocation()` when TTY detected and values missing\n3. Template copying via `getTemplatesForRuntime()` → `getClaudeTemplates()`/`getOpenCodeTemplates()`/`getGeminiTemplates()` from `src/integration/templates.ts`\n4. Hook/plugin installation: Claude/Gemini to `hooks/` subdirectory using `ARE_HOOKS` array (currently empty), OpenCode to `plugins/` using `ARE_PLUGINS` array\n5. Settings.json modification: `registerHooks()` merges SessionStart/SessionEnd hooks with duplicate detection, `registerPermissions()` adds `ARE_PERMISSIONS` bash patterns to Claude's permissions.allow array\n6. Version file creation via `writeVersionFile()` for update checker hooks\n7. Verification via `verifyInstallation()` checking `existsSync()` for all filesCreated paths\n\n**Uninstallation (runUninstall):**\n1. Template path extraction with runtime prefix slicing (`template.path.split('/').slice(1).join('/')`)\n2. File deletion via `unlinkSync()` for templates, hooks, plugins, ARE-VERSION\n3. Settings.json cleanup: `unregisterHooks()` filters hook arrays via pattern matching (current format `node .claude/hooks/${filename}`, legacy `node hooks/${filename}`), `unregisterPermissions()` filters `ARE_PERMISSIONS` from permissions.allow\n4. Directory cleanup: `cleanupAreSkillDirs()` removes empty `are-*` skill directories, `cleanupEmptyDirs()` recursively removes empty parents up to runtime root, `cleanupLegacyGeminiFiles()` deletes obsolete `.md`/`.toml` formats\n5. Config folder deletion via `deleteConfigFolder()` only for local installations (`location === 'local'`)\n\n### Runtime-Specific Patterns\n\n**Claude Code:**\n- Global path: `~/.claude` (override: `CLAUDE_CONFIG_DIR`)\n- Commands: `.claude/skills/<command>/SKILL.md` with frontmatter `name: /are-<command>`\n- Hooks: `.claude/hooks/<filename>.js` registered in `settings.json` with nested structure `{ hooks: { SessionStart?: [{ hooks: [{ type: 'command', command: string }] }] } }`\n- Permissions: `settings.json` permissions.allow array with bash command patterns\n- Settings file: `~/.claude/settings.json`\n\n**OpenCode:**\n- Global path: `~/.config/opencode` (overrides: `OPENCODE_CONFIG_DIR` > `XDG_CONFIG_HOME/opencode`)\n- Commands: `.opencode/commands/<command>.md` with frontmatter `agent: build`\n- Plugins: `.opencode/plugins/<filename>.js` exporting async factory functions with `event['session.created']`/`event['session.deleted']` handlers\n- No settings file (plugin registration via file presence)\n\n**Gemini CLI:**\n- Global path: `~/.gemini` (override: `GEMINI_CONFIG_DIR`)\n- Commands: `.gemini/commands/<command>.toml` with `description`/`prompt` fields\n- Hooks: `.gemini/hooks/<filename>.js` registered in `settings.json` with flat structure `{ hooks: { SessionStart?: [{ name: string, type: 'command', command: string }] } }`\n- Settings file: `~/.gemini/settings.json`\n\n## Behavioral Contracts\n\n### Hook/Plugin Definitions\n\n**ARE_HOOKS (operations.ts):**\n```typescript\nconst ARE_HOOKS: HookDefinition[] = [\n  // Array intentionally empty — hooks disabled due to issues\n];\n```\n\n**ARE_PLUGINS (operations.ts):**\n```typescript\nconst ARE_PLUGINS: PluginDefinition[] = [\n  { srcFilename: 'opencode-are-check-update.js', destFilename: 'are-check-update.js' },\n  // are-session-end.js disabled\n];\n```\n\n**ARE_PERMISSIONS (operations.ts):**\n```typescript\nconst ARE_PERMISSIONS: string[] = [\n  'Bash(npx agents-reverse-engineer@latest init*)',\n  'Bash(npx agents-reverse-engineer@latest discover*)',\n  'Bash(npx agents-reverse-engineer@latest generate*)',\n  'Bash(npx agents-reverse-engineer@latest update*)',\n  'Bash(npx agents-reverse-engineer@latest clean*)',\n  'Bash(rm -f .agents-reverse-engineer/progress.log*)',\n  'Bash(sleep *)',\n];\n```\n\n**Hook command pattern (operations.ts):**\n```javascript\n`node ${runtimeDir}/hooks/${hookDef.filename}`\n```\n\n**Legacy hook patterns (uninstall.ts):**\n```typescript\nfunction getHookPatterns(runtimeDir: string): string[] {\n  return ARE_HOOKS.flatMap(hook => [\n    `node ${runtimeDir}/hooks/${hook.filename}`,  // Current format\n    `node hooks/${hook.filename}`,                 // Legacy format\n  ]);\n}\n```\n\n### ANSI Escape Sequences (prompts.ts)\n\n**Cursor control in arrowKeySelect:**\n- `\\x1b[${n}A` — Move cursor up n lines\n- `\\x1b[2K` — Clear entire line\n- `\\x1b[1B` — Move cursor down 1 line\n\n**Keypress handling:**\n- Up arrow: `key.name === 'up'` → `selectedIndex = Math.max(0, selectedIndex - 1)`\n- Down arrow: `key.name === 'down'` → `selectedIndex = Math.min(options.length - 1, selectedIndex + 1)`\n- Enter: `key.name === 'return'` → resolve with `options[selectedIndex].value`\n- Ctrl+C: `key.ctrl && key.name === 'c'` → `cleanupRawMode()` + `process.exit(0)`\n\n### Settings.json Schemas\n\n**Claude (operations.ts):**\n```typescript\ninterface SettingsJson {\n  hooks?: {\n    SessionStart?: HookEvent[];\n    SessionEnd?: HookEvent[];\n  };\n  permissions?: {\n    allow?: string[];\n    deny?: string[];\n  };\n}\n\ninterface HookEvent {\n  hooks: SessionHook[];\n}\n\ninterface SessionHook {\n  type: 'command';\n  command: string;\n}\n```\n\n**Gemini (operations.ts):**\n```typescript\ninterface GeminiSettingsJson {\n  hooks?: {\n    SessionStart?: GeminiHook[];\n    SessionEnd?: GeminiHook[];\n  };\n}\n\ninterface GeminiHook {\n  name: string;\n  type: 'command';\n  command: string;\n}\n```\n\n### Directory Cleanup Terminal Conditions (uninstall.ts)\n\n**Runtime root check in cleanupEmptyDirs:**\n```typescript\nconst baseName = path.basename(dirPath);\nif (['.claude', '.opencode', '.gemini', '.config'].includes(baseName)) {\n  return;\n}\n```\n\n**ARE skill directory pattern (uninstall.ts):**\n```typescript\nentry.startsWith('are-') && stats.isDirectory()\n```\n\n**Legacy Gemini file patterns (uninstall.ts):**\n```typescript\n// Markdown format\nentry.startsWith('are-') && entry.endsWith('.md')\n\n// TOML namespace directory\n'commands/are/*.toml'\n```\n\n## File Relationships\n\n**Orchestration chain:**\n1. `index.ts` parses args → calls `prompts.ts` selectors → calls `operations.ts` or `uninstall.ts`\n2. `operations.ts`/`uninstall.ts` call `paths.ts` for directory resolution\n3. `operations.ts` calls `src/integration/templates.ts` for command content\n4. `operations.ts` reads bundled hooks from `hooks/dist/` via `getBundledHookPath()`\n5. `operations.ts`/`uninstall.ts` manipulate settings.json via in-memory JSON parse/stringify\n6. `banner.ts` provides display functions consumed by `index.ts` for results rendering\n\n**Shared state:**\n- `prompts.ts` maintains module-level `rawModeActive` flag for terminal cleanup\n- `operations.ts` and `uninstall.ts` share `ARE_HOOKS`, `ARE_PLUGINS`, `ARE_PERMISSIONS` definitions\n- `paths.ts` provides single source of truth for runtime directory mappings\n\n**Type flow:**\n- `types.ts` defines discriminated union `Runtime` with `'all'` excluded via `Exclude<Runtime, 'all'>` in `InstallerResult.runtime`\n- `InstallerArgs` parsed in `index.ts`, threaded through `runInstall()`/`runUninstall()` to `operations.ts`/`uninstall.ts`\n- `RuntimePaths` returned by `paths.ts`, consumed by `operations.ts`/`uninstall.ts` for file path construction\n### src/integration/AGENTS.md\n\n<!-- Generated by agents-reverse-engineer -->\n\n# src/integration\n\n**Platform-specific AI assistant integration layer detecting Claude Code/OpenCode/Gemini/Aider environments, generating command template files with progress-monitoring patterns, deploying bundled session hooks, and enforcing skip-if-exists safety with force override.**\n\n## Contents\n\n### Environment Detection\n\n**[detect.ts](./detect.ts)** — `detectEnvironments()` scans project root for `.claude/`, `.opencode/`, `.aider/`, `.gemini/` directories and `CLAUDE.md`/`.aider.conf.yml` marker files, returns `DetectedEnvironment[]` with `type`, `configDir`, `detected` fields. `hasEnvironment()` filters by `EnvironmentType`.\n\n### File Generation Orchestration\n\n**[generate.ts](./generate.ts)** — `generateIntegrationFiles()` orchestrates template instantiation by detecting/overriding environments via `detectEnvironments()`, retrieving templates via `getTemplatesForEnvironment()`, writing command files with `ensureDir()` + `writeFileSync()`, deploying bundled hooks for Claude via `readBundledHook()` from `hooks/dist/are-session-end.js`, returns `IntegrationResult[]` with `filesCreated`/`filesSkipped` arrays. `GenerateOptions` supports `dryRun`, `force` (overwrite existing), and `environment` (bypass auto-detection).\n\n### Template Library\n\n**[templates.ts](./templates.ts)** — Exports `getClaudeTemplates()`, `getOpenCodeTemplates()`, `getGeminiTemplates()` generating platform-specific command files for seven commands: `generate`, `update`, `init`, `discover`, `clean`, `specify`, `help`. Encapsulates `COMMANDS` object with markdown prompt content, `PLATFORM_CONFIGS` mapping `EnvironmentType` to `PlatformConfig` (command prefix, path structure, frontmatter rules), `buildTemplate()`/`buildGeminiToml()` factories performing placeholder substitution (`COMMAND_PREFIX`, `VERSION_FILE_PATH`). Command content patterns: background execution via `npx agents-reverse-engineer@latest`, progress polling with `.agents-reverse-engineer/progress.log` offset reads, `TaskOutput` checks with `block: false`, completion summaries with phase metrics.\n\n### Type Definitions\n\n**[types.ts](./types.ts)** — `EnvironmentType` union (`'claude' | 'opencode' | 'aider' | 'gemini'`), `DetectedEnvironment` interface (`type`, `configDir`, `detected`), `IntegrationTemplate` interface (`filename`, `path`, `content`), `IntegrationResult` interface (`environment`, `filesCreated`, `filesSkipped`).\n\n## Platform Configuration\n\n| Platform | Command Prefix | Path Pattern | Frontmatter | Version File |\n|----------|---------------|--------------|-------------|--------------|\n| **Claude** | `/are-` | `.claude/skills/are-{command}/SKILL.md` | `name: /are-{command}` | `.claude/ARE-VERSION` |\n| **OpenCode** | `/are-` | `.opencode/commands/are-{command}.md` | `agent: build` | `.opencode/ARE-VERSION` |\n| **Gemini** | `/are-` | `.gemini/commands/are-{command}.toml` | TOML `description`/`prompt` | `.gemini/ARE-VERSION` |\n| **Aider** | N/A | N/A (detection only) | N/A | N/A |\n\n## Command Template Behavior\n\n- **generate/update/specify**: 15s poll intervals, background execution via `run_in_background: true`, offset-based log tailing, three-phase progress reporting (discovery → file analysis → directory/root docs)\n- **discover**: 10s poll intervals, background execution, file count reporting\n- **init**: Synchronous execution, creates `.agents-reverse-engineer/config.yaml`\n- **clean**: Synchronous execution with STRICT RULES enforcing zero flag additions, deletion count reporting\n- **help**: Outputs command reference with platform-specific `COMMAND_PREFIX` placeholder substitution\n\n## Integration with Project\n\nConsumed by `src/installer/` for `npx agents-reverse-engineer --runtime <env>` workflow. `detectEnvironments()` validates presence before installation, `generateIntegrationFiles()` writes command files to `.claude/skills/`, `.opencode/commands/`, or `.gemini/commands/`. Bundled hook deployment via `hooks/dist/are-session-end.js` for Claude's SessionEnd lifecycle.\n### src/orchestration/AGENTS.md\n\n<!-- Generated by agents-reverse-engineer -->\n\n# src/orchestration\n\n**Worker pool concurrency control, promise-chain serialized progress tracking, NDJSON trace emission, and three-phase pipeline orchestration executing concurrent file analysis, post-order directory aggregation, and sequential root synthesis with integrated quality validation, ETA calculation, and subprocess lifecycle tracing.**\n\n## Contents\n\n### Core Orchestration\n\n**[index.ts](./index.ts)** — Barrel export consolidating `runPool`, `ProgressReporter`, `ProgressLog`, `PlanTracker`, `createTraceWriter`, `cleanupOldTraces`, `CommandRunner`, and shared types (`FileTaskResult`, `RunSummary`, `ProgressEvent`, `CommandRunOptions`, `PoolOptions`, `TaskResult`, `ITraceWriter`, `TraceEvent`, `TraceEventPayload`).\n\n**[pool.ts](./pool.ts)** — Iterator-based worker pool executing `Array<() => Promise<T>>` tasks via shared `tasks.entries()` iterator consumed by N workers, preventing batch-stall anti-pattern where `Promise.all()` chunks idle workers waiting for slowest task. Emits `worker:start/end`, `task:pickup/done` trace events. Supports `failFast` via shared `aborted` boolean flag checked at loop start. Effective concurrency via `Math.min(options.concurrency, tasks.length)`.\n\n**[runner.ts](./runner.ts)** — `CommandRunner` orchestrates three-phase pipeline via `executeGenerate(plan)` and `executeUpdate(filesToAnalyze, projectRoot, config)`. Phase 1: concurrent file analysis via `runPool()` with `options.concurrency`, reads source via `readFile()`, caches in `sourceContentCache`, calls `aiService.call()`, computes `contentHash` via `computeContentHashFromString()`, strips preamble via `stripPreamble()`, extracts purpose via `extractPurpose()`, writes via `writeSumFile()`, detects `## Annex References` marker for `writeAnnexFile()`. Post-Phase 1: groups files by directory, runs quality checks concurrently (concurrency=10) via `checkCodeVsDoc(source, oldSum)` (stale), `checkCodeVsDoc(source, newSum)` (fresh), `checkCodeVsCode(filesForCodeVsCode)`, builds `InconsistencyReport`, clears `sourceContentCache`. Phase 2: groups `directoryTasks` by depth, processes in descending order (deepest first) with concurrency `Math.min(options.concurrency, dirsAtDepth.length)`, builds prompts via `buildDirectoryPrompt()`, writes via `writeAgentsMd()`. Post-Phase 2: validates phantom paths via `checkPhantomPaths(agentsMdPath, content, projectRoot)`. Phase 3: sequential root document synthesis (concurrency=1), strips conversational preamble before `writeFile()`. Returns `RunSummary` with counts from `aiService.getSummary()` plus `inconsistenciesCodeVsDoc`, `inconsistenciesCodeVsCode`, `phantomPaths`.\n\n### Progress Tracking\n\n**[progress.ts](./progress.ts)** — `ProgressReporter` streams colored console output with ETA calculation via moving average of last 10 `completionTimes` (window size 10), displays after 2+ completions. `ProgressLog` mirrors plain-text output to `.agents-reverse-engineer/progress.log` via promise-chain serialized writes with ANSI stripping (`/\\x1b\\[[0-9;]*m/g`). Static factory `ProgressLog.create(projectRoot)` constructs path. Methods: `onFileStart(filePath)` (`[X/Y] ANALYZING path`), `onFileDone(filePath, durationMs, tokensIn, tokensOut, model, cacheReadTokens?, cacheCreationTokens?)` (`[X/Y] DONE path Xs in/out tok model ~Ns remaining`), `onFileError(filePath, error)` (`[X/Y] FAIL path error`), `onDirectoryStart(dirPath)`, `onDirectoryDone()`, `onRootDone(docPath)`, `printSummary(summary)`. ETA formatted via `formatETA()` computing `avg * remaining` as `~Ns remaining` (<60s) or `~Mm Ss remaining` (≥60s).\n\n**[plan-tracker.ts](./plan-tracker.ts)** — `PlanTracker` maintains in-memory `GENERATION-PLAN.md` content, serializes concurrent checkbox updates via promise-chain writes. Constructor accepts `projectRoot` and `initialMarkdown`, computes `planPath` as `path.join(projectRoot, CONFIG_DIR, 'GENERATION-PLAN.md')`. `markDone(itemPath)` replaces `` `- [ ] \\`${itemPath}\\`` `` → `` `- [x] \\`${itemPath}\\`` ``, chains write to `writeQueue` promise. `initialize()` creates parent directory via `mkdir(..., {recursive: true})`, writes initial content. `flush()` awaits `writeQueue` completion.\n\n### Trace Infrastructure\n\n**[trace.ts](./trace.ts)** — `createTraceWriter(projectRoot, enabled)` factory returns `NullTraceWriter` (zero overhead) when disabled or `TraceWriter` appending NDJSON to `.agents-reverse-engineer/traces/trace-{timestamp}.ndjson` with promise-chain serialization. `TraceWriter` auto-populates `seq` (monotonic), `ts` (ISO 8601), `pid` (`process.pid`), `elapsedMs` (high-resolution `process.hrtime.bigint()` delta) on `emit(partial: TraceEventPayload)`. `finalize()` awaits `writeQueue`, closes `fd`. `cleanupOldTraces(projectRoot, keepCount=500)` removes old traces, keeps most recent 500.\n\n**Event types:** `phase:start/end` (phase, taskCount, concurrency, durationMs, tasksCompleted, tasksFailed), `worker:start/end` (workerId, phase, tasksExecuted), `task:pickup/done` (workerId, taskIndex, taskLabel, activeTasks, durationMs, success, error?), `task:start` (taskLabel, phase), `subprocess:spawn/exit` (childPid, command, taskLabel, exitCode, signal, durationMs, timedOut), `retry` (attempt, taskLabel, errorCode), `discovery:start/end` (targetPath, filesIncluded, filesExcluded, durationMs), `filter:applied` (filterName, filesMatched, filesRejected), `plan:created` (planType, fileCount, taskCount), `config:loaded` (configPath, model, concurrency).\n\n**[types.ts](./types.ts)** — Defines `FileTaskResult` (path, success, tokensIn, tokensOut, cacheReadTokens, cacheCreationTokens, durationMs, model, error?), `RunSummary` (version, filesProcessed, filesFailed, filesSkipped, totalCalls, totalInputTokens, totalOutputTokens, totalCacheReadTokens, totalCacheCreationTokens, totalDurationMs, errorCount, retryCount, totalFilesRead, uniqueFilesRead, inconsistenciesCodeVsDoc?, inconsistenciesCodeVsCode?, phantomPaths?, inconsistencyReport?), `ProgressEvent` (discriminated type: start|done|error|dir-done|root-done with filePath, index, total, durationMs?, tokensIn?, tokensOut?, model?, error?), `CommandRunOptions` (concurrency, failFast?, debug?, dryRun?, tracer?: ITraceWriter, progressLog?: ProgressLog), `PoolOptions` (concurrency, failFast?, tracer?, phaseLabel?, taskLabels?), `TaskResult<T>` (index, success, value?, error?).\n\n## Architecture\n\n### Shared-Iterator Concurrency\n\n`runPool()` creates `tasks.entries()` iterator shared across N workers via `for (const [index, task] of iterator)` loop. Workers race to pull tasks atomically via iterator protocol, preventing over-allocation where `Promise.all()` chunks spawn tasks eagerly. Effective concurrency via `Math.min(options.concurrency, tasks.length)` prevents idle workers when task count < pool size.\n\n### Promise-Chain Serialization\n\n`PlanTracker`, `ProgressLog`, `TraceWriter` serialize concurrent writes via `this.writeQueue = this.writeQueue.then(() => writeOp())` pattern. Each write operation chains onto tail of promise, forming sequential execution queue despite concurrent worker invocations. Errors caught via `.catch()` suppression (non-critical telemetry).\n\n### Three-Phase Pipeline\n\nPhase 1: concurrent file analysis via `runPool(fileTasks, {concurrency: options.concurrency})`, reads source via `readFile()` once, caches in `sourceContentCache` Map, calls `aiService.call()`, writes `.sum` via `writeSumFile()`, writes `.annex.md` if `## Annex References` detected. Post-Phase 1: groups files by directory, validates via `checkCodeVsDoc(source, oldSum)` (stale docs), `checkCodeVsDoc(source, newSum)` (LLM omissions), `checkCodeVsCode(files)` (duplicate exports) at concurrency=10, clears `sourceContentCache`.\n\nPhase 2: groups `directoryTasks` by depth (`task.metadata.depth`), processes depth levels sequentially in descending order (deepest first) via `Array.from(dirsByDepth.keys()).sort((a,b) => b-a)`, executes depth level concurrently with `Math.min(concurrency, dirsAtDepth.length)`, builds prompts via `buildDirectoryPrompt()`, writes via `writeAgentsMd()`. Post-Phase 2: validates phantom paths via `checkPhantomPaths()` extracting path-like strings via three regex patterns, resolving against directory and project root.\n\nPhase 3: sequential root synthesis (concurrency=1), builds prompts via `buildRootPrompt()`, calls `aiService.call()` with `maxTurns: 1`, strips conversational preamble before `writeFile()`.\n\n### Quality Validation Timing\n\nPre-Phase 1: reads existing `.sum` files concurrently (concurrency=20) into `oldSumCache` Map. Post-Phase 1: compares `oldSum` vs. source (stale documentation from previous runs), compares `newSum` vs. source (LLM omissions in current run), aggregates duplicate exports. Post-Phase 2: validates AGENTS.md path references. Validation failures logged to stderr with `[quality]` prefix, never throw (non-blocking).\n\n### Trace Event Threading\n\n`CommandRunOptions.tracer` threaded through pool → AIService → runner. Phase-level `phase:start/end` emitted by runner. Pool-level `worker:start/end`, `task:pickup/done` emitted by `runPool()`. Subprocess-level `subprocess:spawn/exit`, `retry` emitted by `AIService`. Zero overhead when `tracer` is `NullTraceWriter`.\n\n### ETA Calculation\n\n`ProgressReporter` maintains sliding windows `completionTimes` and `dirCompletionTimes` (max size 10). `formatETA()` computes `avg = sum(completionTimes) / completionTimes.length`, `remaining = totalFiles - completed - failed`, `etaMs = avg * remaining`. Displays after 2+ completions to avoid inaccurate early estimates. Formats as `~Ns remaining` (<60s) or `~Mm Ss remaining` (≥60s).\n\n## Behavioral Contracts\n\n### Preamble Detection Patterns (from runner.ts)\n\nFull patterns preserved in [runner.ts.annex.md](./runner.ts.annex.md).\n\n**Separator pattern:** Detects `\\n---\\n` within first 500 chars.\n\n**Bold header pattern:** Matches `/^[\\s\\S]{0,500}?(\\*\\*[A-Z])/`, strips preceding content if <300 chars, no `##` markers.\n\n**Preamble prefixes (case-insensitive):** `'now i'`, `'perfect'`, `'based on'`, `'let me'`, `'here is'`, `'i\\'ll'`, `'i will'`, `'great'`, `'okay'`, `'sure'`, `'certainly'`, `'alright'`.\n\n### Checkbox Update Pattern (from plan-tracker.ts)\n\nReplaces `` `- [ ] \\`${itemPath}\\`` `` → `` `- [x] \\`${itemPath}\\`` `` via string substitution. Item path conventions: files use relative paths (`src/cli/init.ts`), directories append `/AGENTS.md` suffix (`src/cli/AGENTS.md`), root documents use filename only (`CLAUDE.md`).\n\n### ANSI Stripping Regex (from progress.ts)\n\n`/\\x1b\\[[0-9;]*m/g` matches all SGR, cursor, and erase sequences before writing to `.agents-reverse-engineer/progress.log`.\n\n### Trace Filename Format (from trace.ts)\n\n`trace-{timestamp}.ndjson` where timestamp from `new Date().toISOString().replace(/[:.]/g, '-')` (e.g., `trace-2026-02-09T12-34-56-789Z.ndjson`).\n\n## File Relationships\n\n`CommandRunner` orchestrates via dependencies: `runPool()` for worker execution, `PlanTracker` for checkbox updates, `ProgressReporter` for console output, `ProgressLog` for file mirroring, `TraceWriter` for NDJSON events, `AIService` for subprocess calls, `buildFilePrompt/buildDirectoryPrompt/buildRootPrompt` from generation module, `writeSumFile/readSumFile/writeAnnexFile/writeAgentsMd` from writers, `checkCodeVsDoc/checkCodeVsCode/checkPhantomPaths` from quality module, `computeContentHashFromString` from change-detection.\n\n`runPool()` invokes task factories, emits trace events via `tracer?.emit()`, invokes `onComplete` callback per task settlement, returns `TaskResult[]` array.\n\n`ProgressReporter` receives events via `onFileStart/Done/Error/DirectoryStart/Done/RootDone`, computes ETA via sliding windows, delegates to `ProgressLog.write()` for file mirroring, prints summary via `printSummary(RunSummary)`.\n\n`PlanTracker` initialized with `formatExecutionPlanAsMarkdown()` output, updated via `markDone(itemPath)` from pool workers, flushed via `flush()` before orchestrator return.\n\n`TraceWriter` emits events via `emit(partial)`, auto-populates base fields (`seq`, `ts`, `pid`, `elapsedMs`), serializes writes via promise chain, finalized via `finalize()` after all phases complete.\n### src/output/AGENTS.md\n\n<!-- Generated by agents-reverse-engineer -->\n\n# src/output\n\nTerminal output formatting layer providing color-aware Logger interface for CLI messages with optional ANSI styling via picocolors.\n\n## Contents\n\n**[logger.ts](./logger.ts)** — Exports `createLogger(options)` factory constructing color-aware Logger instances writing to console.log/warn/error with picocolors-based ANSI formatting, `createSilentLogger()` returning no-op Logger for testing, `Logger` interface defining six output methods (info/file/excluded/summary/warn/error), and `LoggerOptions` schema controlling color enablement.\n\n## API Surface\n\n**Logger interface:**\n- `info(message: string): void` — informational messages (plain text)\n- `file(path: string): void` — discovered file paths (`\"  +\" + path` in green)\n- `excluded(path: string, reason: string, filter: string): void` — excluded files (`\"  -\" + path + \" (reason: filter)\"` dimmed)\n- `summary(included: number, excluded: number): void` — discovery count summary (`\"Discovered N files (M excluded)\"` with bold/dim styling)\n- `warn(message: string): void` — warning messages (`\"Warning: \" + message` in yellow)\n- `error(message: string): void` — error messages (`\"Error: \" + message` in red)\n\n**Factory functions:**\n- `createLogger(options: LoggerOptions): Logger` — constructs color-aware logger respecting `options.colors` boolean flag\n- `createSilentLogger(): Logger` — returns no-op logger with all methods bound to empty function\n\n**Configuration:**\n- `LoggerOptions` — schema with single `colors: boolean` field controlling ANSI escape code emission\n\n## Output Format Specification\n\nLogger implements CONTEXT.md-defined format with mandatory prefixes and styling:\n- Discovered files: `\"  +\"` prefix green-styled followed by space and path\n- Excluded files: `\"  -\"` prefix dimmed followed by path and parenthetical `(reason: filter)`\n- Summary line: bold `\"Discovered N files\"` followed by dimmed `\" (M excluded)\"`\n- Warnings: `\"Warning: \"` yellow-styled prefix\n- Errors: `\"Error: \"` red-styled prefix\n\nWhen `options.colors === false`, all picocolors transforms replaced with identity function via `noColor` object mapping `green`/`dim`/`red`/`bold`/`yellow` to pass-through.\n\n## Integration Points\n\nUsed by:\n- `src/discovery/run.ts` — `discoverFiles()` accepts Logger for real-time file discovery progress reporting\n- `src/cli/*.ts` — Command entry points construct logger via `createLogger({ colors: config.output.colors })` from loaded configuration\n- Test suites — `createSilentLogger()` suppresses output during programmatic invocation\n\nImports:\n- `picocolors` (aliased `pc`) — ANSI color code generation library\n### src/quality/AGENTS.md\n\n<!-- Generated by agents-reverse-engineer -->\n\n# src/quality\n\nPost-generation validation system detecting code-documentation inconsistencies (code-vs-doc), duplicate symbol exports (code-vs-code), and unresolvable path references (phantom-paths) through regex-based extraction, filesystem resolution, and structured reporting.\n\n## Contents\n\n**[index.ts](./index.ts)** — Barrel module re-exporting `extractExports()`, `checkCodeVsDoc()`, `checkCodeVsCode()`, `checkPhantomPaths()`, `buildInconsistencyReport()`, `formatReportForCli()`, `validateFindability()` from submodules alongside all type definitions (`Inconsistency`, `InconsistencyReport`, `FindabilityResult`, `CodeDocInconsistency`, `CodeCodeInconsistency`, `PhantomPathInconsistency`).\n\n**[types.ts](./types.ts)** — Defines discriminated union `Inconsistency = CodeDocInconsistency | CodeCodeInconsistency | PhantomPathInconsistency` with `type` discriminator, `InconsistencySeverity` literal union (`'info' | 'warning' | 'error'`), and `InconsistencyReport` structure containing `metadata` (timestamp, projectRoot, filesChecked, durationMs), `issues[]`, and `summary` counts by type/severity.\n\n## Validation Modules\n\n### Code-vs-Doc Consistency\n\n`inconsistency/code-vs-doc.ts` exports `extractExports()` applying regex `/^[ \\t]*export\\s+(?:default\\s+)?(?:function|class|const|let|var|type|interface|enum)\\s+(\\w+)/gm` to extract identifier names, and `checkCodeVsDoc()` performing substring search in `.sum` summary text to detect undocumented exports. Returns `CodeDocInconsistency` with `missingFromDoc[]` array when symbols absent from documentation.\n\n### Code-vs-Code Duplication\n\n`inconsistency/code-vs-code.ts` exports `checkCodeVsCode()` aggregating exports across per-directory file groups into `Map<symbol, paths[]>`, returning `CodeCodeInconsistency[]` for symbols appearing in multiple files with `pattern: 'duplicate-export'`. Caller must scope input to directory boundaries to prevent false positives from legitimate cross-directory symbol reuse.\n\n### Phantom Path Resolution\n\n`phantom-paths/validator.ts` extracts path strings via three regex patterns (markdown links, backtick-quoted paths, prose-embedded keywords), resolves against `AGENTS.md` directory and project root with `.ts`/`.js` extension fallbacks, validates via `existsSync()`. Returns `PhantomPathInconsistency[]` with `severity: 'warning'` for unresolved references after filtering exclusions (`node_modules`, URLs, template literals, globs).\n\n### Report Generation\n\n`inconsistency/reporter.ts` exports `buildInconsistencyReport()` aggregating issues with summary computation (total/per-type/per-severity counts) and `formatReportForCli()` rendering plain text output with severity tags `[ERROR]`, `[WARN]`, `[INFO]` and type-discriminated path formatting.\n\n### Density Validation (Disabled)\n\n`density/validator.ts` exports stub `validateFindability()` returning empty `FindabilityResult[]` array. Originally verified exported symbols from `.sum` files appeared in parent `AGENTS.md` via substring matching; disabled after `SumFileContent.metadata.publicInterface` removal pending structured extraction re-implementation.\n\n## Integration Points\n\nConsumed by `src/generation/orchestrator.ts` post-generation validation phase and `src/cli/generate.ts` for terminal reporting. All validators operate on generated artifacts (`.sum`, `AGENTS.md`) rather than source code, enforcing post-generation validation contract. Report format matches telemetry schema in `src/ai/telemetry/run-log.ts` for `qualityMetrics` aggregation.\n\n## Behavioral Contracts\n\n**Export extraction pattern:**\n```regex\n/^[ \\t]*export\\s+(?:default\\s+)?(?:function|class|const|let|var|type|interface|enum)\\s+(\\w+)/gm\n```\n\n**Phantom path patterns:**\n- Markdown links: `/\\[(?:[^\\]]*)\\]\\((\\.[^)]+)\\)/g`\n- Backtick paths: `` /`((?:src\\/|\\.\\.?\\/)[^`]+\\.[a-z]{1,4})`/g ``\n- Prose-embedded: `/(?:from|in|by|via|see)\\s+`?(src\\/[\\w\\-./]+)`?/gi`\n\n**CLI report format:**\n```\n=== Inconsistency Report ===\nChecked 42 files in 150ms\nFound 3 issue(s)\n\n[ERROR] Documentation out of sync: 2 exports undocumented\n  File: src/foo/bar.ts\n\n[WARN] Symbol \"Config\" exported from 2 files\n  Files: src/config/a.ts, src/config/b.ts\n\n[WARN] Phantom path reference: \"src/missing.ts\" does not exist\n  AGENTS.md: src/quality/AGENTS.md\n  Referenced: src/missing.ts\n```\n\n## Known Limitations\n\nRegex-based `extractExports()` misses destructured exports, namespace exports, dynamic exports, and re-exports with renaming. Substring matching in code-vs-doc yields false negatives for prose mentions unrelated to API surface. Code-vs-code operates on symbol names only without AST analysis to distinguish intentional duplication (interface/implementation pairs, test fixtures) from architectural violations. Phantom path resolution uses heuristic extension fallbacks (`.js` → `.ts`) without TypeScript compiler path mapping awareness.\n### src/quality/density/AGENTS.md\n\n<!-- Generated by agents-reverse-engineer -->\n\n# src/quality/density\n\nStub validation module for findability analysis (symbol presence in aggregated AGENTS.md), disabled after `SumFileContent.metadata.publicInterface` removal pending structured extraction re-implementation.\n\n## Contents\n\n**[validator.ts](./validator.ts)** — Exports `validateFindability(_agentsMdContent, _sumFiles)` returning empty `FindabilityResult[]` array; previously verified exported symbols from `.sum` files appeared in parent `AGENTS.md` via string matching, now disabled awaiting post-processing extraction support.\n\n## Implementation Status\n\n`validateFindability()` signature preserved for future re-implementation but returns `[]` unconditionally. Original logic performed heuristic substring search to detect whether key symbols from source file summaries appeared in aggregated directory documentation. Disabled when `SumFileContent` schema removed structured `publicInterface` field (see `src/generation/writers/sum.ts`). Module retained to support future LLM-based or AST-based symbol extraction passes.\n\n## Type Surface\n\n`FindabilityResult` interface with fields:\n- `filePath: string` — validated `.sum` file path\n- `symbolsTested: string[]` — symbols checked for presence\n- `symbolsFound: string[]` — symbols located in AGENTS.md\n- `symbolsMissing: string[]` — symbols absent from AGENTS.md\n- `score: number` — ratio `symbolsFound.length / symbolsTested.length` (0-1 range)\n\n## Dependencies\n\nImports `SumFileContent` type from `../../generation/writers/sum.js` for parameter type annotation. No runtime dependencies since function body returns empty array.\n### src/quality/inconsistency/AGENTS.md\n\n<!-- Generated by agents-reverse-engineer -->\n\n# src/quality/inconsistency\n\nDetects and reports three categories of code-documentation discrepancies: exported symbols missing from `.sum` summaries (code-vs-doc), duplicate exports across files (code-vs-code), and unresolvable path references in `AGENTS.md` (phantom-paths).\n\n## Contents\n\n### Validators\n\n**[code-vs-doc.ts](./code-vs-doc.ts)** — Exports `extractExports()` (regex-based identifier extraction via `/^[ \\t]*export\\s+(?:default\\s+)?(?:function|class|const|let|var|type|interface|enum)\\s+(\\w+)/gm`) and `checkCodeVsDoc()` (compares source exports against `SumFileContent.summary` via substring matching, returns `CodeDocInconsistency` with `missingFromDoc` array or `null`).\n\n**[code-vs-code.ts](./code-vs-code.ts)** — Exports `checkCodeVsCode()` aggregating exports across scoped file groups into `Map<symbol, paths[]>`, returns `CodeCodeInconsistency[]` for symbols appearing in multiple files with `pattern: 'duplicate-export'`.\n\n**[reporter.ts](./reporter.ts)** — Exports `buildInconsistencyReport()` (aggregates `Inconsistency[]` into `InconsistencyReport` with per-type/per-severity summary counts) and `formatReportForCli()` (renders plain text output with severity tags `[ERROR]`, `[WARN]`, `[INFO]` and type-discriminated path formatting).\n\n## Validation Strategy\n\n**Code-vs-Doc:** Regex extraction from source followed by case-sensitive substring search in `.sum` summary text. Does not use AST analysis, yielding false negatives for destructured/namespace/dynamic exports and prose mentions unrelated to API surface.\n\n**Code-vs-Code:** Symbol-name deduplication across per-directory file groups. Caller must scope input to prevent false positives from legitimate cross-directory symbol reuse (e.g., multiple `index.ts` files exporting `Config`).\n\n**Phantom Paths:** Not implemented in this directory. Handled by `../phantom-paths/validator.ts` which extracts markdown links, backtick paths, and prose-embedded paths from `AGENTS.md`, resolves via `existsSync()` with `.ts`/`.js` fallback.\n\n## File Relationships\n\n`code-vs-code.ts` imports `extractExports()` from `code-vs-doc.ts` for symbol extraction reuse. `reporter.ts` consumes discriminated union `Inconsistency` (from `../types.ts`) unifying `CodeDocInconsistency`, `CodeCodeInconsistency`, and `PhantomPathInconsistency`. Orchestrator in `../index.ts` invokes validators and passes results to `buildInconsistencyReport()` → `formatReportForCli()` pipeline.\n\n## Behavioral Contracts\n\n**Export extraction pattern:** `/^[ \\t]*export\\s+(?:default\\s+)?(?:function|class|const|let|var|type|interface|enum)\\s+(\\w+)/gm` matches line-start whitespace, `export` keyword, optional `default`, declaration keyword (`function`/`class`/`const`/`let`/`var`/`type`/`interface`/`enum`), captures identifier `(\\w+)`.\n\n**CLI output format:**\n```\n=== Inconsistency Report ===\nChecked 42 files in 150ms\nFound 3 issue(s)\n\n[ERROR] Documentation out of sync: 2 exports undocumented\n  File: src/foo/bar.ts\n\n[WARN] Symbol \"Config\" exported from 2 files\n  Files: src/config/a.ts, src/config/b.ts\n```\n\nHuman-readable severity tags followed by description and type-specific paths (code-vs-doc shows `filePath`, code-vs-code shows `files.join(', ')`, phantom-path shows `agentsMdPath` and `details.referencedPath`).\n\n## Known Limitations\n\nRegex-based extraction misses destructured exports (`export const { foo } = obj`), namespace exports (`export * as Utils from './utils'`), dynamic exports (`export { [computedName]: value }`), re-exports with renaming (`export { foo as bar } from './mod'`). No AST analysis to distinguish intentional duplication (interface/implementation pairs, test fixtures) from architectural violations. Substring matching yields false negatives when prose mentions symbols in non-API contexts.\n### src/quality/phantom-paths/AGENTS.md\n\n<!-- Generated by agents-reverse-engineer -->\n\n# src/quality/phantom-paths\n\nValidates path references in `AGENTS.md` files by extracting path-like strings via regex patterns, resolving them against filesystem locations with TypeScript/JavaScript extension fallbacks, and reporting unresolved references as `PhantomPathInconsistency` objects in quality reports.\n\n## Contents\n\n**[index.ts](./index.ts)** — Barrel export of `checkPhantomPaths` validator function.\n\n**[validator.ts](./validator.ts)** — Extracts path references from `AGENTS.md` content using `PATH_PATTERNS` (markdown links, backtick-quoted paths, prose-embedded paths), resolves each path against `agentsMdDir` and `projectRoot` with `.ts`/`.js` extension fallback via `tryPaths[]`, filters excluded patterns (URLs, template literals, globs), and returns `PhantomPathInconsistency[]` array with deduplication via `seen` Set.\n\n## Path Extraction Patterns\n\n`PATH_PATTERNS` captures three reference types:\n- `/\\[(?:[^\\]]*)\\]\\((\\.[^)]+)\\)/g` — Markdown link targets\n- `` /`((?:src\\/|\\.\\.?\\/)[^`]+\\.[a-z]{1,4})`/g `` — Backtick-quoted paths with extensions\n- `/(?:from|in|by|via|see)\\s+`?(src\\/[\\w\\-./]+)`?/gi` — Prose-embedded paths following contextual keywords\n\n## Resolution Strategy\n\nFor each extracted path, `checkPhantomPaths()` attempts resolution in priority order:\n\n1. Relative to `AGENTS.md` directory: `path.resolve(agentsMdDir, rawPath)`\n2. Relative to project root: `path.resolve(projectRoot, rawPath)`\n3. TypeScript convention fallback: append `.replace(/\\.js$/, '.ts')` variants to `tryPaths[]` when `rawPath.endsWith('.js')`\n\nValidates existence via `existsSync()` on all candidate paths; returns `PhantomPathInconsistency` with `severity: 'warning'` when all attempts fail.\n\n## Exclusion Patterns\n\n`SKIP_PATTERNS` filters non-file references before validation:\n- `/node_modules/`, `/\\.git\\//` — Dependency/VCS paths\n- `/^https?:/` — URLs\n- `/\\{\\{/`, `/\\$\\{/` — Template placeholders/literals\n- `/\\*/`, `/\\{[^}]*,[^}]*\\}/` — Glob patterns/brace expansions\n\n## Inconsistency Reporting\n\nEach phantom path generates `PhantomPathInconsistency` containing:\n- `agentsMdPath` — Normalized via `path.relative(projectRoot, agentsMdPath)`\n- `description` — `\"Phantom path reference: \\\"${rawPath}\\\" does not exist\"`\n- `details.referencedPath` — Original extracted path\n- `details.resolvedTo` — Attempted resolution relative to project root\n- `details.context` — Containing line truncated to 120 characters\n### src/specify/AGENTS.md\n\n<!-- Generated by agents-reverse-engineer -->\n\n# src/specify/\n\nProject specification synthesis from AGENTS.md documentation corpus via AI-driven prompt engineering. Exports `buildSpecPrompt()` for constructing two-part prompts (system constraints + aggregated AGENTS.md content), `writeSpec()` for filesystem output with overwrite protection and multi-file splitting, and `SpecExistsError` for conflict detection.\n\n## Contents\n\n**[index.ts](./index.ts)** — Barrel re-export aggregating `buildSpecPrompt`, `SpecPrompt`, `writeSpec`, `SpecExistsError`, `WriteSpecOptions` from `prompts.ts` and `writer.ts` submodules. Consumed by `src/cli/specify.ts` command orchestrator.\n\n**[prompts.ts](./prompts.ts)** — Prompt template factory exporting `buildSpecPrompt(docs, annexFiles?)` which constructs `SpecPrompt` pairs: system instructions (`SPEC_SYSTEM_PROMPT` enforcing 11-section conceptual organization) and user content (concatenated AGENTS.md markdown with section delimiters, optional annex files for reproduction-critical constants). Returns structured prompt with mandatory output format rules (raw markdown, no preamble, verbatim reproduction of prompt templates/regex/IDE templates).\n\n**[writer.ts](./writer.ts)** — Filesystem writer exporting `writeSpec(content, options)` supporting single-file (`specs/SPEC.md`) and multi-file (`specs/<slug>.md`) output modes. Implements pre-flight existence checks throwing `SpecExistsError(conflicts[])` when `force=false`, content splitting via `splitByHeadings()` (regex `/^(?=# )/m`), filename sanitization through `slugify()` (lowercase + hyphenation + alphanumeric filtering), and directory creation with `mkdir(outputDir, { recursive: true })`.\n\n## Architecture\n\n**Prompt Construction Pipeline:**  \n`buildSpecPrompt()` receives `AgentsDocs[]` from `../generation/collector.js` (`collectAgentsDocs()` recursive traversal) and optional annex files (`collectAnnexFiles()`). Constructs user prompt by iterating docs array, emitting `### ${doc.relativePath}\\n\\n${doc.content}` sections, appending annex section if provided, injecting output requirements listing 11 mandatory sections. System prompt (`SPEC_SYSTEM_PROMPT`) prohibits folder-mirroring, exact file path prescription, mandates verbatim reproduction of behavioral contracts (regex patterns, format strings, magic constants, environment variables, prompt templates, IDE templates).\n\n**Filesystem Writing Pipeline:**  \n`writeSpec()` branches on `options.multiFile`. Single-file mode: checks `fileExists(outputPath)`, throws `SpecExistsError([outputPath])` if exists and `force=false`, creates parent directory, writes content. Multi-file mode: calls `splitByHeadings()` to partition by top-level `# ` headings, checks all target paths for conflicts, throws `SpecExistsError(conflicts)` if any exist and `force=false`, writes each section to slugified filename.\n\n## Behavioral Contracts\n\n**SPEC_SYSTEM_PROMPT Section 7 (Behavioral Contracts):**  \nEnforces two subsections: (a) Runtime Behavior — error types/codes, retry formulas/delays, concurrency model, lifecycle hooks; (b) Implementation Contracts — verbatim regex patterns in backticks, format strings with examples, magic constants with meanings, environment variable names with expected values, file format specifications (YAML schemas, NDJSON structures). Prohibits paraphrasing regex into prose.\n\n**SPEC_SYSTEM_PROMPT Section 10 (Prompt Templates):**  \nRequires FULL verbatim text reproduction from annex files or AGENTS.md content, organized by pipeline phase or functional area, preserving placeholder syntax (e.g., `{{FILE_PATH}}`).\n\n**SPEC_SYSTEM_PROMPT Section 11 (IDE Integration):**  \nRequires verbatim reproduction of command templates per platform, platform configuration objects (path prefixes, filename conventions, frontmatter formats), installer permission lists, hook definitions.\n\n**Content Splitting Regex:**  \n`/^(?=# )/m` matches lines starting with exactly `# ` (top-level headings). Content before first heading assigned to `'00-preamble.md'`.\n\n**Filename Sanitization Transform:**  \n`slugify()` applies sequence: lowercase → whitespace (`/\\s+/g`) to hyphen → strip non-alphanumeric except hyphens (`/[^a-z0-9-]/g`) → collapse hyphens (`/-+/g`) → trim edge hyphens (`/^-|-$/g`).\n\n## Integration Points\n\n**Upstream:** `src/cli/specify.ts` orchestrates workflow: validates project root, collects AGENTS.md via `collectAgentsDocs()`, optionally collects annex files via `collectAnnexFiles()`, calls `buildSpecPrompt(docs, annexFiles)`, invokes `AIService.call()` with returned `SpecPrompt`, passes AI output to `writeSpec()` with options from CLI flags (`--force`, `--multi-file`, `--output`).\n\n**Downstream:** `writeSpec()` consumes `node:fs/promises` (`writeFile`, `mkdir`, `access`), `node:fs` (`constants.F_OK`), `node:path` (`dirname`, `join`). Throws `SpecExistsError` caught by CLI for user-facing error messages.\n### src/types/AGENTS.md\n\n<!-- Generated by agents-reverse-engineer -->\n\n# src/types\n\nShared type definitions for file discovery results and statistics consumed across discovery, orchestration, and CLI modules.\n\n## Contents\n\n### [index.ts](./index.ts)\nExports `DiscoveryResult`, `DiscoveryStats`, `ExcludedFile` interfaces defining discovery phase output schema, exclusion metadata, and aggregate metrics.\n\n## Exported Interfaces\n\n### `ExcludedFile`\nRepresents files filtered during discovery with `path: string` and `reason: string` (exclusion rationale: \"gitignore pattern\", \"binary file\", \"vendor directory\").\n\n### `DiscoveryResult`\nAggregates discovery output with `files: string[]` (paths for Phase 1 analysis) and `excluded: ExcludedFile[]` (filtered files with metadata).\n\n### `DiscoveryStats`\nProvides discovery metrics: `totalFiles`, `includedFiles`, `excludedFiles` (counts), `exclusionReasons: Record<string, number>` (histogram of `ExcludedFile.reason` values).\n\n## Data Flow\n\n**Producer:** `runDiscovery()` in `src/discovery/run.ts` returns `DiscoveryResult` after filter chain execution (`src/discovery/walker.ts` + filters: `gitignore.ts`, `binary.ts`, `vendor.ts`, `custom.ts`).\n\n**Consumers:**\n- `src/cli/discover.ts` — Generates `GENERATION-PLAN.md` from `DiscoveryResult.files`\n- `src/cli/generate.ts` — Feeds `DiscoveryResult.files` to Phase 1 worker pool\n- `src/cli/update.ts` — Inputs `DiscoveryResult` to `detectChanges()` for delta computation\n- `src/output/logger.ts` — Computes `DiscoveryStats` from `DiscoveryResult` for terminal output\n\n**Exclusion Metadata:** `ExcludedFile.reason` populated by filter modules:\n- `src/discovery/filters/gitignore.ts` — \"matched .gitignore pattern: `<pattern>`\"\n- `src/discovery/filters/binary.ts` — \"binary file\"\n- `src/discovery/filters/vendor.ts` — \"vendor directory\"\n- `src/discovery/filters/custom.ts` — \"matched exclude pattern: `<pattern>`\"\n### src/update/AGENTS.md\n\n<!-- Generated by agents-reverse-engineer -->\n\n# src/update\n\nOrchestrates incremental documentation updates via SHA-256 content hash comparison against `.sum` YAML frontmatter, deleting orphaned artifacts and computing affected directory sets without requiring git diff parsing.\n\n## Contents\n\n**[index.ts](./index.ts)** — Barrel module exporting `UpdateOrchestrator` class, factory `createUpdateOrchestrator()`, cleanup utilities `cleanupOrphans()`, `cleanupEmptyDirectoryDocs()`, `getAffectedDirectories()`, and interfaces `UpdatePlan`, `UpdateOptions`, `UpdateResult`, `UpdateProgress`, `CleanupResult`.\n\n**[orchestrator.ts](./orchestrator.ts)** — `UpdateOrchestrator` class coordinates change detection by reading stored `content_hash` from `.sum` frontmatter via `readSumFile()`, comparing against `computeContentHash()` output, returning `UpdatePlan` with `filesToAnalyze` (hash mismatch), `filesToSkip` (match), `cleanup` (orphans), `affectedDirs` (sorted depth descending). Methods: `preparePlan()`, `checkPrerequisites()`, `isFirstRun()`. Emits `plan:created` trace events.\n\n**[orphan-cleaner.ts](./orphan-cleaner.ts)** — `cleanupOrphans()` deletes stale `.sum`/`.annex.md` files for deleted/renamed sources via `deleteIfExists()`, `cleanupEmptyDirectoryDocs()` removes `AGENTS.md` from directories with zero source files (excludes `GENERATED_FILES` set and dotfiles), `getAffectedDirectories()` walks parent directories via `path.dirname()` until `.` or absolute path, returning `Set<string>` of directories requiring regeneration.\n\n**[types.ts](./types.ts)** — Interfaces: `CleanupResult` (`deletedSumFiles`, `deletedAgentsMd` arrays), `UpdateOptions` (`includeUncommitted`, `dryRun` flags), `UpdateResult` (`analyzedFiles`, `skippedFiles`, `cleanup`, `regeneratedDirs`, `baseCommit`, `currentCommit`, `dryRun`), `UpdateProgress` callbacks (`onFileStart`, `onFileDone`, `onCleanup`, `onDirRegenerate`).\n\n## Update Workflow\n\n1. **Change Detection:** `preparePlan()` discovers files via `runDiscovery()` from `src/discovery/`, reads `.sum` frontmatter via `readSumFile()` from `src/generation/writers/sum.js`, compares `content_hash` against `computeContentHash()` from `src/change-detection/`, populates `filesToAnalyze` (added/modified) and `filesToSkip` (unchanged).\n\n2. **Orphan Cleanup:** `cleanupOrphans()` processes `FileChange[]` with `status: 'deleted' | 'renamed'`, extracts `path` (deleted) or `oldPath` (renamed), constructs `.sum` and `.annex.md` paths, deletes via `unlink()` unless `dryRun: true`.\n\n3. **Affected Directories:** `getAffectedDirectories()` walks parent chains for all changed files, returns `Set<string>` sorted by `path.sep` depth descending (deepest first), ensuring `AGENTS.md` regeneration propagates upward.\n\n4. **Empty Directory Cleanup:** `cleanupEmptyDirectoryDocs()` filters directory entries excluding dotfiles, `.sum`/`.annex.md` extensions, `GENERATED_FILES` set (`['AGENTS.md', 'CLAUDE.md']`), deletes `AGENTS.md` if zero source files remain.\n\n5. **Execution:** `src/cli/update.ts` invokes `preparePlan()`, executes Phase 1 pool for `filesToAnalyze` via `src/generation/executor.ts`, regenerates `AGENTS.md` sequentially for `affectedDirs` (no post-order traversal required).\n\n## Trace Emissions\n\n`preparePlan()` emits:\n- `phase:start` with `phase: 'update-plan-creation'`, `taskCount: 0`, `concurrency: 1`\n- `plan:created` with `planType: 'update'`, `fileCount: filesToAnalyze.length`, `taskCount: filesToAnalyze.length + affectedDirs.length`\n- `phase:end` with `durationMs` computed via `process.hrtime.bigint()` delta\n\n## Behavioral Contracts\n\n**Depth-descending sort:** `affectedDirs` sorted via `path.relative().split(path.sep).length` comparator (deepest first) ensuring child directories process before parents.\n\n**Orphan path construction:** `.sum` path = `path.join(projectRoot, \\`${relativePath}.sum\\`)`, `.annex.md` path = `path.join(projectRoot, \\`${relativePath}.annex.md\\`)`.\n\n**Directory emptiness filter:** Exclude entries matching `entry.startsWith('.')` OR `entry.endsWith('.sum')` OR `entry.endsWith('.annex.md')` OR `GENERATED_FILES.has(entry)`.\n\n**Upward traversal terminator:** `getAffectedDirectories()` stops when `dir === '.'` OR `path.isAbsolute(dir) === true`.\n\n## API Compatibility No-Ops\n\nMethods retained for backward compatibility but perform no operations in frontmatter-based mode:\n- `recordFileAnalyzed()` — hash stored in `.sum` YAML frontmatter by generation phase\n- `removeFileState()` — `.sum` file cleanup handled by `cleanupOrphans()`\n- `recordRun()` — returns `0`, no run history database in frontmatter mode\n- `getLastRun()` — returns `undefined`, no run history available\n- `close()` — no resources to dispose\n\n## Package Metadata (package.json)\n\n- **Name**: agents-reverse-engineer\n- **Version**: 0.6.6\n- **Description**: CLI tool for reverse-engineering codebase documentation for AI agents\n- **Scripts**:\n  - `build`: `tsc`\n  - `build:hooks`: `node scripts/build-hooks.js`\n  - `prepack`: `rm -f LICENSE.sum README.md.sum`\n  - `prepublishOnly`: `npm run build && npm run build:hooks`\n  - `dev`: `tsx watch src/cli/index.ts`\n\n## Output Requirements\n\nThe document MUST include:\n- Project purpose and description\n- Architecture overview with directory structure\n- Key directories table\n- Getting started (install, build, run commands)\n- Key technologies and dependencies\n\nThis document is the COMPREHENSIVE reference for the entire project.\nIt should contain architecture, configuration, build instructions, and project-wide patterns.\nIndividual file details belong in directory-level AGENTS.md files — reference them, don't duplicate them.\n\nOutput ONLY the markdown content. No preamble.",
      "systemPrompt": "You generate markdown documentation files. Output ONLY the raw markdown content.\nDo NOT include any conversational text, preamble, or meta-commentary.\nDo NOT say \"Here is...\" or \"I've generated...\" — just output the document itself.\nThe output will be written directly to a file.\n\nCRITICAL CONSTRAINT:\n- Synthesize ONLY from the AGENTS.md content provided in the user prompt\n- Do NOT invent, extrapolate, or hallucinate features, hooks, APIs, patterns, or dependencies not explicitly mentioned\n- If information is missing, omit that section rather than guessing\n- Every claim must be traceable to a specific AGENTS.md file provided",
      "response": "# agents-reverse-engineer\n\n**CLI tool implementing Recursive Language Model (RLM) algorithm for AI-driven codebase documentation via three-phase pipeline: concurrent `.sum` file generation with subprocess pools, post-order `AGENTS.md` directory aggregation, and platform-specific root synthesis (`CLAUDE.md`/`GEMINI.md`/`OPENCODE.md`) with gitignore-aware discovery, SHA-256 incremental updates, and NDJSON telemetry.**\n\n## Project Overview\n\nagents-reverse-engineer (ARE) automates brownfield documentation for AI coding assistants by executing a three-phase pipeline that discovers source files, analyzes them concurrently via AI CLI subprocesses (Claude Code, Gemini, OpenCode), generates file summaries with YAML frontmatter containing SHA-256 content hashes, synthesizes directory-level documentation from bottom-up via post-order traversal, and produces root integration documents tailored to each AI platform.\n\n**Version**: 0.6.6  \n**License**: MIT (GeoloeG-IsT, 2026)  \n**Runtime**: Node.js ≥18.0.0 (ES modules)\n\n### Core Capabilities\n\n- **Parallel file analysis** with configurable concurrency pools (default 2 workers for WSL, 5 elsewhere)\n- **Incremental updates** via SHA-256 content hash comparison (skip unchanged files)\n- **Multi-platform AI backend support** (Claude Code, Gemini CLI, OpenCode) with automatic detection\n- **Gitignore-aware file discovery** with binary detection and vendor directory exclusion\n- **Quality validation** detecting code-documentation inconsistencies and phantom path references\n- **Session lifecycle hooks** for automatic documentation refresh on IDE session end\n- **NDJSON telemetry logging** with token cost tracking and run retention management\n\n## Architecture\n\n### Three-Phase Generation Pipeline\n\n**Phase 1: Concurrent File Analysis**\n\nIterator-based worker pool (`src/orchestration/pool.ts`) shares single task iterator across N workers to prevent over-allocation. Each worker invokes `AIService.call()` → `runSubprocess()` → `execFile()` spawning AI CLI subprocesses with resource limits:\n\n- `NODE_OPTIONS='--max-old-space-size=512'` limits heap to 512MB per subprocess\n- `UV_THREADPOOL_SIZE='4'` constrains libuv thread pool to 4 threads\n- `CLAUDE_CODE_DISABLE_BACKGROUND_TASKS='1'` prevents background task spawning\n- `--disallowedTools Task` blocks subagents\n\nProcess group killing (`kill(-pid)`) terminates subprocess trees on timeout. SIGTERM sent at `timeoutMs`, SIGKILL escalation after 5s grace period. Exponential backoff retry on rate limits (stderr patterns: \"rate limit\", \"429\", \"too many requests\", \"overloaded\").\n\nWrites `.sum` files with YAML frontmatter:\n\n```yaml\n---\ngenerated_at: 2026-02-09T12:34:56.789Z\ncontent_hash: a3f5d8e9... (SHA-256 hex)\npurpose: One-line purpose statement\ncritical_todos:\n  - Security issue\nrelated_files: [path1, path2]\n---\n\nMarkdown summary content...\n```\n\n**Phase 2: Post-Order Directory Aggregation**\n\nSorts directories by depth descending via `path.relative().split(path.sep).length` (deepest first). Waits for all child `.sum` files to exist via `isDirectoryComplete()` predicate before processing directory.\n\nPrompts include:\n\n- Aggregated child `.sum` content via `readSumFile()`\n- Subdirectory `AGENTS.md` files via recursive traversal\n- Import maps via `extractDirectoryImports()` with verified path constraints\n- Manifest detection (9 types: package.json, Cargo.toml, go.mod, pyproject.toml, pom.xml, build.gradle, Gemfile, composer.json, CMakeLists.txt/Makefile)\n\nUser-authored `AGENTS.md` files renamed to `AGENTS.local.md` and prepended above generated content. Writes `AGENTS.md` with `<!-- Generated by agents-reverse-engineer -->` marker.\n\n**Phase 3: Root Document Synthesis**\n\nSequential execution (concurrency=1) generates `CLAUDE.md`, `GEMINI.md`, `OPENCODE.md`. Prompts consume all `AGENTS.md` files via `collectAgentsDocs()` recursive tree traversal, parse root `package.json` for project metadata, enforce synthesis-only constraints (no invention of features/hooks/patterns not in source documents). Strips conversational preamble via pattern matching before writing output.\n\n### Directory Structure\n\n```\n.\n├── .github/workflows/    # CI/CD: npm publish workflow with provenance attestation\n├── docs/                 # Original vision document (INPUT.md) defining RLM algorithm\n├── hooks/                # Session lifecycle hooks for Claude/Gemini/OpenCode\n│   ├── are-check-update.js              # SessionStart: npm version check\n│   ├── are-session-end.js               # SessionEnd: auto-update on uncommitted changes\n│   ├── opencode-are-check-update.js     # OpenCode plugin: version check\n│   └── opencode-are-session-end.js      # OpenCode plugin: session end update\n├── scripts/              # Build automation: hook file copying (build-hooks.js)\n└── src/                  # TypeScript source tree\n    ├── ai/               # AI service orchestration with backend abstraction\n    ├── change-detection/ # Git-based delta computation and SHA-256 hashing\n    ├── cli/              # Command entry points (init, discover, generate, update, clean, specify)\n    ├── config/           # YAML config loading with Zod validation\n    ├── discovery/        # File walking with composable filters (gitignore, binary, vendor, custom)\n    ├── generation/       # Three-phase pipeline orchestration and prompt engineering\n    ├── imports/          # Static import analysis for dependency graphs\n    ├── installer/        # npx-based command/hook installation for IDE runtimes\n    ├── integration/      # Platform-specific template generation (Claude/OpenCode/Gemini)\n    ├── orchestration/    # Worker pool, progress reporting, trace emission, plan tracking\n    ├── output/           # Terminal logging with picocolors formatting\n    ├── quality/          # Code-doc consistency validation and phantom path detection\n    ├── specify/          # Project specification synthesis from AGENTS.md corpus\n    ├── types/            # Shared interfaces for discovery results\n    └── update/           # Incremental update workflow with orphan cleanup\n```\n\n## Key Directories\n\n| Directory | Purpose |\n|-----------|---------|\n| **src/ai/** | Backend-agnostic AI service layer with subprocess management, retry logic, telemetry logging, and trace emission. Contains adapters for Claude Code, Gemini CLI, OpenCode. |\n| **src/generation/** | Orchestrates three-phase pipeline: file analysis prompt construction with import maps, directory aggregation with user content preservation, root document synthesis with stack detection. |\n| **src/orchestration/** | Iterator-based worker pool with shared task queue, streaming progress reporter with ETA calculation, promise-chain serialization for concurrent writes (GENERATION-PLAN.md, progress.log, trace NDJSON). |\n| **src/discovery/** | Directory walker with filter chain: gitignore parsing via `ignore` library, binary detection (extension + content analysis), vendor directory exclusion (node_modules/.git/dist), custom glob patterns. |\n| **src/change-detection/** | Git diff parsing (`git diff --name-status -M`) with rename detection and SHA-256 content hashing for non-git workflows. Detects added/modified/deleted/renamed files, optionally merges uncommitted changes. |\n| **src/quality/** | Post-generation validators: regex-based export extraction vs. substring search in summaries (code-vs-doc), duplicate symbol detection (code-vs-code), path resolution via `existsSync()` (phantom-paths). |\n| **src/config/** | Zod schema validation for `.agents-reverse-engineer/config.yaml` with defaults for vendor directories (18 entries), exclude patterns (gitignore-style globs), binary extensions (26 types), AI backend config (concurrency/timeout/model/pricing). |\n| **src/installer/** | npx install orchestrator supporting global (`~/.claude`) and local (`.claude`) installation modes with interactive prompts, platform-specific path resolution (environment overrides: `CLAUDE_CONFIG_DIR`, `OPENCODE_CONFIG_DIR`, `GEMINI_CONFIG_DIR`), hook registration, permission setup. |\n| **hooks/** | Detached background processes for version checking (compares `npm view agents-reverse-engineer version` against `~/.claude/ARE-VERSION`) and session-end updates (spawns `npx agents-reverse-engineer@latest update --quiet` when `git status --porcelain` detects changes). |\n\n## Getting Started\n\n### Installation\n\n**Global install:**\n```bash\nnpm install -g agents-reverse-engineer\n```\n\n**Project-local install:**\n```bash\nnpm install --save-dev agents-reverse-engineer\n```\n\n**npx usage (no install required):**\n```bash\nnpx agents-reverse-engineer init\n```\n\n### Prerequisites\n\n- Node.js ≥18.0.0\n- At least one AI CLI installed:\n  - Claude Code: `npm install -g @anthropic-ai/claude-code`\n  - Gemini CLI: Installation instructions at https://gemini.google.com/cli\n  - OpenCode: Installation instructions at https://opencode.dev\n\n### Basic Workflow\n\n**1. Initialize configuration:**\n```bash\nare init\n# Creates .agents-reverse-engineer/config.yaml with defaults\n```\n\n**2. Discover files (optional preview):**\n```bash\nare discover\n# Scans files, writes GENERATION-PLAN.md with phase breakdown\n```\n\n**3. Generate documentation:**\n```bash\nare generate\n# Three-phase execution: .sum files → AGENTS.md → CLAUDE.md\n# Progress logged to .agents-reverse-engineer/progress.log\n# Monitor with: tail -f .agents-reverse-engineer/progress.log\n```\n\n**4. Incremental updates:**\n```bash\nare update\n# Hash-based change detection, regenerates only modified files\n# Use --uncommitted flag to include working tree changes\n```\n\n**5. Clean artifacts:**\n```bash\nare clean\n# Removes .sum, AGENTS.md (generated only), CLAUDE.md, GENERATION-PLAN.md\n# Restores AGENTS.local.md → AGENTS.md\n```\n\n**6. Generate project spec:**\n```bash\nare specify\n# Synthesizes all AGENTS.md into specs/SPEC.md\n# Use --multi-file for split specs (specs/<dirname>.md)\n```\n\n### CLI Options\n\n**Global flags:**\n- `--debug` — Enable verbose subprocess logging with heap/RSS metrics\n- `--trace` — Emit NDJSON trace events to `.agents-reverse-engineer/traces/`\n- `--concurrency N` — Override worker pool size (1-10, default from config)\n- `--fail-fast` — Abort on first task failure\n- `--dry-run` — Preview operations without writing files\n\n**Generate/Update:**\n- `--uncommitted` — Include working tree changes (update only)\n- `--force` — Overwrite existing files (specify only)\n\n**Trace output:**\n```bash\nare generate --trace\n# Writes trace-<timestamp>.ndjson with events:\n# phase:start/end, worker:start/end, task:pickup/done, subprocess:spawn/exit, retry\n```\n\n### IDE Integration\n\n**Install commands/hooks for Claude Code:**\n```bash\nnpx agents-reverse-engineer --runtime claude -g\n# Installs to ~/.claude/commands/ and ~/.claude/hooks/\n# Registers SessionStart hook: are-check-update.js\n# Registers SessionEnd hook: are-session-end.js\n```\n\n**Install for OpenCode:**\n```bash\nnpx agents-reverse-engineer --runtime opencode -g\n# Installs to ~/.config/opencode/commands/ and ~/.config/opencode/plugins/\n```\n\n**Install for Gemini:**\n```bash\nnpx agents-reverse-engineer --runtime gemini -g\n# Installs to ~/.gemini/commands/ with TOML format\n```\n\n**Available IDE commands:**\n- `/are-init` — Create configuration\n- `/are-discover` — Preview generation plan\n- `/are-generate` — Full documentation generation\n- `/are-update` — Incremental update\n- `/are-clean` — Remove generated artifacts\n- `/are-specify` — Synthesize project specification\n- `/are-help` — Show command list\n\n## Key Technologies\n\n**Runtime & Language:**\n- Node.js ≥18.0.0 (ES modules)\n- TypeScript 5.7.3 (ES2022 target, NodeNext module resolution, strict mode)\n\n**Core Dependencies:**\n- `fast-glob` — File discovery with glob patterns\n- `ignore` — Gitignore parsing\n- `isbinaryfile` — Binary file detection\n- `simple-git` — Change detection via git diff\n- `yaml` — Config parsing\n- `zod` — Schema validation\n- `ora` — Spinner UI\n- `picocolors` — Terminal colors\n\n**Build & Distribution:**\n- TypeScript compiler (`tsc`) emits to `dist/`\n- Binary entry points: `are` and `agents-reverse-engineer` → `dist/cli/index.js`\n- Pre-publish hooks: `build` + `build:hooks` (copies hooks/ to hooks/dist/)\n\n**AI Backends:**\n- `@anthropic-ai/claude-code` (Claude Code CLI adapter)\n- Gemini CLI (stub implementation pending JSON output stability)\n- OpenCode CLI (stub implementation pending JSONL parsing)\n\n## Configuration\n\n**Config file:** `.agents-reverse-engineer/config.yaml`\n\n**Schema sections:**\n\n```yaml\nexclude:\n  patterns:              # Gitignore-style globs (e.g., \"*.log\", \"**/*.test.ts\")\n  vendorDirs:           # Third-party directories to skip (default: node_modules, .git, dist, etc.)\n  binaryExtensions:     # Non-text file extensions (default: .png, .jpg, .zip, .pdf, etc.)\n\noptions:\n  followSymlinks: false # Follow symbolic links during discovery\n  maxFileSize: 1048576  # Binary detection threshold (1MB default)\n\noutput:\n  colors: true          # Enable ANSI color codes in CLI output\n\nai:\n  backend: 'auto'       # 'claude' | 'gemini' | 'opencode' | 'auto' (detect first available)\n  model: null           # Override backend default model\n  timeoutMs: 120000     # Subprocess timeout (2 minutes)\n  maxRetries: 3         # Exponential backoff retry attempts\n  concurrency: 2        # Worker pool size (1-10, default 2 for WSL, 5 elsewhere)\n  \n  telemetry:\n    enabled: true       # Write run logs to .agents-reverse-engineer/logs/\n    keepRuns: 50        # Retention limit for historical logs\n    costThresholdUsd: 10  # Warning threshold for cumulative costs\n  \n  pricing:              # Per-backend token cost configuration (input/output/cacheRead/cacheWrite)\n    claude: { inputCostPer1kTokens: 0.003, outputCostPer1kTokens: 0.015, ... }\n```\n\n**Environment overrides:**\n- `CLAUDE_CONFIG_DIR` — Override `~/.claude` path\n- `OPENCODE_CONFIG_DIR` — Override `~/.config/opencode` path\n- `GEMINI_CONFIG_DIR` — Override `~/.gemini` path\n- `ARE_DISABLE_HOOK` — Disable session-end auto-update (set to `1`)\n\n## Telemetry & Tracing\n\n**Run logs:** `.agents-reverse-engineer/logs/run-<timestamp>.json`\n- Aggregates per-call token counts, costs, durations, errors\n- Tracks `filesRead[]` metadata with `path`, `sizeBytes`, `linesRead`\n- Computes summary: `totalInputTokens`, `totalCacheReadTokens`, `errorCount`, `uniqueFilesRead`\n- Enforces retention via `cleanupOldLogs(keepCount)` after each run\n\n**Trace events:** `.agents-reverse-engineer/traces/trace-<timestamp>.ndjson`\n- Enabled via `--trace` flag\n- Events: `phase:start/end`, `worker:start/end`, `task:pickup/done`, `subprocess:spawn/exit`, `retry`\n- Auto-populated fields: `seq` (monotonic), `ts` (ISO 8601), `pid`, `elapsedMs` (high-resolution delta)\n- Promise-chain serialization ensures NDJSON line order matches emission order despite concurrent workers\n- Retention: keeps 500 most recent traces via `cleanupOldTraces(keepCount)`\n\n**Progress log:** `.agents-reverse-engineer/progress.log`\n- Human-readable streaming output mirroring console\n- ETA calculation via moving average of last 10 task durations\n- Quality metrics: code-vs-doc/code-vs-code inconsistencies, phantom path counts\n- Real-time monitoring: `tail -f .agents-reverse-engineer/progress.log`\n\n## Quality Validation\n\n**Code-vs-Doc Consistency:**\n- Extracts exported symbols via regex: `/^[ \\t]*export\\s+(?:default\\s+)?(?:function|class|const|let|var|type|interface|enum)\\s+(\\w+)/gm`\n- Verifies all exports appear in `.sum` summary text via substring search\n- Reports `CodeDocInconsistency` with `missingFromDoc` arrays\n\n**Code-vs-Code Duplicate Detection:**\n- Aggregates exports across per-directory file groups into `Map<symbol, string[]>`\n- Reports `CodeCodeInconsistency` for symbols appearing in multiple files (pattern: `'duplicate-export'`)\n\n**Phantom Path Resolution:**\n- Extracts path-like strings from `AGENTS.md` via three regex patterns:\n  - Markdown link targets: `/\\[(?:[^\\]]*)\\]\\((\\.[^)]+)\\)/g`\n  - Backtick-quoted paths: `` /`((?:src\\/|\\.\\.?\\/)[^`]+\\.[a-z]{1,4})`/g ``\n  - Prose-embedded paths: `/(?:from|in|by|via|see)\\s+`?(src\\/[\\w\\-./]+)`?/gi`\n- Resolves against AGENTS.md directory and project root with `.ts`/`.js` fallback\n- Reports `PhantomPathInconsistency` for unresolved references\n\n**Report format:** `InconsistencyReport` with `metadata` (timestamp, projectRoot, filesChecked, durationMs), `issues[]` (discriminated union), `summary` (counts by type/severity).\n\n## Incremental Update Strategy\n\n**Workflow:**\n1. Read `content_hash` from each `.sum` file's YAML frontmatter via `readSumFile()`\n2. Compute current file content hash via `computeContentHash()` (SHA-256)\n3. Hash mismatch → add to `filesToAnalyze` as `FileChange` with `status: 'modified'` or `'added'`\n4. Hash match → add to `filesToSkip`\n5. Detect orphans: `.sum` files for deleted source files or renamed oldPaths\n6. Call `cleanupOrphans()` to delete stale `.sum` files\n7. Call `cleanupEmptyDirectoryDocs()` to remove `AGENTS.md` from directories with no remaining sources\n8. Compute `affectedDirs` via `getAffectedDirectories()` by walking parent directories of changed files\n9. Regenerate `.sum` for `filesToAnalyze` via Phase 1 pool execution\n10. Regenerate `AGENTS.md` for `affectedDirs` sequentially (no Phase 2 post-order traversal required)\n\n**Git integration:**\n- Supports both committed changes (`git diff <baseCommit>..HEAD`) and uncommitted (`git status --porcelain` merge via `--uncommitted` flag)\n- Rename detection via `git diff -M` (50% similarity threshold)\n- Fallback to SHA-256 hashing for non-git workflows\n\n## Subprocess Resource Management\n\n**Problem:** Claude CLI spawns excessive Node.js threads (GitHub #5771: 200 instances reported). With concurrency=5, each subprocess spawns internal threads causing RAM exhaustion in WSL environments.\n\n**Mitigations applied in `src/ai/subprocess.ts`:**\n- `NODE_OPTIONS='--max-old-space-size=512'` limits heap to 512MB per subprocess\n- `UV_THREADPOOL_SIZE='4'` constrains libuv thread pool to 4 threads\n- `CLAUDE_CODE_DISABLE_BACKGROUND_TASKS='1'` prevents background task spawning\n- `--disallowedTools Task` prevents subprocess from spawning subagents\n- Process group killing: `kill(-pid)` terminates entire subprocess tree\n- Default concurrency reduced from 5 → 2 for resource-constrained environments\n\n**Timeout enforcement:**\n- SIGTERM sent at `timeoutMs`\n- SIGKILL escalation after 5s grace period\n- Unref'd timeout handle allows Node.js exit without cleanup blocking\n\n## Build & Development\n\n**Install dependencies:**\n```bash\nnpm install\n```\n\n**Development mode (hot reload):**\n```bash\nnpm run dev\n# Equivalent to: tsx watch src/cli/index.ts\n```\n\n**Build TypeScript:**\n```bash\nnpm run build\n# Compiles src/ → dist/ via tsc\n```\n\n**Prepare for publish:**\n```bash\nnpm run prepublishOnly\n# Executes: npm run build && npm run build:hooks\n# Copies hooks/ → hooks/dist/ for npm tarball inclusion\n```\n\n**Run from source:**\n```bash\nnode dist/cli/index.js generate\n# Or via npm link:\nnpm link\nare generate\n```\n\n## Session Hooks\n\n**Claude/Gemini session lifecycle:**\n- `are-check-update.js` (SessionStart): Spawns detached process querying `npm view agents-reverse-engineer version`, compares against `~/.claude/ARE-VERSION`, caches result to `~/.claude/cache/are-update-check.json`\n- `are-session-end.js` (SessionEnd): Runs `git status --porcelain`, spawns `npx agents-reverse-engineer@latest update --quiet` as detached background process if changes detected\n\n**OpenCode plugin system:**\n- `opencode-are-check-update.js`: Exports `AreCheckUpdate()` async factory returning plugin with `event['session.created']` handler\n- `opencode-are-session-end.js`: Exports `AreSessionEnd()` async factory returning plugin with `event['session.deleted']` handler\n\n**Detached spawn pattern:**\n```javascript\nspawn(process.execPath, ['-e', scriptString], {\n  stdio: 'ignore',\n  detached: true,\n  windowsHide: true\n}).unref()\n```\n\n**Disable mechanisms:**\n- Environment variable: `ARE_DISABLE_HOOK=1`\n- Config flag: `hook_enabled: false` in `.agents-reverse-engineer.yaml` (substring search, no YAML parser)\n\n## Integration with AI Assistants\n\n**Platform-specific command formats:**\n\n**Claude Code:**\n- Location: `.claude/skills/are-generate/SKILL.md`\n- Frontmatter: `name: /are-generate`\n- Long-running pattern: Remove stale `progress.log`, run with `run_in_background: true`, poll via `tail -5` every 10-15s, check `TaskOutput` with `block: false`, summarize on completion\n\n**OpenCode:**\n- Location: `.opencode/commands/are-generate.md`\n- Frontmatter: `agent: build`\n- Identical progress monitoring pattern\n\n**Gemini:**\n- Location: `.gemini/commands/are-generate.toml`\n- Format: `description = \"...\"` and `prompt = \"\"\"...\"\"\"` triple-quoted multiline\n\n**Aider:**\n- Detection only via `.aider.conf.yml` or `.aider/` directory\n- No template generation (manual integration required)\n\n## CI/CD\n\n**GitHub Actions workflow:** `.github/workflows/publish.yml`\n- Triggers on `release[published]` events or manual `workflow_dispatch`\n- Executes `ubuntu-latest` job with `id-token: write` permission enabling Sigstore-signed provenance\n- Runs `actions/checkout@v4`, `actions/setup-node@v4` with registry-url `https://registry.npmjs.org`\n- Executes `npm ci`, `npm run build` (invokes `tsc` + `build:hooks` from `prepublishOnly`)\n- Publishes with `npm publish --provenance --access public` using `NPM_TOKEN` secret\n- Cryptographic attestation links published artifact to source commit SHA\n\n## Roadmap & Known Limitations\n\n**Stub Backends:**\n- Gemini CLI adapter throws `SUBPROCESS_ERROR` until JSON output format stabilizes\n- OpenCode CLI adapter throws `SUBPROCESS_ERROR` until JSONL parsing implemented\n\n**Quality Validator Limitations:**\n- Regex-based export extraction misses complex patterns (destructured, namespace, dynamic exports)\n- Substring matching in code-vs-doc yields false negatives for prose mentions unrelated to API surface\n- Code-vs-code operates on symbol names only without AST analysis to distinguish intentional duplication\n\n**Disabled Features:**\n- Density validator (`validateFindability`) disabled after structured `publicInterface` removal from `SumFileContent` schema",
      "model": "claude-sonnet-4-5-20250929",
      "inputTokens": 3,
      "outputTokens": 6246,
      "cacheReadTokens": 16334,
      "cacheCreationTokens": 58184,
      "latencyMs": 151753,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": []
    }
  ],
  "summary": {
    "totalCalls": 126,
    "totalInputTokens": 392,
    "totalOutputTokens": 142848,
    "totalCacheReadTokens": 1718545,
    "totalCacheCreationTokens": 2045307,
    "totalDurationMs": 4026768,
    "errorCount": 0,
    "totalFilesRead": 96,
    "uniqueFilesRead": 96
  }
}