{
  "runId": "2026-02-14T13:46:12.857Z",
  "startTime": "2026-02-14T13:46:12.857Z",
  "endTime": "2026-02-14T13:52:08.430Z",
  "backend": "claude",
  "model": "haiku",
  "command": "generate",
  "entries": [
    {
      "timestamp": "2026-02-14T13:46:12.924Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: src/discovery/filters/custom.ts\n\n```typescript\n/**\n * Custom pattern filter for file discovery.\n *\n * Allows users to specify additional exclusion patterns in their configuration.\n * Uses the `ignore` library for gitignore-style pattern matching.\n */\n\nimport ignore, { type Ignore } from 'ignore';\nimport path from 'node:path';\nimport type { FileFilter } from '../types.js';\n\n/**\n * Creates a custom filter that excludes files matching user-provided patterns.\n *\n * Patterns use gitignore syntax and are checked against relative paths from\n * the root directory. If no patterns are provided, the filter passes all files.\n *\n * @param patterns - Array of gitignore-style patterns to exclude\n * @param root - Root directory for converting absolute paths to relative\n * @returns A FileFilter that checks paths against custom patterns\n *\n * @example\n * ```typescript\n * const filter = createCustomFilter(['*.log', 'tmp/**', 'secret.txt'], '/project');\n * filter.shouldExclude('/project/debug.log'); // true\n * filter.shouldExclude('/project/src/app.ts'); // false\n * ```\n */\nexport function createCustomFilter(patterns: string[], root: string): FileFilter {\n  const ig: Ignore = ignore();\n  const normalizedRoot = path.resolve(root);\n\n  // Add all patterns to the ignore instance\n  if (patterns.length > 0) {\n    ig.add(patterns);\n  }\n\n  return {\n    name: 'custom',\n\n    shouldExclude(absolutePath: string): boolean {\n      // If no patterns, nothing to exclude\n      if (patterns.length === 0) {\n        return false;\n      }\n\n      // Convert to relative path (ignore library requires relative paths)\n      const relativePath = path.relative(normalizedRoot, absolutePath);\n\n      // If path is outside root (starts with ..) or is empty, don't exclude\n      if (!relativePath || relativePath.startsWith('..')) {\n        return false;\n      }\n\n      return ig.ignores(relativePath);\n    },\n  };\n}\n\n```\n\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n- Workflow & convention rules: if this file defines contribution guidelines, PR conventions, commit standards, testing mandates, tool usage requirements, approval workflows, AI agent behavioral instructions, or code conventions (naming standards, formatting rules, import ordering, linting policies), extract them as explicit, actionable rules. Sources: CONTRIBUTING.md, CI configs, PR templates, style guides, linter/formatter configs (.eslintrc, .prettierrc, .editorconfig), tsconfig strictness, README dev sections.\n\n\nTARGET LENGTH (MANDATORY):\n- Source file: 1847 characters\n- Target summary: ~185 characters (10% compression)\n- Maximum: 222 characters\n- Achieve this by: ultra-dense writing, minimal examples, single-sentence descriptions\n\nULTRA-COMPRESSION TECHNIQUES:\n- Use telegraphic style: \"exports foo, bar, baz\" not \"this file exports three functions\"\n- No code examples unless critical to understanding\n- One sentence per export maximum\n- Omit obvious parameter names (\"opts: Options\" not \"options: Options - configuration object\")\n- Use abbreviations: \"params\" not \"parameters\", \"config\" not \"configuration\"\n- Skip section if empty (don't write \"No special concerns\" or \"N/A\")\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nWORKFLOW & CONVENTION RULES (NEVER EXCLUDE):\n- Contribution requirements: branch naming, commit message formats, PR description templates\n- Testing mandates: required test types, coverage thresholds, \"never delete tests\" policies\n- Tool usage rules: required CLI commands, browser automation tools, linter/formatter requirements\n- Approval workflows: review requirements, CI checks that must pass, deployment gates\n- AI agent instructions: explicit directives for automated assistants (coding rules, prohibited actions)\n- Code conventions: naming standards (camelCase, PascalCase, snake_case for what), import ordering rules, file/folder naming patterns, formatting rules (indentation, quotes, semicolons), language idioms, TypeScript strictness policies, preferred patterns vs anti-patterns\nThese define how contributors (human or AI) must work within the project.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion .annex.sum file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**custom.ts exports createCustomFilter() to build a FileFilter using gitignore-style pattern matching via the ignore library.**\n\n## Exports\n\n`createCustomFilter(patterns: string[], root: string): FileFilter` — constructs a FileFilter with `name: 'custom'` and `shouldExclude(absolutePath: string): boolean` method that checks relative paths against gitignore patterns.\n\n## Pattern Matching Behavior\n\nPatterns use gitignore syntax (e.g., `*.log`, `tmp/**`, `secret.txt`). Absolute paths are converted to relative paths via `path.relative()` from the normalized root directory. Paths starting with `..` or empty strings are never excluded. Empty pattern arrays return `false` (pass all files). Pattern matching delegates to `ignore().add(patterns).ignores(relativePath)`.\n\n## Dependencies\n\nImports `ignore` library (Ignore type), Node's `path` module, and `FileFilter` interface from parent types module.",
      "model": "claude-haiku-4-5-20251001",
      "inputTokens": 3,
      "outputTokens": 223,
      "cacheReadTokens": 16147,
      "cacheCreationTokens": 3236,
      "latencyMs": 14362,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/discovery/filters/custom.ts",
          "sizeBytes": 1847
        }
      ]
    },
    {
      "timestamp": "2026-02-14T13:46:12.910Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: src/discovery/filters/binary.ts\n\n```typescript\n/**\n * Binary file filter for file discovery.\n *\n * Uses extension-based detection as a fast path, falling back to content\n * analysis via `isbinaryfile` for unknown extensions. Also handles large\n * files by size threshold.\n */\n\nimport { isBinaryFile } from 'isbinaryfile';\nimport fs from 'node:fs/promises';\nimport path from 'node:path';\nimport type { FileFilter } from '../types.js';\n\n/**\n * Set of file extensions known to be binary.\n * These are excluded without content analysis for performance.\n */\nexport const BINARY_EXTENSIONS = new Set([\n  // Images\n  '.png',\n  '.jpg',\n  '.jpeg',\n  '.gif',\n  '.bmp',\n  '.ico',\n  '.webp',\n  '.svg',\n  '.tiff',\n  '.tif',\n  '.psd',\n  '.raw',\n  '.heif',\n  '.heic',\n  // Archives\n  '.zip',\n  '.tar',\n  '.gz',\n  '.rar',\n  '.7z',\n  '.bz2',\n  '.xz',\n  '.tgz',\n  // Executables\n  '.exe',\n  '.dll',\n  '.so',\n  '.dylib',\n  '.bin',\n  '.msi',\n  '.app',\n  '.dmg',\n  // Media\n  '.mp3',\n  '.mp4',\n  '.wav',\n  '.avi',\n  '.mov',\n  '.mkv',\n  '.flac',\n  '.ogg',\n  '.webm',\n  '.m4a',\n  '.aac',\n  '.wma',\n  '.wmv',\n  '.flv',\n  // Documents (binary formats)\n  '.pdf',\n  '.doc',\n  '.docx',\n  '.xls',\n  '.xlsx',\n  '.ppt',\n  '.pptx',\n  '.odt',\n  '.ods',\n  '.odp',\n  // Fonts\n  '.woff',\n  '.woff2',\n  '.ttf',\n  '.eot',\n  '.otf',\n  // Compiled/bytecode\n  '.class',\n  '.pyc',\n  '.pyo',\n  '.o',\n  '.obj',\n  '.a',\n  '.lib',\n  '.wasm',\n  // Database\n  '.db',\n  '.sqlite',\n  '.sqlite3',\n  '.mdb',\n  // Other\n  '.ico',\n  '.icns',\n  '.cur',\n  '.deb',\n  '.rpm',\n  '.jar',\n  '.war',\n  '.ear',\n]);\n\n/**\n * Options for the binary filter.\n */\nexport interface BinaryFilterOptions {\n  /**\n   * Maximum file size in bytes. Files larger than this are excluded.\n   * Default: 1MB (1048576 bytes)\n   */\n  maxFileSize?: number;\n\n  /**\n   * Additional binary extensions to recognize beyond the defaults.\n   */\n  additionalExtensions?: string[];\n}\n\n/** Default maximum file size: 1MB */\nconst DEFAULT_MAX_FILE_SIZE = 1024 * 1024;\n\n/**\n * Creates a binary filter that excludes binary files and files exceeding size limit.\n *\n * The filter uses a two-phase detection approach:\n * 1. Fast path: Check extension against known binary extensions\n * 2. Slow path: For unknown extensions, analyze file content with isbinaryfile\n *\n * @param options - Filter configuration options\n * @returns A FileFilter that identifies binary files\n *\n * @example\n * ```typescript\n * const filter = createBinaryFilter({ maxFileSize: 500000 });\n * await filter.shouldExclude('/path/to/image.png'); // true (extension)\n * await filter.shouldExclude('/path/to/unknown.xyz'); // checks content\n * ```\n */\nexport function createBinaryFilter(options: BinaryFilterOptions = {}): FileFilter {\n  const { maxFileSize = DEFAULT_MAX_FILE_SIZE, additionalExtensions = [] } = options;\n\n  // Create a combined set of all binary extensions\n  const binaryExtensions = new Set(BINARY_EXTENSIONS);\n  for (const ext of additionalExtensions) {\n    // Ensure extension has leading dot\n    binaryExtensions.add(ext.startsWith('.') ? ext : `.${ext}`);\n  }\n\n  return {\n    name: 'binary',\n\n    async shouldExclude(absolutePath: string): Promise<boolean> {\n      // Fast path: check extension\n      const ext = path.extname(absolutePath).toLowerCase();\n      if (binaryExtensions.has(ext)) {\n        return true;\n      }\n\n      // Check file size and content\n      try {\n        const stats = await fs.stat(absolutePath);\n\n        // Exclude files exceeding size limit\n        if (stats.size > maxFileSize) {\n          return true;\n        }\n\n        // Slow path: content analysis for unknown extensions\n        return await isBinaryFile(absolutePath);\n      } catch {\n        // If we can't read it, skip it\n        return true;\n      }\n    },\n  };\n}\n\n```\n\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n- Workflow & convention rules: if this file defines contribution guidelines, PR conventions, commit standards, testing mandates, tool usage requirements, approval workflows, AI agent behavioral instructions, or code conventions (naming standards, formatting rules, import ordering, linting policies), extract them as explicit, actionable rules. Sources: CONTRIBUTING.md, CI configs, PR templates, style guides, linter/formatter configs (.eslintrc, .prettierrc, .editorconfig), tsconfig strictness, README dev sections.\n\n\nTARGET LENGTH (MANDATORY):\n- Source file: 3701 characters\n- Target summary: ~370 characters (10% compression)\n- Maximum: 444 characters\n- Achieve this by: ultra-dense writing, minimal examples, single-sentence descriptions\n\nULTRA-COMPRESSION TECHNIQUES:\n- Use telegraphic style: \"exports foo, bar, baz\" not \"this file exports three functions\"\n- No code examples unless critical to understanding\n- One sentence per export maximum\n- Omit obvious parameter names (\"opts: Options\" not \"options: Options - configuration object\")\n- Use abbreviations: \"params\" not \"parameters\", \"config\" not \"configuration\"\n- Skip section if empty (don't write \"No special concerns\" or \"N/A\")\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nWORKFLOW & CONVENTION RULES (NEVER EXCLUDE):\n- Contribution requirements: branch naming, commit message formats, PR description templates\n- Testing mandates: required test types, coverage thresholds, \"never delete tests\" policies\n- Tool usage rules: required CLI commands, browser automation tools, linter/formatter requirements\n- Approval workflows: review requirements, CI checks that must pass, deployment gates\n- AI agent instructions: explicit directives for automated assistants (coding rules, prohibited actions)\n- Code conventions: naming standards (camelCase, PascalCase, snake_case for what), import ordering rules, file/folder naming patterns, formatting rules (indentation, quotes, semicolons), language idioms, TypeScript strictness policies, preferred patterns vs anti-patterns\nThese define how contributors (human or AI) must work within the project.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion .annex.sum file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**binary.ts exports a binary file filter with extension-based fast-path detection and fallback content analysis via isbinaryfile.**\n\n## Exports\n\n**BINARY_EXTENSIONS**: `Set<string>` — Pre-computed set of 65 binary file extensions (images, archives, executables, media, documents, fonts, compiled code, databases) for O(1) exclusion without I/O.\n\n**BinaryFilterOptions**: Interface with `maxFileSize?: number` (default 1MB=1048576 bytes) and `additionalExtensions?: string[]` for runtime extension registration.\n\n**createBinaryFilter(options: BinaryFilterOptions): FileFilter** — Returns FileFilter object with `name: 'binary'` and async `shouldExclude(absolutePath): Promise<boolean>`.\n\n## Detection Strategy\n\nTwo-phase approach: (1) fast-path extension check via `path.extname().toLowerCase()` against binaryExtensions Set; (2) slow-path content analysis via `isBinaryFile(absolutePath)` for unknown extensions. Size threshold enforced: `stats.size > maxFileSize` returns true. File I/O errors caught; unreadable files excluded.\n\n## Integration\n\nImplements FileFilter contract from `../types.js`. Depends on `isbinaryfile` npm package for content detection, `node:fs/promises` for stat, `node:path` for extension parsing.",
      "model": "claude-haiku-4-5-20251001",
      "inputTokens": 3,
      "outputTokens": 320,
      "cacheReadTokens": 0,
      "cacheCreationTokens": 20147,
      "latencyMs": 15299,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/discovery/filters/binary.ts",
          "sizeBytes": 3701
        }
      ]
    },
    {
      "timestamp": "2026-02-14T13:46:12.897Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: src/ai/telemetry/cleanup.ts\n\n```typescript\n/**\n * Telemetry log cleanup utility.\n *\n * Removes old run log files from `.agents-reverse-engineer/logs/`,\n * keeping only the N most recent. Files are sorted lexicographically\n * by name, which works correctly because filenames contain ISO timestamps.\n *\n * @module\n */\n\nimport * as fs from 'node:fs/promises';\nimport * as path from 'node:path';\n\n/** Directory name for telemetry log files (relative to project root) */\nconst LOGS_DIR = '.agents-reverse-engineer/logs';\n\n/**\n * Remove old telemetry log files, keeping only the most recent ones.\n *\n * Reads the logs directory, filters for files matching `run-*.json`,\n * sorts newest-first (lexicographic sort on ISO timestamp filenames),\n * and deletes everything beyond `keepCount`.\n *\n * If the logs directory does not exist, returns 0 without error.\n *\n * @param projectRoot - Absolute path to the project root directory\n * @param keepCount - Number of most recent log files to retain\n * @returns Number of files deleted\n *\n * @example\n * ```typescript\n * const deleted = await cleanupOldLogs('/home/user/project', 10);\n * console.log(`Cleaned up ${deleted} old log files`);\n * ```\n */\nexport async function cleanupOldLogs(projectRoot: string, keepCount: number): Promise<number> {\n  const logsDir = path.join(projectRoot, LOGS_DIR);\n\n  let entries: string[];\n  try {\n    const allEntries = await fs.readdir(logsDir);\n    entries = allEntries.filter(\n      (name) => name.startsWith('run-') && name.endsWith('.json'),\n    );\n  } catch (error) {\n    // Directory doesn't exist -- nothing to clean up\n    if ((error as NodeJS.ErrnoException).code === 'ENOENT') {\n      return 0;\n    }\n    throw error;\n  }\n\n  // Sort lexicographically (newest first) and find files to delete\n  entries.sort();\n  entries.reverse();\n\n  const toDelete = entries.slice(keepCount);\n\n  for (const filename of toDelete) {\n    await fs.unlink(path.join(logsDir, filename));\n  }\n\n  return toDelete.length;\n}\n\n```\n\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n- Workflow & convention rules: if this file defines contribution guidelines, PR conventions, commit standards, testing mandates, tool usage requirements, approval workflows, AI agent behavioral instructions, or code conventions (naming standards, formatting rules, import ordering, linting policies), extract them as explicit, actionable rules. Sources: CONTRIBUTING.md, CI configs, PR templates, style guides, linter/formatter configs (.eslintrc, .prettierrc, .editorconfig), tsconfig strictness, README dev sections.\n\n\nTARGET LENGTH (MANDATORY):\n- Source file: 1937 characters\n- Target summary: ~194 characters (10% compression)\n- Maximum: 233 characters\n- Achieve this by: ultra-dense writing, minimal examples, single-sentence descriptions\n\nULTRA-COMPRESSION TECHNIQUES:\n- Use telegraphic style: \"exports foo, bar, baz\" not \"this file exports three functions\"\n- No code examples unless critical to understanding\n- One sentence per export maximum\n- Omit obvious parameter names (\"opts: Options\" not \"options: Options - configuration object\")\n- Use abbreviations: \"params\" not \"parameters\", \"config\" not \"configuration\"\n- Skip section if empty (don't write \"No special concerns\" or \"N/A\")\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nWORKFLOW & CONVENTION RULES (NEVER EXCLUDE):\n- Contribution requirements: branch naming, commit message formats, PR description templates\n- Testing mandates: required test types, coverage thresholds, \"never delete tests\" policies\n- Tool usage rules: required CLI commands, browser automation tools, linter/formatter requirements\n- Approval workflows: review requirements, CI checks that must pass, deployment gates\n- AI agent instructions: explicit directives for automated assistants (coding rules, prohibited actions)\n- Code conventions: naming standards (camelCase, PascalCase, snake_case for what), import ordering rules, file/folder naming patterns, formatting rules (indentation, quotes, semicolons), language idioms, TypeScript strictness policies, preferred patterns vs anti-patterns\nThese define how contributors (human or AI) must work within the project.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion .annex.sum file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**cleanup.ts removes old telemetry log files from `.agents-reverse-engineer/logs/`, retaining only the N most recent.**\n\n## Exports\n\n`cleanupOldLogs(projectRoot: string, keepCount: number): Promise<number>` — deletes log files matching `run-*.json` pattern beyond `keepCount` threshold, returns count of deleted files; returns 0 if logs directory missing.\n\n## Constants\n\n`LOGS_DIR = '.agents-reverse-engineer/logs'` — relative path to telemetry logs directory from project root.\n\n## Behavior\n\n- Filters entries by pattern `run-*.json`\n- Sorts lexicographically (newest-first via reverse); ISO timestamp filenames ensure correct chronological ordering\n- Slices to `keepCount` most recent, deletes remainder\n- Handles `ENOENT` gracefully (returns 0); re-throws other errors",
      "model": "claude-haiku-4-5-20251001",
      "inputTokens": 3,
      "outputTokens": 208,
      "cacheReadTokens": 0,
      "cacheCreationTokens": 19470,
      "latencyMs": 15423,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/ai/telemetry/cleanup.ts",
          "sizeBytes": 1937
        }
      ]
    },
    {
      "timestamp": "2026-02-14T13:46:12.932Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: src/discovery/filters/index.ts\n\n```typescript\n/**\n * Filter chain orchestration for file discovery.\n *\n * This module exports all filter creators and provides the applyFilters\n * function that runs files through the filter chain, recording which\n * filter excluded each file.\n */\n\nimport type { FileFilter, FilterResult, ExcludedFile } from '../types.js';\nimport type { ITraceWriter } from '../../orchestration/trace.js';\nimport type { Logger } from '../../core/logger.js';\nimport { nullLogger } from '../../core/logger.js';\n\n// Re-export all filter creators\nexport { createGitignoreFilter } from './gitignore.js';\nexport { createVendorFilter, DEFAULT_VENDOR_DIRS } from './vendor.js';\nexport { createBinaryFilter, BINARY_EXTENSIONS, type BinaryFilterOptions } from './binary.js';\nexport { createCustomFilter } from './custom.js';\n\n/**\n * Applies a chain of filters to a list of files.\n *\n * Each file is run through filters in order until one excludes it\n * (short-circuit evaluation). Files are processed with bounded concurrency\n * to avoid opening too many file handles simultaneously (important for\n * binary content detection which performs file I/O).\n *\n * @param files - Array of absolute file paths to filter\n * @param filters - Array of filters to apply in order\n * @param options - Optional tracing and debug options\n * @returns Promise resolving to FilterResult with included and excluded lists\n *\n * @example\n * ```typescript\n * const filters = [\n *   createVendorFilter(['node_modules']),\n *   createBinaryFilter({}),\n * ];\n * const result = await applyFilters(['/a/b.js', '/a/node_modules/c.js'], filters);\n * // result.included: ['/a/b.js']\n * // result.excluded: [{ path: '/a/node_modules/c.js', filter: 'vendor', reason: '...' }]\n * ```\n */\nexport async function applyFilters(\n  files: string[],\n  filters: FileFilter[],\n  options?: { tracer?: ITraceWriter; debug?: boolean; logger?: Logger }\n): Promise<FilterResult> {\n  const included: string[] = [];\n  const excluded: ExcludedFile[] = [];\n\n  // Track exclusions per filter for trace events\n  const filterStats = new Map<string, { matched: number; rejected: number }>();\n  for (const filter of filters) {\n    filterStats.set(filter.name, { matched: 0, rejected: 0 });\n  }\n\n  // Process files with bounded concurrency to avoid exhausting file descriptors.\n  // Binary filter calls isBinaryFile() which does file I/O.\n  const CONCURRENCY = 30;\n  const iterator = files.entries();\n\n  async function worker(\n    iter: IterableIterator<[number, string]>,\n  ): Promise<Array<{ index: number; file: string; excluded?: ExcludedFile }>> {\n    const results: Array<{ index: number; file: string; excluded?: ExcludedFile }> = [];\n    for (const [index, file] of iter) {\n      let wasExcluded = false;\n\n      // Run through filters in order, stop at first exclusion\n      for (const filter of filters) {\n        const shouldExclude = await filter.shouldExclude(file);\n\n        if (shouldExclude) {\n          results.push({\n            index,\n            file,\n            excluded: {\n              path: file,\n              reason: `Excluded by ${filter.name} filter`,\n              filter: filter.name,\n            },\n          });\n          wasExcluded = true;\n          break; // Short-circuit: stop checking other filters\n        }\n      }\n\n      if (!wasExcluded) {\n        results.push({ index, file });\n      }\n    }\n    return results;\n  }\n\n  const effectiveConcurrency = Math.min(CONCURRENCY, files.length);\n  const workers = Array.from({ length: effectiveConcurrency }, () =>\n    worker(iterator),\n  );\n\n  const allResults = (await Promise.all(workers)).flat();\n\n  // Sort by original index to preserve order\n  allResults.sort((a, b) => a.index - b.index);\n\n  // Collect results and update filter stats\n  for (const result of allResults) {\n    if (result.excluded) {\n      excluded.push(result.excluded);\n      const stats = filterStats.get(result.excluded.filter);\n      if (stats) {\n        stats.rejected++;\n      }\n    } else {\n      included.push(result.file);\n      // All filters \"matched\" (passed through) this file\n      for (const filter of filters) {\n        const stats = filterStats.get(filter.name);\n        if (stats) {\n          stats.matched++;\n        }\n      }\n    }\n  }\n\n  // Emit trace events for each filter\n  for (const filter of filters) {\n    const stats = filterStats.get(filter.name);\n    if (stats) {\n      options?.tracer?.emit({\n        type: 'filter:applied',\n        filterName: filter.name,\n        filesMatched: stats.matched,\n        filesRejected: stats.rejected,\n      });\n\n      // Debug output\n      if (options?.debug && stats.rejected > 0) {\n        (options?.logger ?? nullLogger).debug(`[debug] Filter [${filter.name}]: ${stats.rejected} files rejected`);\n      }\n    }\n  }\n\n  return { included, excluded };\n}\n\n\n```\n\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n- Workflow & convention rules: if this file defines contribution guidelines, PR conventions, commit standards, testing mandates, tool usage requirements, approval workflows, AI agent behavioral instructions, or code conventions (naming standards, formatting rules, import ordering, linting policies), extract them as explicit, actionable rules. Sources: CONTRIBUTING.md, CI configs, PR templates, style guides, linter/formatter configs (.eslintrc, .prettierrc, .editorconfig), tsconfig strictness, README dev sections.\n\n\nTARGET LENGTH (MANDATORY):\n- Source file: 4776 characters\n- Target summary: ~478 characters (10% compression)\n- Maximum: 574 characters\n- Achieve this by: ultra-dense writing, minimal examples, single-sentence descriptions\n\nULTRA-COMPRESSION TECHNIQUES:\n- Use telegraphic style: \"exports foo, bar, baz\" not \"this file exports three functions\"\n- No code examples unless critical to understanding\n- One sentence per export maximum\n- Omit obvious parameter names (\"opts: Options\" not \"options: Options - configuration object\")\n- Use abbreviations: \"params\" not \"parameters\", \"config\" not \"configuration\"\n- Skip section if empty (don't write \"No special concerns\" or \"N/A\")\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nWORKFLOW & CONVENTION RULES (NEVER EXCLUDE):\n- Contribution requirements: branch naming, commit message formats, PR description templates\n- Testing mandates: required test types, coverage thresholds, \"never delete tests\" policies\n- Tool usage rules: required CLI commands, browser automation tools, linter/formatter requirements\n- Approval workflows: review requirements, CI checks that must pass, deployment gates\n- AI agent instructions: explicit directives for automated assistants (coding rules, prohibited actions)\n- Code conventions: naming standards (camelCase, PascalCase, snake_case for what), import ordering rules, file/folder naming patterns, formatting rules (indentation, quotes, semicolons), language idioms, TypeScript strictness policies, preferred patterns vs anti-patterns\nThese define how contributors (human or AI) must work within the project.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion .annex.sum file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**src/discovery/filters/index.ts orchestrates filter chain execution for file discovery, re-exporting filter creators and providing applyFilters to process files through ordered filters with bounded concurrency and trace recording.**\n\n## Exports\n\n`createGitignoreFilter`, `createVendorFilter` (with `DEFAULT_VENDOR_DIRS`), `createBinaryFilter` (with `BINARY_EXTENSIONS`, `BinaryFilterOptions`), `createCustomFilter` — filter factory functions re-exported from submodules.\n\n`applyFilters(files: string[], filters: FileFilter[], options?: { tracer?: ITraceWriter; debug?: boolean; logger?: Logger }): Promise<FilterResult>` — applies ordered filter chain to file array, short-circuits at first exclusion, returns `{ included: string[], excluded: ExcludedFile[] }`.\n\n## Concurrency & Performance\n\nProcesses files with bounded concurrency (`CONCURRENCY = 30`) via worker pool from shared iterator to prevent file descriptor exhaustion (critical for binary content I/O). Order preserved via index-based sorting post-execution.\n\n## Filter Chain Semantics\n\nFilters evaluated per-file in order; first `shouldExclude()` returning truthy stops further checks. Excluded files record structure: `{ path, filter: filterName, reason: \"Excluded by [name] filter\" }`. Stats tracked per filter (matched/rejected) for trace emission.\n\n## Trace & Debug\n\nEmits `filter:applied` trace events with `filterName`, `filesMatched`, `filesRejected`. Debug logging via optional `logger` (defaults `nullLogger`).",
      "model": "claude-haiku-4-5-20251001",
      "inputTokens": 3,
      "outputTokens": 370,
      "cacheReadTokens": 0,
      "cacheCreationTokens": 20252,
      "latencyMs": 15682,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/discovery/filters/index.ts",
          "sizeBytes": 4776
        }
      ]
    },
    {
      "timestamp": "2026-02-14T13:46:12.928Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: src/discovery/filters/gitignore.ts\n\n```typescript\n/**\n * Gitignore pattern filter for file discovery.\n *\n * Uses the `ignore` library to parse and match .gitignore patterns.\n * This filter loads the root .gitignore file and checks paths against\n * the patterns to determine exclusion.\n */\n\nimport ignore, { type Ignore } from 'ignore';\nimport fs from 'node:fs/promises';\nimport path from 'node:path';\nimport type { FileFilter } from '../types.js';\n\n/**\n * Creates a gitignore filter that excludes files matching .gitignore patterns.\n *\n * @param root - The root directory containing the .gitignore file\n * @returns A FileFilter that checks paths against gitignore patterns\n *\n * @example\n * ```typescript\n * const filter = await createGitignoreFilter('/path/to/project');\n * if (filter.shouldExclude('/path/to/project/dist/bundle.js')) {\n *   console.log('File is gitignored');\n * }\n * ```\n */\nexport async function createGitignoreFilter(root: string): Promise<FileFilter> {\n  const ig: Ignore = ignore();\n  const normalizedRoot = path.resolve(root);\n\n  // Load .gitignore from root if it exists\n  const gitignorePath = path.join(normalizedRoot, '.gitignore');\n  try {\n    const content = await fs.readFile(gitignorePath, 'utf-8');\n    ig.add(content);\n  } catch {\n    // No .gitignore file, filter will pass everything through\n  }\n\n  return {\n    name: 'gitignore',\n\n    shouldExclude(absolutePath: string): boolean {\n      // Convert to relative path (ignore library requires relative paths)\n      const relativePath = path.relative(normalizedRoot, absolutePath);\n\n      // If path is outside root (starts with ..) or is empty, don't exclude\n      if (!relativePath || relativePath.startsWith('..')) {\n        return false;\n      }\n\n      // CRITICAL: The ignore library treats paths differently based on trailing slash\n      // We check the path as-is (for files) - directory handling would need trailing slash\n      // Since our walker returns files only, we don't append slash here\n      return ig.ignores(relativePath);\n    },\n  };\n}\n\n```\n\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n- Workflow & convention rules: if this file defines contribution guidelines, PR conventions, commit standards, testing mandates, tool usage requirements, approval workflows, AI agent behavioral instructions, or code conventions (naming standards, formatting rules, import ordering, linting policies), extract them as explicit, actionable rules. Sources: CONTRIBUTING.md, CI configs, PR templates, style guides, linter/formatter configs (.eslintrc, .prettierrc, .editorconfig), tsconfig strictness, README dev sections.\n\n\nTARGET LENGTH (MANDATORY):\n- Source file: 1990 characters\n- Target summary: ~199 characters (10% compression)\n- Maximum: 239 characters\n- Achieve this by: ultra-dense writing, minimal examples, single-sentence descriptions\n\nULTRA-COMPRESSION TECHNIQUES:\n- Use telegraphic style: \"exports foo, bar, baz\" not \"this file exports three functions\"\n- No code examples unless critical to understanding\n- One sentence per export maximum\n- Omit obvious parameter names (\"opts: Options\" not \"options: Options - configuration object\")\n- Use abbreviations: \"params\" not \"parameters\", \"config\" not \"configuration\"\n- Skip section if empty (don't write \"No special concerns\" or \"N/A\")\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nWORKFLOW & CONVENTION RULES (NEVER EXCLUDE):\n- Contribution requirements: branch naming, commit message formats, PR description templates\n- Testing mandates: required test types, coverage thresholds, \"never delete tests\" policies\n- Tool usage rules: required CLI commands, browser automation tools, linter/formatter requirements\n- Approval workflows: review requirements, CI checks that must pass, deployment gates\n- AI agent instructions: explicit directives for automated assistants (coding rules, prohibited actions)\n- Code conventions: naming standards (camelCase, PascalCase, snake_case for what), import ordering rules, file/folder naming patterns, formatting rules (indentation, quotes, semicolons), language idioms, TypeScript strictness policies, preferred patterns vs anti-patterns\nThese define how contributors (human or AI) must work within the project.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion .annex.sum file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**gitignore.ts exports createGitignoreFilter, a function that creates a FileFilter wrapping the ignore library to exclude paths matching .gitignore patterns.**\n\n## Exports\n\n`createGitignoreFilter(root: string): Promise<FileFilter>` — Async factory returning FileFilter with `name: 'gitignore'` and `shouldExclude(absolutePath: string): boolean` method. Loads .gitignore from root directory; silently continues if file missing.\n\n## Dependencies\n\nUses `ignore` library (type `Ignore`) for pattern matching; `fs.readFile` to load .gitignore content; `path.resolve` and `path.relative` for path normalization.\n\n## Critical Behavior\n\n`shouldExclude()` converts absolutePath to relative path via `path.relative(normalizedRoot, absolutePath)`. Returns false if relativePath is empty or starts with `'..'` (outside root). Ignores library requires relative paths; file-only walker means no trailing-slash directory handling applied.",
      "model": "claude-haiku-4-5-20251001",
      "inputTokens": 3,
      "outputTokens": 234,
      "cacheReadTokens": 16147,
      "cacheCreationTokens": 3300,
      "latencyMs": 16478,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/discovery/filters/gitignore.ts",
          "sizeBytes": 1990
        }
      ]
    },
    {
      "timestamp": "2026-02-14T13:46:12.887Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: src/ai/backends/gemini.ts\n\n```typescript\n/**\n * Gemini CLI backend stub.\n *\n * Implements the {@link AIBackend} interface for the Gemini CLI (`gemini`).\n * This is a stub that demonstrates the extension pattern -- `parseResponse`\n * throws \"not implemented\". Full implementation deferred to a future phase\n * once the Gemini CLI JSON output is stable (see RESEARCH.md Open Question 2).\n *\n * @module\n */\n\nimport type { AIBackend, AICallOptions, AIResponse } from '../types.js';\nimport { AIServiceError } from '../types.js';\nimport { isCommandOnPath } from './claude.js';\n\n/**\n * Gemini CLI backend stub.\n *\n * Detects CLI availability and builds argument arrays, but throws when\n * `parseResponse` is called since the Gemini adapter is not yet implemented.\n *\n * @example\n * ```typescript\n * const backend = new GeminiBackend();\n * console.log(await backend.isAvailable()); // true if `gemini` is on PATH\n * backend.parseResponse('{}', 0, 0);        // throws AIServiceError\n * ```\n */\nexport class GeminiBackend implements AIBackend {\n  readonly name = 'gemini';\n  readonly cliCommand = 'gemini';\n\n  /**\n   * Check if the `gemini` CLI is available on PATH.\n   */\n  async isAvailable(): Promise<boolean> {\n    return isCommandOnPath(this.cliCommand);\n  }\n\n  /**\n   * Build CLI arguments for a Gemini invocation.\n   *\n   * Based on documented Gemini CLI flags from RESEARCH.md.\n   * The prompt goes to stdin via the subprocess wrapper.\n   *\n   * @param _options - Call options (unused in stub)\n   * @returns Argument array for the Gemini CLI\n   */\n  buildArgs(_options: AICallOptions): string[] {\n    return ['-p', '--output-format', 'json'];\n  }\n\n  /**\n   * Parse Gemini CLI output into a normalized {@link AIResponse}.\n   *\n   * @throws {AIServiceError} Always -- Gemini backend is not yet implemented\n   */\n  parseResponse(_stdout: string, _durationMs: number, _exitCode: number): AIResponse {\n    throw new AIServiceError(\n      'SUBPROCESS_ERROR',\n      'Gemini backend is not yet implemented. Use Claude backend.',\n    );\n  }\n\n  /**\n   * Get user-facing install instructions for the Gemini CLI.\n   */\n  getInstallInstructions(): string {\n    return [\n      'Gemini CLI (experimental):',\n      '  npm install -g @anthropic-ai/gemini-cli',\n      '  https://github.com/google-gemini/gemini-cli',\n    ].join('\\n');\n  }\n}\n\n```\n\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n- Workflow & convention rules: if this file defines contribution guidelines, PR conventions, commit standards, testing mandates, tool usage requirements, approval workflows, AI agent behavioral instructions, or code conventions (naming standards, formatting rules, import ordering, linting policies), extract them as explicit, actionable rules. Sources: CONTRIBUTING.md, CI configs, PR templates, style guides, linter/formatter configs (.eslintrc, .prettierrc, .editorconfig), tsconfig strictness, README dev sections.\n\n\nTARGET LENGTH (MANDATORY):\n- Source file: 2281 characters\n- Target summary: ~228 characters (10% compression)\n- Maximum: 274 characters\n- Achieve this by: ultra-dense writing, minimal examples, single-sentence descriptions\n\nULTRA-COMPRESSION TECHNIQUES:\n- Use telegraphic style: \"exports foo, bar, baz\" not \"this file exports three functions\"\n- No code examples unless critical to understanding\n- One sentence per export maximum\n- Omit obvious parameter names (\"opts: Options\" not \"options: Options - configuration object\")\n- Use abbreviations: \"params\" not \"parameters\", \"config\" not \"configuration\"\n- Skip section if empty (don't write \"No special concerns\" or \"N/A\")\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nWORKFLOW & CONVENTION RULES (NEVER EXCLUDE):\n- Contribution requirements: branch naming, commit message formats, PR description templates\n- Testing mandates: required test types, coverage thresholds, \"never delete tests\" policies\n- Tool usage rules: required CLI commands, browser automation tools, linter/formatter requirements\n- Approval workflows: review requirements, CI checks that must pass, deployment gates\n- AI agent instructions: explicit directives for automated assistants (coding rules, prohibited actions)\n- Code conventions: naming standards (camelCase, PascalCase, snake_case for what), import ordering rules, file/folder naming patterns, formatting rules (indentation, quotes, semicolons), language idioms, TypeScript strictness policies, preferred patterns vs anti-patterns\nThese define how contributors (human or AI) must work within the project.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion .annex.sum file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**GeminiBackend is a stub implementation of AIBackend for the Gemini CLI, demonstrating the extension pattern while deferring full implementation until Gemini's JSON output stabilizes.**\n\n## Exports\n\n`GeminiBackend` class implements `AIBackend`:\n- `name = 'gemini'` property\n- `cliCommand = 'gemini'` property\n- `isAvailable(): Promise<boolean>` — checks if `gemini` CLI is on PATH via `isCommandOnPath()`\n- `buildArgs(_options: AICallOptions): string[]` — returns `['-p', '--output-format', 'json']`\n- `parseResponse(_stdout: string, _durationMs: number, _exitCode: number): AIResponse` — throws `AIServiceError` with code `'SUBPROCESS_ERROR'` and message `'Gemini backend is not yet implemented. Use Claude backend.'`\n- `getInstallInstructions(): string` — returns multiline instructions: `'Gemini CLI (experimental):\\n  npm install -g @anthropic-ai/gemini-cli\\n  https://github.com/google-gemini/gemini-cli'`\n\n## Key Dependencies\n\nImports `AIBackend`, `AICallOptions`, `AIResponse` types and `AIServiceError` from `../types.js`; imports `isCommandOnPath` utility from `./claude.js`.\n\n## Behavioral Contract\n\n- `parseResponse()` unconditionally throws `AIServiceError('SUBPROCESS_ERROR', 'Gemini backend is not yet implemented. Use Claude backend.')`\n- `buildArgs()` ignores options parameter and returns fixed argument array for Gemini CLI\n- Implementation deferred pending Gemini CLI JSON output stability (documented in RESEARCH.md Open Question 2)",
      "model": "claude-haiku-4-5-20251001",
      "inputTokens": 3,
      "outputTokens": 417,
      "cacheReadTokens": 16147,
      "cacheCreationTokens": 3431,
      "latencyMs": 16840,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/ai/backends/gemini.ts",
          "sizeBytes": 2281
        }
      ]
    },
    {
      "timestamp": "2026-02-14T13:46:12.984Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: src/generation/prompts/index.ts\n\n```typescript\nexport type {\n  PromptContext,\n} from './types.js';\nexport { SUMMARY_GUIDELINES } from './types.js';\nexport {\n  buildFilePrompt,\n  buildDirectoryPrompt,\n  detectLanguage,\n} from './builder.js';\n\n```\n\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n- Workflow & convention rules: if this file defines contribution guidelines, PR conventions, commit standards, testing mandates, tool usage requirements, approval workflows, AI agent behavioral instructions, or code conventions (naming standards, formatting rules, import ordering, linting policies), extract them as explicit, actionable rules. Sources: CONTRIBUTING.md, CI configs, PR templates, style guides, linter/formatter configs (.eslintrc, .prettierrc, .editorconfig), tsconfig strictness, README dev sections.\n\n\nTARGET LENGTH (MANDATORY):\n- Source file: 194 characters\n- Target summary: ~19 characters (10% compression)\n- Maximum: 23 characters\n- Achieve this by: ultra-dense writing, minimal examples, single-sentence descriptions\n\nULTRA-COMPRESSION TECHNIQUES:\n- Use telegraphic style: \"exports foo, bar, baz\" not \"this file exports three functions\"\n- No code examples unless critical to understanding\n- One sentence per export maximum\n- Omit obvious parameter names (\"opts: Options\" not \"options: Options - configuration object\")\n- Use abbreviations: \"params\" not \"parameters\", \"config\" not \"configuration\"\n- Skip section if empty (don't write \"No special concerns\" or \"N/A\")\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nWORKFLOW & CONVENTION RULES (NEVER EXCLUDE):\n- Contribution requirements: branch naming, commit message formats, PR description templates\n- Testing mandates: required test types, coverage thresholds, \"never delete tests\" policies\n- Tool usage rules: required CLI commands, browser automation tools, linter/formatter requirements\n- Approval workflows: review requirements, CI checks that must pass, deployment gates\n- AI agent instructions: explicit directives for automated assistants (coding rules, prohibited actions)\n- Code conventions: naming standards (camelCase, PascalCase, snake_case for what), import ordering rules, file/folder naming patterns, formatting rules (indentation, quotes, semicolons), language idioms, TypeScript strictness policies, preferred patterns vs anti-patterns\nThese define how contributors (human or AI) must work within the project.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion .annex.sum file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**index.ts is a barrel file aggregating prompt building utilities and type definitions for documentation generation.**\n\n## Exports\n\nRe-exports from `./types.js`:\n- `PromptContext` — type for prompt execution context\n- `SUMMARY_GUIDELINES` — constant defining summary generation rules\n\nRe-exports from `./builder.js`:\n- `buildFilePrompt(...)` — constructs prompts for single-file analysis\n- `buildDirectoryPrompt(...)` — constructs prompts for directory/multi-file analysis\n- `detectLanguage(...)` — identifies programming language from file content or extension\n\n## Module Structure\n\nPublic API surface for `src/generation/prompts/` — all consumers import from this index rather than submodules. Depends on `types.js` (type definitions, guidelines constants) and `builder.js` (prompt generation logic).",
      "model": "claude-haiku-4-5-20251001",
      "inputTokens": 3,
      "outputTokens": 196,
      "cacheReadTokens": 16147,
      "cacheCreationTokens": 2797,
      "latencyMs": 16983,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/generation/prompts/index.ts",
          "sizeBytes": 194
        }
      ]
    },
    {
      "timestamp": "2026-02-14T13:46:12.937Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: src/discovery/filters/vendor.ts\n\n```typescript\n/**\n * Vendor directory filter for file discovery.\n *\n * Excludes files within common vendor/dependency directories that typically\n * contain third-party code not relevant for documentation purposes.\n */\n\nimport path from 'node:path';\nimport type { FileFilter } from '../types.js';\n\n/**\n * Default vendor directories to exclude.\n * These are common directories containing third-party code, build output,\n * or generated files that should not be analyzed.\n */\nexport const DEFAULT_VENDOR_DIRS = [\n  'node_modules',\n  'vendor',\n  '.git',\n  'dist',\n  'build',\n  '__pycache__',\n  '.next',\n  'venv',\n  '.venv',\n  'target',\n] as const;\n\n/**\n * Creates a vendor filter that excludes files within specified directories.\n *\n * Supports two patterns:\n * - Single directory names (e.g., 'node_modules') - matches anywhere in path\n * - Path patterns (e.g., 'apps/vendor' or '.agents/skills') - matches path containing this sequence\n *\n * @param vendorDirs - Array of directory names or path patterns to exclude.\n * @returns A FileFilter that checks if a path is within a vendor directory\n *\n * @example\n * ```typescript\n * const filter = createVendorFilter(['node_modules', '.agents/skills']);\n * filter.shouldExclude('/project/node_modules/lodash/index.js'); // true\n * filter.shouldExclude('/project/apps/foo/.agents/skills/bar.md'); // true\n * filter.shouldExclude('/project/src/utils.js'); // false\n * ```\n */\nexport function createVendorFilter(vendorDirs: string[]): FileFilter {\n  // Separate single segments from path patterns\n  const singleSegments = new Set<string>();\n  const pathPatterns: string[] = [];\n\n  for (const dir of vendorDirs) {\n    // Normalize path separators to current OS\n    const normalized = dir.replace(/[\\\\/]/g, path.sep);\n    if (normalized.includes(path.sep)) {\n      pathPatterns.push(normalized);\n    } else {\n      singleSegments.add(dir);\n    }\n  }\n\n  return {\n    name: 'vendor',\n\n    shouldExclude(absolutePath: string): boolean {\n      // Check single segment matches\n      const segments = absolutePath.split(path.sep);\n      for (const segment of segments) {\n        if (singleSegments.has(segment)) {\n          return true;\n        }\n      }\n\n      // Check path pattern matches\n      for (const pattern of pathPatterns) {\n        if (absolutePath.includes(pattern)) {\n          return true;\n        }\n      }\n\n      return false;\n    },\n  };\n}\n\n```\n\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n- Workflow & convention rules: if this file defines contribution guidelines, PR conventions, commit standards, testing mandates, tool usage requirements, approval workflows, AI agent behavioral instructions, or code conventions (naming standards, formatting rules, import ordering, linting policies), extract them as explicit, actionable rules. Sources: CONTRIBUTING.md, CI configs, PR templates, style guides, linter/formatter configs (.eslintrc, .prettierrc, .editorconfig), tsconfig strictness, README dev sections.\n\n\nTARGET LENGTH (MANDATORY):\n- Source file: 2375 characters\n- Target summary: ~238 characters (10% compression)\n- Maximum: 286 characters\n- Achieve this by: ultra-dense writing, minimal examples, single-sentence descriptions\n\nULTRA-COMPRESSION TECHNIQUES:\n- Use telegraphic style: \"exports foo, bar, baz\" not \"this file exports three functions\"\n- No code examples unless critical to understanding\n- One sentence per export maximum\n- Omit obvious parameter names (\"opts: Options\" not \"options: Options - configuration object\")\n- Use abbreviations: \"params\" not \"parameters\", \"config\" not \"configuration\"\n- Skip section if empty (don't write \"No special concerns\" or \"N/A\")\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nWORKFLOW & CONVENTION RULES (NEVER EXCLUDE):\n- Contribution requirements: branch naming, commit message formats, PR description templates\n- Testing mandates: required test types, coverage thresholds, \"never delete tests\" policies\n- Tool usage rules: required CLI commands, browser automation tools, linter/formatter requirements\n- Approval workflows: review requirements, CI checks that must pass, deployment gates\n- AI agent instructions: explicit directives for automated assistants (coding rules, prohibited actions)\n- Code conventions: naming standards (camelCase, PascalCase, snake_case for what), import ordering rules, file/folder naming patterns, formatting rules (indentation, quotes, semicolons), language idioms, TypeScript strictness policies, preferred patterns vs anti-patterns\nThese define how contributors (human or AI) must work within the project.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion .annex.sum file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**vendor.ts exports a vendor directory filter (createVendorFilter, DEFAULT_VENDOR_DIRS) excluding third-party code from file discovery.**\n\n## Exports\n\n`DEFAULT_VENDOR_DIRS`: readonly array of vendor directory names ('node_modules', 'vendor', '.git', 'dist', 'build', '__pycache__', '.next', 'venv', '.venv', 'target') — directories globally excluded from discovery.\n\n`createVendorFilter(vendorDirs: string[]): FileFilter` — returns filter with `name: 'vendor'` and `shouldExclude(absolutePath: string): boolean` method; filters split input into single-segment directory names (matched anywhere in path via split-and-check) and multi-segment path patterns (matched via substring inclusion after OS-specific path separator normalization).\n\n## Filter Logic\n\n`shouldExclude` splits path by `path.sep`, checks each segment against `singleSegments` Set, then checks full path against all `pathPatterns` via `absolutePath.includes(pattern)`. Returns true if any match found.\n\n## Pattern Matching Behavior\n\nSingle-segment inputs ('node_modules') match as directory names anywhere in path hierarchy; multi-segment inputs ('apps/vendor', '.agents/skills') match as literal path sequences after normalization to OS separators.",
      "model": "claude-haiku-4-5-20251001",
      "inputTokens": 3,
      "outputTokens": 305,
      "cacheReadTokens": 16147,
      "cacheCreationTokens": 3429,
      "latencyMs": 17341,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/discovery/filters/vendor.ts",
          "sizeBytes": 2375
        }
      ]
    },
    {
      "timestamp": "2026-02-14T13:46:12.906Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: src/ai/telemetry/run-log.ts\n\n```typescript\n/**\n * Run log writer for AI telemetry.\n *\n * Writes a completed {@link RunLog} to disk as pretty-printed JSON.\n * Each run produces a single file in `.agents-reverse-engineer/logs/`\n * with a filename derived from the run's start timestamp.\n *\n * @module\n */\n\nimport * as fs from 'node:fs/promises';\nimport * as path from 'node:path';\nimport type { RunLog } from '../types.js';\n\n/** Directory name for telemetry log files (relative to project root) */\nconst LOGS_DIR = '.agents-reverse-engineer/logs';\n\n/**\n * Write a completed run log to disk as pretty-printed JSON.\n *\n * Creates the logs directory if it does not exist. The filename is derived\n * from the run's command, backend, model, and startTime fields with `:` and `.`\n * replaced by `-` so that it forms a valid filename on all platforms.\n *\n * @param projectRoot - Absolute path to the project root directory\n * @param runLog - The completed run log to write\n * @returns Absolute path to the written file\n *\n * @example\n * ```typescript\n * const logPath = await writeRunLog('/home/user/project', runLog);\n * // logPath: /home/user/project/.agents-reverse-engineer/logs/run-generate-claude-sonnet-2026-02-07T12-00-00-000Z.json\n * ```\n */\nexport async function writeRunLog(projectRoot: string, runLog: RunLog): Promise<string> {\n  const logsDir = path.join(projectRoot, LOGS_DIR);\n\n  // Create the logs directory if it does not exist\n  await fs.mkdir(logsDir, { recursive: true });\n\n  // Build filename: include command, backend, model, and timestamp\n  // Replace : and . with - so ISO timestamps become valid filenames\n  const safeTimestamp = runLog.startTime.replace(/[:.]/g, '-');\n  const safeBackend = runLog.backend.toLowerCase().replace(/[^a-z0-9]/g, '-');\n  const safeModel = runLog.model.toLowerCase().replace(/[^a-z0-9]/g, '-');\n  const safeCommand = runLog.command.toLowerCase().replace(/[^a-z0-9]/g, '-');\n  const filename = `run-${safeCommand}-${safeBackend}-${safeModel}-${safeTimestamp}.json`;\n  const filePath = path.join(logsDir, filename);\n\n  // Write pretty-printed JSON\n  await fs.writeFile(filePath, JSON.stringify(runLog, null, 2), 'utf-8');\n\n  return filePath;\n}\n\n```\n\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n- Workflow & convention rules: if this file defines contribution guidelines, PR conventions, commit standards, testing mandates, tool usage requirements, approval workflows, AI agent behavioral instructions, or code conventions (naming standards, formatting rules, import ordering, linting policies), extract them as explicit, actionable rules. Sources: CONTRIBUTING.md, CI configs, PR templates, style guides, linter/formatter configs (.eslintrc, .prettierrc, .editorconfig), tsconfig strictness, README dev sections.\n\n\nTARGET LENGTH (MANDATORY):\n- Source file: 2145 characters\n- Target summary: ~215 characters (10% compression)\n- Maximum: 258 characters\n- Achieve this by: ultra-dense writing, minimal examples, single-sentence descriptions\n\nULTRA-COMPRESSION TECHNIQUES:\n- Use telegraphic style: \"exports foo, bar, baz\" not \"this file exports three functions\"\n- No code examples unless critical to understanding\n- One sentence per export maximum\n- Omit obvious parameter names (\"opts: Options\" not \"options: Options - configuration object\")\n- Use abbreviations: \"params\" not \"parameters\", \"config\" not \"configuration\"\n- Skip section if empty (don't write \"No special concerns\" or \"N/A\")\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nWORKFLOW & CONVENTION RULES (NEVER EXCLUDE):\n- Contribution requirements: branch naming, commit message formats, PR description templates\n- Testing mandates: required test types, coverage thresholds, \"never delete tests\" policies\n- Tool usage rules: required CLI commands, browser automation tools, linter/formatter requirements\n- Approval workflows: review requirements, CI checks that must pass, deployment gates\n- AI agent instructions: explicit directives for automated assistants (coding rules, prohibited actions)\n- Code conventions: naming standards (camelCase, PascalCase, snake_case for what), import ordering rules, file/folder naming patterns, formatting rules (indentation, quotes, semicolons), language idioms, TypeScript strictness policies, preferred patterns vs anti-patterns\nThese define how contributors (human or AI) must work within the project.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion .annex.sum file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**writeRunLog writes completed RunLog telemetry to disk as pretty-printed JSON in .agents-reverse-engineer/logs/ with sanitized filenames.**\n\n## Exports\n\n`writeRunLog(projectRoot: string, runLog: RunLog): Promise<string>` — creates logs directory if missing, generates filename from runLog.command, runLog.backend, runLog.model, runLog.startTime with `:` and `.` replaced by `-`, writes JSON with 2-space indentation, returns absolute file path.\n\n## Constants\n\n`LOGS_DIR = '.agents-reverse-engineer/logs'` — telemetry log directory relative to project root.\n\n## Dependencies\n\n`RunLog` type imported from `../types.js`; uses `node:fs/promises` (mkdir, writeFile) and `node:path` (join).\n\n## Behavioral Contracts\n\nFilename format: `run-{safeCommand}-{safeBackend}-{safeModel}-{safeTimestamp}.json` where safeCommand/backend/model are lowercase with `[^a-z0-9]` replaced by `-`, safeTimestamp is ISO string with `[:.]/g` replaced by `-`. JSON serialized with `null` replacer and `2` space indent. Example output: `run-generate-claude-sonnet-2026-02-07T12-00-00-000Z.json`.",
      "model": "claude-haiku-4-5-20251001",
      "inputTokens": 3,
      "outputTokens": 320,
      "cacheReadTokens": 16147,
      "cacheCreationTokens": 3409,
      "latencyMs": 17680,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/ai/telemetry/run-log.ts",
          "sizeBytes": 2145
        }
      ]
    },
    {
      "timestamp": "2026-02-14T13:46:12.901Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: src/ai/telemetry/logger.ts\n\n```typescript\n/**\n * In-memory telemetry logger for AI service calls.\n *\n * Accumulates {@link TelemetryEntry} instances during a run and computes\n * aggregate summaries. The logger is created once per CLI invocation and\n * finalized when the run completes.\n *\n * @module\n */\n\nimport type { TelemetryEntry, RunLog, FileRead } from '../types.js';\n\n/**\n * Accumulates per-call telemetry entries in memory and produces a\n * complete {@link RunLog} when the run finishes.\n *\n * @example\n * ```typescript\n * const logger = new TelemetryLogger('2026-02-07T12:00:00.000Z', 'Claude', 'sonnet', 'generate');\n * logger.addEntry(entry);\n * const summary = logger.getSummary();\n * const runLog = logger.toRunLog();\n * ```\n */\nexport class TelemetryLogger {\n  /** Unique identifier for this run (ISO timestamp-based) */\n  readonly runId: string;\n\n  /** ISO 8601 timestamp when the run started */\n  readonly startTime: string;\n\n  /** Backend used for this run */\n  readonly backend: string;\n\n  /** Model used for this run */\n  readonly model: string;\n\n  /** Command that triggered this run */\n  readonly command: string;\n\n  /** Accumulated telemetry entries */\n  private readonly entries: TelemetryEntry[] = [];\n\n  /**\n   * Create a new telemetry logger for a run.\n   *\n   * @param runId - Unique run identifier (typically an ISO timestamp)\n   * @param backend - Backend name (e.g., \"Claude\", \"Gemini\", \"OpenCode\")\n   * @param model - Model name (e.g., \"sonnet\", \"opus\", \"haiku\")\n   * @param command - Command name (e.g., \"generate\", \"update\", \"specify\", \"rebuild\")\n   */\n  constructor(runId: string, backend: string, model: string, command: string) {\n    this.runId = runId;\n    this.startTime = new Date().toISOString();\n    this.backend = backend;\n    this.model = model;\n    this.command = command;\n  }\n\n  /**\n   * Record a telemetry entry for a completed AI call.\n   *\n   * @param entry - The telemetry entry to record\n   */\n  addEntry(entry: TelemetryEntry): void {\n    this.entries.push(entry);\n  }\n\n  /**\n   * Get all recorded entries as a read-only array.\n   *\n   * @returns Immutable view of the accumulated entries\n   */\n  getEntries(): readonly TelemetryEntry[] {\n    return this.entries;\n  }\n\n  /**\n   * Update the most recent entry's filesRead array.\n   *\n   * Called by the AI service after the command runner attaches file\n   * metadata to the last call.\n   *\n   * @param filesRead - Array of file-read records to attach\n   */\n  setFilesReadOnLastEntry(filesRead: FileRead[]): void {\n    if (this.entries.length === 0) return;\n    this.entries[this.entries.length - 1]!.filesRead = filesRead;\n  }\n\n  /**\n   * Compute aggregate summary statistics from all recorded entries.\n   *\n   * Totals are computed on every call (not cached) so the summary\n   * always reflects the current state of the entries array.\n   *\n   * @returns Summary with totals for calls, tokens, duration, and errors\n   */\n  getSummary(): RunLog['summary'] {\n    let totalInputTokens = 0;\n    let totalOutputTokens = 0;\n    let totalCacheReadTokens = 0;\n    let totalCacheCreationTokens = 0;\n    let totalDurationMs = 0;\n    let errorCount = 0;\n    let totalFilesRead = 0;\n    const uniqueFilePaths = new Set<string>();\n\n    for (const entry of this.entries) {\n      totalInputTokens += entry.inputTokens;\n      totalOutputTokens += entry.outputTokens;\n      totalCacheReadTokens += entry.cacheReadTokens;\n      totalCacheCreationTokens += entry.cacheCreationTokens;\n      totalDurationMs += entry.latencyMs;\n      if (entry.error !== undefined) {\n        errorCount++;\n      }\n      totalFilesRead += entry.filesRead.length;\n      for (const file of entry.filesRead) {\n        uniqueFilePaths.add(file.path);\n      }\n    }\n\n    return {\n      totalCalls: this.entries.length,\n      totalInputTokens,\n      totalOutputTokens,\n      totalCacheReadTokens,\n      totalCacheCreationTokens,\n      totalDurationMs,\n      errorCount,\n      totalFilesRead,\n      uniqueFilesRead: uniqueFilePaths.size,\n    };\n  }\n\n  /**\n   * Assemble the complete {@link RunLog} for this run.\n   *\n   * Sets `endTime` to the current time, includes all entries, and\n   * computes the summary. Call this once when the run is finished.\n   *\n   * @returns Complete run log ready for serialization\n   */\n  toRunLog(): RunLog {\n    return {\n      runId: this.runId,\n      startTime: this.startTime,\n      endTime: new Date().toISOString(),\n      backend: this.backend,\n      model: this.model,\n      command: this.command,\n      entries: [...this.entries],\n      summary: this.getSummary(),\n    };\n  }\n}\n\n```\n\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n- Workflow & convention rules: if this file defines contribution guidelines, PR conventions, commit standards, testing mandates, tool usage requirements, approval workflows, AI agent behavioral instructions, or code conventions (naming standards, formatting rules, import ordering, linting policies), extract them as explicit, actionable rules. Sources: CONTRIBUTING.md, CI configs, PR templates, style guides, linter/formatter configs (.eslintrc, .prettierrc, .editorconfig), tsconfig strictness, README dev sections.\n\n\nTARGET LENGTH (MANDATORY):\n- Source file: 4529 characters\n- Target summary: ~453 characters (10% compression)\n- Maximum: 544 characters\n- Achieve this by: ultra-dense writing, minimal examples, single-sentence descriptions\n\nULTRA-COMPRESSION TECHNIQUES:\n- Use telegraphic style: \"exports foo, bar, baz\" not \"this file exports three functions\"\n- No code examples unless critical to understanding\n- One sentence per export maximum\n- Omit obvious parameter names (\"opts: Options\" not \"options: Options - configuration object\")\n- Use abbreviations: \"params\" not \"parameters\", \"config\" not \"configuration\"\n- Skip section if empty (don't write \"No special concerns\" or \"N/A\")\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nWORKFLOW & CONVENTION RULES (NEVER EXCLUDE):\n- Contribution requirements: branch naming, commit message formats, PR description templates\n- Testing mandates: required test types, coverage thresholds, \"never delete tests\" policies\n- Tool usage rules: required CLI commands, browser automation tools, linter/formatter requirements\n- Approval workflows: review requirements, CI checks that must pass, deployment gates\n- AI agent instructions: explicit directives for automated assistants (coding rules, prohibited actions)\n- Code conventions: naming standards (camelCase, PascalCase, snake_case for what), import ordering rules, file/folder naming patterns, formatting rules (indentation, quotes, semicolons), language idioms, TypeScript strictness policies, preferred patterns vs anti-patterns\nThese define how contributors (human or AI) must work within the project.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion .annex.sum file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**TelemetryLogger accumulates per-call telemetry data during CLI runs and produces aggregate RunLog summaries.**\n\n## Exports\n\n`TelemetryLogger` class: in-memory accumulator initialized with `runId: string, backend: string, model: string, command: string`. Public properties: `runId`, `startTime` (ISO 8601), `backend`, `model`, `command` (all readonly). Methods:\n- `addEntry(entry: TelemetryEntry): void` — appends call telemetry\n- `getEntries(): readonly TelemetryEntry[]` — returns immutable entry array\n- `setFilesReadOnLastEntry(filesRead: FileRead[]): void` — patches final entry's filesRead\n- `getSummary(): RunLog['summary']` — computes aggregates (totalCalls, totalInputTokens, totalOutputTokens, totalCacheReadTokens, totalCacheCreationTokens, totalDurationMs, errorCount, totalFilesRead, uniqueFilesRead)\n- `toRunLog(): RunLog` — finalizes run with `endTime`, entries array, summary; call once at run completion\n\n## Design\n\nSingle-instance logger per CLI invocation. Entries accumulated as TelemetryEntry objects; summaries computed fresh on each `getSummary()` call (not cached). File tracking via `setFilesReadOnLastEntry()` deferred until after command runner execution. `toRunLog()` snapshot captures current ISO time as `endTime`.\n\n## Dependencies\n\nImports `TelemetryEntry, RunLog, FileRead` from `../types.js`.",
      "model": "claude-haiku-4-5-20251001",
      "inputTokens": 3,
      "outputTokens": 371,
      "cacheReadTokens": 16147,
      "cacheCreationTokens": 4133,
      "latencyMs": 18383,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/ai/telemetry/logger.ts",
          "sizeBytes": 4529
        }
      ]
    },
    {
      "timestamp": "2026-02-14T13:46:12.878Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: src/ai/backends/claude.ts\n\n```typescript\n/**\n * Claude CLI backend adapter.\n *\n * Full implementation of the {@link AIBackend} interface for the Claude Code\n * CLI (`claude`). Builds CLI arguments, parses structured JSON responses\n * with Zod validation, detects CLI availability on PATH, and provides\n * install instructions.\n *\n * The prompt is NOT included in the args array -- it goes to stdin via\n * the subprocess wrapper ({@link runSubprocess}).\n *\n * @module\n */\n\nimport * as fs from 'node:fs/promises';\nimport * as path from 'node:path';\nimport { z } from 'zod';\nimport type { AIBackend, AICallOptions, AIResponse } from '../types.js';\nimport { AIServiceError } from '../types.js';\n\n// ---------------------------------------------------------------------------\n// Zod schema for Claude CLI JSON output\n// ---------------------------------------------------------------------------\n\n/**\n * Schema for Claude CLI JSON result object.\n *\n * Uses `.passthrough()` on nested objects so that new fields added by\n * future CLI versions don't cause validation failures.\n *\n * Supports both legacy single-object output (CLI ≤ 2.1.31) and NDJSON\n * streaming output (CLI ≥ 2.1.38).\n */\nconst ClaudeResponseSchema = z.object({\n  type: z.literal('result'),\n  subtype: z.enum(['success', 'error']),\n  is_error: z.boolean(),\n  duration_ms: z.number(),\n  duration_api_ms: z.number(),\n  num_turns: z.number(),\n  result: z.string(),\n  session_id: z.string(),\n  total_cost_usd: z.number(),\n  usage: z.object({\n    input_tokens: z.number(),\n    cache_creation_input_tokens: z.number(),\n    cache_read_input_tokens: z.number(),\n    output_tokens: z.number(),\n  }).passthrough(),\n  modelUsage: z.record(z.object({\n    inputTokens: z.number(),\n    outputTokens: z.number(),\n    cacheReadInputTokens: z.number(),\n    cacheCreationInputTokens: z.number(),\n    costUSD: z.number(),\n  }).passthrough()),\n}).passthrough();\n\n// ---------------------------------------------------------------------------\n// PATH detection utility\n// ---------------------------------------------------------------------------\n\n/**\n * Check whether a command is available on the system PATH.\n *\n * Splits `process.env.PATH` by the platform delimiter and checks each\n * directory for a file matching the command name. On Windows, also checks\n * each extension from `process.env.PATHEXT` (e.g., `.exe`, `.cmd`, `.bat`).\n *\n * Uses `fs.stat` (not `fs.access` with execute bit) for cross-platform\n * compatibility -- Windows does not have Unix execute permissions.\n *\n * @param command - The bare command name to look for (e.g., \"claude\")\n * @returns `true` if the command exists as a file in any PATH directory\n *\n * @example\n * ```typescript\n * if (await isCommandOnPath('claude')) {\n *   console.log('Claude CLI is available');\n * }\n * ```\n */\nexport async function isCommandOnPath(command: string): Promise<boolean> {\n  const envPath = process.env.PATH ?? '';\n  const pathDirs = envPath\n    .replace(/[\"]+/g, '')\n    .split(path.delimiter)\n    .filter(Boolean);\n\n  // On Windows, PATHEXT lists executable extensions (e.g., \".EXE;.CMD;.BAT\").\n  // On other platforms, PATHEXT is unset; check the bare command name only.\n  const envExt = process.env.PATHEXT ?? '';\n  const extensions = envExt ? envExt.split(';') : [''];\n\n  for (const dir of pathDirs) {\n    for (const ext of extensions) {\n      try {\n        const candidate = path.join(dir, command + ext);\n        const stat = await fs.stat(candidate);\n        if (stat.isFile()) {\n          return true;\n        }\n      } catch {\n        // Not found in this dir/ext combination, continue\n      }\n    }\n  }\n\n  return false;\n}\n\n// ---------------------------------------------------------------------------\n// Claude backend\n// ---------------------------------------------------------------------------\n\n/**\n * Claude Code CLI backend adapter.\n *\n * Implements the {@link AIBackend} interface for the `claude` CLI.\n * This is the primary (and currently only fully implemented) backend.\n *\n * @example\n * ```typescript\n * const backend = new ClaudeBackend();\n * if (await backend.isAvailable()) {\n *   const args = backend.buildArgs({ prompt: 'Summarize this file' });\n *   const result = await runSubprocess('claude', args, {\n *     timeoutMs: 120_000,\n *     input: 'Summarize this file',\n *   });\n *   const response = backend.parseResponse(result.stdout, result.durationMs, result.exitCode);\n * }\n * ```\n */\nexport class ClaudeBackend implements AIBackend {\n  readonly name = 'claude';\n  readonly cliCommand = 'claude';\n\n  /**\n   * Check if the `claude` CLI is available on PATH.\n   */\n  async isAvailable(): Promise<boolean> {\n    return isCommandOnPath(this.cliCommand);\n  }\n\n  /**\n   * Build CLI arguments for a Claude invocation.\n   *\n   * Returns the argument array for `claude -p --output-format json`. The\n   * prompt itself is NOT included -- it goes to stdin via the subprocess\n   * wrapper.\n   *\n   * @param options - Call options (model, systemPrompt, maxTurns)\n   * @returns Argument array suitable for {@link runSubprocess}\n   */\n  buildArgs(options: AICallOptions): string[] {\n    const args: string[] = [\n      '-p',                        // Non-interactive print mode\n      '--output-format', 'json',   // Structured JSON output\n      '--no-session-persistence',  // Don't save session to disk\n      '--allowedTools', 'Read', 'Write',  // Pre-approve minimal tools; avoids bypassPermissions (blocked as root)\n    ];\n\n    if (options.model) {\n      args.push('--model', options.model);\n    }\n\n    if (options.systemPrompt) {\n      args.push('--system-prompt', options.systemPrompt);\n    }\n\n    if (options.maxTurns !== undefined) {\n      args.push('--max-turns', String(options.maxTurns));\n    }\n\n    return args;\n  }\n\n  /**\n   * Parse Claude CLI JSON output into a normalized {@link AIResponse}.\n   *\n   * Supports two output formats:\n   * - **Legacy** (CLI ≤ 2.1.31): Single JSON object on stdout, possibly\n   *   preceded by non-JSON text (upgrade notices, etc.)\n   * - **NDJSON** (CLI ≥ 2.1.38): Multiple newline-delimited JSON objects\n   *   (`system`, `assistant`, `result`). We extract the `{\"type\":\"result\",...}`\n   *   line and parse that.\n   *\n   * Validates the response against the Zod schema and extracts the model\n   * name from `modelUsage`.\n   *\n   * @param stdout - Raw stdout from the Claude CLI process\n   * @param durationMs - Wall-clock duration of the subprocess\n   * @param exitCode - Process exit code\n   * @returns Normalized AI response\n   * @throws {AIServiceError} With code `PARSE_ERROR` if JSON is missing or schema validation fails\n   */\n  parseResponse(stdout: string, durationMs: number, exitCode: number): AIResponse {\n    // Try NDJSON format first: find the line containing the result object.\n    // Claude CLI ≥ 2.1.38 emits multiple JSON lines; we need the \"result\" one.\n    const resultJson = this.extractResultJson(stdout);\n\n    if (resultJson === undefined) {\n      throw new AIServiceError(\n        'PARSE_ERROR',\n        `No JSON result object found in Claude CLI output. Raw output (first 200 chars): ${stdout.slice(0, 200)}`,\n      );\n    }\n\n    let parsed: z.infer<typeof ClaudeResponseSchema>;\n    try {\n      parsed = ClaudeResponseSchema.parse(JSON.parse(resultJson));\n    } catch (error) {\n      const message = error instanceof Error ? error.message : String(error);\n      throw new AIServiceError(\n        'PARSE_ERROR',\n        `Failed to parse Claude CLI JSON response: ${message}`,\n      );\n    }\n\n    // Extract model name from modelUsage keys (first key is the model used)\n    const modelName = Object.keys(parsed.modelUsage)[0] ?? 'unknown';\n\n    return {\n      text: parsed.result,\n      model: modelName,\n      inputTokens: parsed.usage.input_tokens,\n      outputTokens: parsed.usage.output_tokens,\n      cacheReadTokens: parsed.usage.cache_read_input_tokens,\n      cacheCreationTokens: parsed.usage.cache_creation_input_tokens,\n      durationMs,\n      exitCode,\n      raw: parsed,\n    };\n  }\n\n  /**\n   * Extract the result JSON string from Claude CLI stdout.\n   *\n   * Handles three output formats:\n   * - **JSON array** (CLI ≥ 2.1.38): `[{system}, {assistant}, {result}]`\n   *   Parses the array, finds the element with `type: \"result\"`, and\n   *   re-serializes it.\n   * - **NDJSON**: Multiple newline-delimited JSON objects. Finds the\n   *   line with `type: \"result\"`.\n   * - **Legacy** (CLI ≤ 2.1.31): A single JSON object (possibly preceded\n   *   by non-JSON text).\n   *\n   * @param stdout - Raw stdout from the CLI process\n   * @returns The JSON string for the result object, or `undefined` if not found\n   */\n  private extractResultJson(stdout: string): string | undefined {\n    const trimmed = stdout.trim();\n\n    // Strategy 1: JSON array — `[{...}, {...}, {...}]`\n    if (trimmed.startsWith('[')) {\n      try {\n        const arr = JSON.parse(trimmed) as Array<Record<string, unknown>>;\n        const result = arr.find((item) => item.type === 'result');\n        if (result !== undefined) {\n          return JSON.stringify(result);\n        }\n      } catch {\n        // Not a valid JSON array; fall through to other strategies\n      }\n    }\n\n    // Strategy 2: NDJSON — look for a line that is the result object\n    const lines = stdout.split('\\n');\n    for (const line of lines) {\n      const l = line.trim();\n      if (!l.startsWith('{')) continue;\n      try {\n        const obj = JSON.parse(l) as Record<string, unknown>;\n        if (obj.type === 'result') {\n          return l;\n        }\n      } catch {\n        // Not a complete JSON line, skip\n      }\n    }\n\n    // Strategy 3: Legacy single-object — find first `{` and try to parse\n    const jsonStart = stdout.indexOf('{');\n    if (jsonStart !== -1) {\n      return stdout.slice(jsonStart);\n    }\n\n    return undefined;\n  }\n\n  /**\n   * Get user-facing install instructions for the Claude CLI.\n   */\n  getInstallInstructions(): string {\n    return [\n      'Claude Code (recommended):',\n      '  npm install -g @anthropic-ai/claude-code',\n      '  https://code.claude.com',\n    ].join('\\n');\n  }\n}\n\n```\n\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n- Workflow & convention rules: if this file defines contribution guidelines, PR conventions, commit standards, testing mandates, tool usage requirements, approval workflows, AI agent behavioral instructions, or code conventions (naming standards, formatting rules, import ordering, linting policies), extract them as explicit, actionable rules. Sources: CONTRIBUTING.md, CI configs, PR templates, style guides, linter/formatter configs (.eslintrc, .prettierrc, .editorconfig), tsconfig strictness, README dev sections.\n\n\nTARGET LENGTH (MANDATORY):\n- Source file: 10016 characters\n- Target summary: ~1002 characters (10% compression)\n- Maximum: 1202 characters\n- Achieve this by: ultra-dense writing, minimal examples, single-sentence descriptions\n\nULTRA-COMPRESSION TECHNIQUES:\n- Use telegraphic style: \"exports foo, bar, baz\" not \"this file exports three functions\"\n- No code examples unless critical to understanding\n- One sentence per export maximum\n- Omit obvious parameter names (\"opts: Options\" not \"options: Options - configuration object\")\n- Use abbreviations: \"params\" not \"parameters\", \"config\" not \"configuration\"\n- Skip section if empty (don't write \"No special concerns\" or \"N/A\")\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nWORKFLOW & CONVENTION RULES (NEVER EXCLUDE):\n- Contribution requirements: branch naming, commit message formats, PR description templates\n- Testing mandates: required test types, coverage thresholds, \"never delete tests\" policies\n- Tool usage rules: required CLI commands, browser automation tools, linter/formatter requirements\n- Approval workflows: review requirements, CI checks that must pass, deployment gates\n- AI agent instructions: explicit directives for automated assistants (coding rules, prohibited actions)\n- Code conventions: naming standards (camelCase, PascalCase, snake_case for what), import ordering rules, file/folder naming patterns, formatting rules (indentation, quotes, semicolons), language idioms, TypeScript strictness policies, preferred patterns vs anti-patterns\nThese define how contributors (human or AI) must work within the project.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion .annex.sum file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**claude.ts implements the AIBackend interface for the Claude Code CLI, handling argument construction, JSON response parsing (legacy and NDJSON formats), and PATH availability detection.**\n\n## Exports\n\n`isCommandOnPath(command: string): Promise<boolean>` — detects if a command exists in `process.env.PATH`, accounting for Windows `PATHEXT` extensions; uses `fs.stat()` instead of execute-bit checks for cross-platform compatibility.\n\n`ClaudeBackend` class implements `AIBackend` with four public methods:\n- `isAvailable(): Promise<boolean>` — checks `claude` CLI availability via `isCommandOnPath()`\n- `buildArgs(options: AICallOptions): string[]` — constructs CLI args: `-p --output-format json --no-session-persistence --allowedTools Read Write` plus optional `--model`, `--system-prompt`, `--max-turns`\n- `parseResponse(stdout: string, durationMs: number, exitCode: number): AIResponse` — validates JSON against `ClaudeResponseSchema` (Zod), extracts model name from first `modelUsage` key, returns normalized `AIResponse` with token usage and cost\n- `getInstallInstructions(): string` — returns npm install command for `@anthropic-ai/claude-code`\n\n## Response Parsing\n\n`ClaudeResponseSchema` (Zod object schema) validates Claude CLI output with `.passthrough()` on nested objects to tolerate future CLI field additions. Schema requires: `type: 'result'`, `subtype: 'success'|'error'`, `is_error: boolean`, durations, `result: string`, `session_id`, `total_cost_usd`, `usage` (input/output/cache tokens), `modelUsage` object map.\n\n`extractResultJson(stdout: string): string | undefined` (private) handles three CLI output formats in priority order: JSON array `[{system}, {assistant}, {result}]` (CLI ≥ 2.1.38), NDJSON newline-delimited objects, legacy single JSON object. Returns stringified result object or undefined if extraction fails.\n\n## Error Handling\n\n`parseResponse()` throws `AIServiceError` with code `'PARSE_ERROR'` if result JSON is missing (first 200 chars logged) or schema validation fails (error message included).\n\n## Design Notes\n\nPrompt is NOT included in CLI args; it goes to stdin via subprocess wrapper. `--no-session-persistence` disables disk session saving. `--allowedTools Read Write` pre-approves only essential tools, avoiding `bypassPermissions` block when running as root.",
      "model": "claude-haiku-4-5-20251001",
      "inputTokens": 3,
      "outputTokens": 610,
      "cacheReadTokens": 16147,
      "cacheCreationTokens": 5682,
      "latencyMs": 19170,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/ai/backends/claude.ts",
          "sizeBytes": 10036
        }
      ]
    },
    {
      "timestamp": "2026-02-14T13:46:12.894Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: src/ai/providers/subprocess.ts\n\n```typescript\n/**\n * Subprocess-based AI provider.\n *\n * Wraps an {@link AIBackend} to implement the {@link AIProvider} interface\n * by spawning CLI subprocesses. This is the default provider used by the\n * CLI commands and preserves the existing behavior exactly.\n *\n * @module\n */\n\nimport { writeFile, mkdir } from 'node:fs/promises';\nimport path from 'node:path';\nimport type { AIProvider, AIBackend, AICallOptions, AIResponse } from '../types.js';\nimport { AIServiceError } from '../types.js';\nimport { runSubprocess } from '../subprocess.js';\nimport type { SubprocessResult } from '../types.js';\nimport type { ITraceWriter } from '../../orchestration/trace.js';\nimport type { Logger } from '../../core/logger.js';\nimport { nullLogger } from '../../core/logger.js';\n\n/** Patterns in stderr that indicate a transient rate-limit error */\nconst RATE_LIMIT_PATTERNS = [\n  'rate limit',\n  '429',\n  'too many requests',\n  'overloaded',\n];\n\nfunction isRateLimitStderr(stderr: string): boolean {\n  const lower = stderr.toLowerCase();\n  return RATE_LIMIT_PATTERNS.some((pattern) => lower.includes(pattern));\n}\n\nfunction formatBytes(bytes: number): string {\n  if (bytes < 1024) return `${bytes}B`;\n  if (bytes < 1024 * 1024) return `${(bytes / 1024).toFixed(1)}KB`;\n  return `${(bytes / (1024 * 1024)).toFixed(1)}MB`;\n}\n\n/**\n * Options for the subprocess provider.\n */\nexport interface SubprocessProviderOptions {\n  /** Default subprocess timeout in milliseconds */\n  timeoutMs: number;\n  /** Whether debug mode is enabled */\n  debug?: boolean;\n  /** Logger for debug/warn output */\n  logger?: Logger;\n  /** Trace writer for subprocess events */\n  tracer?: ITraceWriter | null;\n}\n\n/**\n * AI provider that spawns CLI subprocesses via an {@link AIBackend}.\n *\n * This is the provider used by existing CLI commands. It preserves the\n * exact subprocess spawning, timeout enforcement, rate-limit detection,\n * and response parsing behavior.\n *\n * @example\n * ```typescript\n * import { SubprocessProvider } from './providers/subprocess.js';\n * import { ClaudeBackend } from '../backends/claude.js';\n *\n * const provider = new SubprocessProvider(new ClaudeBackend(), {\n *   timeoutMs: 120_000,\n * });\n * const response = await provider.call({ prompt: 'Hello' });\n * ```\n */\nexport class SubprocessProvider implements AIProvider {\n  readonly backend: AIBackend;\n  private readonly timeoutMs: number;\n  private readonly debug: boolean;\n  private readonly log: Logger;\n  private tracer: ITraceWriter | null;\n\n  /** Number of currently active subprocesses (for debug logging) */\n  private activeCount: number = 0;\n\n  /** Directory for subprocess output logs (null = disabled) */\n  private subprocessLogDir: string | null = null;\n\n  /** Serializes log writes so concurrent workers don't interleave mkdirs */\n  private logWriteQueue: Promise<void> = Promise.resolve();\n\n  constructor(backend: AIBackend, options: SubprocessProviderOptions) {\n    this.backend = backend;\n    this.timeoutMs = options.timeoutMs;\n    this.debug = options.debug ?? false;\n    this.log = options.logger ?? nullLogger;\n    this.tracer = options.tracer ?? null;\n  }\n\n  /**\n   * Set the trace writer (allows late binding after construction).\n   */\n  setTracer(tracer: ITraceWriter): void {\n    this.tracer = tracer;\n  }\n\n  /**\n   * Set a directory for writing subprocess stdout/stderr log files.\n   */\n  setSubprocessLogDir(dir: string): void {\n    this.subprocessLogDir = dir;\n  }\n\n  async call(options: AICallOptions): Promise<AIResponse> {\n    const taskLabel = options.taskLabel ?? 'unknown';\n    const timeoutMs = options.timeoutMs ?? this.timeoutMs;\n    const args = this.backend.buildArgs(options);\n\n    if (this.debug) {\n      const mem = process.memoryUsage();\n      this.log.debug(\n        `[debug] Spawning subprocess for \"${taskLabel}\" ` +\n        `(active: ${this.activeCount}, ` +\n        `heapUsed: ${formatBytes(mem.heapUsed)}, ` +\n        `rss: ${formatBytes(mem.rss)}, ` +\n        `timeout: ${(timeoutMs / 1000).toFixed(0)}s)`,\n      );\n    }\n\n    this.activeCount++;\n\n    const stdinInput = this.backend.composeStdinInput?.(options) ?? options.prompt;\n\n    const result = await runSubprocess(this.backend.cliCommand, args, {\n      timeoutMs,\n      input: stdinInput,\n      onSpawn: (pid) => {\n        this.tracer?.emit({\n          type: 'subprocess:spawn',\n          childPid: pid ?? -1,\n          command: this.backend.cliCommand,\n          taskLabel,\n        });\n      },\n    });\n\n    this.activeCount--;\n\n    // Emit subprocess:exit trace\n    if (this.tracer && result.childPid !== undefined) {\n      this.tracer.emit({\n        type: 'subprocess:exit',\n        childPid: result.childPid,\n        command: this.backend.cliCommand,\n        taskLabel,\n        exitCode: result.exitCode,\n        signal: result.signal,\n        durationMs: result.durationMs,\n        timedOut: result.timedOut,\n      });\n    }\n\n    // Write subprocess output log (fire-and-forget, non-critical)\n    this.enqueueSubprocessLog(result, taskLabel);\n\n    if (result.timedOut) {\n      this.log.warn(\n        `[warn] Subprocess timed out after ${(result.durationMs / 1000).toFixed(1)}s ` +\n        `for \"${taskLabel}\" (PID ${result.childPid ?? 'unknown'}, ` +\n        `timeout was ${(timeoutMs / 1000).toFixed(0)}s)`,\n      );\n      throw new AIServiceError('TIMEOUT', 'Subprocess timed out');\n    }\n\n    if (this.debug) {\n      this.log.debug(\n        `[debug] Subprocess exited for \"${taskLabel}\" ` +\n        `(PID ${result.childPid ?? 'unknown'}, ` +\n        `exitCode: ${result.exitCode}, ` +\n        `duration: ${(result.durationMs / 1000).toFixed(1)}s, ` +\n        `active: ${this.activeCount})`,\n      );\n    }\n\n    if (result.exitCode !== 0) {\n      if (isRateLimitStderr(result.stderr)) {\n        throw new AIServiceError(\n          'RATE_LIMIT',\n          `Rate limited by ${this.backend.name}: ${result.stderr.slice(0, 200)}`,\n        );\n      }\n      throw new AIServiceError(\n        'SUBPROCESS_ERROR',\n        `${this.backend.name} CLI exited with code ${result.exitCode}: ${result.stderr.slice(0, 500)}`,\n      );\n    }\n\n    // Parse the response\n    try {\n      return this.backend.parseResponse(result.stdout, result.durationMs, result.exitCode);\n    } catch (error) {\n      if (error instanceof AIServiceError) {\n        throw error;\n      }\n      const message = error instanceof Error ? error.message : String(error);\n      throw new AIServiceError('PARSE_ERROR', `Failed to parse response: ${message}`);\n    }\n  }\n\n  /**\n   * Enqueue a subprocess output log write.\n   *\n   * Serializes writes via a promise chain to avoid concurrent mkdir races.\n   * Failures are silently swallowed -- log writing is non-critical.\n   */\n  private enqueueSubprocessLog(result: SubprocessResult, taskLabel: string): void {\n    if (this.subprocessLogDir === null) return;\n\n    const dir = this.subprocessLogDir;\n    const sanitized = taskLabel.replace(/\\//g, '--').replace(/[^a-zA-Z0-9._-]/g, '_');\n    const filename = `${sanitized}_pid${result.childPid ?? 0}.log`;\n    const filePath = path.join(dir, filename);\n\n    const content =\n      `task:      ${taskLabel}\\n` +\n      `pid:       ${result.childPid ?? 'unknown'}\\n` +\n      `command:   ${this.backend.cliCommand}\\n` +\n      `exit:      ${result.exitCode}\\n` +\n      `signal:    ${result.signal ?? 'none'}\\n` +\n      `duration:  ${result.durationMs}ms\\n` +\n      `timed_out: ${result.timedOut}\\n` +\n      `\\n--- stdout ---\\n` +\n      result.stdout +\n      `\\n--- stderr ---\\n` +\n      result.stderr;\n\n    this.logWriteQueue = this.logWriteQueue\n      .then(async () => {\n        await mkdir(dir, { recursive: true });\n        await writeFile(filePath, content, 'utf-8');\n      })\n      .catch(() => { /* non-critical -- log loss is acceptable */ });\n  }\n}\n\n```\n\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n- Workflow & convention rules: if this file defines contribution guidelines, PR conventions, commit standards, testing mandates, tool usage requirements, approval workflows, AI agent behavioral instructions, or code conventions (naming standards, formatting rules, import ordering, linting policies), extract them as explicit, actionable rules. Sources: CONTRIBUTING.md, CI configs, PR templates, style guides, linter/formatter configs (.eslintrc, .prettierrc, .editorconfig), tsconfig strictness, README dev sections.\n\n\nTARGET LENGTH (MANDATORY):\n- Source file: 7752 characters\n- Target summary: ~775 characters (10% compression)\n- Maximum: 930 characters\n- Achieve this by: ultra-dense writing, minimal examples, single-sentence descriptions\n\nULTRA-COMPRESSION TECHNIQUES:\n- Use telegraphic style: \"exports foo, bar, baz\" not \"this file exports three functions\"\n- No code examples unless critical to understanding\n- One sentence per export maximum\n- Omit obvious parameter names (\"opts: Options\" not \"options: Options - configuration object\")\n- Use abbreviations: \"params\" not \"parameters\", \"config\" not \"configuration\"\n- Skip section if empty (don't write \"No special concerns\" or \"N/A\")\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nWORKFLOW & CONVENTION RULES (NEVER EXCLUDE):\n- Contribution requirements: branch naming, commit message formats, PR description templates\n- Testing mandates: required test types, coverage thresholds, \"never delete tests\" policies\n- Tool usage rules: required CLI commands, browser automation tools, linter/formatter requirements\n- Approval workflows: review requirements, CI checks that must pass, deployment gates\n- AI agent instructions: explicit directives for automated assistants (coding rules, prohibited actions)\n- Code conventions: naming standards (camelCase, PascalCase, snake_case for what), import ordering rules, file/folder naming patterns, formatting rules (indentation, quotes, semicolons), language idioms, TypeScript strictness policies, preferred patterns vs anti-patterns\nThese define how contributors (human or AI) must work within the project.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion .annex.sum file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**SubprocessProvider wraps an AIBackend to spawn CLI subprocesses implementing the AIProvider interface, preserving legacy CLI behavior with timeout enforcement, rate-limit detection, and structured logging.**\n\n## Exports\n\n`SubprocessProvider` — class implementing `AIProvider`; constructor takes `AIBackend` and `SubprocessProviderOptions`, exposes `backend` property, `call(options: AICallOptions): Promise<AIResponse>`, `setTracer(tracer: ITraceWriter): void`, `setSubprocessLogDir(dir: string): void`.\n\n`SubprocessProviderOptions` — interface with `timeoutMs: number` (required), `debug?: boolean`, `logger?: Logger`, `tracer?: ITraceWriter | null`.\n\n## Rate-Limit Detection\n\n`RATE_LIMIT_PATTERNS` array: `['rate limit', '429', 'too many requests', 'overloaded']`. `isRateLimitStderr(stderr: string): boolean` performs case-insensitive substring match against patterns; if matched, `call()` throws `AIServiceError('RATE_LIMIT', ...)`.\n\n## Error Handling\n\n`call()` distinguishes three error modes: timeout (after `timeoutMs` elapsed) throws `AIServiceError('TIMEOUT', 'Subprocess timed out')`; non-zero exit with rate-limit stderr throws `AIServiceError('RATE_LIMIT', ...)`; other non-zero exit throws `AIServiceError('SUBPROCESS_ERROR', ...)`; parse failures throw `AIServiceError('PARSE_ERROR', ...)`. All error messages truncate stderr/stdout to first 200–500 chars.\n\n## Subprocess Invocation\n\n`call()` invokes `runSubprocess(this.backend.cliCommand, args, {timeoutMs, input, onSpawn})` where args come from `backend.buildArgs(options)` and input defaults to `options.prompt` or uses `backend.composeStdinInput(options)`. Emits `subprocess:spawn` trace with `{type, childPid, command, taskLabel}` on spawn and `subprocess:exit` trace with `{type, childPid, command, taskLabel, exitCode, signal, durationMs, timedOut}` on exit.\n\n## Debug & Tracing\n\nWhen `debug: true`, logs pre-spawn memory snapshot (`heapUsed`, `rss` via `formatBytes()`) and post-exit exit code/duration to `logger.debug()`. `tracer` (optional `ITraceWriter`) receives structured subprocess lifecycle events. `activeCount` tracks concurrent subprocesses for memory profiling context.\n\n## Subprocess Output Logging\n\n`setSubprocessLogDir(dir: string)` enables file-based logging. `enqueueSubprocessLog()` writes per-subprocess logs asynchronously with serialized `mkdir`/`writeFile` to prevent concurrent races. Log format: `task`, `pid`, `command`, `exit`, `signal`, `duration`, `timed_out`, then `--- stdout ---` and `--- stderr ---` sections. Failures silently ignored (non-critical).\n\n## Memory & Concurrency\n\n`logWriteQueue: Promise<void>` serializes all log writes via promise chaining; each write awaits `mkdir(dir, {recursive: true})` then `writeFile()`. Sanitizes taskLabel for filenames: replaces `/` with `--`, strips non-alphanumeric except `.`, `_`, `-`.",
      "model": "claude-haiku-4-5-20251001",
      "inputTokens": 3,
      "outputTokens": 795,
      "cacheReadTokens": 0,
      "cacheCreationTokens": 21298,
      "latencyMs": 19686,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/ai/providers/subprocess.ts",
          "sizeBytes": 7752
        }
      ]
    },
    {
      "timestamp": "2026-02-14T13:46:12.884Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: src/ai/backends/codex.ts\n\n```typescript\n/**\n * Codex CLI backend adapter.\n *\n * Implements the {@link AIBackend} interface for the Codex CLI (`codex`).\n * Uses `codex exec --json` and parses JSONL events, with a plain-text\n * fallback for compatibility with CLI output changes.\n *\n * @module\n */\n\nimport type { AIBackend, AICallOptions, AIResponse } from '../types.js';\nimport { AIServiceError } from '../types.js';\nimport { isCommandOnPath } from './claude.js';\n\ninterface CodexUsageTotals {\n  inputTokens: number;\n  outputTokens: number;\n  cacheReadTokens: number;\n  cacheCreationTokens: number;\n}\n\n/**\n * Wrap system instructions for CLIs that do not expose a dedicated\n * system-prompt flag.\n */\nfunction composePromptWithSystem(options: AICallOptions): string {\n  if (options.systemPrompt) {\n    return `<system-instructions>\\n${options.systemPrompt}\\n</system-instructions>\\n\\n${options.prompt}`;\n  }\n  return options.prompt;\n}\n\nfunction asRecord(value: unknown): Record<string, unknown> | null {\n  if (value !== null && typeof value === 'object' && !Array.isArray(value)) {\n    return value as Record<string, unknown>;\n  }\n  return null;\n}\n\nfunction asString(value: unknown): string | undefined {\n  return typeof value === 'string' ? value : undefined;\n}\n\nfunction asNumber(value: unknown): number | undefined {\n  return typeof value === 'number' && Number.isFinite(value) ? value : undefined;\n}\n\n/**\n * Recursively collect textual payloads from parsed JSON events.\n *\n * Codex JSONL can evolve across versions, so extraction intentionally\n * accepts multiple shapes (e.g., `text`, nested arrays, output content).\n */\nfunction collectText(\n  value: unknown,\n  out: string[],\n  shouldSkipObject?: (obj: Record<string, unknown>) => boolean,\n): void {\n  if (typeof value === 'string') {\n    return;\n  }\n  if (Array.isArray(value)) {\n    for (const item of value) {\n      collectText(item, out, shouldSkipObject);\n    }\n    return;\n  }\n  const obj = asRecord(value);\n  if (!obj) {\n    return;\n  }\n\n  if (shouldSkipObject?.(obj)) {\n    return;\n  }\n\n  // Common payload key used by responses/events.\n  if (typeof obj.text === 'string' && obj.text.trim().length > 0) {\n    out.push(obj.text.trim());\n  }\n\n  for (const nested of Object.values(obj)) {\n    collectText(nested, out, shouldSkipObject);\n  }\n}\n\nfunction shouldSkipTextObject(obj: Record<string, unknown>): boolean {\n  const type = asString(obj.type) ?? '';\n  if (type === 'reasoning' || type.endsWith('.reasoning')) {\n    return true;\n  }\n  return false;\n}\n\n/**\n * Extract assistant-facing text from Codex `item.completed` payloads.\n *\n * Keeps assistant output (`agent_message`) and drops model reasoning.\n */\nfunction extractAssistantTextFromItem(item: unknown): string[] {\n  const itemObj = asRecord(item);\n  if (!itemObj) {\n    return [];\n  }\n\n  const itemType = asString(itemObj.type) ?? '';\n  if (itemType !== 'agent_message') {\n    return [];\n  }\n\n  const textParts: string[] = [];\n\n  const directText = asString(itemObj.text);\n  if (directText && directText.trim().length > 0) {\n    textParts.push(directText.trim());\n  }\n\n  const content = itemObj.content;\n  if (Array.isArray(content)) {\n    for (const part of content) {\n      const partObj = asRecord(part);\n      if (!partObj) continue;\n      const partType = asString(partObj.type) ?? '';\n      const partText = asString(partObj.text);\n      if ((partType === 'text' || partType === 'output_text') && partText && partText.trim().length > 0) {\n        textParts.push(partText.trim());\n      }\n    }\n  }\n\n  return textParts;\n}\n\n/**\n * Extract token usage from Codex `turn.completed` events.\n */\nfunction extractUsageFromTurnCompleted(usage: unknown): CodexUsageTotals {\n  const usageObj = asRecord(usage);\n  if (!usageObj) {\n    return {\n      inputTokens: 0,\n      outputTokens: 0,\n      cacheReadTokens: 0,\n      cacheCreationTokens: 0,\n    };\n  }\n\n  const rawInput = asNumber(usageObj.input_tokens)\n    ?? asNumber(usageObj.inputTokens)\n    ?? 0;\n  const cacheRead = asNumber(usageObj.cached_input_tokens)\n    ?? asNumber(usageObj.cache_read_input_tokens)\n    ?? asNumber(usageObj.cacheReadInputTokens)\n    ?? 0;\n  const cacheCreation = asNumber(usageObj.cache_creation_input_tokens)\n    ?? asNumber(usageObj.cacheCreationInputTokens)\n    ?? 0;\n  const output = asNumber(usageObj.output_tokens)\n    ?? asNumber(usageObj.outputTokens)\n    ?? 0;\n\n  // Preserve ARE's token semantics:\n  // - inputTokens: non-cached input\n  // - cacheReadTokens/cacheCreationTokens: cached components\n  const nonCachedInput = rawInput >= cacheRead ? rawInput - cacheRead : rawInput;\n\n  return {\n    inputTokens: nonCachedInput,\n    outputTokens: output,\n    cacheReadTokens: cacheRead,\n    cacheCreationTokens: cacheCreation,\n  };\n}\n\n/**\n * Remove duplicate lines while preserving insertion order.\n */\nfunction uniq(items: string[]): string[] {\n  const seen = new Set<string>();\n  const out: string[] = [];\n  for (const item of items) {\n    if (!seen.has(item)) {\n      seen.add(item);\n      out.push(item);\n    }\n  }\n  return out;\n}\n\n/**\n * Codex CLI backend adapter.\n */\nexport class CodexBackend implements AIBackend {\n  readonly name = 'codex';\n  readonly cliCommand = 'codex';\n\n  async isAvailable(): Promise<boolean> {\n    return isCommandOnPath(this.cliCommand);\n  }\n\n  buildArgs(options: AICallOptions): string[] {\n    const args: string[] = [\n      // Approval policy is a global codex flag, so it must come before the\n      // `exec` subcommand (newer CLIs reject it after `exec`).\n      '-a',\n      'never',\n      'exec',\n      '--json',\n      '--skip-git-repo-check',\n      '--ephemeral',\n      '--color',\n      'never',\n    ];\n\n    if (options.model) {\n      args.push('--model', options.model);\n    }\n\n    // Explicit stdin mode for consistent subprocess behavior.\n    args.push('-');\n\n    return args;\n  }\n\n  composeStdinInput(options: AICallOptions): string {\n    return composePromptWithSystem(options);\n  }\n\n  parseResponse(stdout: string, durationMs: number, exitCode: number): AIResponse {\n    const trimmed = stdout.trim();\n    if (trimmed.length === 0) {\n      throw new AIServiceError('PARSE_ERROR', 'Empty Codex CLI output');\n    }\n\n    const lines = trimmed\n      .split('\\n')\n      .map((line) => line.trim())\n      .filter((line) => line.length > 0);\n\n    let parsedJsonLines = 0;\n    const textParts: string[] = [];\n    const parsedEvents: Record<string, unknown>[] = [];\n    const usageTotals: CodexUsageTotals = {\n      inputTokens: 0,\n      outputTokens: 0,\n      cacheReadTokens: 0,\n      cacheCreationTokens: 0,\n    };\n    let model = 'unknown';\n\n    for (const line of lines) {\n      if (!line.startsWith('{') && !line.startsWith('[')) {\n        continue;\n      }\n\n      let json: unknown;\n      try {\n        json = JSON.parse(line);\n      } catch {\n        continue;\n      }\n\n      const events: Record<string, unknown>[] = [];\n      if (Array.isArray(json)) {\n        for (const item of json) {\n          const obj = asRecord(item);\n          if (obj) events.push(obj);\n        }\n      } else {\n        const obj = asRecord(json);\n        if (obj) events.push(obj);\n      }\n\n      for (const obj of events) {\n        parsedJsonLines++;\n        parsedEvents.push(obj);\n\n        const type = asString(obj.type) ?? '';\n        if (\n          type === 'error' ||\n          type === 'thread.started' ||\n          type === 'turn.started' ||\n          type.endsWith('.error') ||\n          type.endsWith('.failed')\n        ) {\n          continue;\n        }\n\n        const directModel = asString(obj.model);\n        if (directModel && directModel.length > 0) {\n          model = directModel;\n        }\n\n        // Preferred path: Codex JSON stream emits assistant output as item.completed.\n        if (type === 'item.completed') {\n          textParts.push(...extractAssistantTextFromItem(obj.item));\n          continue;\n        }\n\n        if (type === 'turn.completed') {\n          const usage = extractUsageFromTurnCompleted(obj.usage);\n          usageTotals.inputTokens += usage.inputTokens;\n          usageTotals.outputTokens += usage.outputTokens;\n          usageTotals.cacheReadTokens += usage.cacheReadTokens;\n          usageTotals.cacheCreationTokens += usage.cacheCreationTokens;\n          continue;\n        }\n      }\n    }\n\n    // Preferred path: assistant message items only (no reasoning leakage).\n    const extracted = uniq(textParts).join('\\n').trim();\n    if (extracted.length > 0) {\n      return {\n        text: extracted,\n        model,\n        inputTokens: usageTotals.inputTokens,\n        outputTokens: usageTotals.outputTokens,\n        cacheReadTokens: usageTotals.cacheReadTokens,\n        cacheCreationTokens: usageTotals.cacheCreationTokens,\n        durationMs,\n        exitCode,\n        raw: { format: 'jsonl', lineCount: parsedJsonLines },\n      };\n    }\n\n    // Compatibility fallback: extract text from JSON objects while skipping reasoning nodes.\n    if (parsedJsonLines > 0) {\n      const fallbackParts: string[] = [];\n      for (const event of parsedEvents) {\n        collectText(event, fallbackParts, shouldSkipTextObject);\n      }\n      const fallbackExtracted = uniq(fallbackParts).join('\\n').trim();\n      if (fallbackExtracted.length > 0) {\n        return {\n          text: fallbackExtracted,\n          model,\n          inputTokens: usageTotals.inputTokens,\n          outputTokens: usageTotals.outputTokens,\n          cacheReadTokens: usageTotals.cacheReadTokens,\n          cacheCreationTokens: usageTotals.cacheCreationTokens,\n          durationMs,\n          exitCode,\n          raw: { format: 'jsonl-fallback', lineCount: parsedJsonLines },\n        };\n      }\n    }\n\n    // Compatibility fallback: treat stdout as the final message body.\n    if (parsedJsonLines === 0 && trimmed.length > 0) {\n      return {\n        text: trimmed,\n        model,\n        inputTokens: usageTotals.inputTokens,\n        outputTokens: usageTotals.outputTokens,\n        cacheReadTokens: usageTotals.cacheReadTokens,\n        cacheCreationTokens: usageTotals.cacheCreationTokens,\n        durationMs,\n        exitCode,\n        raw: { format: 'text' },\n      };\n    }\n\n    throw new AIServiceError(\n      'PARSE_ERROR',\n      'Failed to extract assistant text from Codex CLI output',\n    );\n  }\n\n  getInstallInstructions(): string {\n    return [\n      'Codex CLI:',\n      '  npm install -g @openai/codex',\n      '  https://github.com/openai/codex',\n    ].join('\\n');\n  }\n}\n\n```\n\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n- Workflow & convention rules: if this file defines contribution guidelines, PR conventions, commit standards, testing mandates, tool usage requirements, approval workflows, AI agent behavioral instructions, or code conventions (naming standards, formatting rules, import ordering, linting policies), extract them as explicit, actionable rules. Sources: CONTRIBUTING.md, CI configs, PR templates, style guides, linter/formatter configs (.eslintrc, .prettierrc, .editorconfig), tsconfig strictness, README dev sections.\n\n\nTARGET LENGTH (MANDATORY):\n- Source file: 10375 characters\n- Target summary: ~1038 characters (10% compression)\n- Maximum: 1246 characters\n- Achieve this by: ultra-dense writing, minimal examples, single-sentence descriptions\n\nULTRA-COMPRESSION TECHNIQUES:\n- Use telegraphic style: \"exports foo, bar, baz\" not \"this file exports three functions\"\n- No code examples unless critical to understanding\n- One sentence per export maximum\n- Omit obvious parameter names (\"opts: Options\" not \"options: Options - configuration object\")\n- Use abbreviations: \"params\" not \"parameters\", \"config\" not \"configuration\"\n- Skip section if empty (don't write \"No special concerns\" or \"N/A\")\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nWORKFLOW & CONVENTION RULES (NEVER EXCLUDE):\n- Contribution requirements: branch naming, commit message formats, PR description templates\n- Testing mandates: required test types, coverage thresholds, \"never delete tests\" policies\n- Tool usage rules: required CLI commands, browser automation tools, linter/formatter requirements\n- Approval workflows: review requirements, CI checks that must pass, deployment gates\n- AI agent instructions: explicit directives for automated assistants (coding rules, prohibited actions)\n- Code conventions: naming standards (camelCase, PascalCase, snake_case for what), import ordering rules, file/folder naming patterns, formatting rules (indentation, quotes, semicolons), language idioms, TypeScript strictness policies, preferred patterns vs anti-patterns\nThese define how contributors (human or AI) must work within the project.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion .annex.sum file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**CodexBackend implements AIBackend interface for Codex CLI adapter, parsing JSONL event streams with multi-stage text extraction and token accounting fallbacks.**\n\n## Exports\n\n`CodexBackend` class implements `AIBackend` with properties `name='codex'`, `cliCommand='codex'`; methods: `isAvailable(): Promise<boolean>`, `buildArgs(AICallOptions): string[]`, `composeStdinInput(AICallOptions): string`, `parseResponse(stdout: string, durationMs: number, exitCode: number): AIResponse`, `getInstallInstructions(): string`.\n\n## Key Functions & Types\n\n`CodexUsageTotals` interface: `inputTokens`, `outputTokens`, `cacheReadTokens`, `cacheCreationTokens` (all numbers).\n\n`composePromptWithSystem(AICallOptions)`: wraps `systemPrompt` in `<system-instructions>` tags when present; preserves prompt field.\n\n`extractAssistantTextFromItem(unknown)`: filters `type==='agent_message'` items; extracts `text` field and `content[]` array elements where `type` is `'text'` or `'output_text'`; returns trimmed string array.\n\n`extractUsageFromTurnCompleted(unknown)`: normalizes polymorphic token field names (`input_tokens`, `inputTokens`, `cached_input_tokens`, `cache_read_input_tokens`, `cacheReadInputTokens`, `cache_creation_input_tokens`, `cacheCreationInputTokens`, `output_tokens`, `outputTokens`); subtracts `cacheRead` from `rawInput` to isolate non-cached input tokens.\n\n`collectText(unknown, string[], shouldSkipObject?)`: recursively traverses JSON objects/arrays extracting string values from `text` fields; skips objects where `shouldSkipObject` returns true.\n\n`shouldSkipTextObject(Record)`: returns true for `type==='reasoning'` or `type.endsWith('.reasoning')`.\n\n`uniq(string[])`: deduplicates array preserving insertion order.\n\n## CLI Integration\n\n`buildArgs()` produces: `['-a', 'never', 'exec', '--json', '--skip-git-repo-check', '--ephemeral', '--color', 'never']` plus optional `['--model', model]` and stdin mode flag `'-'`.\n\n`composeStdinInput()` delegates to `composePromptWithSystem()`.\n\n## Response Parsing Strategy\n\n`parseResponse()` implements cascading fallback chain:\n1. **Preferred path (JSONL structured)**: parse each line as JSON; process `type==='item.completed'` via `extractAssistantTextFromItem()`; accumulate `type==='turn.completed'` usage; return `AIResponse` with `raw.format='jsonl'`.\n2. **Fallback (JSONL compatibility)**: if structured extraction yields empty text, recursively `collectText()` across all parsed events with `shouldSkipTextObject()` filter; return `raw.format='jsonl-fallback'`.\n3. **Fallback (plain text)**: if no JSON lines parsed, return `stdout.trim()` as text; return `raw.format='text'`.\n4. **Error**: throw `AIServiceError('PARSE_ERROR', ...)` if all paths fail.\n\nSkips event types: `'error'`, `'thread.started'`, `'turn.started'`, anything ending in `'.error'` or `'.failed'`.\n\nReturns `AIResponse` with `text`, `model` (tracked from direct `obj.model` field), `{input,output,cacheRead,cacheCreation}Tokens`, `durationMs`, `exitCode`, `raw` object.\n\n## Type Guards\n\nHelper functions: `asRecord()` returns typed `Record<string, unknown>` or null; `asString()` returns `string | undefined`; `asNumber()` returns `number | undefined` (finite check).",
      "model": "claude-haiku-4-5-20251001",
      "inputTokens": 3,
      "outputTokens": 938,
      "cacheReadTokens": 0,
      "cacheCreationTokens": 22189,
      "latencyMs": 21676,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/ai/backends/codex.ts",
          "sizeBytes": 10375
        }
      ]
    },
    {
      "timestamp": "2026-02-14T13:46:12.975Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: src/generation/prompts/builder.ts\n\n```typescript\nimport * as path from 'node:path';\nimport { readdir, readFile } from 'node:fs/promises';\nimport type { PromptContext } from './types.js';\nimport { FILE_SYSTEM_PROMPT, FILE_USER_PROMPT, FILE_UPDATE_SYSTEM_PROMPT, DIRECTORY_SYSTEM_PROMPT, DIRECTORY_UPDATE_SYSTEM_PROMPT } from './templates.js';\nimport { readSumFile, getSumPath } from '../writers/sum.js';\nimport { GENERATED_MARKER_PREFIX } from '../writers/agents-md.js';\nimport { extractDirectoryImports, formatImportMap } from '../../imports/index.js';\nimport type { Logger } from '../../core/logger.js';\nimport { nullLogger } from '../../core/logger.js';\n\nfunction logTemplate(logger: Logger, action: string, filePath: string, extra?: string): void {\n  const rel = path.relative(process.cwd(), filePath);\n  const msg = `[prompt] ${action} → ${rel}`;\n  logger.debug(extra ? `${msg} ${extra}` : msg);\n}\n\n/**\n * Detect language from file extension for syntax highlighting.\n */\nexport function detectLanguage(filePath: string): string {\n  const ext = path.extname(filePath).toLowerCase();\n  const langMap: Record<string, string> = {\n    '.ts': 'typescript',\n    '.tsx': 'tsx',\n    '.js': 'javascript',\n    '.jsx': 'jsx',\n    '.py': 'python',\n    '.rb': 'ruby',\n    '.go': 'go',\n    '.rs': 'rust',\n    '.java': 'java',\n    '.kt': 'kotlin',\n    '.swift': 'swift',\n    '.cs': 'csharp',\n    '.php': 'php',\n    '.vue': 'vue',\n    '.svelte': 'svelte',\n    '.json': 'json',\n    '.yaml': 'yaml',\n    '.yml': 'yaml',\n    '.md': 'markdown',\n    '.css': 'css',\n    '.scss': 'scss',\n    '.html': 'html',\n  };\n  return langMap[ext] ?? 'text';\n}\n\n/**\n * Build a complete prompt for file analysis.\n */\nexport function buildFilePrompt(context: PromptContext, debug = false, logger?: Logger): {\n  system: string;\n  user: string;\n} {\n  const lang = detectLanguage(context.filePath);\n  if (debug) logTemplate(logger ?? nullLogger, 'buildFilePrompt', context.filePath, `lang=${lang}`);\n\n  let userPrompt = FILE_USER_PROMPT\n    .replace(/\\{\\{FILE_PATH\\}\\}/g, context.filePath)\n    .replace(/\\{\\{CONTENT\\}\\}/g, context.content)\n    .replace(/\\{\\{LANG\\}\\}/g, lang);\n\n  // Add context files if provided\n  if (context.contextFiles && context.contextFiles.length > 0) {\n    const contextSection = context.contextFiles\n      .map(\n        (f) =>\n          `\\n### ${f.path}\\n\\`\\`\\`${detectLanguage(f.path)}\\n${f.content}\\n\\`\\`\\``\n      )\n      .join('\\n');\n    userPrompt += `\\n\\n## Related Files\\n${contextSection}`;\n  }\n\n  // Build system prompt with optional compression instructions\n  const ratio = context.compressionRatio ?? 0.25;\n  const sourceSize = context.sourceFileSize ?? 0;\n  let systemPrompt = context.existingSum ? FILE_UPDATE_SYSTEM_PROMPT : FILE_SYSTEM_PROMPT;\n\n  // Add aggressive compression instructions when ratio < 0.5\n  if (ratio < 0.5 && sourceSize > 0) {\n    const targetSize = Math.round(sourceSize * ratio);\n    const maxSize = Math.round(targetSize * 1.2);\n    const compressionPercentage = Math.round(ratio * 100);\n\n    const aggressiveRules = `\nTARGET LENGTH (MANDATORY):\n- Source file: ${sourceSize} characters\n- Target summary: ~${targetSize} characters (${compressionPercentage}% compression)\n- Maximum: ${maxSize} characters\n- Achieve this by: ultra-dense writing, minimal examples, single-sentence descriptions\n\nULTRA-COMPRESSION TECHNIQUES:\n- Use telegraphic style: \"exports foo, bar, baz\" not \"this file exports three functions\"\n- No code examples unless critical to understanding\n- One sentence per export maximum\n- Omit obvious parameter names (\"opts: Options\" not \"options: Options - configuration object\")\n- Use abbreviations: \"params\" not \"parameters\", \"config\" not \"configuration\"\n- Skip section if empty (don't write \"No special concerns\" or \"N/A\")\n\n`;\n    // Insert compression rules before DENSITY RULES section\n    systemPrompt = systemPrompt.replace(\n      'DENSITY RULES (MANDATORY):',\n      aggressiveRules + 'DENSITY RULES (MANDATORY):'\n    );\n\n    if (debug) {\n      (logger ?? nullLogger).debug(`[prompt] Aggressive compression: ${sourceSize} → ${targetSize} chars (${compressionPercentage}%)`);\n    }\n  }\n\n  // For incremental updates: include existing summary\n  if (context.existingSum) {\n    userPrompt += `\\n\\n## Existing Summary (update this — preserve stable content, modify only what changed)\\n\\n${context.existingSum}`;\n    return {\n      system: systemPrompt,\n      user: userPrompt,\n    };\n  }\n\n  return {\n    system: systemPrompt,\n    user: userPrompt,\n  };\n}\n\n/**\n * Build a prompt for generating a directory-level AGENTS.md.\n *\n * Reads all .sum files in the directory, child AGENTS.md files,\n * and AGENTS.local.md to provide full context to the LLM.\n */\nexport async function buildDirectoryPrompt(\n  dirPath: string,\n  projectRoot: string,\n  debug = false,\n  knownDirs?: Set<string>,\n  projectStructure?: string,\n  existingAgentsMd?: string,\n  logger?: Logger,\n  variant?: string,\n): Promise<{ system: string; user: string }> {\n  const relativePath = path.relative(projectRoot, dirPath) || '.';\n  const dirName = path.basename(dirPath) || 'root';\n\n  // Collect .sum file summaries and subdirectory sections in parallel\n  const entries = await readdir(dirPath, { withFileTypes: true });\n\n  const fileEntries = entries.filter(\n    (e) => e.isFile() && !e.name.endsWith('.sum') && !e.name.startsWith('.'),\n  );\n  const dirEntries = entries.filter((e) => {\n    if (!e.isDirectory()) return false;\n    if (!knownDirs) return true;\n    const relDir = path.relative(projectRoot, path.join(dirPath, e.name));\n    return knownDirs.has(relDir);\n  });\n\n  // Read all .sum files in parallel\n  const fileResults = await Promise.all(\n    fileEntries.map(async (entry) => {\n      const entryPath = path.join(dirPath, entry.name);\n      const sumPath = getSumPath(entryPath, variant);\n      const sumContent = await readSumFile(sumPath);\n      if (sumContent) {\n        return `### ${entry.name}\\n**Purpose:** ${sumContent.metadata.purpose}\\n\\n${sumContent.summary}`;\n      }\n      return null;\n    }),\n  );\n  const fileSummaries = fileResults.filter((r): r is string => r !== null);\n\n  // Read all child AGENTS.md in parallel\n  const childAgentsFilename = variant ? `AGENTS.${variant}.md` : 'AGENTS.md';\n  const subdirResults = await Promise.all(\n    dirEntries.map(async (entry) => {\n      const childAgentsPath = path.join(dirPath, entry.name, childAgentsFilename);\n      try {\n        const childContent = await readFile(childAgentsPath, 'utf-8');\n        return `### ${entry.name}/\\n${childContent}`;\n      } catch {\n        if (debug) {\n          (logger ?? nullLogger).debug(`[prompt] Skipping missing ${childAgentsPath}`);\n        }\n        return null;\n      }\n    }),\n  );\n  const subdirSections = subdirResults.filter((r): r is string => r !== null);\n\n  // Check for user-defined documentation: AGENTS.local.md or non-ARE AGENTS.md\n  let localSection = '';\n  try {\n    const localContent = await readFile(path.join(dirPath, 'AGENTS.local.md'), 'utf-8');\n    localSection = `\\n## User Notes (AGENTS.local.md)\\n\\n${localContent}\\n\\nNote: Reference [AGENTS.local.md](./AGENTS.local.md) for additional documentation.`;\n  } catch {\n    // No AGENTS.local.md — check if current AGENTS.md is user-authored (first run)\n    try {\n      const agentsContent = await readFile(path.join(dirPath, 'AGENTS.md'), 'utf-8');\n      if (!agentsContent.includes(GENERATED_MARKER_PREFIX)) {\n        localSection = `\\n## User Notes (existing AGENTS.md)\\n\\n${agentsContent}\\n\\nNote: This user-defined content will be preserved as [AGENTS.local.md](./AGENTS.local.md).`;\n      }\n    } catch {\n      // No AGENTS.md either\n    }\n  }\n\n  // Detect manifest files to hint at package root\n  const manifestNames = ['package.json', 'Cargo.toml', 'go.mod', 'pyproject.toml', 'pom.xml', 'build.gradle', 'Gemfile', 'composer.json', 'CMakeLists.txt', 'Makefile'];\n  const foundManifests = fileEntries\n    .filter((e) => manifestNames.includes(e.name))\n    .map((e) => e.name);\n\n  // Extract actual import statements for cross-reference accuracy\n  const sourceExtensions = /\\.(ts|tsx|js|jsx|py|go|rs|java|kt)$/;\n  const sourceFileNames = fileEntries\n    .filter((e) => sourceExtensions.test(e.name))\n    .map((e) => e.name);\n\n  const fileImports = await extractDirectoryImports(dirPath, sourceFileNames);\n  const importMapText = formatImportMap(fileImports);\n\n  if (debug) logTemplate(logger ?? nullLogger, 'buildDirectoryPrompt', dirPath, `files=${fileSummaries.length} subdirs=${subdirSections.length} imports=${fileImports.length}`);\n\n  const userSections: string[] = [\n    `Generate AGENTS.md for directory: \"${relativePath}\" (${dirName})`,\n    '',\n    `## File Summaries (${fileSummaries.length} files)`,\n    '',\n    ...fileSummaries,\n  ];\n\n  if (importMapText) {\n    userSections.push(\n      '',\n      '## Import Map (verified — use these exact paths)',\n      '',\n      importMapText,\n    );\n  }\n\n  if (projectStructure) {\n    userSections.push(\n      '',\n      '## Project Directory Structure',\n      '',\n      '<project-structure>',\n      projectStructure,\n      '</project-structure>',\n    );\n  }\n\n  // Scan for annex files in the directory\n  const annexFiles = entries\n    .filter((e) => e.isFile() && e.name.endsWith('.annex.sum'))\n    .map((e) => e.name);\n  if (annexFiles.length > 0) {\n    userSections.push(\n      '',\n      '## Annex Files (reproduction-critical constants)',\n      '',\n      ...annexFiles.map((f) => `- ${f}`),\n    );\n  }\n\n  if (subdirSections.length > 0) {\n    userSections.push('', '## Subdirectories', '', ...subdirSections);\n  }\n\n  if (foundManifests.length > 0) {\n    userSections.push('', '## Directory Hints', '', `Contains manifest file(s): ${foundManifests.join(', ')} — likely a package or project root.`);\n  }\n\n  if (localSection) {\n    userSections.push(localSection);\n  }\n\n  // For incremental updates: include existing AGENTS.md and use update-specific system prompt\n  if (existingAgentsMd) {\n    userSections.push(\n      '',\n      '## Existing AGENTS.md (update this — preserve stable content, modify only what changed)',\n      '',\n      existingAgentsMd,\n    );\n    return {\n      system: DIRECTORY_UPDATE_SYSTEM_PROMPT,\n      user: userSections.join('\\n'),\n    };\n  }\n\n  return {\n    system: DIRECTORY_SYSTEM_PROMPT,\n    user: userSections.join('\\n'),\n  };\n}\n\n\n```\n\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n- Workflow & convention rules: if this file defines contribution guidelines, PR conventions, commit standards, testing mandates, tool usage requirements, approval workflows, AI agent behavioral instructions, or code conventions (naming standards, formatting rules, import ordering, linting policies), extract them as explicit, actionable rules. Sources: CONTRIBUTING.md, CI configs, PR templates, style guides, linter/formatter configs (.eslintrc, .prettierrc, .editorconfig), tsconfig strictness, README dev sections.\n\n\nTARGET LENGTH (MANDATORY):\n- Source file: 10254 characters\n- Target summary: ~1025 characters (10% compression)\n- Maximum: 1230 characters\n- Achieve this by: ultra-dense writing, minimal examples, single-sentence descriptions\n\nULTRA-COMPRESSION TECHNIQUES:\n- Use telegraphic style: \"exports foo, bar, baz\" not \"this file exports three functions\"\n- No code examples unless critical to understanding\n- One sentence per export maximum\n- Omit obvious parameter names (\"opts: Options\" not \"options: Options - configuration object\")\n- Use abbreviations: \"params\" not \"parameters\", \"config\" not \"configuration\"\n- Skip section if empty (don't write \"No special concerns\" or \"N/A\")\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nWORKFLOW & CONVENTION RULES (NEVER EXCLUDE):\n- Contribution requirements: branch naming, commit message formats, PR description templates\n- Testing mandates: required test types, coverage thresholds, \"never delete tests\" policies\n- Tool usage rules: required CLI commands, browser automation tools, linter/formatter requirements\n- Approval workflows: review requirements, CI checks that must pass, deployment gates\n- AI agent instructions: explicit directives for automated assistants (coding rules, prohibited actions)\n- Code conventions: naming standards (camelCase, PascalCase, snake_case for what), import ordering rules, file/folder naming patterns, formatting rules (indentation, quotes, semicolons), language idioms, TypeScript strictness policies, preferred patterns vs anti-patterns\nThese define how contributors (human or AI) must work within the project.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion .annex.sum file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**builder.ts constructs LLM prompts for file and directory analysis with language detection, compression rules, and context assembly.**\n\n## Exports\n\n`detectLanguage(filePath: string): string` — maps file extensions to syntax highlighting language codes (`.ts`→`typescript`, `.py`→`python`, etc.; defaults to `text`).\n\n`buildFilePrompt(context: PromptContext, debug?: boolean, logger?: Logger): {system: string; user: string}` — assembles system and user prompts for single-file analysis. Substitutes `{{FILE_PATH}}`, `{{CONTENT}}`, `{{LANG}}` in `FILE_USER_PROMPT`; appends optional `context.contextFiles` as \"Related Files\" section; injects aggressive compression rules when `compressionRatio < 0.5` and `sourceFileSize > 0`, calculating target size and max size; uses `FILE_UPDATE_SYSTEM_PROMPT` if `context.existingSum` exists (incremental mode), otherwise `FILE_SYSTEM_PROMPT`; appends existing summary under \"Existing Summary\" header for updates.\n\n`buildDirectoryPrompt(dirPath: string, projectRoot: string, debug?: boolean, knownDirs?: Set<string>, projectStructure?: string, existingAgentsMd?: string, logger?: Logger, variant?: string): Promise<{system: string; user: string}>` — async; generates directory-level `AGENTS.md` prompt by: reading `.sum` file summaries in parallel via `readSumFile()`, extracting `metadata.purpose`; collecting child `AGENTS.md`/`AGENTS.{variant}.md` from subdirectories; scanning for `AGENTS.local.md` or user-authored `AGENTS.md` (detected by absence of `GENERATED_MARKER_PREFIX`), preserving as \"User Notes\"; detecting manifest files (`package.json`, `Cargo.toml`, `go.mod`, `pyproject.toml`, `pom.xml`, `build.gradle`, `Gemfile`, `composer.json`, `CMakeLists.txt`, `Makefile`); extracting actual imports from source files (`.ts|tsx|js|jsx|py|go|rs|java|kt`) via `extractDirectoryImports()` and formatting with `formatImportMap()`; scanning for `.annex.sum` files; assembling user sections: directory path, file summaries, import map, project structure, annex files, subdirectories, manifest hints, local content; returns `DIRECTORY_UPDATE_SYSTEM_PROMPT` if `existingAgentsMd` provided, else `DIRECTORY_SYSTEM_PROMPT`.\n\n## Dependencies\n\nImports system prompts (`FILE_SYSTEM_PROMPT`, `FILE_USER_PROMPT`, `FILE_UPDATE_SYSTEM_PROMPT`, `DIRECTORY_SYSTEM_PROMPT`, `DIRECTORY_UPDATE_SYSTEM_PROMPT`) from `./templates.js`; reads `.sum` files via `readSumFile(getSumPath())` from `../writers/sum.js`; detects generated markers via `GENERATED_MARKER_PREFIX` from `../writers/agents-md.js`; extracts and formats imports via `extractDirectoryImports(dirPath, sourceFileNames)` and `formatImportMap(fileImports)` from `../../imports/index.js`; uses `Logger` type and `nullLogger` singleton from `../../core/logger.js`.\n\n## Behavioral Contracts\n\n**Compression thresholds:** Aggressive compression rules injected when `compressionRatio < 0.5` and `sourceFileSize > 0`; target size = `Math.round(sourceFileSize * ratio)`; max size = `Math.round(targetSize * 1.2)`.\n\n**Template substitution patterns:** `{{FILE_PATH}}`, `{{CONTENT}}`, `{{LANG}}` replaced via `.replace(/\\{\\{...\\}\\}/g, ...)` in `FILE_USER_PROMPT`.\n\n**Generated marker detection:** `GENERATED_MARKER_PREFIX` substring check distinguishes generated vs. user-authored `AGENTS.md`.\n\n**Directory manifest indicators:** `package.json`, `Cargo.toml`, `go.mod`, `pyproject.toml`, `pom.xml`, `build.gradle`, `Gemfile`, `composer.json`, `CMakeLists.txt`, `Makefile` signal project roots.\n\n**Incremental update signatures:** `FILE_UPDATE_SYSTEM_PROMPT` used when `context.existingSum` truthy; `DIRECTORY_UPDATE_SYSTEM_PROMPT` used when `existingAgentsMd` truthy; existing content appended under \"Existing Summary\"/\"Existing AGENTS.md\" headers with instruction to \"preserve stable content, modify only what changed.\"",
      "model": "claude-haiku-4-5-20251001",
      "inputTokens": 3,
      "outputTokens": 1111,
      "cacheReadTokens": 16147,
      "cacheCreationTokens": 6032,
      "latencyMs": 23520,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/generation/prompts/builder.ts",
          "sizeBytes": 10268
        }
      ]
    },
    {
      "timestamp": "2026-02-14T13:46:28.212Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: src/generation/prompts/types.ts\n\n```typescript\n/**\n * Context provided when building a prompt.\n */\nexport interface PromptContext {\n  /** Absolute path to the file */\n  filePath: string;\n  /** File content to analyze */\n  content: string;\n  /** Related files for additional context */\n  contextFiles?: Array<{\n    path: string;\n    content: string;\n  }>;\n  /** Existing .sum summary text for incremental updates */\n  existingSum?: string;\n  /** Source file size in characters for calculating compression target */\n  sourceFileSize?: number;\n  /** Target compression ratio for .sum files (0.1-1.0, from config) */\n  compressionRatio?: number;\n}\n\n/**\n * Guidelines for summary generation (from CONTEXT.md).\n */\nexport const SUMMARY_GUIDELINES = {\n  /** Target word count range */\n  targetLength: { min: 300, max: 500 },\n  /** What to include */\n  include: [\n    'Purpose and responsibility',\n    'Public interface (exports, key functions)',\n    'Key patterns and notable algorithms',\n    'Dependencies with usage context',\n    'Key function signatures as code snippets',\n    'Tightly coupled sibling files',\n    'Behavioral contracts: verbatim regex patterns, format strings, magic constants, sentinel values, output templates, environment variables',\n    'Annex references: for files with large string constants (prompt templates, config arrays, IDE templates), list each constant name with a one-line description in an ## Annex References section',\n  ],\n  /** What to exclude */\n  exclude: [\n    'Control flow minutiae (loop structures, variable naming, temporary state)',\n    'Generic TODOs/FIXMEs (keep only security/breaking)',\n    'Broad architectural relationships (handled by AGENTS.md)',\n  ],\n} as const;\n\n```\n\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n- Workflow & convention rules: if this file defines contribution guidelines, PR conventions, commit standards, testing mandates, tool usage requirements, approval workflows, AI agent behavioral instructions, or code conventions (naming standards, formatting rules, import ordering, linting policies), extract them as explicit, actionable rules. Sources: CONTRIBUTING.md, CI configs, PR templates, style guides, linter/formatter configs (.eslintrc, .prettierrc, .editorconfig), tsconfig strictness, README dev sections.\n\n\nTARGET LENGTH (MANDATORY):\n- Source file: 1665 characters\n- Target summary: ~167 characters (10% compression)\n- Maximum: 200 characters\n- Achieve this by: ultra-dense writing, minimal examples, single-sentence descriptions\n\nULTRA-COMPRESSION TECHNIQUES:\n- Use telegraphic style: \"exports foo, bar, baz\" not \"this file exports three functions\"\n- No code examples unless critical to understanding\n- One sentence per export maximum\n- Omit obvious parameter names (\"opts: Options\" not \"options: Options - configuration object\")\n- Use abbreviations: \"params\" not \"parameters\", \"config\" not \"configuration\"\n- Skip section if empty (don't write \"No special concerns\" or \"N/A\")\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nWORKFLOW & CONVENTION RULES (NEVER EXCLUDE):\n- Contribution requirements: branch naming, commit message formats, PR description templates\n- Testing mandates: required test types, coverage thresholds, \"never delete tests\" policies\n- Tool usage rules: required CLI commands, browser automation tools, linter/formatter requirements\n- Approval workflows: review requirements, CI checks that must pass, deployment gates\n- AI agent instructions: explicit directives for automated assistants (coding rules, prohibited actions)\n- Code conventions: naming standards (camelCase, PascalCase, snake_case for what), import ordering rules, file/folder naming patterns, formatting rules (indentation, quotes, semicolons), language idioms, TypeScript strictness policies, preferred patterns vs anti-patterns\nThese define how contributors (human or AI) must work within the project.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion .annex.sum file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**types.ts defines TypeScript interfaces and guidelines for prompt context and summary generation across the documentation pipeline.**\n\n## Exports\n\n`PromptContext` interface: context object with `filePath` (string), `content` (string), `contextFiles` (optional array of {path, content}), `existingSum` (optional string for incremental updates), `sourceFileSize` (optional number), `compressionRatio` (optional number 0.1-1.0).\n\n`SUMMARY_GUIDELINES` constant: configuration object with `targetLength` ({min: 300, max: 500}), `include` (array of 8 required documentation topics), `exclude` (array of 3 content exclusion rules).\n\n## Design\n\nType-only module establishing contracts for prompt builders consuming PromptContext and enforcing documentation standards via immutable SUMMARY_GUIDELINES constant.",
      "model": "claude-haiku-4-5-20251001",
      "inputTokens": 3,
      "outputTokens": 196,
      "cacheReadTokens": 16147,
      "cacheCreationTokens": 3165,
      "latencyMs": 8738,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/generation/prompts/types.ts",
          "sizeBytes": 1665
        }
      ]
    },
    {
      "timestamp": "2026-02-14T13:46:12.891Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: src/ai/backends/opencode.ts\n\n```typescript\n/**\n * OpenCode CLI backend adapter.\n *\n * Full implementation of the {@link AIBackend} interface for the OpenCode CLI\n * (`opencode`). Parses NDJSON streaming output, aggregates token usage across\n * turns, calculates cost when not provided, and extracts response text from\n * `text` events.\n *\n * OpenCode outputs NDJSON (one JSON event per line) with key event types:\n * - `text`: assistant text output (`part.text`)\n * - `step_finish`: per-turn token usage and cost (`part.tokens`, `part.cost`)\n * - `step_start`, `tool_use`, `tool_result`: other lifecycle events (ignored)\n *\n * @module\n */\n\nimport { writeFile, mkdir } from 'node:fs/promises';\nimport path from 'node:path';\nimport { z } from 'zod';\nimport type { AIBackend, AICallOptions, AIResponse } from '../types.js';\nimport { AIServiceError } from '../types.js';\nimport { isCommandOnPath } from './claude.js';\n\n// ---------------------------------------------------------------------------\n// Zod schemas for OpenCode NDJSON events\n// ---------------------------------------------------------------------------\n\n/**\n * Schema for token breakdown within a `step_finish` event.\n */\nconst OpenCodeTokensSchema = z.object({\n  total: z.number().optional().default(0),\n  input: z.number().optional().default(0),\n  output: z.number().optional().default(0),\n  reasoning: z.number().optional().default(0),\n  cache: z.object({\n    read: z.number().optional().default(0),\n    write: z.number().optional().default(0),\n  }).optional().default({ read: 0, write: 0 }),\n}).passthrough();\n\n/**\n * Schema for the `part` object within a `step_finish` event.\n */\nconst OpenCodeStepFinishPartSchema = z.object({\n  type: z.literal('step-finish'),\n  cost: z.number().optional().default(0),\n  tokens: OpenCodeTokensSchema.optional(),\n}).passthrough();\n\n/**\n * Schema for a `step_finish` NDJSON event line.\n */\nconst OpenCodeStepFinishSchema = z.object({\n  type: z.literal('step_finish'),\n  part: OpenCodeStepFinishPartSchema,\n}).passthrough();\n\n/**\n * Schema for the `part` object within a `text` event.\n */\nconst OpenCodeTextPartSchema = z.object({\n  type: z.literal('text'),\n  text: z.string(),\n}).passthrough();\n\n/**\n * Schema for a `text` NDJSON event line.\n */\nconst OpenCodeTextSchema = z.object({\n  type: z.literal('text'),\n  part: OpenCodeTextPartSchema,\n}).passthrough();\n\n// ---------------------------------------------------------------------------\n// Cost calculation constants (Anthropic Claude Sonnet pricing as default)\n// ---------------------------------------------------------------------------\n\n/** Input token price per million tokens (USD) */\nconst INPUT_COST_PER_MTOK = 15;\n/** Output token price per million tokens (USD) */\nconst OUTPUT_COST_PER_MTOK = 75;\n/** Cache write token price per million tokens (USD) */\nconst CACHE_WRITE_COST_PER_MTOK = 18.75;\n/** Cache read token price per million tokens (USD) */\nconst CACHE_READ_COST_PER_MTOK = 1.50;\n\n/**\n * Calculate cost from token counts when OpenCode doesn't provide it.\n *\n * Uses Anthropic Claude Sonnet pricing as the default since OpenCode\n * is typically used with Anthropic models.\n */\nfunction calculateCostFromTokens(\n  inputTokens: number,\n  outputTokens: number,\n  cacheReadTokens: number,\n  cacheWriteTokens: number,\n): number {\n  const inputCost = (inputTokens / 1_000_000) * INPUT_COST_PER_MTOK;\n  const outputCost = (outputTokens / 1_000_000) * OUTPUT_COST_PER_MTOK;\n  const cacheWriteCost = (cacheWriteTokens / 1_000_000) * CACHE_WRITE_COST_PER_MTOK;\n  const cacheReadCost = (cacheReadTokens / 1_000_000) * CACHE_READ_COST_PER_MTOK;\n  return inputCost + outputCost + cacheWriteCost + cacheReadCost;\n}\n\n// ---------------------------------------------------------------------------\n// Aggregated metrics from NDJSON parsing\n// ---------------------------------------------------------------------------\n\n/**\n * Aggregated metrics collected from all NDJSON events in a single\n * OpenCode CLI invocation.\n */\ninterface ParsedOpenCodeOutput {\n  /** Concatenated text from all `text` events */\n  text: string;\n  /** Number of `step_finish` events (agentic turns) */\n  numTurns: number;\n  /** Sum of input tokens across all turns */\n  inputTokens: number;\n  /** Sum of output tokens across all turns */\n  outputTokens: number;\n  /** Sum of reasoning tokens across all turns */\n  reasoningTokens: number;\n  /** Sum of cache read tokens across all turns */\n  cacheReadTokens: number;\n  /** Sum of cache write tokens across all turns */\n  cacheWriteTokens: number;\n  /** Sum of cost across all turns (often 0 from OpenCode) */\n  totalCost: number;\n  /** All parsed NDJSON events (for `raw` field) */\n  events: unknown[];\n}\n\n// ---------------------------------------------------------------------------\n// Model name mapping (short form → OpenCode provider/model format)\n// ---------------------------------------------------------------------------\n\n/**\n * Map of short model names to OpenCode's `provider/model` format.\n *\n * OpenCode requires fully-qualified model identifiers (e.g.,\n * `anthropic/claude-sonnet-4-5`), while ARE config uses short aliases\n * (e.g., `sonnet`). Names already containing `/` are passed through\n * unchanged.\n */\nconst MODEL_ALIASES: Record<string, string> = {\n  'sonnet': 'anthropic/claude-sonnet-4-5',\n  'opus': 'anthropic/claude-opus-4-6',\n  'haiku': 'anthropic/claude-haiku-4-5',\n};\n\n/**\n * Resolve a model name to OpenCode's `provider/model` format.\n *\n * If the name already contains `/`, it's assumed to be fully-qualified\n * and is returned as-is. Otherwise, looks up the short alias.\n *\n * @param model - Short alias or fully-qualified model identifier\n * @returns Fully-qualified model identifier for OpenCode CLI\n */\nfunction resolveModelForOpenCode(model: string): string {\n  if (model.includes('/')) return model;\n  return MODEL_ALIASES[model] ?? model;\n}\n\n// ---------------------------------------------------------------------------\n// OpenCode agent config for ARE\n// ---------------------------------------------------------------------------\n\n/** Agent name used in `.opencode/agents/` and `--agent` flag */\nconst OPENCODE_AGENT_NAME = 'are-summarizer';\n\n/**\n * Content of the `.opencode/agents/are-summarizer.md` agent file.\n *\n * Disables all tools and limits to 1 agentic step so the model produces\n * a single text response rather than entering agentic multi-turn mode.\n * The dynamic ARE system prompt is delivered via `<system-instructions>`\n * XML tags in stdin (see {@link OpenCodeBackend.composeStdinInput}).\n */\nconst OPENCODE_AGENT_CONTENT = `---\ndescription: \"ARE documentation summarizer — single-turn, no tools, raw markdown output\"\nsteps: 5\ntools:\n  \"*\": false\n---\n\nYou are a documentation generator for the agents-reverse-engineer (ARE) tool.\n\nCRITICAL RULES:\n- Output ONLY the raw content requested — your entire response IS the document\n- Do NOT include preamble, thinking, planning, or meta-commentary\n- Do NOT say \"Here is...\", \"I'll generate...\", \"Let me...\", \"Perfect!\", or similar\n- Do NOT summarize what you did or list changes you made\n- When \\`<system-instructions>\\` tags are present in the input, follow them exactly\n- The content after \\`</system-instructions>\\` is the user prompt — respond to it directly\n`;\n\n// ---------------------------------------------------------------------------\n// OpenCode backend\n// ---------------------------------------------------------------------------\n\n/**\n * OpenCode CLI backend adapter.\n *\n * Implements the {@link AIBackend} interface for the `opencode` CLI.\n * Parses NDJSON streaming output, aggregates tokens across turns,\n * and calculates cost when not provided by the CLI.\n *\n * @example\n * ```typescript\n * const backend = new OpenCodeBackend();\n * if (await backend.isAvailable()) {\n *   const args = backend.buildArgs({ prompt: 'Summarize this file' });\n *   const result = await runSubprocess('opencode', args, {\n *     timeoutMs: 120_000,\n *     input: 'Summarize this file',\n *   });\n *   const response = backend.parseResponse(result.stdout, result.durationMs, result.exitCode);\n * }\n * ```\n */\nexport class OpenCodeBackend implements AIBackend {\n  readonly name = 'opencode';\n  readonly cliCommand = 'opencode';\n\n  /**\n   * Check if the `opencode` CLI is available on PATH.\n   */\n  async isAvailable(): Promise<boolean> {\n    return isCommandOnPath(this.cliCommand);\n  }\n\n  /**\n   * Build CLI arguments for an OpenCode invocation.\n   *\n   * Returns the argument array for `opencode run --format json`. The\n   * prompt itself is NOT included — it goes to stdin via the subprocess\n   * wrapper.\n   *\n   * OpenCode limitations compared to Claude CLI:\n   * - No `--max-turns` equivalent (mitigated via agent `steps: 1`)\n   * - No `--allowedTools` equivalent (mitigated via agent `tools: {\"*\": false}`)\n   * - No `--system-prompt` equivalent (mitigated via {@link composeStdinInput})\n   * - No `--no-session-persistence` equivalent\n   *\n   * @param options - Call options (model selection supported)\n   * @returns Argument array suitable for {@link runSubprocess}\n   */\n  buildArgs(options: AICallOptions): string[] {\n    const args: string[] = [\n      'run',\n      '--format', 'json',\n      '--agent', OPENCODE_AGENT_NAME,\n    ];\n\n    if (options.model) {\n      args.push('--model', resolveModelForOpenCode(options.model));\n    }\n\n    return args;\n  }\n\n  /**\n   * Compose stdin input, folding the system prompt into the payload.\n   *\n   * OpenCode has no `--system-prompt` CLI flag, so the dynamic system\n   * prompt is wrapped in `<system-instructions>` XML tags and prepended\n   * to the user prompt. The static agent prompt (in the agent markdown\n   * file) instructs the model to follow these tags.\n   */\n  composeStdinInput(options: AICallOptions): string {\n    if (options.systemPrompt) {\n      return `<system-instructions>\\n${options.systemPrompt}\\n</system-instructions>\\n\\n${options.prompt}`;\n    }\n    return options.prompt;\n  }\n\n  /**\n   * Ensure the ARE agent config exists in the target project.\n   *\n   * Creates `.opencode/agents/are-summarizer.md` with tool restrictions\n   * (`\"*\": false`) and step limit (`steps: 5`) so OpenCode runs in\n   * a constrained, non-agentic mode when invoked by ARE.\n   *\n   * Always overwrites — the file is an ARE-owned artifact whose content\n   * may evolve across versions.\n   */\n  async ensureProjectConfig(projectRoot: string): Promise<void> {\n    const agentDir = path.join(projectRoot, '.opencode', 'agents');\n    await mkdir(agentDir, { recursive: true });\n    await writeFile(\n      path.join(agentDir, `${OPENCODE_AGENT_NAME}.md`),\n      OPENCODE_AGENT_CONTENT,\n      'utf-8',\n    );\n  }\n\n  /**\n   * Parse OpenCode CLI NDJSON output into a normalized {@link AIResponse}.\n   *\n   * OpenCode emits NDJSON (one JSON object per line) with no single \"result\"\n   * summary object (unlike Claude CLI). The response text is spread across\n   * multiple `text` events, and token usage is in `step_finish` events.\n   *\n   * Parsing strategy:\n   * 1. Split stdout by newlines, filter empty lines\n   * 2. Parse each line as JSON (skip malformed lines gracefully)\n   * 3. Collect `text` events → concatenate `part.text` for final response\n   * 4. Collect `step_finish` events → aggregate tokens across all turns\n   * 5. If aggregated cost === 0, calculate from token counts\n   * 6. Return AIResponse with aggregated metrics\n   *\n   * @param stdout - Raw NDJSON stdout from the OpenCode CLI process\n   * @param durationMs - Wall-clock duration of the subprocess\n   * @param exitCode - Process exit code\n   * @returns Normalized AI response\n   * @throws {AIServiceError} With code `PARSE_ERROR` if no text content found\n   */\n  parseResponse(stdout: string, durationMs: number, exitCode: number): AIResponse {\n    const parsed = this.parseNdjson(stdout);\n\n    if (!parsed.text) {\n      throw new AIServiceError(\n        'PARSE_ERROR',\n        `No text content found in OpenCode CLI output. ` +\n        `Parsed ${parsed.events.length} event(s), ${parsed.numTurns} turn(s). ` +\n        `Raw output (first 200 chars): ${stdout.slice(0, 200)}`,\n      );\n    }\n\n    // Handle OpenCode step-limit marker.\n    // When the agent hits its step limit, OpenCode appends \"MAXIMUM STEPS\n    // REACHED\" to the output. If the model already produced substantial\n    // content before hitting the limit, strip the marker and use the content.\n    // Only reject if stripping leaves nothing useful (< 100 chars).\n    if (parsed.text.includes('MAXIMUM STEPS REACHED')) {\n      const cleaned = parsed.text\n        .replace(/\\n*MAXIMUM STEPS REACHED\\n*/g, '')\n        .trim();\n      if (cleaned.length < 100) {\n        throw new AIServiceError(\n          'PARSE_ERROR',\n          `OpenCode hit agent step limit — response is meta-commentary, not content. ` +\n          `Response (first 200 chars): ${parsed.text.slice(0, 200)}`,\n        );\n      }\n      parsed.text = cleaned;\n    }\n\n    // Calculate cost if OpenCode didn't provide it\n    let cost = parsed.totalCost;\n    if (cost === 0 && (parsed.inputTokens > 0 || parsed.outputTokens > 0)) {\n      cost = calculateCostFromTokens(\n        parsed.inputTokens,\n        parsed.outputTokens,\n        parsed.cacheReadTokens,\n        parsed.cacheWriteTokens,\n      );\n    }\n\n    return {\n      text: parsed.text,\n      model: 'unknown',  // OpenCode NDJSON doesn't include model name\n      inputTokens: parsed.inputTokens,\n      outputTokens: parsed.outputTokens,\n      cacheReadTokens: parsed.cacheReadTokens,\n      cacheCreationTokens: parsed.cacheWriteTokens,\n      durationMs,\n      exitCode,\n      raw: {\n        events: parsed.events,\n        numTurns: parsed.numTurns,\n        reasoningTokens: parsed.reasoningTokens,\n        calculatedCost: cost,\n      },\n    };\n  }\n\n  /**\n   * Parse NDJSON stdout into aggregated metrics.\n   *\n   * Each line is parsed independently. Malformed lines are skipped\n   * gracefully to handle partial output from killed processes or\n   * interleaved stderr.\n   *\n   * @param stdout - Raw NDJSON output from OpenCode CLI\n   * @returns Aggregated metrics from all parsed events\n   */\n  private parseNdjson(stdout: string): ParsedOpenCodeOutput {\n    const result: ParsedOpenCodeOutput = {\n      text: '',\n      numTurns: 0,\n      inputTokens: 0,\n      outputTokens: 0,\n      reasoningTokens: 0,\n      cacheReadTokens: 0,\n      cacheWriteTokens: 0,\n      totalCost: 0,\n      events: [],\n    };\n\n    const textParts: string[] = [];\n    const lines = stdout.split('\\n');\n\n    for (const line of lines) {\n      const trimmed = line.trim();\n      if (!trimmed || !trimmed.startsWith('{')) continue;\n\n      let event: Record<string, unknown>;\n      try {\n        event = JSON.parse(trimmed) as Record<string, unknown>;\n      } catch {\n        // Malformed JSON line — skip gracefully\n        continue;\n      }\n\n      result.events.push(event);\n\n      // Extract text from `text` events\n      if (event.type === 'text') {\n        const textParsed = OpenCodeTextSchema.safeParse(event);\n        if (textParsed.success) {\n          textParts.push(textParsed.data.part.text);\n        }\n      }\n\n      // Aggregate tokens from `step_finish` events\n      // Check both top-level `type` and nested `part.type` for robustness\n      if (event.type === 'step_finish' || (event.part as Record<string, unknown>)?.type === 'step-finish') {\n        const stepParsed = OpenCodeStepFinishSchema.safeParse(event);\n        if (stepParsed.success) {\n          result.numTurns++;\n          const tokens = stepParsed.data.part.tokens;\n          if (tokens) {\n            result.inputTokens += tokens.input;\n            result.outputTokens += tokens.output;\n            result.reasoningTokens += tokens.reasoning;\n            result.cacheReadTokens += tokens.cache.read;\n            result.cacheWriteTokens += tokens.cache.write;\n          }\n          result.totalCost += stepParsed.data.part.cost;\n        }\n      }\n    }\n\n    result.text = textParts.join('');\n\n    return result;\n  }\n\n  /**\n   * Get user-facing install instructions for the OpenCode CLI.\n   */\n  getInstallInstructions(): string {\n    return [\n      'OpenCode:',\n      '  curl -fsSL https://opencode.ai/install | bash',\n      '  https://opencode.ai',\n    ].join('\\n');\n  }\n}\n\n```\n\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n- Workflow & convention rules: if this file defines contribution guidelines, PR conventions, commit standards, testing mandates, tool usage requirements, approval workflows, AI agent behavioral instructions, or code conventions (naming standards, formatting rules, import ordering, linting policies), extract them as explicit, actionable rules. Sources: CONTRIBUTING.md, CI configs, PR templates, style guides, linter/formatter configs (.eslintrc, .prettierrc, .editorconfig), tsconfig strictness, README dev sections.\n\n\nTARGET LENGTH (MANDATORY):\n- Source file: 16171 characters\n- Target summary: ~1617 characters (10% compression)\n- Maximum: 1940 characters\n- Achieve this by: ultra-dense writing, minimal examples, single-sentence descriptions\n\nULTRA-COMPRESSION TECHNIQUES:\n- Use telegraphic style: \"exports foo, bar, baz\" not \"this file exports three functions\"\n- No code examples unless critical to understanding\n- One sentence per export maximum\n- Omit obvious parameter names (\"opts: Options\" not \"options: Options - configuration object\")\n- Use abbreviations: \"params\" not \"parameters\", \"config\" not \"configuration\"\n- Skip section if empty (don't write \"No special concerns\" or \"N/A\")\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nWORKFLOW & CONVENTION RULES (NEVER EXCLUDE):\n- Contribution requirements: branch naming, commit message formats, PR description templates\n- Testing mandates: required test types, coverage thresholds, \"never delete tests\" policies\n- Tool usage rules: required CLI commands, browser automation tools, linter/formatter requirements\n- Approval workflows: review requirements, CI checks that must pass, deployment gates\n- AI agent instructions: explicit directives for automated assistants (coding rules, prohibited actions)\n- Code conventions: naming standards (camelCase, PascalCase, snake_case for what), import ordering rules, file/folder naming patterns, formatting rules (indentation, quotes, semicolons), language idioms, TypeScript strictness policies, preferred patterns vs anti-patterns\nThese define how contributors (human or AI) must work within the project.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion .annex.sum file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**OpenCodeBackend class implements AIBackend for OpenCode CLI, parsing NDJSON streaming output, aggregating token usage across agentic turns, calculating cost from tokens when unprovided, and extracting response text.**\n\n## Exports\n\n- `OpenCodeBackend` — class implementing `AIBackend` interface with methods: `isAvailable()` → `Promise<boolean>`, `buildArgs(options: AICallOptions)` → `string[]`, `composeStdinInput(options: AICallOptions)` → `string`, `ensureProjectConfig(projectRoot: string)` → `Promise<void>`, `parseResponse(stdout: string, durationMs: number, exitCode: number)` → `AIResponse`\n\n## NDJSON Event Schemas\n\nZod schemas validate OpenCode's streaming NDJSON format: `OpenCodeTokensSchema` (token breakdown with `total`, `input`, `output`, `reasoning`, cache `read`/`write`), `OpenCodeStepFinishSchema` (`type: 'step_finish'` events containing per-turn token counts and cost), `OpenCodeTextSchema` (`type: 'text'` events carrying `part.text` response chunks). Other lifecycle events (`step_start`, `tool_use`, `tool_result`) are emitted but ignored.\n\n## Cost Calculation\n\nWhen OpenCode omits cost, `calculateCostFromTokens(inputTokens, outputTokens, cacheReadTokens, cacheWriteTokens)` → `number` uses Anthropic Claude Sonnet pricing: `INPUT_COST_PER_MTOK = 15`, `OUTPUT_COST_PER_MTOK = 75`, `CACHE_WRITE_COST_PER_MTOK = 18.75`, `CACHE_READ_COST_PER_MTOK = 1.50` (all per million tokens in USD).\n\n## Model Name Mapping\n\n`MODEL_ALIASES: Record<string, string>` maps short names to OpenCode's `provider/model` format: `'sonnet'` → `'anthropic/claude-sonnet-4-5'`, `'opus'` → `'anthropic/claude-opus-4-6'`, `'haiku'` → `'anthropic/claude-haiku-4-5'`. `resolveModelForOpenCode(model: string)` → `string` passes through identifiers containing `/` unchanged, looks up others in aliases.\n\n## OpenCode Agent Config\n\n`OPENCODE_AGENT_NAME = 'are-summarizer'` (used in `.opencode/agents/` directory and `--agent` CLI flag). `OPENCODE_AGENT_CONTENT` — markdown agent file disabling all tools (`tools: {\"*\": false}`) and limiting to `steps: 5` to force single-turn non-agentic mode. Expects ARE system prompt injected via `<system-instructions>` XML tags in stdin; agent file instructs model to follow these tags exactly, output only raw content without preamble/meta-commentary.\n\n## Response Parsing Strategy\n\n`parseResponse(stdout, durationMs, exitCode)` splits stdout by newlines, parses each as JSON (skipping malformed lines), aggregates: (1) all `text` event chunks via `part.text` concatenation, (2) all `step_finish` event token counts across turns, (3) cost from aggregated tokens if OpenCode provided zero. Special handling: strips \"MAXIMUM STEPS REACHED\" marker if substantial content (≥100 chars) remains; throws `AIServiceError` with code `'PARSE_ERROR'` if no text found or marker leaves <100 chars. Returns `AIResponse` with aggregated metrics in `raw` field including `numTurns`, `reasoningTokens`, `calculatedCost`.\n\n## Interface Compliance\n\nProperties: `name = 'opencode'`, `cliCommand = 'opencode'`. `buildArgs(options: AICallOptions)` returns `['run', '--format', 'json', '--agent', OPENCODE_AGENT_NAME]` plus `--model <resolved>` if specified. OpenCode lacks `--max-turns`, `--allowedTools`, `--system-prompt` equivalents — mitigated via agent `steps: 5`, `tools: {\"*\": false}`, and `<system-instructions>` XML wrapping. `ensureProjectConfig(projectRoot)` writes agent file to `.opencode/agents/are-summarizer.md` with `mkdir -p` recursion, always overwrites. `getInstallInstructions()` returns curl command and URL.",
      "model": "claude-haiku-4-5-20251001",
      "inputTokens": 3,
      "outputTokens": 1068,
      "cacheReadTokens": 16147,
      "cacheCreationTokens": 7488,
      "latencyMs": 25709,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/ai/backends/opencode.ts",
          "sizeBytes": 16191
        }
      ]
    },
    {
      "timestamp": "2026-02-14T13:46:28.323Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: src/generation/writers/agents-md.ts\n\n```typescript\nimport { writeFile, readFile, mkdir, rename } from 'node:fs/promises';\nimport { existsSync } from 'node:fs';\nimport * as path from 'node:path';\nimport { getVersion } from '../../version.js';\n\n/** Base marker prefix to identify generated files (version-agnostic) */\nexport const GENERATED_MARKER_PREFIX = '<!-- Generated by agents-reverse-engineer';\n\n/** Marker comment to identify generated AGENTS.md files */\nexport const GENERATED_MARKER = `${GENERATED_MARKER_PREFIX} v${getVersion()} -->`;\n\n/**\n * Check if an AGENTS.md file was generated by us (contains marker).\n * Uses prefix-based check to work across different versions.\n */\nexport async function isGeneratedAgentsMd(filePath: string): Promise<boolean> {\n  try {\n    const content = await readFile(filePath, 'utf-8');\n    return content.includes(GENERATED_MARKER_PREFIX);\n  } catch {\n    return false;\n  }\n}\n\n/**\n * Write AGENTS.md for a directory with LLM-generated content.\n *\n * If a user-defined AGENTS.md exists (not generated by us), it will be\n * preserved as AGENTS.local.md. The user content is then prepended\n * verbatim above the generated content so AI agents see it first.\n *\n * With variant: writes to AGENTS.${variant}.md (no user-preservation needed).\n * Without variant: current behavior unchanged.\n *\n * @param dirPath - Directory to write AGENTS.md for\n * @param projectRoot - Project root for relative paths\n * @param content - LLM-generated AGENTS.md content\n * @param variant - Optional eval variant name (e.g., \"claude.haiku\")\n * @returns Path to written file\n */\nexport async function writeAgentsMd(\n  dirPath: string,\n  _projectRoot: string,\n  content: string,\n  variant?: string,\n): Promise<string> {\n  // Strip marker from LLM content (we add it ourselves)\n  let llmContent = content;\n  if (llmContent.startsWith(GENERATED_MARKER_PREFIX)) {\n    const markerEnd = llmContent.indexOf('-->');\n    if (markerEnd !== -1) {\n      llmContent = llmContent.slice(markerEnd + 3).replace(/^\\n+/, '');\n    }\n  }\n\n  // Variant mode: write directly to AGENTS.${variant}.md, no user-preservation\n  if (variant) {\n    const variantPath = path.join(dirPath, `AGENTS.${variant}.md`);\n    const parts: string[] = [GENERATED_MARKER, '', llmContent];\n    const finalContent = parts.join('\\n');\n    await mkdir(path.dirname(variantPath), { recursive: true });\n    await writeFile(variantPath, finalContent, 'utf-8');\n    return variantPath;\n  }\n\n  // Standard mode: preserve user-authored AGENTS.md\n  const agentsPath = path.join(dirPath, 'AGENTS.md');\n  const localPath = path.join(dirPath, 'AGENTS.local.md');\n\n  // Step 1: Preserve user-authored AGENTS.md (rename to AGENTS.local.md)\n  let hasLocalContent = false;\n  try {\n    const existingContent = await readFile(agentsPath, 'utf-8');\n    if (!existingContent.includes(GENERATED_MARKER_PREFIX)) {\n      await rename(agentsPath, localPath);\n      hasLocalContent = true;\n    }\n  } catch {\n    // No existing AGENTS.md\n  }\n\n  // Step 2: Check for already-renamed AGENTS.local.md (from a previous run)\n  if (!hasLocalContent && existsSync(localPath)) {\n    hasLocalContent = true;\n  }\n\n  // Step 3: Build final content with marker + optional @import + LLM content\n  const parts: string[] = [GENERATED_MARKER, ''];\n\n  if (hasLocalContent) {\n    parts.push('@AGENTS.local.md', '');\n  }\n\n  parts.push(llmContent);\n\n  const finalContent = parts.join('\\n');\n\n  await mkdir(path.dirname(agentsPath), { recursive: true });\n  await writeFile(agentsPath, finalContent, 'utf-8');\n\n  return agentsPath;\n}\n\n/**\n * Write a hub AGENTS.md that @-references the active eval variant.\n *\n * Handles AGENTS.local.md preservation (same as writeAgentsMd).\n *\n * @param dirPath - Directory to write AGENTS.md hub for\n * @param activeVariant - Variant name to reference (e.g., \"claude.haiku\")\n * @returns Path to written AGENTS.md hub\n */\nexport async function writeAgentsMdHub(\n  dirPath: string,\n  activeVariant: string,\n): Promise<string> {\n  const agentsPath = path.join(dirPath, 'AGENTS.md');\n  const localPath = path.join(dirPath, 'AGENTS.local.md');\n\n  // Preserve user-authored AGENTS.md (rename to AGENTS.local.md)\n  let hasLocalContent = false;\n  try {\n    const existingContent = await readFile(agentsPath, 'utf-8');\n    if (!existingContent.includes(GENERATED_MARKER_PREFIX)) {\n      await rename(agentsPath, localPath);\n      hasLocalContent = true;\n    }\n  } catch {\n    // No existing AGENTS.md\n  }\n\n  if (!hasLocalContent && existsSync(localPath)) {\n    hasLocalContent = true;\n  }\n\n  // Build hub content\n  const parts: string[] = [GENERATED_MARKER, ''];\n\n  if (hasLocalContent) {\n    parts.push('@AGENTS.local.md', '');\n  }\n\n  parts.push(`@AGENTS.${activeVariant}.md`, '');\n\n  const finalContent = parts.join('\\n');\n\n  await mkdir(path.dirname(agentsPath), { recursive: true });\n  await writeFile(agentsPath, finalContent, 'utf-8');\n\n  return agentsPath;\n}\n\n```\n\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n- Workflow & convention rules: if this file defines contribution guidelines, PR conventions, commit standards, testing mandates, tool usage requirements, approval workflows, AI agent behavioral instructions, or code conventions (naming standards, formatting rules, import ordering, linting policies), extract them as explicit, actionable rules. Sources: CONTRIBUTING.md, CI configs, PR templates, style guides, linter/formatter configs (.eslintrc, .prettierrc, .editorconfig), tsconfig strictness, README dev sections.\n\n\nTARGET LENGTH (MANDATORY):\n- Source file: 4868 characters\n- Target summary: ~487 characters (10% compression)\n- Maximum: 584 characters\n- Achieve this by: ultra-dense writing, minimal examples, single-sentence descriptions\n\nULTRA-COMPRESSION TECHNIQUES:\n- Use telegraphic style: \"exports foo, bar, baz\" not \"this file exports three functions\"\n- No code examples unless critical to understanding\n- One sentence per export maximum\n- Omit obvious parameter names (\"opts: Options\" not \"options: Options - configuration object\")\n- Use abbreviations: \"params\" not \"parameters\", \"config\" not \"configuration\"\n- Skip section if empty (don't write \"No special concerns\" or \"N/A\")\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nWORKFLOW & CONVENTION RULES (NEVER EXCLUDE):\n- Contribution requirements: branch naming, commit message formats, PR description templates\n- Testing mandates: required test types, coverage thresholds, \"never delete tests\" policies\n- Tool usage rules: required CLI commands, browser automation tools, linter/formatter requirements\n- Approval workflows: review requirements, CI checks that must pass, deployment gates\n- AI agent instructions: explicit directives for automated assistants (coding rules, prohibited actions)\n- Code conventions: naming standards (camelCase, PascalCase, snake_case for what), import ordering rules, file/folder naming patterns, formatting rules (indentation, quotes, semicolons), language idioms, TypeScript strictness policies, preferred patterns vs anti-patterns\nThese define how contributors (human or AI) must work within the project.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion .annex.sum file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**agents-md.ts manages AGENTS.md file generation and preservation, handling marker-based detection, user-content preservation via AGENTS.local.md, and multi-variant file writing.**\n\n## Exports\n\n`GENERATED_MARKER_PREFIX` — constant `'<!-- Generated by agents-reverse-engineer'` for version-agnostic marker detection.\n\n`GENERATED_MARKER` — constant `${GENERATED_MARKER_PREFIX} v${version} -->` for version-specific file identification.\n\n`isGeneratedAgentsMd(filePath: string): Promise<boolean>` — returns true if file contains GENERATED_MARKER_PREFIX (catches readFile errors as false).\n\n`writeAgentsMd(dirPath, _projectRoot, content, variant?): Promise<string>` — writes AGENTS.md with LLM content; if variant provided, writes to AGENTS.${variant}.md directly; otherwise preserves user-authored AGENTS.md by renaming to AGENTS.local.md and prepending `@AGENTS.local.md` reference above generated content; strips GENERATED_MARKER_PREFIX from input content before writing.\n\n`writeAgentsMdHub(dirPath, activeVariant): Promise<string>` — writes hub AGENTS.md referencing active variant via `@AGENTS.${activeVariant}.md`; preserves user content as AGENTS.local.md with `@AGENTS.local.md` reference.\n\n## Key Behaviors\n\nBoth `writeAgentsMd` and `writeAgentsMdHub` follow identical user-preservation workflow: (1) check existing AGENTS.md for GENERATED_MARKER_PREFIX, (2) if absent, rename to AGENTS.local.md, (3) check for pre-existing AGENTS.local.md from prior runs, (4) prepend `@AGENTS.local.md` to generated content if local content exists. All file writes include GENERATED_MARKER with version. Marker stripping removes leading `GENERATED_MARKER_PREFIX` + content up to `-->` + leading newlines from LLM input.",
      "model": "claude-haiku-4-5-20251001",
      "inputTokens": 3,
      "outputTokens": 474,
      "cacheReadTokens": 16147,
      "cacheCreationTokens": 4277,
      "latencyMs": 11060,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/generation/writers/agents-md.ts",
          "sizeBytes": 4868
        }
      ]
    },
    {
      "timestamp": "2026-02-14T13:46:28.616Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: src/generation/writers/claude-md.ts\n\n```typescript\nimport { readFile, rename, writeFile } from 'node:fs/promises';\nimport { existsSync } from 'node:fs';\nimport path from 'node:path';\nimport { GENERATED_MARKER, GENERATED_MARKER_PREFIX } from './agents-md.js';\n\n/**\n * Write a CLAUDE.md file that imports the companion AGENTS.md via @ reference.\n * If a user-authored CLAUDE.md already exists (no marker), rename it to\n * CLAUDE.local.md and add an @CLAUDE.local.md import to preserve user content.\n * No AI call needed — this is a deterministic template.\n */\nexport async function writeClaudeMdPointer(dirAbsolutePath: string): Promise<string> {\n  const claudeMdPath = path.join(dirAbsolutePath, 'CLAUDE.md');\n  const claudeLocalPath = path.join(dirAbsolutePath, 'CLAUDE.local.md');\n  let hasLocalContent = false;\n\n  // Check for existing CLAUDE.md\n  if (existsSync(claudeMdPath)) {\n    try {\n      const existing = await readFile(claudeMdPath, 'utf-8');\n      if (!existing.includes(GENERATED_MARKER_PREFIX)) {\n        // User-authored — preserve as CLAUDE.local.md\n        await rename(claudeMdPath, claudeLocalPath);\n        hasLocalContent = true;\n      }\n    } catch {\n      // Unreadable — overwrite\n    }\n  }\n\n  // Check if CLAUDE.local.md exists (from this run or a previous one)\n  if (!hasLocalContent && existsSync(claudeLocalPath)) {\n    hasLocalContent = true;\n  }\n\n  // Build content with imports\n  const lines = [GENERATED_MARKER, ''];\n  if (hasLocalContent) {\n    lines.push('@CLAUDE.local.md');\n  }\n  lines.push('@AGENTS.md', '');\n\n  await writeFile(claudeMdPath, lines.join('\\n'), 'utf-8');\n  return claudeMdPath;\n}\n\n```\n\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n- Workflow & convention rules: if this file defines contribution guidelines, PR conventions, commit standards, testing mandates, tool usage requirements, approval workflows, AI agent behavioral instructions, or code conventions (naming standards, formatting rules, import ordering, linting policies), extract them as explicit, actionable rules. Sources: CONTRIBUTING.md, CI configs, PR templates, style guides, linter/formatter configs (.eslintrc, .prettierrc, .editorconfig), tsconfig strictness, README dev sections.\n\n\nTARGET LENGTH (MANDATORY):\n- Source file: 1581 characters\n- Target summary: ~158 characters (10% compression)\n- Maximum: 190 characters\n- Achieve this by: ultra-dense writing, minimal examples, single-sentence descriptions\n\nULTRA-COMPRESSION TECHNIQUES:\n- Use telegraphic style: \"exports foo, bar, baz\" not \"this file exports three functions\"\n- No code examples unless critical to understanding\n- One sentence per export maximum\n- Omit obvious parameter names (\"opts: Options\" not \"options: Options - configuration object\")\n- Use abbreviations: \"params\" not \"parameters\", \"config\" not \"configuration\"\n- Skip section if empty (don't write \"No special concerns\" or \"N/A\")\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nWORKFLOW & CONVENTION RULES (NEVER EXCLUDE):\n- Contribution requirements: branch naming, commit message formats, PR description templates\n- Testing mandates: required test types, coverage thresholds, \"never delete tests\" policies\n- Tool usage rules: required CLI commands, browser automation tools, linter/formatter requirements\n- Approval workflows: review requirements, CI checks that must pass, deployment gates\n- AI agent instructions: explicit directives for automated assistants (coding rules, prohibited actions)\n- Code conventions: naming standards (camelCase, PascalCase, snake_case for what), import ordering rules, file/folder naming patterns, formatting rules (indentation, quotes, semicolons), language idioms, TypeScript strictness policies, preferred patterns vs anti-patterns\nThese define how contributors (human or AI) must work within the project.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion .annex.sum file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**writeClaudeMdPointer() generates CLAUDE.md with @-references to AGENTS.md and optional CLAUDE.local.md, preserving user content.**\n\n## Exports\n- `writeClaudeMdPointer(dirAbsolutePath: string): Promise<string>` — creates/updates CLAUDE.md at path, returns claudeMdPath\n\n## Dependencies\n- `GENERATED_MARKER`, `GENERATED_MARKER_PREFIX` from `./agents-md.js` — sentinel strings for distinguishing AI-generated vs user-authored files\n- Node fs/promises: `readFile`, `rename`, `writeFile`; fs: `existsSync`; path module\n\n## Workflow\n1. Checks existing CLAUDE.md for `GENERATED_MARKER_PREFIX`; if absent (user-authored), renames to CLAUDE.local.md\n2. Detects CLAUDE.local.md from current or prior runs via `existsSync`\n3. Builds lines array: [GENERATED_MARKER, '', optional '@CLAUDE.local.md', '@AGENTS.md', '']\n4. Writes joined content to claudeMdPath with utf-8 encoding\n\n## Behavioral Contracts\n- File paths: `CLAUDE.md`, `CLAUDE.local.md` (sibling in dirAbsolutePath)\n- Content format: newline-joined lines; @-references must be exact identifiers (`@CLAUDE.local.md`, `@AGENTS.md`)\n- Detection rule: user content identified by absence of GENERATED_MARKER_PREFIX substring\n- Idempotent: CLAUDE.local.md existence checked twice (after rename attempt, then independent check) to handle both fresh and repeat runs",
      "model": "claude-haiku-4-5-20251001",
      "inputTokens": 3,
      "outputTokens": 394,
      "cacheReadTokens": 0,
      "cacheCreationTokens": 19392,
      "latencyMs": 11240,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/generation/writers/claude-md.ts",
          "sizeBytes": 1587
        }
      ]
    },
    {
      "timestamp": "2026-02-14T13:46:27.294Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: src/generation/prompts/templates.ts\n\n```typescript\n/**\n * Prompt constants for file and directory analysis.\n */\n\nexport const FILE_SYSTEM_PROMPT = `You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n- Workflow & convention rules: if this file defines contribution guidelines, PR conventions, commit standards, testing mandates, tool usage requirements, approval workflows, AI agent behavioral instructions, or code conventions (naming standards, formatting rules, import ordering, linting policies), extract them as explicit, actionable rules. Sources: CONTRIBUTING.md, CI configs, PR templates, style guides, linter/formatter configs (.eslintrc, .prettierrc, .editorconfig), tsconfig strictness, README dev sections.\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nWORKFLOW & CONVENTION RULES (NEVER EXCLUDE):\n- Contribution requirements: branch naming, commit message formats, PR description templates\n- Testing mandates: required test types, coverage thresholds, \"never delete tests\" policies\n- Tool usage rules: required CLI commands, browser automation tools, linter/formatter requirements\n- Approval workflows: review requirements, CI checks that must pass, deployment gates\n- AI agent instructions: explicit directives for automated assistants (coding rules, prohibited actions)\n- Code conventions: naming standards (camelCase, PascalCase, snake_case for what), import ordering rules, file/folder naming patterns, formatting rules (indentation, quotes, semicolons), language idioms, TypeScript strictness policies, preferred patterns vs anti-patterns\nThese define how contributors (human or AI) must work within the project.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - \\`FILE_SYSTEM_PROMPT\\` — system prompt for file analysis (250 lines)\n  - \\`DIRECTORY_SYSTEM_PROMPT\\` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion .annex.sum file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation`;\n\nexport const FILE_USER_PROMPT = `Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: {{FILE_PATH}}\n\n\\`\\`\\`typescript\n{{CONTENT}}\n\\`\\`\\`\n\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.`;\n\n/**\n * System prompt for directory-level AGENTS.md generation.\n * Used by buildDirectoryPrompt() in builder.ts.\n */\nexport const DIRECTORY_SYSTEM_PROMPT = `You are generating an AGENTS.md file — a directory-level overview for AI coding assistants.\n\nCRITICAL: Output ONLY the raw markdown content. No code fences, no preamble, no explanations, no conversational text. Your entire response IS the AGENTS.md file content.\n\nOUTPUT FORMAT:\n- First line MUST be exactly: <!-- Generated by agents-reverse-engineer -->\n- Use a # heading with the directory name\n- Write a one-paragraph purpose statement for the directory\nusage\nADAPTIVE SECTIONS:\nAnalyze the directory contents and choose the most relevant sections. Do NOT use a fixed template. Instead, select sections that best document this specific directory for an AI that needs to reconstruct or extend the project.\n\nConsider these section types (choose what applies):\n- **Contents**: Group files by purpose/category under ## headings. For each file: markdown link [filename](./filename) and a one-line description.\n- **Subdirectories**: If subdirectories exist, list them with links [dirname/](./dirname/) and brief summaries.\n- **Architecture / Data Flow**: If files form a pipeline, request/response chain, or layered architecture, document it.\n- **Stack**: If this is a package root (has package.json, Cargo.toml, go.mod, etc.), document the technology stack, key scripts, and entry points.\n- **Structure**: If the directory layout follows a convention (feature-sliced, domain-driven, MVC, etc.), document it.\n- **Patterns**: If files share recurring design patterns (factory, strategy, middleware, barrel re-export), name and document them.\n- **Configuration**: If the directory contains config files, schemas, or environment definitions, document the config surface area.\n- **API Surface**: If the directory exports a public API (barrel index, route definitions, SDK), document the interface contract.\n- **File Relationships**: How files collaborate, depend on each other, or share state.\n- **Behavioral Contracts**: If files contain regex patterns, format specifications, magic constants, or template strings that define observable behavior, collect them in a dedicated section. Preserve verbatim patterns from file summaries — do NOT paraphrase regex into prose. This section is MANDATORY when file summaries contain behavioral artifacts.\n- **Reproduction-Critical Constants**: If file summaries reference annex files (via ## Annex References sections), list them with links. Example: \"Full prompt template text: [templates.annex.sum](./prompts/templates.annex.sum)\". Do NOT reproduce annex content in AGENTS.md — just link to it.\n- **Workflow & Conventions**: If file summaries contain contribution guidelines, PR conventions, commit standards, testing mandates, tool usage rules, AI agent instructions, or code conventions (naming, formatting, linting), collect them into actionable directives. Source from CONTRIBUTING.md, CI configs, PR templates, linter/formatter configs, README dev sections. Present as concrete rules (\"commits must use conventionalcommits format\", \"use camelCase for functions, PascalCase for types\") not vague descriptions (\"the project has guidelines\").\n\nChoose any relevant sections or create your own based on the directory contents. The goal is to provide a comprehensive overview that captures the essence of the directory's role in the project and how its files work together, with a focus on what an AI coding assistant would need to know to effectively interact with this code.\n\nSCOPE:\n- AGENTS.md is a NAVIGATIONAL INDEX — help an AI find the right file quickly\n- Focus on: what each file does, how files relate, directory-level patterns\n- Do NOT reproduce full architecture sections — those belong in the root CLAUDE.md\n\nPATH ACCURACY (MANDATORY):\n- When referencing files or modules outside this directory, use ONLY paths from the \"Import Map\" section\n- Do NOT invent, rename, or guess module paths — if a path isn't in the Import Map, don't reference it\n- Use the exact directory names from \"Project Directory Structure\" — do NOT rename directories\n  (e.g., if the directory is called \"cli\", write \"src/cli/\", NOT \"src/commands/\")\n- Cross-module references must use the specifier format from actual import statements\n  (e.g., \"../generation/writers/sum.js\", NOT \"../fs/sum-file.js\")\n- If you are unsure about a path, omit the cross-reference rather than guessing\n\nCONSISTENCY (MANDATORY):\n- Do not contradict yourself within the same document\n- If you describe a technique (e.g., \"regex-based\"), do not call it something else later (e.g., \"AST-based\")\n- When stating version numbers, engines, or config fields, use ONLY values present in the file summaries\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this directory\", \"this module\", \"provides\", \"responsible for\", \"is used to\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Per-file descriptions in Contents sections: 1-2 sentences maximum. Reference key symbols but do not reproduce full summaries.\n- Behavioral contracts (regex patterns, format specs, constants) belong in a separate Behavioral Contracts section, not in per-file descriptions.\n- Annex file references: link to .annex.sum files, do not inline their content. One line per annex reference.\n- Subdirectory descriptions: 1-2 sentences maximum. Capture the directory's role, not its full contents.\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- Key exported symbols from file summaries MUST appear in the directory overview\n- Preserve exact casing of identifiers\n\nUSER NOTES:\n- If \"User Notes\" are provided in the prompt, they contain user-defined instructions that will be automatically prepended to your output\n- Do NOT repeat or paraphrase user notes in your generated content — they are included separately\n- You may reference information from user notes for context`;\n\n/**\n * System prompt for incremental file summary updates.\n * Used by buildFilePrompt() when existingSum is provided.\n */\nexport const FILE_UPDATE_SYSTEM_PROMPT = `You are updating an existing file summary for an AI coding assistant. The source code has changed and the summary needs to reflect those changes.\n\nCRITICAL — INCREMENTAL UPDATE RULES:\n- You are given the EXISTING summary and the UPDATED source code\n- Preserve the structure, section headings, and phrasing of the existing summary wherever the underlying code is unchanged\n- Only modify content that is directly affected by the code changes\n- If a section describes code that has not changed, keep it VERBATIM — do not rephrase, reorganize, or \"improve\" stable text\n- Add new sections only if the code changes introduce entirely new concepts\n- Remove sections only if the code they described has been deleted\n- Update signatures, type names, and identifiers to match the current source exactly\n\nBEHAVIORAL CONTRACT PRESERVATION (MANDATORY):\n- Regex patterns, format strings, magic constants, and template content from the existing summary MUST be preserved verbatim unless the source code changed them\n- If source code changes a regex pattern or constant, update the summary to show the NEW value verbatim\n- Never summarize or paraphrase regex patterns — always show the exact pattern in backticks\n\nWORKFLOW & CONVENTION RULE PRESERVATION (MANDATORY):\n- Contribution guidelines, testing mandates, PR conventions, code conventions, and AI agent instructions from the existing summary MUST be preserved verbatim unless the source file changed them\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers\n- Missing any exported identifier is a failure\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation`;\n\n/**\n * System prompt for incremental directory AGENTS.md updates.\n * Used by buildDirectoryPrompt() when existingAgentsMd is provided.\n */\nexport const DIRECTORY_UPDATE_SYSTEM_PROMPT = `You are updating an existing AGENTS.md file — a directory-level overview for AI coding assistants. Some file summaries or subdirectory documents have changed, and AGENTS.md needs to reflect those changes.\n\nCRITICAL — INCREMENTAL UPDATE RULES:\n- You are given the EXISTING AGENTS.md and the CURRENT file summaries and subdirectory documents\n- Preserve the structure, section headings, and descriptions that are still accurate\n- Only modify entries for files or subdirectories whose summaries have changed\n- Add entries for new files, remove entries for deleted files\n- Do NOT reorganize, rephrase, or restructure sections that are unaffected by changes\n- Keep the same section ordering unless files were added/removed in a way that requires regrouping\n- Behavioral Contracts section: preserve verbatim regex patterns and constants unless source file summaries show they changed\n- Reproduction-Critical Constants: if file summaries reference annex files (.annex.sum), preserve the links. Add links for new annexes, remove links for deleted ones.\n- Workflow & Conventions section: preserve existing rules unless source file summaries show they changed. Add rules from new files, remove rules for deleted files.\n\nCRITICAL: Output ONLY the raw markdown content. No code fences, no preamble, no explanations, no conversational text. Your entire response IS the AGENTS.md file content.\n\nOUTPUT FORMAT:\n- First line MUST be exactly: <!-- Generated by agents-reverse-engineer -->\n- Use a # heading with the directory name\n- Preserve the existing purpose statement unless the directory's role has fundamentally changed\n\nSCOPE:\n- AGENTS.md is a NAVIGATIONAL INDEX — help an AI find the right file quickly\n- Focus on: what each file does, how files relate, directory-level patterns\n- Do NOT reproduce full architecture sections — those belong in the root CLAUDE.md\n\nPATH ACCURACY (MANDATORY):\n- When referencing files or modules outside this directory, use ONLY paths from the \"Import Map\" section\n- Do NOT invent, rename, or guess module paths — if a path isn't in the Import Map, don't reference it\n- Use the exact directory names from \"Project Directory Structure\" — do NOT rename directories\n- Cross-module references must use the specifier format from actual import statements\n- If you are unsure about a path, omit the cross-reference rather than guessing\n\nCONSISTENCY (MANDATORY):\n- Do not contradict yourself within the same document\n- If you describe a technique (e.g., \"regex-based\"), do not call it something else later (e.g., \"AST-based\")\n- When stating version numbers, engines, or config fields, use ONLY values present in the file summaries\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this directory\", \"this module\", \"provides\", \"responsible for\", \"is used to\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Per-file descriptions: 1-2 sentences maximum. Reference key symbols but do not reproduce full summaries.\n- Subdirectory descriptions: 1-2 sentences maximum. Capture the directory's role, not its full contents.\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- Key exported symbols from file summaries MUST appear in the directory overview\n- Preserve exact casing of identifiers\n\nUSER NOTES:\n- If \"User Notes\" are provided in the prompt, they contain user-defined instructions that will be automatically prepended to your output\n- Do NOT repeat or paraphrase user notes in your generated content — they are included separately\n- You may reference information from user notes for context`;\n\n\n```\n\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n- Workflow & convention rules: if this file defines contribution guidelines, PR conventions, commit standards, testing mandates, tool usage requirements, approval workflows, AI agent behavioral instructions, or code conventions (naming standards, formatting rules, import ordering, linting policies), extract them as explicit, actionable rules. Sources: CONTRIBUTING.md, CI configs, PR templates, style guides, linter/formatter configs (.eslintrc, .prettierrc, .editorconfig), tsconfig strictness, README dev sections.\n\n\nTARGET LENGTH (MANDATORY):\n- Source file: 19318 characters\n- Target summary: ~1932 characters (10% compression)\n- Maximum: 2318 characters\n- Achieve this by: ultra-dense writing, minimal examples, single-sentence descriptions\n\nULTRA-COMPRESSION TECHNIQUES:\n- Use telegraphic style: \"exports foo, bar, baz\" not \"this file exports three functions\"\n- No code examples unless critical to understanding\n- One sentence per export maximum\n- Omit obvious parameter names (\"opts: Options\" not \"options: Options - configuration object\")\n- Use abbreviations: \"params\" not \"parameters\", \"config\" not \"configuration\"\n- Skip section if empty (don't write \"No special concerns\" or \"N/A\")\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nWORKFLOW & CONVENTION RULES (NEVER EXCLUDE):\n- Contribution requirements: branch naming, commit message formats, PR description templates\n- Testing mandates: required test types, coverage thresholds, \"never delete tests\" policies\n- Tool usage rules: required CLI commands, browser automation tools, linter/formatter requirements\n- Approval workflows: review requirements, CI checks that must pass, deployment gates\n- AI agent instructions: explicit directives for automated assistants (coding rules, prohibited actions)\n- Code conventions: naming standards (camelCase, PascalCase, snake_case for what), import ordering rules, file/folder naming patterns, formatting rules (indentation, quotes, semicolons), language idioms, TypeScript strictness policies, preferred patterns vs anti-patterns\nThese define how contributors (human or AI) must work within the project.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion .annex.sum file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**templates.ts exports four system prompts that orchestrate AI-driven code documentation generation across file and directory analysis workflows.**\n\n## Exported Constants\n\n`FILE_SYSTEM_PROMPT` — System prompt instructing AI to analyze individual source files and produce dense, identifier-rich summaries (19,318 chars). Mandates density rules (every sentence references specific identifiers), anchor term preservation (exact function/class/type names), behavioral contract extraction (regex patterns, format strings, magic constants, error codes, environment variables), and workflow/convention rules (contribution guidelines, testing mandates, code conventions, AI agent directives). Defines reproduction-critical content handling via Annex References sections for large string constants.\n\n`FILE_USER_PROMPT` — User-facing prompt template for file analysis (templated with `{{FILE_PATH}}` and `{{CONTENT}}`). Requires purpose statement as first bold line followed by ## headings organizing exported symbols with signatures; mandates minimum sections for purpose and exported symbols.\n\n`DIRECTORY_SYSTEM_PROMPT` — System prompt for AGENTS.md generation at directory scope (used by buildDirectoryPrompt() in builder.ts). Outputs raw markdown only. Mandates first line comment `<!-- Generated by agents-reverse-engineer -->`, adaptive section selection (Contents, Subdirectories, Architecture, Stack, Structure, Patterns, Configuration, API Surface, File Relationships, Behavioral Contracts, Reproduction-Critical Constants, Workflow & Conventions). Enforces path accuracy via Import Map and Project Directory Structure, density rules, and anchor term preservation. Collects behavioral contracts (regex patterns, format specs, constants) in dedicated section with verbatim preservation.\n\n`DIRECTORY_UPDATE_SYSTEM_PROMPT` — System prompt for incremental AGENTS.md updates when directory contents change. Preserves existing structure/headings for unchanged files, modifies only affected entries, adds/removes files as needed. Mandates verbatim preservation of behavioral contracts and reproduction-critical constants unless source summaries show changes. Outputs raw markdown with `<!-- Generated by agents-reverse-engineer -->` header.\n\n`FILE_UPDATE_SYSTEM_PROMPT` — System prompt for incremental file summary updates when source code changes. Preserves existing structure and phrasing for unchanged code, modifies only affected sections, updates signatures/types to match current source. Mandates verbatim preservation of regex patterns, format strings, magic constants, and template content unless source changed them.",
      "model": "claude-haiku-4-5-20251001",
      "inputTokens": 3,
      "outputTokens": 542,
      "cacheReadTokens": 16147,
      "cacheCreationTokens": 7193,
      "latencyMs": 12872,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/generation/prompts/templates.ts",
          "sizeBytes": 19374
        }
      ]
    },
    {
      "timestamp": "2026-02-14T13:46:29.412Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: src/generation/writers/index.ts\n\n```typescript\nexport {\n  writeSumFile,\n  readSumFile,\n  getSumPath,\n  sumFileExists,\n  getAnnexPath,\n  writeAnnexFile,\n  type SumFileContent,\n} from './sum.js';\n\nexport {\n  writeAgentsMd,\n  writeAgentsMdHub,\n} from './agents-md.js';\n\nexport {\n  writeClaudeMdPointer,\n} from './claude-md.js';\n\n```\n\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n- Workflow & convention rules: if this file defines contribution guidelines, PR conventions, commit standards, testing mandates, tool usage requirements, approval workflows, AI agent behavioral instructions, or code conventions (naming standards, formatting rules, import ordering, linting policies), extract them as explicit, actionable rules. Sources: CONTRIBUTING.md, CI configs, PR templates, style guides, linter/formatter configs (.eslintrc, .prettierrc, .editorconfig), tsconfig strictness, README dev sections.\n\n\nTARGET LENGTH (MANDATORY):\n- Source file: 278 characters\n- Target summary: ~28 characters (10% compression)\n- Maximum: 34 characters\n- Achieve this by: ultra-dense writing, minimal examples, single-sentence descriptions\n\nULTRA-COMPRESSION TECHNIQUES:\n- Use telegraphic style: \"exports foo, bar, baz\" not \"this file exports three functions\"\n- No code examples unless critical to understanding\n- One sentence per export maximum\n- Omit obvious parameter names (\"opts: Options\" not \"options: Options - configuration object\")\n- Use abbreviations: \"params\" not \"parameters\", \"config\" not \"configuration\"\n- Skip section if empty (don't write \"No special concerns\" or \"N/A\")\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nWORKFLOW & CONVENTION RULES (NEVER EXCLUDE):\n- Contribution requirements: branch naming, commit message formats, PR description templates\n- Testing mandates: required test types, coverage thresholds, \"never delete tests\" policies\n- Tool usage rules: required CLI commands, browser automation tools, linter/formatter requirements\n- Approval workflows: review requirements, CI checks that must pass, deployment gates\n- AI agent instructions: explicit directives for automated assistants (coding rules, prohibited actions)\n- Code conventions: naming standards (camelCase, PascalCase, snake_case for what), import ordering rules, file/folder naming patterns, formatting rules (indentation, quotes, semicolons), language idioms, TypeScript strictness policies, preferred patterns vs anti-patterns\nThese define how contributors (human or AI) must work within the project.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion .annex.sum file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**src/generation/writers/index.ts re-exports documentation generation writers for .sum files, AGENTS.md, and Claude.md pointers.**\n\n## Exports\n\n- `writeSumFile(content: SumFileContent): void` — writes AI-friendly documentation to .sum file\n- `readSumFile(path: string): SumFileContent` — parses .sum file content\n- `getSumPath(filePath: string): string` — resolves .sum output path\n- `sumFileExists(path: string): boolean` — checks .sum file existence\n- `getAnnexPath(filePath: string): string` — resolves .annex.sum output path for reproduction-critical constants\n- `writeAnnexFile(path: string, content: string): void` — writes annex file with large constant definitions\n- `writeAgentsMd(agents: Agent[]): void` — generates AGENTS.md hub documentation\n- `writeAgentsMdHub(config: HubConfig): void` — generates AGENTS.md with hub-level metadata\n- `writeClaudeMdPointer(pointer: ClaudePointer): void` — writes Claude.md navigation pointer\n\n## Architecture\n\nAggregation barrel: centralizes all writer module exports (sum.js, agents-md.js, claude-md.js) for unified import paths.",
      "model": "claude-haiku-4-5-20251001",
      "inputTokens": 3,
      "outputTokens": 308,
      "cacheReadTokens": 16147,
      "cacheCreationTokens": 2844,
      "latencyMs": 12224,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/generation/writers/index.ts",
          "sizeBytes": 278
        }
      ]
    },
    {
      "timestamp": "2026-02-14T13:46:29.972Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: src/quality/inconsistency/code-vs-code.ts\n\n```typescript\n/**\n * Heuristic cross-file inconsistency detection.\n *\n * Detects duplicate exports: the same symbol name exported from multiple\n * files within a group. The caller is responsible for scoping the file\n * list (typically per-directory) to avoid false positives across\n * unrelated modules.\n */\n\nimport { extractExports } from './code-vs-doc.js';\nimport type { CodeCodeInconsistency } from '../types.js';\n\n/**\n * Detect duplicate exports across a group of files.\n *\n * For each file, extracts exported symbol names via {@link extractExports}\n * and flags any symbol that appears in more than one file.\n *\n * This is a heuristic-only check (no AI calls). The caller should scope\n * the input to per-directory file groups to avoid false positives.\n *\n * @param files - Array of source files with path and content\n * @returns Array of detected cross-file inconsistencies\n */\nexport function checkCodeVsCode(\n  files: Array<{ path: string; content: string }>,\n): CodeCodeInconsistency[] {\n  // Map: export name -> list of file paths that export it\n  const exportMap = new Map<string, string[]>();\n\n  for (const file of files) {\n    const exports = extractExports(file.content);\n    for (const name of exports) {\n      const paths = exportMap.get(name);\n      if (paths) {\n        paths.push(file.path);\n      } else {\n        exportMap.set(name, [file.path]);\n      }\n    }\n  }\n\n  const inconsistencies: CodeCodeInconsistency[] = [];\n\n  for (const [name, paths] of exportMap) {\n    if (paths.length > 1) {\n      inconsistencies.push({\n        type: 'code-vs-code',\n        severity: 'warning',\n        files: paths,\n        description: `Symbol \"${name}\" exported from ${paths.length} files`,\n        pattern: 'duplicate-export',\n      });\n    }\n  }\n\n  return inconsistencies;\n}\n\n```\n\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n- Workflow & convention rules: if this file defines contribution guidelines, PR conventions, commit standards, testing mandates, tool usage requirements, approval workflows, AI agent behavioral instructions, or code conventions (naming standards, formatting rules, import ordering, linting policies), extract them as explicit, actionable rules. Sources: CONTRIBUTING.md, CI configs, PR templates, style guides, linter/formatter configs (.eslintrc, .prettierrc, .editorconfig), tsconfig strictness, README dev sections.\n\n\nTARGET LENGTH (MANDATORY):\n- Source file: 1774 characters\n- Target summary: ~177 characters (10% compression)\n- Maximum: 212 characters\n- Achieve this by: ultra-dense writing, minimal examples, single-sentence descriptions\n\nULTRA-COMPRESSION TECHNIQUES:\n- Use telegraphic style: \"exports foo, bar, baz\" not \"this file exports three functions\"\n- No code examples unless critical to understanding\n- One sentence per export maximum\n- Omit obvious parameter names (\"opts: Options\" not \"options: Options - configuration object\")\n- Use abbreviations: \"params\" not \"parameters\", \"config\" not \"configuration\"\n- Skip section if empty (don't write \"No special concerns\" or \"N/A\")\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nWORKFLOW & CONVENTION RULES (NEVER EXCLUDE):\n- Contribution requirements: branch naming, commit message formats, PR description templates\n- Testing mandates: required test types, coverage thresholds, \"never delete tests\" policies\n- Tool usage rules: required CLI commands, browser automation tools, linter/formatter requirements\n- Approval workflows: review requirements, CI checks that must pass, deployment gates\n- AI agent instructions: explicit directives for automated assistants (coding rules, prohibited actions)\n- Code conventions: naming standards (camelCase, PascalCase, snake_case for what), import ordering rules, file/folder naming patterns, formatting rules (indentation, quotes, semicolons), language idioms, TypeScript strictness policies, preferred patterns vs anti-patterns\nThese define how contributors (human or AI) must work within the project.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion .annex.sum file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**code-vs-code.ts detects duplicate symbol exports across co-located source files via heuristic analysis.**\n\n## Exports\n\n`checkCodeVsCode(files: Array<{ path: string; content: string }>): CodeCodeInconsistency[]` — scans files for symbols exported from multiple paths, returns inconsistencies with `type: 'code-vs-code'`, `severity: 'warning'`, `pattern: 'duplicate-export'`.\n\n## Dependencies & Integration\n\nCalls `extractExports(content: string)` from `code-vs-doc.js` to parse export names; returns `CodeCodeCodeInconsistency[]` typed from `../types.js`. Caller must scope input per-directory to avoid cross-module false positives.\n\n## Algorithm & Data Structure\n\nBuilds `Map<string, string[]>` tracking export names to file paths, flags entries with `paths.length > 1`; heuristic-only, no AI/LLM calls.\n\n## Behavioral Contract\n\nInconsistency objects: `{type: 'code-vs-code', severity: 'warning', files: paths[], description: 'Symbol \"[name]\" exported from [count] files', pattern: 'duplicate-export'}`.",
      "model": "claude-haiku-4-5-20251001",
      "inputTokens": 3,
      "outputTokens": 288,
      "cacheReadTokens": 16147,
      "cacheCreationTokens": 3255,
      "latencyMs": 11917,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/quality/inconsistency/code-vs-code.ts",
          "sizeBytes": 1774
        }
      ]
    },
    {
      "timestamp": "2026-02-14T13:46:31.294Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: src/quality/phantom-paths/index.ts\n\n```typescript\nexport { checkPhantomPaths } from './validator.js';\n\n```\n\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n- Workflow & convention rules: if this file defines contribution guidelines, PR conventions, commit standards, testing mandates, tool usage requirements, approval workflows, AI agent behavioral instructions, or code conventions (naming standards, formatting rules, import ordering, linting policies), extract them as explicit, actionable rules. Sources: CONTRIBUTING.md, CI configs, PR templates, style guides, linter/formatter configs (.eslintrc, .prettierrc, .editorconfig), tsconfig strictness, README dev sections.\n\n\nTARGET LENGTH (MANDATORY):\n- Source file: 52 characters\n- Target summary: ~5 characters (10% compression)\n- Maximum: 6 characters\n- Achieve this by: ultra-dense writing, minimal examples, single-sentence descriptions\n\nULTRA-COMPRESSION TECHNIQUES:\n- Use telegraphic style: \"exports foo, bar, baz\" not \"this file exports three functions\"\n- No code examples unless critical to understanding\n- One sentence per export maximum\n- Omit obvious parameter names (\"opts: Options\" not \"options: Options - configuration object\")\n- Use abbreviations: \"params\" not \"parameters\", \"config\" not \"configuration\"\n- Skip section if empty (don't write \"No special concerns\" or \"N/A\")\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nWORKFLOW & CONVENTION RULES (NEVER EXCLUDE):\n- Contribution requirements: branch naming, commit message formats, PR description templates\n- Testing mandates: required test types, coverage thresholds, \"never delete tests\" policies\n- Tool usage rules: required CLI commands, browser automation tools, linter/formatter requirements\n- Approval workflows: review requirements, CI checks that must pass, deployment gates\n- AI agent instructions: explicit directives for automated assistants (coding rules, prohibited actions)\n- Code conventions: naming standards (camelCase, PascalCase, snake_case for what), import ordering rules, file/folder naming patterns, formatting rules (indentation, quotes, semicolons), language idioms, TypeScript strictness policies, preferred patterns vs anti-patterns\nThese define how contributors (human or AI) must work within the project.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion .annex.sum file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**index.ts re-exports checkPhantomPaths validator for phantom path detection.**\n\n## Exports\n\n- `checkPhantomPaths` — validator function from './validator.js'",
      "model": "claude-haiku-4-5-20251001",
      "inputTokens": 3,
      "outputTokens": 45,
      "cacheReadTokens": 16147,
      "cacheCreationTokens": 2748,
      "latencyMs": 12106,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/quality/phantom-paths/index.ts",
          "sizeBytes": 52
        }
      ]
    },
    {
      "timestamp": "2026-02-14T13:46:30.594Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: src/quality/inconsistency/reporter.ts\n\n```typescript\n/**\n * Structured report builder and CLI formatter for inconsistency results.\n *\n * Aggregates individual inconsistency issues into a typed report with\n * summary counts, and formats the report as human-readable plain text\n * for CLI output. No color dependencies -- the reporter stays pure\n * so it can be tested easily.\n */\n\nimport type {\n  Inconsistency,\n  InconsistencyReport,\n} from '../types.js';\n\n/**\n * Build a structured inconsistency report from a list of issues.\n *\n * Computes summary counts by type (code-vs-doc, code-vs-code) and\n * severity (error, warning, info). Attaches run metadata including\n * timestamp, project root, files checked, and duration.\n *\n * @param issues - All detected inconsistencies\n * @param metadata - Run context (projectRoot, filesChecked, durationMs)\n * @returns Structured report with issues and summary counts\n */\nexport function buildInconsistencyReport(\n  issues: Inconsistency[],\n  metadata: { projectRoot: string; filesChecked: number; durationMs: number },\n): InconsistencyReport {\n  let codeVsDoc = 0;\n  let codeVsCode = 0;\n  let phantomPaths = 0;\n  let errors = 0;\n  let warnings = 0;\n  let info = 0;\n\n  for (const issue of issues) {\n    if (issue.type === 'code-vs-doc') codeVsDoc++;\n    if (issue.type === 'code-vs-code') codeVsCode++;\n    if (issue.type === 'phantom-path') phantomPaths++;\n    if (issue.severity === 'error') errors++;\n    if (issue.severity === 'warning') warnings++;\n    if (issue.severity === 'info') info++;\n  }\n\n  return {\n    metadata: {\n      timestamp: new Date().toISOString(),\n      projectRoot: metadata.projectRoot,\n      filesChecked: metadata.filesChecked,\n      durationMs: metadata.durationMs,\n    },\n    issues,\n    summary: {\n      total: issues.length,\n      codeVsDoc,\n      codeVsCode,\n      phantomPaths,\n      errors,\n      warnings,\n      info,\n    },\n  };\n}\n\n/**\n * Format an inconsistency report as human-readable plain text.\n *\n * Output format:\n * ```\n * === Inconsistency Report ===\n * Checked 42 files in 150ms\n * Found 3 issue(s)\n *\n * [ERROR] Documentation out of sync: 2 exports undocumented, 1 ...\n *   File: src/foo/bar.ts\n *\n * [WARN] Symbol \"Config\" exported from 2 files\n *   Files: src/config/a.ts, src/config/b.ts\n * ```\n *\n * Uses plain text only (no picocolors). Color can be added by the\n * CLI layer if needed.\n *\n * @param report - The structured inconsistency report\n * @returns Formatted string for stderr output\n */\nexport function formatReportForCli(report: InconsistencyReport): string {\n  const lines: string[] = [];\n\n  lines.push('=== Inconsistency Report ===');\n  lines.push(`Checked ${report.metadata.filesChecked} files in ${report.metadata.durationMs}ms`);\n  lines.push(`Found ${report.summary.total} issue(s)`);\n  lines.push('');\n\n  for (const issue of report.issues) {\n    const severityTag =\n      issue.severity === 'error' ? '[ERROR]' :\n      issue.severity === 'warning' ? '[WARN]' :\n      '[INFO]';\n\n    lines.push(`${severityTag} ${issue.description}`);\n\n    if (issue.type === 'code-vs-doc') {\n      lines.push(`  File: ${issue.filePath}`);\n    } else if (issue.type === 'phantom-path') {\n      lines.push(`  Doc: ${issue.agentsMdPath}`);\n      lines.push(`  Path: ${issue.details.referencedPath}`);\n    } else {\n      lines.push(`  Files: ${issue.files.join(', ')}`);\n    }\n\n    lines.push('');\n  }\n\n  return lines.join('\\n');\n}\n\n/**\n * Format an inconsistency report as GitHub-flavored Markdown.\n *\n * Suitable for PR comments, issue bodies, or documentation files.\n *\n * @param report - The structured inconsistency report\n * @returns Markdown-formatted string\n */\nexport function formatReportAsMarkdown(report: InconsistencyReport): string {\n  const lines: string[] = [];\n\n  lines.push('## Inconsistency Report');\n  lines.push('');\n  lines.push(`Checked **${report.metadata.filesChecked}** files in ${report.metadata.durationMs}ms — found **${report.summary.total}** issue(s).`);\n  lines.push('');\n\n  if (report.issues.length === 0) {\n    lines.push('No issues found.');\n    return lines.join('\\n');\n  }\n\n  lines.push('| Severity | Type | Description | Location |');\n  lines.push('|----------|------|-------------|----------|');\n\n  for (const issue of report.issues) {\n    const severity =\n      issue.severity === 'error' ? '`ERROR`' :\n      issue.severity === 'warning' ? '`WARN`' :\n      '`INFO`';\n\n    const type =\n      issue.type === 'code-vs-doc' ? 'code-vs-doc' :\n      issue.type === 'code-vs-code' ? 'code-vs-code' :\n      'phantom-path';\n\n    let location: string;\n    if (issue.type === 'code-vs-doc') {\n      location = `\\`${issue.filePath}\\``;\n    } else if (issue.type === 'phantom-path') {\n      location = `\\`${issue.agentsMdPath}\\` → \\`${issue.details.referencedPath}\\``;\n    } else {\n      location = issue.files.map(f => `\\`${f}\\``).join(', ');\n    }\n\n    lines.push(`| ${severity} | ${type} | ${issue.description} | ${location} |`);\n  }\n\n  lines.push('');\n\n  return lines.join('\\n');\n}\n\n```\n\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n- Workflow & convention rules: if this file defines contribution guidelines, PR conventions, commit standards, testing mandates, tool usage requirements, approval workflows, AI agent behavioral instructions, or code conventions (naming standards, formatting rules, import ordering, linting policies), extract them as explicit, actionable rules. Sources: CONTRIBUTING.md, CI configs, PR templates, style guides, linter/formatter configs (.eslintrc, .prettierrc, .editorconfig), tsconfig strictness, README dev sections.\n\n\nTARGET LENGTH (MANDATORY):\n- Source file: 4947 characters\n- Target summary: ~495 characters (10% compression)\n- Maximum: 594 characters\n- Achieve this by: ultra-dense writing, minimal examples, single-sentence descriptions\n\nULTRA-COMPRESSION TECHNIQUES:\n- Use telegraphic style: \"exports foo, bar, baz\" not \"this file exports three functions\"\n- No code examples unless critical to understanding\n- One sentence per export maximum\n- Omit obvious parameter names (\"opts: Options\" not \"options: Options - configuration object\")\n- Use abbreviations: \"params\" not \"parameters\", \"config\" not \"configuration\"\n- Skip section if empty (don't write \"No special concerns\" or \"N/A\")\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nWORKFLOW & CONVENTION RULES (NEVER EXCLUDE):\n- Contribution requirements: branch naming, commit message formats, PR description templates\n- Testing mandates: required test types, coverage thresholds, \"never delete tests\" policies\n- Tool usage rules: required CLI commands, browser automation tools, linter/formatter requirements\n- Approval workflows: review requirements, CI checks that must pass, deployment gates\n- AI agent instructions: explicit directives for automated assistants (coding rules, prohibited actions)\n- Code conventions: naming standards (camelCase, PascalCase, snake_case for what), import ordering rules, file/folder naming patterns, formatting rules (indentation, quotes, semicolons), language idioms, TypeScript strictness policies, preferred patterns vs anti-patterns\nThese define how contributors (human or AI) must work within the project.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion .annex.sum file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**reporter.ts builds structured inconsistency reports and formats them as CLI and Markdown output.**\n\n## Exports\n\n`buildInconsistencyReport(issues: Inconsistency[], metadata: { projectRoot: string; filesChecked: number; durationMs: number }): InconsistencyReport` — aggregates issues into typed report with summary counts (codeVsDoc, codeVsCode, phantomPaths) and metadata (timestamp ISO, projectRoot, filesChecked, durationMs).\n\n`formatReportForCli(report: InconsistencyReport): string` — formats report as plain-text CLI output with severity tags `[ERROR]`, `[WARN]`, `[INFO]` and location info (filePath for code-vs-doc, agentsMdPath+referencedPath for phantom-path, files array for code-vs-code).\n\n`formatReportAsMarkdown(report: InconsistencyReport): string` — formats report as GitHub Markdown table with columns Severity, Type, Description, Location; handles empty issue list.\n\n## Summary Structure\n\nInconsistencyReport contains: `metadata` (timestamp, projectRoot, filesChecked, durationMs), `issues` (Inconsistency[] array), `summary` (total, codeVsDoc, codeVsCode, phantomPaths, errors, warnings, info counts).\n\n## Issue Types & Severity\n\nInconsistency types: `'code-vs-doc'`, `'code-vs-code'`, `'phantom-path'`. Severities: `'error'`, `'warning'`, `'info'`. Each issue carries filePath, description, and type-specific properties (files array for code-vs-code; agentsMdPath + details.referencedPath for phantom-path).",
      "model": "claude-haiku-4-5-20251001",
      "inputTokens": 3,
      "outputTokens": 414,
      "cacheReadTokens": 16147,
      "cacheCreationTokens": 4372,
      "latencyMs": 13224,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/quality/inconsistency/reporter.ts",
          "sizeBytes": 4951
        }
      ]
    },
    {
      "timestamp": "2026-02-14T13:46:30.287Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: src/quality/inconsistency/code-vs-doc.ts\n\n```typescript\n/**\n * Heuristic code-vs-doc inconsistency detection.\n *\n * Compares exported symbols in TypeScript/JavaScript source against the\n * content of the corresponding .sum file to flag documentation drift.\n */\n\nimport type { SumFileContent } from '../../generation/writers/sum.js';\nimport type { CodeDocInconsistency } from '../types.js';\n\n/**\n * Extract named and default export identifiers from TypeScript/JavaScript source.\n *\n * Matches declarations like `export function foo`, `export const BAR`,\n * `export default class App`, etc. Ignores re-exports, commented-out lines,\n * and internal (non-exported) declarations.\n *\n * @param sourceContent - Raw source file content\n * @returns Array of exported identifier names\n */\nexport function extractExports(sourceContent: string): string[] {\n  const exports: string[] = [];\n  const exportRegex =\n    /^[ \\t]*export\\s+(?:default\\s+)?(?:function|class|const|let|var|type|interface|enum)\\s+(\\w+)/gm;\n  let match;\n  while ((match = exportRegex.exec(sourceContent)) !== null) {\n    exports.push(match[1]);\n  }\n  return exports;\n}\n\n/**\n * Compare source exports against .sum documentation content.\n *\n * Detects two kinds of inconsistency:\n * - **missingFromDoc**: symbols exported in source but not mentioned in .sum text\n * - **missingFromCode**: items listed in `publicInterface` with no matching export\n *\n * Uses case-sensitive matching. Returns `null` when documentation is consistent.\n *\n * @param sourceContent - Raw source file content\n * @param sumContent - Parsed .sum file content\n * @param filePath - Path to the source file (used in report)\n * @returns Inconsistency descriptor, or null if consistent\n */\nexport function checkCodeVsDoc(\n  sourceContent: string,\n  sumContent: SumFileContent,\n  filePath: string,\n): CodeDocInconsistency | null {\n  const exports = extractExports(sourceContent);\n  const sumText = sumContent.summary;\n\n  // Exports present in source but not mentioned anywhere in .sum text\n  const missingFromDoc = exports.filter((e) => !sumText.includes(e));\n\n  if (missingFromDoc.length === 0) {\n    return null;\n  }\n\n  return {\n    type: 'code-vs-doc',\n    severity: 'warning',\n    filePath,\n    sumPath: `${filePath}.sum`,\n    description: `Documentation out of sync: ${missingFromDoc.length} exports undocumented`,\n    details: { missingFromDoc, missingFromCode: [] },\n  };\n}\n\n```\n\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n- Workflow & convention rules: if this file defines contribution guidelines, PR conventions, commit standards, testing mandates, tool usage requirements, approval workflows, AI agent behavioral instructions, or code conventions (naming standards, formatting rules, import ordering, linting policies), extract them as explicit, actionable rules. Sources: CONTRIBUTING.md, CI configs, PR templates, style guides, linter/formatter configs (.eslintrc, .prettierrc, .editorconfig), tsconfig strictness, README dev sections.\n\n\nTARGET LENGTH (MANDATORY):\n- Source file: 2350 characters\n- Target summary: ~235 characters (10% compression)\n- Maximum: 282 characters\n- Achieve this by: ultra-dense writing, minimal examples, single-sentence descriptions\n\nULTRA-COMPRESSION TECHNIQUES:\n- Use telegraphic style: \"exports foo, bar, baz\" not \"this file exports three functions\"\n- No code examples unless critical to understanding\n- One sentence per export maximum\n- Omit obvious parameter names (\"opts: Options\" not \"options: Options - configuration object\")\n- Use abbreviations: \"params\" not \"parameters\", \"config\" not \"configuration\"\n- Skip section if empty (don't write \"No special concerns\" or \"N/A\")\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nWORKFLOW & CONVENTION RULES (NEVER EXCLUDE):\n- Contribution requirements: branch naming, commit message formats, PR description templates\n- Testing mandates: required test types, coverage thresholds, \"never delete tests\" policies\n- Tool usage rules: required CLI commands, browser automation tools, linter/formatter requirements\n- Approval workflows: review requirements, CI checks that must pass, deployment gates\n- AI agent instructions: explicit directives for automated assistants (coding rules, prohibited actions)\n- Code conventions: naming standards (camelCase, PascalCase, snake_case for what), import ordering rules, file/folder naming patterns, formatting rules (indentation, quotes, semicolons), language idioms, TypeScript strictness policies, preferred patterns vs anti-patterns\nThese define how contributors (human or AI) must work within the project.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion .annex.sum file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**code-vs-doc.ts detects drift between TypeScript/JavaScript exports and .sum documentation via heuristic symbol matching.**\n\n## Exports\n\n`extractExports(sourceContent: string): string[]` — extracts all named/default export identifiers from source using regex `/^[ \\t]*export\\s+(?:default\\s+)?(?:function|class|const|let|var|type|interface|enum)\\s+(\\w+)/gm`, matching function/class/const/let/var/type/interface/enum declarations, ignoring re-exports and comments.\n\n`checkCodeVsDoc(sourceContent: string, sumContent: SumFileContent, filePath: string): CodeDocInconsistency | null` — compares source exports against SumFileContent.summary text; returns CodeDocInconsistency with type='code-vs-doc', severity='warning', listing missingFromDoc (exports absent from sum text) and empty missingFromCode, or null if consistent.\n\n## Detection Logic\n\nCase-sensitive substring matching: exports in source are flagged if not found via `.includes()` in sumContent.summary. Reports only when missingFromDoc.length > 0; description format is \"Documentation out of sync: N exports undocumented\".",
      "model": "claude-haiku-4-5-20251001",
      "inputTokens": 3,
      "outputTokens": 286,
      "cacheReadTokens": 16147,
      "cacheCreationTokens": 3428,
      "latencyMs": 14519,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/quality/inconsistency/code-vs-doc.ts",
          "sizeBytes": 2350
        }
      ]
    },
    {
      "timestamp": "2026-02-14T13:46:29.732Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: src/generation/writers/sum.ts\n\n```typescript\nimport { writeFile, readFile, mkdir } from 'node:fs/promises';\nimport * as path from 'node:path';\nimport type { SummaryMetadata } from '../types.js';\nimport { GENERATED_MARKER } from './agents-md.js';\n\n/**\n * Content structure for a .sum file.\n */\nexport interface SumFileContent {\n  /** Main summary text (detailed description) */\n  summary: string;\n  /** Extracted metadata */\n  metadata: SummaryMetadata;\n  /** Generation timestamp */\n  generatedAt: string;\n  /** SHA-256 hash of source file content (for change detection) */\n  contentHash: string;\n}\n\n/**\n * Parse a .sum file back into structured content.\n * Returns null if file doesn't exist or is invalid.\n */\nexport async function readSumFile(sumPath: string): Promise<SumFileContent | null> {\n  try {\n    const content = await readFile(sumPath, 'utf-8');\n    return parseSumFile(content);\n  } catch {\n    return null;\n  }\n}\n\n/**\n * Parse a YAML-style array from frontmatter.\n * Supports both inline [a, b, c] and multi-line formats.\n */\nfunction parseYamlArray(frontmatter: string, key: string): string[] {\n  // Try inline format first: key: [a, b, c]\n  const inlineMatch = frontmatter.match(new RegExp(`${key}:\\\\s*\\\\[([^\\\\]]*)\\\\]`));\n  if (inlineMatch) {\n    return inlineMatch[1]\n      .split(',')\n      .map(s => s.trim().replace(/^[\"']|[\"']$/g, ''))\n      .filter(s => s.length > 0);\n  }\n\n  // Try multi-line format:\n  // key:\n  //   - item1\n  //   - item2\n  const multiLineMatch = frontmatter.match(new RegExp(`${key}:\\\\s*\\\\n((?:\\\\s+-\\\\s+.+\\\\n?)+)`, 'm'));\n  if (multiLineMatch) {\n    return multiLineMatch[1]\n      .split('\\n')\n      .map(line => line.replace(/^\\s*-\\s*/, '').trim())\n      .filter(s => s.length > 0);\n  }\n\n  return [];\n}\n\n/**\n * Parse .sum file content into structured data.\n */\nfunction parseSumFile(content: string): SumFileContent | null {\n  try {\n    // Extract frontmatter\n    const frontmatterMatch = content.match(/^---\\n([\\s\\S]*?)\\n---\\n/);\n    if (!frontmatterMatch) return null;\n\n    const frontmatter = frontmatterMatch[1];\n    const summary = content.slice(frontmatterMatch[0].length).trim();\n\n    // Parse frontmatter (simple YAML-like parsing)\n    const generatedAt = frontmatter.match(/generated_at:\\s*(.+)/)?.[1]?.trim() ?? '';\n    const contentHash = frontmatter.match(/content_hash:\\s*(.+)/)?.[1]?.trim() ?? '';\n\n    // Parse purpose (single line value)\n    const purpose = frontmatter.match(/purpose:\\s*(.+)/)?.[1]?.trim() ?? '';\n\n    const metadata: SummaryMetadata = {\n      purpose,\n    };\n\n    // Parse optional fields\n    const criticalTodos = parseYamlArray(frontmatter, 'critical_todos');\n    if (criticalTodos.length > 0) {\n      metadata.criticalTodos = criticalTodos;\n    }\n\n    const relatedFiles = parseYamlArray(frontmatter, 'related_files');\n    if (relatedFiles.length > 0) {\n      metadata.relatedFiles = relatedFiles;\n    }\n\n    return {\n      summary,\n      metadata,\n      generatedAt,\n      contentHash,\n    };\n  } catch {\n    return null;\n  }\n}\n\n/**\n * Format a YAML array for frontmatter.\n * Uses inline format for short arrays, multi-line for longer ones.\n */\nfunction formatYamlArray(key: string, values: string[]): string {\n  if (values.length === 0) {\n    return `${key}: []`;\n  }\n  if (values.length <= 3 && values.every(v => v.length < 40)) {\n    // Inline format for short arrays\n    return `${key}: [${values.join(', ')}]`;\n  }\n  // Multi-line format for longer arrays\n  return `${key}:\\n${values.map(v => `  - ${v}`).join('\\n')}`;\n}\n\n/**\n * Format .sum file content for writing.\n */\nfunction formatSumFile(content: SumFileContent): string {\n  const lines = [\n    '---',\n    `generated_at: ${content.generatedAt}`,\n    `content_hash: ${content.contentHash}`,\n    `purpose: ${content.metadata.purpose}`,\n  ];\n\n  // Add optional fields if present\n  if (content.metadata.criticalTodos && content.metadata.criticalTodos.length > 0) {\n    lines.push(formatYamlArray('critical_todos', content.metadata.criticalTodos));\n  }\n  if (content.metadata.relatedFiles && content.metadata.relatedFiles.length > 0) {\n    lines.push(formatYamlArray('related_files', content.metadata.relatedFiles));\n  }\n\n  lines.push('---', '');\n\n  return lines.join('\\n') + content.summary;\n}\n\n/**\n * Write a .sum file alongside a source file.\n * Creates: foo.ts -> foo.ts.sum (or foo.ts.claude.haiku.sum with variant)\n *\n * @param sourcePath - Path to the source file\n * @param content - Summary content to write\n * @param variant - Optional eval variant name (e.g., \"claude.haiku\")\n * @returns Path to the written .sum file\n */\nexport async function writeSumFile(\n  sourcePath: string,\n  content: SumFileContent,\n  variant?: string,\n): Promise<string> {\n  const sumPath = getSumPath(sourcePath, variant);\n  const dir = path.dirname(sumPath);\n\n  // Ensure directory exists\n  await mkdir(dir, { recursive: true });\n\n  // Write file\n  const formatted = formatSumFile(content);\n  await writeFile(sumPath, formatted, 'utf-8');\n\n  return sumPath;\n}\n\n/**\n * Get the .sum path for a source file.\n *\n * @param sourcePath - Path to the source file\n * @param variant - Optional eval variant name (e.g., \"claude.haiku\")\n */\nexport function getSumPath(sourcePath: string, variant?: string): string {\n  if (variant) {\n    return `${sourcePath}.${variant}.sum`;\n  }\n  return `${sourcePath}.sum`;\n}\n\n/**\n * Check if a .sum file exists for a source file.\n *\n * @param sourcePath - Path to the source file\n * @param variant - Optional eval variant name (e.g., \"claude.haiku\")\n */\nexport async function sumFileExists(sourcePath: string, variant?: string): Promise<boolean> {\n  const sumPath = getSumPath(sourcePath, variant);\n  const content = await readSumFile(sumPath);\n  return content !== null;\n}\n\n/**\n * Write an annex file alongside a source file.\n * Contains the full source content for reproduction-critical files\n * whose verbatim constants cannot fit within .sum word limits.\n *\n * Example: foo.ts -> foo.annex.sum (or foo.annex.claude.haiku.sum with variant)\n *\n * @param sourcePath - Absolute path to the source file\n * @param sourceContent - Full source file content\n * @param variant - Optional eval variant name (e.g., \"claude.haiku\")\n * @returns Path to the written annex file\n */\nexport async function writeAnnexFile(\n  sourcePath: string,\n  sourceContent: string,\n  variant?: string,\n): Promise<string> {\n  const annexPath = getAnnexPath(sourcePath, variant);\n  const fileName = path.basename(sourcePath);\n  const content = [\n    GENERATED_MARKER,\n    `# Annex: ${fileName}`,\n    '',\n    `Reproduction-critical source content from \\`${fileName}\\`.`,\n    `Referenced by \\`${fileName}.sum\\`.`,\n    '',\n    '```',\n    sourceContent,\n    '```',\n    '',\n  ].join('\\n');\n  await writeFile(annexPath, content, 'utf-8');\n  return annexPath;\n}\n\n/**\n * Get the .annex.sum path for a source file.\n *\n * Strips the source extension: foo.ts -> foo.annex.sum (or foo.annex.claude.haiku.sum with variant)\n *\n * @param sourcePath - Path to the source file\n * @param variant - Optional eval variant name (e.g., \"claude.haiku\")\n */\nexport function getAnnexPath(sourcePath: string, variant?: string): string {\n  const parsed = path.parse(sourcePath);\n  if (variant) {\n    return path.join(parsed.dir, `${parsed.name}.annex.${variant}.sum`);\n  }\n  return path.join(parsed.dir, `${parsed.name}.annex.sum`);\n}\n\n```\n\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n- Workflow & convention rules: if this file defines contribution guidelines, PR conventions, commit standards, testing mandates, tool usage requirements, approval workflows, AI agent behavioral instructions, or code conventions (naming standards, formatting rules, import ordering, linting policies), extract them as explicit, actionable rules. Sources: CONTRIBUTING.md, CI configs, PR templates, style guides, linter/formatter configs (.eslintrc, .prettierrc, .editorconfig), tsconfig strictness, README dev sections.\n\n\nTARGET LENGTH (MANDATORY):\n- Source file: 7286 characters\n- Target summary: ~729 characters (10% compression)\n- Maximum: 875 characters\n- Achieve this by: ultra-dense writing, minimal examples, single-sentence descriptions\n\nULTRA-COMPRESSION TECHNIQUES:\n- Use telegraphic style: \"exports foo, bar, baz\" not \"this file exports three functions\"\n- No code examples unless critical to understanding\n- One sentence per export maximum\n- Omit obvious parameter names (\"opts: Options\" not \"options: Options - configuration object\")\n- Use abbreviations: \"params\" not \"parameters\", \"config\" not \"configuration\"\n- Skip section if empty (don't write \"No special concerns\" or \"N/A\")\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nWORKFLOW & CONVENTION RULES (NEVER EXCLUDE):\n- Contribution requirements: branch naming, commit message formats, PR description templates\n- Testing mandates: required test types, coverage thresholds, \"never delete tests\" policies\n- Tool usage rules: required CLI commands, browser automation tools, linter/formatter requirements\n- Approval workflows: review requirements, CI checks that must pass, deployment gates\n- AI agent instructions: explicit directives for automated assistants (coding rules, prohibited actions)\n- Code conventions: naming standards (camelCase, PascalCase, snake_case for what), import ordering rules, file/folder naming patterns, formatting rules (indentation, quotes, semicolons), language idioms, TypeScript strictness policies, preferred patterns vs anti-patterns\nThese define how contributors (human or AI) must work within the project.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion .annex.sum file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**sum.ts serializes AI documentation into YAML-frontmatter .sum files with structural parsing and SHA-256 change detection.**\n\n## Exported Functions\n\n`readSumFile(sumPath: string): Promise<SumFileContent | null>` — reads and parses .sum file, returns null on missing/invalid file.\n\n`writeSumFile(sourcePath: string, content: SumFileContent, variant?: string): Promise<string>` — writes .sum alongside source file with optional variant suffix (e.g., `file.ts.claude.haiku.sum`); creates parent directories; returns written path.\n\n`getSumPath(sourcePath: string, variant?: string): string` — computes .sum filename: `foo.ts` → `foo.ts.sum` or `foo.ts.${variant}.sum`.\n\n`sumFileExists(sourcePath: string, variant?: string): Promise<boolean>` — checks .sum file existence by attempting read.\n\n`writeAnnexFile(sourcePath: string, sourceContent: string, variant?: string): Promise<string>` — writes reproduction-critical source to `foo.annex.sum` (or `foo.annex.${variant}.sum`); wraps content in markdown code block with header referencing original file.\n\n`getAnnexPath(sourcePath: string, variant?: string): string` — computes annex filename with extension stripped: `foo.ts` → `foo.annex.sum` or `foo.annex.${variant}.sum`.\n\n## Exported Types\n\n`SumFileContent` — struct with `summary: string` (main content), `metadata: SummaryMetadata`, `generatedAt: string` (timestamp), `contentHash: string` (SHA-256 for change detection).\n\n## Frontmatter Format\n\n.sum files use YAML frontmatter delimited by `---\\n...\\n---\\n`. Required fields: `generated_at`, `content_hash`, `purpose`. Optional fields: `critical_todos`, `related_files`. Arrays use inline `[a, b, c]` for ≤3 short items; multi-line `- item` format otherwise.\n\n## Parsing Patterns\n\n`parseYamlArray(frontmatter: string, key: string): string[]` — extracts YAML arrays with regex `/key:\\s*\\[([^\\]]*)\\]/` (inline) or `/key:\\s*\\n((?:\\s+-\\s+.+\\n?)+)/m` (multi-line); strips whitespace and quotes.\n\n`parseSumFile(content: string): SumFileContent | null` — extracts frontmatter via `/^---\\n([\\s\\S]*?)\\n---\\n/`; parses `generated_at`, `content_hash`, `purpose` fields; populates optional `criticalTodos`, `relatedFiles` in metadata; returns null on invalid structure.\n\n## Dependencies\n\nImports `writeFile`, `readFile`, `mkdir` from `node:fs/promises`; `path` module for file operations; `SummaryMetadata` type from `../types.js`; `GENERATED_MARKER` string from `./agents-md.js` for annex headers.",
      "model": "claude-haiku-4-5-20251001",
      "inputTokens": 3,
      "outputTokens": 738,
      "cacheReadTokens": 0,
      "cacheCreationTokens": 21256,
      "latencyMs": 16965,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/generation/writers/sum.ts",
          "sizeBytes": 7286
        }
      ]
    },
    {
      "timestamp": "2026-02-14T13:46:32.066Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: src/quality/phantom-paths/validator.ts\n\n```typescript\nimport { existsSync } from 'node:fs';\nimport * as path from 'node:path';\nimport type { PhantomPathInconsistency } from '../types.js';\n\n/**\n * Regex patterns matching path-like references in AGENTS.md content.\n *\n * Matches patterns like:\n * - `src/foo/bar.ts` (source file paths in backticks)\n * - `../foo/bar.js` (relative import paths in backticks)\n * - [text](./path) (markdown links)\n */\nconst PATH_PATTERNS: RegExp[] = [\n  // Markdown links: [text](./path) or [text](path)\n  /\\[(?:[^\\]]*)\\]\\((\\.[^)]+)\\)/g,\n  // Backtick-quoted paths: `src/foo/bar.ts` or `../foo/bar.js`\n  /`((?:src\\/|\\.\\.?\\/)[^`]+\\.[a-z]{1,4})`/g,\n  // Prose paths: \"from src/foo/\" or \"in src/foo/bar.ts\"\n  /(?:from|in|by|via|see)\\s+`?(src\\/[\\w\\-./]+)`?/gi,\n];\n\n/**\n * Paths/patterns to skip during validation (not actual file references).\n */\nconst SKIP_PATTERNS: RegExp[] = [\n  /node_modules/,\n  /\\.git\\//,\n  /^https?:/,\n  /\\{\\{/,          // template placeholders\n  /\\$\\{/,          // template literals\n  /\\*/,            // glob patterns\n  /\\{[^}]*,[^}]*\\}/,  // brace expansion: {a,b,c}\n];\n\n/**\n * Check an AGENTS.md file for phantom path references.\n *\n * Extracts all path-like strings from the document, resolves them\n * relative to the AGENTS.md file location, and verifies they exist.\n *\n * @param agentsMdPath - Absolute path to the AGENTS.md file\n * @param content - Content of the AGENTS.md file\n * @param projectRoot - Project root for resolving src/ paths\n * @returns Array of phantom path inconsistencies\n */\nexport function checkPhantomPaths(\n  agentsMdPath: string,\n  content: string,\n  projectRoot: string,\n): PhantomPathInconsistency[] {\n  const issues: PhantomPathInconsistency[] = [];\n  const agentsMdDir = path.dirname(agentsMdPath);\n  const seen = new Set<string>();\n\n  for (const pattern of PATH_PATTERNS) {\n    pattern.lastIndex = 0;\n    let match;\n\n    while ((match = pattern.exec(content)) !== null) {\n      const rawPath = match[1];\n      if (!rawPath || seen.has(rawPath)) continue;\n      if (SKIP_PATTERNS.some((p) => p.test(rawPath))) continue;\n\n      seen.add(rawPath);\n\n      // Try resolving relative to AGENTS.md location\n      const fromAgentsMd = path.resolve(agentsMdDir, rawPath);\n      // Try resolving relative to project root (for src/ paths)\n      const fromRoot = path.resolve(projectRoot, rawPath);\n\n      // Strip .js extension and try .ts (TypeScript import convention)\n      const tryPaths = [fromAgentsMd, fromRoot];\n      if (rawPath.endsWith('.js')) {\n        tryPaths.push(fromAgentsMd.replace(/\\.js$/, '.ts'));\n        tryPaths.push(fromRoot.replace(/\\.js$/, '.ts'));\n      }\n\n      const exists = tryPaths.some((p) => existsSync(p));\n\n      if (!exists) {\n        // Find the line containing this reference for context\n        const lines = content.split('\\n');\n        const contextLine = lines.find((l) => l.includes(rawPath)) ?? '';\n\n        issues.push({\n          type: 'phantom-path',\n          severity: 'warning',\n          agentsMdPath: path.relative(projectRoot, agentsMdPath),\n          description: `Phantom path reference: \"${rawPath}\" does not exist`,\n          details: {\n            referencedPath: rawPath,\n            resolvedTo: path.relative(projectRoot, fromAgentsMd),\n            context: contextLine.trim().slice(0, 120),\n          },\n        });\n      }\n    }\n  }\n\n  return issues;\n}\n\n```\n\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n- Workflow & convention rules: if this file defines contribution guidelines, PR conventions, commit standards, testing mandates, tool usage requirements, approval workflows, AI agent behavioral instructions, or code conventions (naming standards, formatting rules, import ordering, linting policies), extract them as explicit, actionable rules. Sources: CONTRIBUTING.md, CI configs, PR templates, style guides, linter/formatter configs (.eslintrc, .prettierrc, .editorconfig), tsconfig strictness, README dev sections.\n\n\nTARGET LENGTH (MANDATORY):\n- Source file: 3338 characters\n- Target summary: ~334 characters (10% compression)\n- Maximum: 401 characters\n- Achieve this by: ultra-dense writing, minimal examples, single-sentence descriptions\n\nULTRA-COMPRESSION TECHNIQUES:\n- Use telegraphic style: \"exports foo, bar, baz\" not \"this file exports three functions\"\n- No code examples unless critical to understanding\n- One sentence per export maximum\n- Omit obvious parameter names (\"opts: Options\" not \"options: Options - configuration object\")\n- Use abbreviations: \"params\" not \"parameters\", \"config\" not \"configuration\"\n- Skip section if empty (don't write \"No special concerns\" or \"N/A\")\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nWORKFLOW & CONVENTION RULES (NEVER EXCLUDE):\n- Contribution requirements: branch naming, commit message formats, PR description templates\n- Testing mandates: required test types, coverage thresholds, \"never delete tests\" policies\n- Tool usage rules: required CLI commands, browser automation tools, linter/formatter requirements\n- Approval workflows: review requirements, CI checks that must pass, deployment gates\n- AI agent instructions: explicit directives for automated assistants (coding rules, prohibited actions)\n- Code conventions: naming standards (camelCase, PascalCase, snake_case for what), import ordering rules, file/folder naming patterns, formatting rules (indentation, quotes, semicolons), language idioms, TypeScript strictness policies, preferred patterns vs anti-patterns\nThese define how contributors (human or AI) must work within the project.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion .annex.sum file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**validator.ts detects invalid path references in AGENTS.md documentation files by matching regex patterns against content and verifying file existence.**\n\n## Exports\n\n`checkPhantomPaths(agentsMdPath: string, content: string, projectRoot: string): PhantomPathInconsistency[]` — extracts path-like strings from AGENTS.md using regex patterns, resolves them relative to file location or project root, and returns array of unresolved paths with context.\n\n## Path Detection Patterns\n\n`PATH_PATTERNS` — three RegExp instances matching: markdown links `\\[(?:[^\\]]*)\\]\\((\\.[^)]+)\\)/g`, backtick-quoted paths `` /`((?:src\\/|\\.\\.?\\/)[^`]+\\.[a-z]{1,4})`/g ``, and prose references `/(?:from|in|by|via|see)\\s+`?(src\\/[\\w\\-./]+)`?/gi`.\n\n## Validation Rules\n\n`SKIP_PATTERNS` — six RegExp instances excluding node_modules, .git/, URLs starting with `https?:`, template placeholders `{{`, template literals `${`, glob wildcards `*`, and brace expansion `{a,b,c}` from validation.\n\n## Resolution Strategy\n\nPaths resolved in order: (1) relative to AGENTS.md directory, (2) relative to projectRoot for src/ references, (3) with .js→.ts substitution for TypeScript import conventions. Deduplicates via `seen` Set to avoid duplicate issue reports.\n\n## Issue Structure\n\nReturns `PhantomPathInconsistency[]` objects with type `'phantom-path'`, severity `'warning'`, relative agentsMdPath, rawPath that failed resolution, absolute resolvedTo path, and 120-char context line snippet.",
      "model": "claude-haiku-4-5-20251001",
      "inputTokens": 3,
      "outputTokens": 418,
      "cacheReadTokens": 16147,
      "cacheCreationTokens": 3853,
      "latencyMs": 15996,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/quality/phantom-paths/validator.ts",
          "sizeBytes": 3338
        }
      ]
    },
    {
      "timestamp": "2026-02-14T13:46:32.586Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: .github/workflows/pages.yml\n\n```yaml\nname: Deploy Blog to GitHub Pages\n\non:\n  push:\n    branches: [main]\n    paths: [site/**, blog/**]\n  workflow_dispatch:\n\npermissions:\n  contents: read\n  pages: write\n  id-token: write\n\nconcurrency:\n  group: pages\n  cancel-in-progress: true\n\njobs:\n  build:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Checkout\n        uses: actions/checkout@v4\n\n      - name: Setup Node.js\n        uses: actions/setup-node@v4\n        with:\n          node-version: '20'\n\n      - name: Install dependencies\n        run: npm ci\n\n      - name: Build site\n        run: node site/build.mjs\n\n      - name: Upload Pages artifact\n        uses: actions/upload-pages-artifact@v3\n        with:\n          path: site\n\n  deploy:\n    needs: build\n    runs-on: ubuntu-latest\n    environment:\n      name: github-pages\n      url: ${{ steps.deployment.outputs.page_url }}\n    steps:\n      - name: Deploy to GitHub Pages\n        id: deployment\n        uses: actions/deploy-pages@v4\n\n```\n\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n- Workflow & convention rules: if this file defines contribution guidelines, PR conventions, commit standards, testing mandates, tool usage requirements, approval workflows, AI agent behavioral instructions, or code conventions (naming standards, formatting rules, import ordering, linting policies), extract them as explicit, actionable rules. Sources: CONTRIBUTING.md, CI configs, PR templates, style guides, linter/formatter configs (.eslintrc, .prettierrc, .editorconfig), tsconfig strictness, README dev sections.\n\n\nTARGET LENGTH (MANDATORY):\n- Source file: 950 characters\n- Target summary: ~95 characters (10% compression)\n- Maximum: 114 characters\n- Achieve this by: ultra-dense writing, minimal examples, single-sentence descriptions\n\nULTRA-COMPRESSION TECHNIQUES:\n- Use telegraphic style: \"exports foo, bar, baz\" not \"this file exports three functions\"\n- No code examples unless critical to understanding\n- One sentence per export maximum\n- Omit obvious parameter names (\"opts: Options\" not \"options: Options - configuration object\")\n- Use abbreviations: \"params\" not \"parameters\", \"config\" not \"configuration\"\n- Skip section if empty (don't write \"No special concerns\" or \"N/A\")\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nWORKFLOW & CONVENTION RULES (NEVER EXCLUDE):\n- Contribution requirements: branch naming, commit message formats, PR description templates\n- Testing mandates: required test types, coverage thresholds, \"never delete tests\" policies\n- Tool usage rules: required CLI commands, browser automation tools, linter/formatter requirements\n- Approval workflows: review requirements, CI checks that must pass, deployment gates\n- AI agent instructions: explicit directives for automated assistants (coding rules, prohibited actions)\n- Code conventions: naming standards (camelCase, PascalCase, snake_case for what), import ordering rules, file/folder naming patterns, formatting rules (indentation, quotes, semicolons), language idioms, TypeScript strictness policies, preferred patterns vs anti-patterns\nThese define how contributors (human or AI) must work within the project.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion .annex.sum file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**pages.yml defines CI/CD workflow triggering on main branch pushes to site/ or blog/ directories, building with Node 20 and site/build.mjs, then deploying to GitHub Pages.**\n\n## Trigger Configuration\n- **Push events**: branches=[main], paths=[site/**, blog/**]\n- **Manual trigger**: workflow_dispatch enabled\n- Concurrent workflow cancellation: group=pages, cancel-in-progress=true\n\n## Permissions\n- contents: read\n- pages: write\n- id-token: write\n\n## Build Job (ubuntu-latest)\nExecutes sequentially: checkout@v4 → setup-node@v4 (node-version='20') → npm ci → node site/build.mjs → upload-pages-artifact@v3 (path: site)\n\n## Deploy Job\nDepends on: build. Runs on ubuntu-latest in github-pages environment. Executes actions/deploy-pages@v4, outputs page_url to steps.deployment.outputs.page_url.\n\n## Build Artifact\n- Source: site/ directory\n- Upload action: actions/upload-pages-artifact@v3\n- Triggers: post-build before deploy",
      "model": "claude-haiku-4-5-20251001",
      "inputTokens": 3,
      "outputTokens": 270,
      "cacheReadTokens": 16147,
      "cacheCreationTokens": 3014,
      "latencyMs": 15497,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": ".github/workflows/pages.yml",
          "sizeBytes": 950
        }
      ]
    },
    {
      "timestamp": "2026-02-14T13:46:34.564Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: .github/workflows/publish.yml\n\n```yaml\nname: Publish to npm\n\non:\n  release:\n    types: [published]\n  workflow_dispatch:\n\njobs:\n  publish:\n    runs-on: ubuntu-latest\n    permissions:\n      contents: read\n      id-token: write\n    steps:\n      - name: Checkout\n        uses: actions/checkout@v4\n\n      - name: Setup Node.js\n        uses: actions/setup-node@v4\n        with:\n          node-version: '20'\n          registry-url: 'https://registry.npmjs.org'\n\n      - name: Install dependencies\n        run: npm ci\n\n      - name: Build\n        run: npm run build\n\n      - name: Publish to npm\n        run: npm publish --provenance --access public\n        env:\n          NODE_AUTH_TOKEN: ${{ secrets.NPM_TOKEN }}\n\n```\n\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n- Workflow & convention rules: if this file defines contribution guidelines, PR conventions, commit standards, testing mandates, tool usage requirements, approval workflows, AI agent behavioral instructions, or code conventions (naming standards, formatting rules, import ordering, linting policies), extract them as explicit, actionable rules. Sources: CONTRIBUTING.md, CI configs, PR templates, style guides, linter/formatter configs (.eslintrc, .prettierrc, .editorconfig), tsconfig strictness, README dev sections.\n\n\nTARGET LENGTH (MANDATORY):\n- Source file: 668 characters\n- Target summary: ~67 characters (10% compression)\n- Maximum: 80 characters\n- Achieve this by: ultra-dense writing, minimal examples, single-sentence descriptions\n\nULTRA-COMPRESSION TECHNIQUES:\n- Use telegraphic style: \"exports foo, bar, baz\" not \"this file exports three functions\"\n- No code examples unless critical to understanding\n- One sentence per export maximum\n- Omit obvious parameter names (\"opts: Options\" not \"options: Options - configuration object\")\n- Use abbreviations: \"params\" not \"parameters\", \"config\" not \"configuration\"\n- Skip section if empty (don't write \"No special concerns\" or \"N/A\")\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nWORKFLOW & CONVENTION RULES (NEVER EXCLUDE):\n- Contribution requirements: branch naming, commit message formats, PR description templates\n- Testing mandates: required test types, coverage thresholds, \"never delete tests\" policies\n- Tool usage rules: required CLI commands, browser automation tools, linter/formatter requirements\n- Approval workflows: review requirements, CI checks that must pass, deployment gates\n- AI agent instructions: explicit directives for automated assistants (coding rules, prohibited actions)\n- Code conventions: naming standards (camelCase, PascalCase, snake_case for what), import ordering rules, file/folder naming patterns, formatting rules (indentation, quotes, semicolons), language idioms, TypeScript strictness policies, preferred patterns vs anti-patterns\nThese define how contributors (human or AI) must work within the project.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion .annex.sum file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**publish.yml automates npm package publishing on GitHub release events using Node.js 20 with provenance attestation.**\n\n## Trigger Events\n\nPublishes on `release.types: [published]` (manual release creation) or `workflow_dispatch` (manual trigger).\n\n## Execution Environment\n\nRuns on `ubuntu-latest` with Node.js 20 configured for `https://registry.npmjs.org` registry. Permissions: `contents: read`, `id-token: write` (required for npm provenance).\n\n## Build & Publish Pipeline\n\n1. `actions/checkout@v4` — clone repository\n2. `actions/setup-node@v4` — configure Node.js 20 and npm registry\n3. `npm ci` — install dependencies from lockfile\n4. `npm run build` — execute build script\n5. `npm publish --provenance --access public` — publish with provenance attestation and public access, consuming `NODE_AUTH_TOKEN` secret\n\n## Secrets & Authentication\n\n`NODE_AUTH_TOKEN` (required) — npm authentication token from repository secrets; enables publishing without interactive auth.",
      "model": "claude-haiku-4-5-20251001",
      "inputTokens": 3,
      "outputTokens": 254,
      "cacheReadTokens": 16147,
      "cacheCreationTokens": 2928,
      "latencyMs": 15903,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": ".github/workflows/publish.yml",
          "sizeBytes": 668
        }
      ]
    },
    {
      "timestamp": "2026-02-14T13:46:36.505Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: src/ai/index.ts\n\n```typescript\n/**\n * Public API for the AI service layer.\n *\n * This barrel export is the ONLY import point for the AI service layer.\n * No other module should reach into `src/ai/backends/` or `src/ai/telemetry/`\n * directly.\n *\n * @module\n *\n * @example\n * ```typescript\n * import {\n *   AIService,\n *   createBackendRegistry,\n *   resolveBackend,\n * } from './ai/index.js';\n *\n * const registry = createBackendRegistry();\n * const backend = await resolveBackend(registry, 'auto');\n * const service = new AIService(backend, {\n *   timeoutMs: 120_000,\n *   maxRetries: 3,\n *   telemetry: { keepRuns: 10 },\n * });\n *\n * const response = await service.call({ prompt: 'Hello' });\n * ```\n */\n\n// ---------------------------------------------------------------------------\n// Types and errors\n// ---------------------------------------------------------------------------\n\nexport type {\n  AIProvider,\n  AIBackend,\n  AIResponse,\n  AICallOptions,\n  SubprocessResult,\n  RetryOptions,\n  TelemetryEntry,\n  RunLog,\n  FileRead,\n} from './types.js';\n\nexport { AIServiceError } from './types.js';\n\n// ---------------------------------------------------------------------------\n// Service orchestrator\n// ---------------------------------------------------------------------------\n\nexport { AIService } from './service.js';\nexport type { AIServiceOptions } from './service.js';\n\n// ---------------------------------------------------------------------------\n// Backend registry\n// ---------------------------------------------------------------------------\n\nexport {\n  BackendRegistry,\n  createBackendRegistry,\n  resolveBackend,\n  detectBackend,\n  getInstallInstructions,\n} from './registry.js';\n\n// ---------------------------------------------------------------------------\n// Retry utility\n// ---------------------------------------------------------------------------\n\nexport { withRetry, DEFAULT_RETRY_OPTIONS } from './retry.js';\n\n// ---------------------------------------------------------------------------\n// Subprocess wrapper\n// ---------------------------------------------------------------------------\n\nexport { runSubprocess } from './subprocess.js';\n\n// ---------------------------------------------------------------------------\n// Providers\n// ---------------------------------------------------------------------------\n\nexport { SubprocessProvider } from './providers/subprocess.js';\nexport type { SubprocessProviderOptions } from './providers/subprocess.js';\n\n// ---------------------------------------------------------------------------\n// Backend utilities\n// ---------------------------------------------------------------------------\n\nexport { isCommandOnPath } from './backends/claude.js';\n\n\n```\n\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n- Workflow & convention rules: if this file defines contribution guidelines, PR conventions, commit standards, testing mandates, tool usage requirements, approval workflows, AI agent behavioral instructions, or code conventions (naming standards, formatting rules, import ordering, linting policies), extract them as explicit, actionable rules. Sources: CONTRIBUTING.md, CI configs, PR templates, style guides, linter/formatter configs (.eslintrc, .prettierrc, .editorconfig), tsconfig strictness, README dev sections.\n\n\nTARGET LENGTH (MANDATORY):\n- Source file: 2689 characters\n- Target summary: ~269 characters (10% compression)\n- Maximum: 323 characters\n- Achieve this by: ultra-dense writing, minimal examples, single-sentence descriptions\n\nULTRA-COMPRESSION TECHNIQUES:\n- Use telegraphic style: \"exports foo, bar, baz\" not \"this file exports three functions\"\n- No code examples unless critical to understanding\n- One sentence per export maximum\n- Omit obvious parameter names (\"opts: Options\" not \"options: Options - configuration object\")\n- Use abbreviations: \"params\" not \"parameters\", \"config\" not \"configuration\"\n- Skip section if empty (don't write \"No special concerns\" or \"N/A\")\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nWORKFLOW & CONVENTION RULES (NEVER EXCLUDE):\n- Contribution requirements: branch naming, commit message formats, PR description templates\n- Testing mandates: required test types, coverage thresholds, \"never delete tests\" policies\n- Tool usage rules: required CLI commands, browser automation tools, linter/formatter requirements\n- Approval workflows: review requirements, CI checks that must pass, deployment gates\n- AI agent instructions: explicit directives for automated assistants (coding rules, prohibited actions)\n- Code conventions: naming standards (camelCase, PascalCase, snake_case for what), import ordering rules, file/folder naming patterns, formatting rules (indentation, quotes, semicolons), language idioms, TypeScript strictness policies, preferred patterns vs anti-patterns\nThese define how contributors (human or AI) must work within the project.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion .annex.sum file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**src/ai/index.ts is a barrel export defining the public API for the AI service layer, enforcing strict module boundaries.**\n\n## Exported Symbols\n\n**Types:** `AIProvider`, `AIBackend`, `AIResponse`, `AICallOptions`, `SubprocessResult`, `RetryOptions`, `TelemetryEntry`, `RunLog`, `FileRead`\n\n**Classes:** `AIService` (service orchestrator), `BackendRegistry` (backend registry), `SubprocessProvider` (subprocess implementation)\n\n**Functions:** `createBackendRegistry()`, `resolveBackend()`, `detectBackend()`, `getInstallInstructions()`, `withRetry()`, `runSubprocess()`, `isCommandOnPath()`\n\n**Error:** `AIServiceError`\n\n**Constants:** `DEFAULT_RETRY_OPTIONS`\n\n**Type:** `AIServiceOptions`, `SubprocessProviderOptions`\n\n## Module Boundaries\n\nEnforces encapsulation: `src/ai/backends/` and `src/ai/telemetry/` are internal; all consumption must route through this barrel. Re-exports from `types.js`, `service.js`, `registry.js`, `retry.js`, `subprocess.js`, `providers/subprocess.js`, `backends/claude.js`.\n\n## Core Architecture Pattern\n\nComposition: `BackendRegistry` → `resolveBackend()` → `AIService` initialization with `AIServiceOptions` (timeout, retries, telemetry config) → `service.call()` for requests. `SubprocessProvider` bridges subprocess communication; `withRetry()` wraps transient failures.",
      "model": "claude-haiku-4-5-20251001",
      "inputTokens": 3,
      "outputTokens": 366,
      "cacheReadTokens": 16147,
      "cacheCreationTokens": 3296,
      "latencyMs": 15926,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/ai/index.ts",
          "sizeBytes": 2689
        }
      ]
    },
    {
      "timestamp": "2026-02-14T13:46:36.959Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: src/ai/registry.ts\n\n```typescript\n/**\n * Backend registry, factory, and auto-detection.\n *\n * Manages the set of registered AI CLI backends, selects the appropriate\n * backend at runtime via auto-detection or explicit request, and provides\n * actionable error messages when no CLI is found.\n *\n * @module\n */\n\nimport type { AIBackend } from './types.js';\nimport { AIServiceError } from './types.js';\nimport { ClaudeBackend } from './backends/claude.js';\nimport { CodexBackend } from './backends/codex.js';\nimport { GeminiBackend } from './backends/gemini.js';\nimport { OpenCodeBackend } from './backends/opencode.js';\n\n// ---------------------------------------------------------------------------\n// Backend registry\n// ---------------------------------------------------------------------------\n\n/**\n * Registry of available AI CLI backends.\n *\n * Stores backends in insertion order, which determines the priority for\n * auto-detection. Use {@link createBackendRegistry} to get a pre-populated\n * registry with all supported backends.\n *\n * @example\n * ```typescript\n * const registry = createBackendRegistry();\n * const claude = registry.get('claude');\n * const all = registry.getAll(); // [ClaudeBackend, GeminiBackend, OpenCodeBackend]\n * ```\n */\nexport class BackendRegistry {\n  private readonly backends = new Map<string, AIBackend>();\n\n  /**\n   * Register a backend adapter.\n   *\n   * @param backend - The backend to register (keyed by its `name` property)\n   */\n  register(backend: AIBackend): void {\n    this.backends.set(backend.name, backend);\n  }\n\n  /**\n   * Get a specific backend by name.\n   *\n   * @param name - The backend name (e.g., \"claude\", \"gemini\", \"opencode\")\n   * @returns The backend, or `undefined` if not registered\n   */\n  get(name: string): AIBackend | undefined {\n    return this.backends.get(name);\n  }\n\n  /**\n   * Get all registered backends in priority order.\n   *\n   * @returns Array of all registered backends\n   */\n  getAll(): AIBackend[] {\n    return Array.from(this.backends.values());\n  }\n}\n\n// ---------------------------------------------------------------------------\n// Factory\n// ---------------------------------------------------------------------------\n\n/**\n * Create a new backend registry pre-populated with all supported backends.\n *\n * Registration order determines auto-detection priority:\n * 1. Claude (recommended, fully implemented)\n * 2. Codex (production)\n * 3. Gemini (experimental, stub)\n * 4. OpenCode (production)\n *\n * @returns A populated {@link BackendRegistry}\n *\n * @example\n * ```typescript\n * const registry = createBackendRegistry();\n * const backend = await detectBackend(registry);\n * ```\n */\nexport function createBackendRegistry(): BackendRegistry {\n  const registry = new BackendRegistry();\n  registry.register(new ClaudeBackend());\n  registry.register(new CodexBackend());\n  registry.register(new GeminiBackend());\n  registry.register(new OpenCodeBackend());\n  return registry;\n}\n\n// ---------------------------------------------------------------------------\n// Auto-detection\n// ---------------------------------------------------------------------------\n\n/**\n * Detect the first available backend on PATH in priority order.\n *\n * Iterates all registered backends and calls `isAvailable()` on each.\n * Returns the first backend whose CLI is found, or `null` if none are\n * available.\n *\n * Priority order is determined by registration order in\n * {@link createBackendRegistry}: Claude > Codex > Gemini > OpenCode.\n *\n * @param registry - The backend registry to search\n * @returns The first available backend, or `null` if none found\n *\n * @example\n * ```typescript\n * const registry = createBackendRegistry();\n * const backend = await detectBackend(registry);\n * if (backend) {\n *   console.log(`Using ${backend.name} backend`);\n * }\n * ```\n */\nexport async function detectBackend(registry: BackendRegistry): Promise<AIBackend | null> {\n  for (const backend of registry.getAll()) {\n    if (await backend.isAvailable()) {\n      return backend;\n    }\n  }\n  return null;\n}\n\n// ---------------------------------------------------------------------------\n// Install instructions\n// ---------------------------------------------------------------------------\n\n/**\n * Get formatted install instructions for all registered backends.\n *\n * Returns a multi-line string suitable for error messages when no CLI\n * is found. Matches the error message template from RESEARCH.md.\n *\n * @param registry - The backend registry\n * @returns Formatted install instructions string\n *\n * @example\n * ```typescript\n * const instructions = getInstallInstructions(registry);\n * console.error(`No AI CLI found.\\n\\nInstall one of the following:\\n\\n${instructions}`);\n * ```\n */\nexport function getInstallInstructions(registry: BackendRegistry): string {\n  return registry\n    .getAll()\n    .map((backend) => backend.getInstallInstructions())\n    .join('\\n\\n');\n}\n\n// ---------------------------------------------------------------------------\n// Resolution\n// ---------------------------------------------------------------------------\n\n/**\n * Resolve a backend by name or auto-detect the best available one.\n *\n * - If `requested` is `'auto'`: runs {@link detectBackend} and throws with\n *   install instructions if nothing is found.\n * - If `requested` is a specific name: looks it up in the registry, checks\n *   availability, and throws if not found or not available.\n *\n * @param registry - The backend registry\n * @param requested - Backend name or `'auto'` for auto-detection\n * @returns The resolved backend adapter\n * @throws {AIServiceError} With code `CLI_NOT_FOUND` if no backend is available\n *\n * @example\n * ```typescript\n * const registry = createBackendRegistry();\n *\n * // Auto-detect\n * const backend = await resolveBackend(registry, 'auto');\n *\n * // Explicit selection\n * const claude = await resolveBackend(registry, 'claude');\n * ```\n */\nexport async function resolveBackend(\n  registry: BackendRegistry,\n  requested: string | 'auto',\n): Promise<AIBackend> {\n  if (requested === 'auto') {\n    // PATH scan in registration order (Claude > Codex > Gemini > OpenCode)\n    const detected = await detectBackend(registry);\n    if (detected) {\n      return detected;\n    }\n\n    const instructions = getInstallInstructions(registry);\n    throw new AIServiceError(\n      'CLI_NOT_FOUND',\n      `No AI CLI found on your system.\\n\\nInstall one of the following:\\n\\n${instructions}\\n\\nThen run this command again.`,\n    );\n  }\n\n  // Explicit backend requested\n  const backend = registry.get(requested);\n  if (!backend) {\n    const known = registry\n      .getAll()\n      .map((b) => b.name)\n      .join(', ');\n    throw new AIServiceError(\n      'CLI_NOT_FOUND',\n      `Unknown backend \"${requested}\". Available backends: ${known}`,\n    );\n  }\n\n  if (!(await backend.isAvailable())) {\n    throw new AIServiceError(\n      'CLI_NOT_FOUND',\n      `Backend \"${requested}\" is not available. The \"${backend.cliCommand}\" CLI was not found on PATH.\\n\\n${backend.getInstallInstructions()}`,\n    );\n  }\n\n  return backend;\n}\n\n```\n\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n- Workflow & convention rules: if this file defines contribution guidelines, PR conventions, commit standards, testing mandates, tool usage requirements, approval workflows, AI agent behavioral instructions, or code conventions (naming standards, formatting rules, import ordering, linting policies), extract them as explicit, actionable rules. Sources: CONTRIBUTING.md, CI configs, PR templates, style guides, linter/formatter configs (.eslintrc, .prettierrc, .editorconfig), tsconfig strictness, README dev sections.\n\n\nTARGET LENGTH (MANDATORY):\n- Source file: 7042 characters\n- Target summary: ~704 characters (10% compression)\n- Maximum: 845 characters\n- Achieve this by: ultra-dense writing, minimal examples, single-sentence descriptions\n\nULTRA-COMPRESSION TECHNIQUES:\n- Use telegraphic style: \"exports foo, bar, baz\" not \"this file exports three functions\"\n- No code examples unless critical to understanding\n- One sentence per export maximum\n- Omit obvious parameter names (\"opts: Options\" not \"options: Options - configuration object\")\n- Use abbreviations: \"params\" not \"parameters\", \"config\" not \"configuration\"\n- Skip section if empty (don't write \"No special concerns\" or \"N/A\")\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nWORKFLOW & CONVENTION RULES (NEVER EXCLUDE):\n- Contribution requirements: branch naming, commit message formats, PR description templates\n- Testing mandates: required test types, coverage thresholds, \"never delete tests\" policies\n- Tool usage rules: required CLI commands, browser automation tools, linter/formatter requirements\n- Approval workflows: review requirements, CI checks that must pass, deployment gates\n- AI agent instructions: explicit directives for automated assistants (coding rules, prohibited actions)\n- Code conventions: naming standards (camelCase, PascalCase, snake_case for what), import ordering rules, file/folder naming patterns, formatting rules (indentation, quotes, semicolons), language idioms, TypeScript strictness policies, preferred patterns vs anti-patterns\nThese define how contributors (human or AI) must work within the project.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion .annex.sum file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**registry.ts manages backend registration, runtime selection, and auto-detection for AI CLI providers (Claude, Codex, Gemini, OpenCode) with error handling and install instructions.**\n\n## Exports\n\n`BackendRegistry` class: stores `AIBackend` instances in insertion-order Map with `register(backend)`, `get(name)`, `getAll()` methods.\n\n`createBackendRegistry()` → `BackendRegistry`: factory populating registry in priority order: Claude > Codex > Gemini > OpenCode.\n\n`detectBackend(registry: BackendRegistry)` → `Promise<AIBackend | null>`: iterates registered backends, returns first where `isAvailable()` succeeds.\n\n`getInstallInstructions(registry)` → `string`: concatenates `backend.getInstallInstructions()` for all backends with `\\n\\n` separator.\n\n`resolveBackend(registry, requested: string | 'auto')` → `Promise<AIBackend>`: selects backend by name or auto-detection; throws `AIServiceError` with code `'CLI_NOT_FOUND'` if unavailable.\n\n## Key Behaviors\n\n`resolveBackend` with `requested='auto'` scans PATH in registration priority order; throws with formatted install instructions if all backends fail availability check. Explicit name selection validates registry membership and CLI presence on PATH. Error messages include available backend names or install command help.",
      "model": "claude-haiku-4-5-20251001",
      "inputTokens": 3,
      "outputTokens": 326,
      "cacheReadTokens": 16147,
      "cacheCreationTokens": 4615,
      "latencyMs": 16705,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/ai/registry.ts",
          "sizeBytes": 7042
        }
      ]
    },
    {
      "timestamp": "2026-02-14T13:46:41.918Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: src/change-detection/index.ts\n\n```typescript\n/**\n * Change detection module\n *\n * Provides git-based change detection for incremental updates.\n * Detects added, modified, deleted, and renamed files.\n */\nexport {\n  isGitRepo,\n  getCurrentCommit,\n  getChangedFiles,\n  computeContentHash,\n  computeContentHashFromString,\n} from './detector.js';\n\nexport type {\n  ChangeType,\n  FileChange,\n  ChangeDetectionResult,\n  ChangeDetectionOptions,\n} from './types.js';\n\n```\n\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n- Workflow & convention rules: if this file defines contribution guidelines, PR conventions, commit standards, testing mandates, tool usage requirements, approval workflows, AI agent behavioral instructions, or code conventions (naming standards, formatting rules, import ordering, linting policies), extract them as explicit, actionable rules. Sources: CONTRIBUTING.md, CI configs, PR templates, style guides, linter/formatter configs (.eslintrc, .prettierrc, .editorconfig), tsconfig strictness, README dev sections.\n\n\nTARGET LENGTH (MANDATORY):\n- Source file: 412 characters\n- Target summary: ~41 characters (10% compression)\n- Maximum: 49 characters\n- Achieve this by: ultra-dense writing, minimal examples, single-sentence descriptions\n\nULTRA-COMPRESSION TECHNIQUES:\n- Use telegraphic style: \"exports foo, bar, baz\" not \"this file exports three functions\"\n- No code examples unless critical to understanding\n- One sentence per export maximum\n- Omit obvious parameter names (\"opts: Options\" not \"options: Options - configuration object\")\n- Use abbreviations: \"params\" not \"parameters\", \"config\" not \"configuration\"\n- Skip section if empty (don't write \"No special concerns\" or \"N/A\")\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nWORKFLOW & CONVENTION RULES (NEVER EXCLUDE):\n- Contribution requirements: branch naming, commit message formats, PR description templates\n- Testing mandates: required test types, coverage thresholds, \"never delete tests\" policies\n- Tool usage rules: required CLI commands, browser automation tools, linter/formatter requirements\n- Approval workflows: review requirements, CI checks that must pass, deployment gates\n- AI agent instructions: explicit directives for automated assistants (coding rules, prohibited actions)\n- Code conventions: naming standards (camelCase, PascalCase, snake_case for what), import ordering rules, file/folder naming patterns, formatting rules (indentation, quotes, semicolons), language idioms, TypeScript strictness policies, preferred patterns vs anti-patterns\nThese define how contributors (human or AI) must work within the project.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion .annex.sum file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**src/change-detection/index.ts is a barrel export module aggregating git-based change detection utilities and types.**\n\n## Exported Functions\n- `isGitRepo()` — detects if directory is git repository\n- `getCurrentCommit()` — retrieves current HEAD commit\n- `getChangedFiles()` — returns added, modified, deleted, renamed files\n- `computeContentHash()` — hashes file content\n- `computeContentHashFromString()` — hashes string content\n\n## Exported Types\n- `ChangeType` — enum/union for file change classification\n- `FileChange` — object structure for individual file changes\n- `ChangeDetectionResult` — aggregated change detection output\n- `ChangeDetectionOptions` — configuration params for change detection\n\n## Architecture\nRe-exports from `./detector.js` (functions) and `./types.js` (types); serves as public API surface for incremental update workflows.",
      "model": "claude-haiku-4-5-20251001",
      "inputTokens": 3,
      "outputTokens": 215,
      "cacheReadTokens": 16147,
      "cacheCreationTokens": 2861,
      "latencyMs": 17651,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/change-detection/index.ts",
          "sizeBytes": 412
        }
      ]
    },
    {
      "timestamp": "2026-02-14T13:46:38.611Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: src/ai/retry.ts\n\n```typescript\n/**\n * Retry utility with exponential backoff for transient AI service failures.\n *\n * Wraps any async operation with configurable retry logic. Uses exponential\n * delays with jitter to prevent thundering herd when multiple callers hit\n * the same rate limit.\n *\n * @module\n */\n\nimport type { RetryOptions } from './types.js';\n\n/**\n * Default retry configuration values.\n *\n * Does NOT include `isRetryable` or `onRetry` since those are\n * caller-specific. Spread these defaults and provide your own predicates:\n *\n * @example\n * ```typescript\n * import { withRetry, DEFAULT_RETRY_OPTIONS } from './retry.js';\n * import { AIServiceError } from './types.js';\n *\n * const result = await withRetry(() => callAI(prompt), {\n *   ...DEFAULT_RETRY_OPTIONS,\n *   isRetryable: (err) => err instanceof AIServiceError && err.code === 'RATE_LIMIT',\n *   onRetry: (attempt, err) => console.warn(`Retry ${attempt}:`, err),\n * });\n * ```\n */\nexport const DEFAULT_RETRY_OPTIONS = {\n  /** Maximum number of retries (3 retries = 4 total attempts) */\n  maxRetries: 3,\n  /** Base delay before first retry: 1 second */\n  baseDelayMs: 1_000,\n  /** Maximum delay cap: 8 seconds */\n  maxDelayMs: 8_000,\n  /** Exponential multiplier: delay doubles each attempt */\n  multiplier: 2,\n} as const satisfies Omit<RetryOptions, 'isRetryable' | 'onRetry'>;\n\n/**\n * Execute an async function with exponential backoff retry on failure.\n *\n * - On success: returns the result immediately.\n * - On transient failure (isRetryable returns true): waits with exponential\n *   backoff + jitter, then retries up to `maxRetries` times.\n * - On permanent failure (isRetryable returns false): throws immediately\n *   without retrying.\n * - After exhausting all retries: throws the last error.\n *\n * Delay formula: `min(baseDelayMs * multiplier^attempt, maxDelayMs) + jitter`\n * where jitter is a random value in [0, 500ms].\n *\n * @typeParam T - The return type of the wrapped function\n * @param fn - Async function to execute (and potentially retry)\n * @param options - Retry configuration including backoff timing and predicates\n * @returns The result of a successful `fn()` invocation\n * @throws The last error if all retry attempts are exhausted or if the error is not retryable\n *\n * @example\n * ```typescript\n * import { withRetry } from './retry.js';\n *\n * // Retry up to 3 times on rate limit errors\n * const response = await withRetry(\n *   () => runAICall(prompt),\n *   {\n *     maxRetries: 3,\n *     baseDelayMs: 1000,\n *     maxDelayMs: 8000,\n *     multiplier: 2,\n *     isRetryable: (err) => isRateLimitError(err),\n *     onRetry: (attempt, err) => logger.warn(`Attempt ${attempt} failed, retrying...`),\n *   },\n * );\n * ```\n */\nexport async function withRetry<T>(\n  fn: () => Promise<T>,\n  options: RetryOptions,\n): Promise<T> {\n  for (let attempt = 0; attempt <= options.maxRetries; attempt++) {\n    try {\n      return await fn();\n    } catch (error) {\n      // If we've exhausted retries or the error is permanent, throw immediately.\n      if (attempt === options.maxRetries || !options.isRetryable(error)) {\n        throw error;\n      }\n\n      // Compute exponential delay with cap.\n      const exponentialDelay = options.baseDelayMs * Math.pow(options.multiplier, attempt);\n      const cappedDelay = Math.min(exponentialDelay, options.maxDelayMs);\n\n      // Add jitter (0-500ms) to prevent thundering herd.\n      const jitter = Math.random() * 500;\n      const delay = cappedDelay + jitter;\n\n      // Notify caller before waiting.\n      options.onRetry?.(attempt + 1, error);\n\n      await new Promise((resolve) => setTimeout(resolve, delay));\n    }\n  }\n\n  // This is unreachable -- the loop either returns or throws.\n  // TypeScript needs it for exhaustiveness.\n  throw new Error('withRetry: unreachable');\n}\n\n```\n\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n- Workflow & convention rules: if this file defines contribution guidelines, PR conventions, commit standards, testing mandates, tool usage requirements, approval workflows, AI agent behavioral instructions, or code conventions (naming standards, formatting rules, import ordering, linting policies), extract them as explicit, actionable rules. Sources: CONTRIBUTING.md, CI configs, PR templates, style guides, linter/formatter configs (.eslintrc, .prettierrc, .editorconfig), tsconfig strictness, README dev sections.\n\n\nTARGET LENGTH (MANDATORY):\n- Source file: 3781 characters\n- Target summary: ~378 characters (10% compression)\n- Maximum: 454 characters\n- Achieve this by: ultra-dense writing, minimal examples, single-sentence descriptions\n\nULTRA-COMPRESSION TECHNIQUES:\n- Use telegraphic style: \"exports foo, bar, baz\" not \"this file exports three functions\"\n- No code examples unless critical to understanding\n- One sentence per export maximum\n- Omit obvious parameter names (\"opts: Options\" not \"options: Options - configuration object\")\n- Use abbreviations: \"params\" not \"parameters\", \"config\" not \"configuration\"\n- Skip section if empty (don't write \"No special concerns\" or \"N/A\")\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nWORKFLOW & CONVENTION RULES (NEVER EXCLUDE):\n- Contribution requirements: branch naming, commit message formats, PR description templates\n- Testing mandates: required test types, coverage thresholds, \"never delete tests\" policies\n- Tool usage rules: required CLI commands, browser automation tools, linter/formatter requirements\n- Approval workflows: review requirements, CI checks that must pass, deployment gates\n- AI agent instructions: explicit directives for automated assistants (coding rules, prohibited actions)\n- Code conventions: naming standards (camelCase, PascalCase, snake_case for what), import ordering rules, file/folder naming patterns, formatting rules (indentation, quotes, semicolons), language idioms, TypeScript strictness policies, preferred patterns vs anti-patterns\nThese define how contributors (human or AI) must work within the project.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion .annex.sum file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**retry.ts exports withRetry<T>() and DEFAULT_RETRY_OPTIONS for exponential backoff retry logic on transient async failures.**\n\n## Exports\n\n`withRetry<T>(fn: () => Promise<T>, options: RetryOptions): Promise<T>` — executes async function with exponential backoff retry; returns on success, throws after exhausting maxRetries or on permanent errors (isRetryable returns false).\n\n`DEFAULT_RETRY_OPTIONS` — const object: {maxRetries: 3, baseDelayMs: 1_000, maxDelayMs: 8_000, multiplier: 2}; spread and augment with caller-specific isRetryable predicate and onRetry callback.\n\n## Key Behaviors\n\n**Delay formula:** `min(baseDelayMs × multiplier^attempt, maxDelayMs) + jitter[0-500ms]` — prevents thundering herd on rate limits.\n\n**Retry predicate:** error classification via options.isRetryable(error); permanent errors throw immediately without retry.\n\n**Callback hook:** options.onRetry?(attemptNumber, error) called before each wait.\n\n## Integration Notes\n\nDepends on `RetryOptions` type from './types.js' (defines isRetryable signature and onRetry callback shape). Loop exhaustiveness requires unreachable throw statement for TypeScript.",
      "model": "claude-haiku-4-5-20251001",
      "inputTokens": 3,
      "outputTokens": 314,
      "cacheReadTokens": 16147,
      "cacheCreationTokens": 3938,
      "latencyMs": 22782,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/ai/retry.ts",
          "sizeBytes": 3781
        }
      ]
    },
    {
      "timestamp": "2026-02-14T13:46:40.174Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: src/ai/types.ts\n\n```typescript\n/**\n * Shared types for the AI service layer.\n *\n * Defines the contract for backends, responses, subprocess results,\n * retry configuration, and telemetry logging. Every AI service module\n * imports from this file.\n */\n\n// ---------------------------------------------------------------------------\n// Subprocess\n// ---------------------------------------------------------------------------\n\n/**\n * Result returned by the subprocess wrapper after a CLI process completes.\n *\n * Always populated -- even on error or timeout, the fields are filled\n * with whatever information was available.\n */\nexport interface SubprocessResult {\n  /** Standard output captured from the child process */\n  stdout: string;\n  /** Standard error captured from the child process */\n  stderr: string;\n  /** Numeric exit code (0 = success, non-zero = failure) */\n  exitCode: number;\n  /** Signal that terminated the process, or `null` if it exited normally */\n  signal: string | null;\n  /** Wall-clock duration in milliseconds */\n  durationMs: number;\n  /** Whether the process was killed because it exceeded its timeout */\n  timedOut: boolean;\n  /** OS PID of the child process (undefined if spawn failed) */\n  childPid?: number;\n}\n\n// ---------------------------------------------------------------------------\n// AI Call\n// ---------------------------------------------------------------------------\n\n/**\n * Input options for an AI call.\n *\n * Only `prompt` is required; all other fields have backend-specific defaults.\n */\nexport interface AICallOptions {\n  /** The prompt to send to the AI model */\n  prompt: string;\n  /** Optional system prompt to set context/behavior */\n  systemPrompt?: string;\n  /** Model identifier (e.g., \"sonnet\", \"opus\") -- backend interprets this */\n  model?: string;\n  /** Subprocess timeout in milliseconds (overrides config default) */\n  timeoutMs?: number;\n  /** Maximum number of agentic turns (backend-specific) */\n  maxTurns?: number;\n  /** Label for tracing (e.g., file path being processed) */\n  taskLabel?: string;\n}\n\n/**\n * Normalized response from any AI CLI backend.\n *\n * Every backend adapter must parse its CLI's raw output into this shape\n * so that callers never need to know which backend was used.\n */\nexport interface AIResponse {\n  /** The AI model's text response */\n  text: string;\n  /** Model identifier as reported by the backend */\n  model: string;\n  /** Number of input tokens consumed */\n  inputTokens: number;\n  /** Number of output tokens generated */\n  outputTokens: number;\n  /** Number of tokens served from cache reads */\n  cacheReadTokens: number;\n  /** Number of tokens written to cache */\n  cacheCreationTokens: number;\n  /** Wall-clock duration in milliseconds */\n  durationMs: number;\n  /** Process exit code from the CLI */\n  exitCode: number;\n  /** Original CLI JSON output for debugging */\n  raw: unknown;\n}\n\n// ---------------------------------------------------------------------------\n// AIProvider (injectable abstraction)\n// ---------------------------------------------------------------------------\n\n/**\n * Injectable interface for making AI calls.\n *\n * Implement this to swap the underlying AI transport (subprocess, HTTP API,\n * in-memory mock, etc.) without changing the rest of the pipeline.\n *\n * {@link AIService} wraps any `AIProvider` with retry logic, telemetry,\n * and tracing.\n *\n * @example\n * ```typescript\n * // Custom provider using Anthropic SDK directly\n * class AnthropicAPIProvider implements AIProvider {\n *   async call(options: AICallOptions): Promise<AIResponse> {\n *     const response = await this.client.messages.create({ ... });\n *     return { text: response.content[0].text, ... };\n *   }\n * }\n * ```\n */\nexport interface AIProvider {\n  /** Make an AI call and return the normalized response. */\n  call(options: AICallOptions): Promise<AIResponse>;\n}\n\n// ---------------------------------------------------------------------------\n// Backend\n// ---------------------------------------------------------------------------\n\n/**\n * Contract for an AI CLI backend adapter.\n *\n * Each supported CLI (Claude, Gemini, OpenCode) implements this interface.\n * The registry selects the appropriate backend at runtime.\n *\n * @example\n * ```typescript\n * const backend: AIBackend = new ClaudeBackend();\n * if (await backend.isAvailable()) {\n *   const args = backend.buildArgs({ prompt: 'Hello' });\n *   // spawn the process with backend.cliCommand and args\n * }\n * ```\n */\nexport interface AIBackend {\n  /** Human-readable backend name (e.g., \"Claude\", \"Gemini\") */\n  readonly name: string;\n  /** CLI executable name on PATH (e.g., \"claude\", \"gemini\") */\n  readonly cliCommand: string;\n\n  /** Check whether this backend's CLI is available on PATH */\n  isAvailable(): Promise<boolean>;\n\n  /** Build the CLI argument array for a given call */\n  buildArgs(options: AICallOptions): string[];\n\n  /** Parse the CLI's stdout into a normalized {@link AIResponse} */\n  parseResponse(stdout: string, durationMs: number, exitCode: number): AIResponse;\n\n  /** Get user-facing install instructions when the CLI is not found */\n  getInstallInstructions(): string;\n\n  /**\n   * Compose the stdin input for the subprocess.\n   *\n   * Backends that cannot pass system prompts via CLI flags (e.g., OpenCode)\n   * use this to fold the system prompt into the stdin payload.\n   * When not implemented, {@link SubprocessProvider} falls back to `options.prompt`.\n   */\n  composeStdinInput?(options: AICallOptions): string;\n\n  /**\n   * Provision backend-specific resources in the target project.\n   *\n   * Called once per CLI invocation, before any AI calls.\n   * For example, OpenCode creates an agent config file in `.opencode/agents/`.\n   */\n  ensureProjectConfig?(projectRoot: string): Promise<void>;\n}\n\n// ---------------------------------------------------------------------------\n// Retry\n// ---------------------------------------------------------------------------\n\n/**\n * Configuration for the retry utility.\n *\n * Controls exponential backoff timing and which errors are retryable.\n */\nexport interface RetryOptions {\n  /** Maximum number of retries (e.g., 3 means up to 4 total attempts) */\n  maxRetries: number;\n  /** Base delay in milliseconds before first retry */\n  baseDelayMs: number;\n  /** Maximum delay cap in milliseconds */\n  maxDelayMs: number;\n  /** Exponential multiplier applied to the base delay */\n  multiplier: number;\n  /** Predicate that returns `true` if the error is transient and retryable */\n  isRetryable: (error: unknown) => boolean;\n  /** Optional callback invoked before each retry attempt */\n  onRetry?: (attempt: number, error: unknown) => void;\n}\n\n// ---------------------------------------------------------------------------\n// Telemetry\n// ---------------------------------------------------------------------------\n\n/**\n * Record of a single file read that was sent as context to an AI call.\n */\nexport interface FileRead {\n  /** File path relative to project root */\n  path: string;\n  /** File size in bytes at time of read */\n  sizeBytes: number;\n}\n\n/**\n * Per-call telemetry log entry.\n *\n * Captures everything needed to replay or debug a single AI call\n * without re-running it.\n */\nexport interface TelemetryEntry {\n  /** ISO 8601 timestamp when the call was initiated */\n  timestamp: string;\n  /** The prompt that was sent */\n  prompt: string;\n  /** The system prompt, if one was used */\n  systemPrompt?: string;\n  /** The AI model's text response */\n  response: string;\n  /** Model identifier */\n  model: string;\n  /** Number of input tokens consumed */\n  inputTokens: number;\n  /** Number of output tokens generated */\n  outputTokens: number;\n  /** Number of tokens served from cache reads */\n  cacheReadTokens: number;\n  /** Number of tokens written to cache */\n  cacheCreationTokens: number;\n  /** Wall-clock latency in milliseconds */\n  latencyMs: number;\n  /** Process exit code */\n  exitCode: number;\n  /** Error message, if the call failed */\n  error?: string;\n  /** Number of retries that occurred before this result */\n  retryCount: number;\n  /** AI thinking/reasoning content. \"not supported\" when backend doesn't provide it */\n  thinking: string;\n  /** Files sent as context for this call */\n  filesRead: FileRead[];\n}\n\n/**\n * Per-run log file structure.\n *\n * Aggregates all {@link TelemetryEntry} instances for a single CLI run,\n * plus a computed summary for quick performance review.\n */\nexport interface RunLog {\n  /** Unique run identifier (ISO timestamp-based) */\n  runId: string;\n  /** ISO 8601 timestamp when the run started */\n  startTime: string;\n  /** ISO 8601 timestamp when the run finished */\n  endTime: string;\n  /** Backend used for this run (e.g., \"Claude\", \"Gemini\", \"OpenCode\") */\n  backend: string;\n  /** Model used for this run (e.g., \"sonnet\", \"opus\", \"haiku\") */\n  model: string;\n  /** Command that triggered this run (e.g., \"generate\", \"update\", \"specify\", \"rebuild\") */\n  command: string;\n  /** All individual call entries */\n  entries: TelemetryEntry[];\n  /** Aggregated summary across all entries */\n  summary: {\n    /** Total number of AI calls made */\n    totalCalls: number;\n    /** Sum of input tokens across all calls */\n    totalInputTokens: number;\n    /** Sum of output tokens across all calls */\n    totalOutputTokens: number;\n    /** Total wall-clock duration in milliseconds */\n    totalDurationMs: number;\n    /** Number of calls that resulted in an error */\n    errorCount: number;\n    /** Sum of cache read tokens across all calls */\n    totalCacheReadTokens: number;\n    /** Sum of cache creation tokens across all calls */\n    totalCacheCreationTokens: number;\n    /** Total file reads across all calls (including duplicates) */\n    totalFilesRead: number;\n    /** Unique files read (deduped by path) */\n    uniqueFilesRead: number;\n  };\n}\n\n// ---------------------------------------------------------------------------\n// Errors\n// ---------------------------------------------------------------------------\n\n/** Error codes for typed error handling in the AI service layer */\nexport type AIServiceErrorCode =\n  | 'CLI_NOT_FOUND'\n  | 'TIMEOUT'\n  | 'PARSE_ERROR'\n  | 'SUBPROCESS_ERROR'\n  | 'RATE_LIMIT';\n\n/**\n * Typed error for AI service failures.\n *\n * Carries a machine-readable {@link AIServiceErrorCode} so callers can\n * branch on the error type without parsing message strings.\n *\n * @example\n * ```typescript\n * try {\n *   await callAI(options);\n * } catch (error) {\n *   if (error instanceof AIServiceError && error.code === 'RATE_LIMIT') {\n *     // handle rate limiting\n *   }\n * }\n * ```\n */\nexport class AIServiceError extends Error {\n  /** Machine-readable error code */\n  readonly code: AIServiceErrorCode;\n\n  constructor(code: AIServiceErrorCode, message: string) {\n    super(message);\n    this.name = 'AIServiceError';\n    this.code = code;\n  }\n}\n\n```\n\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n- Workflow & convention rules: if this file defines contribution guidelines, PR conventions, commit standards, testing mandates, tool usage requirements, approval workflows, AI agent behavioral instructions, or code conventions (naming standards, formatting rules, import ordering, linting policies), extract them as explicit, actionable rules. Sources: CONTRIBUTING.md, CI configs, PR templates, style guides, linter/formatter configs (.eslintrc, .prettierrc, .editorconfig), tsconfig strictness, README dev sections.\n\n\nTARGET LENGTH (MANDATORY):\n- Source file: 10827 characters\n- Target summary: ~1083 characters (10% compression)\n- Maximum: 1300 characters\n- Achieve this by: ultra-dense writing, minimal examples, single-sentence descriptions\n\nULTRA-COMPRESSION TECHNIQUES:\n- Use telegraphic style: \"exports foo, bar, baz\" not \"this file exports three functions\"\n- No code examples unless critical to understanding\n- One sentence per export maximum\n- Omit obvious parameter names (\"opts: Options\" not \"options: Options - configuration object\")\n- Use abbreviations: \"params\" not \"parameters\", \"config\" not \"configuration\"\n- Skip section if empty (don't write \"No special concerns\" or \"N/A\")\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nWORKFLOW & CONVENTION RULES (NEVER EXCLUDE):\n- Contribution requirements: branch naming, commit message formats, PR description templates\n- Testing mandates: required test types, coverage thresholds, \"never delete tests\" policies\n- Tool usage rules: required CLI commands, browser automation tools, linter/formatter requirements\n- Approval workflows: review requirements, CI checks that must pass, deployment gates\n- AI agent instructions: explicit directives for automated assistants (coding rules, prohibited actions)\n- Code conventions: naming standards (camelCase, PascalCase, snake_case for what), import ordering rules, file/folder naming patterns, formatting rules (indentation, quotes, semicolons), language idioms, TypeScript strictness policies, preferred patterns vs anti-patterns\nThese define how contributors (human or AI) must work within the project.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion .annex.sum file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**src/ai/types.ts defines the contract for AI backends, providers, and responses across the service layer.**\n\n## Exported Interfaces & Types\n\n**SubprocessResult** — result of CLI process execution with `stdout`, `stderr`, `exitCode`, `signal`, `durationMs`, `timedOut`, `childPid?`.\n\n**AICallOptions** — input for AI calls: `prompt` (required), `systemPrompt?`, `model?`, `timeoutMs?`, `maxTurns?`, `taskLabel?`.\n\n**AIResponse** — normalized backend output: `text`, `model`, `inputTokens`, `outputTokens`, `cacheReadTokens`, `cacheCreationTokens`, `durationMs`, `exitCode`, `raw`.\n\n**AIProvider** — injectable interface with `call(options: AICallOptions): Promise<AIResponse>`.\n\n**AIBackend** — CLI adapter contract with `name`, `cliCommand` properties; `isAvailable()`, `buildArgs(options)`, `parseResponse(stdout, durationMs, exitCode)`, `getInstallInstructions()`, optional `composeStdinInput?(options)`, optional `ensureProjectConfig?(projectRoot)`.\n\n**RetryOptions** — exponential backoff config: `maxRetries`, `baseDelayMs`, `maxDelayMs`, `multiplier`, `isRetryable(error)`, `onRetry?(attempt, error)`.\n\n**FileRead** — telemetry record: `path`, `sizeBytes`.\n\n**TelemetryEntry** — per-call log with `timestamp`, `prompt`, `systemPrompt?`, `response`, `model`, token counts (input/output/cache), `latencyMs`, `exitCode`, `error?`, `retryCount`, `thinking`, `filesRead[]`.\n\n**RunLog** — per-run aggregation: `runId`, `startTime`, `endTime`, `backend`, `model`, `command`, `entries[]`, `summary` (totalCalls, token totals, errorCount, fileRead stats).\n\n## Error Handling\n\n**AIServiceErrorCode** — union type: `'CLI_NOT_FOUND' | 'TIMEOUT' | 'PARSE_ERROR' | 'SUBPROCESS_ERROR' | 'RATE_LIMIT'`.\n\n**AIServiceError** — typed error class extending `Error` with `code: AIServiceErrorCode` property for pattern matching on error type.\n\n## Design Patterns\n\nDependency injection via `AIProvider` interface enables swapping backends (subprocess, HTTP API, mock) without changing caller code. `AIBackend` implements Strategy pattern for multiple CLI toolchains (Claude, Gemini, OpenCode); `AIService` wraps any provider with retry/telemetry cross-cutting concerns.",
      "model": "claude-haiku-4-5-20251001",
      "inputTokens": 3,
      "outputTokens": 637,
      "cacheReadTokens": 16147,
      "cacheCreationTokens": 5562,
      "latencyMs": 21508,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/ai/types.ts",
          "sizeBytes": 10827
        }
      ]
    },
    {
      "timestamp": "2026-02-14T13:46:41.639Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: src/change-detection/detector.ts\n\n```typescript\n/**\n * Git change detection and content hashing\n *\n * Uses simple-git for git operations and Node.js crypto for hashing.\n */\nimport { simpleGit } from 'simple-git';\nimport { createHash } from 'node:crypto';\nimport { readFile } from 'node:fs/promises';\nimport type { FileChange, ChangeDetectionResult, ChangeDetectionOptions } from './types.js';\n\n/**\n * Check if a path is inside a git repository.\n */\nexport async function isGitRepo(projectRoot: string): Promise<boolean> {\n  const git = simpleGit(projectRoot);\n  return git.checkIsRepo();\n}\n\n/**\n * Get the current HEAD commit hash.\n */\nexport async function getCurrentCommit(projectRoot: string): Promise<string> {\n  const git = simpleGit(projectRoot);\n  const hash = await git.revparse(['HEAD']);\n  return hash.trim();\n}\n\n/**\n * Detect files changed since a base commit.\n *\n * Uses git diff with --name-status and -M for rename detection.\n * Optionally includes uncommitted changes (staged + working directory).\n */\nexport async function getChangedFiles(\n  projectRoot: string,\n  baseCommit: string,\n  options: ChangeDetectionOptions = {}\n): Promise<ChangeDetectionResult> {\n  const git = simpleGit(projectRoot);\n  const currentCommit = await getCurrentCommit(projectRoot);\n  const changes: FileChange[] = [];\n\n  // Get committed changes from baseCommit to HEAD\n  const diff = await git.diff([\n    '--name-status',\n    '-M', // Detect renames (50% similarity threshold)\n    baseCommit,\n    'HEAD',\n  ]);\n\n  // Parse diff output\n  // Format: STATUS\\tFILE (or STATUS\\tOLD\\tNEW for renames)\n  const lines = diff.trim().split('\\n').filter(line => line.length > 0);\n\n  for (const line of lines) {\n    const parts = line.split('\\t');\n    if (parts.length < 2) continue;\n\n    const status = parts[0];\n    const filePath = parts[parts.length - 1]; // Last part is always the (new) path\n\n    if (status === 'A') {\n      changes.push({ path: filePath, status: 'added' });\n    } else if (status === 'M') {\n      changes.push({ path: filePath, status: 'modified' });\n    } else if (status === 'D') {\n      changes.push({ path: filePath, status: 'deleted' });\n    } else if (status.startsWith('R')) {\n      // Rename: R100 old new (R followed by similarity percentage)\n      const oldPath = parts[1];\n      changes.push({\n        path: filePath,\n        status: 'renamed',\n        oldPath,\n      });\n    }\n  }\n\n  // Optionally include uncommitted changes\n  if (options.includeUncommitted) {\n    const status = await git.status();\n\n    // Modified but not staged\n    for (const file of status.modified) {\n      if (!changes.some(c => c.path === file)) {\n        changes.push({ path: file, status: 'modified' });\n      }\n    }\n\n    // Staged for deletion\n    for (const file of status.deleted) {\n      if (!changes.some(c => c.path === file)) {\n        changes.push({ path: file, status: 'deleted' });\n      }\n    }\n\n    // Untracked files (new files not yet added)\n    for (const file of status.not_added) {\n      if (!changes.some(c => c.path === file)) {\n        changes.push({ path: file, status: 'added' });\n      }\n    }\n\n    // Staged files (new or modified)\n    for (const file of status.staged) {\n      if (!changes.some(c => c.path === file)) {\n        changes.push({ path: file, status: 'added' });\n      }\n    }\n  }\n\n  return {\n    currentCommit,\n    baseCommit,\n    changes,\n    includesUncommitted: options.includeUncommitted ?? false,\n  };\n}\n\n/**\n * Compute SHA-256 hash of a file's content.\n *\n * @param filePath - Absolute path to the file\n * @returns Hex-encoded SHA-256 hash\n */\nexport async function computeContentHash(filePath: string): Promise<string> {\n  const content = await readFile(filePath);\n  return createHash('sha256').update(content).digest('hex');\n}\n\n/**\n * Compute SHA-256 hash from an already-loaded string.\n *\n * Use this when the file content is already in memory to avoid\n * a redundant disk read.\n *\n * @param content - The file content as a string\n * @returns Hex-encoded SHA-256 hash\n */\nexport function computeContentHashFromString(content: string): string {\n  return createHash('sha256').update(content).digest('hex');\n}\n\n```\n\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n- Workflow & convention rules: if this file defines contribution guidelines, PR conventions, commit standards, testing mandates, tool usage requirements, approval workflows, AI agent behavioral instructions, or code conventions (naming standards, formatting rules, import ordering, linting policies), extract them as explicit, actionable rules. Sources: CONTRIBUTING.md, CI configs, PR templates, style guides, linter/formatter configs (.eslintrc, .prettierrc, .editorconfig), tsconfig strictness, README dev sections.\n\n\nTARGET LENGTH (MANDATORY):\n- Source file: 4102 characters\n- Target summary: ~410 characters (10% compression)\n- Maximum: 492 characters\n- Achieve this by: ultra-dense writing, minimal examples, single-sentence descriptions\n\nULTRA-COMPRESSION TECHNIQUES:\n- Use telegraphic style: \"exports foo, bar, baz\" not \"this file exports three functions\"\n- No code examples unless critical to understanding\n- One sentence per export maximum\n- Omit obvious parameter names (\"opts: Options\" not \"options: Options - configuration object\")\n- Use abbreviations: \"params\" not \"parameters\", \"config\" not \"configuration\"\n- Skip section if empty (don't write \"No special concerns\" or \"N/A\")\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nWORKFLOW & CONVENTION RULES (NEVER EXCLUDE):\n- Contribution requirements: branch naming, commit message formats, PR description templates\n- Testing mandates: required test types, coverage thresholds, \"never delete tests\" policies\n- Tool usage rules: required CLI commands, browser automation tools, linter/formatter requirements\n- Approval workflows: review requirements, CI checks that must pass, deployment gates\n- AI agent instructions: explicit directives for automated assistants (coding rules, prohibited actions)\n- Code conventions: naming standards (camelCase, PascalCase, snake_case for what), import ordering rules, file/folder naming patterns, formatting rules (indentation, quotes, semicolons), language idioms, TypeScript strictness policies, preferred patterns vs anti-patterns\nThese define how contributors (human or AI) must work within the project.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion .annex.sum file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**detector.ts implements git change detection and content hashing for tracking file modifications and computing integrity checksums.**\n\n## Exports\n\n`isGitRepo(projectRoot: string): Promise<boolean>` — checks if projectRoot is a git repository via `git.checkIsRepo()`.\n\n`getCurrentCommit(projectRoot: string): Promise<string>` — returns current HEAD commit hash from `git revparse HEAD`, trimmed.\n\n`getChangedFiles(projectRoot: string, baseCommit: string, options?: ChangeDetectionOptions): Promise<ChangeDetectionResult>` — detects file changes between baseCommit and HEAD using `git diff --name-status -M`; optionally includes uncommitted changes (staged, modified, deleted, untracked) when `options.includeUncommitted` is true; parses diff output mapping status codes (A=added, M=modified, D=deleted, R*=renamed) to FileChange objects with `path`, `status`, and optional `oldPath`.\n\n`computeContentHash(filePath: string): Promise<string>` — computes SHA-256 hex digest of file content via `readFile()` + `createHash('sha256')`.\n\n`computeContentHashFromString(content: string): string` — computes SHA-256 hex digest synchronously from string content; avoids redundant disk reads.\n\n## Key Behaviors\n\nRename detection uses `-M` flag with 50% similarity threshold; status prefix R followed by percentage (R100, R95, etc.) maps old/new paths from tab-separated parts[1] and parts[last].\n\nUncommitted change tracking deduplicates across `status.modified`, `status.deleted`, `status.not_added`, `status.staged` to avoid duplicate FileChange entries for same path.\n\nDiff output format: `STATUS\\tFILE` (or `STATUS\\tOLD\\tNEW` for renames); lines filtered for non-empty and split on tab.\n\n## Dependencies\n\n`simple-git` for git operations (`checkIsRepo()`, `revparse()`, `diff()`, `status()`); `node:crypto.createHash()` for SHA-256; `node:fs/promises.readFile()` for file I/O.",
      "model": "claude-haiku-4-5-20251001",
      "inputTokens": 3,
      "outputTokens": 510,
      "cacheReadTokens": 16147,
      "cacheCreationTokens": 4001,
      "latencyMs": 20758,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/change-detection/detector.ts",
          "sizeBytes": 4102
        }
      ]
    },
    {
      "timestamp": "2026-02-14T13:46:43.413Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: src/change-detection/types.ts\n\n```typescript\n/**\n * Types for git change detection\n */\n\n/**\n * Type of change detected for a file\n */\nexport type ChangeType = 'added' | 'modified' | 'deleted' | 'renamed';\n\n/**\n * A file change detected from git diff\n */\nexport interface FileChange {\n  /** Relative path to the file (new path for renames) */\n  path: string;\n  /** Type of change */\n  status: ChangeType;\n  /** Original path for renamed files */\n  oldPath?: string;\n}\n\n/**\n * Result of change detection\n */\nexport interface ChangeDetectionResult {\n  /** Current commit hash */\n  currentCommit: string;\n  /** Commit hash we're comparing from */\n  baseCommit: string;\n  /** List of changed files */\n  changes: FileChange[];\n  /** Whether uncommitted changes were included */\n  includesUncommitted: boolean;\n}\n\n/**\n * Options for change detection\n */\nexport interface ChangeDetectionOptions {\n  /** Include uncommitted (staged and working directory) changes */\n  includeUncommitted?: boolean;\n}\n\n```\n\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n- Workflow & convention rules: if this file defines contribution guidelines, PR conventions, commit standards, testing mandates, tool usage requirements, approval workflows, AI agent behavioral instructions, or code conventions (naming standards, formatting rules, import ordering, linting policies), extract them as explicit, actionable rules. Sources: CONTRIBUTING.md, CI configs, PR templates, style guides, linter/formatter configs (.eslintrc, .prettierrc, .editorconfig), tsconfig strictness, README dev sections.\n\n\nTARGET LENGTH (MANDATORY):\n- Source file: 946 characters\n- Target summary: ~95 characters (10% compression)\n- Maximum: 114 characters\n- Achieve this by: ultra-dense writing, minimal examples, single-sentence descriptions\n\nULTRA-COMPRESSION TECHNIQUES:\n- Use telegraphic style: \"exports foo, bar, baz\" not \"this file exports three functions\"\n- No code examples unless critical to understanding\n- One sentence per export maximum\n- Omit obvious parameter names (\"opts: Options\" not \"options: Options - configuration object\")\n- Use abbreviations: \"params\" not \"parameters\", \"config\" not \"configuration\"\n- Skip section if empty (don't write \"No special concerns\" or \"N/A\")\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nWORKFLOW & CONVENTION RULES (NEVER EXCLUDE):\n- Contribution requirements: branch naming, commit message formats, PR description templates\n- Testing mandates: required test types, coverage thresholds, \"never delete tests\" policies\n- Tool usage rules: required CLI commands, browser automation tools, linter/formatter requirements\n- Approval workflows: review requirements, CI checks that must pass, deployment gates\n- AI agent instructions: explicit directives for automated assistants (coding rules, prohibited actions)\n- Code conventions: naming standards (camelCase, PascalCase, snake_case for what), import ordering rules, file/folder naming patterns, formatting rules (indentation, quotes, semicolons), language idioms, TypeScript strictness policies, preferred patterns vs anti-patterns\nThese define how contributors (human or AI) must work within the project.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion .annex.sum file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**types.ts defines TypeScript interfaces and types for git change detection operations.**\n\n## Exported Types\n\n`ChangeType` — union of `'added' | 'modified' | 'deleted' | 'renamed'` for file operation classification.\n\n`FileChange` — interface with `path: string`, `status: ChangeType`, `oldPath?: string`; represents single file change from git diff.\n\n`ChangeDetectionResult` — interface with `currentCommit: string`, `baseCommit: string`, `changes: FileChange[]`, `includesUncommitted: boolean`; encapsulates change detection output.\n\n`ChangeDetectionOptions` — interface with `includeUncommitted?: boolean`; controls whether to include staged and working directory changes in detection.",
      "model": "claude-haiku-4-5-20251001",
      "inputTokens": 3,
      "outputTokens": 175,
      "cacheReadTokens": 16147,
      "cacheCreationTokens": 3000,
      "latencyMs": 19485,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/change-detection/types.ts",
          "sizeBytes": 946
        }
      ]
    },
    {
      "timestamp": "2026-02-14T13:46:39.407Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: src/ai/service.ts\n\n```typescript\n/**\n * AI service orchestrator.\n *\n * The {@link AIService} class is the main entry point for making AI calls.\n * It wraps any {@link AIProvider} with retry logic, telemetry recording,\n * and optional subprocess log writing.\n *\n * For CLI usage, pass an {@link AIBackend} which is auto-wrapped in a\n * {@link SubprocessProvider}. For library usage, pass any custom\n * {@link AIProvider} implementation directly.\n *\n * @module\n */\n\nimport { writeFile, mkdir } from 'node:fs/promises';\nimport path from 'node:path';\nimport type { AIBackend, AIProvider, AICallOptions, AIResponse, SubprocessResult, RunLog, FileRead } from './types.js';\nimport { AIServiceError } from './types.js';\nimport { withRetry, DEFAULT_RETRY_OPTIONS } from './retry.js';\nimport { TelemetryLogger } from './telemetry/logger.js';\nimport { writeRunLog } from './telemetry/run-log.js';\nimport { cleanupOldLogs } from './telemetry/cleanup.js';\nimport { SubprocessProvider } from './providers/subprocess.js';\nimport type { ITraceWriter } from '../orchestration/trace.js';\nimport type { Logger } from '../core/logger.js';\nimport { nullLogger } from '../core/logger.js';\n\n// ---------------------------------------------------------------------------\n// AIService options\n// ---------------------------------------------------------------------------\n\n/**\n * Configuration options for the {@link AIService}.\n *\n * These are typically sourced from the config schema's `ai` section.\n */\nexport interface AIServiceOptions {\n  /** Default subprocess timeout in milliseconds */\n  timeoutMs: number;\n  /** Maximum number of retries for transient errors */\n  maxRetries: number;\n  /** Default model identifier (e.g., \"sonnet\", \"opus\") applied to all calls unless overridden per-call */\n  model?: string;\n  /** Command that triggered this run (e.g., \"generate\", \"update\", \"specify\", \"rebuild\") */\n  command: string;\n  /** Telemetry settings */\n  telemetry: {\n    /** Number of most recent run logs to keep on disk */\n    keepRuns: number;\n  };\n}\n\n// ---------------------------------------------------------------------------\n// AIService\n// ---------------------------------------------------------------------------\n\n/**\n * Orchestrates AI calls with retry, timeout, and telemetry.\n *\n * Wraps any {@link AIProvider} with retry logic and telemetry recording.\n * Create one instance per run. Call {@link call} for each AI invocation.\n * Call {@link finalize} at the end to write the run log.\n *\n * @example\n * ```typescript\n * // CLI usage (backward compatible): pass AIBackend directly\n * import { AIService } from './service.js';\n * import { resolveBackend, createBackendRegistry } from './registry.js';\n *\n * const backend = await resolveBackend(createBackendRegistry(), 'auto');\n * const service = new AIService(backend, {\n *   timeoutMs: 120_000,\n *   maxRetries: 3,\n *   telemetry: { keepRuns: 10 },\n * });\n * ```\n *\n * @example\n * ```typescript\n * // Library usage: pass custom AIProvider\n * import { AIService } from './service.js';\n * import type { AIProvider } from './types.js';\n *\n * const provider: AIProvider = new MyCustomProvider();\n * const service = new AIService(provider, {\n *   timeoutMs: 120_000,\n *   maxRetries: 3,\n *   telemetry: { keepRuns: 10 },\n * });\n * ```\n */\nexport class AIService {\n  /** The provider used for AI calls */\n  private readonly provider: AIProvider;\n\n  /** Service configuration */\n  private readonly options: AIServiceOptions;\n\n  /** In-memory telemetry logger for this run */\n  private readonly logger: TelemetryLogger;\n\n  /** Running count of calls made (used for entry tracking) */\n  private callCount: number = 0;\n\n  /** Trace writer for retry event tracing */\n  private tracer: ITraceWriter | null = null;\n\n  /** Whether debug mode is enabled */\n  private debug: boolean = false;\n\n  /** Directory for subprocess output logs (null = disabled) */\n  private subprocessLogDir: string | null = null;\n\n  /** Serializes log writes so concurrent workers don't interleave mkdirs */\n  private logWriteQueue: Promise<void> = Promise.resolve();\n\n  /** Debug/warn/error logger (injected; defaults to silent) */\n  private readonly log: Logger;\n\n  /** Backend reference kept for subprocess log writing (null when using custom provider) */\n  private readonly backend: AIBackend | null;\n\n  /**\n   * Create a new AI service instance.\n   *\n   * Accepts either an {@link AIBackend} (auto-wrapped in {@link SubprocessProvider})\n   * or a custom {@link AIProvider} implementation.\n   *\n   * @param providerOrBackend - An AIProvider or AIBackend\n   * @param options - Service configuration (timeout, retries, telemetry)\n   * @param debugLogger - Optional debug logger (defaults to nullLogger)\n   */\n  constructor(providerOrBackend: AIProvider | AIBackend, options: AIServiceOptions, debugLogger?: Logger) {\n    this.options = options;\n    this.log = debugLogger ?? nullLogger;\n\n    // Auto-wrap AIBackend in SubprocessProvider for backward compatibility\n    if (isAIBackend(providerOrBackend)) {\n      this.backend = providerOrBackend;\n      this.provider = new SubprocessProvider(providerOrBackend, {\n        timeoutMs: options.timeoutMs,\n        logger: this.log,\n      });\n      // Create logger with backend name, model, and command\n      this.logger = new TelemetryLogger(\n        new Date().toISOString(),\n        providerOrBackend.name,\n        options.model ?? 'unknown',\n        options.command,\n      );\n    } else {\n      this.backend = null;\n      this.provider = providerOrBackend;\n      // For custom providers, use 'custom' as backend name\n      this.logger = new TelemetryLogger(\n        new Date().toISOString(),\n        'custom',\n        options.model ?? 'unknown',\n        options.command,\n      );\n    }\n  }\n\n  /**\n   * Set the trace writer for retry event tracing.\n   *\n   * If the provider is a {@link SubprocessProvider}, also sets the tracer\n   * on it for subprocess spawn/exit events.\n   *\n   * @param tracer - The trace writer instance\n   */\n  setTracer(tracer: ITraceWriter): void {\n    this.tracer = tracer;\n    // Forward to subprocess provider if applicable\n    if (this.provider instanceof SubprocessProvider) {\n      this.provider.setTracer(tracer);\n    }\n  }\n\n  /**\n   * Enable debug mode for verbose subprocess logging to stderr.\n   */\n  setDebug(enabled: boolean): void {\n    this.debug = enabled;\n  }\n\n  /**\n   * Set a directory for writing subprocess stdout/stderr log files.\n   *\n   * When set, each subprocess invocation writes a `.log` file containing\n   * the metadata header, stdout, and stderr. Useful for diagnosing\n   * timed-out or failed subprocesses whose output would otherwise be lost.\n   *\n   * @param dir - Absolute path to the log directory (created on first write)\n   */\n  setSubprocessLogDir(dir: string): void {\n    this.subprocessLogDir = dir;\n    // Forward to subprocess provider if applicable\n    if (this.provider instanceof SubprocessProvider) {\n      this.provider.setSubprocessLogDir(dir);\n    }\n  }\n\n  /**\n   * Make an AI call with retry logic and telemetry recording.\n   *\n   * The call flow:\n   * 1. Merge service-level model default\n   * 2. Delegate to the provider with retry wrapping\n   * 3. On success: record telemetry entry\n   * 4. On failure: record error telemetry entry, throw the error\n   *\n   * Retries are attempted for `RATE_LIMIT` errors only.\n   *\n   * @param options - The call options (prompt, model, timeout, etc.)\n   * @returns The normalized AI response\n   * @throws {AIServiceError} On timeout, rate limit exhaustion, parse error, or subprocess failure\n   */\n  async call(options: AICallOptions): Promise<AIResponse> {\n    this.callCount++;\n    const callStart = Date.now();\n    const timestamp = new Date().toISOString();\n    const taskLabel = options.taskLabel ?? 'unknown';\n\n    // Merge service-level model as default (per-call options.model wins)\n    const effectiveOptions: AICallOptions = {\n      ...options,\n      model: options.model ?? this.options.model,\n    };\n\n    let retryCount = 0;\n\n    try {\n      const response = await withRetry(\n        async () => {\n          return this.provider.call(effectiveOptions);\n        },\n        {\n          ...DEFAULT_RETRY_OPTIONS,\n          maxRetries: this.options.maxRetries,\n          isRetryable: (error: unknown): boolean => {\n            // Only retry rate limits. Timeouts are NOT retried because\n            // spawning another heavyweight subprocess on a system that's\n            // already struggling (or against an unresponsive API) makes\n            // things worse and can exhaust system resources.\n            return (\n              error instanceof AIServiceError &&\n              error.code === 'RATE_LIMIT'\n            );\n          },\n          onRetry: (attempt: number, error: unknown) => {\n            retryCount++;\n\n            const errorCode = error instanceof AIServiceError ? error.code : 'UNKNOWN';\n\n            // Always warn on retry (not just debug) -- retries are noteworthy\n            this.log.warn(\n              `[warn] Retrying \"${taskLabel}\" (attempt ${attempt}/${this.options.maxRetries}, reason: ${errorCode})`,\n            );\n\n            // Emit retry trace event\n            if (this.tracer) {\n              this.tracer.emit({\n                type: 'retry',\n                attempt,\n                taskLabel,\n                errorCode,\n              });\n            }\n          },\n        },\n      );\n\n      // Record successful call\n      this.logger.addEntry({\n        timestamp,\n        prompt: options.prompt,\n        systemPrompt: options.systemPrompt,\n        response: response.text,\n        model: response.model,\n        inputTokens: response.inputTokens,\n        outputTokens: response.outputTokens,\n        cacheReadTokens: response.cacheReadTokens,\n        cacheCreationTokens: response.cacheCreationTokens,\n        latencyMs: response.durationMs,\n        exitCode: response.exitCode,\n        retryCount,\n        thinking: 'not supported',\n        filesRead: [],\n      });\n\n      return response;\n    } catch (error) {\n      // Record failed call\n      const latencyMs = Date.now() - callStart;\n      const errorMessage = error instanceof Error ? error.message : String(error);\n\n      this.logger.addEntry({\n        timestamp,\n        prompt: options.prompt,\n        systemPrompt: options.systemPrompt,\n        response: '',\n        model: options.model ?? 'unknown',\n        inputTokens: 0,\n        outputTokens: 0,\n        cacheReadTokens: 0,\n        cacheCreationTokens: 0,\n        latencyMs,\n        exitCode: 1,\n        error: errorMessage,\n        retryCount,\n        thinking: 'not supported',\n        filesRead: [],\n      });\n\n      throw error;\n    }\n  }\n\n  /**\n   * Finalize the run: write the run log to disk and clean up old files.\n   *\n   * Call this once at the end of a CLI invocation, after all `call()`\n   * invocations have completed (or failed).\n   *\n   * @param projectRoot - Absolute path to the project root directory\n   * @returns The log file path and the run summary\n   */\n  async finalize(projectRoot: string): Promise<{ logPath: string; summary: RunLog['summary'] }> {\n    const runLog = this.logger.toRunLog();\n    const logPath = await writeRunLog(projectRoot, runLog);\n    await cleanupOldLogs(projectRoot, this.options.telemetry.keepRuns);\n    return { logPath, summary: runLog.summary };\n  }\n\n  /**\n   * Attach file-read metadata to the most recent telemetry entry.\n   *\n   * Called by the command runner after an AI call completes, to record\n   * which source files were sent as context for that call.\n   *\n   * @param filesRead - Array of file-read records (path + size)\n   */\n  addFilesReadToLastEntry(filesRead: FileRead[]): void {\n    this.logger.setFilesReadOnLastEntry(filesRead);\n  }\n\n  /**\n   * Get the current run summary without finalizing.\n   *\n   * Useful for displaying progress during a run.\n   *\n   * @returns Current summary statistics\n   */\n  getSummary(): RunLog['summary'] {\n    return this.logger.getSummary();\n  }\n}\n\n// ---------------------------------------------------------------------------\n// Type guard\n// ---------------------------------------------------------------------------\n\n/**\n * Check whether the given object is an AIBackend (has buildArgs, parseResponse)\n * rather than an AIProvider (has only call).\n */\nfunction isAIBackend(obj: AIProvider | AIBackend): obj is AIBackend {\n  return 'buildArgs' in obj && 'parseResponse' in obj && 'cliCommand' in obj;\n}\n\n```\n\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n- Workflow & convention rules: if this file defines contribution guidelines, PR conventions, commit standards, testing mandates, tool usage requirements, approval workflows, AI agent behavioral instructions, or code conventions (naming standards, formatting rules, import ordering, linting policies), extract them as explicit, actionable rules. Sources: CONTRIBUTING.md, CI configs, PR templates, style guides, linter/formatter configs (.eslintrc, .prettierrc, .editorconfig), tsconfig strictness, README dev sections.\n\n\nTARGET LENGTH (MANDATORY):\n- Source file: 12411 characters\n- Target summary: ~1241 characters (10% compression)\n- Maximum: 1489 characters\n- Achieve this by: ultra-dense writing, minimal examples, single-sentence descriptions\n\nULTRA-COMPRESSION TECHNIQUES:\n- Use telegraphic style: \"exports foo, bar, baz\" not \"this file exports three functions\"\n- No code examples unless critical to understanding\n- One sentence per export maximum\n- Omit obvious parameter names (\"opts: Options\" not \"options: Options - configuration object\")\n- Use abbreviations: \"params\" not \"parameters\", \"config\" not \"configuration\"\n- Skip section if empty (don't write \"No special concerns\" or \"N/A\")\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nWORKFLOW & CONVENTION RULES (NEVER EXCLUDE):\n- Contribution requirements: branch naming, commit message formats, PR description templates\n- Testing mandates: required test types, coverage thresholds, \"never delete tests\" policies\n- Tool usage rules: required CLI commands, browser automation tools, linter/formatter requirements\n- Approval workflows: review requirements, CI checks that must pass, deployment gates\n- AI agent instructions: explicit directives for automated assistants (coding rules, prohibited actions)\n- Code conventions: naming standards (camelCase, PascalCase, snake_case for what), import ordering rules, file/folder naming patterns, formatting rules (indentation, quotes, semicolons), language idioms, TypeScript strictness policies, preferred patterns vs anti-patterns\nThese define how contributors (human or AI) must work within the project.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion .annex.sum file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**AIService orchestrates AI calls with retry logic, telemetry recording, and subprocess management.**\n\n## Exports\n\n`AIService` class: main orchestrator wrapping any `AIProvider` with configurable retry (rate-limit only), timeout, and telemetry; constructor accepts `AIProvider | AIBackend`, options `AIServiceOptions`, optional `Logger`; methods: `call(AICallOptions): Promise<AIResponse>`, `finalize(projectRoot: string): Promise<{ logPath, summary }>`, `setTracer(ITraceWriter): void`, `setDebug(boolean): void`, `setSubprocessLogDir(string): void`, `addFilesReadToLastEntry(FileRead[]): void`, `getSummary(): RunLog['summary']`.\n\n`AIServiceOptions` interface: `timeoutMs` (subprocess timeout), `maxRetries` (transient error retries), `model?` (default model ID applied unless overridden per-call), `command` (triggering command: \"generate\"|\"update\"|\"specify\"|\"rebuild\"), `telemetry.keepRuns` (max recent logs retained).\n\n`isAIBackend(obj: AIProvider | AIBackend): obj is AIBackend` type guard: checks for `buildArgs`, `parseResponse`, `cliCommand` properties to distinguish backend from provider.\n\n## Integration & Dependencies\n\n- Wraps `AIProvider` implementations; auto-wraps `AIBackend` in `SubprocessProvider`\n- Delegates `call()` to provider with `withRetry` using `DEFAULT_RETRY_OPTIONS`\n- Records telemetry via `TelemetryLogger`; writes run logs with `writeRunLog`; cleans old logs via `cleanupOldLogs`\n- Accepts `ITraceWriter` for retry event tracing; forwards to subprocess provider if applicable\n- Imports `Logger` for debug output (defaults to `nullLogger`)\n\n## Retry & Error Handling\n\nRetries only on `RATE_LIMIT` errors (code explicitly excludes timeout retries to prevent resource exhaustion). `onRetry` callback logs warn message with attempt count and emits trace event. `AIServiceError` with code `RATE_LIMIT` triggers retry; other `AIServiceError` codes (timeout, parse, subprocess failure) throw immediately. Failed calls recorded as error entries in telemetry with `exitCode: 1`.\n\n## Telemetry & State\n\n`TelemetryLogger` initialized with ISO timestamp, backend/provider name (\"custom\" for non-backend), model, command. Each `call()` increments `callCount`; successful calls record response metadata (model, tokens, latency); failed calls record error message. `finalize()` writes run log to disk, returns `logPath` and `summary`. `getSummary()` exposes current statistics without finalization. `addFilesReadToLastEntry()` attaches file metadata to most recent entry post-call.\n\n## Configuration & Options Merging\n\nService-level `options.model` serves as default; per-call `options.model` wins. `options.command` identifies triggering operation. Subprocess timeout forwarded to `SubprocessProvider`. Subprocess log directory (if set) creates files on first write with queued I/O (`logWriteQueue: Promise<void>`) to prevent concurrent mkdir interleaving.",
      "model": "claude-haiku-4-5-20251001",
      "inputTokens": 3,
      "outputTokens": 757,
      "cacheReadTokens": 16147,
      "cacheCreationTokens": 6152,
      "latencyMs": 24014,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/ai/service.ts",
          "sizeBytes": 12411
        }
      ]
    },
    {
      "timestamp": "2026-02-14T13:46:44.850Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: src/cli/discover.ts\n\n```typescript\n/**\n * `are discover` command - Discover files to analyze\n *\n * Walks a directory tree and applies filters (gitignore, vendor, binary, custom)\n * to identify files suitable for analysis.\n */\n\nimport path from 'node:path';\nimport { access, mkdir, writeFile } from 'node:fs/promises';\nimport { constants } from 'node:fs';\nimport pc from 'picocolors';\nimport { loadConfig, findProjectRoot } from '../config/loader.js';\nimport { discoverFiles } from '../discovery/run.js';\nimport { createLogger } from '../output/logger.js';\nimport { createOrchestrator } from '../orchestration/orchestrator.js';\nimport { buildExecutionPlan, formatExecutionPlanAsMarkdown } from '../generation/executor.js';\nimport { ProgressLog } from '../orchestration/index.js';\nimport type { DiscoveryResult } from '../types/index.js';\nimport type { ITraceWriter } from '../orchestration/trace.js';\n\n/**\n * Options for the discover command.\n */\nexport interface DiscoverOptions {\n  /**\n   * Optional trace writer for emitting discovery events.\n   */\n  tracer?: ITraceWriter;\n\n  /**\n   * Enable debug output.\n   * @default false\n   */\n  debug?: boolean;\n\n  /**\n   * Show excluded files in output.\n   * @default false\n   */\n  showExcluded?: boolean;\n}\n\n/**\n * Execute the `are discover` command.\n *\n * Discovers files in the target directory, applying all configured filters\n * (gitignore, vendor, binary, custom patterns).\n *\n * @param targetPath - Directory to scan (defaults to current working directory)\n * @param options - Command options\n *\n * @example\n * ```typescript\n * await discoverCommand('.', {});\n * ```\n */\nexport async function discoverCommand(\n  targetPath: string,\n  options: DiscoverOptions\n): Promise<void> {\n  // Resolve to absolute path (default to cwd), then walk up to find project root\n  const resolvedPath = await findProjectRoot(path.resolve(targetPath || process.cwd()));\n\n  // Load configuration (uses defaults if no config file)\n  const config = await loadConfig(resolvedPath);\n\n  const logger = createLogger({ colors: config.output.colors });\n\n  // Verify target path exists\n  try {\n    await access(resolvedPath, constants.R_OK);\n  } catch (err) {\n    const error = err as NodeJS.ErrnoException;\n    if (error.code === 'ENOENT') {\n      logger.error(`Directory not found: ${resolvedPath}`);\n      process.exit(1);\n    }\n    if (error.code === 'EACCES' || error.code === 'EPERM') {\n      logger.error(`Permission denied: ${resolvedPath}`);\n      process.exit(1);\n    }\n    throw error;\n  }\n\n  // Create progress log for tail -f monitoring\n  const progressLog = ProgressLog.create(resolvedPath);\n  progressLog.write(`=== ARE Discover (${new Date().toISOString()}) ===`);\n  progressLog.write(`Project: ${resolvedPath}`);\n  progressLog.write('');\n\n  logger.info(`Discovering files in ${resolvedPath}...`);\n  logger.info('');\n  progressLog.write(`Discovering files in ${resolvedPath}...`);\n\n  // Emit discovery start trace event\n  const discoveryStartTime = process.hrtime.bigint();\n  options.tracer?.emit({\n    type: 'discovery:start',\n    targetPath: resolvedPath,\n  });\n\n  if (options.debug) {\n    console.error(pc.dim(`[debug] Discovering files in: ${resolvedPath}`));\n  }\n\n  // Run shared discovery pipeline (walk + filter)\n  const result = await discoverFiles(resolvedPath, config, {\n    tracer: options.tracer,\n    debug: options.debug,\n  });\n\n  // Emit discovery end trace event\n  const discoveryEndTime = process.hrtime.bigint();\n  const discoveryDurationMs = Number(discoveryEndTime - discoveryStartTime) / 1_000_000;\n  options.tracer?.emit({\n    type: 'discovery:end',\n    filesIncluded: result.included.length,\n    filesExcluded: result.excluded.length,\n    durationMs: discoveryDurationMs,\n  });\n\n  if (options.debug) {\n    console.error(\n      pc.dim(\n        `[debug] Discovery complete: ${result.included.length} files included, ${result.excluded.length} excluded`\n      )\n    );\n  }\n\n  // Log results\n  // Make paths relative for cleaner output\n  const relativePath = (absPath: string): string =>\n    path.relative(resolvedPath, absPath);\n\n  // Show each included file\n  for (const file of result.included) {\n    const rel = relativePath(file);\n    logger.file(rel);\n    progressLog.write(`  + ${rel}`);\n  }\n\n  // Show each excluded file (only with --show-excluded)\n  if (options.showExcluded) {\n    for (const excluded of result.excluded) {\n      const rel = relativePath(excluded.path);\n      logger.excluded(rel, excluded.reason, excluded.filter);\n      progressLog.write(`  - ${rel} (${excluded.reason}: ${excluded.filter})`);\n    }\n  }\n\n  // Summary\n  logger.summary(result.included.length, result.excluded.length);\n  progressLog.write(`\\nDiscovered ${result.included.length} files (${result.excluded.length} excluded)`);\n\n  // Generate GENERATION-PLAN.md\n  {\n    logger.info('');\n    logger.info('Generating execution plan...');\n    progressLog.write('');\n    progressLog.write('Generating execution plan...');\n\n    // Create discovery result for orchestrator\n    const discoveryResult: DiscoveryResult = {\n      files: result.included,\n      excluded: result.excluded.map(e => ({ path: e.path, reason: e.reason })),\n    };\n\n    // Create orchestrator and build generation plan\n    const orchestrator = createOrchestrator(config, resolvedPath);\n    const generationPlan = await orchestrator.createPlan(discoveryResult);\n\n    // Build execution plan with post-order traversal\n    const executionPlan = buildExecutionPlan(generationPlan, resolvedPath);\n\n    // Format as markdown\n    const markdown = formatExecutionPlanAsMarkdown(executionPlan);\n\n    // Write to .agents-reverse-engineer/GENERATION-PLAN.md\n    const configDir = path.join(resolvedPath, '.agents-reverse-engineer');\n    const planPath = path.join(configDir, 'GENERATION-PLAN.md');\n\n    try {\n      await mkdir(configDir, { recursive: true });\n      await writeFile(planPath, markdown, 'utf8');\n      const planRelPath = path.relative(resolvedPath, planPath);\n      logger.info(`Created ${planRelPath}`);\n      progressLog.write(`Created ${planRelPath}`);\n    } catch (err) {\n      const msg = (err as Error).message;\n      logger.error(`Failed to write plan: ${msg}`);\n      progressLog.write(`Error: Failed to write plan: ${msg}`);\n      await progressLog.finalize();\n      process.exit(1);\n    }\n  }\n\n  await progressLog.finalize();\n}\n\n```\n\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n- Workflow & convention rules: if this file defines contribution guidelines, PR conventions, commit standards, testing mandates, tool usage requirements, approval workflows, AI agent behavioral instructions, or code conventions (naming standards, formatting rules, import ordering, linting policies), extract them as explicit, actionable rules. Sources: CONTRIBUTING.md, CI configs, PR templates, style guides, linter/formatter configs (.eslintrc, .prettierrc, .editorconfig), tsconfig strictness, README dev sections.\n\n\nTARGET LENGTH (MANDATORY):\n- Source file: 6316 characters\n- Target summary: ~632 characters (10% compression)\n- Maximum: 758 characters\n- Achieve this by: ultra-dense writing, minimal examples, single-sentence descriptions\n\nULTRA-COMPRESSION TECHNIQUES:\n- Use telegraphic style: \"exports foo, bar, baz\" not \"this file exports three functions\"\n- No code examples unless critical to understanding\n- One sentence per export maximum\n- Omit obvious parameter names (\"opts: Options\" not \"options: Options - configuration object\")\n- Use abbreviations: \"params\" not \"parameters\", \"config\" not \"configuration\"\n- Skip section if empty (don't write \"No special concerns\" or \"N/A\")\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nWORKFLOW & CONVENTION RULES (NEVER EXCLUDE):\n- Contribution requirements: branch naming, commit message formats, PR description templates\n- Testing mandates: required test types, coverage thresholds, \"never delete tests\" policies\n- Tool usage rules: required CLI commands, browser automation tools, linter/formatter requirements\n- Approval workflows: review requirements, CI checks that must pass, deployment gates\n- AI agent instructions: explicit directives for automated assistants (coding rules, prohibited actions)\n- Code conventions: naming standards (camelCase, PascalCase, snake_case for what), import ordering rules, file/folder naming patterns, formatting rules (indentation, quotes, semicolons), language idioms, TypeScript strictness policies, preferred patterns vs anti-patterns\nThese define how contributors (human or AI) must work within the project.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion .annex.sum file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**discover.ts implements the `are discover` command for walking directory trees with gitignore/vendor/binary/custom filters to identify files suitable for analysis.**\n\n## Exports\n\n**discoverCommand**(targetPath: string, options: DiscoverOptions): Promise<void> — Entry point; resolves targetPath to project root via findProjectRoot, loads config, calls discoverFiles to filter candidates, emits discovery:start and discovery:end trace events, logs results (included/excluded files), and generates GENERATION-PLAN.md via orchestrator.createPlan and buildExecutionPlan.\n\n**DiscoverOptions** interface — tracer?: ITraceWriter; debug?: boolean; showExcluded?: boolean — controls trace emission, debug console output, and exclusion reporting.\n\n## Integration & Dependencies\n\ndiscoverCommand orchestrates **discoverFiles** (discovery/run.js) for the walk+filter pipeline, **loadConfig** (config/loader.js) for filter rules, **createLogger** (output/logger.js) for formatted output, **createOrchestrator** (orchestration/orchestrator.js) for plan generation, **buildExecutionPlan** and **formatExecutionPlanAsMarkdown** (generation/executor.js) for post-order execution plan markdown, and **ProgressLog** (orchestration/index.js) for tail -f monitoring.\n\n## Behavioral Contracts\n\n**Trace events**: discovery:start (targetPath), discovery:end (filesIncluded, filesExcluded, durationMs) — emitted via options.tracer?.emit().\n\n**Error handling**: ENOENT → \"Directory not found\"; EACCES/EPERM → \"Permission denied\" — both exit(1).\n\n**Output paths**: Relative paths computed via `path.relative(resolvedPath, absPath)`. Excluded files formatted as `- ${rel} (${reason}: ${filter})`. Results written to `.agents-reverse-engineer/GENERATION-PLAN.md`.\n\n**Timing**: Discovery duration measured with `process.hrtime.bigint()` and converted to milliseconds via division by 1_000_000.\n\n**ProgressLog format**: Timestamped header `=== ARE Discover (${ISO})`, status lines, file listings with + (included) and - (excluded) prefixes.",
      "model": "claude-haiku-4-5-20251001",
      "inputTokens": 3,
      "outputTokens": 525,
      "cacheReadTokens": 16147,
      "cacheCreationTokens": 4596,
      "latencyMs": 21774,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/cli/discover.ts",
          "sizeBytes": 6316
        }
      ]
    },
    {
      "timestamp": "2026-02-14T13:46:43.838Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: src/cli/clean.ts\n\n```typescript\n/**\n * `are clean` command - Delete all generated documentation artifacts\n *\n * Removes .sum files, generated AGENTS.md files, generated CLAUDE.md files,\n * and the GENERATION-PLAN.md file. Restores user-authored .local.md files.\n */\n\nimport path from 'node:path';\nimport { access, readFile, rename, unlink } from 'node:fs/promises';\nimport { constants } from 'node:fs';\nimport fg from 'fast-glob';\nimport pc from 'picocolors';\nimport { findProjectRoot } from '../config/loader.js';\nimport { createLogger } from '../output/logger.js';\nimport { GENERATED_MARKER_PREFIX } from '../generation/writers/agents-md.js';\n\n/**\n * Options for the clean command.\n */\nexport interface CleanOptions {\n  /**\n   * Show files that would be deleted without deleting them.\n   * @default false\n   */\n  dryRun: boolean;\n}\n\n/**\n * Execute the `are clean` command.\n *\n * Finds and deletes all generated documentation artifacts:\n * - `*.sum` files\n * - Generated `AGENTS.md` files (marker-checked)\n * - Generated `CLAUDE.md` files (marker-checked)\n * - `.agents-reverse-engineer/GENERATION-PLAN.md`\n *\n * @param targetPath - Project root directory (defaults to current working directory)\n * @param options - Command options\n */\nexport async function cleanCommand(\n  targetPath: string,\n  options: CleanOptions\n): Promise<void> {\n  const resolvedPath = await findProjectRoot(path.resolve(targetPath || process.cwd()));\n\n  const logger = createLogger({ colors: true });\n\n  // Verify target path exists\n  try {\n    await access(resolvedPath, constants.R_OK);\n  } catch (err) {\n    const error = err as NodeJS.ErrnoException;\n    if (error.code === 'ENOENT') {\n      logger.error(`Directory not found: ${resolvedPath}`);\n      process.exit(1);\n    }\n    if (error.code === 'EACCES' || error.code === 'EPERM') {\n      logger.error(`Permission denied: ${resolvedPath}`);\n      process.exit(1);\n    }\n    throw error;\n  }\n\n  // Find all artifacts (*.sum glob catches both .sum and .annex.sum)\n  const [sumFiles, agentsFiles, variantAgentsFiles, localAgentsFiles, claudeFiles, localClaudeFiles] = await Promise.all([\n    fg.glob('**/*.sum', {\n      cwd: resolvedPath,\n      absolute: true,\n      onlyFiles: true,\n      dot: true,\n      ignore: ['**/node_modules/**', '**/.git/**'],\n    }),\n    fg.glob('**/AGENTS.md', {\n      cwd: resolvedPath,\n      absolute: true,\n      onlyFiles: true,\n      dot: true,\n      ignore: ['**/node_modules/**', '**/.git/**'],\n    }),\n    fg.glob('**/AGENTS.*.md', {\n      cwd: resolvedPath,\n      absolute: true,\n      onlyFiles: true,\n      dot: true,\n      ignore: ['**/node_modules/**', '**/.git/**', '**/AGENTS.local.md'],\n    }),\n    fg.glob('**/AGENTS.local.md', {\n      cwd: resolvedPath,\n      absolute: true,\n      onlyFiles: true,\n      dot: true,\n      ignore: ['**/node_modules/**', '**/.git/**'],\n    }),\n    fg.glob('**/CLAUDE.md', {\n      cwd: resolvedPath,\n      absolute: true,\n      onlyFiles: true,\n      dot: true,\n      ignore: ['**/node_modules/**', '**/.git/**'],\n    }),\n    fg.glob('**/CLAUDE.local.md', {\n      cwd: resolvedPath,\n      absolute: true,\n      onlyFiles: true,\n      dot: true,\n      ignore: ['**/node_modules/**', '**/.git/**'],\n    }),\n  ]);\n\n  // Filter AGENTS.md to only those generated by ARE (contain the marker).\n  // User-authored AGENTS.md files (e.g. SDK docs) must not be deleted.\n  const generatedAgentsFiles: string[] = [];\n  const skippedAgentsFiles: string[] = [];\n  for (const file of agentsFiles) {\n    try {\n      const content = await readFile(file, 'utf-8');\n      if (content.includes(GENERATED_MARKER_PREFIX)) {\n        generatedAgentsFiles.push(file);\n      } else {\n        skippedAgentsFiles.push(file);\n      }\n    } catch {\n      // Can't read — skip silently\n    }\n  }\n\n  // Filter variant AGENTS.*.md files by generated marker\n  for (const file of variantAgentsFiles) {\n    try {\n      const content = await readFile(file, 'utf-8');\n      if (content.includes(GENERATED_MARKER_PREFIX)) {\n        generatedAgentsFiles.push(file);\n      } else {\n        skippedAgentsFiles.push(file);\n      }\n    } catch {\n      // Can't read — skip silently\n    }\n  }\n\n  // Filter CLAUDE.md to only those generated by ARE (contain the marker).\n  const generatedClaudeFiles: string[] = [];\n  const skippedClaudeFiles: string[] = [];\n  for (const file of claudeFiles) {\n    try {\n      const content = await readFile(file, 'utf-8');\n      if (content.includes(GENERATED_MARKER_PREFIX)) {\n        generatedClaudeFiles.push(file);\n      } else {\n        skippedClaudeFiles.push(file);\n      }\n    } catch {\n      // Can't read — skip silently\n    }\n  }\n\n  // Check for plan file\n  const singleFiles: string[] = [];\n  const planFile = path.join(resolvedPath, '.agents-reverse-engineer', 'GENERATION-PLAN.md');\n\n  try {\n    await access(planFile, constants.F_OK);\n    singleFiles.push(planFile);\n  } catch {\n    // File doesn't exist - skip\n  }\n\n  const allLocalFiles = [...localAgentsFiles, ...localClaudeFiles];\n  const allFiles = [...sumFiles, ...generatedAgentsFiles, ...generatedClaudeFiles, ...singleFiles];\n\n  if (allFiles.length === 0 && allLocalFiles.length === 0) {\n    logger.info('No generated artifacts found.');\n    return;\n  }\n\n  // Display found files\n  const relativePath = (absPath: string): string =>\n    path.relative(resolvedPath, absPath);\n\n  if (options.dryRun) {\n    logger.info('Files that would be deleted:');\n  }\n\n  for (const file of allFiles) {\n    logger.info(`  ${relativePath(file)}`);\n  }\n\n  const allSkippedFiles = [...skippedAgentsFiles, ...skippedClaudeFiles];\n  if (allSkippedFiles.length > 0) {\n    logger.info('');\n    logger.info('Preserving user-authored files:');\n    for (const file of allSkippedFiles) {\n      logger.info(`  ${relativePath(file)}`);\n    }\n  }\n\n  if (allLocalFiles.length > 0) {\n    logger.info('');\n    logger.info(options.dryRun ? 'Files that would be restored:' : 'Restoring user-defined files:');\n    for (const file of localAgentsFiles) {\n      const target = path.join(path.dirname(file), 'AGENTS.md');\n      logger.info(`  ${relativePath(file)} → ${relativePath(target)}`);\n    }\n    for (const file of localClaudeFiles) {\n      const target = path.join(path.dirname(file), 'CLAUDE.md');\n      logger.info(`  ${relativePath(file)} → ${relativePath(target)}`);\n    }\n  }\n\n  logger.info('');\n  logger.info(\n    `${pc.bold(String(sumFiles.length))} .sum file(s), ` +\n    `${pc.bold(String(generatedAgentsFiles.length))} AGENTS*.md file(s), ` +\n    `${pc.bold(String(generatedClaudeFiles.length))} CLAUDE.md file(s), ` +\n    `${pc.bold(String(allLocalFiles.length))} local file(s) to restore`\n  );\n\n  if (options.dryRun) {\n    logger.info('');\n    logger.info(pc.yellow('Dry run — no files were changed.'));\n    return;\n  }\n\n  // Delete all generated files\n  let deleted = 0;\n  for (const file of allFiles) {\n    try {\n      await unlink(file);\n      deleted++;\n    } catch (err) {\n      logger.error(`Failed to delete ${relativePath(file)}: ${(err as Error).message}`);\n    }\n  }\n\n  // Restore AGENTS.local.md → AGENTS.md and CLAUDE.local.md → CLAUDE.md\n  let restored = 0;\n  for (const localFile of localAgentsFiles) {\n    const agentsPath = path.join(path.dirname(localFile), 'AGENTS.md');\n    try {\n      await rename(localFile, agentsPath);\n      restored++;\n    } catch (err) {\n      logger.error(`Failed to restore ${relativePath(localFile)}: ${(err as Error).message}`);\n    }\n  }\n  for (const localFile of localClaudeFiles) {\n    const claudePath = path.join(path.dirname(localFile), 'CLAUDE.md');\n    try {\n      await rename(localFile, claudePath);\n      restored++;\n    } catch (err) {\n      logger.error(`Failed to restore ${relativePath(localFile)}: ${(err as Error).message}`);\n    }\n  }\n\n  logger.info('');\n  const parts = [`Deleted ${deleted} file(s)`];\n  if (restored > 0) {\n    parts.push(`restored ${restored} local file(s)`);\n  }\n  logger.info(pc.green(`${parts.join(', ')}.`));\n}\n\n```\n\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n- Workflow & convention rules: if this file defines contribution guidelines, PR conventions, commit standards, testing mandates, tool usage requirements, approval workflows, AI agent behavioral instructions, or code conventions (naming standards, formatting rules, import ordering, linting policies), extract them as explicit, actionable rules. Sources: CONTRIBUTING.md, CI configs, PR templates, style guides, linter/formatter configs (.eslintrc, .prettierrc, .editorconfig), tsconfig strictness, README dev sections.\n\n\nTARGET LENGTH (MANDATORY):\n- Source file: 7913 characters\n- Target summary: ~791 characters (10% compression)\n- Maximum: 949 characters\n- Achieve this by: ultra-dense writing, minimal examples, single-sentence descriptions\n\nULTRA-COMPRESSION TECHNIQUES:\n- Use telegraphic style: \"exports foo, bar, baz\" not \"this file exports three functions\"\n- No code examples unless critical to understanding\n- One sentence per export maximum\n- Omit obvious parameter names (\"opts: Options\" not \"options: Options - configuration object\")\n- Use abbreviations: \"params\" not \"parameters\", \"config\" not \"configuration\"\n- Skip section if empty (don't write \"No special concerns\" or \"N/A\")\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nWORKFLOW & CONVENTION RULES (NEVER EXCLUDE):\n- Contribution requirements: branch naming, commit message formats, PR description templates\n- Testing mandates: required test types, coverage thresholds, \"never delete tests\" policies\n- Tool usage rules: required CLI commands, browser automation tools, linter/formatter requirements\n- Approval workflows: review requirements, CI checks that must pass, deployment gates\n- AI agent instructions: explicit directives for automated assistants (coding rules, prohibited actions)\n- Code conventions: naming standards (camelCase, PascalCase, snake_case for what), import ordering rules, file/folder naming patterns, formatting rules (indentation, quotes, semicolons), language idioms, TypeScript strictness policies, preferred patterns vs anti-patterns\nThese define how contributors (human or AI) must work within the project.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion .annex.sum file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**`clean.ts` implements the `are clean` command to delete generated documentation artifacts and restore user-authored files.**\n\n## Exported Symbols\n\n`cleanCommand(targetPath: string, options: CleanOptions): Promise<void>` — executes artifact cleanup, finding and deleting `.sum`, generated `AGENTS.md`, generated `CLAUDE.md`, and `.agents-reverse-engineer/GENERATION-PLAN.md` files; restores `AGENTS.local.md` → `AGENTS.md` and `CLAUDE.local.md` → `CLAUDE.md`.\n\n`CleanOptions` interface with `dryRun: boolean` (default `false`) — controls preview-only mode.\n\n## Deletion & Preservation Logic\n\nDeletes all `*.sum` files (includes `.annex.sum`) via glob `**/*.sum`. Filters `AGENTS.md`, `AGENTS.*.md`, and `CLAUDE.md` by checking `content.includes(GENERATED_MARKER_PREFIX)` — only removes ARE-generated files; preserves user-authored files without the marker. Always deletes `.agents-reverse-engineer/GENERATION-PLAN.md` if present. Silently skips unreadable files.\n\n## File Restoration\n\nRenames `.AGENTS.local.md` → `AGENTS.md` and `.CLAUDE.local.md` → `CLAUDE.md` in their original directories, restoring user-defined versions before generation overwrites them.\n\n## Glob Patterns & Ignores\n\nAll globs use `{ cwd: resolvedPath, absolute: true, onlyFiles: true, dot: true, ignore: ['**/node_modules/**', '**/.git/**'] }` to exclude dependencies and VCS.\n\n## Error Handling\n\nVerifies directory accessibility with error codes: `ENOENT` (not found), `EACCES`/`EPERM` (permission denied) exit with code 1. Suppresses individual file deletion/restoration errors via try-catch logging without halting. Read failures during marker filtering skip silently.\n\n## Output & Dry Run\n\nLogs relative paths for all operations. `dryRun: true` prints files without deletion/restoration, showing preview with `pc.yellow('Dry run — no files were changed.')`. Final summary: `Deleted X file(s), restored Y local file(s).`\n\n## Dependencies\n\n`findProjectRoot()` resolves target path; `createLogger({ colors: true })` produces colored output via `picocolors` (`pc`); `fg.glob()` from `fast-glob` finds artifacts; `GENERATED_MARKER_PREFIX` from `agents-md.js` identifies ARE-generated files.",
      "model": "claude-haiku-4-5-20251001",
      "inputTokens": 3,
      "outputTokens": 618,
      "cacheReadTokens": 16147,
      "cacheCreationTokens": 5337,
      "latencyMs": 23551,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/cli/clean.ts",
          "sizeBytes": 7929
        }
      ]
    },
    {
      "timestamp": "2026-02-14T13:46:48.122Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: src/cli/init.ts\n\n```typescript\n/**\n * `are init` command - Create default configuration\n *\n * Creates the `.agents-reverse/config.yaml` file with documented defaults.\n * Warns if configuration already exists.\n */\n\nimport { existsSync } from 'node:fs';\nimport { readFile, writeFile, mkdir } from 'node:fs/promises';\nimport path from 'node:path';\nimport { parse, modify, applyEdits, type ParseError } from 'jsonc-parser';\nimport { configExists, writeDefaultConfig, CONFIG_DIR, CONFIG_FILE } from '../config/loader.js';\nimport { createLogger } from '../output/logger.js';\n\nconst GITIGNORE_SECTION = '# agents-reverse-engineer';\nconst SUM_PATTERN = '*.sum';\n\n/**\n * Ensure `*.sum` is listed in `.gitignore`.\n *\n * If a `# agents-reverse-engineer` section already exists, appends within it.\n * Otherwise creates the section. Idempotent — skips if already present.\n *\n * @returns true if the file was modified\n */\nasync function ensureGitignoreEntry(root: string): Promise<boolean> {\n  const gitignorePath = path.join(root, '.gitignore');\n\n  let content = '';\n  try {\n    content = await readFile(gitignorePath, 'utf-8');\n  } catch {\n    // File doesn't exist — will create\n  }\n\n  // Already has the entry\n  if (content.split('\\n').some((line) => line.trim() === SUM_PATTERN)) {\n    return false;\n  }\n\n  const sectionIdx = content.indexOf(GITIGNORE_SECTION);\n  if (sectionIdx !== -1) {\n    // Insert after the section header line\n    const endOfLine = content.indexOf('\\n', sectionIdx);\n    const insertAt = endOfLine === -1 ? content.length : endOfLine;\n    content = content.slice(0, insertAt) + '\\n' + SUM_PATTERN + content.slice(insertAt);\n  } else {\n    // Append new section\n    const separator = content.length > 0 && !content.endsWith('\\n') ? '\\n\\n' : content.length > 0 ? '\\n' : '';\n    content += `${separator}${GITIGNORE_SECTION}\\n${SUM_PATTERN}\\n`;\n  }\n\n  await writeFile(gitignorePath, content, 'utf-8');\n  return true;\n}\n\n/**\n * Ensure `.vscode/settings.json` has `files.exclude[\"**\\/*.sum\"] = true`.\n *\n * Creates the file and directory if needed. Handles JSONC (comments,\n * trailing commas) used by VS Code — edits are applied surgically via\n * `jsonc-parser` so existing comments and formatting are preserved.\n * If an existing file cannot be parsed, the file is left untouched.\n * Idempotent.\n *\n * @returns true if the file was modified\n */\nasync function ensureVscodeExclude(root: string): Promise<boolean> {\n  const vscodePath = path.join(root, '.vscode');\n  const settingsPath = path.join(vscodePath, 'settings.json');\n\n  let content = '{}';\n  if (existsSync(settingsPath)) {\n    content = await readFile(settingsPath, 'utf-8');\n  }\n\n  // Parse JSONC — bail out on truly broken files\n  const errors: ParseError[] = [];\n  const settings = (parse(content, errors) ?? {}) as Record<string, unknown>;\n  if (errors.length > 0 && Object.keys(settings).length === 0) {\n    // Truly unparseable — leave untouched to avoid data loss\n    return false;\n  }\n\n  const filesExclude = (settings['files.exclude'] ?? {}) as Record<string, boolean>;\n  if (filesExclude['**/*.sum'] === true) {\n    return false;\n  }\n\n  // Targeted edit preserving existing comments and formatting\n  const edits = modify(content, ['files.exclude', '**/*.sum'], true, {\n    formattingOptions: { tabSize: 1, insertSpaces: false },\n  });\n  const updated = applyEdits(content, edits);\n\n  await mkdir(vscodePath, { recursive: true });\n  await writeFile(settingsPath, updated, 'utf-8');\n  return true;\n}\n\n/**\n * Execute the `are init` command.\n *\n * Creates a default configuration file at `.agents-reverse/config.yaml`.\n * If the file already exists, logs a warning and returns without modification.\n *\n * @param root - Root directory where config will be created\n *\n * @example\n * ```typescript\n * await initCommand('.');\n * // Creates .agents-reverse/config.yaml in current directory\n * ```\n */\nexport async function initCommand(root: string, options?: { force?: boolean }): Promise<void> {\n  const resolvedRoot = path.resolve(root);\n  const configPath = path.join(resolvedRoot, CONFIG_DIR, CONFIG_FILE);\n  const force = options?.force ?? false;\n\n  const logger = createLogger({ colors: true });\n\n  try {\n    // Check if config already exists\n    if (!force && await configExists(resolvedRoot)) {\n      logger.warn(`Config already exists at ${configPath}`);\n      logger.info('Edit the file to customize exclusions and options.');\n    } else {\n      // Create default config\n      await writeDefaultConfig(resolvedRoot);\n\n      logger.info(`Created configuration at ${configPath}`);\n\n      // Configure project files to hide generated *.sum artifacts\n      try {\n        if (await ensureGitignoreEntry(resolvedRoot)) {\n          logger.info('Added *.sum to .gitignore');\n        }\n      } catch {\n        logger.warn('Could not update .gitignore');\n      }\n\n      try {\n        if (await ensureVscodeExclude(resolvedRoot)) {\n          logger.info('Added *.sum to .vscode/settings.json files.exclude');\n        }\n      } catch {\n        logger.warn('Could not update .vscode/settings.json');\n      }\n\n      logger.info('');\n      logger.info('Edit the file to customize:');\n      logger.info('  - exclude.patterns: Custom glob patterns to exclude');\n      logger.info('  - ai.concurrency: Parallel AI calls (1-20, default: auto)');\n      logger.info('  - ai.timeoutMs: Subprocess timeout (default: 300,000ms = 5 min)');\n      logger.info('  - ai.backend: AI backend (claude/codex/gemini/opencode/auto)');\n      logger.info('');\n      logger.info('See README.md for full configuration reference.');\n    }\n  } catch (err) {\n    const error = err as NodeJS.ErrnoException;\n\n    // Permission error\n    if (error.code === 'EACCES' || error.code === 'EPERM') {\n      logger.error(`Permission denied: Cannot create ${configPath}`);\n      logger.info('Check that you have write permissions to this directory.');\n      process.exit(1);\n    }\n\n    // Other error\n    logger.error(`Failed to create configuration: ${error.message}`);\n    process.exit(1);\n  }\n}\n\n```\n\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n- Workflow & convention rules: if this file defines contribution guidelines, PR conventions, commit standards, testing mandates, tool usage requirements, approval workflows, AI agent behavioral instructions, or code conventions (naming standards, formatting rules, import ordering, linting policies), extract them as explicit, actionable rules. Sources: CONTRIBUTING.md, CI configs, PR templates, style guides, linter/formatter configs (.eslintrc, .prettierrc, .editorconfig), tsconfig strictness, README dev sections.\n\n\nTARGET LENGTH (MANDATORY):\n- Source file: 5996 characters\n- Target summary: ~600 characters (10% compression)\n- Maximum: 720 characters\n- Achieve this by: ultra-dense writing, minimal examples, single-sentence descriptions\n\nULTRA-COMPRESSION TECHNIQUES:\n- Use telegraphic style: \"exports foo, bar, baz\" not \"this file exports three functions\"\n- No code examples unless critical to understanding\n- One sentence per export maximum\n- Omit obvious parameter names (\"opts: Options\" not \"options: Options - configuration object\")\n- Use abbreviations: \"params\" not \"parameters\", \"config\" not \"configuration\"\n- Skip section if empty (don't write \"No special concerns\" or \"N/A\")\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nWORKFLOW & CONVENTION RULES (NEVER EXCLUDE):\n- Contribution requirements: branch naming, commit message formats, PR description templates\n- Testing mandates: required test types, coverage thresholds, \"never delete tests\" policies\n- Tool usage rules: required CLI commands, browser automation tools, linter/formatter requirements\n- Approval workflows: review requirements, CI checks that must pass, deployment gates\n- AI agent instructions: explicit directives for automated assistants (coding rules, prohibited actions)\n- Code conventions: naming standards (camelCase, PascalCase, snake_case for what), import ordering rules, file/folder naming patterns, formatting rules (indentation, quotes, semicolons), language idioms, TypeScript strictness policies, preferred patterns vs anti-patterns\nThese define how contributors (human or AI) must work within the project.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion .annex.sum file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**init.ts initializes ARE project configuration by creating `.agents-reverse/config.yaml`, updating `.gitignore` and VS Code settings to exclude `*.sum` artifacts.**\n\n## Exports\n\n`initCommand(root: string, options?: { force?: boolean }): Promise<void>` — main CLI entry point; creates default config at `[root]/.agents-reverse/config.yaml`, optionally updating `.gitignore` and `.vscode/settings.json` to hide generated files; warns if config exists unless `force: true`.\n\n## Key Functions\n\n`ensureGitignoreEntry(root: string): Promise<boolean>` — appends `*.sum` to `.gitignore` under `# agents-reverse-engineer` section (creates section if missing); idempotent, returns true if modified.\n\n`ensureVscodeExclude(root: string): Promise<boolean>` — sets `files.exclude[\"**/*.sum\"] = true` in `.vscode/settings.json` using `jsonc-parser` to preserve comments/formatting; creates `.vscode` dir if needed; returns false on parse errors (data-loss safety).\n\n## Constants & Patterns\n\n`GITIGNORE_SECTION = '# agents-reverse-engineer'` — marker for ARE-managed gitignore block.\n\n`SUM_PATTERN = '*.sum'` — glob pattern excluded from version control and IDE file tree.\n\n## Dependencies\n\n`jsonc-parser` (`parse`, `modify`, `applyEdits`) — JSONC parsing/editing for `.vscode/settings.json`; preserves formatting and comments via surgical edits.\n\n`config/loader.js` (`configExists`, `writeDefaultConfig`, `CONFIG_DIR`, `CONFIG_FILE`) — config file I/O and paths.\n\n`output/logger.js` (`createLogger`) — status/warning/error logging to console.\n\n## Configuration Options\n\nConfig file edits documented in info logs: `exclude.patterns` (glob exclusions), `ai.concurrency` (1–20, default auto), `ai.timeoutMs` (subprocess timeout, default 300000ms), `ai.backend` (claude/codex/gemini/opencode/auto).\n\n## Error Handling\n\nEACCES/EPERM — permission denied, exits code 1 with helpful message. Parse errors in `.vscode/settings.json` — skips modification to prevent data loss. `.gitignore` missing or unreadable — creates new file.",
      "model": "claude-haiku-4-5-20251001",
      "inputTokens": 3,
      "outputTokens": 564,
      "cacheReadTokens": 16147,
      "cacheCreationTokens": 4555,
      "latencyMs": 22857,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/cli/init.ts",
          "sizeBytes": 6006
        }
      ]
    },
    {
      "timestamp": "2026-02-14T13:46:39.876Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: src/ai/subprocess.ts\n\n```typescript\n/**\n * Low-level subprocess wrapper for AI CLI invocations.\n *\n * This is the ONLY place in the codebase that spawns AI CLI processes.\n * Centralizes timeout enforcement, stdin piping, zombie prevention,\n * SIGKILL escalation, and exit code extraction.\n *\n * @module\n */\n\nimport { execFile } from 'node:child_process';\nimport type { ExecFileException } from 'node:child_process';\nimport type { SubprocessResult } from './types.js';\n\n/** Grace period after SIGTERM before escalating to SIGKILL (ms) */\nconst SIGKILL_GRACE_MS = 5_000;\n\n/**\n * Track active subprocesses for debugging concurrency.\n * Maps PID to spawn timestamp and command.\n */\nconst activeSubprocesses = new Map<number, { command: string; spawnedAt: number }>();\n\n/**\n * Get current count of active subprocesses.\n */\nexport function getActiveSubprocessCount(): number {\n  return activeSubprocesses.size;\n}\n\n/**\n * Get details of all active subprocesses.\n */\nexport function getActiveSubprocesses(): Array<{ pid: number; command: string; spawnedAt: number; runningMs: number }> {\n  const now = Date.now();\n  return Array.from(activeSubprocesses.entries()).map(([pid, info]) => ({\n    pid,\n    command: info.command,\n    spawnedAt: info.spawnedAt,\n    runningMs: now - info.spawnedAt,\n  }));\n}\n\n/**\n * Options for subprocess execution.\n */\nexport interface SubprocessOptions {\n  /** Maximum time in milliseconds before the process is killed */\n  timeoutMs: number;\n  /** Optional stdin input to pipe to the process */\n  input?: string;\n  /** Optional environment variable overrides for the child process */\n  env?: NodeJS.ProcessEnv;\n  /**\n   * Callback fired synchronously when the child process is spawned.\n   * Use this for trace events that need the actual spawn time.\n   */\n  onSpawn?: (pid: number | undefined) => void;\n}\n\n/**\n * Spawn a CLI subprocess with timeout enforcement and stdin piping.\n *\n * Always resolves -- never rejects. Errors are captured in the returned\n * {@link SubprocessResult} fields (`exitCode`, `timedOut`, `stderr`) so\n * that callers can decide how to handle failures.\n *\n * When the subprocess exceeds its timeout, `execFile` sends SIGTERM.\n * If the process doesn't exit within {@link SIGKILL_GRACE_MS} after\n * SIGTERM, we escalate to SIGKILL to prevent hung processes from\n * lingering indefinitely.\n *\n * @param command - The CLI executable to run (e.g., \"claude\", \"gemini\")\n * @param args - Argument array passed to the executable\n * @param options - Timeout, optional stdin input, and spawn callback\n * @returns Resolved result with stdout, stderr, exit code, timing, and timeout flag\n *\n * @example\n * ```typescript\n * import { runSubprocess } from './subprocess.js';\n *\n * const result = await runSubprocess('claude', ['-p', '--output-format', 'json'], {\n *   timeoutMs: 120_000,\n *   input: 'Summarize this codebase',\n *   onSpawn: (pid) => console.log(`Spawned PID ${pid}`),\n * });\n *\n * if (result.timedOut) {\n *   console.error('CLI timed out after 120s');\n * } else if (result.exitCode !== 0) {\n *   console.error('CLI failed:', result.stderr);\n * } else {\n *   const response = JSON.parse(result.stdout);\n * }\n * ```\n */\nexport function runSubprocess(\n  command: string,\n  args: string[],\n  options: SubprocessOptions,\n): Promise<SubprocessResult> {\n  return new Promise((resolve) => {\n    const startTime = Date.now();\n    let sigkillTimer: ReturnType<typeof setTimeout> | undefined;\n\n    // const activeCount = activeSubprocesses.size;\n    // console.error(`[subprocess] Currently active: ${activeCount} subprocess(es)`);\n    // console.error(`[subprocess] Spawning: ${command} ${args.join(' ')}`);\n\n    const child = execFile(\n      command,\n      args,\n      {\n        timeout: options.timeoutMs,\n        killSignal: 'SIGTERM',\n        maxBuffer: 10 * 1024 * 1024, // 10MB for large AI responses\n        encoding: 'utf-8',\n        env: {\n          ...process.env,\n          ...(options.env ?? {}),\n        },\n      },\n      (error: ExecFileException | null, stdout: string, stderr: string) => {\n        const durationMs = Date.now() - startTime;\n        // console.error(`[subprocess:${child.pid}] Callback fired after ${durationMs}ms`);\n\n        // Clear SIGKILL escalation timer -- process has exited\n        if (sigkillTimer !== undefined) {\n          // console.error(`[subprocess:${child.pid}] Clearing SIGKILL timer`);\n          clearTimeout(sigkillTimer);\n        }\n\n        // Ensure the process is fully terminated (no-op if already dead)\n        // This handles edge cases where child processes might still be running\n        // console.error(`[subprocess:${child.pid}] Attempting explicit SIGKILL cleanup`);\n        try {\n          if (child.pid !== undefined) {\n            // Kill the entire process tree by sending signal to process group\n            // Use negative PID to target the process group\n            try {\n              process.kill(-child.pid, 'SIGKILL');\n              // console.error(`[subprocess:${child.pid}] Sent SIGKILL to process group -${child.pid}`);\n            } catch (pgError) {\n              // Process group kill failed, try single process\n              process.kill(child.pid, 'SIGKILL');\n              // console.error(`[subprocess:${child.pid}] Sent SIGKILL to single process (pg kill failed)`);\n            }\n          }\n        } catch (killError) {\n          // Process is already dead or doesn't exist -- expected in most cases\n          // console.error(`[subprocess:${child.pid}] SIGKILL failed (process already dead): ${killError instanceof Error ? killError.message : String(killError)}`);\n        }\n\n        // Detect timeout: execFile sets `killed = true` when the process\n        // is terminated due to exceeding the timeout option.\n        const timedOut = error !== null && 'killed' in error && error.killed === true;\n\n        // Extract exit code from the error or child process.\n        // execFile puts the exit code in error.code when the process exits\n        // with a non-zero code, but error.code can also be a string like\n        // 'ERR_CHILD_PROCESS_STDIO_MAXBUFFER'. Fall back to child.exitCode,\n        // then default to 1 for unknown failures and 0 for no error.\n        let exitCode: number;\n        if (error === null) {\n          exitCode = 0;\n        } else if (typeof error.code === 'number') {\n          exitCode = error.code;\n        } else if (child.exitCode !== null) {\n          exitCode = child.exitCode;\n        } else {\n          exitCode = 1;\n        }\n\n        // console.error(`[subprocess:${child.pid}] Exited: code=${exitCode}, timedOut=${timedOut}, signal=${error !== null && 'signal' in error ? error.signal : null}`);\n\n        // Remove from active tracking\n        if (child.pid !== undefined) {\n          activeSubprocesses.delete(child.pid);\n          // console.error(`[subprocess:${child.pid}] Removed from active list (remaining: ${activeSubprocesses.size})`);\n        }\n\n        resolve({\n          stdout: stdout ?? '',\n          stderr: stderr ?? '',\n          exitCode,\n          signal: (error !== null && 'signal' in error ? error.signal as string : null) ?? null,\n          durationMs,\n          timedOut,\n          childPid: child.pid,\n        });\n      },\n    );\n\n    // console.error(`[subprocess:${child.pid}] Spawned with PID ${child.pid}`);\n\n    // Track this subprocess\n    if (child.pid !== undefined) {\n      activeSubprocesses.set(child.pid, {\n        command: `${command} ${args.join(' ')}`,\n        spawnedAt: startTime,\n      });\n      // console.error(`[subprocess:${child.pid}] Registered in active list (total: ${activeSubprocesses.size})`);\n    }\n\n    // Notify caller of spawn (for trace events at actual spawn time)\n    options.onSpawn?.(child.pid);\n    // console.error(`[subprocess:${child.pid}] onSpawn callback invoked`);\n\n    // SIGKILL escalation: if the process doesn't exit within\n    // timeout + grace period, force-kill it. This handles cases where\n    // SIGTERM is caught/ignored by the child or its process tree.\n    if (child.pid !== undefined) {\n      const killDelay = options.timeoutMs + SIGKILL_GRACE_MS;\n      // console.error(`[subprocess:${child.pid}] Setting SIGKILL timer for ${killDelay}ms (timeout=${options.timeoutMs}ms + grace=${SIGKILL_GRACE_MS}ms)`);\n\n      sigkillTimer = setTimeout(() => {\n        // console.error(`[subprocess:${child.pid}] SIGKILL timer fired - process exceeded timeout+grace`);\n        try {\n          child.kill('SIGKILL');\n          // console.error(`[subprocess:${child.pid}] Sent escalation SIGKILL`);\n        } catch (killError) {\n          // Process may already be dead -- ignore\n          // console.error(`[subprocess:${child.pid}] Escalation SIGKILL failed: ${killError instanceof Error ? killError.message : String(killError)}`);\n        }\n      }, killDelay);\n\n      // Don't let this timer keep the event loop alive\n      sigkillTimer.unref();\n      // console.error(`[subprocess:${child.pid}] SIGKILL timer unref'd`);\n    }\n\n    // Write prompt to stdin if provided, then close the stream.\n    // IMPORTANT: Always call .end() -- the child process blocks waiting\n    // for EOF on stdin otherwise (see RESEARCH.md Pitfall 1).\n    if (options.input !== undefined && child.stdin !== null) {\n      const inputSize = Buffer.byteLength(options.input, 'utf-8');\n      // console.error(`[subprocess:${child.pid}] Writing ${inputSize} bytes to stdin`);\n      child.stdin.write(options.input);\n      child.stdin.end();\n      // console.error(`[subprocess:${child.pid}] Stdin closed`);\n    } else {\n      // console.error(`[subprocess:${child.pid}] No stdin input provided`);\n    }\n  });\n}\n\n```\n\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n- Workflow & convention rules: if this file defines contribution guidelines, PR conventions, commit standards, testing mandates, tool usage requirements, approval workflows, AI agent behavioral instructions, or code conventions (naming standards, formatting rules, import ordering, linting policies), extract them as explicit, actionable rules. Sources: CONTRIBUTING.md, CI configs, PR templates, style guides, linter/formatter configs (.eslintrc, .prettierrc, .editorconfig), tsconfig strictness, README dev sections.\n\n\nTARGET LENGTH (MANDATORY):\n- Source file: 9588 characters\n- Target summary: ~959 characters (10% compression)\n- Maximum: 1151 characters\n- Achieve this by: ultra-dense writing, minimal examples, single-sentence descriptions\n\nULTRA-COMPRESSION TECHNIQUES:\n- Use telegraphic style: \"exports foo, bar, baz\" not \"this file exports three functions\"\n- No code examples unless critical to understanding\n- One sentence per export maximum\n- Omit obvious parameter names (\"opts: Options\" not \"options: Options - configuration object\")\n- Use abbreviations: \"params\" not \"parameters\", \"config\" not \"configuration\"\n- Skip section if empty (don't write \"No special concerns\" or \"N/A\")\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nWORKFLOW & CONVENTION RULES (NEVER EXCLUDE):\n- Contribution requirements: branch naming, commit message formats, PR description templates\n- Testing mandates: required test types, coverage thresholds, \"never delete tests\" policies\n- Tool usage rules: required CLI commands, browser automation tools, linter/formatter requirements\n- Approval workflows: review requirements, CI checks that must pass, deployment gates\n- AI agent instructions: explicit directives for automated assistants (coding rules, prohibited actions)\n- Code conventions: naming standards (camelCase, PascalCase, snake_case for what), import ordering rules, file/folder naming patterns, formatting rules (indentation, quotes, semicolons), language idioms, TypeScript strictness policies, preferred patterns vs anti-patterns\nThese define how contributors (human or AI) must work within the project.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion .annex.sum file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**src/ai/subprocess.ts centralizes spawning of AI CLI processes with guaranteed timeout enforcement, stdin piping, zombie prevention, and exit code extraction.**\n\n## Exports\n\n`runSubprocess(command: string, args: string[], options: SubprocessOptions): Promise<SubprocessResult>` — spawns CLI subprocess with timeout-enforced execution, stdin piping, and always-resolving result; escalates SIGTERM to SIGKILL after grace period.\n\n`getActiveSubprocessCount(): number` — returns count of actively running subprocesses.\n\n`getActiveSubprocesses(): Array<{ pid: number; command: string; spawnedAt: number; runningMs: number }>` — returns details of all active subprocesses with PID, command string, spawn timestamp, and elapsed duration.\n\n## Interface & Types\n\n`SubprocessOptions` — configuration with `timeoutMs: number` (kill threshold), `input?: string` (stdin), `env?: NodeJS.ProcessEnv` (environment overrides), `onSpawn?: (pid: number | undefined) => void` (synchronous spawn callback for trace events).\n\n## Behavioral Contracts\n\n**Timeout escalation**: SIGTERM sent by `execFile` when `timeoutMs` exceeded; if process hasn't exited within `SIGKILL_GRACE_MS` (5000ms), escalates to SIGKILL via `child.kill('SIGKILL')` and negative PID process group kill `-child.pid`.\n\n**Exit code extraction**: prioritizes `error.code` (number) from ExecFileException, falls back to `child.exitCode`, defaults to 1 for unknown failures or 0 if no error.\n\n**Timeout detection**: checks `error.killed === true` flag set by execFile.\n\n**Signal handling**: attempts process group kill first with `process.kill(-child.pid, 'SIGKILL')`, falls back to single process, silently catches ENOENT when process already dead.\n\n**Stdin closure**: unconditionally calls `child.stdin.end()` to signal EOF (blocking pitfall documented in RESEARCH.md).\n\n**Buffer**: 10MB maxBuffer for large AI responses.\n\n## State Tracking\n\n`activeSubprocesses: Map<number, { command: string; spawnedAt: number }>` — tracks running processes by PID; used for concurrency debugging; entries deleted on process exit.",
      "model": "claude-haiku-4-5-20251001",
      "inputTokens": 3,
      "outputTokens": 546,
      "cacheReadTokens": 16147,
      "cacheCreationTokens": 5537,
      "latencyMs": 31403,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/ai/subprocess.ts",
          "sizeBytes": 9588
        }
      ]
    },
    {
      "timestamp": "2026-02-14T13:46:48.088Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: src/cli/index.ts\n\n```typescript\n#!/usr/bin/env node\n/**\n * CLI entry point for agents-reverse\n *\n * Commands:\n *   init              Create default configuration\n *   discover [path]   Discover files to analyze\n *   generate [path]   Generate documentation plan\n *   update [path]     Update docs incrementally\n *   specify [path]    Generate project specification from AGENTS.md docs\n *   clean [path]      Delete all generated artifacts\n */\n\nimport { initCommand } from './init.js';\nimport { discoverCommand } from './discover.js';\nimport { generateCommand, type GenerateOptions } from './generate.js';\nimport { updateCommand, type UpdateOptions } from './update.js';\nimport { cleanCommand, type CleanOptions } from './clean.js';\nimport { specifyCommand, type SpecifyOptions } from './specify.js';\nimport { rebuildCommand, type RebuildOptions } from './rebuild.js';\n\nimport { runInstaller, parseInstallerArgs } from '../installer/index.js';\nimport { getVersion } from '../version.js';\n\nconst VERSION = getVersion();\n\nconst USAGE = `\nagents-reverse-engineer - AI-friendly codebase documentation\n\nCommands:\n  install           Install commands and hooks to AI assistant\n  uninstall         Remove installed commands and hooks\n  init              Create default configuration\n  discover [path]   Discover files to analyze (default: current directory)\n  generate [path]   Generate documentation plan (default: current directory)\n  update [path]     Update docs incrementally (default: current directory)\n  specify [path]    Generate project specification from AGENTS.md docs\n  rebuild [path]    Reconstruct project from specification\n  clean [path]      Delete all generated artifacts (.sum, AGENTS.md, etc.)\n\nInstall/Uninstall Options:\n  --runtime <name>  Runtime to target (claude, codex, opencode, gemini, all)\n  -g, --global      Target global config directory\n  -l, --local       Target current project directory\n  --force           Overwrite existing files (init, install, specify, generate)\n\nGeneral Options:\n  --debug           Show AI prompts and backend details\n  --trace           Enable concurrency tracing (.agents-reverse-engineer/traces/)\n  --dry-run         Show plan without writing files (generate, update, specify, rebuild)\n  --output <path>   Output path (specify: spec file, rebuild: output directory)\n  --multi-file      Split specification into multiple files (specify only)\n  --eval            Eval mode: namespace output by backend.model for comparison\n  --model <name>    AI model to use (e.g., sonnet, opus, haiku)\n  --backend <name>  AI backend to use (claude, codex, gemini, opencode, auto)\n  --concurrency <n> Number of concurrent AI calls (default: auto)\n  --show-excluded   Show excluded files during discovery\n  --fail-fast       Stop on first file analysis failure\n  --uncommitted     Include uncommitted changes (update only)\n  --help, -h        Show this help\n  --version, -V     Show version number\n\nExamples:\n  are install\n  are install --runtime claude -g\n  are install --runtime codex -g\n  are uninstall\n  are uninstall --runtime claude -g\n  are init\n  are discover\n  are generate --dry-run\n  are generate --concurrency 3\n  are generate ./my-project --concurrency 3\n  are update\n  are update --uncommitted\n  are specify --dry-run\n  are specify --output ./docs/spec.md --force\n  are rebuild --dry-run\n  are rebuild --output ./out --force\n`;\n\n/**\n * Parse command-line arguments.\n *\n * Extracts the command, positional arguments, and flags.\n * Handles global flags (--help, -h) that may appear before the command.\n */\nfunction parseArgs(args: string[]): {\n  command: string | undefined;\n  positional: string[];\n  flags: Set<string>;\n  values: Map<string, string>;\n} {\n  let command: string | undefined;\n  const positional: string[] = [];\n  const flags = new Set<string>();\n  const values = new Map<string, string>();\n\n  for (let i = 0; i < args.length; i++) {\n    const arg = args[i];\n    if (arg.startsWith('--')) {\n      const flagName = arg.slice(2);\n      // Check if next arg is a value (not starting with -)\n      if (i + 1 < args.length && !args[i + 1].startsWith('-')) {\n        values.set(flagName, args[i + 1]);\n        i++; // Skip the value\n      } else {\n        flags.add(flagName);\n      }\n    } else if (arg.startsWith('-')) {\n      // Handle short flags (e.g., -h, -g, -l)\n      for (const char of arg.slice(1)) {\n        switch (char) {\n          case 'h':\n            flags.add('help');\n            break;\n          case 'g':\n            flags.add('global');\n            break;\n          case 'l':\n            flags.add('local');\n            break;\n          case 'V':\n            flags.add('version');\n            break;\n          default:\n            // Unknown short flag - ignore\n            break;\n        }\n      }\n    } else if (!command) {\n      // First non-flag argument is the command\n      command = arg;\n    } else {\n      // Subsequent non-flag arguments are positional\n      positional.push(arg);\n    }\n  }\n\n  return { command, positional, flags, values };\n}\n\n/**\n * Show version and exit.\n */\nfunction showVersion(): void {\n  console.log(`agents-reverse-engineer v${VERSION}`);\n  process.exit(0);\n}\n\n/**\n * Display version banner.\n */\nfunction showVersionBanner(): void {\n  console.log(`agents-reverse-engineer v${VERSION}\\n`);\n}\n\n/**\n * Show usage information and exit.\n */\nfunction showHelp(): void {\n  console.log(USAGE);\n  process.exit(0);\n}\n\n/**\n * Show error for unknown command and exit.\n */\nfunction showUnknownCommand(command: string): void {\n  console.error(`Unknown command: ${command}`);\n  console.error(`Run 'are --help' for usage information.`);\n  process.exit(1);\n}\n\n/**\n * Check if command-line has installer-related flags.\n *\n * Used to detect direct installer invocation without 'install' command.\n */\nfunction hasInstallerFlags(flags: Set<string>, values: Map<string, string>): boolean {\n  return (\n    flags.has('global') ||\n    flags.has('local') ||\n    flags.has('force') ||\n    values.has('runtime')\n  );\n}\n\n/**\n * Main CLI entry point.\n */\nasync function main(): Promise<void> {\n  const args = process.argv.slice(2);\n  const { command, positional, flags, values } = parseArgs(args);\n\n  // Handle version flag\n  if (flags.has('version')) {\n    showVersion();\n  }\n\n  // Handle help flag anywhere (but not if --help is for install command)\n  if (flags.has('help') && !command && !hasInstallerFlags(flags, values)) {\n    showHelp();\n  }\n\n  // No command and no args - launch interactive installer\n  if (args.length === 0) {\n    await runInstaller({\n      global: false,\n      local: false,\n      uninstall: false,\n      force: false,\n      help: false,\n      quiet: false,\n    });\n    return;\n  }\n\n  // Direct installer invocation without 'install' command\n  // Supports: npx agents-reverse-engineer --runtime claude -g\n  if (!command && hasInstallerFlags(flags, values)) {\n    const installerArgs = parseInstallerArgs(args);\n    await runInstaller(installerArgs);\n    return;\n  }\n\n  // Show version banner\n  showVersionBanner();\n\n  // Route to command handlers\n  switch (command) {\n    case 'install': {\n      // Re-parse args for installer-specific flags\n      const installerArgs = parseInstallerArgs(args);\n      await runInstaller(installerArgs);\n      break;\n    }\n\n    case 'uninstall': {\n      // Re-parse args and force uninstall mode\n      const installerArgs = parseInstallerArgs(args);\n      installerArgs.uninstall = true;\n      await runInstaller(installerArgs);\n      break;\n    }\n\n    case 'init': {\n      await initCommand(positional[0] || '.', { force: flags.has('force') });\n      break;\n    }\n\n    case 'clean': {\n      const cleanOpts: CleanOptions = {\n        dryRun: flags.has('dry-run'),\n      };\n      await cleanCommand(positional[0] || '.', cleanOpts);\n      break;\n    }\n\n    case 'discover': {\n      await discoverCommand(positional[0] || '.', {\n        showExcluded: flags.has('show-excluded'),\n        debug: flags.has('debug'),\n      });\n      break;\n    }\n\n    case 'generate': {\n      const options: GenerateOptions = {\n        force: flags.has('force'),\n        dryRun: flags.has('dry-run'),\n        concurrency: values.has('concurrency') ? parseInt(values.get('concurrency')!, 10) : undefined,\n        failFast: flags.has('fail-fast'),\n        debug: flags.has('debug'),\n        trace: flags.has('trace'),\n        model: values.get('model'),\n        backend: values.get('backend'),\n        eval: flags.has('eval'),\n      };\n      await generateCommand(positional[0] || '.', options);\n      break;\n    }\n\n    case 'update': {\n      const options: UpdateOptions = {\n        uncommitted: flags.has('uncommitted'),\n        dryRun: flags.has('dry-run'),\n        concurrency: values.has('concurrency') ? parseInt(values.get('concurrency')!, 10) : undefined,\n        failFast: flags.has('fail-fast'),\n        debug: flags.has('debug'),\n        trace: flags.has('trace'),\n        model: values.get('model'),\n        backend: values.get('backend'),\n        eval: flags.has('eval'),\n      };\n      await updateCommand(positional[0] || '.', options);\n      break;\n    }\n\n    case 'specify': {\n      const specifyOpts: SpecifyOptions = {\n        output: values.get('output'),\n        force: flags.has('force'),\n        dryRun: flags.has('dry-run'),\n        multiFile: flags.has('multi-file'),\n        debug: flags.has('debug'),\n        trace: flags.has('trace'),\n        model: values.get('model'),\n        backend: values.get('backend'),\n      };\n      await specifyCommand(positional[0] || '.', specifyOpts);\n      break;\n    }\n\n    case 'rebuild': {\n      const rebuildOpts: RebuildOptions = {\n        output: values.get('output'),\n        force: flags.has('force'),\n        dryRun: flags.has('dry-run'),\n        concurrency: values.has('concurrency')\n          ? parseInt(values.get('concurrency')!, 10) : undefined,\n        failFast: flags.has('fail-fast'),\n        debug: flags.has('debug'),\n        trace: flags.has('trace'),\n        model: values.get('model'),\n        backend: values.get('backend'),\n      };\n      await rebuildCommand(positional[0] || '.', rebuildOpts);\n      break;\n    }\n\n    default:\n      if (command) {\n        showUnknownCommand(command);\n      }\n      showHelp();\n  }\n}\n\n// Run main and handle any uncaught errors\nmain().catch((err: Error) => {\n  console.error(`Error: ${err.message}`);\n  process.exit(1);\n});\n\n```\n\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n- Workflow & convention rules: if this file defines contribution guidelines, PR conventions, commit standards, testing mandates, tool usage requirements, approval workflows, AI agent behavioral instructions, or code conventions (naming standards, formatting rules, import ordering, linting policies), extract them as explicit, actionable rules. Sources: CONTRIBUTING.md, CI configs, PR templates, style guides, linter/formatter configs (.eslintrc, .prettierrc, .editorconfig), tsconfig strictness, README dev sections.\n\n\nTARGET LENGTH (MANDATORY):\n- Source file: 10327 characters\n- Target summary: ~1033 characters (10% compression)\n- Maximum: 1240 characters\n- Achieve this by: ultra-dense writing, minimal examples, single-sentence descriptions\n\nULTRA-COMPRESSION TECHNIQUES:\n- Use telegraphic style: \"exports foo, bar, baz\" not \"this file exports three functions\"\n- No code examples unless critical to understanding\n- One sentence per export maximum\n- Omit obvious parameter names (\"opts: Options\" not \"options: Options - configuration object\")\n- Use abbreviations: \"params\" not \"parameters\", \"config\" not \"configuration\"\n- Skip section if empty (don't write \"No special concerns\" or \"N/A\")\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nWORKFLOW & CONVENTION RULES (NEVER EXCLUDE):\n- Contribution requirements: branch naming, commit message formats, PR description templates\n- Testing mandates: required test types, coverage thresholds, \"never delete tests\" policies\n- Tool usage rules: required CLI commands, browser automation tools, linter/formatter requirements\n- Approval workflows: review requirements, CI checks that must pass, deployment gates\n- AI agent instructions: explicit directives for automated assistants (coding rules, prohibited actions)\n- Code conventions: naming standards (camelCase, PascalCase, snake_case for what), import ordering rules, file/folder naming patterns, formatting rules (indentation, quotes, semicolons), language idioms, TypeScript strictness policies, preferred patterns vs anti-patterns\nThese define how contributors (human or AI) must work within the project.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion .annex.sum file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**CLI entry point orchestrating agents-reverse-engineer commands (install, discover, generate, update, specify, rebuild, clean) with argument parsing and command routing.**\n\n## Exported Symbols\n\n`main()` — async entry point dispatching to command handlers based CLI args.\n`parseArgs(args: string[])` — returns `{command, positional, flags, values}` parsing POSIX-style flags (--long, -h/-g/-l/-V) and positional args; stops at first non-flag for command name.\n`showVersion()`, `showVersionBanner()`, `showHelp()`, `showUnknownCommand(command)` — output routines; first two exit(0), last two exit(1).\n`hasInstallerFlags(flags, values)` — detects installer-mode activation (global/local/force/runtime presence).\n\n## Key Dependencies & Integration\n\nImports command handlers: `initCommand`, `discoverCommand`, `generateCommand`, `updateCommand`, `cleanCommand`, `specifyCommand`, `rebuildCommand` from sibling modules; `runInstaller`, `parseInstallerArgs` from `../installer/`; `getVersion` from `../version.js`.\n\n## Command Routing Logic\n\nHandles 8 explicit commands (install, uninstall, init, clean, discover, generate, update, specify, rebuild); auto-detects installer mode via `hasInstallerFlags()` for direct flag invocation (e.g., `npx agents-reverse-engineer --runtime claude -g`); empty args launches interactive installer; unknown commands call `showUnknownCommand()`.\n\n## Options Structures\n\n`GenerateOptions` — force, dryRun, concurrency, failFast, debug, trace, model, backend, eval.\n`UpdateOptions` — uncommitted, dryRun, concurrency, failFast, debug, trace, model, backend, eval.\n`CleanOptions` — dryRun.\n`SpecifyOptions` — output, force, dryRun, multiFile, debug, trace, model, backend.\n`RebuildOptions` — output, force, dryRun, concurrency, failFast, debug, trace, model, backend.\n\n## Behavioral Contracts\n\nShort flag mapping (parseArgs): `-h` → help, `-g` → global, `-l` → local, `-V` → version.\nLong flags with values parsed via map; next arg taken as value if present and non-flag.\nVersion string sourced from `getVersion()` output as `agents-reverse-engineer v${VERSION}`.\nExit code 0 on help/version, 1 on error or unknown command.\nDefault path: `.` (current directory) if positional arg omitted.\n\n## Workflow & Convention Rules\n\nInstaller re-parses args via `parseInstallerArgs()` for install/uninstall commands; all other commands use extracted flags/values directly. `--dry-run` supported on generate, update, specify, rebuild. `--force` overrides existing on init, generate, specify, rebuild. `--concurrency` parsed as integer for generate, update, rebuild. Model/backend optional for analysis commands. Help flag anywhere exits before command dispatch unless installer-mode active.",
      "model": "claude-haiku-4-5-20251001",
      "inputTokens": 3,
      "outputTokens": 736,
      "cacheReadTokens": 16147,
      "cacheCreationTokens": 5731,
      "latencyMs": 27320,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/cli/index.ts",
          "sizeBytes": 10327
        }
      ]
    },
    {
      "timestamp": "2026-02-14T13:46:50.503Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: src/cli/rebuild.ts\n\n```typescript\n/**\n * CLI rebuild command\n *\n * Reconstructs a project from specification files by:\n * 1. Reading spec files from specs/ directory\n * 2. Partitioning into ordered rebuild units\n * 3. Resolving an AI CLI backend\n * 4. Running the rebuild orchestrator with checkpoint-based session continuity\n * 5. Writing generated source files to an output directory\n *\n * With --dry-run, shows the rebuild plan without making any AI calls.\n */\n\nimport * as path from 'node:path';\nimport pc from 'picocolors';\nimport { loadConfig, findProjectRoot } from '../config/loader.js';\nimport { consoleLogger } from '../core/logger.js';\nimport {\n  AIService,\n  AIServiceError,\n  createBackendRegistry,\n  resolveBackend,\n  getInstallInstructions,\n} from '../ai/index.js';\nimport { ProgressLog, createTraceWriter, cleanupOldTraces } from '../orchestration/index.js';\nimport {\n  readSpecFiles,\n  partitionSpec,\n  CheckpointManager,\n  executeRebuild,\n} from '../rebuild/index.js';\n\n/**\n * Options for the rebuild command.\n */\nexport interface RebuildOptions {\n  /** Custom output directory (default: rebuild/) */\n  output?: string;\n  /** Wipe output directory and start fresh */\n  force?: boolean;\n  /** Show plan without executing */\n  dryRun?: boolean;\n  /** Override worker pool size */\n  concurrency?: number;\n  /** Stop on first failure */\n  failFast?: boolean;\n  /** Verbose subprocess logging */\n  debug?: boolean;\n  /** Enable NDJSON tracing */\n  trace?: boolean;\n  /** Override AI model (defaults to \"opus\" for rebuild) */\n  model?: string;\n  /** Override AI backend (e.g., \"claude\", \"codex\", \"opencode\", \"gemini\") */\n  backend?: string;\n}\n\n/**\n * Rebuild command - reconstructs a project from specification files via\n * AI-driven code generation with checkpoint-based session continuity.\n *\n * @param targetPath - Directory containing specs/ to rebuild from\n * @param options - Command options (output, force, dryRun, concurrency, etc.)\n */\nexport async function rebuildCommand(\n  targetPath: string,\n  options: RebuildOptions,\n): Promise<void> {\n  const absolutePath = await findProjectRoot(path.resolve(targetPath));\n  const outputDir = options.output\n    ? path.resolve(options.output)\n    : path.join(absolutePath, 'rebuild');\n\n  // Create trace writer\n  const tracer = createTraceWriter(absolutePath, options.trace ?? false);\n  if (options.trace && tracer.filePath) {\n    console.error(pc.dim(`[trace] Writing to ${tracer.filePath}`));\n  }\n\n  // Load configuration\n  const config = await loadConfig(absolutePath, {\n    tracer,\n    debug: options.debug,\n  });\n\n  // Read spec files early (before backend resolution)\n  const specFiles = await readSpecFiles(absolutePath);\n\n  // Partition specs into rebuild units\n  const units = partitionSpec(specFiles);\n\n  // -------------------------------------------------------------------------\n  // Dry-run mode: show rebuild plan without making AI calls\n  // -------------------------------------------------------------------------\n\n  if (options.dryRun) {\n    console.log(pc.bold('\\n--- Dry Run Summary ---\\n'));\n    console.log(`  Spec files:        ${pc.cyan(String(specFiles.length))}`);\n    console.log(`  Rebuild units:     ${pc.cyan(String(units.length))}`);\n    console.log(`  Output directory:  ${pc.cyan(outputDir)}`);\n    console.log('');\n    console.log(pc.dim('Rebuild units:'));\n    for (const unit of units) {\n      console.log(pc.dim(`  ${unit.order}. ${unit.name}`));\n    }\n\n    // Check for existing checkpoint\n    const unitNames = units.map((u) => u.name);\n    const { manager: checkpoint, isResume } = await CheckpointManager.load(\n      outputDir,\n      specFiles,\n      unitNames,\n    );\n    if (isResume) {\n      const pending = checkpoint.getPendingUnits();\n      console.log('');\n      console.log(pc.yellow(`Checkpoint found: ${units.length - pending.length} of ${units.length} modules already complete.`));\n      console.log(pc.yellow(`Would resume with ${pending.length} remaining modules.`));\n    }\n\n    console.log('');\n    console.log(pc.dim('No AI calls made (dry run).'));\n    return;\n  }\n\n  // -------------------------------------------------------------------------\n  // Handle --force warning\n  // -------------------------------------------------------------------------\n\n  if (options.force) {\n    console.log(pc.yellow(`Forcing fresh rebuild: output directory will be wiped (${outputDir})`));\n  }\n\n  // -------------------------------------------------------------------------\n  // Resolve backend and create AI service\n  // -------------------------------------------------------------------------\n\n  const registry = createBackendRegistry();\n  let backend;\n  try {\n    backend = await resolveBackend(registry, options.backend ?? config.ai.backend);\n  } catch (error) {\n    if (error instanceof AIServiceError && error.code === 'CLI_NOT_FOUND') {\n      console.error(pc.red('Error: No AI CLI found.\\n'));\n      console.error(getInstallInstructions(registry));\n      process.exit(2);\n    }\n    throw error;\n  }\n\n  // Provision backend-specific resources (e.g., OpenCode agent config)\n  await backend.ensureProjectConfig?.(absolutePath);\n\n  // Resolve effective model: CLI flag > config override > opus default\n  // Rebuild benefits from the best model; upgrade default sonnet to opus\n  const effectiveModel = options.model\n    ?? (config.ai.model === 'sonnet' ? 'opus' : config.ai.model);\n\n  // Debug: log backend info\n  if (options.debug) {\n    console.error(pc.dim(`[debug] Backend: ${backend.name}`));\n    console.error(pc.dim(`[debug] CLI command: ${backend.cliCommand}`));\n    console.error(pc.dim(`[debug] Model: ${effectiveModel}`));\n  }\n\n  // Create AI service with extended timeout (rebuild modules are large)\n  const aiService = new AIService(backend, {\n    timeoutMs: Math.max(config.ai.timeoutMs, 900_000), // 15min minimum\n    maxRetries: config.ai.maxRetries,\n    model: effectiveModel,\n    command: 'rebuild',\n    telemetry: { keepRuns: config.ai.telemetry.keepRuns },\n  }, consoleLogger);\n\n  if (options.debug) {\n    aiService.setDebug(true);\n  }\n\n  // Enable subprocess output logging alongside tracing\n  if (options.trace) {\n    const logDir = path.join(\n      absolutePath, '.agents-reverse-engineer', 'subprocess-logs',\n      new Date().toISOString().replace(/[:.]/g, '-'),\n    );\n    aiService.setSubprocessLogDir(logDir);\n    console.error(pc.dim(`[trace] Subprocess logs -> ${logDir}`));\n  }\n\n  // Create progress log for tail -f monitoring\n  const progressLog = ProgressLog.create(absolutePath);\n  progressLog.write(`=== ARE Rebuild (${new Date().toISOString()}) ===`);\n  progressLog.write(`Project: ${absolutePath}`);\n  progressLog.write(`Output: ${outputDir}`);\n  progressLog.write(`Rebuild units: ${units.length}`);\n  progressLog.write('');\n\n  // Determine concurrency\n  const concurrency = options.concurrency ?? config.ai.concurrency;\n\n  // -------------------------------------------------------------------------\n  // Execute rebuild\n  // -------------------------------------------------------------------------\n\n  console.log(pc.bold(`Rebuilding project from ${specFiles.length} spec file(s)...`));\n  console.log(pc.dim(`Output directory: ${outputDir}`));\n  console.log(pc.dim(`Rebuild units: ${units.length}`));\n  console.log('');\n\n  const result = await executeRebuild(aiService, absolutePath, {\n    outputDir,\n    concurrency,\n    failFast: options.failFast,\n    force: options.force,\n    debug: options.debug,\n    tracer,\n    progressLog,\n  });\n\n  // -------------------------------------------------------------------------\n  // Finalize telemetry, trace, and progress log\n  // -------------------------------------------------------------------------\n\n  await aiService.finalize(absolutePath);\n  await progressLog.finalize();\n  await tracer.finalize();\n  if (options.trace) {\n    await cleanupOldTraces(absolutePath);\n  }\n\n  // -------------------------------------------------------------------------\n  // Print summary and set exit code\n  // -------------------------------------------------------------------------\n\n  console.log('');\n  console.log(pc.bold('Rebuild complete:'));\n  console.log(`  Modules processed: ${pc.green(String(result.modulesProcessed))}`);\n  if (result.modulesSkipped > 0) {\n    console.log(`  Modules skipped:   ${pc.yellow(String(result.modulesSkipped))} (already complete)`);\n  }\n  if (result.modulesFailed > 0) {\n    console.log(`  Modules failed:    ${pc.red(String(result.modulesFailed))}`);\n  }\n  console.log(`  Output directory:  ${pc.cyan(outputDir)}`);\n\n  if (result.modulesProcessed === 0 && result.modulesFailed > 0) {\n    process.exit(2);\n  } else if (result.modulesFailed > 0) {\n    process.exit(1);\n  }\n}\n\n```\n\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n- Workflow & convention rules: if this file defines contribution guidelines, PR conventions, commit standards, testing mandates, tool usage requirements, approval workflows, AI agent behavioral instructions, or code conventions (naming standards, formatting rules, import ordering, linting policies), extract them as explicit, actionable rules. Sources: CONTRIBUTING.md, CI configs, PR templates, style guides, linter/formatter configs (.eslintrc, .prettierrc, .editorconfig), tsconfig strictness, README dev sections.\n\n\nTARGET LENGTH (MANDATORY):\n- Source file: 8655 characters\n- Target summary: ~866 characters (10% compression)\n- Maximum: 1039 characters\n- Achieve this by: ultra-dense writing, minimal examples, single-sentence descriptions\n\nULTRA-COMPRESSION TECHNIQUES:\n- Use telegraphic style: \"exports foo, bar, baz\" not \"this file exports three functions\"\n- No code examples unless critical to understanding\n- One sentence per export maximum\n- Omit obvious parameter names (\"opts: Options\" not \"options: Options - configuration object\")\n- Use abbreviations: \"params\" not \"parameters\", \"config\" not \"configuration\"\n- Skip section if empty (don't write \"No special concerns\" or \"N/A\")\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nWORKFLOW & CONVENTION RULES (NEVER EXCLUDE):\n- Contribution requirements: branch naming, commit message formats, PR description templates\n- Testing mandates: required test types, coverage thresholds, \"never delete tests\" policies\n- Tool usage rules: required CLI commands, browser automation tools, linter/formatter requirements\n- Approval workflows: review requirements, CI checks that must pass, deployment gates\n- AI agent instructions: explicit directives for automated assistants (coding rules, prohibited actions)\n- Code conventions: naming standards (camelCase, PascalCase, snake_case for what), import ordering rules, file/folder naming patterns, formatting rules (indentation, quotes, semicolons), language idioms, TypeScript strictness policies, preferred patterns vs anti-patterns\nThese define how contributors (human or AI) must work within the project.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion .annex.sum file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**rebuild.ts exports the rebuildCommand CLI handler that reconstructs projects from specification files via AI-driven code generation with checkpoint-based session continuity.**\n\n## Exported Interface & Function\n\n`RebuildOptions` — configuration object with: `output` (string, custom output dir), `force` (boolean, wipe output), `dryRun` (boolean, show plan only), `concurrency` (number, worker pool override), `failFast` (boolean, stop on first failure), `debug` (boolean, verbose logging), `trace` (boolean, NDJSON tracing), `model` (string, override AI model), `backend` (string, override backend: \"claude\", \"codex\", \"opencode\", \"gemini\").\n\n`rebuildCommand(targetPath: string, options: RebuildOptions): Promise<void>` — main CLI entry point; reads spec files from `specs/` directory, partitions into rebuild units, resolves AI backend, executes checkpoint-resumable rebuild via `executeRebuild`, writes output to `rebuild/` (or `options.output`).\n\n## Workflow & Execution Model\n\n1. **Spec partition**: `readSpecFiles(absolutePath)` → `partitionSpec(specFiles)` creates ordered rebuild units.\n2. **Checkpoint resumption**: `CheckpointManager.load(outputDir, specFiles, unitNames)` enables resuming incomplete rebuilds; `--dry-run` displays checkpoint status without AI calls.\n3. **Backend resolution**: `resolveBackend(registry, backend)` loads CLI backend (throws `AIServiceError` with code `'CLI_NOT_FOUND'` if missing); `backend.ensureProjectConfig(absolutePath)` provisions backend resources.\n4. **Model selection**: CLI flag > config override > \"opus\" default (upgrades \"sonnet\" to \"opus\" for rebuild quality).\n5. **AI service**: `AIService` with `timeoutMs: Math.max(config.ai.timeoutMs, 900_000)` (15min minimum), `maxRetries`, `command: 'rebuild'`; subprocess logs written to `.agents-reverse-engineer/subprocess-logs/[ISO-timestamp]/` if `--trace` enabled.\n6. **Progress tracking**: `ProgressLog.create(absolutePath)` writes human-readable progress; `--trace` also invokes `createTraceWriter` for NDJSON output.\n7. **Execution**: `executeRebuild(aiService, absolutePath, {...})` runs rebuild; result contains `modulesProcessed`, `modulesSkipped`, `modulesFailed`.\n8. **Exit codes**: 0 (success), 1 (partial failure), 2 (all failed or no AI CLI).\n\n## Key Dependencies\n\n`loadConfig` — reads config with tracer/debug opts; `findProjectRoot` — resolves absolute path; `createBackendRegistry`, `resolveBackend`, `getInstallInstructions` — AI backend provisioning; `ProgressLog`, `createTraceWriter`, `cleanupOldTraces` — monitoring & tracing; `readSpecFiles`, `partitionSpec`, `CheckpointManager`, `executeRebuild` — rebuild orchestration.",
      "model": "claude-haiku-4-5-20251001",
      "inputTokens": 3,
      "outputTokens": 733,
      "cacheReadTokens": 16147,
      "cacheCreationTokens": 5163,
      "latencyMs": 36609,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/cli/rebuild.ts",
          "sizeBytes": 8655
        }
      ]
    },
    {
      "timestamp": "2026-02-14T13:46:46.765Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: src/cli/generate.ts\n\n```typescript\n/**\n * CLI generate command\n *\n * Creates and executes a documentation generation plan by:\n * 1. Discovering files to analyze\n * 2. Detecting file types and creating analysis tasks\n * 3. Resolving an AI CLI backend\n * 4. Running concurrent AI analysis via CommandRunner\n * 5. Producing .sum files, AGENTS.md, and companion CLAUDE.md per directory\n *\n * With --dry-run, shows the plan without making any AI calls.\n */\n\nimport * as path from 'node:path';\nimport pc from 'picocolors';\nimport { loadConfig, findProjectRoot } from '../config/loader.js';\nimport { createLogger } from '../output/logger.js';\nimport { consoleLogger } from '../core/logger.js';\nimport { discoverFiles } from '../discovery/run.js';\nimport { createOrchestrator, type GenerationPlan } from '../orchestration/orchestrator.js';\nimport { buildExecutionPlan } from '../generation/executor.js';\nimport {\n  AIService,\n  AIServiceError,\n  createBackendRegistry,\n  resolveBackend,\n  getInstallInstructions,\n} from '../ai/index.js';\nimport { CommandRunner, ProgressLog, createTraceWriter, cleanupOldTraces } from '../orchestration/index.js';\n\n/**\n * Options for the generate command.\n */\nexport interface GenerateOptions {\n  /** Force full regeneration (skip nothing) */\n  force?: boolean;\n  /** Dry run - show plan without generating */\n  dryRun?: boolean;\n  /** Number of concurrent AI calls */\n  concurrency?: number;\n  /** Stop on first file analysis failure */\n  failFast?: boolean;\n  /** Show AI prompts and backend details */\n  debug?: boolean;\n  /** Enable concurrency tracing to .agents-reverse-engineer/traces/ */\n  trace?: boolean;\n  /** Override AI model (e.g., \"sonnet\", \"opus\") */\n  model?: string;\n  /** Override AI backend (e.g., \"claude\", \"codex\", \"opencode\", \"gemini\") */\n  backend?: string;\n  /** Eval mode: namespace output by backend.model for comparison */\n  eval?: boolean;\n}\n\n/**\n * Format the generation plan for display.\n */\nfunction formatPlan(plan: GenerationPlan): string {\n  const lines: string[] = [];\n\n  lines.push(`\\n=== Generation Plan ===\\n`);\n\n  // File summary\n  lines.push(`Files to analyze: ${plan.files.length}`);\n  if (plan.skippedFiles && plan.skippedFiles.length > 0) {\n    lines.push(`Files skipped:    ${plan.skippedFiles.length} (existing .sum)`);\n  }\n  lines.push(`Tasks to execute: ${plan.tasks.length}`);\n  if (plan.skippedDirs && plan.skippedDirs.length > 0) {\n    lines.push(`Dirs skipped:     ${plan.skippedDirs.length} (existing AGENTS.md)`);\n  }\n  lines.push('');\n\n  // Complexity\n  lines.push('Complexity:');\n  lines.push(`  Files: ${plan.complexity.fileCount}`);\n  lines.push(`  Directory depth: ${plan.complexity.directoryDepth}`);\n  lines.push('');\n\n  return lines.join('\\n');\n}\n\n/**\n * Generate command - discovers files, plans analysis, and executes AI-driven\n * documentation generation.\n *\n * Default behavior: resolves an AI CLI backend, builds an execution plan,\n * and runs concurrent AI analysis via the CommandRunner. Produces .sum files,\n * AGENTS.md per directory, and CLAUDE.md pointers.\n *\n * @param targetPath - Directory to generate documentation for\n * @param options - Command options (concurrency, failFast, debug, etc.)\n */\nexport async function generateCommand(\n  targetPath: string,\n  options: GenerateOptions\n): Promise<void> {\n  const absolutePath = await findProjectRoot(path.resolve(targetPath));\n  const logger = createLogger({ colors: true });\n\n  logger.info(`Generating documentation plan for: ${absolutePath}`);\n\n  // Create trace writer (moved earlier to use in config loading and discovery)\n  const tracer = createTraceWriter(absolutePath, options.trace ?? false);\n  if (options.trace && tracer.filePath) {\n    console.error(pc.dim(`[trace] Writing to ${tracer.filePath}`));\n  }\n\n  // Load configuration\n  const config = await loadConfig(absolutePath, {\n    tracer,\n    debug: options.debug,\n  });\n\n  // Discover files\n  logger.info('Discovering files...');\n\n  const filterResult = await discoverFiles(absolutePath, config, {\n    tracer,\n    debug: options.debug,\n  });\n\n  // Create discovery result for orchestrator\n  const discoveryResult = {\n    files: filterResult.included,\n    excluded: filterResult.excluded.map(e => ({ path: e.path, reason: e.reason })),\n  };\n\n  logger.info(`Found ${discoveryResult.files.length} files to analyze`);\n\n  // Create generation plan\n  logger.info('Creating generation plan...');\n  const orchestrator = createOrchestrator(\n    config,\n    absolutePath,\n    { tracer, debug: options.debug }\n  );\n  const plan = await orchestrator.createPlan(discoveryResult, { force: options.force });\n\n  // Report skip stats\n  if (plan.skippedFiles && plan.skippedFiles.length > 0) {\n    logger.info(`Skipping ${plan.skippedFiles.length} files (existing .sum)`);\n  }\n  if (plan.skippedDirs && plan.skippedDirs.length > 0) {\n    logger.info(`Skipping ${plan.skippedDirs.length} directories (existing AGENTS.md)`);\n  }\n\n  // Early exit if nothing to do\n  if (plan.files.length === 0 && plan.tasks.length === 0) {\n    logger.info('All files already documented. Nothing to do.');\n    return;\n  }\n\n  // Display plan\n  console.log(formatPlan(plan));\n\n  // ---------------------------------------------------------------------------\n  // Dry-run: show execution plan summary without making AI calls\n  // ---------------------------------------------------------------------------\n\n  if (options.dryRun) {\n    // Dry-run doesn't resolve backend, so compute variant from config defaults\n    const dryRunVariant = options.eval ? `${options.backend ?? config.ai.backend}.${options.model ?? config.ai.model}` : undefined;\n    const executionPlan = buildExecutionPlan(plan, absolutePath, dryRunVariant);\n    const dirCount = Object.keys(executionPlan.directoryFileMap).length;\n\n    console.log(pc.bold('\\n--- Dry Run Summary ---\\n'));\n    console.log(`  Files to analyze:     ${pc.cyan(String(executionPlan.fileTasks.length))}`);\n    if (plan.skippedFiles && plan.skippedFiles.length > 0) {\n      console.log(`  Files skipped:        ${pc.yellow(String(plan.skippedFiles.length))}`);\n    }\n    console.log(`  Directories:          ${pc.cyan(String(dirCount))}`);\n    if (plan.skippedDirs && plan.skippedDirs.length > 0) {\n      console.log(`  Dirs skipped:         ${pc.yellow(String(plan.skippedDirs.length))}`);\n    }\n    console.log(`  Estimated AI calls:   ${pc.cyan(String(executionPlan.tasks.length))}`);\n    console.log('');\n    if (executionPlan.fileTasks.length > 0) {\n      console.log(pc.dim('Files to process:'));\n      for (const task of executionPlan.fileTasks) {\n        console.log(pc.dim(`  ${task.path}`));\n      }\n      console.log('');\n    }\n    if (plan.skippedFiles && plan.skippedFiles.length > 0) {\n      console.log(pc.dim('Files skipped (existing .sum):'));\n      for (const f of plan.skippedFiles) {\n        console.log(pc.dim(`  ${f}`));\n      }\n      console.log('');\n    }\n    console.log(pc.dim('No AI calls made (dry run).'));\n    return;\n  }\n\n  // ---------------------------------------------------------------------------\n  // Resolve backend and run AI analysis\n  // ---------------------------------------------------------------------------\n\n  // Resolve AI CLI backend\n  const registry = createBackendRegistry();\n  let backend;\n  try {\n    backend = await resolveBackend(registry, options.backend ?? config.ai.backend);\n  } catch (error) {\n    if (error instanceof AIServiceError && error.code === 'CLI_NOT_FOUND') {\n      console.error(pc.red('Error: No AI CLI found.\\n'));\n      console.error(getInstallInstructions(registry));\n      process.exit(2);\n    }\n    throw error;\n  }\n\n  // Provision backend-specific resources (e.g., OpenCode agent config)\n  await backend.ensureProjectConfig?.(absolutePath);\n\n  // Resolve effective model (CLI flag > config)\n  const effectiveModel = options.model ?? config.ai.model;\n\n  // Compute eval variant\n  const variant = options.eval ? `${backend.name}.${effectiveModel}` : undefined;\n\n  // Debug: log backend info\n  if (options.debug) {\n    console.log(pc.dim(`[debug] Backend: ${backend.name}`));\n    console.log(pc.dim(`[debug] CLI command: ${backend.cliCommand}`));\n    console.log(pc.dim(`[debug] Model: ${effectiveModel}`));\n    if (variant) {\n      console.log(pc.dim(`[debug] Eval variant: ${variant}`));\n    }\n  }\n\n  if (variant) {\n    console.log(pc.cyan(`[eval] Variant: ${variant}`));\n    console.log(pc.dim(`[eval] Output files: *.${variant}.sum, AGENTS.${variant}.md`));\n  }\n\n  // Create AI service\n  const aiService = new AIService(backend, {\n    timeoutMs: config.ai.timeoutMs,\n    maxRetries: config.ai.maxRetries,\n    model: effectiveModel,\n    command: 'generate',\n    telemetry: { keepRuns: config.ai.telemetry.keepRuns },\n  }, consoleLogger);\n\n  if (options.debug) {\n    aiService.setDebug(true);\n  }\n\n  // Build execution plan\n  const executionPlan = buildExecutionPlan(plan, absolutePath, variant);\n\n  // Determine concurrency\n  const concurrency = options.concurrency ?? config.ai.concurrency;\n\n  // Enable subprocess output logging alongside tracing\n  if (options.trace) {\n    const logDir = path.join(\n      absolutePath, '.agents-reverse-engineer', 'subprocess-logs',\n      new Date().toISOString().replace(/[:.]/g, '-'),\n    );\n    aiService.setSubprocessLogDir(logDir);\n    console.error(pc.dim(`[trace] Subprocess logs → ${logDir}`));\n  }\n\n  // Create progress log for tail -f monitoring\n  const progressLog = ProgressLog.create(absolutePath);\n  progressLog.write(`=== ARE Generate (${new Date().toISOString()}) ===`);\n  progressLog.write(`Project: ${absolutePath}`);\n  const skippedCount = plan.skippedFiles?.length ?? 0;\n  const skipInfo = skippedCount > 0 ? ` | Skipped: ${skippedCount}` : '';\n  progressLog.write(`Files: ${executionPlan.fileTasks.length} | Directories: ${executionPlan.directoryTasks.length}${skipInfo}`);\n  progressLog.write('');\n\n  // Create command runner\n  const runner = new CommandRunner(aiService, {\n    concurrency,\n    failFast: options.failFast,\n    debug: options.debug,\n    tracer,\n    progressLog,\n    variant,\n  });\n\n  // Execute the two-phase pipeline\n  const summary = await runner.executeGenerate(executionPlan, {\n    skippedFiles: skippedCount,\n    skippedDirs: executionPlan.skippedDirs?.length ?? 0,\n  });\n\n  // Write telemetry run log\n  await aiService.finalize(absolutePath);\n\n  // Finalize trace, progress log, and clean up old trace files\n  await progressLog.finalize();\n  await tracer.finalize();\n  if (options.trace) {\n    await cleanupOldTraces(absolutePath);\n  }\n\n  // Determine exit code from RunSummary\n  //   0: all files succeeded\n  //   1: some files failed (partial failure)\n  //   2: no files succeeded (total failure)\n  if (summary.filesProcessed === 0 && summary.filesFailed > 0) {\n    process.exit(2);\n  } else if (summary.filesFailed > 0) {\n    process.exit(1);\n  }\n  // Exit code 0 -- all files succeeded (or no files to process)\n}\n\n```\n\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n- Workflow & convention rules: if this file defines contribution guidelines, PR conventions, commit standards, testing mandates, tool usage requirements, approval workflows, AI agent behavioral instructions, or code conventions (naming standards, formatting rules, import ordering, linting policies), extract them as explicit, actionable rules. Sources: CONTRIBUTING.md, CI configs, PR templates, style guides, linter/formatter configs (.eslintrc, .prettierrc, .editorconfig), tsconfig strictness, README dev sections.\n\n\nTARGET LENGTH (MANDATORY):\n- Source file: 10859 characters\n- Target summary: ~1086 characters (10% compression)\n- Maximum: 1303 characters\n- Achieve this by: ultra-dense writing, minimal examples, single-sentence descriptions\n\nULTRA-COMPRESSION TECHNIQUES:\n- Use telegraphic style: \"exports foo, bar, baz\" not \"this file exports three functions\"\n- No code examples unless critical to understanding\n- One sentence per export maximum\n- Omit obvious parameter names (\"opts: Options\" not \"options: Options - configuration object\")\n- Use abbreviations: \"params\" not \"parameters\", \"config\" not \"configuration\"\n- Skip section if empty (don't write \"No special concerns\" or \"N/A\")\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nWORKFLOW & CONVENTION RULES (NEVER EXCLUDE):\n- Contribution requirements: branch naming, commit message formats, PR description templates\n- Testing mandates: required test types, coverage thresholds, \"never delete tests\" policies\n- Tool usage rules: required CLI commands, browser automation tools, linter/formatter requirements\n- Approval workflows: review requirements, CI checks that must pass, deployment gates\n- AI agent instructions: explicit directives for automated assistants (coding rules, prohibited actions)\n- Code conventions: naming standards (camelCase, PascalCase, snake_case for what), import ordering rules, file/folder naming patterns, formatting rules (indentation, quotes, semicolons), language idioms, TypeScript strictness policies, preferred patterns vs anti-patterns\nThese define how contributors (human or AI) must work within the project.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion .annex.sum file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**generate.ts executes the documentation generation pipeline: discovers files, creates an analysis plan, resolves an AI backend, and runs concurrent AI-driven .sum file and AGENTS.md generation.**\n\n## Exports\n\n`generateCommand(targetPath: string, options: GenerateOptions): Promise<void>` — Main CLI handler; discovers files in targetPath, builds execution plan via createOrchestrator and buildExecutionPlan, resolves AI backend via resolveBackend, provisions backend config with ensureProjectConfig, creates AIService with configurable timeouts/retries, executes two-phase pipeline via CommandRunner.executeGenerate, writes telemetry/traces, exits with code 0 (success), 1 (partial failure), or 2 (total failure).\n\n`interface GenerateOptions` — Command configuration: force (skip nothing), dryRun (plan without AI calls), concurrency (default from config.ai.concurrency), failFast, debug, trace (writes to .agents-reverse-engineer/traces/), model (override config.ai.model), backend (override config.ai.backend), eval (namespace output by backend.model for comparison).\n\n`formatPlan(plan: GenerationPlan): string` — Formats plan summary showing file counts, skipped files/dirs (existing .sum, AGENTS.md), task counts, and complexity metrics (fileCount, directoryDepth).\n\n## Integration Points\n\n- **Discovery**: discoverFiles(absolutePath, config, {tracer, debug}) filters source files, returns included/excluded lists mapped to discoveryResult.\n- **Planning**: createOrchestrator creates GenerationPlan via orchestrator.createPlan(discoveryResult, {force}); plan.files/tasks/skipped{Files,Dirs} drive execution.\n- **Execution**: buildExecutionPlan(plan, absolutePath, variant) creates executionPlan with fileTasks, directoryTasks, directoryFileMap; CommandRunner.executeGenerate processes concurrent tasks and returns RunSummary (filesProcessed, filesFailed).\n- **Backend**: createBackendRegistry() → resolveBackend(registry, backendName) resolves AIServiceError if CLI_NOT_FOUND (code='CLI_NOT_FOUND'); backend.ensureProjectConfig?(path) provisions config; getInstallInstructions(registry) shows setup help.\n- **AI Service**: AIService(backend, {timeoutMs, maxRetries, model, command: 'generate', telemetry: {keepRuns}}, consoleLogger); setDebug(true), setSubprocessLogDir(logDir) enable logging.\n- **Tracing**: createTraceWriter(absolutePath, enabled) → tracer.filePath, used in loadConfig/discoverFiles/createOrchestrator; cleanupOldTraces(absolutePath) removes old .agents-reverse-engineer/traces/; ProgressLog.create(absolutePath) logs progress to .agents-reverse-engineer/progress.log.\n- **Config**: loadConfig(absolutePath, {tracer, debug}) resolves config.ai.{backend, model, concurrency, timeoutMs, maxRetries, telemetry.keepRuns}.\n\n## Behavioral Contracts\n\n- **Dry-run**: When options.dryRun=true, computes dryRunVariant = `${backend}.${model}` (if eval), prints execution plan summary (file counts, directories, estimated AI calls), then returns without AI calls.\n- **Eval mode**: When options.eval=true, variant = `${backend.name}.${effectiveModel}`; outputs prefixed with variant (e.g., *.{variant}.sum, AGENTS.{variant}.md); logs \"[eval] Variant: {variant}\" and \"[eval] Output files: *.{variant}.sum, AGENTS.{variant}.md\".\n- **Subprocess logs**: If options.trace=true, setSubprocessLogDir(logDir) where logDir = `.agents-reverse-engineer/subprocess-logs/{ISO-timestamp}` (colons/periods replaced with hyphens).\n- **Progress log**: ProgressLog.write entries: \"=== ARE Generate ({timestamp}) ===\", \"Project: {absolutePath}\", \"Files: {count} | Directories: {count} | Skipped: {count}\".\n- **Exit codes**: 0 (all succeeded), 1 (summary.filesFailed > 0 && filesProcessed > 0), 2 (filesProcessed === 0 && filesFailed > 0).\n- **Error handling**: AIServiceError with code='CLI_NOT_FOUND' → exit(2) after printing error and install instructions.\n- **Skip conditions**: plan.skippedFiles (existing .sum files), plan.skippedDirs (existing AGENTS.md); early return if plan.files.length === 0 && plan.tasks.length === 0.",
      "model": "claude-haiku-4-5-20251001",
      "inputTokens": 3,
      "outputTokens": 1117,
      "cacheReadTokens": 16147,
      "cacheCreationTokens": 6045,
      "latencyMs": 29999,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/cli/generate.ts",
          "sizeBytes": 10861
        }
      ]
    },
    {
      "timestamp": "2026-02-14T13:47:02.936Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: src/core/logger.ts\n\n```typescript\n/**\n * Debug logger interface for core library modules.\n *\n * Allows library consumers to inject their own logging backend\n * (or silence debug output entirely with {@link nullLogger}).\n * CLI code passes {@link consoleLogger} to preserve existing behavior.\n *\n * @module\n */\n\n/**\n * Minimal logger contract for debug/warn/error output.\n *\n * All core library modules accept an optional `Logger` to decouple\n * from direct `console.error` calls.\n */\nexport interface Logger {\n  debug(message: string): void;\n  warn(message: string): void;\n  error(message: string): void;\n}\n\n/**\n * Silent logger -- all methods are no-ops.\n *\n * Used as the default when no logger is provided, ensuring library\n * consumers get zero output unless they opt in.\n */\nexport const nullLogger: Logger = {\n  debug() {},\n  warn() {},\n  error() {},\n};\n\n/**\n * Console logger -- writes to stderr (matching existing CLI behavior).\n *\n * CLI entry points pass this to preserve the current debug output.\n */\nexport const consoleLogger: Logger = {\n  debug: (msg) => console.error(msg),\n  warn: (msg) => console.error(msg),\n  error: (msg) => console.error(msg),\n};\n\n```\n\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n- Workflow & convention rules: if this file defines contribution guidelines, PR conventions, commit standards, testing mandates, tool usage requirements, approval workflows, AI agent behavioral instructions, or code conventions (naming standards, formatting rules, import ordering, linting policies), extract them as explicit, actionable rules. Sources: CONTRIBUTING.md, CI configs, PR templates, style guides, linter/formatter configs (.eslintrc, .prettierrc, .editorconfig), tsconfig strictness, README dev sections.\n\n\nTARGET LENGTH (MANDATORY):\n- Source file: 1133 characters\n- Target summary: ~113 characters (10% compression)\n- Maximum: 136 characters\n- Achieve this by: ultra-dense writing, minimal examples, single-sentence descriptions\n\nULTRA-COMPRESSION TECHNIQUES:\n- Use telegraphic style: \"exports foo, bar, baz\" not \"this file exports three functions\"\n- No code examples unless critical to understanding\n- One sentence per export maximum\n- Omit obvious parameter names (\"opts: Options\" not \"options: Options - configuration object\")\n- Use abbreviations: \"params\" not \"parameters\", \"config\" not \"configuration\"\n- Skip section if empty (don't write \"No special concerns\" or \"N/A\")\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nWORKFLOW & CONVENTION RULES (NEVER EXCLUDE):\n- Contribution requirements: branch naming, commit message formats, PR description templates\n- Testing mandates: required test types, coverage thresholds, \"never delete tests\" policies\n- Tool usage rules: required CLI commands, browser automation tools, linter/formatter requirements\n- Approval workflows: review requirements, CI checks that must pass, deployment gates\n- AI agent instructions: explicit directives for automated assistants (coding rules, prohibited actions)\n- Code conventions: naming standards (camelCase, PascalCase, snake_case for what), import ordering rules, file/folder naming patterns, formatting rules (indentation, quotes, semicolons), language idioms, TypeScript strictness policies, preferred patterns vs anti-patterns\nThese define how contributors (human or AI) must work within the project.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion .annex.sum file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**logger.ts defines a minimal Logger interface and default implementations for decoupled debug/warn/error output across core library modules.**\n\n## Exports\n\n`Logger` interface: contract with `debug(message: string)`, `warn(message: string)`, `error(message: string)` methods.\n\n`nullLogger`: Logger implementation with no-op methods; default when no logger injected.\n\n`consoleLogger`: Logger implementation routing all methods to `console.error(msg)` for stderr output.\n\n## Design\n\nDependency injection pattern: core modules accept optional `Logger` parameter to decouple from direct `console` calls, enabling consumers to inject custom backends or silence output entirely. CLI code passes `consoleLogger` to preserve existing behavior.",
      "model": "claude-haiku-4-5-20251001",
      "inputTokens": 3,
      "outputTokens": 164,
      "cacheReadTokens": 16147,
      "cacheCreationTokens": 3056,
      "latencyMs": 17113,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/core/logger.ts",
          "sizeBytes": 1133
        }
      ]
    },
    {
      "timestamp": "2026-02-14T13:46:52.448Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: src/cli/specify.ts\n\n```typescript\n/**\n * CLI specify command\n *\n * Generates a project specification from AGENTS.md documentation by:\n * 1. Loading configuration\n * 2. Collecting all AGENTS.md files (auto-generating if none exist)\n * 3. Building a synthesis prompt from the collected docs\n * 4. Resolving an AI CLI backend and calling the AI service\n * 5. Writing the specification to disk (single or multi-file)\n *\n * With --dry-run, shows input statistics without making any AI calls.\n */\n\nimport * as path from 'node:path';\nimport { access, readdir } from 'node:fs/promises';\nimport { constants } from 'node:fs';\nimport pc from 'picocolors';\nimport { loadConfig, findProjectRoot } from '../config/loader.js';\nimport { consoleLogger } from '../core/logger.js';\nimport { collectAgentsDocs, collectAnnexFiles } from '../generation/collector.js';\nimport { buildSpecPrompt, writeSpec, SpecExistsError } from '../specify/index.js';\nimport {\n  AIService,\n  AIServiceError,\n  createBackendRegistry,\n  resolveBackend,\n  getInstallInstructions,\n} from '../ai/index.js';\nimport { ProgressLog, createTraceWriter, cleanupOldTraces } from '../orchestration/index.js';\nimport { generateCommand } from './generate.js';\n\n/**\n * Options for the specify command.\n */\nexport interface SpecifyOptions {\n  /** Custom output path (default: specs/SPEC.md) */\n  output?: string;\n  /** Overwrite existing specs */\n  force?: boolean;\n  /** Show plan without calling AI */\n  dryRun?: boolean;\n  /** Split output into multiple files */\n  multiFile?: boolean;\n  /** Show verbose debug info */\n  debug?: boolean;\n  /** Enable tracing */\n  trace?: boolean;\n  /** Override AI model (defaults to \"opus\" for specify) */\n  model?: string;\n  /** Override AI backend (e.g., \"claude\", \"codex\", \"opencode\", \"gemini\") */\n  backend?: string;\n}\n\n/**\n * Specify command - collects AGENTS.md documentation, synthesizes it via AI,\n * and writes a comprehensive project specification.\n *\n * @param targetPath - Directory to generate specification for\n * @param options - Command options (output, force, dryRun, multiFile, debug, trace)\n */\nexport async function specifyCommand(\n  targetPath: string,\n  options: SpecifyOptions,\n): Promise<void> {\n  const absolutePath = await findProjectRoot(path.resolve(targetPath));\n  const outputPath = options.output\n    ? path.resolve(options.output)\n    : path.join(absolutePath, 'specs', 'SPEC.md');\n\n  // Early exit if spec file(s) already exist (avoids waiting for AI call)\n  if (!options.force && !options.dryRun) {\n    const conflicts: string[] = [];\n    if (options.multiFile) {\n      const outputDir = path.dirname(outputPath);\n      try {\n        const entries = await readdir(outputDir);\n        for (const entry of entries) {\n          if (entry.endsWith('.md')) {\n            conflicts.push(path.join(outputDir, entry));\n          }\n        }\n      } catch {\n        // Directory doesn't exist — no conflicts\n      }\n    } else {\n      try {\n        await access(outputPath, constants.F_OK);\n        conflicts.push(outputPath);\n      } catch {\n        // File doesn't exist — no conflict\n      }\n    }\n\n    if (conflicts.length > 0) {\n      const list = conflicts.map((p) => `  - ${p}`).join('\\n');\n      console.error(pc.red(`Spec file(s) already exist:\\n${list}\\nUse --force to overwrite.`));\n      process.exit(1);\n    }\n  }\n\n  // Load configuration\n  const config = await loadConfig(absolutePath, { debug: options.debug });\n\n  // Collect AGENTS.md files\n  let docs = await collectAgentsDocs(absolutePath);\n\n  // ---------------------------------------------------------------------------\n  // Dry-run mode: show summary without calling AI or generating\n  // ---------------------------------------------------------------------------\n\n  // Collect annex files for reproduction-critical content\n  let annexFiles = await collectAnnexFiles(absolutePath);\n\n  if (options.dryRun) {\n    const totalChars = docs.reduce((sum, d) => sum + d.content.length, 0)\n      + annexFiles.reduce((sum, d) => sum + d.content.length, 0);\n    const estimatedTokensK = Math.ceil(totalChars / 4) / 1000;\n\n    console.log(pc.bold('\\n--- Dry Run Summary ---\\n'));\n    console.log(`  AGENTS.md files:   ${pc.cyan(String(docs.length))}`);\n    console.log(`  Annex files:       ${pc.cyan(String(annexFiles.length))}`);\n    console.log(`  Total input:       ${pc.cyan(`~${estimatedTokensK}K tokens`)}`);\n    console.log(`  Output:            ${pc.cyan(outputPath)}`);\n    console.log(`  Mode:              ${pc.cyan(options.multiFile ? 'multi-file' : 'single-file')}`);\n    console.log('');\n    console.log(pc.dim('No AI calls made (dry run).'));\n\n    if (docs.length === 0) {\n      console.log('');\n      console.log(pc.yellow('Warning: No AGENTS.md files found. Run `are generate` first or omit --dry-run to auto-generate.'));\n    } else if (estimatedTokensK > 150) {\n      console.log('');\n      console.log(pc.yellow('Warning: Input exceeds 150K tokens. Consider using a model with extended context.'));\n    }\n\n    return;\n  }\n\n  // Auto-generate if no AGENTS.md files exist\n  if (docs.length === 0) {\n    console.log(pc.yellow('No AGENTS.md files found. Running generate first...'));\n    await generateCommand(targetPath, {\n      debug: options.debug,\n      trace: options.trace,\n    });\n    docs = await collectAgentsDocs(absolutePath);\n    annexFiles = await collectAnnexFiles(absolutePath);\n    if (docs.length === 0) {\n      console.error(pc.red('Error: No AGENTS.md files found after generation. Cannot proceed.'));\n      process.exit(1);\n    }\n  }\n\n  // ---------------------------------------------------------------------------\n  // Resolve backend and run AI synthesis\n  // ---------------------------------------------------------------------------\n\n  const registry = createBackendRegistry();\n  let backend;\n  try {\n    backend = await resolveBackend(registry, options.backend ?? config.ai.backend);\n  } catch (error) {\n    if (error instanceof AIServiceError && error.code === 'CLI_NOT_FOUND') {\n      console.error(pc.red('Error: No AI CLI found.\\n'));\n      console.error(getInstallInstructions(registry));\n      process.exit(2);\n    }\n    throw error;\n  }\n\n  // Provision backend-specific resources (e.g., OpenCode agent config)\n  await backend.ensureProjectConfig?.(absolutePath);\n\n  // Resolve effective model: CLI flag > config override > opus default\n  // Specify benefits from the best model; upgrade default sonnet to opus\n  const effectiveModel = options.model\n    ?? (config.ai.model === 'sonnet' ? 'opus' : config.ai.model);\n\n  // Create trace writer\n  const tracer = createTraceWriter(absolutePath, options.trace ?? false);\n  if (options.trace && tracer.filePath) {\n    console.error(pc.dim(`[trace] Writing to ${tracer.filePath}`));\n  }\n\n  // Debug: log backend info\n  if (options.debug) {\n    console.error(pc.dim(`[debug] Backend: ${backend.name}`));\n    console.error(pc.dim(`[debug] CLI command: ${backend.cliCommand}`));\n    console.error(pc.dim(`[debug] Model: ${effectiveModel}`));\n  }\n\n  // Create AI service with extended timeout (spec generation takes longer)\n  const aiService = new AIService(backend, {\n    timeoutMs: Math.max(config.ai.timeoutMs, 900_000),\n    maxRetries: config.ai.maxRetries,\n    model: effectiveModel,\n    command: 'specify',\n    telemetry: { keepRuns: config.ai.telemetry.keepRuns },\n  }, consoleLogger);\n\n  if (options.debug) {\n    aiService.setDebug(true);\n  }\n\n  // Wire tracer into AIService for subprocess/retry trace events\n  if (options.trace) {\n    aiService.setTracer(tracer);\n    const logDir = path.join(\n      absolutePath, '.agents-reverse-engineer', 'subprocess-logs',\n      new Date().toISOString().replace(/[:.]/g, '-'),\n    );\n    aiService.setSubprocessLogDir(logDir);\n    console.error(pc.dim(`[trace] Subprocess logs → ${logDir}`));\n  }\n\n  // Build prompt from collected docs and annex files\n  const prompt = buildSpecPrompt(docs, annexFiles.length > 0 ? annexFiles : undefined);\n\n  if (options.debug) {\n    console.error(pc.dim(`[debug] System prompt: ${prompt.system.length} chars`));\n    console.error(pc.dim(`[debug] User prompt: ${prompt.user.length} chars`));\n  }\n\n  // Create progress log for tail -f monitoring\n  const progressLog = ProgressLog.create(absolutePath);\n  progressLog.write(`=== ARE Specify (${new Date().toISOString()}) ===`);\n  progressLog.write(`Project: ${absolutePath}`);\n  progressLog.write(`AGENTS.md files: ${docs.length}`);\n  progressLog.write(`Annex files: ${annexFiles.length}`);\n  progressLog.write('');\n\n  console.log(pc.bold('Generating specification...'));\n  console.log(pc.dim('This may take several minutes depending on project size.'));\n  progressLog.write('Generating specification...');\n\n  tracer.emit({ type: 'phase:start', phase: 'specify', taskCount: 1, concurrency: 1 });\n  const specifyStart = Date.now();\n\n  const response = await aiService.call({\n    prompt: prompt.user,\n    systemPrompt: prompt.system,\n    taskLabel: 'specify',\n  });\n\n  tracer.emit({\n    type: 'phase:end',\n    phase: 'specify',\n    durationMs: Date.now() - specifyStart,\n    tasksCompleted: 1,\n    tasksFailed: 0,\n  });\n\n  // ---------------------------------------------------------------------------\n  // Write output\n  // ---------------------------------------------------------------------------\n\n  try {\n    const writtenFiles = await writeSpec(response.text, {\n      outputPath,\n      force: options.force ?? false,\n      multiFile: options.multiFile ?? false,\n    });\n\n    console.log('');\n    console.log(pc.green(pc.bold('Specification written successfully:')));\n    for (const file of writtenFiles) {\n      console.log(pc.green(`  ${file}`));\n      progressLog.write(`Written: ${file}`);\n    }\n  } catch (error) {\n    if (error instanceof SpecExistsError) {\n      progressLog.write(`Error: ${error.message}`);\n      await progressLog.finalize();\n      console.error(pc.red(error.message));\n      process.exit(1);\n    }\n    throw error;\n  }\n\n  // ---------------------------------------------------------------------------\n  // Finalize telemetry\n  // ---------------------------------------------------------------------------\n\n  const { summary } = await aiService.finalize(absolutePath);\n\n  const summaryLine = `Tokens: ${summary.totalInputTokens} in / ${summary.totalOutputTokens} out` +\n    ` | Duration: ${(summary.totalDurationMs / 1000).toFixed(1)}s` +\n    ` | Output: ${outputPath}`;\n  console.log('');\n  console.log(pc.dim(summaryLine));\n  progressLog.write(summaryLine);\n  await progressLog.finalize();\n  await tracer.finalize();\n  if (options.trace) {\n    await cleanupOldTraces(absolutePath);\n  }\n}\n\n```\n\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n- Workflow & convention rules: if this file defines contribution guidelines, PR conventions, commit standards, testing mandates, tool usage requirements, approval workflows, AI agent behavioral instructions, or code conventions (naming standards, formatting rules, import ordering, linting policies), extract them as explicit, actionable rules. Sources: CONTRIBUTING.md, CI configs, PR templates, style guides, linter/formatter configs (.eslintrc, .prettierrc, .editorconfig), tsconfig strictness, README dev sections.\n\n\nTARGET LENGTH (MANDATORY):\n- Source file: 10572 characters\n- Target summary: ~1057 characters (10% compression)\n- Maximum: 1268 characters\n- Achieve this by: ultra-dense writing, minimal examples, single-sentence descriptions\n\nULTRA-COMPRESSION TECHNIQUES:\n- Use telegraphic style: \"exports foo, bar, baz\" not \"this file exports three functions\"\n- No code examples unless critical to understanding\n- One sentence per export maximum\n- Omit obvious parameter names (\"opts: Options\" not \"options: Options - configuration object\")\n- Use abbreviations: \"params\" not \"parameters\", \"config\" not \"configuration\"\n- Skip section if empty (don't write \"No special concerns\" or \"N/A\")\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nWORKFLOW & CONVENTION RULES (NEVER EXCLUDE):\n- Contribution requirements: branch naming, commit message formats, PR description templates\n- Testing mandates: required test types, coverage thresholds, \"never delete tests\" policies\n- Tool usage rules: required CLI commands, browser automation tools, linter/formatter requirements\n- Approval workflows: review requirements, CI checks that must pass, deployment gates\n- AI agent instructions: explicit directives for automated assistants (coding rules, prohibited actions)\n- Code conventions: naming standards (camelCase, PascalCase, snake_case for what), import ordering rules, file/folder naming patterns, formatting rules (indentation, quotes, semicolons), language idioms, TypeScript strictness policies, preferred patterns vs anti-patterns\nThese define how contributors (human or AI) must work within the project.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion .annex.sum file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**specify.ts implements the CLI `specify` command, which synthesizes AGENTS.md documentation into a comprehensive project specification via AI.**\n\n## Exported Interface & Command\n\n`SpecifyOptions` interface: properties `output` (string), `force` (boolean), `dryRun` (boolean), `multiFile` (boolean), `debug` (boolean), `trace` (boolean), `model` (string), `backend` (string) — all optional.\n\n`specifyCommand(targetPath: string, options: SpecifyOptions): Promise<void>` — main entry point; resolves project root, checks for conflicts, loads config, collects AGENTS.md + annex files, optionally dry-runs or auto-generates docs, resolves AI backend, builds synthesis prompt, calls AIService, writes output files (single or multi-file), finalizes telemetry & tracing.\n\n## Workflow & Integration Points\n\n**Early conflict detection**: checks `specs/SPEC.md` (or `options.output`) for existence unless `--force` or `--dry-run` is set; scans output directory for `.md` files in multi-file mode; exits with code 1 if conflicts found.\n\n**Auto-generation trigger**: if `collectAgentsDocs()` returns empty array, automatically runs `generateCommand()` before proceeding; re-collects after generation; exits with code 1 if still empty.\n\n**Model resolution cascade**: `options.model` > `config.ai.model` (upgraded from 'sonnet' to 'opus') > default 'opus'. Specify command defaults to opus model for quality.\n\n**Backend resolution**: calls `resolveBackend(registry, options.backend ?? config.ai.backend)`; catches `AIServiceError` with code `'CLI_NOT_FOUND'`, logs install instructions via `getInstallInstructions()`, exits with code 2. Calls `backend.ensureProjectConfig?.(absolutePath)` if defined.\n\n**AIService configuration**: timeout fixed at `Math.max(config.ai.timeoutMs, 900_000)` ms (min 900s for spec generation); uses `config.ai.maxRetries`, config telemetry `keepRuns` setting.\n\n## Tracing, Debugging & Progress Monitoring\n\n**Tracer integration**: `createTraceWriter(absolutePath, options.trace)` writes to `.agents-reverse-engineer/subprocess-logs/{ISO-timestamp}/`; emits `phase:start` and `phase:end` events with `taskCount: 1, concurrency: 1`; `aiService.setTracer(tracer)` wires subprocess/retry events; `cleanupOldTraces()` called at finish.\n\n**Progress log**: `ProgressLog.create(absolutePath)` writes to monitoring-friendly log with startup metadata, AI request lifecycle events, output files written, token/duration summary.\n\n**Debug mode** (`--debug`): logs backend name, CLI command, effective model, prompt sizes (chars).\n\n## Input Collection & Prompt Building\n\n`collectAgentsDocs(absolutePath)` returns array of docs with `content` property; `collectAnnexFiles(absolutePath)` returns reproduction-critical content array. `buildSpecPrompt(docs, annexFiles?)` constructs `{ system: string, user: string }` prompt object.\n\n**Dry-run statistics**: calculates `totalChars / 4` as token estimate, displays file counts, token estimate (K), output path, mode (single/multi).\n\n## Output Writing\n\n`writeSpec(response.text, { outputPath, force, multiFile })` writes specification; returns array of written file paths. Throws `SpecExistsError` (caught, logged, exits code 1) if output exists and `force: false`. Multi-file mode splits into multiple `.md` files in output directory.\n\n## Dependencies & Logging\n\nUses `picocolors` (pc) for colored terminal output: `pc.red()`, `pc.green()`, `pc.bold()`, `pc.dim()`, `pc.cyan()`, `pc.yellow()`. Uses `consoleLogger` for AIService debug logging. Loads config via `loadConfig(absolutePath, { debug })`.",
      "model": "claude-haiku-4-5-20251001",
      "inputTokens": 3,
      "outputTokens": 959,
      "cacheReadTokens": 16147,
      "cacheCreationTokens": 5837,
      "latencyMs": 27672,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/cli/specify.ts",
          "sizeBytes": 10578
        }
      ]
    },
    {
      "timestamp": "2026-02-14T13:46:59.584Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: src/config/defaults.ts\n\n```typescript\n/**\n * Default configuration values for agents-reverse\n */\n\nimport os from 'node:os';\n\n/** Multiplier applied to CPU core count for default concurrency */\nconst CONCURRENCY_MULTIPLIER = 5;\n\n/** Minimum default concurrency */\nconst MIN_CONCURRENCY = 2;\n\n/** Maximum default concurrency (matches schema .max(20)) */\nconst MAX_CONCURRENCY = 20;\n\n/** Heap budget per subprocess in GB (matches NODE_OPTIONS --max-old-space-size=512) */\nconst SUBPROCESS_HEAP_GB = 0.512;\n\n/** Fraction of total system memory to allocate to subprocesses */\nconst MEMORY_FRACTION = 0.5;\n\n/**\n * Compute the default concurrency based on available CPU cores and system memory.\n *\n * Formula: clamp(cores * 5, MIN, min(memCap, MAX))\n * - cores: os.availableParallelism() or os.cpus().length\n * - memCap: floor(totalMemGB * 0.5 / 0.512) — use at most 50% of RAM for subprocesses\n *\n * @returns Default concurrency value (integer between MIN_CONCURRENCY and MAX_CONCURRENCY)\n */\nexport function getDefaultConcurrency(): number {\n  const cores = typeof os.availableParallelism === 'function'\n    ? os.availableParallelism()\n    : (os.cpus().length || MIN_CONCURRENCY);\n\n  const totalMemGB = os.totalmem() / (1024 ** 3);\n  const memCap = totalMemGB > 1\n    ? Math.floor((totalMemGB * MEMORY_FRACTION) / SUBPROCESS_HEAP_GB)\n    : Infinity;\n\n  const computed = cores * CONCURRENCY_MULTIPLIER;\n  return Math.max(MIN_CONCURRENCY, Math.min(computed, memCap, MAX_CONCURRENCY));\n}\n\n/**\n * Default vendor directories to exclude from analysis.\n * These are typically package managers, build outputs, or version control directories.\n */\nexport const DEFAULT_VENDOR_DIRS = [\n  'node_modules',\n  'vendor',\n  '.git',\n  'dist',\n  'build',\n  '__pycache__',\n  '.next',\n  'venv',\n  '.venv',\n  'target',\n  '.cargo',\n  '.gradle',\n  // AI assistant tooling directories\n  '.agents-reverse-engineer',\n  '.agents',\n  '.planning',\n  '.claude',\n  '.codex',\n  '.opencode',\n  '.gemini',\n] as const;\n\n/**\n * Default file patterns to exclude from analysis.\n * These patterns use gitignore syntax and are matched by the custom filter.\n */\nexport const DEFAULT_EXCLUDE_PATTERNS = [\n  // AI assistant documentation files\n  'AGENTS.md',\n  'AGENTS.override.md',\n  'CLAUDE.md',\n  'OPENCODE.md',\n  'GEMINI.md',\n  '**/AGENTS.md',\n  '**/AGENTS.override.md',\n  '**/CLAUDE.md',\n  '**/OPENCODE.md',\n  '**/GEMINI.md',\n  '*.local.md',\n  '**/*.local.md',\n  // Eval variant AGENTS files (e.g., AGENTS.claude.haiku.md)\n  'AGENTS.*.md',\n  '**/AGENTS.*.md',\n  // Lock files (not useful for documentation, can be very large)\n  '*.lock',\n  'package-lock.json',\n  'yarn.lock',\n  'pnpm-lock.yaml',\n  'bun.lock',\n  'bun.lockb',\n  'Gemfile.lock',\n  'Cargo.lock',\n  'poetry.lock',\n  'composer.lock',\n  'go.sum',\n  // Dotfiles and generated artifacts (path.extname returns '' for dotfiles,\n  // so these must be matched as glob patterns, not binary extensions)\n  '.gitignore',\n  '.gitattributes',\n  '.gitkeep',\n  '.env',\n  '**/.env',\n  '**/.env.*',\n  '*.log',\n  '*.sum',\n  '**/*.sum',\n  '**/SKILL.md',\n] as const;\n\n/**\n * Default binary file extensions to exclude from analysis.\n * These files cannot be meaningfully analyzed as text.\n */\nexport const DEFAULT_BINARY_EXTENSIONS = [\n  // Images\n  '.png',\n  '.jpg',\n  '.jpeg',\n  '.gif',\n  '.bmp',\n  '.ico',\n  '.webp',\n  // Archives\n  '.zip',\n  '.tar',\n  '.gz',\n  '.rar',\n  '.7z',\n  // Executables\n  '.exe',\n  '.dll',\n  '.so',\n  '.dylib',\n  // Media\n  '.mp3',\n  '.mp4',\n  '.wav',\n  // Documents\n  '.pdf',\n  // Fonts\n  '.woff',\n  '.woff2',\n  '.ttf',\n  '.eot',\n  // Compiled\n  '.class',\n  '.pyc',\n] as const;\n\n/**\n * Default maximum file size in bytes (1MB).\n * Files larger than this will be skipped with a warning.\n */\nexport const DEFAULT_MAX_FILE_SIZE = 1024 * 1024;\n\n/**\n * Default compression ratio for .sum files (0.25 = 25% of source size).\n * This produces balanced summaries that preserve important details while\n * reducing token usage. Lower values enable aggressive compression.\n */\nexport const DEFAULT_COMPRESSION_RATIO = 0.25;\n\n/**\n * Default configuration object matching the schema structure.\n * This is used when no config file is present or for missing fields.\n */\nexport const DEFAULT_CONFIG = {\n  exclude: {\n    patterns: [...DEFAULT_EXCLUDE_PATTERNS],\n    vendorDirs: [...DEFAULT_VENDOR_DIRS],\n    binaryExtensions: [...DEFAULT_BINARY_EXTENSIONS],\n  },\n  options: {\n    followSymlinks: false,\n    maxFileSize: DEFAULT_MAX_FILE_SIZE,\n  },\n  output: {\n    colors: true,\n  },\n  generation: {\n    compressionRatio: DEFAULT_COMPRESSION_RATIO,\n  },\n} as const;\n\n```\n\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n- Workflow & convention rules: if this file defines contribution guidelines, PR conventions, commit standards, testing mandates, tool usage requirements, approval workflows, AI agent behavioral instructions, or code conventions (naming standards, formatting rules, import ordering, linting policies), extract them as explicit, actionable rules. Sources: CONTRIBUTING.md, CI configs, PR templates, style guides, linter/formatter configs (.eslintrc, .prettierrc, .editorconfig), tsconfig strictness, README dev sections.\n\n\nTARGET LENGTH (MANDATORY):\n- Source file: 4535 characters\n- Target summary: ~454 characters (10% compression)\n- Maximum: 545 characters\n- Achieve this by: ultra-dense writing, minimal examples, single-sentence descriptions\n\nULTRA-COMPRESSION TECHNIQUES:\n- Use telegraphic style: \"exports foo, bar, baz\" not \"this file exports three functions\"\n- No code examples unless critical to understanding\n- One sentence per export maximum\n- Omit obvious parameter names (\"opts: Options\" not \"options: Options - configuration object\")\n- Use abbreviations: \"params\" not \"parameters\", \"config\" not \"configuration\"\n- Skip section if empty (don't write \"No special concerns\" or \"N/A\")\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nWORKFLOW & CONVENTION RULES (NEVER EXCLUDE):\n- Contribution requirements: branch naming, commit message formats, PR description templates\n- Testing mandates: required test types, coverage thresholds, \"never delete tests\" policies\n- Tool usage rules: required CLI commands, browser automation tools, linter/formatter requirements\n- Approval workflows: review requirements, CI checks that must pass, deployment gates\n- AI agent instructions: explicit directives for automated assistants (coding rules, prohibited actions)\n- Code conventions: naming standards (camelCase, PascalCase, snake_case for what), import ordering rules, file/folder naming patterns, formatting rules (indentation, quotes, semicolons), language idioms, TypeScript strictness policies, preferred patterns vs anti-patterns\nThese define how contributors (human or AI) must work within the project.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion .annex.sum file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**src/config/defaults.ts exports default configuration values and concurrency calculation for agents-reverse-engineer.**\n\n## Exports\n\n`getDefaultConcurrency()` → number: Computes concurrency by clamping `cores * CONCURRENCY_MULTIPLIER` (5) against MIN_CONCURRENCY (2), MAX_CONCURRENCY (20), and memory cap derived from `(totalMemGB * MEMORY_FRACTION) / SUBPROCESS_HEAP_GB` (50% of RAM ÷ 0.512 GB per subprocess).\n\n`DEFAULT_VENDOR_DIRS` → readonly string[]: Directories to exclude: `node_modules`, `vendor`, `.git`, `dist`, `build`, `__pycache__`, `.next`, `venv`, `.venv`, `target`, `.cargo`, `.gradle`, `.agents-reverse-engineer`, `.agents`, `.planning`, `.claude`, `.codex`, `.opencode`, `.gemini`.\n\n`DEFAULT_EXCLUDE_PATTERNS` → readonly string[]: Gitignore-syntax patterns excluding AI assistant docs (AGENTS.md, CLAUDE.md, OPENCODE.md, GEMINI.md, variants with `AGENTS.*.md`), lock files (package-lock.json, yarn.lock, pnpm-lock.yaml, Cargo.lock, poetry.lock, go.sum, etc.), dotfiles (.env, .gitignore, .gitattributes), logs (*.log), and summaries (*.sum, **/SKILL.md).\n\n`DEFAULT_BINARY_EXTENSIONS` → readonly string[]: Image (`.png`, `.jpg`, `.gif`, `.webp`), archive (`.zip`, `.tar`, `.gz`), executable (`.exe`, `.dll`, `.so`, `.dylib`), media (`.mp3`, `.mp4`), document (`.pdf`), font (`.woff`, `.ttf`), and compiled (`.class`, `.pyc`) extensions.\n\n`DEFAULT_MAX_FILE_SIZE` → 1048576: 1 MB file size limit.\n\n`DEFAULT_COMPRESSION_RATIO` → 0.25: Target 25% compression for .sum files.\n\n`DEFAULT_CONFIG` → object: Merges all defaults with structure: `exclude.patterns`, `exclude.vendorDirs`, `exclude.binaryExtensions`, `options.followSymlinks` (false), `options.maxFileSize`, `output.colors` (true), `generation.compressionRatio`.\n\n## Constants & Thresholds\n\nCONCURRENCY_MULTIPLIER = 5; MIN_CONCURRENCY = 2; MAX_CONCURRENCY = 20; SUBPROCESS_HEAP_GB = 0.512 GB; MEMORY_FRACTION = 0.5.",
      "model": "claude-haiku-4-5-20251001",
      "inputTokens": 3,
      "outputTokens": 632,
      "cacheReadTokens": 16147,
      "cacheCreationTokens": 4304,
      "latencyMs": 21859,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/config/defaults.ts",
          "sizeBytes": 4537
        }
      ]
    },
    {
      "timestamp": "2026-02-14T13:47:01.692Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: src/config/schema.ts\n\n```typescript\n/**\n * Zod schema for configuration validation\n *\n * This schema defines the structure of `.agents-reverse/config.yaml`\n * and provides sensible defaults for all fields.\n */\n\nimport { z } from 'zod';\nimport {\n  DEFAULT_VENDOR_DIRS,\n  DEFAULT_BINARY_EXTENSIONS,\n  DEFAULT_MAX_FILE_SIZE,\n  DEFAULT_EXCLUDE_PATTERNS,\n  getDefaultConcurrency,\n} from './defaults.js';\n\n/**\n * Schema for exclusion configuration\n */\nconst ExcludeSchema = z.object({\n  /** Custom glob patterns to exclude (e.g., [\"*.log\", \"temp/**\"]) */\n  patterns: z.array(z.string()).default([...DEFAULT_EXCLUDE_PATTERNS]),\n  /** Vendor directories to exclude */\n  vendorDirs: z.array(z.string()).default([...DEFAULT_VENDOR_DIRS]),\n  /** Binary file extensions to exclude */\n  binaryExtensions: z.array(z.string()).default([...DEFAULT_BINARY_EXTENSIONS]),\n}).default({});\n\n/**\n * Schema for options configuration\n */\nconst OptionsSchema = z.object({\n  /** Whether to follow symbolic links during traversal */\n  followSymlinks: z.boolean().default(false),\n  /** Maximum file size in bytes (files larger than this are skipped) */\n  maxFileSize: z.number().positive().default(DEFAULT_MAX_FILE_SIZE),\n}).default({});\n\n/**\n * Schema for output configuration\n */\nconst OutputSchema = z.object({\n  /** Whether to use colors in terminal output */\n  colors: z.boolean().default(true),\n}).default({});\n\n/**\n * Schema for generation configuration.\n *\n * Controls documentation generation behavior including compression ratio\n * for .sum files. All fields have sensible defaults.\n */\nconst GenerationSchema = z.object({\n  /**\n   * Target compression ratio for .sum files (0.1-1.0, default 0.25).\n   *\n   * - 0.10 = very concise (aggressive compression, ~10% of source size)\n   * - 0.25 = standard (default, ~25% of source size)\n   * - 0.50 = detailed (verbose, ~50% of source size)\n   *\n   * Lower ratios produce more compact summaries but may omit some details.\n   * The annex mechanism bypasses compression for reproduction-critical content.\n   */\n  compressionRatio: z.number().min(0.1).max(1.0).default(0.25),\n  /**\n   * Active eval variant (e.g., \"claude.haiku\").\n   * When set, AGENTS.md hub files will reference this variant.\n   * Set automatically by `--eval` flag.\n   */\n  activeVariant: z.string().optional(),\n}).default({});\n\n/**\n * Schema for AI service configuration.\n *\n * Controls backend selection, model, timeout, retry behavior, and\n * telemetry log retention. All fields have sensible defaults.\n */\nconst AISchema = z.object({\n  /** AI CLI backend to use ('auto' detects from PATH) */\n  backend: z.enum(['claude', 'codex', 'gemini', 'opencode', 'auto']).default('auto'),\n  /** Model identifier (backend-specific, e.g., \"sonnet\", \"opus\") */\n  model: z.string().default('sonnet'),\n  /** Default subprocess timeout in milliseconds */\n  timeoutMs: z.number().positive().default(300_000),\n  /** Maximum number of retries for transient errors */\n  maxRetries: z.number().min(0).default(3),\n  /** Default parallelism for concurrent AI calls (1-20). Auto-detected from CPU cores and available memory. */\n  concurrency: z.number().min(1).max(20).default(getDefaultConcurrency),\n  /** Telemetry settings */\n  telemetry: z.object({\n    /** Number of most recent run logs to keep on disk */\n    keepRuns: z.number().min(0).default(50),\n  }).default({}),\n}).default({});\n\n/**\n * Main configuration schema for agents-reverse.\n *\n * All fields have sensible defaults, so an empty object `{}` is valid\n * and will result in a fully populated configuration.\n *\n * @example\n * ```typescript\n * // Parse with defaults\n * const config = ConfigSchema.parse({});\n *\n * // Parse with partial overrides\n * const config = ConfigSchema.parse({\n *   exclude: { patterns: ['*.log'] },\n *   ai: { backend: 'claude', model: 'opus' },\n * });\n * ```\n */\nexport const ConfigSchema = z.object({\n  /** Exclusion rules for files and directories */\n  exclude: ExcludeSchema,\n  /** Discovery options */\n  options: OptionsSchema,\n  /** Output formatting options */\n  output: OutputSchema,\n  /** Generation settings */\n  generation: GenerationSchema,\n  /** AI service configuration */\n  ai: AISchema,\n}).default({});\n\n/**\n * Inferred TypeScript type from the schema.\n * Use this type for function parameters and return types.\n */\nexport type Config = z.infer<typeof ConfigSchema>;\n\n/**\n * Type for the exclude section of config\n */\nexport type ExcludeConfig = z.infer<typeof ExcludeSchema>;\n\n/**\n * Type for the options section of config\n */\nexport type OptionsConfig = z.infer<typeof OptionsSchema>;\n\n/**\n * Type for the output section of config\n */\nexport type OutputConfig = z.infer<typeof OutputSchema>;\n\n/**\n * Type for the generation section of config\n */\nexport type GenerationConfig = z.infer<typeof GenerationSchema>;\n\n/**\n * Type for the AI service section of config\n */\nexport type AIConfig = z.infer<typeof AISchema>;\n\n```\n\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n- Workflow & convention rules: if this file defines contribution guidelines, PR conventions, commit standards, testing mandates, tool usage requirements, approval workflows, AI agent behavioral instructions, or code conventions (naming standards, formatting rules, import ordering, linting policies), extract them as explicit, actionable rules. Sources: CONTRIBUTING.md, CI configs, PR templates, style guides, linter/formatter configs (.eslintrc, .prettierrc, .editorconfig), tsconfig strictness, README dev sections.\n\n\nTARGET LENGTH (MANDATORY):\n- Source file: 4858 characters\n- Target summary: ~486 characters (10% compression)\n- Maximum: 583 characters\n- Achieve this by: ultra-dense writing, minimal examples, single-sentence descriptions\n\nULTRA-COMPRESSION TECHNIQUES:\n- Use telegraphic style: \"exports foo, bar, baz\" not \"this file exports three functions\"\n- No code examples unless critical to understanding\n- One sentence per export maximum\n- Omit obvious parameter names (\"opts: Options\" not \"options: Options - configuration object\")\n- Use abbreviations: \"params\" not \"parameters\", \"config\" not \"configuration\"\n- Skip section if empty (don't write \"No special concerns\" or \"N/A\")\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nWORKFLOW & CONVENTION RULES (NEVER EXCLUDE):\n- Contribution requirements: branch naming, commit message formats, PR description templates\n- Testing mandates: required test types, coverage thresholds, \"never delete tests\" policies\n- Tool usage rules: required CLI commands, browser automation tools, linter/formatter requirements\n- Approval workflows: review requirements, CI checks that must pass, deployment gates\n- AI agent instructions: explicit directives for automated assistants (coding rules, prohibited actions)\n- Code conventions: naming standards (camelCase, PascalCase, snake_case for what), import ordering rules, file/folder naming patterns, formatting rules (indentation, quotes, semicolons), language idioms, TypeScript strictness policies, preferred patterns vs anti-patterns\nThese define how contributors (human or AI) must work within the project.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion .annex.sum file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**src/config/schema.ts defines Zod validation schemas for .agents-reverse/config.yaml with sensible defaults and exports Config type inferred from ConfigSchema.**\n\n## Exports\n\n- `ConfigSchema` — Zod object schema validating root config; all fields default to empty objects or DEFAULT_* values.\n- `Config` — TypeScript type inferred from ConfigSchema via `z.infer<typeof ConfigSchema>`.\n- `ExcludeSchema`, `ExcludeConfig` — Schema and type for exclude section (patterns, vendorDirs, binaryExtensions arrays).\n- `OptionsSchema`, `OptionsConfig` — Schema and type for options section (followSymlinks: boolean, maxFileSize: number).\n- `OutputSchema`, `OutputConfig` — Schema and type for output section (colors: boolean).\n- `GenerationSchema`, `GenerationConfig` — Schema and type for generation section (compressionRatio: 0.1–1.0, activeVariant: optional string).\n- `AISchema`, `AIConfig` — Schema and type for AI service section (backend enum, model string, timeoutMs, maxRetries, concurrency 1–20, telemetry.keepRuns).\n\n## Configuration Contracts\n\n**compressionRatio** defaults to 0.25 (25% of source); range 0.1–1.0 controls .sum file verbosity; 0.1 = aggressive compression, 0.5 = verbose.\n\n**backend** enum: `['claude', 'codex', 'gemini', 'opencode', 'auto']`; 'auto' detects from PATH.\n\n**concurrency** bounds: 1–20; auto-detected via `getDefaultConcurrency()` if omitted.\n\n**timeoutMs** default: `300_000` (300 seconds for subprocess execution).\n\n**maxRetries** default: 3 for transient errors.\n\n**telemetry.keepRuns** default: 50 (retained run logs on disk).\n\n## Dependencies\n\nImports `DEFAULT_VENDOR_DIRS`, `DEFAULT_BINARY_EXTENSIONS`, `DEFAULT_MAX_FILE_SIZE`, `DEFAULT_EXCLUDE_PATTERNS`, `getDefaultConcurrency` from `./defaults.js`.",
      "model": "claude-haiku-4-5-20251001",
      "inputTokens": 3,
      "outputTokens": 521,
      "cacheReadTokens": 16147,
      "cacheCreationTokens": 4184,
      "latencyMs": 19700,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/config/schema.ts",
          "sizeBytes": 4858
        }
      ]
    },
    {
      "timestamp": "2026-02-14T13:47:03.431Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: src/discovery/run.ts\n\n```typescript\n/**\n * High-level file discovery pipeline.\n *\n * Combines directory walking and filter application into a single function\n * that all commands can share.\n */\n\nimport { walkDirectory } from './walker.js';\nimport {\n  applyFilters,\n  createGitignoreFilter,\n  createVendorFilter,\n  createBinaryFilter,\n  createCustomFilter,\n} from './filters/index.js';\nimport type { FilterResult } from './types.js';\nimport type { ITraceWriter } from '../orchestration/trace.js';\n\n/**\n * Configuration subset needed for file discovery.\n * Structurally compatible with the full Config type from config/schema.ts.\n */\nexport interface DiscoveryConfig {\n  exclude: {\n    vendorDirs: string[];\n    binaryExtensions: string[];\n    patterns: string[];\n  };\n  options: {\n    maxFileSize: number;\n    followSymlinks: boolean;\n  };\n}\n\n/**\n * Options for the discovery pipeline.\n */\nexport interface DiscoverFilesOptions {\n  tracer?: ITraceWriter;\n  debug?: boolean;\n}\n\n/**\n * Discover files in a directory by walking and applying the standard filter chain.\n *\n * Creates the four standard filters (gitignore, vendor, binary, custom),\n * walks the directory tree, and applies the filters. Returns the full\n * FilterResult so callers can access both included and excluded files\n * with filter attribution.\n *\n * @param root - Absolute path to the directory to scan\n * @param config - Discovery-related configuration (exclude rules and options)\n * @param options - Optional tracing and debug settings\n * @returns FilterResult with included and excluded file lists\n */\nexport async function discoverFiles(\n  root: string,\n  config: DiscoveryConfig,\n  options?: DiscoverFilesOptions,\n): Promise<FilterResult> {\n  // Create filters in standard order\n  const gitignoreFilter = await createGitignoreFilter(root);\n  const vendorFilter = createVendorFilter(config.exclude.vendorDirs);\n  const binaryFilter = createBinaryFilter({\n    maxFileSize: config.options.maxFileSize,\n    additionalExtensions: config.exclude.binaryExtensions,\n  });\n  const customFilter = createCustomFilter(config.exclude.patterns, root);\n  const filters = [gitignoreFilter, vendorFilter, binaryFilter, customFilter];\n\n  // Walk directory\n  const files = await walkDirectory({\n    cwd: root,\n    followSymlinks: config.options.followSymlinks,\n  });\n\n  // Apply filters and return\n  return applyFilters(files, filters, {\n    tracer: options?.tracer,\n    debug: options?.debug,\n  });\n}\n\n```\n\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n- Workflow & convention rules: if this file defines contribution guidelines, PR conventions, commit standards, testing mandates, tool usage requirements, approval workflows, AI agent behavioral instructions, or code conventions (naming standards, formatting rules, import ordering, linting policies), extract them as explicit, actionable rules. Sources: CONTRIBUTING.md, CI configs, PR templates, style guides, linter/formatter configs (.eslintrc, .prettierrc, .editorconfig), tsconfig strictness, README dev sections.\n\n\nTARGET LENGTH (MANDATORY):\n- Source file: 2419 characters\n- Target summary: ~242 characters (10% compression)\n- Maximum: 290 characters\n- Achieve this by: ultra-dense writing, minimal examples, single-sentence descriptions\n\nULTRA-COMPRESSION TECHNIQUES:\n- Use telegraphic style: \"exports foo, bar, baz\" not \"this file exports three functions\"\n- No code examples unless critical to understanding\n- One sentence per export maximum\n- Omit obvious parameter names (\"opts: Options\" not \"options: Options - configuration object\")\n- Use abbreviations: \"params\" not \"parameters\", \"config\" not \"configuration\"\n- Skip section if empty (don't write \"No special concerns\" or \"N/A\")\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nWORKFLOW & CONVENTION RULES (NEVER EXCLUDE):\n- Contribution requirements: branch naming, commit message formats, PR description templates\n- Testing mandates: required test types, coverage thresholds, \"never delete tests\" policies\n- Tool usage rules: required CLI commands, browser automation tools, linter/formatter requirements\n- Approval workflows: review requirements, CI checks that must pass, deployment gates\n- AI agent instructions: explicit directives for automated assistants (coding rules, prohibited actions)\n- Code conventions: naming standards (camelCase, PascalCase, snake_case for what), import ordering rules, file/folder naming patterns, formatting rules (indentation, quotes, semicolons), language idioms, TypeScript strictness policies, preferred patterns vs anti-patterns\nThese define how contributors (human or AI) must work within the project.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion .annex.sum file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**run.ts orchestrates the file discovery pipeline by composing directory walking with sequential filter application to identify includable vs. excludable files.**\n\n## Exports\n\n`discoverFiles(root: string, config: DiscoveryConfig, options?: DiscoverFilesOptions): Promise<FilterResult>` — walks directory tree from `root`, instantiates four filters (gitignore, vendor, binary, custom) in strict order, applies them sequentially via `applyFilters()`, returns `FilterResult` with included/excluded file lists and filter attribution.\n\n`DiscoveryConfig` interface — config subset with `exclude.vendorDirs[]`, `exclude.binaryExtensions[]`, `exclude.patterns[]`, `options.maxFileSize`, `options.followSymlinks`.\n\n`DiscoverFilesOptions` interface — optional `tracer?: ITraceWriter`, `debug?: boolean` for pipeline instrumentation.\n\n## Dependencies & Integration\n\nImports `walkDirectory` from `./walker.js` (directory traversal); imports `applyFilters`, `createGitignoreFilter`, `createVendorFilter`, `createBinaryFilter`, `createCustomFilter` from `./filters/index.js` (filter composition); imports `FilterResult` from `./types.js` (return type); imports `ITraceWriter` from `../orchestration/trace.js` (trace instrumentation interface).\n\n## Filter Pipeline Architecture\n\n`discoverFiles()` instantiates filters in fixed order: gitignore (async) → vendor → binary → custom. All four filters are composed into array and passed to `applyFilters()`. Binary filter receives `maxFileSize` and `additionalExtensions` config tuple. Custom filter receives patterns array and root path. Gitignore filter is async-created; others are synchronous.",
      "model": "claude-haiku-4-5-20251001",
      "inputTokens": 3,
      "outputTokens": 407,
      "cacheReadTokens": 16147,
      "cacheCreationTokens": 3408,
      "latencyMs": 18560,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/discovery/run.ts",
          "sizeBytes": 2419
        }
      ]
    },
    {
      "timestamp": "2026-02-14T13:47:01.477Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: src/config/loader.ts\n\n```typescript\n/**\n * Configuration loader for agents-reverse\n *\n * Loads and validates configuration from `.agents-reverse/config.yaml`.\n * Returns sensible defaults when no config file exists.\n */\n\nimport { readFile, writeFile, mkdir, access } from 'node:fs/promises';\nimport { constants } from 'node:fs';\nimport path from 'node:path';\nimport { parse, stringify } from 'yaml';\nimport { ZodError } from 'zod';\nimport { ConfigSchema, Config } from './schema.js';\nimport type { Logger } from '../core/logger.js';\nimport { nullLogger } from '../core/logger.js';\nimport { DEFAULT_VENDOR_DIRS, DEFAULT_BINARY_EXTENSIONS, DEFAULT_MAX_FILE_SIZE, DEFAULT_EXCLUDE_PATTERNS, getDefaultConcurrency } from './defaults.js';\nimport type { ITraceWriter } from '../orchestration/trace.js';\n\n/**\n * Quote a string value for YAML output if it contains characters that\n * would be misinterpreted (e.g. `*` is the YAML alias indicator).\n */\nfunction yamlScalar(value: string): string {\n  // Characters that require quoting: *, {, }, [, ], ?, :, #, &, !, |, >, etc.\n  if (/[*{}\\[\\]?,:#&!|>'\"%@`]/.test(value)) {\n    return `\"${value.replace(/\\\\/g, '\\\\\\\\').replace(/\"/g, '\\\\\"')}\"`;\n  }\n  return value;\n}\n\n/** Directory name for agents-reverse-engineer configuration */\nexport const CONFIG_DIR = '.agents-reverse-engineer';\n\n/** Configuration file name */\nexport const CONFIG_FILE = 'config.yaml';\n\n/**\n * Walk up from `startDir` looking for an existing `.agents-reverse-engineer/` directory.\n * Returns the directory containing it, or `startDir` if none found.\n */\nexport async function findProjectRoot(startDir: string): Promise<string> {\n  let current = path.resolve(startDir);\n  while (true) {\n    try {\n      await access(path.join(current, CONFIG_DIR), constants.F_OK);\n      return current;\n    } catch {\n      // not found here, try parent\n    }\n    const parent = path.dirname(current);\n    if (parent === current) break;\n    current = parent;\n  }\n  return path.resolve(startDir);\n}\n\n/**\n * Error thrown when configuration parsing or validation fails\n */\nexport class ConfigError extends Error {\n  constructor(\n    message: string,\n    public readonly filePath: string,\n    public readonly cause?: Error\n  ) {\n    super(message);\n    this.name = 'ConfigError';\n  }\n}\n\n/**\n * Load configuration from `.agents-reverse/config.yaml`.\n *\n * If the file doesn't exist, returns default configuration.\n * If the file exists but is invalid, throws a ConfigError with details.\n *\n * @param root - Root directory containing `.agents-reverse/` folder\n * @param options - Optional configuration loading options\n * @param options.tracer - Trace writer for emitting config:loaded events\n * @param options.debug - Enable debug output for configuration loading\n * @returns Validated configuration object with all defaults applied\n * @throws ConfigError if the config file exists but is invalid\n *\n * @example\n * ```typescript\n * const config = await loadConfig('/path/to/project');\n * console.log(config.exclude.vendorDirs);\n * ```\n */\nexport async function loadConfig(\n  root: string,\n  options?: { tracer?: ITraceWriter; debug?: boolean; logger?: Logger }\n): Promise<Config> {\n  const configPath = path.join(root, CONFIG_DIR, CONFIG_FILE);\n\n  try {\n    const content = await readFile(configPath, 'utf-8');\n    const raw = parse(content);\n\n    try {\n      const config = ConfigSchema.parse(raw);\n\n      // Emit trace event\n      options?.tracer?.emit({\n        type: 'config:loaded',\n        configPath: path.relative(root, configPath),\n        model: config.ai.model,\n        concurrency: config.ai.concurrency,\n      });\n\n      // Debug output\n      if (options?.debug) {\n        const log = options.logger ?? nullLogger;\n        log.debug(`[debug] Config loaded from: ${path.relative(root, configPath)}`);\n        log.debug(`[debug] Model: ${config.ai.model}, Concurrency: ${config.ai.concurrency}`);\n      }\n\n      return config;\n    } catch (err) {\n      if (err instanceof ZodError) {\n        const issues = err.issues\n          .map((issue) => `  - ${issue.path.join('.')}: ${issue.message}`)\n          .join('\\n');\n        throw new ConfigError(\n          `Invalid configuration in ${configPath}:\\n${issues}`,\n          configPath,\n          err\n        );\n      }\n      throw err;\n    }\n  } catch (err) {\n    // File not found - return defaults\n    if ((err as NodeJS.ErrnoException).code === 'ENOENT') {\n      const config = ConfigSchema.parse({});\n\n      // Emit trace event for defaults\n      options?.tracer?.emit({\n        type: 'config:loaded',\n        configPath: '(defaults)',\n        model: config.ai.model,\n        concurrency: config.ai.concurrency,\n      });\n\n      // Debug output\n      if (options?.debug) {\n        const log = options.logger ?? nullLogger;\n        log.debug(`[debug] Config file not found, using defaults`);\n        log.debug(`[debug] Model: ${config.ai.model}, Concurrency: ${config.ai.concurrency}`);\n      }\n\n      return config;\n    }\n\n    // Re-throw ConfigError as-is\n    if (err instanceof ConfigError) {\n      throw err;\n    }\n\n    // YAML parse error\n    throw new ConfigError(\n      `Failed to parse ${configPath}: ${(err as Error).message}`,\n      configPath,\n      err as Error\n    );\n  }\n}\n\n/**\n * Check if a configuration file exists.\n *\n * @param root - Root directory to check\n * @returns true if `.agents-reverse/config.yaml` exists\n *\n * @example\n * ```typescript\n * if (!await configExists('.')) {\n *   console.log('Run `are init` to create configuration');\n * }\n * ```\n */\nexport async function configExists(root: string): Promise<boolean> {\n  const configPath = path.join(root, CONFIG_DIR, CONFIG_FILE);\n  try {\n    await access(configPath, constants.F_OK);\n    return true;\n  } catch {\n    return false;\n  }\n}\n\n/**\n * Write a default configuration file with helpful comments.\n *\n * Creates the `.agents-reverse/` directory if it doesn't exist.\n * The generated file includes comments explaining each option.\n *\n * @param root - Root directory where `.agents-reverse/` will be created\n *\n * @example\n * ```typescript\n * await writeDefaultConfig('/path/to/project');\n * // Creates /path/to/project/.agents-reverse/config.yaml\n * ```\n */\nexport async function writeDefaultConfig(root: string): Promise<void> {\n  const configDir = path.join(root, CONFIG_DIR);\n  const configPath = path.join(configDir, CONFIG_FILE);\n\n  // Create directory if needed\n  await mkdir(configDir, { recursive: true });\n\n  // Generate config content with comments\n  const configContent = `# agents-reverse-engineer configuration\n# See: https://github.com/your-org/agents-reverse-engineer\n\n# ============================================================================\n# FILE & DIRECTORY EXCLUSIONS\n# ============================================================================\nexclude:\n  # Custom glob patterns to exclude (e.g., [\"*.log\", \"temp/**\"])\n  # Default patterns exclude AI-generated documentation files\n  patterns:\n${DEFAULT_EXCLUDE_PATTERNS.map((pattern) => `    - ${yamlScalar(pattern)}`).join('\\n')}\n\n  # Vendor directories to exclude from analysis\n  # These are typically package managers, build outputs, or version control\n  vendorDirs:\n${DEFAULT_VENDOR_DIRS.map((dir) => `    - ${dir}`).join('\\n')}\n\n  # Binary file extensions to exclude from analysis\n  # These files cannot be meaningfully analyzed as text\n  binaryExtensions:\n${DEFAULT_BINARY_EXTENSIONS.map((ext) => `    - ${ext}`).join('\\n')}\n\n# ============================================================================\n# DISCOVERY OPTIONS\n# ============================================================================\noptions:\n  # Whether to follow symbolic links during traversal\n  followSymlinks: false\n\n  # Maximum file size in bytes (files larger than this are skipped)\n  # Default: ${DEFAULT_MAX_FILE_SIZE} (1MB)\n  maxFileSize: ${DEFAULT_MAX_FILE_SIZE}\n\n# ============================================================================\n# OUTPUT FORMATTING\n# ============================================================================\noutput:\n  # Whether to use colors in terminal output\n  colors: true\n\n# ============================================================================\n# GENERATION SETTINGS\n# ============================================================================\ngeneration:\n  # Target compression ratio for .sum files (0.1-1.0)\n  # - 0.10 = very concise (aggressive compression, ~10% of source size)\n  # - 0.25 = standard (default, ~25% of source size)\n  # - 0.50 = detailed (verbose, ~50% of source size)\n  # Lower values produce more compact summaries but may omit some details.\n  # The annex mechanism bypasses compression for reproduction-critical content.\n  compressionRatio: 0.25\n\n# ============================================================================\n# AI SERVICE CONFIGURATION\n# ============================================================================\nai:\n  # AI CLI backend to use\n  # Options: 'claude', 'codex', 'gemini', 'opencode', 'auto' (auto-detect from PATH)\n  backend: auto\n\n  # Model identifier (backend-specific)\n  # Examples: \"sonnet\", \"opus\", \"haiku\" (for Claude)\n  model: sonnet\n\n  # Subprocess timeout in milliseconds\n  # Default: 300,000ms (5 minutes)\n  # Increase for very large files or slow connections\n  timeoutMs: 300000\n\n  # Maximum number of retries for transient errors\n  # Default: 3\n  maxRetries: 3\n\n  # Number of concurrent AI calls (parallelism)\n  # Range: 1-20, Default: auto-detected from CPU cores and available memory\n  # Current machine default: ${getDefaultConcurrency()}\n  # Uncomment to override:\n  # concurrency: ${getDefaultConcurrency()}\n\n  # Telemetry settings\n  telemetry:\n    # Number of most recent run logs to keep on disk\n    # Logs stored in .agents-reverse-engineer/logs/\n    keepRuns: 50\n`;\n\n  await writeFile(configPath, configContent, 'utf-8');\n}\n\n```\n\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n- Workflow & convention rules: if this file defines contribution guidelines, PR conventions, commit standards, testing mandates, tool usage requirements, approval workflows, AI agent behavioral instructions, or code conventions (naming standards, formatting rules, import ordering, linting policies), extract them as explicit, actionable rules. Sources: CONTRIBUTING.md, CI configs, PR templates, style guides, linter/formatter configs (.eslintrc, .prettierrc, .editorconfig), tsconfig strictness, README dev sections.\n\n\nTARGET LENGTH (MANDATORY):\n- Source file: 9806 characters\n- Target summary: ~981 characters (10% compression)\n- Maximum: 1177 characters\n- Achieve this by: ultra-dense writing, minimal examples, single-sentence descriptions\n\nULTRA-COMPRESSION TECHNIQUES:\n- Use telegraphic style: \"exports foo, bar, baz\" not \"this file exports three functions\"\n- No code examples unless critical to understanding\n- One sentence per export maximum\n- Omit obvious parameter names (\"opts: Options\" not \"options: Options - configuration object\")\n- Use abbreviations: \"params\" not \"parameters\", \"config\" not \"configuration\"\n- Skip section if empty (don't write \"No special concerns\" or \"N/A\")\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nWORKFLOW & CONVENTION RULES (NEVER EXCLUDE):\n- Contribution requirements: branch naming, commit message formats, PR description templates\n- Testing mandates: required test types, coverage thresholds, \"never delete tests\" policies\n- Tool usage rules: required CLI commands, browser automation tools, linter/formatter requirements\n- Approval workflows: review requirements, CI checks that must pass, deployment gates\n- AI agent instructions: explicit directives for automated assistants (coding rules, prohibited actions)\n- Code conventions: naming standards (camelCase, PascalCase, snake_case for what), import ordering rules, file/folder naming patterns, formatting rules (indentation, quotes, semicolons), language idioms, TypeScript strictness policies, preferred patterns vs anti-patterns\nThese define how contributors (human or AI) must work within the project.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion .annex.sum file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**loader.ts loads and validates agent-reverse configuration from `.agents-reverse-engineer/config.yaml`, returning defaults when absent.**\n\n## Exports\n\n`findProjectRoot(startDir: string): Promise<string>` — walks directory tree upward to locate `.agents-reverse-engineer/` directory; returns containing directory or startDir if not found.\n\n`ConfigError` — extends Error with `filePath` and optional `cause` properties for configuration parsing/validation failures.\n\n`loadConfig(root: string, options?: {tracer?: ITraceWriter; debug?: boolean; logger?: Logger}): Promise<Config>` — parses and validates config via ConfigSchema; emits `config:loaded` trace event with model, concurrency, configPath; returns Config object or defaults if file missing; throws ConfigError on parse/validation failure.\n\n`configExists(root: string): Promise<boolean>` — checks existence of config.yaml in `.agents-reverse-engineer/`.\n\n`writeDefaultConfig(root: string): Promise<void>` — creates `.agents-reverse-engineer/` directory and writes config.yaml template with formatted comments and inline defaults.\n\n`yamlScalar(value: string): string` — escapes and quotes YAML scalar values containing special chars `*{}\\[\\]?,:#&!|>'\"%@`` ` to prevent misinterpretation.\n\n## Constants\n\n`CONFIG_DIR` = `'.agents-reverse-engineer'` — configuration directory name.\n\n`CONFIG_FILE` = `'config.yaml'` — configuration filename.\n\n## Configuration File Structure\n\nGenerated config includes sections: `exclude` (patterns, vendorDirs, binaryExtensions), `options` (followSymlinks, maxFileSize), `output` (colors), `generation` (compressionRatio: 0.1–1.0), `ai` (backend, model, timeoutMs, maxRetries, concurrency, telemetry.keepRuns).\n\n## Integration\n\nDependencies: ConfigSchema and Config from schema.js; DEFAULT_VENDOR_DIRS, DEFAULT_BINARY_EXTENSIONS, DEFAULT_MAX_FILE_SIZE, DEFAULT_EXCLUDE_PATTERNS, getDefaultConcurrency from defaults.js; ITraceWriter for trace event emission; Logger for debug output. Node fs promises (readFile, writeFile, mkdir, access) and yaml (parse, stringify).\n\n## Error Handling\n\nFile-not-found (ENOENT) triggers default config return; ZodError validation issues formatted as indented path:message list in ConfigError; YAML parse errors wrapped in ConfigError with source error preserved.",
      "model": "claude-haiku-4-5-20251001",
      "inputTokens": 3,
      "outputTokens": 580,
      "cacheReadTokens": 16147,
      "cacheCreationTokens": 5444,
      "latencyMs": 21830,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/config/loader.ts",
          "sizeBytes": 9806
        }
      ]
    },
    {
      "timestamp": "2026-02-14T13:47:07.416Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: src/discovery/walker.ts\n\n```typescript\n/**\n * Directory walker for the agents-reverse file discovery system.\n * \n * Uses fast-glob to traverse directories and return all candidate files.\n * Filters are applied separately via the filter chain (not in this module).\n */\n\nimport fg from 'fast-glob';\nimport type { WalkerOptions } from './types.js';\n\n/**\n * Walk a directory tree and return all files.\n * \n * This walker returns ALL files in the directory tree. Filtering happens\n * separately via the filter chain (gitignore, binary, vendor, custom patterns).\n * \n * @param options - Walker configuration\n * @returns Array of absolute file paths\n * \n * @example\n * ```typescript\n * const files = await walkDirectory({ cwd: '/path/to/repo' });\n * console.log(`Found ${files.length} files`);\n * ```\n */\nexport async function walkDirectory(options: WalkerOptions): Promise<string[]> {\n  return fg.glob('**/*', {\n    cwd: options.cwd,\n    absolute: true,\n    onlyFiles: true,\n    dot: options.dot ?? true,\n    followSymbolicLinks: options.followSymlinks ?? false,\n    suppressErrors: true, // Don't throw on permission errors (per RESEARCH.md)\n    // Always exclude .git internals for performance\n    ignore: ['**/.git/**'],\n  });\n}\n\n```\n\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n- Workflow & convention rules: if this file defines contribution guidelines, PR conventions, commit standards, testing mandates, tool usage requirements, approval workflows, AI agent behavioral instructions, or code conventions (naming standards, formatting rules, import ordering, linting policies), extract them as explicit, actionable rules. Sources: CONTRIBUTING.md, CI configs, PR templates, style guides, linter/formatter configs (.eslintrc, .prettierrc, .editorconfig), tsconfig strictness, README dev sections.\n\n\nTARGET LENGTH (MANDATORY):\n- Source file: 1187 characters\n- Target summary: ~119 characters (10% compression)\n- Maximum: 143 characters\n- Achieve this by: ultra-dense writing, minimal examples, single-sentence descriptions\n\nULTRA-COMPRESSION TECHNIQUES:\n- Use telegraphic style: \"exports foo, bar, baz\" not \"this file exports three functions\"\n- No code examples unless critical to understanding\n- One sentence per export maximum\n- Omit obvious parameter names (\"opts: Options\" not \"options: Options - configuration object\")\n- Use abbreviations: \"params\" not \"parameters\", \"config\" not \"configuration\"\n- Skip section if empty (don't write \"No special concerns\" or \"N/A\")\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nWORKFLOW & CONVENTION RULES (NEVER EXCLUDE):\n- Contribution requirements: branch naming, commit message formats, PR description templates\n- Testing mandates: required test types, coverage thresholds, \"never delete tests\" policies\n- Tool usage rules: required CLI commands, browser automation tools, linter/formatter requirements\n- Approval workflows: review requirements, CI checks that must pass, deployment gates\n- AI agent instructions: explicit directives for automated assistants (coding rules, prohibited actions)\n- Code conventions: naming standards (camelCase, PascalCase, snake_case for what), import ordering rules, file/folder naming patterns, formatting rules (indentation, quotes, semicolons), language idioms, TypeScript strictness policies, preferred patterns vs anti-patterns\nThese define how contributors (human or AI) must work within the project.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion .annex.sum file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**walker.ts exports walkDirectory() to traverse directories and collect absolute file paths via fast-glob.**\n\n## Exported Symbols\n\n- `walkDirectory(options: WalkerOptions): Promise<string[]>` — Returns all files in directory tree as absolute paths; filtering (gitignore, binary, vendor) applied separately via filter chain.\n\n## Configuration & Behavior\n\n`walkDirectory` accepts `WalkerOptions` and passes to `fg.glob('**/*', {...})` with:\n- `cwd` from options (working directory)\n- `absolute: true` (returns absolute paths)\n- `onlyFiles: true` (excludes directories)\n- `dot: options.dot ?? true` (includes dotfiles by default)\n- `followSymlinks: options.followSymlinks ?? false` (no symlink following by default)\n- `suppressErrors: true` (ignores permission errors per RESEARCH.md)\n- `ignore: ['**/.git/**']` (always excludes .git for performance)\n\n## Design Pattern\n\nSeparation of concerns: walker only collects files; all filtering (gitignore, binary detection, vendor directories, custom patterns) delegated to filter chain module.",
      "model": "claude-haiku-4-5-20251001",
      "inputTokens": 3,
      "outputTokens": 268,
      "cacheReadTokens": 16147,
      "cacheCreationTokens": 3074,
      "latencyMs": 17811,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/discovery/walker.ts",
          "sizeBytes": 1187
        }
      ]
    },
    {
      "timestamp": "2026-02-14T13:47:06.632Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: src/discovery/types.ts\n\n```typescript\n/**\n * Discovery types for the agents-reverse file discovery system.\n * \n * This module defines the core interfaces and types used by the directory\n * walker and filter chain for file discovery operations.\n */\n\nimport type { Stats } from 'node:fs';\n\n/**\n * Interface for file filters in the discovery pipeline.\n * \n * Filters are applied to each file discovered by the walker to determine\n * whether it should be included or excluded from analysis. Filters can be\n * synchronous or asynchronous.\n * \n * Examples: GitignoreFilter, BinaryFilter, VendorFilter, CustomPatternFilter\n */\nexport interface FileFilter {\n  /** Name of the filter for logging which filter excluded a file */\n  readonly name: string;\n  \n  /**\n   * Determine whether a file should be excluded from discovery.\n   * \n   * @param path - Absolute path to the file\n   * @param stats - Optional file stats (for size-based filtering, etc.)\n   * @returns true if the file should be excluded, false to include\n   */\n  shouldExclude(path: string, stats?: Stats): Promise<boolean> | boolean;\n}\n\n/**\n * Record of an excluded file with reason and responsible filter.\n */\nexport interface ExcludedFile {\n  /** Absolute path to the excluded file */\n  path: string;\n  /** Human-readable reason for exclusion */\n  reason: string;\n  /** Name of the filter that excluded this file */\n  filter: string;\n}\n\n/**\n * Result of running the discovery and filter chain.\n */\nexport interface FilterResult {\n  /** Files that passed all filters and should be analyzed */\n  included: string[];\n  /** Files that were excluded with reasons */\n  excluded: ExcludedFile[];\n}\n\n/**\n * Options for the directory walker.\n */\nexport interface WalkerOptions {\n  /** Root directory to walk (absolute path) */\n  cwd: string;\n  \n  /** \n   * Whether to follow symbolic links.\n   * Default: false (per CONTEXT.md - skip symlinks by default)\n   */\n  followSymlinks?: boolean;\n  \n  /**\n   * Whether to include dotfiles (files starting with .).\n   * Default: true (include dotfiles for analysis)\n   */\n  dot?: boolean;\n}\n\n```\n\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n- Workflow & convention rules: if this file defines contribution guidelines, PR conventions, commit standards, testing mandates, tool usage requirements, approval workflows, AI agent behavioral instructions, or code conventions (naming standards, formatting rules, import ordering, linting policies), extract them as explicit, actionable rules. Sources: CONTRIBUTING.md, CI configs, PR templates, style guides, linter/formatter configs (.eslintrc, .prettierrc, .editorconfig), tsconfig strictness, README dev sections.\n\n\nTARGET LENGTH (MANDATORY):\n- Source file: 2043 characters\n- Target summary: ~204 characters (10% compression)\n- Maximum: 245 characters\n- Achieve this by: ultra-dense writing, minimal examples, single-sentence descriptions\n\nULTRA-COMPRESSION TECHNIQUES:\n- Use telegraphic style: \"exports foo, bar, baz\" not \"this file exports three functions\"\n- No code examples unless critical to understanding\n- One sentence per export maximum\n- Omit obvious parameter names (\"opts: Options\" not \"options: Options - configuration object\")\n- Use abbreviations: \"params\" not \"parameters\", \"config\" not \"configuration\"\n- Skip section if empty (don't write \"No special concerns\" or \"N/A\")\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nWORKFLOW & CONVENTION RULES (NEVER EXCLUDE):\n- Contribution requirements: branch naming, commit message formats, PR description templates\n- Testing mandates: required test types, coverage thresholds, \"never delete tests\" policies\n- Tool usage rules: required CLI commands, browser automation tools, linter/formatter requirements\n- Approval workflows: review requirements, CI checks that must pass, deployment gates\n- AI agent instructions: explicit directives for automated assistants (coding rules, prohibited actions)\n- Code conventions: naming standards (camelCase, PascalCase, snake_case for what), import ordering rules, file/folder naming patterns, formatting rules (indentation, quotes, semicolons), language idioms, TypeScript strictness policies, preferred patterns vs anti-patterns\nThese define how contributors (human or AI) must work within the project.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion .annex.sum file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**src/discovery/types.ts defines core interfaces for the file discovery pipeline: FileFilter, ExcludedFile, FilterResult, WalkerOptions.**\n\n## Exported Types\n\n`FileFilter` — filter interface with `name: string` and `shouldExclude(path: string, stats?: Stats): Promise<boolean> | boolean`. Determines file inclusion/exclusion in discovery chain.\n\n`ExcludedFile` — record with `path: string`, `reason: string`, `filter: string`. Tracks excluded files with reasoning.\n\n`FilterResult` — result object with `included: string[]` (passed files) and `excluded: ExcludedFile[]` (rejected files with reasons).\n\n`WalkerOptions` — directory walker config: `cwd: string` (root path, required), `followSymlinks?: boolean` (default: false), `dot?: boolean` (default: true, include dotfiles).\n\n## Key Design\n\nFileFilter uses asynchronous-first pattern: `shouldExclude()` returns `Promise<boolean> | boolean`, allowing both sync and async filters (e.g., size checks, I/O operations). Filter name field required for logging excluded file sources.\n\n## Defaults & Conventions\n\n`followSymlinks` defaults to false per CONTEXT.md symlink exclusion policy. `dot` defaults to true to capture dotfiles for analysis (configuration files, git metadata).",
      "model": "claude-haiku-4-5-20251001",
      "inputTokens": 3,
      "outputTokens": 314,
      "cacheReadTokens": 16147,
      "cacheCreationTokens": 3284,
      "latencyMs": 19206,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/discovery/types.ts",
          "sizeBytes": 2043
        }
      ]
    },
    {
      "timestamp": "2026-02-14T13:47:02.421Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: src/core/index.ts\n\n```typescript\n/**\n * Public programmatic API for agents-reverse-engineer.\n *\n * Import from `'agents-reverse-engineer/core'` to use the engine\n * without CLI dependencies (no `process.exit`, `ora`, `picocolors`).\n *\n * @beta — API surface is experimental until v1.0.0\n * @module\n *\n * @example\n * ```typescript\n * import {\n *   discoverFiles,\n *   buildFilePrompt,\n *   AIService,\n *   type AIProvider,\n * } from 'agents-reverse-engineer/core';\n * ```\n */\n\n// === Logger ===\nexport { nullLogger, consoleLogger } from './logger.js';\nexport type { Logger } from './logger.js';\n\n// === AI ===\nexport { AIService } from '../ai/service.js';\nexport type { AIServiceOptions } from '../ai/service.js';\nexport type {\n  AIProvider,\n  AICallOptions,\n  AIResponse,\n  AIBackend,\n  RetryOptions,\n  AIServiceErrorCode,\n} from '../ai/types.js';\nexport { AIServiceError } from '../ai/types.js';\nexport { withRetry, DEFAULT_RETRY_OPTIONS } from '../ai/retry.js';\nexport { SubprocessProvider } from '../ai/providers/subprocess.js';\nexport type { SubprocessProviderOptions } from '../ai/providers/subprocess.js';\n\n// === Discovery ===\nexport { discoverFiles } from '../discovery/run.js';\nexport { walkDirectory } from '../discovery/walker.js';\nexport { applyFilters } from '../discovery/filters/index.js';\nexport type {\n  FileFilter,\n  FilterResult,\n  WalkerOptions,\n} from '../discovery/types.js';\n\n// === Generation: Prompts ===\nexport {\n  buildFilePrompt,\n  buildDirectoryPrompt,\n  detectLanguage,\n} from '../generation/prompts/builder.js';\nexport type { PromptContext } from '../generation/prompts/types.js';\n\n// === Generation: Writers ===\nexport {\n  writeSumFile,\n  readSumFile,\n  getSumPath,\n  sumFileExists,\n  getAnnexPath,\n  writeAnnexFile,\n} from '../generation/writers/sum.js';\nexport type { SumFileContent } from '../generation/writers/sum.js';\nexport {\n  writeAgentsMd,\n  writeAgentsMdHub,\n  isGeneratedAgentsMd,\n} from '../generation/writers/agents-md.js';\nexport { writeClaudeMdPointer } from '../generation/writers/claude-md.js';\n\n// === Generation: Orchestration ===\nexport {\n  DocumentationOrchestrator as GenerationOrchestrator,\n  DocumentationOrchestrator as UpdateOrchestrator,\n} from '../orchestration/orchestrator.js';\nexport type { GenerationPlan, PreparedFile, AnalysisTask, UpdatePlan } from '../orchestration/orchestrator.js';\nexport {\n  buildExecutionPlan,\n  formatExecutionPlanAsMarkdown,\n} from '../generation/executor.js';\nexport type { ExecutionPlan, ExecutionTask } from '../generation/executor.js';\n\n// === Quality ===\nexport {\n  extractExports,\n  checkCodeVsDoc,\n} from '../quality/inconsistency/code-vs-doc.js';\nexport { checkCodeVsCode } from '../quality/inconsistency/code-vs-code.js';\nexport { checkPhantomPaths } from '../quality/phantom-paths/validator.js';\nexport {\n  buildInconsistencyReport,\n  formatReportForCli,\n  formatReportAsMarkdown,\n} from '../quality/inconsistency/reporter.js';\nexport type {\n  Inconsistency,\n  InconsistencyReport,\n  InconsistencySeverity,\n  CodeDocInconsistency,\n  CodeCodeInconsistency,\n  PhantomPathInconsistency,\n} from '../quality/types.js';\n\n// === Orchestration ===\nexport { runPool } from '../orchestration/pool.js';\nexport type { PoolOptions, TaskResult } from '../orchestration/pool.js';\n\n// === Change Detection ===\nexport {\n  computeContentHash,\n  computeContentHashFromString,\n  isGitRepo,\n  getCurrentCommit,\n  getChangedFiles,\n} from '../change-detection/detector.js';\nexport type {\n  FileChange,\n  ChangeDetectionResult,\n} from '../change-detection/types.js';\n\n// === Config ===\nexport { loadConfig, findProjectRoot } from '../config/loader.js';\nexport { ConfigSchema } from '../config/schema.js';\nexport type { Config } from '../config/schema.js';\nexport {\n  getDefaultConcurrency,\n  DEFAULT_EXCLUDE_PATTERNS,\n  DEFAULT_VENDOR_DIRS,\n} from '../config/defaults.js';\n\n// === Imports ===\nexport {\n  extractImports,\n  extractDirectoryImports,\n  formatImportMap,\n} from '../imports/extractor.js';\n\n```\n\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n- Workflow & convention rules: if this file defines contribution guidelines, PR conventions, commit standards, testing mandates, tool usage requirements, approval workflows, AI agent behavioral instructions, or code conventions (naming standards, formatting rules, import ordering, linting policies), extract them as explicit, actionable rules. Sources: CONTRIBUTING.md, CI configs, PR templates, style guides, linter/formatter configs (.eslintrc, .prettierrc, .editorconfig), tsconfig strictness, README dev sections.\n\n\nTARGET LENGTH (MANDATORY):\n- Source file: 3945 characters\n- Target summary: ~395 characters (10% compression)\n- Maximum: 474 characters\n- Achieve this by: ultra-dense writing, minimal examples, single-sentence descriptions\n\nULTRA-COMPRESSION TECHNIQUES:\n- Use telegraphic style: \"exports foo, bar, baz\" not \"this file exports three functions\"\n- No code examples unless critical to understanding\n- One sentence per export maximum\n- Omit obvious parameter names (\"opts: Options\" not \"options: Options - configuration object\")\n- Use abbreviations: \"params\" not \"parameters\", \"config\" not \"configuration\"\n- Skip section if empty (don't write \"No special concerns\" or \"N/A\")\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nWORKFLOW & CONVENTION RULES (NEVER EXCLUDE):\n- Contribution requirements: branch naming, commit message formats, PR description templates\n- Testing mandates: required test types, coverage thresholds, \"never delete tests\" policies\n- Tool usage rules: required CLI commands, browser automation tools, linter/formatter requirements\n- Approval workflows: review requirements, CI checks that must pass, deployment gates\n- AI agent instructions: explicit directives for automated assistants (coding rules, prohibited actions)\n- Code conventions: naming standards (camelCase, PascalCase, snake_case for what), import ordering rules, file/folder naming patterns, formatting rules (indentation, quotes, semicolons), language idioms, TypeScript strictness policies, preferred patterns vs anti-patterns\nThese define how contributors (human or AI) must work within the project.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion .annex.sum file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**index.ts is the public programmatic API entrypoint for agents-reverse-engineer, aggregating 40+ exports across logging, AI services, file discovery, documentation generation, quality checking, and configuration modules.**\n\n## Exports\n\n### Logger\n- `nullLogger` — no-op logger\n- `consoleLogger` — stdout logger  \n- `Logger` — type\n\n### AI Services\n- `AIService` — main AI orchestration class\n- `AIServiceOptions`, `AIProvider`, `AICallOptions`, `AIResponse`, `AIBackend`, `RetryOptions`, `AIServiceErrorCode` — types\n- `AIServiceError` — error class\n- `withRetry(fn, opts): Promise` — wraps function with retry logic\n- `DEFAULT_RETRY_OPTIONS` — constant\n- `SubprocessProvider` — spawns subprocess for external LLM calls\n- `SubprocessProviderOptions` — type\n\n### Discovery\n- `discoverFiles(opts): Promise<File[]>` — finds files matching filters\n- `walkDirectory(path, opts): AsyncIterable<FilePath>` — traverses directory tree\n- `applyFilters(files, filters): FilterResult[]` — applies inclusion/exclusion rules\n- `FileFilter`, `FilterResult`, `WalkerOptions` — types\n\n### Prompt Generation\n- `buildFilePrompt(file, context): string` — constructs single-file analysis prompt\n- `buildDirectoryPrompt(dir, context): string` — constructs multi-file analysis prompt\n- `detectLanguage(path): Language` — identifies programming language\n- `PromptContext` — type\n\n### Documentation Writers\n- `writeSumFile(path, content): void` — writes .sum summary file\n- `readSumFile(path): SumFileContent` — parses .sum file\n- `getSumPath(filePath): string` — derives .sum path from source\n- `sumFileExists(path): boolean` — checks if .sum exists\n- `getAnnexPath(path): string` — derives .annex path\n- `writeAnnexFile(path, content): void` — writes .annex reference file\n- `SumFileContent` — type\n- `writeAgentsMd(dir, summaries): void` — generates AGENTS.md hub\n- `writeAgentsMdHub(dir, summaries): void` — AGENTS.md variant\n- `isGeneratedAgentsMd(path): boolean` — detects generated marker\n- `writeClaudeMdPointer(dir, target): void` — creates .claude.md redirect\n\n### Generation Orchestration\n- `DocumentationOrchestrator` / `GenerationOrchestrator` / `UpdateOrchestrator` — aliases for main orchestration engine\n- `GenerationPlan`, `PreparedFile`, `AnalysisTask`, `UpdatePlan` — types\n- `buildExecutionPlan(files, opts): ExecutionPlan` — sequences analysis tasks\n- `formatExecutionPlanAsMarkdown(plan): string` — renders plan as MD\n- `ExecutionPlan`, `ExecutionTask` — types\n\n### Quality Checks\n- `extractExports(sourceCode): Export[]` — parses function/class/const declarations\n- `checkCodeVsDoc(source, doc): Inconsistency[]` — compares code exports to documentation\n- `checkCodeVsCode(before, after): CodeCodeInconsistency[]` — detects breaking changes\n- `checkPhantomPaths(summaries, rootDir): PhantomPathInconsistency[]` — validates referenced file paths\n- `buildInconsistencyReport(issues): InconsistencyReport` — aggregates quality findings\n- `formatReportForCli(report): string` — CLI output format\n- `formatReportAsMarkdown(report): string` — MD output format\n- `Inconsistency`, `InconsistencyReport`, `InconsistencySeverity`, `CodeDocInconsistency`, `CodeCodeInconsistency`, `PhantomPathInconsistency` — types\n\n### Task Pooling\n- `runPool(tasks, opts): Promise<TaskResult[]>` — concurrent task execution\n- `PoolOptions`, `TaskResult` — types\n\n### Change Detection\n- `computeContentHash(filePath): string` — SHA256 file hash\n- `computeContentHashFromString(content): string` — SHA256 string hash\n- `isGitRepo(dir): boolean` — detects git repository\n- `getCurrentCommit(): string` — retrieves HEAD SHA\n- `getChangedFiles(since, filter): FileChange[]` — lists modified files since commit\n- `FileChange`, `ChangeDetectionResult` — types\n\n### Configuration\n- `loadConfig(path?): Config` — parses .are.json or discovers defaults\n- `findProjectRoot(startPath?): string` — locates project root via marker files\n- `ConfigSchema` — Zod schema for validation\n- `Config` — type\n- `getDefaultConcurrency(): number` — CPU-aware default worker count\n- `DEFAULT_EXCLUDE_PATTERNS` — constant string array (node_modules, .git, dist, etc.)\n- `DEFAULT_VENDOR_DIRS` — constant string array\n\n### Imports\n- `extractImports(sourceCode): ImportMap` — parses ES module imports\n- `extractDirectoryImports(dir): ImportMap` — aggregates imports across directory\n- `formatImportMap(map): string` — renders import analysis\n\n## Context\nRe-exports are from `../` paths (ai/, discovery/, generation/, orchestration/, quality/, change-detection/, config/, imports/) — this aggregation module enables single-line imports for library consumers. Marked `@beta` until v1.0.0; no CLI-only dependencies (process, ora, picocolors) in public API.",
      "model": "claude-haiku-4-5-20251001",
      "inputTokens": 3,
      "outputTokens": 1355,
      "cacheReadTokens": 16147,
      "cacheCreationTokens": 3991,
      "latencyMs": 23550,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/core/index.ts",
          "sizeBytes": 3947
        }
      ]
    },
    {
      "timestamp": "2026-02-14T13:47:10.986Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: src/generation/collector.ts\n\n```typescript\nimport * as path from 'node:path';\nimport { readdir, readFile } from 'node:fs/promises';\n\n/**\n * A collected AGENTS.md document with its project-relative path and content.\n */\nexport type AgentsDocs = Array<{ relativePath: string; content: string }>;\n\n/** Directories to skip during recursive AGENTS.md collection. */\nconst SKIP_DIRS = new Set([\n  'node_modules', '.git', '.agents-reverse-engineer',\n  'vendor', 'dist', 'build', '__pycache__', '.next',\n  'venv', '.venv', 'target', '.cargo', '.gradle',\n]);\n\n/**\n * Recursively collect all AGENTS.md files under `projectRoot`,\n * returning their relative paths and content sorted alphabetically.\n *\n * Skips vendor/build/meta directories and gracefully handles\n * unreadable directories or files.\n */\nexport async function collectAgentsDocs(projectRoot: string): Promise<AgentsDocs> {\n  const results: AgentsDocs = [];\n\n  async function walk(currentDir: string): Promise<void> {\n    let entries;\n    try {\n      entries = await readdir(currentDir, { withFileTypes: true });\n    } catch {\n      return; // Permission denied or inaccessible\n    }\n    for (const entry of entries) {\n      if (entry.isDirectory() && !SKIP_DIRS.has(entry.name)) {\n        await walk(path.join(currentDir, entry.name));\n      } else if (entry.name === 'AGENTS.md') {\n        try {\n          const filePath = path.join(currentDir, entry.name);\n          const content = await readFile(filePath, 'utf-8');\n          results.push({\n            relativePath: path.relative(projectRoot, filePath),\n            content,\n          });\n        } catch {\n          // Skip unreadable files\n        }\n      }\n    }\n  }\n\n  await walk(projectRoot);\n  results.sort((a, b) => a.relativePath.localeCompare(b.relativePath));\n  return results;\n}\n\n/**\n * Recursively collect all `.annex.sum` files under `projectRoot`,\n * returning their relative paths and content sorted alphabetically.\n *\n * Uses the same skip-list as `collectAgentsDocs()`.\n */\nexport async function collectAnnexFiles(projectRoot: string): Promise<AgentsDocs> {\n  const results: AgentsDocs = [];\n\n  async function walk(currentDir: string): Promise<void> {\n    let entries;\n    try {\n      entries = await readdir(currentDir, { withFileTypes: true });\n    } catch {\n      return;\n    }\n    for (const entry of entries) {\n      if (entry.isDirectory() && !SKIP_DIRS.has(entry.name)) {\n        await walk(path.join(currentDir, entry.name));\n      } else if (entry.isFile() && entry.name.endsWith('.annex.sum')) {\n        try {\n          const filePath = path.join(currentDir, entry.name);\n          const content = await readFile(filePath, 'utf-8');\n          results.push({\n            relativePath: path.relative(projectRoot, filePath),\n            content,\n          });\n        } catch {\n          // Skip unreadable files\n        }\n      }\n    }\n  }\n\n  await walk(projectRoot);\n  results.sort((a, b) => a.relativePath.localeCompare(b.relativePath));\n  return results;\n}\n\n```\n\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n- Workflow & convention rules: if this file defines contribution guidelines, PR conventions, commit standards, testing mandates, tool usage requirements, approval workflows, AI agent behavioral instructions, or code conventions (naming standards, formatting rules, import ordering, linting policies), extract them as explicit, actionable rules. Sources: CONTRIBUTING.md, CI configs, PR templates, style guides, linter/formatter configs (.eslintrc, .prettierrc, .editorconfig), tsconfig strictness, README dev sections.\n\n\nTARGET LENGTH (MANDATORY):\n- Source file: 2951 characters\n- Target summary: ~295 characters (10% compression)\n- Maximum: 354 characters\n- Achieve this by: ultra-dense writing, minimal examples, single-sentence descriptions\n\nULTRA-COMPRESSION TECHNIQUES:\n- Use telegraphic style: \"exports foo, bar, baz\" not \"this file exports three functions\"\n- No code examples unless critical to understanding\n- One sentence per export maximum\n- Omit obvious parameter names (\"opts: Options\" not \"options: Options - configuration object\")\n- Use abbreviations: \"params\" not \"parameters\", \"config\" not \"configuration\"\n- Skip section if empty (don't write \"No special concerns\" or \"N/A\")\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nWORKFLOW & CONVENTION RULES (NEVER EXCLUDE):\n- Contribution requirements: branch naming, commit message formats, PR description templates\n- Testing mandates: required test types, coverage thresholds, \"never delete tests\" policies\n- Tool usage rules: required CLI commands, browser automation tools, linter/formatter requirements\n- Approval workflows: review requirements, CI checks that must pass, deployment gates\n- AI agent instructions: explicit directives for automated assistants (coding rules, prohibited actions)\n- Code conventions: naming standards (camelCase, PascalCase, snake_case for what), import ordering rules, file/folder naming patterns, formatting rules (indentation, quotes, semicolons), language idioms, TypeScript strictness policies, preferred patterns vs anti-patterns\nThese define how contributors (human or AI) must work within the project.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion .annex.sum file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**collector.ts recursively discovers and aggregates AGENTS.md and .annex.sum documentation files from a project tree.**\n\n## Exports\n\n`AgentsDocs` type: array of `{ relativePath: string; content: string }` tuples representing collected documentation.\n\n`collectAgentsDocs(projectRoot: string): Promise<AgentsDocs>` — recursively walks `projectRoot`, locates all AGENTS.md files, reads content, and returns sorted by relative path; gracefully skips unreadable directories/files.\n\n`collectAnnexFiles(projectRoot: string): Promise<AgentsDocs>` — recursively walks `projectRoot`, locates all .annex.sum files, reads content, and returns sorted by relative path using identical skip logic.\n\n## Skip Directories\n\n`SKIP_DIRS` excludes: node_modules, .git, .agents-reverse-engineer, vendor, dist, build, __pycache__, .next, venv, .venv, target, .cargo, .gradle.\n\n## Integration\n\nBoth functions use shared recursion pattern with identical error handling: `readdir` failures trigger silent return; `readFile` failures skip individual files. Results always sorted alphabetically by `relativePath` using `localeCompare`.",
      "model": "claude-haiku-4-5-20251001",
      "inputTokens": 3,
      "outputTokens": 285,
      "cacheReadTokens": 16147,
      "cacheCreationTokens": 3610,
      "latencyMs": 16262,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/generation/collector.ts",
          "sizeBytes": 2951
        }
      ]
    },
    {
      "timestamp": "2026-02-14T13:47:11.297Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: src/generation/complexity.ts\n\n```typescript\nimport * as path from 'node:path';\n\n/**\n * Metrics about codebase complexity.\n */\nexport interface ComplexityMetrics {\n  /** Total number of source files */\n  fileCount: number;\n  /** Maximum directory depth */\n  directoryDepth: number;\n  /** List of source file paths */\n  files: string[];\n  /** Unique directory paths */\n  directories: Set<string>;\n}\n\n/**\n * Calculate maximum directory depth from file paths.\n */\nfunction calculateDirectoryDepth(files: string[], projectRoot: string): number {\n  let maxDepth = 0;\n\n  for (const file of files) {\n    const relativePath = path.relative(projectRoot, file);\n    const depth = relativePath.split(path.sep).length - 1; // -1 for the file itself\n    maxDepth = Math.max(maxDepth, depth);\n  }\n\n  return maxDepth;\n}\n\n/**\n * Extract unique directories from file paths.\n */\nfunction extractDirectories(files: string[]): Set<string> {\n  const directories = new Set<string>();\n\n  for (const file of files) {\n    let dir = path.dirname(file);\n    while (dir && dir !== '.') {\n      directories.add(dir);\n      const parent = path.dirname(dir);\n      if (parent === dir) break; // Reached root\n      dir = parent;\n    }\n  }\n\n  return directories;\n}\n\n/**\n * Analyze codebase complexity from discovered files.\n *\n * @param files - List of source file paths\n * @param projectRoot - Project root directory\n * @returns Complexity metrics\n */\nexport function analyzeComplexity(\n  files: string[],\n  projectRoot: string\n): ComplexityMetrics {\n  return {\n    fileCount: files.length,\n    directoryDepth: calculateDirectoryDepth(files, projectRoot),\n    files,\n    directories: extractDirectories(files),\n  };\n}\n\n\n```\n\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n- Workflow & convention rules: if this file defines contribution guidelines, PR conventions, commit standards, testing mandates, tool usage requirements, approval workflows, AI agent behavioral instructions, or code conventions (naming standards, formatting rules, import ordering, linting policies), extract them as explicit, actionable rules. Sources: CONTRIBUTING.md, CI configs, PR templates, style guides, linter/formatter configs (.eslintrc, .prettierrc, .editorconfig), tsconfig strictness, README dev sections.\n\n\nTARGET LENGTH (MANDATORY):\n- Source file: 1642 characters\n- Target summary: ~164 characters (10% compression)\n- Maximum: 197 characters\n- Achieve this by: ultra-dense writing, minimal examples, single-sentence descriptions\n\nULTRA-COMPRESSION TECHNIQUES:\n- Use telegraphic style: \"exports foo, bar, baz\" not \"this file exports three functions\"\n- No code examples unless critical to understanding\n- One sentence per export maximum\n- Omit obvious parameter names (\"opts: Options\" not \"options: Options - configuration object\")\n- Use abbreviations: \"params\" not \"parameters\", \"config\" not \"configuration\"\n- Skip section if empty (don't write \"No special concerns\" or \"N/A\")\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nWORKFLOW & CONVENTION RULES (NEVER EXCLUDE):\n- Contribution requirements: branch naming, commit message formats, PR description templates\n- Testing mandates: required test types, coverage thresholds, \"never delete tests\" policies\n- Tool usage rules: required CLI commands, browser automation tools, linter/formatter requirements\n- Approval workflows: review requirements, CI checks that must pass, deployment gates\n- AI agent instructions: explicit directives for automated assistants (coding rules, prohibited actions)\n- Code conventions: naming standards (camelCase, PascalCase, snake_case for what), import ordering rules, file/folder naming patterns, formatting rules (indentation, quotes, semicolons), language idioms, TypeScript strictness policies, preferred patterns vs anti-patterns\nThese define how contributors (human or AI) must work within the project.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion .annex.sum file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**complexity.ts analyzes codebase structure to compute file counts, directory depths, and directory trees from file path lists.**\n\n## Exported Symbols\n\n- `ComplexityMetrics` interface: `{ fileCount: number; directoryDepth: number; files: string[]; directories: Set<string> }`\n- `analyzeComplexity(files: string[], projectRoot: string): ComplexityMetrics` — computes metrics from file list and root path\n\n## Internal Functions\n\n`calculateDirectoryDepth(files: string[], projectRoot: string): number` — measures maximum nesting depth by splitting relative paths on `path.sep`, subtracting 1 for the file itself.\n\n`extractDirectories(files: string[]): Set<string>` — traverses parent directories recursively via `path.dirname()` until reaching filesystem root (detects root when `path.dirname(dir) === dir`), accumulating all ancestors.\n\n## Dependencies\n\nUses `node:path` module for `relative()`, `dirname()`, and `sep` operations.",
      "model": "claude-haiku-4-5-20251001",
      "inputTokens": 3,
      "outputTokens": 238,
      "cacheReadTokens": 16147,
      "cacheCreationTokens": 3213,
      "latencyMs": 16165,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/generation/complexity.ts",
          "sizeBytes": 1642
        }
      ]
    },
    {
      "timestamp": "2026-02-14T13:47:16.311Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: src/generation/types.ts\n\n```typescript\n/**\n * Types for the documentation generation pipeline\n */\n\n/**\n * Metadata extracted during analysis\n */\nexport interface SummaryMetadata {\n  /** Primary purpose of the file */\n  purpose: string;\n  /** Only security/breaking issues */\n  criticalTodos?: string[];\n  /** Tightly coupled siblings */\n  relatedFiles?: string[];\n}\n\n\n```\n\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n- Workflow & convention rules: if this file defines contribution guidelines, PR conventions, commit standards, testing mandates, tool usage requirements, approval workflows, AI agent behavioral instructions, or code conventions (naming standards, formatting rules, import ordering, linting policies), extract them as explicit, actionable rules. Sources: CONTRIBUTING.md, CI configs, PR templates, style guides, linter/formatter configs (.eslintrc, .prettierrc, .editorconfig), tsconfig strictness, README dev sections.\n\n\nTARGET LENGTH (MANDATORY):\n- Source file: 328 characters\n- Target summary: ~33 characters (10% compression)\n- Maximum: 40 characters\n- Achieve this by: ultra-dense writing, minimal examples, single-sentence descriptions\n\nULTRA-COMPRESSION TECHNIQUES:\n- Use telegraphic style: \"exports foo, bar, baz\" not \"this file exports three functions\"\n- No code examples unless critical to understanding\n- One sentence per export maximum\n- Omit obvious parameter names (\"opts: Options\" not \"options: Options - configuration object\")\n- Use abbreviations: \"params\" not \"parameters\", \"config\" not \"configuration\"\n- Skip section if empty (don't write \"No special concerns\" or \"N/A\")\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nWORKFLOW & CONVENTION RULES (NEVER EXCLUDE):\n- Contribution requirements: branch naming, commit message formats, PR description templates\n- Testing mandates: required test types, coverage thresholds, \"never delete tests\" policies\n- Tool usage rules: required CLI commands, browser automation tools, linter/formatter requirements\n- Approval workflows: review requirements, CI checks that must pass, deployment gates\n- AI agent instructions: explicit directives for automated assistants (coding rules, prohibited actions)\n- Code conventions: naming standards (camelCase, PascalCase, snake_case for what), import ordering rules, file/folder naming patterns, formatting rules (indentation, quotes, semicolons), language idioms, TypeScript strictness policies, preferred patterns vs anti-patterns\nThese define how contributors (human or AI) must work within the project.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion .annex.sum file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**src/generation/types.ts defines type contracts for documentation metadata extraction during pipeline analysis.**\n\n## Exports\n\n`SummaryMetadata` interface with three properties:\n- `purpose: string` — file's primary purpose\n- `criticalTodos?: string[]` — security/breaking issues only\n- `relatedFiles?: string[]` — tightly coupled sibling files",
      "model": "claude-haiku-4-5-20251001",
      "inputTokens": 3,
      "outputTokens": 85,
      "cacheReadTokens": 16147,
      "cacheCreationTokens": 2816,
      "latencyMs": 16511,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/generation/types.ts",
          "sizeBytes": 328
        }
      ]
    },
    {
      "timestamp": "2026-02-14T13:46:53.681Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: src/cli/update.ts\n\n```typescript\n/**\n * CLI update command\n *\n * Updates documentation incrementally based on git changes since last run.\n * Uses the AI service for real file analysis (not placeholders) and processes\n * changed files concurrently via the CommandRunner orchestration engine.\n * Regenerates AGENTS.md for affected directories after analysis.\n */\nimport * as path from 'node:path';\nimport { readFile } from 'node:fs/promises';\nimport pc from 'picocolors';\nimport { loadConfig, findProjectRoot } from '../config/loader.js';\nimport { createLogger } from '../output/logger.js';\nimport { consoleLogger } from '../core/logger.js';\nimport {\n  createUpdateOrchestrator,\n  type UpdatePlan,\n} from '../orchestration/orchestrator.js';\nimport { writeAgentsMd, writeAgentsMdHub, GENERATED_MARKER_PREFIX } from '../generation/writers/agents-md.js';\nimport { writeClaudeMdPointer } from '../generation/writers/claude-md.js';\nimport { buildDirectoryPrompt } from '../generation/prompts/index.js';\nimport {\n  AIService,\n  AIServiceError,\n  createBackendRegistry,\n  resolveBackend,\n  getInstallInstructions,\n} from '../ai/index.js';\nimport { CommandRunner, ProgressReporter, ProgressLog, createTraceWriter, cleanupOldTraces } from '../orchestration/index.js';\n\n/**\n * Options for the update command.\n */\nexport interface UpdateOptions {\n  /** Include uncommitted changes (staged + working directory) */\n  uncommitted?: boolean;\n  /** Dry run - show plan without making changes */\n  dryRun?: boolean;\n  /** Number of concurrent AI calls */\n  concurrency?: number;\n  /** Stop on first file analysis failure */\n  failFast?: boolean;\n  /** Show AI prompts and backend details */\n  debug?: boolean;\n  /** Enable concurrency tracing to .agents-reverse-engineer/traces/ */\n  trace?: boolean;\n  /** Override AI model (e.g., \"sonnet\", \"opus\") */\n  model?: string;\n  /** Override AI backend (e.g., \"claude\", \"codex\", \"opencode\", \"gemini\") */\n  backend?: string;\n  /** Eval mode: namespace output by backend.model for comparison */\n  eval?: boolean;\n}\n\n/**\n * Format cleanup results for display.\n */\nfunction formatCleanup(plan: UpdatePlan): string[] {\n  const lines: string[] = [];\n\n  if (plan.cleanup.deletedSumFiles.length > 0) {\n    lines.push(pc.yellow('Cleanup (deleted .sum files):'));\n    for (const file of plan.cleanup.deletedSumFiles) {\n      lines.push(`  ${pc.red('-')} ${file}`);\n    }\n  }\n\n  if (plan.cleanup.deletedAgentsMd.length > 0) {\n    lines.push(pc.yellow('Cleanup (deleted AGENTS.md from empty dirs):'));\n    for (const file of plan.cleanup.deletedAgentsMd) {\n      lines.push(`  ${pc.red('-')} ${file}`);\n    }\n  }\n\n  return lines;\n}\n\n/**\n * Format the update plan for display.\n */\nfunction formatPlan(plan: UpdatePlan): string {\n  const lines: string[] = [];\n\n  // Header\n  lines.push('');\n  lines.push(pc.bold('=== Update Plan ==='));\n  lines.push('');\n\n  // Baseline info\n  if (plan.isFirstRun) {\n    lines.push(pc.yellow('First run detected. Use \"are generate\" for initial documentation.'));\n    lines.push('');\n  } else {\n    lines.push(`Current commit: ${pc.dim(plan.currentCommit.slice(0, 7))}`);\n    lines.push('');\n  }\n\n  // Summary\n  const analyzeCount = plan.filesToAnalyze.length;\n  const skipCount = plan.filesToSkip.length;\n  const cleanupCount = plan.cleanup.deletedSumFiles.length + plan.cleanup.deletedAgentsMd.length;\n\n  if (analyzeCount === 0 && skipCount === 0 && cleanupCount === 0) {\n    lines.push(pc.green('No changes detected since last run.'));\n    lines.push('');\n    return lines.join('\\n');\n  }\n\n  lines.push(`Files to analyze: ${pc.cyan(String(analyzeCount))}`);\n  lines.push(`Files unchanged: ${pc.dim(String(skipCount))}`);\n  if (cleanupCount > 0) {\n    lines.push(`Cleanup actions: ${pc.yellow(String(cleanupCount))}`);\n  }\n  lines.push('');\n\n  // File list with status markers\n  if (plan.filesToAnalyze.length > 0) {\n    lines.push(pc.cyan('Files to analyze:'));\n    for (const file of plan.filesToAnalyze) {\n      const status = file.status === 'added' ? pc.green('+') :\n                    file.status === 'renamed' ? pc.blue('R') :\n                    pc.yellow('M');\n      lines.push(`  ${status} ${file.path}`);\n      if (file.status === 'renamed' && file.oldPath) {\n        lines.push(`    ${pc.dim(`(was: ${file.oldPath})`)}`);\n      }\n    }\n    lines.push('');\n  }\n\n  if (plan.filesToSkip.length > 0) {\n    lines.push(pc.dim('Files unchanged (skipped):'));\n    for (const file of plan.filesToSkip) {\n      lines.push(`  ${pc.dim('=')} ${pc.dim(file)}`);\n    }\n    lines.push('');\n  }\n\n  // Cleanup\n  lines.push(...formatCleanup(plan));\n\n  // Affected directories\n  if (plan.affectedDirs.length > 0) {\n    lines.push('');\n    lines.push(pc.cyan('Directories for AGENTS.md regeneration:'));\n    for (const dir of plan.affectedDirs) {\n      lines.push(`  ${dir}`);\n    }\n  }\n\n  lines.push('');\n  return lines.join('\\n');\n}\n\n/**\n * Update command - incrementally updates documentation based on git changes.\n *\n * This command:\n * 1. Checks git repository status\n * 2. Detects files changed since last run (via content-hash comparison)\n * 3. Cleans up orphaned .sum files\n * 4. Resolves an AI CLI backend and creates the AI service\n * 5. Analyzes changed files concurrently via CommandRunner\n * 6. Regenerates AGENTS.md for affected directories\n * 7. Writes telemetry run log and prints run summary\n *\n * Exit codes: 0 = all success, 1 = partial failure, 2 = total failure / no CLI\n */\nexport async function updateCommand(\n  targetPath: string,\n  options: UpdateOptions\n): Promise<void> {\n  const absolutePath = await findProjectRoot(path.resolve(targetPath));\n  const logger = createLogger({ colors: true });\n\n  logger.info(`Checking for updates in: ${absolutePath}`);\n\n  // Create trace writer (moved earlier to use in config loading and orchestrator)\n  const tracer = createTraceWriter(absolutePath, options.trace ?? false);\n  if (options.trace && tracer.filePath) {\n    console.error(pc.dim(`[trace] Writing to ${tracer.filePath}`));\n  }\n\n  // Load configuration\n  const config = await loadConfig(absolutePath, {\n    tracer,\n    debug: options.debug,\n  });\n\n  // Create orchestrator\n  const orchestrator = createUpdateOrchestrator(config, absolutePath, {\n    tracer,\n    debug: options.debug,\n  });\n\n  try {\n    // Compute preliminary variant for plan creation (uses config/CLI values before backend resolution)\n    const preliminaryVariant = options.eval\n      ? `${options.backend ?? config.ai.backend}.${options.model ?? config.ai.model}`\n      : undefined;\n\n    // Prepare update plan\n    const plan = await orchestrator.preparePlan({\n      includeUncommitted: options.uncommitted,\n      dryRun: options.dryRun,\n      variant: preliminaryVariant,\n    });\n\n    // Display plan\n    console.log(formatPlan(plan));\n\n    // Handle first run\n    if (plan.isFirstRun) {\n      console.log(pc.yellow('Hint: Run \"are generate\" first to create initial documentation.'));\n      console.log(pc.yellow('Then run \"are update\" after making changes.'));\n      return;\n    }\n\n    // Handle no changes\n    if (plan.filesToAnalyze.length === 0 &&\n        plan.cleanup.deletedSumFiles.length === 0 &&\n        plan.cleanup.deletedAgentsMd.length === 0) {\n      console.log(pc.green('All files are up to date.'));\n      return;\n    }\n\n    if (options.dryRun) {\n      logger.info('Dry run complete. No files written.');\n      return;\n    }\n\n    // -------------------------------------------------------------------------\n    // Backend resolution (same pattern as generate command)\n    // -------------------------------------------------------------------------\n\n    const registry = createBackendRegistry();\n    let backend;\n    try {\n      backend = await resolveBackend(registry, options.backend ?? config.ai.backend);\n    } catch (error) {\n      if (error instanceof AIServiceError && error.code === 'CLI_NOT_FOUND') {\n        console.error(pc.red('Error: No AI CLI found.\\n'));\n        console.error(getInstallInstructions(registry));\n        process.exit(2);\n      }\n      throw error;\n    }\n\n    // Provision backend-specific resources (e.g., OpenCode agent config)\n    await backend.ensureProjectConfig?.(absolutePath);\n\n    // Resolve effective model (CLI flag > config)\n    const effectiveModel = options.model ?? config.ai.model;\n\n    // Compute eval variant\n    const variant = options.eval ? `${backend.name}.${effectiveModel}` : undefined;\n\n    // Debug: log backend info\n    if (options.debug) {\n      console.log(pc.dim(`[debug] Backend: ${backend.name}`));\n      console.log(pc.dim(`[debug] CLI command: ${backend.cliCommand}`));\n      console.log(pc.dim(`[debug] Model: ${effectiveModel}`));\n      if (variant) {\n        console.log(pc.dim(`[debug] Eval variant: ${variant}`));\n      }\n    }\n\n    if (variant) {\n      console.log(pc.cyan(`[eval] Variant: ${variant}`));\n      console.log(pc.dim(`[eval] Output files: *.${variant}.sum, AGENTS.${variant}.md`));\n    }\n\n    // -------------------------------------------------------------------------\n    // AI service setup\n    // -------------------------------------------------------------------------\n\n    const aiService = new AIService(backend, {\n      timeoutMs: config.ai.timeoutMs,\n      maxRetries: config.ai.maxRetries,\n      model: effectiveModel,\n      command: 'update',\n      telemetry: { keepRuns: config.ai.telemetry.keepRuns },\n    }, consoleLogger);\n\n    if (options.debug) {\n      aiService.setDebug(true);\n    }\n\n    // Determine concurrency\n    const concurrency = options.concurrency ?? config.ai.concurrency;\n\n    // Enable subprocess output logging alongside tracing\n    if (options.trace) {\n      const logDir = path.join(\n        absolutePath, '.agents-reverse-engineer', 'subprocess-logs',\n        new Date().toISOString().replace(/[:.]/g, '-'),\n      );\n      aiService.setSubprocessLogDir(logDir);\n      console.error(pc.dim(`[trace] Subprocess logs → ${logDir}`));\n    }\n\n    // Create progress log for tail -f monitoring\n    const progressLog = ProgressLog.create(absolutePath);\n    progressLog.write(`=== ARE Update (${new Date().toISOString()}) ===`);\n    progressLog.write(`Project: ${absolutePath}`);\n    progressLog.write(`Files to analyze: ${plan.filesToAnalyze.length} | Directories: ${plan.affectedDirs.length}`);\n    progressLog.write('');\n\n    // Create command runner\n    const runner = new CommandRunner(aiService, {\n      concurrency,\n      failFast: options.failFast,\n      debug: options.debug,\n      tracer,\n      progressLog,\n      variant,\n    });\n\n    // -------------------------------------------------------------------------\n    // Phase 1: File analysis via CommandRunner (concurrent AI calls)\n    // -------------------------------------------------------------------------\n\n    const runStart = Date.now();\n    const summary = await runner.executeUpdate(\n      plan.fileTasks,\n      absolutePath,\n      config,\n    );\n\n    // -------------------------------------------------------------------------\n    // Phase 2: AGENTS.md regeneration for affected directories\n    // -------------------------------------------------------------------------\n\n    let dirsCompleted = 0;\n    let dirsFailed = 0;\n\n    if (plan.affectedDirs.length > 0) {\n      const knownDirs = new Set(plan.affectedDirs);\n      const phase2Start = Date.now();\n\n      // Emit phase start\n      tracer.emit({\n        type: 'phase:start',\n        phase: 'update-phase-dir-regen',\n        taskCount: plan.affectedDirs.length,\n        concurrency: 1,\n      });\n\n      const dirReporter = new ProgressReporter(0, plan.affectedDirs.length, progressLog);\n      for (const dir of plan.affectedDirs) {\n        const taskStart = Date.now();\n        const taskLabel = dir || '.';\n\n        // Emit task:start\n        tracer.emit({\n          type: 'task:start',\n          taskLabel,\n          phase: 'update-phase-dir-regen',\n        });\n\n        const dirPath = dir === '.' ? absolutePath : path.join(absolutePath, dir);\n        dirReporter.onDirectoryStart(dir || '.');\n        try {\n          // Read existing generated AGENTS.md for incremental update context\n          let existingAgentsMd: string | undefined;\n          const agentsFilename = variant ? `AGENTS.${variant}.md` : 'AGENTS.md';\n          try {\n            const agentsContent = await readFile(path.join(dirPath, agentsFilename), 'utf-8');\n            if (agentsContent.includes(GENERATED_MARKER_PREFIX)) {\n              existingAgentsMd = agentsContent;\n            }\n          } catch {\n            // No existing AGENTS.md — will generate from scratch\n          }\n\n          const prompt = await buildDirectoryPrompt(dirPath, absolutePath, options.debug, knownDirs, undefined, existingAgentsMd, undefined, variant);\n          const response = await aiService.call({\n            prompt: prompt.user,\n            systemPrompt: prompt.system,\n          });\n          await writeAgentsMd(dirPath, absolutePath, response.text, variant);\n          if (variant) {\n            await writeAgentsMdHub(dirPath, variant);\n          }\n          await writeClaudeMdPointer(dirPath);\n          const dirDurationMs = Date.now() - taskStart;\n          dirReporter.onDirectoryDone(\n            dir || '.',\n            dirDurationMs,\n            response.inputTokens,\n            response.outputTokens,\n            response.model,\n            response.cacheReadTokens,\n            response.cacheCreationTokens,\n          );\n          dirsCompleted++;\n\n          // Emit task:done (success)\n          tracer.emit({\n            type: 'task:done',\n            workerId: 0,\n            taskIndex: dirsCompleted - 1,\n            taskLabel,\n            durationMs: Date.now() - taskStart,\n            success: true,\n            activeTasks: 0,\n          });\n        } catch (error) {\n          dirsFailed++;\n          const errorMsg = error instanceof Error ? error.message : String(error);\n          console.log(`${pc.dim('[dir]')} ${pc.yellow('WARN')} ${dir || '.'}: ${errorMsg}`);\n\n          // Emit task:done (failure)\n          tracer.emit({\n            type: 'task:done',\n            workerId: 0,\n            taskIndex: dirsCompleted + dirsFailed - 1,\n            taskLabel,\n            durationMs: Date.now() - taskStart,\n            success: false,\n            error: errorMsg,\n            activeTasks: 0,\n          });\n        }\n      }\n\n      // Emit phase end\n      tracer.emit({\n        type: 'phase:end',\n        phase: 'update-phase-dir-regen',\n        durationMs: Date.now() - phase2Start,\n        tasksCompleted: dirsCompleted,\n        tasksFailed: dirsFailed,\n      });\n    }\n\n    // -------------------------------------------------------------------------\n    // Print combined summary (files + directories)\n    // -------------------------------------------------------------------------\n\n    const aiSummary = aiService.getSummary();\n    summary.dirsProcessed = dirsCompleted;\n    summary.dirsFailed = dirsFailed;\n    summary.totalCalls = aiSummary.totalCalls;\n    summary.totalInputTokens = aiSummary.totalInputTokens;\n    summary.totalOutputTokens = aiSummary.totalOutputTokens;\n    summary.totalCacheReadTokens = aiSummary.totalCacheReadTokens;\n    summary.totalCacheCreationTokens = aiSummary.totalCacheCreationTokens;\n    summary.totalFilesRead = aiSummary.totalFilesRead;\n    summary.uniqueFilesRead = aiSummary.uniqueFilesRead;\n    summary.errorCount = aiSummary.errorCount;\n    summary.totalDurationMs = Date.now() - runStart;\n\n    const summaryReporter = new ProgressReporter(0, 0, progressLog);\n    summaryReporter.printSummary(summary);\n\n    // -------------------------------------------------------------------------\n    // Telemetry finalization\n    // -------------------------------------------------------------------------\n\n    await aiService.finalize(absolutePath);\n\n    // Finalize progress log, trace, and clean up old trace files\n    await progressLog.finalize();\n    await tracer.finalize();\n    if (options.trace) {\n      await cleanupOldTraces(absolutePath);\n    }\n\n    // -------------------------------------------------------------------------\n    // Record run state (no-op in frontmatter mode, kept for API compatibility)\n    // -------------------------------------------------------------------------\n\n    const filesSkipped = plan.filesToSkip.length;\n    await orchestrator.recordRun(\n      plan.currentCommit,\n      summary.filesProcessed,\n      filesSkipped\n    );\n\n    // -------------------------------------------------------------------------\n    // Exit code: 0 = all success, 1 = partial failure, 2 = total failure\n    // -------------------------------------------------------------------------\n\n    if (summary.filesProcessed === 0 && summary.filesFailed > 0) {\n      process.exit(2);\n    } else if (summary.filesFailed > 0) {\n      process.exit(1);\n    }\n    // Exit code 0 -- all files succeeded (or no files to process)\n\n  } finally {\n    orchestrator.close();\n  }\n}\n\n```\n\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n- Workflow & convention rules: if this file defines contribution guidelines, PR conventions, commit standards, testing mandates, tool usage requirements, approval workflows, AI agent behavioral instructions, or code conventions (naming standards, formatting rules, import ordering, linting policies), extract them as explicit, actionable rules. Sources: CONTRIBUTING.md, CI configs, PR templates, style guides, linter/formatter configs (.eslintrc, .prettierrc, .editorconfig), tsconfig strictness, README dev sections.\n\n\nTARGET LENGTH (MANDATORY):\n- Source file: 16861 characters\n- Target summary: ~1686 characters (10% compression)\n- Maximum: 2023 characters\n- Achieve this by: ultra-dense writing, minimal examples, single-sentence descriptions\n\nULTRA-COMPRESSION TECHNIQUES:\n- Use telegraphic style: \"exports foo, bar, baz\" not \"this file exports three functions\"\n- No code examples unless critical to understanding\n- One sentence per export maximum\n- Omit obvious parameter names (\"opts: Options\" not \"options: Options - configuration object\")\n- Use abbreviations: \"params\" not \"parameters\", \"config\" not \"configuration\"\n- Skip section if empty (don't write \"No special concerns\" or \"N/A\")\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nWORKFLOW & CONVENTION RULES (NEVER EXCLUDE):\n- Contribution requirements: branch naming, commit message formats, PR description templates\n- Testing mandates: required test types, coverage thresholds, \"never delete tests\" policies\n- Tool usage rules: required CLI commands, browser automation tools, linter/formatter requirements\n- Approval workflows: review requirements, CI checks that must pass, deployment gates\n- AI agent instructions: explicit directives for automated assistants (coding rules, prohibited actions)\n- Code conventions: naming standards (camelCase, PascalCase, snake_case for what), import ordering rules, file/folder naming patterns, formatting rules (indentation, quotes, semicolons), language idioms, TypeScript strictness policies, preferred patterns vs anti-patterns\nThese define how contributors (human or AI) must work within the project.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion .annex.sum file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**update.ts implements the incremental documentation update command via git diff detection, concurrent AI analysis through CommandRunner, and AGENTS.md regeneration for affected directories.**\n\n## Exports\n\n**updateCommand**(targetPath: string, options: UpdateOptions): Promise<void> — Main CLI entry point; detects changed files since last run via content-hash, resolves AI backend, analyzes files concurrently via CommandRunner orchestrator, regenerates AGENTS.md per affected directory, writes telemetry and progress logs, exits with 0 (success), 1 (partial failure), or 2 (total failure/no CLI).\n\n**UpdateOptions** interface — Configuration for update command:\n- `uncommitted?: boolean` — include staged + working directory changes\n- `dryRun?: boolean` — show plan without writing\n- `concurrency?: number` — concurrent AI calls\n- `failFast?: boolean` — stop on first file analysis failure\n- `debug?: boolean` — show AI prompts and backend details\n- `trace?: boolean` — enable concurrency tracing to .agents-reverse-engineer/traces/\n- `model?: string` — override AI model (e.g., \"sonnet\", \"opus\")\n- `backend?: string` — override AI backend (e.g., \"claude\", \"codex\", \"opencode\", \"gemini\")\n- `eval?: boolean` — namespace output by backend.model for A/B comparison\n\n## Workflow & Execution Flow\n\nupdateCommand executes seven ordered phases:\n\n1. **Git & plan preparation**: findProjectRoot resolves absolute path; createUpdateOrchestrator().preparePlan() detects changed files via content-hash, marks first-run/no-changes conditions, surfaces filesToAnalyze, filesToSkip, cleanup deletions (deletedSumFiles, deletedAgentsMd), affectedDirs.\n2. **First-run / no-changes bailout**: exits early if plan.isFirstRun or no file deltas.\n3. **Dry-run bailout**: returns without writing if dryRun=true.\n4. **Backend resolution**: createBackendRegistry() + resolveBackend() validates CLI availability (exits code 2 if CLI_NOT_FOUND); backend.ensureProjectConfig?() provisions backend-specific resources (e.g., OpenCode agent config).\n5. **AI service initialization**: new AIService(backend, {...}) with model from options.model or config; setSubprocessLogDir() if tracing enabled; setDebug() if debug=true.\n6. **Phase 1 – File analysis**: CommandRunner.executeUpdate(plan.fileTasks, absolutePath, config) runs file analysis concurrently (concurrency from options or config); emits tracer events (phase:start, task:start, task:done, phase:end).\n7. **Phase 2 – Directory AGENTS.md regeneration**: iterates plan.affectedDirs sequentially (concurrency=1); per directory: readFile agentsFilename (variant-aware: AGENTS.${variant}.md or AGENTS.md), check for GENERATED_MARKER_PREFIX to detect handwritten vs. generated, buildDirectoryPrompt(dirPath, absolutePath, debug, knownDirs, undefined, existingAgentsMd, undefined, variant), aiService.call(), writeAgentsMd(dirPath, absolutePath, response.text, variant), writeAgentsMdHub() if variant, writeClaudeMdPointer().\n8. **Telemetry & finalization**: aggregates summary (filesProcessed, filesFailed, dirsProcessed, dirsFailed, totalCalls, token counts, totalDurationMs); ProgressReporter.printSummary(summary); aiService.finalize(absolutePath); progressLog.finalize(); tracer.finalize(); cleanupOldTraces() if trace=true; orchestrator.recordRun(currentCommit, filesProcessed, filesSkipped); exit(0/1/2).\n\n## Variant (Eval) Mode\n\nWhen eval=true, outputs are namespace-prefixed: `*.${backend.name}.${model}.sum` files and `AGENTS.${variant}.md` for directory regeneration. Variant computed from options.backend/options.model (preliminary) or backend.name/effectiveModel (final after CLI resolution). Debug output prints `[eval] Variant: ${variant}` and `[eval] Output files: *.${variant}.sum, AGENTS.${variant}.md`.\n\n## Error Handling & Exit Codes\n\n- Exit 2: total failure (filesProcessed=0 && filesFailed>0 OR CLI not found via AIServiceError.code='CLI_NOT_FOUND')\n- Exit 1: partial failure (filesFailed>0)\n- Exit 0: all success or no files to process\n- Directory regen failures (phase 2) emit task:done with success=false and log as WARN; do not halt execution.\n\n## Key Dependencies\n\n- **createUpdateOrchestrator**(config, absolutePath, {tracer, debug}) → UpdatePlan with filesToAnalyze, filesToSkip, cleanup, affectedDirs, isFirstRun, currentCommit\n- **CommandRunner**(aiService, {concurrency, failFast, debug, tracer, progressLog, variant}) → runner.executeUpdate(fileTasks, absolutePath, config)\n- **AIService**(backend, {timeoutMs, maxRetries, model, command, telemetry}, consoleLogger) → aiService.call({prompt, systemPrompt}) → {text, inputTokens, outputTokens, model, cacheReadTokens, cacheCreationTokens}\n- **createBackendRegistry**() + **resolveBackend**(registry, backendName) → backend with name, cliCommand, ensureProjectConfig?()\n- **buildDirectoryPrompt**(dirPath, absolutePath, debug, knownDirs, undefined, existingAgentsMd, undefined, variant) → {user, system}\n- **writeAgentsMd, writeAgentsMdHub, writeClaudeMdPointer** — output writers for AGENTS.md, hub index, claude.md pointer\n- **ProgressLog.create**(absolutePath) → progressLog.write(), finalize()\n- **createTraceWriter**(absolutePath, enabled) → tracer.emit({type, ...}), finalize()\n- **cleanupOldTraces**(absolutePath)\n\n## Formatting & Display\n\n- **formatPlan**(plan): renders plan summary with status markers: `+` (added, green), `R` (renamed, blue), `M` (modified, yellow); displays file counts, affected directories, cleanup actions; hints \"are generate\" on first run.\n- **formatCleanup**(plan): formats deletedSumFiles and deletedAgentsMd arrays as red `-` prefixed lists.\n- Color codes via picocolors: yellow (warnings, hints), cyan (sections, eval output), red (errors, cleanup), green (success, added files), blue (renamed), dim (secondary info, debug).\n\n## Tracing & Debugging\n\nWhen trace=true: creates .agents-reverse-engineer/traces/${ISO}.json; emits phase/task lifecycle events with phase, taskLabel, durationMs, success, error, activeTasks; subprocess logs written to .agents-reverse-engineer/subprocess-logs/${ISO timestamp}/ via aiService.setSubprocessLogDir(). When debug=true: logs backend name, cliCommand, effectiveModel, variant; aiService.setDebug(true) enables verbose output.",
      "model": "claude-haiku-4-5-20251001",
      "inputTokens": 3,
      "outputTokens": 1740,
      "cacheReadTokens": 16147,
      "cacheCreationTokens": 7562,
      "latencyMs": 39202,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/cli/update.ts",
          "sizeBytes": 16865
        }
      ]
    },
    {
      "timestamp": "2026-02-14T13:47:20.057Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: src/imports/index.ts\n\n```typescript\nexport { extractImports, extractDirectoryImports, formatImportMap } from './extractor.js';\nexport type { ImportEntry, FileImports } from './types.js';\n\n```\n\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n- Workflow & convention rules: if this file defines contribution guidelines, PR conventions, commit standards, testing mandates, tool usage requirements, approval workflows, AI agent behavioral instructions, or code conventions (naming standards, formatting rules, import ordering, linting policies), extract them as explicit, actionable rules. Sources: CONTRIBUTING.md, CI configs, PR templates, style guides, linter/formatter configs (.eslintrc, .prettierrc, .editorconfig), tsconfig strictness, README dev sections.\n\n\nTARGET LENGTH (MANDATORY):\n- Source file: 151 characters\n- Target summary: ~15 characters (10% compression)\n- Maximum: 18 characters\n- Achieve this by: ultra-dense writing, minimal examples, single-sentence descriptions\n\nULTRA-COMPRESSION TECHNIQUES:\n- Use telegraphic style: \"exports foo, bar, baz\" not \"this file exports three functions\"\n- No code examples unless critical to understanding\n- One sentence per export maximum\n- Omit obvious parameter names (\"opts: Options\" not \"options: Options - configuration object\")\n- Use abbreviations: \"params\" not \"parameters\", \"config\" not \"configuration\"\n- Skip section if empty (don't write \"No special concerns\" or \"N/A\")\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nWORKFLOW & CONVENTION RULES (NEVER EXCLUDE):\n- Contribution requirements: branch naming, commit message formats, PR description templates\n- Testing mandates: required test types, coverage thresholds, \"never delete tests\" policies\n- Tool usage rules: required CLI commands, browser automation tools, linter/formatter requirements\n- Approval workflows: review requirements, CI checks that must pass, deployment gates\n- AI agent instructions: explicit directives for automated assistants (coding rules, prohibited actions)\n- Code conventions: naming standards (camelCase, PascalCase, snake_case for what), import ordering rules, file/folder naming patterns, formatting rules (indentation, quotes, semicolons), language idioms, TypeScript strictness policies, preferred patterns vs anti-patterns\nThese define how contributors (human or AI) must work within the project.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion .annex.sum file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**src/imports/index.ts re-exports import extraction utilities and type definitions from submodules.**\n\n## Exports\n\n- `extractImports` — extracts imports from individual files\n- `extractDirectoryImports` — extracts imports from directory trees\n- `formatImportMap` — formats import maps for output\n- `ImportEntry` — type for individual import entries\n- `FileImports` — type for file-level import collections\n\n## Module Structure\n\nBarrel export aggregating `./extractor.js` (functions) and `./types.js` (types) into single public API surface.",
      "model": "claude-haiku-4-5-20251001",
      "inputTokens": 3,
      "outputTokens": 136,
      "cacheReadTokens": 16147,
      "cacheCreationTokens": 2701,
      "latencyMs": 16705,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/imports/index.ts",
          "sizeBytes": 151
        }
      ]
    },
    {
      "timestamp": "2026-02-14T13:47:20.144Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: src/imports/types.ts\n\n```typescript\n/** A single import statement extracted from source. */\nexport interface ImportEntry {\n  /** The raw import specifier as written in source (e.g., '../ai/index.js') */\n  specifier: string;\n  /** Imported symbols (e.g., ['AIService', 'AIResponse']) */\n  symbols: string[];\n  /** Whether this is a type-only import */\n  typeOnly: boolean;\n}\n\n/** All imports from a single source file. */\nexport interface FileImports {\n  /** Relative file path (e.g., 'runner.ts') */\n  fileName: string;\n  /** External imports (from other modules, not same directory) */\n  externalImports: ImportEntry[];\n  /** Internal imports (from same directory) */\n  internalImports: ImportEntry[];\n}\n\n```\n\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n- Workflow & convention rules: if this file defines contribution guidelines, PR conventions, commit standards, testing mandates, tool usage requirements, approval workflows, AI agent behavioral instructions, or code conventions (naming standards, formatting rules, import ordering, linting policies), extract them as explicit, actionable rules. Sources: CONTRIBUTING.md, CI configs, PR templates, style guides, linter/formatter configs (.eslintrc, .prettierrc, .editorconfig), tsconfig strictness, README dev sections.\n\n\nTARGET LENGTH (MANDATORY):\n- Source file: 669 characters\n- Target summary: ~67 characters (10% compression)\n- Maximum: 80 characters\n- Achieve this by: ultra-dense writing, minimal examples, single-sentence descriptions\n\nULTRA-COMPRESSION TECHNIQUES:\n- Use telegraphic style: \"exports foo, bar, baz\" not \"this file exports three functions\"\n- No code examples unless critical to understanding\n- One sentence per export maximum\n- Omit obvious parameter names (\"opts: Options\" not \"options: Options - configuration object\")\n- Use abbreviations: \"params\" not \"parameters\", \"config\" not \"configuration\"\n- Skip section if empty (don't write \"No special concerns\" or \"N/A\")\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nWORKFLOW & CONVENTION RULES (NEVER EXCLUDE):\n- Contribution requirements: branch naming, commit message formats, PR description templates\n- Testing mandates: required test types, coverage thresholds, \"never delete tests\" policies\n- Tool usage rules: required CLI commands, browser automation tools, linter/formatter requirements\n- Approval workflows: review requirements, CI checks that must pass, deployment gates\n- AI agent instructions: explicit directives for automated assistants (coding rules, prohibited actions)\n- Code conventions: naming standards (camelCase, PascalCase, snake_case for what), import ordering rules, file/folder naming patterns, formatting rules (indentation, quotes, semicolons), language idioms, TypeScript strictness policies, preferred patterns vs anti-patterns\nThese define how contributors (human or AI) must work within the project.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion .annex.sum file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**types.ts defines TypeScript interfaces for representing parsed import statements and file-level import collections.**\n\n## Exported Types\n\n`ImportEntry` — represents single import statement with `specifier: string`, `symbols: string[]`, `typeOnly: boolean`.\n\n`FileImports` — represents all imports from source file with `fileName: string`, `externalImports: ImportEntry[]`, `internalImports: ImportEntry[]`.\n\n## Design\n\nSeparates external vs internal imports by origin (other modules vs same directory). Type-only imports flagged via `typeOnly` boolean. Symbols captured as string array to preserve import names.",
      "model": "claude-haiku-4-5-20251001",
      "inputTokens": 3,
      "outputTokens": 140,
      "cacheReadTokens": 16147,
      "cacheCreationTokens": 2847,
      "latencyMs": 16707,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/imports/types.ts",
          "sizeBytes": 669
        }
      ]
    },
    {
      "timestamp": "2026-02-14T13:47:16.799Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: src/imports/extractor.ts\n\n```typescript\nimport { readFile } from 'node:fs/promises';\nimport * as path from 'node:path';\nimport type { ImportEntry, FileImports } from './types.js';\n\n/**\n * Regex matching TypeScript/JavaScript import statements.\n *\n * Captures:\n * - Group 1: 'type' keyword if present (type-only import)\n * - Group 2: named symbols (between braces)\n * - Group 3: namespace import (* as name)\n * - Group 4: default import (bare identifier)\n * - Group 5: module specifier (the string after 'from')\n */\nconst IMPORT_REGEX =\n  /^import\\s+(type\\s+)?(?:\\{([^}]*)\\}|(\\*\\s+as\\s+\\w+)|(\\w+))\\s+from\\s+['\"]([^'\"]+)['\"]/gm;\n\n/**\n * Extract import statements from source content.\n *\n * Only processes lines starting with 'import' to avoid matching\n * dynamic imports or imports inside comments/strings.\n */\nexport function extractImports(sourceContent: string): ImportEntry[] {\n  const entries: ImportEntry[] = [];\n  let match;\n  IMPORT_REGEX.lastIndex = 0;\n\n  while ((match = IMPORT_REGEX.exec(sourceContent)) !== null) {\n    const typeOnly = !!match[1];\n    const namedSymbols = match[2]; // { Foo, Bar }\n    const namespaceImport = match[3]; // * as name\n    const defaultImport = match[4]; // name\n    const specifier = match[5];\n\n    let symbols: string[] = [];\n    if (namedSymbols) {\n      symbols = namedSymbols\n        .split(',')\n        .map((s) => s.trim().replace(/\\s+as\\s+\\w+/, ''))\n        .filter(Boolean);\n    } else if (namespaceImport) {\n      symbols = [namespaceImport.replace('* as ', '').trim()];\n    } else if (defaultImport) {\n      symbols = [defaultImport];\n    }\n\n    entries.push({ specifier, symbols, typeOnly });\n  }\n\n  return entries;\n}\n\n/**\n * Extract and classify imports for all source files in a directory.\n *\n * Reads only the first 100 lines of each file (imports are at the top)\n * for performance. Classifies imports as internal (same directory via './')\n * or external (everything else).\n *\n * Skips node: built-ins and bare package specifiers (npm packages).\n */\nexport async function extractDirectoryImports(\n  dirPath: string,\n  fileNames: string[],\n): Promise<FileImports[]> {\n  const results: FileImports[] = [];\n\n  for (const fileName of fileNames) {\n    const filePath = path.join(dirPath, fileName);\n    try {\n      const content = await readFile(filePath, 'utf-8');\n      // Only read import region (first 100 lines) for performance\n      const importRegion = content.split('\\n').slice(0, 100).join('\\n');\n      const imports = extractImports(importRegion);\n\n      // Filter out bare specifiers (npm packages) and node: builtins\n      const relativeImports = imports.filter(\n        (i) => i.specifier.startsWith('.') || i.specifier.startsWith('..'),\n      );\n\n      const internal = relativeImports.filter((i) =>\n        i.specifier.startsWith('./'),\n      );\n      const external = relativeImports.filter((i) =>\n        i.specifier.startsWith('../'),\n      );\n\n      if (external.length > 0 || internal.length > 0) {\n        results.push({\n          fileName,\n          externalImports: external,\n          internalImports: internal,\n        });\n      }\n    } catch {\n      // Skip unreadable files\n    }\n  }\n\n  return results;\n}\n\n/**\n * Format import data as a structured text block for LLM prompts.\n *\n * Example output:\n * ```\n * runner.ts:\n *   ../ai/index.js → AIService\n *   ../generation/executor.js → ExecutionPlan, ExecutionTask\n *\n * pool.ts:\n *   ./trace.js → ITraceWriter (type)\n * ```\n */\nexport function formatImportMap(fileImports: FileImports[]): string {\n  const sections: string[] = [];\n\n  for (const fi of fileImports) {\n    const lines: string[] = [`${fi.fileName}:`];\n    for (const imp of fi.externalImports) {\n      const typeTag = imp.typeOnly ? ' (type)' : '';\n      lines.push(`  ${imp.specifier} → ${imp.symbols.join(', ')}${typeTag}`);\n    }\n    if (lines.length > 1) {\n      sections.push(lines.join('\\n'));\n    }\n  }\n\n  return sections.join('\\n\\n');\n}\n\n```\n\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n- Workflow & convention rules: if this file defines contribution guidelines, PR conventions, commit standards, testing mandates, tool usage requirements, approval workflows, AI agent behavioral instructions, or code conventions (naming standards, formatting rules, import ordering, linting policies), extract them as explicit, actionable rules. Sources: CONTRIBUTING.md, CI configs, PR templates, style guides, linter/formatter configs (.eslintrc, .prettierrc, .editorconfig), tsconfig strictness, README dev sections.\n\n\nTARGET LENGTH (MANDATORY):\n- Source file: 3896 characters\n- Target summary: ~390 characters (10% compression)\n- Maximum: 468 characters\n- Achieve this by: ultra-dense writing, minimal examples, single-sentence descriptions\n\nULTRA-COMPRESSION TECHNIQUES:\n- Use telegraphic style: \"exports foo, bar, baz\" not \"this file exports three functions\"\n- No code examples unless critical to understanding\n- One sentence per export maximum\n- Omit obvious parameter names (\"opts: Options\" not \"options: Options - configuration object\")\n- Use abbreviations: \"params\" not \"parameters\", \"config\" not \"configuration\"\n- Skip section if empty (don't write \"No special concerns\" or \"N/A\")\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nWORKFLOW & CONVENTION RULES (NEVER EXCLUDE):\n- Contribution requirements: branch naming, commit message formats, PR description templates\n- Testing mandates: required test types, coverage thresholds, \"never delete tests\" policies\n- Tool usage rules: required CLI commands, browser automation tools, linter/formatter requirements\n- Approval workflows: review requirements, CI checks that must pass, deployment gates\n- AI agent instructions: explicit directives for automated assistants (coding rules, prohibited actions)\n- Code conventions: naming standards (camelCase, PascalCase, snake_case for what), import ordering rules, file/folder naming patterns, formatting rules (indentation, quotes, semicolons), language idioms, TypeScript strictness policies, preferred patterns vs anti-patterns\nThese define how contributors (human or AI) must work within the project.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion .annex.sum file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**extractor.ts parses TypeScript/JavaScript import statements from source files and formats them for LLM consumption.**\n\n## Exports\n\n- `extractImports(sourceContent: string): ImportEntry[]` — extracts import entries from source content using regex matching.\n- `extractDirectoryImports(dirPath: string, fileNames: string[]): Promise<FileImports[]>` — reads and classifies imports across multiple files, filtering to first 100 lines per file; categorizes as internal (`./`) or external (`../`); skips bare npm packages and `node:` builtins.\n- `formatImportMap(fileImports: FileImports[]): string` — formats FileImports array as structured text block for LLM prompts, grouping by file with `specifier → symbols` format and `(type)` tag for type-only imports.\n\n## Behavioral Contracts\n\n**IMPORT_REGEX** pattern (global, multiline):\n```\n/^import\\s+(type\\s+)?(?:\\{([^}]*)\\}|(\\*\\s+as\\s+\\w+)|(\\w+))\\s+from\\s+['\"]([^'\"]+)['\"]/gm\n```\nCaptures: (1) `type` keyword, (2) named symbols `{...}`, (3) namespace `* as name`, (4) default import, (5) module specifier.\n\n**Import extraction logic**: parses Group 2 (named) via `.split(',').map(s => s.trim().replace(/\\s+as\\s+\\w+/, ''))` to strip aliasing; Group 3 (namespace) via `.replace('* as ', '')` to isolate name; Group 4 (default) used as-is.\n\n**Filter rules**: `extractDirectoryImports` skips specifiers not starting with `.` or `..` (excludes npm packages and `node:` builtins); classifies internal as `./`, external as `../`.\n\n**Performance optimization**: reads only first 100 lines per file (imports at top).\n\n**Format output template**: `{fileName}:\\n  {specifier} → {symbols}{typeTag}\\n\\n` with `typeTag = ' (type)'` for type-only imports.",
      "model": "claude-haiku-4-5-20251001",
      "inputTokens": 3,
      "outputTokens": 518,
      "cacheReadTokens": 16147,
      "cacheCreationTokens": 3992,
      "latencyMs": 20314,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/imports/extractor.ts",
          "sizeBytes": 3904
        }
      ]
    },
    {
      "timestamp": "2026-02-14T13:47:21.460Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: src/installer/banner.ts\n\n```typescript\n/**\n * ASCII banner and styled output for the installer\n *\n * Provides colored banner display, help text, and styled message helpers.\n * Uses picocolors for terminal coloring.\n */\n\nimport pc from 'picocolors';\nimport { getVersion } from '../version.js';\n\n/** Package version read from package.json */\nexport const VERSION = getVersion();\n\n/**\n * Display the ASCII banner at installer launch\n *\n * Shows big ASCII art \"ARE\" letters in green with version and tagline.\n */\nexport function displayBanner(): void {\n  const art = pc.green;\n  const dim = pc.dim;\n\n  console.log();\n  console.log(art('  █████╗ ██████╗ ███████╗'));\n  console.log(art(' ██╔══██╗██╔══██╗██╔════╝'));\n  console.log(art(' ███████║██████╔╝█████╗  '));\n  console.log(art(' ██╔══██║██╔══██╗██╔══╝  '));\n  console.log(art(' ██║  ██║██║  ██║███████╗'));\n  console.log(art(' ╚═╝  ╚═╝╚═╝  ╚═╝╚══════╝'));\n  console.log();\n  console.log(dim(` agents-reverse-engineer v${VERSION}`));\n  console.log(dim(' AI-friendly codebase documentation'));\n  console.log();\n}\n\n/**\n * Display help text showing usage, flags, and examples\n */\nexport function showHelp(): void {\n  console.log(pc.bold('Usage:') + ' npx agents-reverse-engineer [options]');\n  console.log();\n  console.log(pc.bold('Options:'));\n  console.log('  --runtime <runtime>  Select runtime: claude, codex, opencode, gemini, or all');\n  console.log('  -g, --global         Install to global config (~/.claude, ~/.agents, etc.)');\n  console.log('  -l, --local          Install to local project (./.claude, ./.agents, etc.)');\n  console.log('  -u, --uninstall      Remove installed files');\n  console.log('  --force              Overwrite existing files');\n  console.log('  -q, --quiet          Suppress banner and info messages');\n  console.log('  -h, --help           Show this help');\n  console.log();\n  console.log(pc.bold('Examples:'));\n  console.log('  npx agents-reverse-engineer');\n  console.log('    Interactive mode - prompts for runtime and location');\n  console.log();\n  console.log('  npx agents-reverse-engineer --runtime claude -g');\n  console.log('    Install Claude Code commands globally');\n  console.log();\n  console.log('  npx agents-reverse-engineer --runtime codex -g');\n  console.log('    Install Codex commands globally');\n  console.log();\n  console.log('  npx agents-reverse-engineer --runtime all -l');\n  console.log('    Install commands for all runtimes to local project');\n  console.log();\n  console.log('  npx agents-reverse-engineer --runtime claude -g -u');\n  console.log('    Uninstall global Claude Code commands');\n}\n\n/**\n * Display a success message with green checkmark prefix\n *\n * @param msg - Message to display\n */\nexport function showSuccess(msg: string): void {\n  console.log(pc.green('✓') + ' ' + msg);\n}\n\n/**\n * Display an error message with red X prefix\n *\n * @param msg - Message to display\n */\nexport function showError(msg: string): void {\n  console.log(pc.red('✗') + ' ' + msg);\n}\n\n/**\n * Display a warning message with yellow exclamation prefix\n *\n * @param msg - Message to display\n */\nexport function showWarning(msg: string): void {\n  console.log(pc.yellow('!') + ' ' + msg);\n}\n\n/**\n * Display an info message with cyan arrow prefix\n *\n * @param msg - Message to display\n */\nexport function showInfo(msg: string): void {\n  console.log(pc.cyan('>') + ' ' + msg);\n}\n\n/**\n * Display post-install next steps\n *\n * Shows what to do after installation with helpful links.\n *\n * @param runtime - Which runtime was installed\n * @param filesCreated - Number of files created\n */\nexport function showNextSteps(runtime: string, filesCreated: number): void {\n  console.log();\n  console.log(pc.bold('Installation complete!'));\n  console.log(pc.dim(`${filesCreated} files installed for ${runtime}`));\n  console.log();\n  console.log(pc.bold('Next steps:'));\n  console.log('  1. Run ' + pc.cyan('/are-help') + ' in your AI assistant to verify');\n  console.log('  2. Run ' + pc.cyan('/are-init') + ' to initialize a project');\n  console.log('  3. Run ' + pc.cyan('/are-discover') + ' to create the generation plan');\n  console.log('  4. Run ' + pc.cyan('/are-generate') + ' to generate documentation');\n  console.log('  5. Run ' + pc.cyan('/are-update') + ' to update documentation after changes');\n  console.log('  6. Run ' + pc.cyan('/are-specify') + ' to generate a specification document');\n  console.log('  7. Run ' + pc.cyan('/are-clean') + ' to remove all generated artifacts');\n  console.log();\n  console.log(pc.dim('Docs: https://github.com/GeoloeG-IsT/agents-reverse-engineer'));\n}\n\n```\n\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n- Workflow & convention rules: if this file defines contribution guidelines, PR conventions, commit standards, testing mandates, tool usage requirements, approval workflows, AI agent behavioral instructions, or code conventions (naming standards, formatting rules, import ordering, linting policies), extract them as explicit, actionable rules. Sources: CONTRIBUTING.md, CI configs, PR templates, style guides, linter/formatter configs (.eslintrc, .prettierrc, .editorconfig), tsconfig strictness, README dev sections.\n\n\nTARGET LENGTH (MANDATORY):\n- Source file: 4542 characters\n- Target summary: ~454 characters (10% compression)\n- Maximum: 545 characters\n- Achieve this by: ultra-dense writing, minimal examples, single-sentence descriptions\n\nULTRA-COMPRESSION TECHNIQUES:\n- Use telegraphic style: \"exports foo, bar, baz\" not \"this file exports three functions\"\n- No code examples unless critical to understanding\n- One sentence per export maximum\n- Omit obvious parameter names (\"opts: Options\" not \"options: Options - configuration object\")\n- Use abbreviations: \"params\" not \"parameters\", \"config\" not \"configuration\"\n- Skip section if empty (don't write \"No special concerns\" or \"N/A\")\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nWORKFLOW & CONVENTION RULES (NEVER EXCLUDE):\n- Contribution requirements: branch naming, commit message formats, PR description templates\n- Testing mandates: required test types, coverage thresholds, \"never delete tests\" policies\n- Tool usage rules: required CLI commands, browser automation tools, linter/formatter requirements\n- Approval workflows: review requirements, CI checks that must pass, deployment gates\n- AI agent instructions: explicit directives for automated assistants (coding rules, prohibited actions)\n- Code conventions: naming standards (camelCase, PascalCase, snake_case for what), import ordering rules, file/folder naming patterns, formatting rules (indentation, quotes, semicolons), language idioms, TypeScript strictness policies, preferred patterns vs anti-patterns\nThese define how contributors (human or AI) must work within the project.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion .annex.sum file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**banner.ts exports styled console output functions and version constant for installer UI.**\n\n## Exports\n\n- `VERSION` — string constant from `getVersion()`, displayed in banner\n- `displayBanner()` — void; prints ASCII \"ARE\" art in green with version/tagline using `pc.green()` and `pc.dim()`\n- `showHelp()` — void; prints usage text covering `--runtime`, `-g/--global`, `-l/--local`, `-u/--uninstall`, `--force`, `-q/--quiet`, `-h/--help` flags and examples\n- `showSuccess(msg: string)` — void; prefixes msg with green `✓`\n- `showError(msg: string)` — void; prefixes msg with red `✗`\n- `showWarning(msg: string)` — void; prefixes msg with yellow `!`\n- `showInfo(msg: string)` — void; prefixes msg with cyan `>`\n- `showNextSteps(runtime: string, filesCreated: number)` — void; displays post-install steps referencing `/are-help`, `/are-init`, `/are-discover`, `/are-generate`, `/are-update`, `/are-specify`, `/are-clean` commands\n\n## Design\n\nUses picocolors (`pc`) for terminal styling: `green()`, `red()`, `yellow()`, `cyan()`, `bold()`, `dim()`. All messaging functions follow prefix pattern: colored symbol + space + text.",
      "model": "claude-haiku-4-5-20251001",
      "inputTokens": 3,
      "outputTokens": 344,
      "cacheReadTokens": 16147,
      "cacheCreationTokens": 4119,
      "latencyMs": 18097,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/installer/banner.ts",
          "sizeBytes": 4804
        }
      ]
    },
    {
      "timestamp": "2026-02-14T13:47:15.476Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: src/generation/executor.ts\n\n```typescript\n/**\n * Plan executor for documentation generation\n *\n * Builds execution plans from generation plans:\n * - File tasks as individual analysis jobs\n * - Directory completion tracking\n * - Markdown plan output for dry-run display\n */\n\nimport * as path from 'node:path';\nimport type { GenerationPlan } from '../orchestration/orchestrator.js';\nimport { getSumPath, sumFileExists } from './writers/sum.js';\n\n/**\n * Execution task ready for AI processing.\n */\nexport interface ExecutionTask {\n  /** Unique task ID */\n  id: string;\n  /** Task type */\n  type: 'file' | 'directory';\n  /** File or directory path (relative) */\n  path: string;\n  /** Absolute path */\n  absolutePath: string;\n  /** System prompt for AI */\n  systemPrompt: string;\n  /** User prompt for AI */\n  userPrompt: string;\n  /** Dependencies (task IDs that must complete first) */\n  dependencies: string[];\n  /** Output path for generated content */\n  outputPath: string;\n  /** Metadata for tracking */\n  metadata: {\n    directoryFiles?: string[];\n    /** Directory depth (for post-order traversal) */\n    depth?: number;\n    /** Package root path (for supplementary docs) */\n    packageRoot?: string;\n  };\n}\n\n/**\n * Execution plan with dependency graph.\n */\nexport interface ExecutionPlan {\n  /** Project root */\n  projectRoot: string;\n  /** All tasks in execution order */\n  tasks: ExecutionTask[];\n  /** File tasks (can run in parallel) */\n  fileTasks: ExecutionTask[];\n  /** Directory tasks (depend on file tasks) */\n  directoryTasks: ExecutionTask[];\n  /** Directory to file mapping */\n  directoryFileMap: Record<string, string[]>;\n  /** Compact project directory listing for directory prompt context */\n  projectStructure?: string;\n  /** Files skipped due to existing .sum artifacts */\n  skippedFiles?: string[];\n  /** Directories skipped due to existing AGENTS.md */\n  skippedDirs?: string[];\n}\n\n/**\n * Calculate directory depth (number of path segments).\n * Root \".\" has depth 0, \"src\" has depth 1, \"src/cli\" has depth 2, etc.\n */\nfunction getDirectoryDepth(dir: string): number {\n  if (dir === '.') return 0;\n  return dir.split(path.sep).length;\n}\n\n/**\n * Build execution plan from generation plan.\n *\n * Directory tasks are sorted using post-order traversal (deepest directories first)\n * so child AGENTS.md files are generated before their parents.\n *\n * @param plan - Generation plan from orchestrator\n * @param projectRoot - Absolute path to project root\n * @param variant - Optional eval variant name (e.g., \"claude.haiku\")\n */\nexport function buildExecutionPlan(\n  plan: GenerationPlan,\n  projectRoot: string,\n  variant?: string,\n): ExecutionPlan {\n  const fileTasks: ExecutionTask[] = [];\n  const directoryTasks: ExecutionTask[] = [];\n  const directoryFileMap: Record<string, string[]> = {};\n\n  // Track files by directory — use all discovered files (including skipped)\n  // so directory tasks know about ALL child .sum files for prompt building\n  const allFiles = plan.allDiscoveredFiles ?? plan.files;\n  for (const file of allFiles) {\n    const dir = path.dirname(file.relativePath);\n    if (!directoryFileMap[dir]) {\n      directoryFileMap[dir] = [];\n    }\n    directoryFileMap[dir].push(file.relativePath);\n  }\n\n  // Create file tasks\n  for (const task of plan.tasks) {\n    if (task.type === 'file') {\n      const absolutePath = path.join(projectRoot, task.filePath);\n      fileTasks.push({\n        id: `file:${task.filePath}`,\n        type: 'file',\n        path: task.filePath,\n        absolutePath,\n        systemPrompt: task.systemPrompt!,\n        userPrompt: task.userPrompt!,\n        dependencies: [],\n        outputPath: getSumPath(absolutePath, variant),\n        metadata: {},\n      });\n    }\n  }\n\n  // Sort file tasks by directory depth (deepest first) for post-order traversal\n  fileTasks.sort((a, b) => {\n    const depthA = getDirectoryDepth(path.dirname(a.path));\n    const depthB = getDirectoryDepth(path.dirname(b.path));\n    return depthB - depthA;\n  });\n\n  // Collect directories that need processing (from filtered plan tasks)\n  const plannedDirs = new Set<string>();\n  for (const task of plan.tasks) {\n    if (task.type === 'directory') {\n      plannedDirs.add(task.filePath);\n    }\n  }\n\n  // Create directory tasks in post-order (deepest first)\n  // Sort directories by depth descending so children are processed before parents\n  // Only include directories that are in the filtered plan (not skipped)\n  const sortedDirs = Object.entries(directoryFileMap).sort(\n    ([dirA], [dirB]) => getDirectoryDepth(dirB) - getDirectoryDepth(dirA)\n  );\n\n  for (const [dir, files] of sortedDirs) {\n    // Skip directories not in the filtered plan (already have up-to-date AGENTS.md).\n    // If plannedDirs is empty, no directory work is needed.\n    if (!plannedDirs.has(dir)) continue;\n\n    const dirAbsPath = path.join(projectRoot, dir);\n    const fileTaskIds = files.map(f => `file:${f}`);\n\n    const agentsFilename = variant ? `AGENTS.${variant}.md` : 'AGENTS.md';\n    directoryTasks.push({\n      id: `dir:${dir}`,\n      type: 'directory',\n      path: dir,\n      absolutePath: dirAbsPath,\n      systemPrompt: 'Built at execution time by buildDirectoryPrompt()',\n      userPrompt: `Directory \"${dir}\" — ${files.length} files. Prompt populated from .sum files at runtime.`,\n      dependencies: fileTaskIds,\n      outputPath: path.join(dirAbsPath, agentsFilename),\n      metadata: {\n        directoryFiles: files,\n        depth: getDirectoryDepth(dir),\n      },\n    });\n  }\n\n  return {\n    projectRoot,\n    tasks: [...fileTasks, ...directoryTasks],\n    fileTasks,\n    directoryTasks,\n    directoryFileMap,\n    projectStructure: plan.projectStructure,\n    skippedFiles: plan.skippedFiles,\n    skippedDirs: plan.skippedDirs,\n  };\n}\n\n/**\n * Check if all files in a directory have been analyzed (.sum files exist).\n *\n * @param dirPath - Directory path\n * @param expectedFiles - Relative file paths expected in this directory\n * @param projectRoot - Absolute path to project root\n * @param variant - Optional eval variant name\n */\nexport async function isDirectoryComplete(\n  dirPath: string,\n  expectedFiles: string[],\n  projectRoot: string,\n  variant?: string,\n): Promise<{ complete: boolean; missing: string[] }> {\n  const missing: string[] = [];\n\n  for (const relativePath of expectedFiles) {\n    const absolutePath = path.join(projectRoot, relativePath);\n    const exists = await sumFileExists(absolutePath, variant);\n    if (!exists) {\n      missing.push(relativePath);\n    }\n  }\n\n  return {\n    complete: missing.length === 0,\n    missing,\n  };\n}\n\n/**\n * Get all directories that are ready for AGENTS.md generation.\n * A directory is ready when all its files have .sum files.\n *\n * @param executionPlan - The execution plan\n * @param variant - Optional eval variant name\n */\nexport async function getReadyDirectories(\n  executionPlan: ExecutionPlan,\n  variant?: string,\n): Promise<string[]> {\n  const ready: string[] = [];\n\n  for (const [dir, files] of Object.entries(executionPlan.directoryFileMap)) {\n    const { complete } = await isDirectoryComplete(\n      dir,\n      files,\n      executionPlan.projectRoot,\n      variant,\n    );\n    if (complete) {\n      ready.push(dir);\n    }\n  }\n\n  return ready;\n}\n\n/**\n * Format execution plan as markdown for GENERATION-PLAN.md.\n * Uses post-order traversal (deepest directories first).\n */\nexport function formatExecutionPlanAsMarkdown(plan: ExecutionPlan): string {\n  const lines: string[] = [];\n  const today = new Date().toISOString().split('T')[0];\n\n  // Header\n  lines.push('# Documentation Generation Plan');\n  lines.push('');\n  lines.push(`Generated: ${today}`);\n  lines.push(`Project: ${plan.projectRoot}`);\n  lines.push('');\n\n  // Summary\n  lines.push('## Summary');\n  lines.push('');\n  lines.push(`- **Total Tasks**: ${plan.tasks.length}`);\n  lines.push(`- **File Tasks**: ${plan.fileTasks.length}`);\n  if (plan.skippedFiles && plan.skippedFiles.length > 0) {\n    lines.push(`- **Files Skipped**: ${plan.skippedFiles.length} (existing .sum)`);\n  }\n  lines.push(`- **Directory Tasks**: ${plan.directoryTasks.length}`);\n  if (plan.skippedDirs && plan.skippedDirs.length > 0) {\n    lines.push(`- **Dirs Skipped**: ${plan.skippedDirs.length} (existing AGENTS.md)`);\n  }\n  lines.push('- **Traversal**: Post-order (children before parents)');\n  lines.push('');\n  lines.push('---');\n  lines.push('');\n\n  // Phase 1: File Analysis\n  lines.push('## Phase 1: File Analysis (Post-Order Traversal)');\n  lines.push('');\n\n  // Group files by directory, use directory task order (already post-order)\n  // Deduplicate paths\n  const filesByDir: Record<string, Set<string>> = {};\n  for (const task of plan.fileTasks) {\n    const dir = task.path.includes('/')\n      ? task.path.substring(0, task.path.lastIndexOf('/'))\n      : '.';\n    if (!filesByDir[dir]) filesByDir[dir] = new Set();\n    filesByDir[dir].add(task.path);\n  }\n\n  // Output files grouped by directory in post-order (using directoryTasks order)\n  for (const dirTask of plan.directoryTasks) {\n    const dir = dirTask.path;\n    const filesSet = filesByDir[dir];\n    if (filesSet && filesSet.size > 0) {\n      const files = Array.from(filesSet);\n      const depth = dirTask.metadata.depth ?? 0;\n      lines.push(`### Depth ${depth}: ${dir}/ (${files.length} files)`);\n      for (const file of files) {\n        lines.push(`- [ ] \\`${file}\\``);\n      }\n      lines.push('');\n    }\n  }\n\n  lines.push('---');\n  lines.push('');\n\n  // Phase 2: Directory AGENTS.md\n  lines.push(`## Phase 2: Directory AGENTS.md (Post-Order Traversal, ${plan.directoryTasks.length} directories)`);\n  lines.push('');\n\n  // Group by depth\n  const dirsByDepth: Record<number, string[]> = {};\n  for (const task of plan.directoryTasks) {\n    const depth = task.metadata.depth ?? 0;\n    if (!dirsByDepth[depth]) dirsByDepth[depth] = [];\n    dirsByDepth[depth].push(task.path);\n  }\n\n  // Output in depth order (descending)\n  const depths = Object.keys(dirsByDepth).map(Number).sort((a, b) => b - a);\n  for (const depth of depths) {\n    lines.push(`### Depth ${depth}`);\n    for (const dir of dirsByDepth[depth]) {\n      const suffix = dir === '.' ? ' (root)' : '';\n      lines.push(`- [ ] \\`${dir}/AGENTS.md\\`${suffix}`);\n    }\n    lines.push('');\n  }\n\n  // Skipped section (if any files/dirs were skipped)\n  if ((plan.skippedFiles && plan.skippedFiles.length > 0) ||\n      (plan.skippedDirs && plan.skippedDirs.length > 0)) {\n    lines.push('---');\n    lines.push('');\n    lines.push('## Skipped (Existing Artifacts)');\n    lines.push('');\n\n    if (plan.skippedFiles && plan.skippedFiles.length > 0) {\n      lines.push(`### Files (${plan.skippedFiles.length} with existing .sum)`);\n      for (const f of plan.skippedFiles) {\n        lines.push(`- \\`${f}\\``);\n      }\n      lines.push('');\n    }\n\n    if (plan.skippedDirs && plan.skippedDirs.length > 0) {\n      lines.push(`### Directories (${plan.skippedDirs.length} with existing AGENTS.md)`);\n      for (const d of plan.skippedDirs) {\n        lines.push(`- \\`${d}/AGENTS.md\\``);\n      }\n      lines.push('');\n    }\n  }\n\n  return lines.join('\\n');\n}\n\n```\n\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n- Workflow & convention rules: if this file defines contribution guidelines, PR conventions, commit standards, testing mandates, tool usage requirements, approval workflows, AI agent behavioral instructions, or code conventions (naming standards, formatting rules, import ordering, linting policies), extract them as explicit, actionable rules. Sources: CONTRIBUTING.md, CI configs, PR templates, style guides, linter/formatter configs (.eslintrc, .prettierrc, .editorconfig), tsconfig strictness, README dev sections.\n\n\nTARGET LENGTH (MANDATORY):\n- Source file: 11055 characters\n- Target summary: ~1106 characters (10% compression)\n- Maximum: 1327 characters\n- Achieve this by: ultra-dense writing, minimal examples, single-sentence descriptions\n\nULTRA-COMPRESSION TECHNIQUES:\n- Use telegraphic style: \"exports foo, bar, baz\" not \"this file exports three functions\"\n- No code examples unless critical to understanding\n- One sentence per export maximum\n- Omit obvious parameter names (\"opts: Options\" not \"options: Options - configuration object\")\n- Use abbreviations: \"params\" not \"parameters\", \"config\" not \"configuration\"\n- Skip section if empty (don't write \"No special concerns\" or \"N/A\")\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nWORKFLOW & CONVENTION RULES (NEVER EXCLUDE):\n- Contribution requirements: branch naming, commit message formats, PR description templates\n- Testing mandates: required test types, coverage thresholds, \"never delete tests\" policies\n- Tool usage rules: required CLI commands, browser automation tools, linter/formatter requirements\n- Approval workflows: review requirements, CI checks that must pass, deployment gates\n- AI agent instructions: explicit directives for automated assistants (coding rules, prohibited actions)\n- Code conventions: naming standards (camelCase, PascalCase, snake_case for what), import ordering rules, file/folder naming patterns, formatting rules (indentation, quotes, semicolons), language idioms, TypeScript strictness policies, preferred patterns vs anti-patterns\nThese define how contributors (human or AI) must work within the project.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion .annex.sum file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**executor.ts builds execution plans from generation plans, transforming file/directory analysis tasks into ordered execution graphs with dependency tracking and post-order traversal for cascading AGENTS.md generation.**\n\n## Exported Types\n\n`ExecutionTask` — task object with id, type ('file'|'directory'), path, absolutePath, systemPrompt, userPrompt, dependencies (string[]), outputPath, metadata (directoryFiles?: string[], depth?: number, packageRoot?: string).\n\n`ExecutionPlan` — plan object with projectRoot, tasks, fileTasks, directoryTasks, directoryFileMap (Record<string, string[]>), projectStructure?, skippedFiles?, skippedDirs?.\n\n## Exported Functions\n\n`buildExecutionPlan(plan: GenerationPlan, projectRoot: string, variant?: string): ExecutionPlan` — transforms GenerationPlan into ExecutionPlan with post-order sorted directory tasks (deepest first); tracks all discovered files in directoryFileMap; creates file tasks with dependencies=[]; creates directory tasks with fileTaskIds as dependencies; skips directories not in filtered plan.\n\n`isDirectoryComplete(dirPath: string, expectedFiles: string[], projectRoot: string, variant?: string): Promise<{complete: boolean; missing: string[]}>` — checks if all files in expectedFiles have .sum artifacts via sumFileExists(); returns missing file list.\n\n`getReadyDirectories(executionPlan: ExecutionPlan, variant?: string): Promise<string[]>` — returns directories where all files have .sum files (isDirectoryComplete returns complete=true).\n\n`formatExecutionPlanAsMarkdown(plan: ExecutionPlan): string` — generates GENERATION-PLAN.md with header (date, projectRoot), Summary section (task counts, skipped counts, \"Post-order\" traversal note), Phase 1 (file tasks grouped by directory depth), Phase 2 (directory tasks grouped by depth), Skipped section.\n\n## Key Internals\n\n`getDirectoryDepth(dir: string): number` — calculates path segment count: '.' = 0, 'src' = 1, 'src/cli' = 2; determines post-order traversal order.\n\nPost-order traversal ensures child AGENTS.md generation completes before parents; fileTasks and directoryTasks sorted by depth descending; directoryFileMap includes all discovered files (plan.allDiscoveredFiles ?? plan.files) even if skipped, so directory prompts have complete context.\n\nDirectory task dependencies are fileTaskIds (`file:${f}` format) from directoryFileMap entries; only directories in plan.tasks (plannedDirs) become ExecutionTasks.\n\nVariant parameter (e.g., \"claude.haiku\") propagates through getSumPath() and sumFileExists() for conditional artifact naming (AGENTS.md vs AGENTS.{variant}.md).",
      "model": "claude-haiku-4-5-20251001",
      "inputTokens": 3,
      "outputTokens": 656,
      "cacheReadTokens": 16147,
      "cacheCreationTokens": 6198,
      "latencyMs": 25409,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/generation/executor.ts",
          "sizeBytes": 11059
        }
      ]
    },
    {
      "timestamp": "2026-02-14T13:47:23.318Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: src/installer/paths.ts\n\n```typescript\n/**\n * Path resolution for installer module\n *\n * Provides cross-platform path resolution for AI coding assistant runtimes.\n * Uses os.homedir() for global paths and path.join() for cross-platform compatibility.\n */\n\nimport * as os from 'node:os';\nimport * as path from 'node:path';\nimport { stat } from 'node:fs/promises';\nimport type { Runtime, Location, RuntimePaths } from './types.js';\n\n/**\n * Get all supported runtime identifiers (excludes 'all' meta-runtime)\n *\n * @returns Array of concrete runtime identifiers\n */\nexport function getAllRuntimes(): Array<Exclude<Runtime, 'all'>> {\n  return ['claude', 'codex', 'opencode', 'gemini'];\n}\n\n/**\n * Get path configuration for a specific runtime\n *\n * Returns global and local installation paths plus settings file location.\n * All paths are cross-platform using os.homedir() and path.join().\n *\n * Environment variable overrides (in priority order):\n * - Claude: CLAUDE_CONFIG_DIR\n * - Codex: none (Codex skills follow ~/.agents and .agents)\n * - OpenCode: OPENCODE_CONFIG_DIR > XDG_CONFIG_HOME/opencode\n * - Gemini: GEMINI_CONFIG_DIR\n *\n * @param runtime - Target runtime (claude, codex, opencode, or gemini)\n * @returns Path configuration object with global, local, and settingsFile paths\n */\nexport function getRuntimePaths(runtime: Exclude<Runtime, 'all'>): RuntimePaths {\n  const home = os.homedir();\n\n  switch (runtime) {\n    case 'claude': {\n      // CLAUDE_CONFIG_DIR overrides default ~/.claude\n      const globalPath = process.env.CLAUDE_CONFIG_DIR || path.join(home, '.claude');\n      return {\n        global: globalPath,\n        local: '.claude',\n        settingsFile: path.join(globalPath, 'settings.json'),\n      };\n    }\n\n    case 'codex': {\n      // Codex skills follow ~/.agents and .agents (repo-local)\n      const globalPath = path.join(home, '.agents');\n      return {\n        global: globalPath,\n        local: '.agents',\n        settingsFile: path.join(globalPath, 'settings.json'),\n      };\n    }\n\n    case 'opencode': {\n      // OPENCODE_CONFIG_DIR > XDG_CONFIG_HOME/opencode > ~/.config/opencode\n      const xdgConfig = process.env.XDG_CONFIG_HOME || path.join(home, '.config');\n      const globalPath = process.env.OPENCODE_CONFIG_DIR || path.join(xdgConfig, 'opencode');\n      return {\n        global: globalPath,\n        local: '.opencode',\n        settingsFile: path.join(globalPath, 'settings.json'),\n      };\n    }\n\n    case 'gemini': {\n      // GEMINI_CONFIG_DIR overrides default ~/.gemini\n      const globalPath = process.env.GEMINI_CONFIG_DIR || path.join(home, '.gemini');\n      return {\n        global: globalPath,\n        local: '.gemini',\n        settingsFile: path.join(globalPath, 'settings.json'),\n      };\n    }\n  }\n}\n\n/**\n * Resolve full installation path for a runtime and location\n *\n * For global location, returns the absolute global path.\n * For local location, returns the local path joined with project root.\n *\n * @param runtime - Target runtime (claude, codex, opencode, or gemini)\n * @param location - Installation location (global or local)\n * @param projectRoot - Project root directory for local installs (defaults to cwd)\n * @returns Resolved absolute path for installation\n */\nexport function resolveInstallPath(\n  runtime: Exclude<Runtime, 'all'>,\n  location: Location,\n  projectRoot?: string,\n): string {\n  const paths = getRuntimePaths(runtime);\n\n  if (location === 'global') {\n    return paths.global;\n  }\n\n  // Local installation - join with project root or cwd\n  const root = projectRoot || process.cwd();\n  return path.join(root, paths.local);\n}\n\n/**\n * Resolve Codex CLI config directory for global/local scope.\n *\n * Codex command rules live under `<config>/rules/*.rules`.\n * This is separate from ARE skill install paths (`.agents` / `~/.agents`).\n */\nexport function resolveCodexConfigPath(\n  location: Location,\n  projectRoot?: string,\n): string {\n  if (location === 'global') {\n    return path.join(os.homedir(), '.codex');\n  }\n\n  const root = projectRoot || process.cwd();\n  return path.join(root, '.codex');\n}\n\n/**\n * Get the settings file path for a runtime\n *\n * Settings files are used for hook registration (Claude Code uses settings.json).\n *\n * @param runtime - Target runtime (claude, codex, opencode, or gemini)\n * @returns Absolute path to the settings file\n */\nexport function getSettingsPath(runtime: Exclude<Runtime, 'all'>): string {\n  return getRuntimePaths(runtime).settingsFile;\n}\n\n/**\n * Check if a runtime is installed locally in a project.\n *\n * Checks for the presence of the local config directory (e.g., .claude, .agents, .opencode, .gemini).\n *\n * @param runtime - Target runtime (claude, codex, opencode, or gemini)\n * @param projectRoot - Project root directory to check\n * @returns True if the runtime's local config directory exists\n */\nexport async function isRuntimeInstalledLocally(\n  runtime: Exclude<Runtime, 'all'>,\n  projectRoot: string\n): Promise<boolean> {\n  const paths = getRuntimePaths(runtime);\n  const localPath = path.join(projectRoot, paths.local);\n\n  try {\n    const stats = await stat(localPath);\n    return stats.isDirectory();\n  } catch {\n    return false;\n  }\n}\n\n/**\n * Check if a runtime is installed globally.\n *\n * Checks for the presence of the global config directory.\n *\n * @param runtime - Target runtime (claude, codex, opencode, or gemini)\n * @returns True if the runtime's global config directory exists\n */\nexport async function isRuntimeInstalledGlobally(\n  runtime: Exclude<Runtime, 'all'>\n): Promise<boolean> {\n  const paths = getRuntimePaths(runtime);\n\n  try {\n    const stats = await stat(paths.global);\n    return stats.isDirectory();\n  } catch {\n    return false;\n  }\n}\n\n/**\n * Get list of runtimes installed locally in a project.\n *\n * @param projectRoot - Project root directory to check\n * @returns Array of runtime identifiers that are installed locally\n */\nexport async function getInstalledRuntimes(\n  projectRoot: string\n): Promise<Array<Exclude<Runtime, 'all'>>> {\n  const runtimes = getAllRuntimes();\n  const installed: Array<Exclude<Runtime, 'all'>> = [];\n\n  for (const runtime of runtimes) {\n    if (await isRuntimeInstalledLocally(runtime, projectRoot)) {\n      installed.push(runtime);\n    }\n  }\n\n  return installed;\n}\n\n```\n\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n- Workflow & convention rules: if this file defines contribution guidelines, PR conventions, commit standards, testing mandates, tool usage requirements, approval workflows, AI agent behavioral instructions, or code conventions (naming standards, formatting rules, import ordering, linting policies), extract them as explicit, actionable rules. Sources: CONTRIBUTING.md, CI configs, PR templates, style guides, linter/formatter configs (.eslintrc, .prettierrc, .editorconfig), tsconfig strictness, README dev sections.\n\n\nTARGET LENGTH (MANDATORY):\n- Source file: 6227 characters\n- Target summary: ~623 characters (10% compression)\n- Maximum: 748 characters\n- Achieve this by: ultra-dense writing, minimal examples, single-sentence descriptions\n\nULTRA-COMPRESSION TECHNIQUES:\n- Use telegraphic style: \"exports foo, bar, baz\" not \"this file exports three functions\"\n- No code examples unless critical to understanding\n- One sentence per export maximum\n- Omit obvious parameter names (\"opts: Options\" not \"options: Options - configuration object\")\n- Use abbreviations: \"params\" not \"parameters\", \"config\" not \"configuration\"\n- Skip section if empty (don't write \"No special concerns\" or \"N/A\")\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nWORKFLOW & CONVENTION RULES (NEVER EXCLUDE):\n- Contribution requirements: branch naming, commit message formats, PR description templates\n- Testing mandates: required test types, coverage thresholds, \"never delete tests\" policies\n- Tool usage rules: required CLI commands, browser automation tools, linter/formatter requirements\n- Approval workflows: review requirements, CI checks that must pass, deployment gates\n- AI agent instructions: explicit directives for automated assistants (coding rules, prohibited actions)\n- Code conventions: naming standards (camelCase, PascalCase, snake_case for what), import ordering rules, file/folder naming patterns, formatting rules (indentation, quotes, semicolons), language idioms, TypeScript strictness policies, preferred patterns vs anti-patterns\nThese define how contributors (human or AI) must work within the project.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion .annex.sum file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**paths.ts resolves cross-platform installation paths for AI runtime config directories (Claude, Codex, OpenCode, Gemini) with environment variable overrides and filesystem detection.**\n\n## Exports\n\n`getAllRuntimes()` → `Array<Exclude<Runtime, 'all'>>` returns concrete runtime identifiers: `['claude', 'codex', 'opencode', 'gemini']`.\n\n`getRuntimePaths(runtime: Exclude<Runtime, 'all'>) → RuntimePaths` returns `{global, local, settingsFile}` with env var fallbacks: `CLAUDE_CONFIG_DIR` (claude), `OPENCODE_CONFIG_DIR` / `XDG_CONFIG_HOME` (opencode), `GEMINI_CONFIG_DIR` (gemini); codex uses fixed `~/.agents` and `.agents`.\n\n`resolveInstallPath(runtime, location, projectRoot?) → string` joins project root with local path (`paths.local`) or returns global path.\n\n`resolveCodexConfigPath(location, projectRoot?) → string` resolves Codex CLI rules directory (`.codex` local, `~/.codex` global), separate from skill install paths.\n\n`getSettingsPath(runtime) → string` returns `RuntimePaths.settingsFile` for hook registration.\n\n`isRuntimeInstalledLocally(runtime, projectRoot) → Promise<boolean>` checks directory existence via `stat()` on local config path.\n\n`isRuntimeInstalledGlobally(runtime) → Promise<boolean>` checks directory existence via `stat()` on global config path.\n\n`getInstalledRuntimes(projectRoot) → Promise<Array<Exclude<Runtime, 'all'>>>` iterates `getAllRuntimes()` with `isRuntimeInstalledLocally()` checks.\n\n## Behavioral Contracts\n\n**Runtime config directory names:** claude=`.claude`, codex=`.agents`, opencode=`.opencode`, gemini=`.gemini`; codex additionally uses `.codex` for CLI rules.\n\n**Settings file location:** `<configPath>/settings.json` for all runtimes; used for hook registration.\n\n**Path resolution:** global paths via `os.homedir()` + env vars; local paths via `path.join(projectRoot || process.cwd(), localDirName)`.\n\n**Error handling:** filesystem checks suppress exceptions and return `false` on missing directories.",
      "model": "claude-haiku-4-5-20251001",
      "inputTokens": 3,
      "outputTokens": 559,
      "cacheReadTokens": 16147,
      "cacheCreationTokens": 4604,
      "latencyMs": 20990,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/installer/paths.ts",
          "sizeBytes": 6227
        }
      ]
    },
    {
      "timestamp": "2026-02-14T13:47:25.932Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: src/installer/types.ts\n\n```typescript\n/**\n * Installer types for npx installation workflow\n *\n * Defines types for the interactive installer that copies command files and hooks\n * to runtime-specific directories (Claude Code, Codex, OpenCode, Gemini).\n */\n\n/**\n * Supported AI coding assistant runtimes for installation\n *\n * - 'claude': Claude Code (~/.claude or .claude)\n * - 'codex': Codex (~/.agents or .agents)\n * - 'opencode': OpenCode (~/.config/opencode or .opencode)\n * - 'gemini': Gemini (~/.gemini or .gemini)\n * - 'all': Install to all supported runtimes\n */\nexport type Runtime = 'claude' | 'codex' | 'opencode' | 'gemini' | 'all';\n\n/**\n * Installation location target\n *\n * - 'global': User-level installation (~/.claude, ~/.agents, ~/.config/opencode, etc.)\n * - 'local': Project-level installation (.claude, .agents, .opencode, etc.)\n */\nexport type Location = 'global' | 'local';\n\n/**\n * Arguments parsed from installer command line\n *\n * Supports both interactive mode (prompts) and non-interactive mode (flags).\n */\nexport interface InstallerArgs {\n  /** Target runtime (claude, codex, opencode, gemini, or all) */\n  runtime?: Runtime;\n  /** Install to global/user location */\n  global: boolean;\n  /** Install to local/project location */\n  local: boolean;\n  /** Uninstall instead of install */\n  uninstall: boolean;\n  /** Force overwrite existing files */\n  force: boolean;\n  /** Show help and exit */\n  help: boolean;\n  /** Suppress banner and info messages */\n  quiet: boolean;\n}\n\n/**\n * Result of an installation operation for a single runtime/location\n */\nexport interface InstallerResult {\n  /** Whether the installation succeeded */\n  success: boolean;\n  /** Runtime that was installed */\n  runtime: Exclude<Runtime, 'all'>;\n  /** Location that was installed to */\n  location: Location;\n  /** Files that were successfully created */\n  filesCreated: string[];\n  /** Files that were skipped (already exist, no --force) */\n  filesSkipped: string[];\n  /** Error messages if any */\n  errors: string[];\n  /** Whether hook was registered in settings.json (Claude/Gemini only) */\n  hookRegistered?: boolean;\n  /** Whether VERSION file was written */\n  versionWritten?: boolean;\n}\n\n/**\n * Path configuration for a specific runtime\n *\n * Contains resolved paths for global and local installation locations.\n */\nexport interface RuntimePaths {\n  /** Global installation path (e.g., ~/.claude) */\n  global: string;\n  /** Local installation path (e.g., .claude) */\n  local: string;\n  /** Path to settings.json for hook registration */\n  settingsFile: string;\n}\n\n```\n\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n- Workflow & convention rules: if this file defines contribution guidelines, PR conventions, commit standards, testing mandates, tool usage requirements, approval workflows, AI agent behavioral instructions, or code conventions (naming standards, formatting rules, import ordering, linting policies), extract them as explicit, actionable rules. Sources: CONTRIBUTING.md, CI configs, PR templates, style guides, linter/formatter configs (.eslintrc, .prettierrc, .editorconfig), tsconfig strictness, README dev sections.\n\n\nTARGET LENGTH (MANDATORY):\n- Source file: 2536 characters\n- Target summary: ~254 characters (10% compression)\n- Maximum: 305 characters\n- Achieve this by: ultra-dense writing, minimal examples, single-sentence descriptions\n\nULTRA-COMPRESSION TECHNIQUES:\n- Use telegraphic style: \"exports foo, bar, baz\" not \"this file exports three functions\"\n- No code examples unless critical to understanding\n- One sentence per export maximum\n- Omit obvious parameter names (\"opts: Options\" not \"options: Options - configuration object\")\n- Use abbreviations: \"params\" not \"parameters\", \"config\" not \"configuration\"\n- Skip section if empty (don't write \"No special concerns\" or \"N/A\")\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nWORKFLOW & CONVENTION RULES (NEVER EXCLUDE):\n- Contribution requirements: branch naming, commit message formats, PR description templates\n- Testing mandates: required test types, coverage thresholds, \"never delete tests\" policies\n- Tool usage rules: required CLI commands, browser automation tools, linter/formatter requirements\n- Approval workflows: review requirements, CI checks that must pass, deployment gates\n- AI agent instructions: explicit directives for automated assistants (coding rules, prohibited actions)\n- Code conventions: naming standards (camelCase, PascalCase, snake_case for what), import ordering rules, file/folder naming patterns, formatting rules (indentation, quotes, semicolons), language idioms, TypeScript strictness policies, preferred patterns vs anti-patterns\nThese define how contributors (human or AI) must work within the project.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion .annex.sum file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**src/installer/types.ts defines type signatures for npx installation workflows across AI coding assistant runtimes (Claude Code, Codex, OpenCode, Gemini).**\n\n## Exported Types\n\n`Runtime` union: 'claude' | 'codex' | 'opencode' | 'gemini' | 'all' — target runtime for installation.\n\n`Location` union: 'global' | 'local' — installation scope (user-level or project-level).\n\n`InstallerArgs` interface — parsed CLI arguments with properties: `runtime?: Runtime`, `global: boolean`, `local: boolean`, `uninstall: boolean`, `force: boolean`, `help: boolean`, `quiet: boolean`.\n\n`InstallerResult` interface — operation outcome per runtime/location with `success: boolean`, `runtime: Exclude<Runtime, 'all'>`, `location: Location`, `filesCreated: string[]`, `filesSkipped: string[]`, `errors: string[]`, `hookRegistered?: boolean`, `versionWritten?: boolean`.\n\n`RuntimePaths` interface — resolved paths with `global: string`, `local: string`, `settingsFile: string`.\n\n## Installation Model\n\nGlobal paths: ~/.claude, ~/.agents, ~/.config/opencode, ~/.gemini. Local paths: .claude, .agents, .opencode, .gemini. Hook registration supported for Claude/Gemini only via settings.json. Force flag (`force: boolean`) overrides existing file skipping.",
      "model": "claude-haiku-4-5-20251001",
      "inputTokens": 3,
      "outputTokens": 339,
      "cacheReadTokens": 16147,
      "cacheCreationTokens": 3466,
      "latencyMs": 20167,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/installer/types.ts",
          "sizeBytes": 2536
        }
      ]
    },
    {
      "timestamp": "2026-02-14T13:47:21.414Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: src/installer/index.ts\n\n```typescript\n/**\n * Main installer entry point for agents-reverse-engineer\n *\n * Provides the runInstaller function for npx installation workflow.\n * Supports interactive prompts and non-interactive flags for CI/scripted installs.\n */\n\nimport type { InstallerArgs, InstallerResult, Runtime, Location } from './types.js';\nimport { getAllRuntimes, resolveInstallPath } from './paths.js';\nimport {\n  displayBanner,\n  showHelp,\n  showSuccess,\n  showError,\n  showWarning,\n  showInfo,\n  showNextSteps,\n} from './banner.js';\nimport { selectRuntime, selectLocation, confirmAction, isInteractive } from './prompts.js';\nimport { installFiles, verifyInstallation, formatInstallResult } from './operations.js';\nimport { uninstallFiles, deleteConfigFolder } from './uninstall.js';\n\n// Re-export types for external consumers\nexport type { InstallerArgs, InstallerResult, Runtime, Location, RuntimePaths } from './types.js';\nexport { getRuntimePaths, getAllRuntimes, resolveInstallPath, getSettingsPath } from './paths.js';\nexport { displayBanner, showHelp, showSuccess, showError, showWarning, showInfo, showNextSteps, VERSION } from './banner.js';\nexport { selectRuntime, selectLocation, confirmAction, isInteractive } from './prompts.js';\n\n/**\n * Parse command-line arguments for the installer\n *\n * Handles both short (-g, -l, -h) and long (--global, --local, --help) flags.\n * Uses pattern from cli/index.ts for consistency.\n *\n * @param args - Command line arguments (process.argv.slice(2))\n * @returns Parsed installer arguments\n */\nexport function parseInstallerArgs(args: string[]): InstallerArgs {\n  const flags = new Set<string>();\n  const values = new Map<string, string>();\n\n  for (let i = 0; i < args.length; i++) {\n    const arg = args[i];\n\n    if (arg === '--runtime' && i + 1 < args.length) {\n      // --runtime requires a value\n      values.set('runtime', args[++i]);\n    } else if (arg === '-g' || arg === '--global') {\n      flags.add('global');\n    } else if (arg === '-l' || arg === '--local') {\n      flags.add('local');\n    } else if (arg === '--force') {\n      flags.add('force');\n    } else if (arg === '-q' || arg === '--quiet') {\n      flags.add('quiet');\n    } else if (arg === '-h' || arg === '--help') {\n      flags.add('help');\n    }\n  }\n\n  // Validate runtime value if provided\n  const runtimeValue = values.get('runtime');\n  const validRuntimes: Runtime[] = ['claude', 'codex', 'opencode', 'gemini', 'all'];\n  const runtime = runtimeValue && validRuntimes.includes(runtimeValue as Runtime)\n    ? (runtimeValue as Runtime)\n    : undefined;\n\n  return {\n    runtime,\n    global: flags.has('global'),\n    local: flags.has('local'),\n    uninstall: false, // Set by 'uninstall' command, not flags\n    force: flags.has('force'),\n    help: flags.has('help'),\n    quiet: flags.has('quiet'),\n  };\n}\n\n/**\n * Determine installation location from args or return undefined for prompt\n *\n * @param args - Parsed installer arguments\n * @returns Location if specified, undefined if needs prompt\n */\nfunction determineLocation(args: InstallerArgs): Location | undefined {\n  if (args.global && !args.local) {\n    return 'global';\n  }\n  if (args.local && !args.global) {\n    return 'local';\n  }\n  // Both or neither - needs interactive prompt\n  return undefined;\n}\n\n/**\n * Determine target runtimes from args\n *\n * @param runtime - Runtime from args (may be 'all' or specific runtime)\n * @returns Array of specific runtimes to install to\n */\nfunction determineRuntimes(runtime: Runtime | undefined): Array<Exclude<Runtime, 'all'>> {\n  if (!runtime) {\n    // No runtime specified - will need interactive prompt\n    return [];\n  }\n  if (runtime === 'all') {\n    return getAllRuntimes();\n  }\n  return [runtime];\n}\n\n/**\n * Run the installer workflow\n *\n * This is the main entry point for the installation process.\n * Supports both interactive mode (prompts) and non-interactive mode (flags).\n *\n * @param args - Parsed installer arguments\n * @returns Array of installation results (one per runtime/location combination)\n */\nexport async function runInstaller(args: InstallerArgs): Promise<InstallerResult[]> {\n  // Handle help flag\n  if (args.help) {\n    showHelp();\n    return [];\n  }\n\n  // Display banner unless quiet mode\n  if (!args.quiet) {\n    displayBanner();\n  }\n\n  // Determine location and runtimes from flags\n  let location = determineLocation(args);\n  const runtimeArg = args.runtime;\n\n  // Non-interactive mode: require all flags\n  if (!isInteractive()) {\n    if (!runtimeArg) {\n      showError('Missing --runtime flag (required in non-interactive mode)');\n      process.exit(1);\n    }\n    if (!location) {\n      showError('Missing -g/--global or -l/--local flag (required in non-interactive mode)');\n      process.exit(1);\n    }\n  }\n\n  // Interactive mode: prompt for missing values\n  const mode = args.uninstall ? 'uninstall' : 'install';\n  let selectedRuntime: Runtime | undefined = runtimeArg;\n  if (!selectedRuntime && isInteractive()) {\n    selectedRuntime = await selectRuntime(mode);\n  }\n\n  if (!location && isInteractive()) {\n    location = await selectLocation(mode);\n  }\n\n  // Safety check - should not reach here without values\n  if (!selectedRuntime || !location) {\n    showError('Unable to determine runtime and location');\n    process.exit(1);\n  }\n\n  // UNINSTALL MODE\n  if (args.uninstall) {\n    return runUninstall(selectedRuntime, location, args.quiet);\n  }\n\n  // INSTALL MODE\n  return runInstall(selectedRuntime, location, args.force, args.quiet);\n}\n\n/**\n * Run the installation workflow\n *\n * @param runtime - Target runtime or 'all'\n * @param location - Installation location\n * @param force - Overwrite existing files\n * @param quiet - Suppress output\n * @returns Array of installation results\n */\nasync function runInstall(\n  runtime: Runtime,\n  location: Location,\n  force: boolean,\n  quiet: boolean,\n): Promise<InstallerResult[]> {\n  // Install files\n  const results = installFiles(runtime, location, { force, dryRun: false });\n\n  // Verify installation\n  const allCreatedFiles = results.flatMap((r) => r.filesCreated);\n  const verification = verifyInstallation(allCreatedFiles);\n\n  if (!verification.success) {\n    showError('Installation verification failed - some files missing:');\n    for (const missing of verification.missing) {\n      showWarning(`  Missing: ${missing}`);\n    }\n  }\n\n  // Display results\n  if (!quiet) {\n    displayInstallResults(results);\n  }\n\n  return results;\n}\n\n/**\n * Run the uninstallation workflow\n *\n * @param runtime - Target runtime or 'all'\n * @param location - Installation location\n * @param quiet - Suppress output\n * @returns Array of uninstallation results\n */\nfunction runUninstall(\n  runtime: Runtime,\n  location: Location,\n  quiet: boolean,\n): InstallerResult[] {\n  const results = uninstallFiles(runtime, location, false);\n\n  // Delete .agents-reverse-engineer config folder (local only)\n  const configDeleted = deleteConfigFolder(location, false);\n\n  // Display results\n  if (!quiet) {\n    displayUninstallResults(results, configDeleted);\n  }\n\n  return results;\n}\n\n/**\n * Display installation results with styled output\n *\n * Shows checkmarks for successful actions, warnings for skipped files,\n * and next steps for using the installed commands.\n *\n * @param results - Array of installation results\n */\nfunction displayInstallResults(results: InstallerResult[]): void {\n  console.log();\n\n  let totalCreated = 0;\n  let totalSkipped = 0;\n  let hooksRegistered = 0;\n\n  for (const result of results) {\n    if (result.success) {\n      showSuccess(`Installed ${result.runtime} (${result.location})`);\n    } else {\n      showError(`Failed to install ${result.runtime} (${result.location})`);\n      for (const err of result.errors) {\n        showWarning(`  ${err}`);\n      }\n    }\n\n    totalCreated += result.filesCreated.length;\n    totalSkipped += result.filesSkipped.length;\n\n    if (result.hookRegistered) {\n      hooksRegistered++;\n    }\n  }\n\n  // Summary\n  console.log();\n  if (totalCreated > 0) {\n    showSuccess(`Created ${totalCreated} command files`);\n  }\n  if (hooksRegistered > 0) {\n    showSuccess(`Registered ${hooksRegistered} session hook(s)`);\n  }\n  if (totalSkipped > 0) {\n    showWarning(`Skipped ${totalSkipped} existing files (use --force to overwrite)`);\n  }\n\n  // Next steps\n  const primaryRuntime = results[0]?.runtime || 'claude';\n  showNextSteps(primaryRuntime, totalCreated);\n\n  // GitHub link\n  console.log();\n  showInfo('Docs: https://github.com/GeoloeG-IsT/agents-reverse-engineer');\n}\n\n/**\n * Display uninstallation results with styled output\n *\n * @param results - Array of uninstallation results\n * @param configDeleted - Whether the .agents-reverse-engineer folder was deleted\n */\nfunction displayUninstallResults(results: InstallerResult[], configDeleted: boolean = false): void {\n  console.log();\n\n  let totalDeleted = 0;\n  let hooksUnregistered = 0;\n\n  for (const result of results) {\n    // In uninstall context, filesCreated tracks deleted files\n    const deletedCount = result.filesCreated.length;\n    const notFoundCount = result.filesSkipped.length;\n\n    if (result.success) {\n      if (deletedCount > 0) {\n        showSuccess(`Uninstalled ${result.runtime} (${result.location}) - ${deletedCount} files removed`);\n      } else {\n        showInfo(`No ${result.runtime} files found in ${result.location}`);\n      }\n    } else {\n      showError(`Failed to uninstall ${result.runtime} (${result.location})`);\n      for (const err of result.errors) {\n        showWarning(`  ${err}`);\n      }\n    }\n\n    totalDeleted += deletedCount;\n\n    // hookRegistered is repurposed for uninstall to mean \"hook was unregistered\"\n    if (result.hookRegistered) {\n      hooksUnregistered++;\n    }\n  }\n\n  // Summary\n  console.log();\n  if (totalDeleted > 0) {\n    showSuccess(`Removed ${totalDeleted} files`);\n  }\n  if (hooksUnregistered > 0) {\n    showSuccess(`Unregistered ${hooksUnregistered} session hook(s)`);\n  }\n  if (configDeleted) {\n    showSuccess(`Removed .agents-reverse-engineer folder`);\n  }\n  if (totalDeleted === 0 && !configDeleted) {\n    showInfo('No files were removed');\n  }\n}\n\n```\n\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n- Workflow & convention rules: if this file defines contribution guidelines, PR conventions, commit standards, testing mandates, tool usage requirements, approval workflows, AI agent behavioral instructions, or code conventions (naming standards, formatting rules, import ordering, linting policies), extract them as explicit, actionable rules. Sources: CONTRIBUTING.md, CI configs, PR templates, style guides, linter/formatter configs (.eslintrc, .prettierrc, .editorconfig), tsconfig strictness, README dev sections.\n\n\nTARGET LENGTH (MANDATORY):\n- Source file: 10075 characters\n- Target summary: ~1008 characters (10% compression)\n- Maximum: 1210 characters\n- Achieve this by: ultra-dense writing, minimal examples, single-sentence descriptions\n\nULTRA-COMPRESSION TECHNIQUES:\n- Use telegraphic style: \"exports foo, bar, baz\" not \"this file exports three functions\"\n- No code examples unless critical to understanding\n- One sentence per export maximum\n- Omit obvious parameter names (\"opts: Options\" not \"options: Options - configuration object\")\n- Use abbreviations: \"params\" not \"parameters\", \"config\" not \"configuration\"\n- Skip section if empty (don't write \"No special concerns\" or \"N/A\")\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nWORKFLOW & CONVENTION RULES (NEVER EXCLUDE):\n- Contribution requirements: branch naming, commit message formats, PR description templates\n- Testing mandates: required test types, coverage thresholds, \"never delete tests\" policies\n- Tool usage rules: required CLI commands, browser automation tools, linter/formatter requirements\n- Approval workflows: review requirements, CI checks that must pass, deployment gates\n- AI agent instructions: explicit directives for automated assistants (coding rules, prohibited actions)\n- Code conventions: naming standards (camelCase, PascalCase, snake_case for what), import ordering rules, file/folder naming patterns, formatting rules (indentation, quotes, semicolons), language idioms, TypeScript strictness policies, preferred patterns vs anti-patterns\nThese define how contributors (human or AI) must work within the project.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion .annex.sum file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**src/installer/index.ts is the main entry point for the agents-reverse-engineer installer, handling interactive/non-interactive workflows for installing or uninstalling CLI commands and session hooks across runtime environments (Claude, Codex, OpenCode, Gemini).**\n\n## Exported Functions\n\n`parseInstallerArgs(args: string[]): InstallerArgs` — parses command-line flags (`--runtime`, `-g`/`--global`, `-l`/`--local`, `--force`, `-q`/`--quiet`, `-h`/`--help`) into structured config; validates runtime against `['claude', 'codex', 'opencode', 'gemini', 'all']`.\n\n`runInstaller(args: InstallerArgs): Promise<InstallerResult[]>` — main orchestrator: displays banner, determines location and runtime from flags or interactive prompts, delegates to `runInstall()` or `runUninstall()`, returns per-runtime results.\n\n## Exported Types & Paths\n\nRe-exports from `./types.js`: `InstallerArgs`, `InstallerResult`, `Runtime`, `Location`, `RuntimePaths`.\nRe-exports from `./paths.js`: `getRuntimePaths`, `getAllRuntimes`, `resolveInstallPath`, `getSettingsPath`.\nRe-exports from `./banner.js`: display functions (`displayBanner`, `showHelp`, `showSuccess`, `showError`, `showWarning`, `showInfo`, `showNextSteps`) and `VERSION`.\nRe-exports from `./prompts.js`: `selectRuntime`, `selectLocation`, `confirmAction`, `isInteractive`.\n\n## Mode Determination & Workflows\n\n`determineLocation(args: InstallerArgs): Location | undefined` — returns `'global'` if `-g` only, `'local'` if `-l` only, `undefined` if both or neither (requires prompt).\n\n`determineRuntimes(runtime: Runtime | undefined): Array<Exclude<Runtime, 'all'>>` — expands `'all'` to `getAllRuntimes()`, returns empty array if undefined (requires prompt).\n\n`runInstall(runtime, location, force, quiet): Promise<InstallerResult[]>` — calls `installFiles()`, verifies via `verifyInstallation()`, displays results via `displayInstallResults()`.\n\n`runUninstall(runtime, location, quiet): InstallerResult[]` — calls `uninstallFiles()`, calls `deleteConfigFolder()`, displays results via `displayUninstallResults()`.\n\n## Interactive vs Non-Interactive Behavior\n\nNon-interactive mode (when `!isInteractive()`) requires `--runtime` and `-g`/`--local` flags; missing flags trigger error and `process.exit(1)`. Interactive mode prompts via `selectRuntime()` and `selectLocation()` for missing values. Help flag (`-h`/`--help`) calls `showHelp()` and returns empty array.\n\n## Result Formatting\n\n`displayInstallResults(results: InstallerResult[])` — aggregates `filesCreated.length`, `filesSkipped.length`, `hookRegistered` counts; displays per-result success/error, summary counts, next steps via `showNextSteps(primaryRuntime, totalCreated)`, GitHub docs link.\n\n`displayUninstallResults(results: InstallerResult[], configDeleted?: boolean)` — reuses `InstallerResult` fields where `filesCreated` tracks deleted files, `filesSkipped` tracks not-found, `hookRegistered` tracks unregistered hooks; displays aggregate deletions and config folder removal.\n\n## Coupling & Dependencies\n\nImports structured across: `types.js` (interfaces), `paths.js` (path resolution), `banner.js` (styled output), `prompts.js` (interactive selection), `operations.js` (install logic), `uninstall.js` (uninstall logic). All display output routed through `show*` functions for consistent styling.",
      "model": "claude-haiku-4-5-20251001",
      "inputTokens": 3,
      "outputTokens": 923,
      "cacheReadTokens": 16147,
      "cacheCreationTokens": 5766,
      "latencyMs": 25912,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/installer/index.ts",
          "sizeBytes": 10075
        }
      ]
    },
    {
      "timestamp": "2026-02-14T13:47:27.278Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: src/integration/detect.ts\n\n```typescript\n/**\n * Environment detection for AI coding assistants\n *\n * Detects which AI coding assistant environments are present in a project\n * by checking for their configuration directories and files.\n */\n\nimport { existsSync } from 'node:fs';\nimport * as path from 'node:path';\nimport type { DetectedEnvironment, EnvironmentType } from './types.js';\n\n/**\n * Detect all AI coding assistant environments present in a project\n *\n * Checks for:\n * - Claude Code: .claude/ directory OR CLAUDE.md file\n * - OpenCode: .opencode/ directory\n * - Aider: .aider.conf.yml file OR .aider/ directory\n *\n * @param projectRoot - Root directory to check for environments\n * @returns Array of detected environments (may include multiple)\n */\nexport function detectEnvironments(projectRoot: string): DetectedEnvironment[] {\n  const environments: DetectedEnvironment[] = [];\n\n  // Check for Claude Code\n  const claudeDir = path.join(projectRoot, '.claude');\n  const claudeMd = path.join(projectRoot, 'CLAUDE.md');\n  if (existsSync(claudeDir) || existsSync(claudeMd)) {\n    environments.push({\n      type: 'claude',\n      configDir: '.claude',\n      detected: true,\n    });\n  }\n\n  // Check for OpenCode\n  const openCodeDir = path.join(projectRoot, '.opencode');\n  if (existsSync(openCodeDir)) {\n    environments.push({\n      type: 'opencode',\n      configDir: '.opencode',\n      detected: true,\n    });\n  }\n\n  // Check for Aider\n  const aiderConfig = path.join(projectRoot, '.aider.conf.yml');\n  const aiderDir = path.join(projectRoot, '.aider');\n  if (existsSync(aiderConfig) || existsSync(aiderDir)) {\n    environments.push({\n      type: 'aider',\n      configDir: '.aider',\n      detected: true,\n    });\n  }\n\n  return environments;\n}\n\n/**\n * Check if a specific AI coding assistant environment is present\n *\n * @param projectRoot - Root directory to check\n * @param type - Environment type to check for\n * @returns true if the environment is detected\n */\nexport function hasEnvironment(\n  projectRoot: string,\n  type: EnvironmentType\n): boolean {\n  const environments = detectEnvironments(projectRoot);\n  return environments.some((env) => env.type === type);\n}\n\n```\n\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n- Workflow & convention rules: if this file defines contribution guidelines, PR conventions, commit standards, testing mandates, tool usage requirements, approval workflows, AI agent behavioral instructions, or code conventions (naming standards, formatting rules, import ordering, linting policies), extract them as explicit, actionable rules. Sources: CONTRIBUTING.md, CI configs, PR templates, style guides, linter/formatter configs (.eslintrc, .prettierrc, .editorconfig), tsconfig strictness, README dev sections.\n\n\nTARGET LENGTH (MANDATORY):\n- Source file: 2136 characters\n- Target summary: ~214 characters (10% compression)\n- Maximum: 257 characters\n- Achieve this by: ultra-dense writing, minimal examples, single-sentence descriptions\n\nULTRA-COMPRESSION TECHNIQUES:\n- Use telegraphic style: \"exports foo, bar, baz\" not \"this file exports three functions\"\n- No code examples unless critical to understanding\n- One sentence per export maximum\n- Omit obvious parameter names (\"opts: Options\" not \"options: Options - configuration object\")\n- Use abbreviations: \"params\" not \"parameters\", \"config\" not \"configuration\"\n- Skip section if empty (don't write \"No special concerns\" or \"N/A\")\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nWORKFLOW & CONVENTION RULES (NEVER EXCLUDE):\n- Contribution requirements: branch naming, commit message formats, PR description templates\n- Testing mandates: required test types, coverage thresholds, \"never delete tests\" policies\n- Tool usage rules: required CLI commands, browser automation tools, linter/formatter requirements\n- Approval workflows: review requirements, CI checks that must pass, deployment gates\n- AI agent instructions: explicit directives for automated assistants (coding rules, prohibited actions)\n- Code conventions: naming standards (camelCase, PascalCase, snake_case for what), import ordering rules, file/folder naming patterns, formatting rules (indentation, quotes, semicolons), language idioms, TypeScript strictness policies, preferred patterns vs anti-patterns\nThese define how contributors (human or AI) must work within the project.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion .annex.sum file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**detect.ts detects which AI coding assistant environments (Claude Code, OpenCode, Aider) are present in a project by checking for config directories and files.**\n\n## Exports\n\n`detectEnvironments(projectRoot: string): DetectedEnvironment[]` — scans projectRoot for .claude/.opencode/.aider config artifacts; returns array of detected environments with type and configDir.\n\n`hasEnvironment(projectRoot: string, type: EnvironmentType): boolean` — returns true if specified EnvironmentType is present.\n\n## Detection Rules\n\nClaude Code: `.claude/` directory OR `CLAUDE.md` file present.\n\nOpenCode: `.opencode/` directory present.\n\nAider: `.aider.conf.yml` file OR `.aider/` directory present.\n\n## Dependencies\n\n`existsSync` from `node:fs` — filesystem checks; `path` module for directory joining; `DetectedEnvironment`, `EnvironmentType` from `./types.js`.\n\n## Data Structure\n\nDetectedEnvironment objects contain: `type` (environment identifier), `configDir` (string like `.claude`), `detected` (boolean true).",
      "model": "claude-haiku-4-5-20251001",
      "inputTokens": 3,
      "outputTokens": 260,
      "cacheReadTokens": 16147,
      "cacheCreationTokens": 3362,
      "latencyMs": 23090,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/integration/detect.ts",
          "sizeBytes": 2136
        }
      ]
    },
    {
      "timestamp": "2026-02-14T13:47:25.601Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: src/installer/prompts.ts\n\n```typescript\n/**\n * Interactive prompts module for the installer\n *\n * Provides arrow key selection in TTY mode with numbered fallback for CI/non-interactive.\n * Uses Node.js readline module with raw mode for keypress handling.\n *\n * CRITICAL: Raw mode is always cleaned up via try/finally and process exit handlers.\n */\n\nimport * as readline from 'node:readline';\nimport pc from 'picocolors';\nimport type { Runtime, Location } from './types.js';\n\n/**\n * Check if stdin is a TTY (interactive terminal)\n *\n * @returns true if running in interactive terminal, false for CI/piped input\n */\nexport function isInteractive(): boolean {\n  return process.stdin.isTTY === true;\n}\n\n/**\n * Option type for selection prompts\n */\ninterface SelectOption<T> {\n  label: string;\n  value: T;\n}\n\n/**\n * Raw mode state tracker for cleanup\n */\nlet rawModeActive = false;\n\n/**\n * Cleanup function to restore terminal state\n */\nfunction cleanupRawMode(): void {\n  if (rawModeActive && process.stdin.isTTY) {\n    try {\n      process.stdin.setRawMode(false);\n      process.stdin.pause();\n    } catch {\n      // Ignore errors during cleanup\n    }\n    rawModeActive = false;\n  }\n}\n\n// Register global cleanup handlers\nprocess.on('exit', cleanupRawMode);\nprocess.on('SIGINT', () => {\n  cleanupRawMode();\n  process.exit(0);\n});\n\n/**\n * Generic option selector that uses arrow keys in TTY, numbered in non-TTY\n *\n * @param prompt - Question to display\n * @param options - Array of options with labels and values\n * @returns Selected value\n */\nexport async function selectOption<T>(\n  prompt: string,\n  options: SelectOption<T>[],\n): Promise<T> {\n  if (isInteractive()) {\n    return arrowKeySelect(prompt, options);\n  }\n  return numberedSelect(prompt, options);\n}\n\n/**\n * Arrow key selection for interactive terminals\n *\n * Uses raw mode to capture keypresses for up/down/enter navigation.\n * Always cleans up raw mode even on error or Ctrl+C.\n *\n * @param prompt - Question to display\n * @param options - Array of options with labels and values\n * @returns Selected value\n */\nasync function arrowKeySelect<T>(\n  prompt: string,\n  options: SelectOption<T>[],\n): Promise<T> {\n  return new Promise((resolve) => {\n    let selectedIndex = 0;\n\n    // Render the current selection state\n    const render = (clear: boolean = false): void => {\n      // Move cursor up and clear lines if re-rendering\n      if (clear) {\n        process.stdout.write(`\\x1b[${options.length + 1}A`);\n        for (let i = 0; i <= options.length; i++) {\n          process.stdout.write('\\x1b[2K\\x1b[1B');\n        }\n        process.stdout.write(`\\x1b[${options.length + 1}A`);\n      }\n\n      console.log(pc.bold(prompt));\n      options.forEach((opt, idx) => {\n        const prefix = idx === selectedIndex ? pc.cyan('> ') : '  ';\n        const label = idx === selectedIndex ? pc.cyan(opt.label) : opt.label;\n        console.log(prefix + label);\n      });\n    };\n\n    // Handle keypress events\n    const handleKeypress = (\n      _str: string | undefined,\n      key: { name?: string; ctrl?: boolean },\n    ): void => {\n      if (key.ctrl && key.name === 'c') {\n        cleanupRawMode();\n        process.exit(0);\n      }\n\n      switch (key.name) {\n        case 'up':\n          selectedIndex = Math.max(0, selectedIndex - 1);\n          render(true);\n          break;\n        case 'down':\n          selectedIndex = Math.min(options.length - 1, selectedIndex + 1);\n          render(true);\n          break;\n        case 'return':\n          // Cleanup and resolve\n          process.stdin.off('keypress', handleKeypress);\n          cleanupRawMode();\n          console.log();\n          resolve(options[selectedIndex].value);\n          break;\n      }\n    };\n\n    try {\n      // Setup raw mode for keypress handling\n      readline.emitKeypressEvents(process.stdin);\n      if (process.stdin.isTTY) {\n        process.stdin.setRawMode(true);\n        rawModeActive = true;\n      }\n      process.stdin.resume();\n      process.stdin.on('keypress', handleKeypress);\n\n      // Initial render\n      render(false);\n    } catch (err) {\n      cleanupRawMode();\n      throw err;\n    }\n  });\n}\n\n/**\n * Numbered selection for non-interactive environments\n *\n * Prints numbered options and reads a number from stdin.\n * Used in CI environments or when stdin is piped.\n *\n * @param prompt - Question to display\n * @param options - Array of options with labels and values\n * @returns Selected value\n */\nasync function numberedSelect<T>(\n  prompt: string,\n  options: SelectOption<T>[],\n): Promise<T> {\n  return new Promise((resolve, reject) => {\n    console.log(pc.bold(prompt));\n    options.forEach((opt, idx) => {\n      console.log(`  ${idx + 1}. ${opt.label}`);\n    });\n\n    const rl = readline.createInterface({\n      input: process.stdin,\n      output: process.stdout,\n    });\n\n    rl.question('Enter number: ', (answer) => {\n      rl.close();\n\n      const num = parseInt(answer, 10);\n      if (isNaN(num) || num < 1 || num > options.length) {\n        reject(new Error(`Invalid selection: ${answer}. Expected 1-${options.length}`));\n        return;\n      }\n\n      resolve(options[num - 1].value);\n    });\n  });\n}\n\n/**\n * Prompt user to select a runtime\n *\n * @param mode - 'install' or 'uninstall' to customize prompt text\n * @returns Selected runtime value\n */\nexport async function selectRuntime(mode: 'install' | 'uninstall' = 'install'): Promise<Runtime> {\n  const prompt = mode === 'uninstall' ? 'Select runtime to uninstall:' : 'Select runtime to install:';\n  return selectOption<Runtime>(prompt, [\n    { label: 'Claude Code', value: 'claude' },\n    { label: 'Codex', value: 'codex' },\n    { label: 'OpenCode', value: 'opencode' },\n    { label: 'Gemini CLI', value: 'gemini' },\n    { label: 'All runtimes', value: 'all' },\n  ]);\n}\n\n/**\n * Prompt user to select installation location\n *\n * @param mode - 'install' or 'uninstall' to customize prompt text\n * @returns Selected location value\n */\nexport async function selectLocation(mode: 'install' | 'uninstall' = 'install'): Promise<Location> {\n  const prompt = mode === 'uninstall' ? 'Select uninstallation location:' : 'Select installation location:';\n  return selectOption<Location>(prompt, [\n    { label: 'Global (~/.claude, ~/.agents, ~/.config/opencode, etc.)', value: 'global' },\n    { label: 'Local (./.claude, ./.agents, ./.opencode, etc.)', value: 'local' },\n  ]);\n}\n\n/**\n * Prompt user to confirm an action\n *\n * @param message - Confirmation message to display\n * @returns true if confirmed, false if declined\n */\nexport async function confirmAction(message: string): Promise<boolean> {\n  return selectOption<boolean>(message, [\n    { label: 'Yes', value: true },\n    { label: 'No', value: false },\n  ]);\n}\n\n```\n\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n- Workflow & convention rules: if this file defines contribution guidelines, PR conventions, commit standards, testing mandates, tool usage requirements, approval workflows, AI agent behavioral instructions, or code conventions (naming standards, formatting rules, import ordering, linting policies), extract them as explicit, actionable rules. Sources: CONTRIBUTING.md, CI configs, PR templates, style guides, linter/formatter configs (.eslintrc, .prettierrc, .editorconfig), tsconfig strictness, README dev sections.\n\n\nTARGET LENGTH (MANDATORY):\n- Source file: 6671 characters\n- Target summary: ~667 characters (10% compression)\n- Maximum: 800 characters\n- Achieve this by: ultra-dense writing, minimal examples, single-sentence descriptions\n\nULTRA-COMPRESSION TECHNIQUES:\n- Use telegraphic style: \"exports foo, bar, baz\" not \"this file exports three functions\"\n- No code examples unless critical to understanding\n- One sentence per export maximum\n- Omit obvious parameter names (\"opts: Options\" not \"options: Options - configuration object\")\n- Use abbreviations: \"params\" not \"parameters\", \"config\" not \"configuration\"\n- Skip section if empty (don't write \"No special concerns\" or \"N/A\")\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nWORKFLOW & CONVENTION RULES (NEVER EXCLUDE):\n- Contribution requirements: branch naming, commit message formats, PR description templates\n- Testing mandates: required test types, coverage thresholds, \"never delete tests\" policies\n- Tool usage rules: required CLI commands, browser automation tools, linter/formatter requirements\n- Approval workflows: review requirements, CI checks that must pass, deployment gates\n- AI agent instructions: explicit directives for automated assistants (coding rules, prohibited actions)\n- Code conventions: naming standards (camelCase, PascalCase, snake_case for what), import ordering rules, file/folder naming patterns, formatting rules (indentation, quotes, semicolons), language idioms, TypeScript strictness policies, preferred patterns vs anti-patterns\nThese define how contributors (human or AI) must work within the project.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion .annex.sum file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**src/installer/prompts.ts provides interactive terminal UI for installer selection prompts with arrow-key support in TTY and numbered fallback for CI.**\n\n## Exports\n\n`isInteractive()` → `boolean`: detects TTY mode via `process.stdin.isTTY`.\n\n`selectOption<T>(prompt: string, options: SelectOption<T>[])` → `Promise<T>`: routes to `arrowKeySelect` or `numberedSelect` based on TTY detection.\n\n`selectRuntime(mode: 'install' | 'uninstall')` → `Promise<Runtime>`: selects from `{ label, value }` pairs for 'claude' | 'codex' | 'opencode' | 'gemini' | 'all'.\n\n`selectLocation(mode: 'install' | 'uninstall')` → `Promise<Location>`: selects 'global' (home dirs: ~/.claude, ~/.agents, ~/.config/opencode) or 'local' (./ equivalents).\n\n`confirmAction(message: string)` → `Promise<boolean>`: yes/no confirmation.\n\n## Terminal Control & Raw Mode\n\n`rawModeActive` flag tracks state. `cleanupRawMode()` calls `process.stdin.setRawMode(false)` + `pause()` with try-catch swallowing errors. Global handlers registered: `process.on('exit')` and `process.on('SIGINT')` both call `cleanupRawMode()` before exit.\n\n## Arrow Key Handler (Interactive)\n\n`arrowKeySelect` uses `readline.emitKeypressEvents()` + `setRawMode(true)` to capture keypresses. Navigation: `key.name === 'up'` (decrement bounded to 0) / `'down'` (increment bounded to length-1) / `'return'` (resolve). Renders via ANSI escape codes: `\\x1b[${n}A` (cursor up), `\\x1b[2K` (clear line), `\\x1b[1B` (cursor down). Selected option prefixed `pc.cyan('> ')` and colored; unselected plain. Ctrl+C invokes `cleanupRawMode()` + `process.exit(0)`. Re-render clears previous output before redrawing.\n\n## Numbered Fallback (Non-TTY)\n\n`numberedSelect` creates `readline.createInterface`, lists options as `1. Label`, `2. Label`, reads input via `rl.question('Enter number: ')`, validates `1 ≤ num ≤ options.length` (1-indexed), rejects on parse/range error with message `Invalid selection: {answer}. Expected 1-{options.length}`.\n\n## Types & Dependencies\n\n`SelectOption<T>` interface: `{ label: string; value: T }`.\nImports: `node:readline`, `picocolors` (pc.*), `Runtime` | `Location` from ./types.js.",
      "model": "claude-haiku-4-5-20251001",
      "inputTokens": 3,
      "outputTokens": 699,
      "cacheReadTokens": 0,
      "cacheCreationTokens": 20943,
      "latencyMs": 25723,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/installer/prompts.ts",
          "sizeBytes": 6671
        }
      ]
    },
    {
      "timestamp": "2026-02-14T13:47:27.476Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: src/integration/generate.ts\n\n```typescript\n/**\n * Integration file generation for AI coding assistants\n *\n * Generates command files and hooks for detected AI assistant environments.\n * Handles file creation with directory creation and skip-if-exists behavior.\n */\n\nimport { existsSync, mkdirSync, writeFileSync, readFileSync } from 'node:fs';\nimport * as path from 'node:path';\nimport { fileURLToPath } from 'node:url';\nimport type { IntegrationResult, EnvironmentType } from './types.js';\nimport { detectEnvironments } from './detect.js';\nimport {\n  getClaudeTemplates,\n  getOpenCodeTemplates,\n  getGeminiTemplates,\n} from './templates.js';\n\n/**\n * Get the path to a bundled hook file\n *\n * @param hookName - Name of the hook file (e.g., 'are-session-end.js')\n * @returns Absolute path to the bundled hook file\n */\nfunction getBundledHookPath(hookName: string): string {\n  const __filename = fileURLToPath(import.meta.url);\n  const __dirname = path.dirname(__filename);\n  // From dist/integration/ go up two levels to project root, then to hooks/dist/\n  return path.join(__dirname, '..', '..', 'hooks', 'dist', hookName);\n}\n\n/**\n * Read bundled hook content\n *\n * @param hookName - Name of the hook file\n * @returns Hook file content as string\n * @throws Error if hook file not found\n */\nfunction readBundledHook(hookName: string): string {\n  const hookPath = getBundledHookPath(hookName);\n  if (!existsSync(hookPath)) {\n    throw new Error(`Bundled hook not found: ${hookPath}`);\n  }\n  return readFileSync(hookPath, 'utf-8');\n}\n\n/**\n * Options for generating integration files\n */\nexport interface GenerateOptions {\n  /** If true, don't actually write files - just report what would be done */\n  dryRun?: boolean;\n  /** If true, overwrite existing files instead of skipping them */\n  force?: boolean;\n  /** Specific environment to generate for (bypasses auto-detection) */\n  environment?: EnvironmentType;\n}\n\n/**\n * Ensure parent directories exist for a file path\n *\n * @param filePath - Full path to the file\n */\nfunction ensureDir(filePath: string): void {\n  const dir = path.dirname(filePath);\n  if (!existsSync(dir)) {\n    mkdirSync(dir, { recursive: true });\n  }\n}\n\n/**\n * Generate integration files for all detected AI assistant environments\n *\n * For each detected environment:\n * - Gets appropriate templates (command files)\n * - Creates files if they don't exist (or if force=true)\n * - For Claude: also creates the session-end hook\n *\n * @param projectRoot - Root directory of the project\n * @param options - Generation options\n * @returns Array of results, one per environment\n *\n * @example\n * ```typescript\n * const results = await generateIntegrationFiles('/path/to/project');\n * // [{ environment: 'claude', filesCreated: ['...'], filesSkipped: [] }]\n * ```\n */\nexport async function generateIntegrationFiles(\n  projectRoot: string,\n  options: GenerateOptions = {}\n): Promise<IntegrationResult[]> {\n  const { dryRun = false, force = false, environment: specificEnv } = options;\n  const results: IntegrationResult[] = [];\n\n  // Use specific environment if provided, otherwise auto-detect\n  let environments: { type: EnvironmentType; configDir: string }[];\n  if (specificEnv) {\n    // Map environment type to config directory\n    const configDirMap: Record<EnvironmentType, string> = {\n      claude: '.claude',\n      opencode: '.opencode',\n      aider: '.aider',\n      gemini: '.gemini',\n    };\n    environments = [{ type: specificEnv, configDir: configDirMap[specificEnv] }];\n  } else {\n    // Detect which environments are present\n    environments = detectEnvironments(projectRoot);\n  }\n\n  for (const env of environments) {\n    const result: IntegrationResult = {\n      environment: env.type,\n      filesCreated: [],\n      filesSkipped: [],\n    };\n\n    // Get templates for this environment\n    const templates = getTemplatesForEnvironment(env.type);\n\n    // Process each template\n    for (const template of templates) {\n      const fullPath = path.join(projectRoot, template.path);\n\n      if (existsSync(fullPath) && !force) {\n        // File exists and force is not set - skip it\n        result.filesSkipped.push(template.path);\n      } else {\n        // Create the file\n        if (!dryRun) {\n          ensureDir(fullPath);\n          writeFileSync(fullPath, template.content, 'utf-8');\n        }\n        result.filesCreated.push(template.path);\n      }\n    }\n\n    // For Claude, also generate the hook file\n    if (env.type === 'claude') {\n      const hookPath = '.claude/hooks/are-session-end.js';\n      const fullHookPath = path.join(projectRoot, hookPath);\n\n      if (existsSync(fullHookPath) && !force) {\n        result.filesSkipped.push(hookPath);\n      } else {\n        if (!dryRun) {\n          ensureDir(fullHookPath);\n          const hookContent = readBundledHook('are-session-end.js');\n          writeFileSync(fullHookPath, hookContent, 'utf-8');\n        }\n        result.filesCreated.push(hookPath);\n      }\n    }\n\n    results.push(result);\n  }\n\n  return results;\n}\n\n/**\n * Get templates for a specific environment type\n *\n * @param type - Environment type\n * @returns Array of templates for that environment\n */\nfunction getTemplatesForEnvironment(\n  type: EnvironmentType\n): ReturnType<typeof getClaudeTemplates> {\n  switch (type) {\n    case 'claude':\n      return getClaudeTemplates();\n    case 'opencode':\n      return getOpenCodeTemplates();\n    case 'gemini':\n      return getGeminiTemplates();\n    case 'aider':\n      // Aider doesn't have command files yet - return empty\n      return [];\n    default:\n      return [];\n  }\n}\n\n```\n\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n- Workflow & convention rules: if this file defines contribution guidelines, PR conventions, commit standards, testing mandates, tool usage requirements, approval workflows, AI agent behavioral instructions, or code conventions (naming standards, formatting rules, import ordering, linting policies), extract them as explicit, actionable rules. Sources: CONTRIBUTING.md, CI configs, PR templates, style guides, linter/formatter configs (.eslintrc, .prettierrc, .editorconfig), tsconfig strictness, README dev sections.\n\n\nTARGET LENGTH (MANDATORY):\n- Source file: 5521 characters\n- Target summary: ~552 characters (10% compression)\n- Maximum: 662 characters\n- Achieve this by: ultra-dense writing, minimal examples, single-sentence descriptions\n\nULTRA-COMPRESSION TECHNIQUES:\n- Use telegraphic style: \"exports foo, bar, baz\" not \"this file exports three functions\"\n- No code examples unless critical to understanding\n- One sentence per export maximum\n- Omit obvious parameter names (\"opts: Options\" not \"options: Options - configuration object\")\n- Use abbreviations: \"params\" not \"parameters\", \"config\" not \"configuration\"\n- Skip section if empty (don't write \"No special concerns\" or \"N/A\")\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nWORKFLOW & CONVENTION RULES (NEVER EXCLUDE):\n- Contribution requirements: branch naming, commit message formats, PR description templates\n- Testing mandates: required test types, coverage thresholds, \"never delete tests\" policies\n- Tool usage rules: required CLI commands, browser automation tools, linter/formatter requirements\n- Approval workflows: review requirements, CI checks that must pass, deployment gates\n- AI agent instructions: explicit directives for automated assistants (coding rules, prohibited actions)\n- Code conventions: naming standards (camelCase, PascalCase, snake_case for what), import ordering rules, file/folder naming patterns, formatting rules (indentation, quotes, semicolons), language idioms, TypeScript strictness policies, preferred patterns vs anti-patterns\nThese define how contributors (human or AI) must work within the project.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion .annex.sum file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**generate.ts generates integration command files and hooks for detected AI assistant environments (Claude, OpenCode, Gemini, Aider).**\n\n## Exports\n\n`generateIntegrationFiles(projectRoot: string, options?: GenerateOptions): Promise<IntegrationResult[]>` — Creates environment-specific template files and Claude session-end hooks; respects skip-if-exists unless `force: true`; supports `dryRun` mode and `environment` override to bypass auto-detection.\n\n## Types & Interfaces\n\n`GenerateOptions` — config with `dryRun` (no-op mode), `force` (overwrite existing), `environment` (EnvironmentType override for specific env).\n\n## Key Dependencies\n\n- `detectEnvironments(projectRoot)` from ./detect.js — returns array of detected environments with config dirs\n- `getClaudeTemplates()`, `getOpenCodeTemplates()`, `getGeminiTemplates()` from ./templates.js — environment-specific template arrays with `path` and `content` fields\n- `readBundledHook('are-session-end.js')` — reads pre-built hook from `hooks/dist/`; throws if missing\n\n## Environment Mapping\n\nConfig directory names: `claude` → `.claude`, `opencode` → `.opencode`, `aider` → `.aider`, `gemini` → `.gemini`.\n\n## Workflow\n\nFor each detected environment: (1) fetch templates via `getTemplatesForEnvironment(type)`; (2) write files unless they exist (or `force` overrides); (3) for `claude`, additionally write `.claude/hooks/are-session-end.js` from bundled hook. Returns `IntegrationResult[]` with `filesCreated`, `filesSkipped` per environment.\n\n## Critical Path Resolution\n\n`getBundledHookPath()` resolves from `dist/integration/` → `../../hooks/dist/[hookName]`; `readBundledHook()` validates existence before reading; `ensureDir()` creates parent directories recursively with `mkdirSync({ recursive: true })`.",
      "model": "claude-haiku-4-5-20251001",
      "inputTokens": 3,
      "outputTokens": 483,
      "cacheReadTokens": 16147,
      "cacheCreationTokens": 4370,
      "latencyMs": 24142,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/integration/generate.ts",
          "sizeBytes": 5521
        }
      ]
    },
    {
      "timestamp": "2026-02-14T13:47:32.913Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: src/integration/types.ts\n\n```typescript\n/**\n * Integration types for AI coding assistant environments\n *\n * Defines types for detecting AI assistant environments (Claude Code, OpenCode, etc.)\n * and generating appropriate integration templates (command files, hooks).\n */\n\n/**\n * Supported AI coding assistant environment types\n */\nexport type EnvironmentType = 'claude' | 'opencode' | 'aider' | 'gemini';\n\n/**\n * Result of detecting an AI coding assistant environment\n */\nexport interface DetectedEnvironment {\n  /** Type of AI assistant detected */\n  type: EnvironmentType;\n  /** Configuration directory for this environment (e.g., '.claude', '.opencode') */\n  configDir: string;\n  /** Whether this environment was detected in the project */\n  detected: boolean;\n}\n\n/**\n * Template for integration files (command files, hooks, etc.)\n */\nexport interface IntegrationTemplate {\n  /** File name (e.g., 'generate.md') */\n  filename: string;\n  /** Relative path from project root (e.g., '.claude/commands/ar/generate.md') */\n  path: string;\n  /** Template content to write to the file */\n  content: string;\n}\n\n/**\n * Result of generating integration files for an environment\n */\nexport interface IntegrationResult {\n  /** Environment type that was configured */\n  environment: EnvironmentType;\n  /** Files that were successfully created */\n  filesCreated: string[];\n  /** Files that were skipped (already exist) */\n  filesSkipped: string[];\n}\n\n```\n\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n- Workflow & convention rules: if this file defines contribution guidelines, PR conventions, commit standards, testing mandates, tool usage requirements, approval workflows, AI agent behavioral instructions, or code conventions (naming standards, formatting rules, import ordering, linting policies), extract them as explicit, actionable rules. Sources: CONTRIBUTING.md, CI configs, PR templates, style guides, linter/formatter configs (.eslintrc, .prettierrc, .editorconfig), tsconfig strictness, README dev sections.\n\n\nTARGET LENGTH (MANDATORY):\n- Source file: 1400 characters\n- Target summary: ~140 characters (10% compression)\n- Maximum: 168 characters\n- Achieve this by: ultra-dense writing, minimal examples, single-sentence descriptions\n\nULTRA-COMPRESSION TECHNIQUES:\n- Use telegraphic style: \"exports foo, bar, baz\" not \"this file exports three functions\"\n- No code examples unless critical to understanding\n- One sentence per export maximum\n- Omit obvious parameter names (\"opts: Options\" not \"options: Options - configuration object\")\n- Use abbreviations: \"params\" not \"parameters\", \"config\" not \"configuration\"\n- Skip section if empty (don't write \"No special concerns\" or \"N/A\")\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nWORKFLOW & CONVENTION RULES (NEVER EXCLUDE):\n- Contribution requirements: branch naming, commit message formats, PR description templates\n- Testing mandates: required test types, coverage thresholds, \"never delete tests\" policies\n- Tool usage rules: required CLI commands, browser automation tools, linter/formatter requirements\n- Approval workflows: review requirements, CI checks that must pass, deployment gates\n- AI agent instructions: explicit directives for automated assistants (coding rules, prohibited actions)\n- Code conventions: naming standards (camelCase, PascalCase, snake_case for what), import ordering rules, file/folder naming patterns, formatting rules (indentation, quotes, semicolons), language idioms, TypeScript strictness policies, preferred patterns vs anti-patterns\nThese define how contributors (human or AI) must work within the project.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion .annex.sum file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**src/integration/types.ts defines type system for detecting AI coding assistant environments and generating integration templates.**\n\n## Exported Types\n\n- `EnvironmentType` — union type: `'claude' | 'opencode' | 'aider' | 'gemini'`\n- `DetectedEnvironment` — interface with `type: EnvironmentType`, `configDir: string`, `detected: boolean`\n- `IntegrationTemplate` — interface with `filename: string`, `path: string`, `content: string`\n- `IntegrationResult` — interface with `environment: EnvironmentType`, `filesCreated: string[]`, `filesSkipped: string[]`\n\n## Design Pattern\n\nDiscriminated union architecture: `EnvironmentType` acts as discriminant for environment detection workflows; `DetectedEnvironment` represents detection state; `IntegrationTemplate` encapsulates file generation specs; `IntegrationResult` captures creation outcomes.",
      "model": "claude-haiku-4-5-20251001",
      "inputTokens": 3,
      "outputTokens": 215,
      "cacheReadTokens": 16147,
      "cacheCreationTokens": 3031,
      "latencyMs": 19019,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/integration/types.ts",
          "sizeBytes": 1400
        }
      ]
    },
    {
      "timestamp": "2026-02-14T13:47:21.997Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: src/installer/operations.ts\n\n```typescript\n/**\n * File operations for installer module\n *\n * Handles copying command/hook files to runtime directories,\n * verifying installations, and registering hooks in settings.json.\n */\n\nimport { existsSync, mkdirSync, writeFileSync, readFileSync } from 'node:fs';\nimport * as path from 'node:path';\nimport { fileURLToPath } from 'node:url';\nimport { parse, modify, applyEdits } from 'jsonc-parser';\nimport type { Runtime, Location, InstallerResult } from './types.js';\nimport { resolveInstallPath, getAllRuntimes, resolveCodexConfigPath } from './paths.js';\nimport {\n  getClaudeTemplates,\n  getCodexTemplates,\n  getOpenCodeTemplates,\n  getGeminiTemplates,\n} from '../integration/templates.js';\n\n/**\n * Options for install operations\n */\nexport interface InstallOptions {\n  /** Overwrite existing files */\n  force: boolean;\n  /** Preview mode - don't write files */\n  dryRun: boolean;\n}\n\n/**\n * Ensure directory exists for a file path\n *\n * @param filePath - Full path to the file\n */\nfunction ensureDir(filePath: string): void {\n  const dir = path.dirname(filePath);\n  if (!existsSync(dir)) {\n    mkdirSync(dir, { recursive: true });\n  }\n}\n\nconst CODEX_RULES_FILENAME = 'are.rules';\nconst LOCAL_CONTEXT_RULES_FILENAME = 'AGENTS.override.md';\nconst LOCAL_CONTEXT_RULES_MARKER =\n  '<!-- Generated by agents-reverse-engineer installer: local-context-rules -->';\nconst CODEX_PREFIX_RULES: string[][] = [\n  ['npx', 'are', 'init'],\n  ['npx', 'are', 'discover'],\n  ['npx', 'are', 'generate'],\n  ['npx', 'are', 'update'],\n  ['npx', 'are', 'specify'],\n  ['npx', 'are', 'rebuild'],\n  ['npx', 'are', 'clean'],\n  ['rm', '-f', '.agents-reverse-engineer/progress.log'],\n  ['sleep'],\n];\n\nfunction getCodexRulesFilePath(location: Location, basePath: string): string {\n  const projectRoot = location === 'local' ? path.dirname(basePath) : undefined;\n  const codexConfigDir = resolveCodexConfigPath(location, projectRoot);\n  return path.join(codexConfigDir, 'rules', CODEX_RULES_FILENAME);\n}\n\nfunction getCodexContextRulesFilePath(location: Location, basePath: string): string {\n  if (location === 'local') {\n    // Local install targets the repository root so Codex loads it as project rules.\n    return path.join(path.dirname(basePath), LOCAL_CONTEXT_RULES_FILENAME);\n  }\n\n  // Global install targets Codex home config so rules apply across repositories.\n  const codexConfigDir = resolveCodexConfigPath('global');\n  return path.join(codexConfigDir, LOCAL_CONTEXT_RULES_FILENAME);\n}\n\nfunction buildCodexRulesFileContent(): string {\n  const lines = [\n    '# Generated by agents-reverse-engineer installer.',\n    '# Allows ARE command prefixes for Codex CLI approvals.',\n    '',\n  ];\n\n  for (const prefix of CODEX_PREFIX_RULES) {\n    lines.push(`prefix_rule = ${JSON.stringify(prefix)}`);\n  }\n\n  lines.push('');\n  return lines.join('\\n');\n}\n\nfunction buildLocalContextRulesContent(): string {\n  return `${LOCAL_CONTEXT_RULES_MARKER}\n\n# ARE Local Context Rules\n\nWhen working in this repository:\n- After reading a project file, load \\`AGENTS.override.md\\` and \\`AGENTS.md\\` in the file's directory (if present).\n- Then load \\`AGENTS.override.md\\` and \\`AGENTS.md\\` from each parent directory up to repo root (if present).\n- Skip reloading unchanged instruction files already loaded in the current task.\n- If instructions conflict, nearest-directory instructions override parent/root instructions.\n`;\n}\n\n/**\n * Determine if a managed file should be overwritten.\n *\n * For safety, --force only overwrites files previously generated by ARE\n * (identified by marker). User-authored files are preserved.\n */\nfunction shouldWriteManagedFile(filePath: string, marker: string, force: boolean): boolean {\n  if (!existsSync(filePath)) {\n    return true;\n  }\n  if (!force) {\n    return false;\n  }\n  try {\n    return readFileSync(filePath, 'utf-8').includes(marker);\n  } catch {\n    return false;\n  }\n}\n\n/**\n * Get the path to a bundled hook file\n *\n * Hooks are bundled in hooks/dist/ during npm prepublishOnly.\n *\n * @param hookName - Name of the hook file (e.g., 'are-context-loader.js')\n * @returns Absolute path to the bundled hook file\n */\nfunction getBundledHookPath(hookName: string): string {\n  // Navigate from dist/installer/operations.js to hooks/dist/\n  const __filename = fileURLToPath(import.meta.url);\n  const __dirname = path.dirname(__filename);\n  // From dist/installer/ go up two levels to project root, then to hooks/dist/\n  return path.join(__dirname, '..', '..', 'hooks', 'dist', hookName);\n}\n\n/**\n * Read bundled hook content\n *\n * @param hookName - Name of the hook file\n * @returns Hook file content as string\n * @throws Error if hook file not found\n */\nfunction readBundledHook(hookName: string): string {\n  const hookPath = getBundledHookPath(hookName);\n  if (!existsSync(hookPath)) {\n    throw new Error(`Bundled hook not found: ${hookPath}`);\n  }\n  return readFileSync(hookPath, 'utf-8');\n}\n\n/**\n * Get templates for a specific runtime\n *\n * @param runtime - Target runtime (claude, codex, opencode, or gemini)\n * @returns Array of template objects for the runtime\n */\nfunction getTemplatesForRuntime(runtime: Exclude<Runtime, 'all'>) {\n  switch (runtime) {\n    case 'claude':\n      return getClaudeTemplates();\n    case 'codex':\n      return getCodexTemplates();\n    case 'opencode':\n      return getOpenCodeTemplates();\n    case 'gemini':\n      return getGeminiTemplates();\n  }\n}\n\n/**\n * Install files for one or all runtimes\n *\n * If runtime is 'all', installs to all supported runtimes.\n * Otherwise, installs to the specified runtime only.\n *\n * @param runtime - Target runtime or 'all'\n * @param location - Installation location (global or local)\n * @param options - Install options (force, dryRun)\n * @returns Array of installation results (one per runtime)\n */\nexport function installFiles(\n  runtime: Runtime,\n  location: Location,\n  options: InstallOptions,\n): InstallerResult[] {\n  if (runtime === 'all') {\n    return getAllRuntimes().map((r) => installFilesForRuntime(r, location, options));\n  }\n  return [installFilesForRuntime(runtime, location, options)];\n}\n\n/**\n * Install files for a specific runtime\n *\n * Copies command templates and hook files to the installation directory.\n * Skips existing files unless force=true.\n *\n * @param runtime - Target runtime (claude, codex, opencode, or gemini)\n * @param location - Installation location (global or local)\n * @param options - Install options (force, dryRun)\n * @returns Installation result with files created/skipped\n */\nfunction installFilesForRuntime(\n  runtime: Exclude<Runtime, 'all'>,\n  location: Location,\n  options: InstallOptions,\n): InstallerResult {\n  const basePath = resolveInstallPath(runtime, location);\n  const templates = getTemplatesForRuntime(runtime);\n  const filesCreated: string[] = [];\n  const filesSkipped: string[] = [];\n  const errors: string[] = [];\n\n  // Install command templates\n  for (const template of templates) {\n    // Template path is relative (e.g., .claude/commands/are/generate.md)\n    // Extract the part after the runtime directory (e.g., commands/are/generate.md)\n    const relativePath = template.path.split('/').slice(1).join('/');\n    const fullPath = path.join(basePath, relativePath);\n\n    if (existsSync(fullPath) && !options.force) {\n      filesSkipped.push(fullPath);\n    } else {\n      if (!options.dryRun) {\n        try {\n          ensureDir(fullPath);\n          writeFileSync(fullPath, template.content, 'utf-8');\n        } catch (err) {\n          errors.push(`Failed to write ${fullPath}: ${err}`);\n          continue;\n        }\n      }\n      filesCreated.push(fullPath);\n    }\n  }\n\n  // Install hooks/plugins based on runtime\n  let hookRegistered = false;\n  if (runtime === 'claude' || runtime === 'gemini') {\n    // Claude and Gemini: install session hooks\n    for (const hookDef of ARE_HOOKS) {\n      const hookPath = path.join(basePath, 'hooks', hookDef.filename);\n      if (existsSync(hookPath) && !options.force) {\n        filesSkipped.push(hookPath);\n      } else {\n        if (!options.dryRun) {\n          try {\n            ensureDir(hookPath);\n            const hookContent = readBundledHook(hookDef.filename);\n            writeFileSync(hookPath, hookContent, 'utf-8');\n          } catch (err) {\n            errors.push(`Failed to write hook ${hookPath}: ${err}`);\n          }\n        }\n        if (!errors.some((e) => e.includes(hookPath))) {\n          filesCreated.push(hookPath);\n        }\n      }\n    }\n\n    // Register hooks in settings.json\n    hookRegistered = registerHooks(basePath, runtime, options.dryRun);\n\n    // Register permissions for Claude (reduces friction for users).\n    if (runtime === 'claude') {\n      const settingsPath = path.join(basePath, 'settings.json');\n      registerPermissions(settingsPath, options.dryRun);\n    }\n  } else if (runtime === 'codex') {\n    // Codex uses rules files (`prefix_rule`) instead of settings.json allowlists.\n    const rulesPath = getCodexRulesFilePath(location, basePath);\n\n    if (existsSync(rulesPath) && !options.force) {\n      filesSkipped.push(rulesPath);\n    } else {\n      if (!options.dryRun) {\n        try {\n          ensureDir(rulesPath);\n          writeFileSync(rulesPath, buildCodexRulesFileContent(), 'utf-8');\n        } catch (err) {\n          errors.push(`Failed to write Codex rules ${rulesPath}: ${err}`);\n        }\n      }\n      if (!errors.some((e) => e.includes(rulesPath))) {\n        filesCreated.push(rulesPath);\n      }\n    }\n\n    // Seed Codex context-loading instructions:\n    // - local install: repo root AGENTS.override.md\n    // - global install: ~/.codex/AGENTS.override.md\n    const contextRulesPath = getCodexContextRulesFilePath(location, basePath);\n    const shouldWrite = shouldWriteManagedFile(\n      contextRulesPath,\n      LOCAL_CONTEXT_RULES_MARKER,\n      options.force,\n    );\n\n    if (!shouldWrite) {\n      filesSkipped.push(contextRulesPath);\n    } else {\n      if (!options.dryRun) {\n        try {\n          ensureDir(contextRulesPath);\n          writeFileSync(contextRulesPath, buildLocalContextRulesContent(), 'utf-8');\n        } catch (err) {\n          errors.push(`Failed to write context rules ${contextRulesPath}: ${err}`);\n        }\n      }\n      if (!errors.some((e) => e.includes(contextRulesPath))) {\n        filesCreated.push(contextRulesPath);\n      }\n    }\n  } else if (runtime === 'opencode') {\n    // OpenCode: install plugins (auto-loaded from plugins/ directory)\n    for (const pluginDef of ARE_PLUGINS) {\n      const pluginPath = path.join(basePath, 'plugins', pluginDef.destFilename);\n      if (existsSync(pluginPath) && !options.force) {\n        filesSkipped.push(pluginPath);\n      } else {\n        if (!options.dryRun) {\n          try {\n            ensureDir(pluginPath);\n            const pluginContent = readBundledHook(pluginDef.srcFilename);\n            writeFileSync(pluginPath, pluginContent, 'utf-8');\n          } catch (err) {\n            errors.push(`Failed to write plugin ${pluginPath}: ${err}`);\n          }\n        }\n        if (!errors.some((e) => e.includes(pluginPath))) {\n          filesCreated.push(pluginPath);\n          hookRegistered = true;\n        }\n      }\n    }\n  }\n\n  // Write VERSION file if files were created and not dry run\n  let versionWritten = false;\n  if (filesCreated.length > 0 && !options.dryRun) {\n    try {\n      writeVersionFile(basePath, options.dryRun);\n      versionWritten = true;\n    } catch {\n      // Non-fatal, don't add to errors\n    }\n  }\n\n  return {\n    success: errors.length === 0,\n    runtime,\n    location,\n    filesCreated,\n    filesSkipped,\n    errors,\n    hookRegistered,\n    versionWritten,\n  };\n}\n\n/**\n * Verify that installed files exist\n *\n * @param files - Array of file paths to verify\n * @returns Object with success status and list of missing files\n */\nexport function verifyInstallation(files: string[]): { success: boolean; missing: string[] } {\n  const missing = files.filter((f) => !existsSync(f));\n  return {\n    success: missing.length === 0,\n    missing,\n  };\n}\n\n/**\n * Session hook configuration for settings.json\n */\ninterface SessionHook {\n  type: 'command';\n  command: string;\n}\n\ninterface HookEvent {\n  matcher?: string;\n  hooks: SessionHook[];\n}\n\ninterface SettingsJson {\n  hooks?: {\n    SessionStart?: HookEvent[];\n    PostToolUse?: HookEvent[];\n  };\n  permissions?: {\n    allow?: string[];\n    deny?: string[];\n  };\n  [key: string]: unknown;\n}\n\n/**\n * Gemini hook configuration (simpler format)\n */\ninterface GeminiHook {\n  name: string;\n  type: 'command';\n  command: string;\n}\n\ninterface GeminiSettingsJson {\n  hooks?: {\n    SessionStart?: GeminiHook[];\n  };\n  [key: string]: unknown;\n}\n\n/**\n * Hook definitions for ARE (Claude only — Gemini skips PostToolUse hooks)\n */\ninterface HookDefinition {\n  event: 'SessionStart' | 'PostToolUse';\n  filename: string;\n  name: string;\n  /** Regex matcher for tool-scoped hooks (PostToolUse) */\n  matcher?: string;\n}\n\nconst ARE_HOOKS: HookDefinition[] = [\n  { event: 'SessionStart', filename: 'are-check-update.js', name: 'are-check-update' },\n  { event: 'PostToolUse', filename: 'are-context-loader.js', name: 'are-context-loader', matcher: 'Read' },\n];\n\n/**\n * Plugin definitions for ARE (OpenCode)\n *\n * OpenCode uses a plugin system (.opencode/plugins/) instead of hooks.\n * Plugins are JS/TS modules that export async functions returning event handlers.\n */\ninterface PluginDefinition {\n  /** Source filename in hooks/dist/ (prefixed with opencode-) */\n  srcFilename: string;\n  /** Destination filename in .opencode/plugins/ */\n  destFilename: string;\n}\n\nconst ARE_PLUGINS: PluginDefinition[] = [\n  // Disabled - causing issues in OpenCode\n  // { srcFilename: 'opencode-are-check-update.js', destFilename: 'are-check-update.js' },\n];\n\n/**\n * Register ARE hooks in settings.json\n *\n * Registers PostToolUse hooks (context loader) for Claude Code.\n * Merges with existing hooks, doesn't overwrite.\n *\n * @param basePath - Base installation path (e.g., ~/.claude or ~/.gemini)\n * @param runtime - Target runtime (claude or gemini)\n * @param dryRun - If true, don't write changes\n * @returns true if any hook was added, false if all already existed\n */\nexport function registerHooks(\n  basePath: string,\n  runtime: Exclude<Runtime, 'all'>,\n  dryRun: boolean,\n): boolean {\n  // Only for Claude and Gemini installations\n  if (runtime !== 'claude' && runtime !== 'gemini') {\n    return false;\n  }\n\n  const settingsPath = path.join(basePath, 'settings.json');\n  const runtimeDir = runtime === 'claude' ? '.claude' : '.gemini';\n\n  if (runtime === 'gemini') {\n    return registerGeminiHooks(settingsPath, runtimeDir, dryRun);\n  }\n\n  return registerClaudeHooks(settingsPath, runtimeDir, dryRun);\n}\n\n/**\n * Register ARE hooks in Claude Code settings.json format\n */\nfunction registerClaudeHooks(settingsPath: string, runtimeDir: string, dryRun: boolean): boolean {\n  // Load or create settings (JSONC-aware)\n  let content = '{}';\n  if (existsSync(settingsPath)) {\n    try {\n      content = readFileSync(settingsPath, 'utf-8');\n    } catch {\n      // If can't read, start with empty object\n    }\n  }\n\n  const settings = (parse(content) ?? {}) as SettingsJson;\n\n  // Ensure hooks structure exists\n  if (!settings.hooks) {\n    settings.hooks = {};\n  }\n\n  let addedAny = false;\n\n  for (const hookDef of ARE_HOOKS) {\n    const hookCommand = `node ${runtimeDir}/hooks/${hookDef.filename}`;\n\n    // Ensure event array exists\n    if (!settings.hooks[hookDef.event]) {\n      settings.hooks[hookDef.event] = [];\n    }\n\n    // Check if hook already exists (by command string match)\n    const hookExists = settings.hooks[hookDef.event]!.some((event) =>\n      event.hooks?.some((h) => h.command === hookCommand),\n    );\n\n    if (!hookExists) {\n      // Define our hook (Claude format: nested hooks array, optional matcher for PostToolUse)\n      const newHook: HookEvent = {\n        ...(hookDef.matcher ? { matcher: hookDef.matcher } : {}),\n        hooks: [\n          {\n            type: 'command',\n            command: hookCommand,\n          },\n        ],\n      };\n      settings.hooks[hookDef.event]!.push(newHook);\n      addedAny = true;\n    }\n  }\n\n  if (!addedAny) {\n    return false;\n  }\n\n  // Write settings preserving comments outside the hooks section\n  if (!dryRun) {\n    ensureDir(settingsPath);\n    const updated = applyEdits(content, modify(content, ['hooks'], settings.hooks, {\n      formattingOptions: { tabSize: 2, insertSpaces: true },\n    }));\n    writeFileSync(settingsPath, updated, 'utf-8');\n  }\n\n  return true;\n}\n\n/**\n * Permissions to auto-allow for ARE commands\n */\nconst ARE_PERMISSIONS = [\n  'Bash(npx are init*)',\n  'Bash(npx are discover*)',\n  'Bash(npx are generate*)',\n  'Bash(npx are update*)',\n  'Bash(npx are specify*)',\n  'Bash(npx are rebuild*)',\n  'Bash(npx are clean*)',\n  'Bash(rm -f .agents-reverse-engineer/progress.log*)',\n  'Bash(sleep *)',\n];\n\n/**\n * Register ARE permissions in settings.json\n *\n * Adds bash command permissions for ARE commands to reduce friction.\n *\n * @param settingsPath - Path to settings.json\n * @param dryRun - If true, don't write changes\n * @returns true if permissions were added, false if already existed\n */\nexport function registerPermissions(settingsPath: string, dryRun: boolean): boolean {\n  // Load or create settings (JSONC-aware)\n  let content = '{}';\n  if (existsSync(settingsPath)) {\n    try {\n      content = readFileSync(settingsPath, 'utf-8');\n    } catch {\n      // If can't read, start with empty object\n    }\n  }\n\n  const settings = (parse(content) ?? {}) as SettingsJson;\n\n  // Ensure permissions structure exists\n  if (!settings.permissions) {\n    settings.permissions = {};\n  }\n  if (!settings.permissions.allow) {\n    settings.permissions.allow = [];\n  }\n\n  // Add any missing ARE permissions\n  let addedAny = false;\n  for (const perm of ARE_PERMISSIONS) {\n    if (!settings.permissions.allow.includes(perm)) {\n      settings.permissions.allow.push(perm);\n      addedAny = true;\n    }\n  }\n\n  if (!addedAny) {\n    return false;\n  }\n\n  // Write settings preserving comments outside the permissions section\n  if (!dryRun) {\n    ensureDir(settingsPath);\n    const updated = applyEdits(content, modify(content, ['permissions'], settings.permissions, {\n      formattingOptions: { tabSize: 2, insertSpaces: true },\n    }));\n    writeFileSync(settingsPath, updated, 'utf-8');\n  }\n\n  return true;\n}\n\n/**\n * Register ARE hooks in Gemini CLI settings.json format\n */\nfunction registerGeminiHooks(settingsPath: string, runtimeDir: string, dryRun: boolean): boolean {\n  // Load or create settings (JSONC-aware)\n  let content = '{}';\n  if (existsSync(settingsPath)) {\n    try {\n      content = readFileSync(settingsPath, 'utf-8');\n    } catch {\n      // If can't read, start with empty object\n    }\n  }\n\n  const settings = (parse(content) ?? {}) as GeminiSettingsJson;\n\n  // Ensure hooks structure exists\n  if (!settings.hooks) {\n    settings.hooks = {};\n  }\n\n  let addedAny = false;\n\n  for (const hookDef of ARE_HOOKS) {\n    // Gemini only supports SessionStart hooks\n    if (hookDef.event !== 'SessionStart') continue;\n\n    const hookCommand = `node ${runtimeDir}/hooks/${hookDef.filename}`;\n\n    // Ensure event array exists\n    if (!settings.hooks[hookDef.event]) {\n      settings.hooks[hookDef.event] = [];\n    }\n\n    // Check if hook already exists (by command string match)\n    const hookExists = settings.hooks[hookDef.event]!.some((h) => h.command === hookCommand);\n\n    if (!hookExists) {\n      // Define our hook (Gemini format: flat object with name)\n      const newHook: GeminiHook = {\n        name: hookDef.name,\n        type: 'command',\n        command: hookCommand,\n      };\n      settings.hooks[hookDef.event]!.push(newHook);\n      addedAny = true;\n    }\n  }\n\n  if (!addedAny) {\n    return false;\n  }\n\n  // Write settings preserving comments outside the hooks section\n  if (!dryRun) {\n    ensureDir(settingsPath);\n    const updated = applyEdits(content, modify(content, ['hooks'], settings.hooks, {\n      formattingOptions: { tabSize: 2, insertSpaces: true },\n    }));\n    writeFileSync(settingsPath, updated, 'utf-8');\n  }\n\n  return true;\n}\n\n/**\n * Get package version from package.json\n *\n * @returns Version string or 'unknown' if can't read\n */\nexport function getPackageVersion(): string {\n  try {\n    // Navigate from dist/installer/operations.js to package.json\n    // In ESM, we need to use import.meta.url\n    const __filename = fileURLToPath(import.meta.url);\n    const __dirname = path.dirname(__filename);\n    // From src/installer/ go up two levels to project root\n    const packagePath = path.join(__dirname, '..', '..', 'package.json');\n    const content = readFileSync(packagePath, 'utf-8');\n    const pkg = JSON.parse(content) as { version?: string };\n    return pkg.version || 'unknown';\n  } catch {\n    return 'unknown';\n  }\n}\n\n/**\n * Write ARE-VERSION file to track installed version\n *\n * @param basePath - Base installation path\n * @param dryRun - If true, don't write the file\n */\nexport function writeVersionFile(basePath: string, dryRun: boolean): void {\n  if (dryRun) {\n    return;\n  }\n\n  const versionPath = path.join(basePath, 'ARE-VERSION');\n  const version = getPackageVersion();\n\n  ensureDir(versionPath);\n  writeFileSync(versionPath, version, 'utf-8');\n}\n\n/**\n * Format installation result for display\n *\n * Generates human-readable lines showing created/skipped files.\n *\n * @param result - Installation result to format\n * @returns Array of formatted lines for display\n */\nexport function formatInstallResult(result: InstallerResult): string[] {\n  const lines: string[] = [];\n\n  // Header with runtime and location\n  lines.push(`  ${result.runtime} (${result.location}):`);\n\n  // Created files\n  for (const file of result.filesCreated) {\n    lines.push(`    Created: ${file}`);\n  }\n\n  // Skipped files\n  for (const file of result.filesSkipped) {\n    lines.push(`    Skipped: ${file} (already exists)`);\n  }\n\n  // Hook registration status (Claude only)\n  if (result.hookRegistered) {\n    lines.push(`    Registered: hooks in settings.json`);\n  }\n\n  // Summary line\n  const created = result.filesCreated.length;\n  const skipped = result.filesSkipped.length;\n  lines.push(`    ${created} files installed, ${skipped} skipped`);\n\n  return lines;\n}\n\n```\n\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n- Workflow & convention rules: if this file defines contribution guidelines, PR conventions, commit standards, testing mandates, tool usage requirements, approval workflows, AI agent behavioral instructions, or code conventions (naming standards, formatting rules, import ordering, linting policies), extract them as explicit, actionable rules. Sources: CONTRIBUTING.md, CI configs, PR templates, style guides, linter/formatter configs (.eslintrc, .prettierrc, .editorconfig), tsconfig strictness, README dev sections.\n\n\nTARGET LENGTH (MANDATORY):\n- Source file: 22217 characters\n- Target summary: ~2222 characters (10% compression)\n- Maximum: 2666 characters\n- Achieve this by: ultra-dense writing, minimal examples, single-sentence descriptions\n\nULTRA-COMPRESSION TECHNIQUES:\n- Use telegraphic style: \"exports foo, bar, baz\" not \"this file exports three functions\"\n- No code examples unless critical to understanding\n- One sentence per export maximum\n- Omit obvious parameter names (\"opts: Options\" not \"options: Options - configuration object\")\n- Use abbreviations: \"params\" not \"parameters\", \"config\" not \"configuration\"\n- Skip section if empty (don't write \"No special concerns\" or \"N/A\")\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nWORKFLOW & CONVENTION RULES (NEVER EXCLUDE):\n- Contribution requirements: branch naming, commit message formats, PR description templates\n- Testing mandates: required test types, coverage thresholds, \"never delete tests\" policies\n- Tool usage rules: required CLI commands, browser automation tools, linter/formatter requirements\n- Approval workflows: review requirements, CI checks that must pass, deployment gates\n- AI agent instructions: explicit directives for automated assistants (coding rules, prohibited actions)\n- Code conventions: naming standards (camelCase, PascalCase, snake_case for what), import ordering rules, file/folder naming patterns, formatting rules (indentation, quotes, semicolons), language idioms, TypeScript strictness policies, preferred patterns vs anti-patterns\nThese define how contributors (human or AI) must work within the project.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion .annex.sum file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**operations.ts orchestrates installation of ARE command templates, hooks, and plugins to Claude/Gemini/Codex/OpenCode runtimes with settings.json registration and version tracking.**\n\n## Exports\n\n`installFiles(runtime: Runtime, location: Location, options: InstallOptions): InstallerResult[]` — routes to runtime-specific installer; expands 'all' to all supported runtimes.\n\n`registerHooks(basePath: string, runtime: Exclude<Runtime, 'all'>, dryRun: boolean): boolean` — registers hooks in settings.json for Claude/Gemini; returns true if any hook added.\n\n`registerPermissions(settingsPath: string, dryRun: boolean): boolean` — adds ARE command permissions to Claude settings.json to reduce friction.\n\n`verifyInstallation(files: string[]): { success: boolean; missing: string[] }` — checks file existence; returns missing files list.\n\n`getPackageVersion(): string` — reads version from package.json via import.meta.url path resolution; returns 'unknown' on failure.\n\n`writeVersionFile(basePath: string, dryRun: boolean): void` — writes ARE-VERSION file tracking installed version.\n\n`formatInstallResult(result: InstallerResult): string[]` — formats InstallerResult to human-readable status lines (created/skipped counts, hook registration status).\n\n## Key Types\n\n`InstallOptions { force: boolean; dryRun: boolean }` — controls overwrite behavior and preview mode.\n\n`HookDefinition { event: 'SessionStart' | 'PostToolUse'; filename: string; name: string; matcher?: string }` — metadata for session hooks; matcher applies tool-scoped filtering (PostToolUse only).\n\n`PluginDefinition { srcFilename: string; destFilename: string }` — OpenCode plugin mapping from bundled hooks/dist/ to .opencode/plugins/.\n\n`SettingsJson { hooks?: { SessionStart?: HookEvent[]; PostToolUse?: HookEvent[] }; permissions?: { allow?: string[] } }` — Claude/Codex settings structure.\n\n`GeminiSettingsJson { hooks?: { SessionStart?: GeminiHook[] } }` — Gemini settings (SessionStart only, no PostToolUse).\n\n## Installation Strategy\n\nRuntime-specific branches: Claude/Gemini install session hooks + register in settings.json; Codex generates rules files (`are.rules`) with prefix_rule entries; OpenCode installs plugins from bundled sources (currently empty ARE_PLUGINS array).\n\nCommand templates copied from `getTemplatesForRuntime()` (Claude/Codex/OpenCode/Gemini sources) to `basePath/commands/` hierarchy. Hooks bundled in `hooks/dist/` copied to `basePath/hooks/` (runtime-relative).\n\n`shouldWriteManagedFile(filePath: string, marker: string, force: boolean): boolean` — allows --force to overwrite only ARE-generated files (identified by marker); preserves user-authored files.\n\n## Hooks & Permissions\n\n`ARE_HOOKS` array: `are-check-update.js` (SessionStart), `are-context-loader.js` (PostToolUse, matcher='Read' for tool-scoped execution).\n\n`ARE_PERMISSIONS` array: bash command patterns (`'Bash(npx are init*)'`, `'Bash(npx are discover*)'`, etc.) + sleep and progress.log cleanup — registered in Claude settings.allow to bypass permission prompts.\n\nClaude hooks registered as nested `HookEvent { hooks: [{ type: 'command', command: '...' }] }` with optional matcher. Gemini uses flat `{ name, type, command }` format and skips PostToolUse.\n\n## Codex Rules\n\n`buildCodexRulesFileContent()` generates `are.rules` with `prefix_rule` entries for all CODEX_PREFIX_RULES (npx are subcommands + sleep + rm).\n\n`buildLocalContextRulesContent()` seeds Codex context-loading instructions (marker: `'<!-- Generated by agents-reverse-engineer installer: local-context-rules -->'`): loads AGENTS.override.md and AGENTS.md from file directory, parent dirs, up to repo root; nearest-directory rules override parent/root.\n\n## Path Resolution\n\n`getBundledHookPath(hookName: string): string` — resolves hook files: from `dist/installer/operations.js` navigate `../../hooks/dist/`.\n\n`getCodexRulesFilePath(location: Location, basePath: string): string` — local: `<repo-root>/rules/are.rules`; global: `~/.codex/rules/are.rules`.\n\n`getCodexContextRulesFilePath(location: Location, basePath: string): string` — local: `<repo-root>/AGENTS.override.md`; global: `~/.codex/AGENTS.override.md`.\n\n`ensureDir(filePath: string): void` — mkdir -p pattern; parent directory creation before file writes.\n\n## Behavioral Contracts\n\n**Codex prefix rules** (`CODEX_PREFIX_RULES`): `['npx', 'are', 'init']`, `['npx', 'are', 'discover']`, `['npx', 'are', 'generate']`, `['npx', 'are', 'update']`, `['npx', 'are', 'specify']`, `['npx', 'are', 'rebuild']`, `['npx', 'are', 'clean']`, `['rm', '-f', '.agents-reverse-engineer/progress.log']`, `['sleep']` — serialized as `prefix_rule = [...]` in are.rules.\n\n**Settings.json hook command format** (Claude): `node .claude/hooks/are-check-update.js`, `node .claude/hooks/are-context-loader.js`.\n\n**Gemini format** (simpler, SessionStart only): flat hook objects, name field required.\n\n**ARE-VERSION filename**: written to basePath root to track installation version.\n\n**Hook existence check**: command string match in settings.json; merges without duplicating.\n\n**Dry run behavior**: sets dryRun=true to skip writeFileSync calls; returns result with filesCreated populated for preview.",
      "model": "claude-haiku-4-5-20251001",
      "inputTokens": 3,
      "outputTokens": 1478,
      "cacheReadTokens": 16147,
      "cacheCreationTokens": 9724,
      "latencyMs": 33847,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/installer/operations.ts",
          "sizeBytes": 22219
        }
      ]
    },
    {
      "timestamp": "2026-02-14T13:47:36.794Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: src/orchestration/index.ts\n\n```typescript\n/**\n * Public API for the orchestration module.\n *\n * This barrel export is the single import point for the orchestration\n * engine. It re-exports the concurrency pool, progress reporter,\n * command runner, and all shared types.\n *\n * @module\n *\n * @example\n * ```typescript\n * import {\n *   CommandRunner,\n *   ProgressReporter,\n *   runPool,\n * } from './orchestration/index.js';\n *\n * const runner = new CommandRunner(aiService, { concurrency: 5 });\n * const summary = await runner.executeGenerate(plan);\n * ```\n */\n\n// ---------------------------------------------------------------------------\n// Types\n// ---------------------------------------------------------------------------\n\nexport type {\n  FileTaskResult,\n  RunSummary,\n  ProgressEvent,\n  CommandRunOptions,\n} from './types.js';\n\n// ---------------------------------------------------------------------------\n// Concurrency pool\n// ---------------------------------------------------------------------------\n\nexport { runPool } from './pool.js';\nexport type { PoolOptions, TaskResult } from './pool.js';\n\n// ---------------------------------------------------------------------------\n// Progress reporter\n// ---------------------------------------------------------------------------\n\nexport { ProgressReporter, ProgressLog } from './progress.js';\n\n// ---------------------------------------------------------------------------\n// Plan tracker\n// ---------------------------------------------------------------------------\n\nexport { PlanTracker } from './plan-tracker.js';\n\n// ---------------------------------------------------------------------------\n// Tracing\n// ---------------------------------------------------------------------------\n\nexport type { ITraceWriter, TraceEvent, TraceEventPayload } from './trace.js';\nexport { createTraceWriter, cleanupOldTraces } from './trace.js';\n\n// ---------------------------------------------------------------------------\n// Command runner\n// ---------------------------------------------------------------------------\n\nexport { CommandRunner } from './runner.js';\n\n```\n\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n- Workflow & convention rules: if this file defines contribution guidelines, PR conventions, commit standards, testing mandates, tool usage requirements, approval workflows, AI agent behavioral instructions, or code conventions (naming standards, formatting rules, import ordering, linting policies), extract them as explicit, actionable rules. Sources: CONTRIBUTING.md, CI configs, PR templates, style guides, linter/formatter configs (.eslintrc, .prettierrc, .editorconfig), tsconfig strictness, README dev sections.\n\n\nTARGET LENGTH (MANDATORY):\n- Source file: 2076 characters\n- Target summary: ~208 characters (10% compression)\n- Maximum: 250 characters\n- Achieve this by: ultra-dense writing, minimal examples, single-sentence descriptions\n\nULTRA-COMPRESSION TECHNIQUES:\n- Use telegraphic style: \"exports foo, bar, baz\" not \"this file exports three functions\"\n- No code examples unless critical to understanding\n- One sentence per export maximum\n- Omit obvious parameter names (\"opts: Options\" not \"options: Options - configuration object\")\n- Use abbreviations: \"params\" not \"parameters\", \"config\" not \"configuration\"\n- Skip section if empty (don't write \"No special concerns\" or \"N/A\")\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nWORKFLOW & CONVENTION RULES (NEVER EXCLUDE):\n- Contribution requirements: branch naming, commit message formats, PR description templates\n- Testing mandates: required test types, coverage thresholds, \"never delete tests\" policies\n- Tool usage rules: required CLI commands, browser automation tools, linter/formatter requirements\n- Approval workflows: review requirements, CI checks that must pass, deployment gates\n- AI agent instructions: explicit directives for automated assistants (coding rules, prohibited actions)\n- Code conventions: naming standards (camelCase, PascalCase, snake_case for what), import ordering rules, file/folder naming patterns, formatting rules (indentation, quotes, semicolons), language idioms, TypeScript strictness policies, preferred patterns vs anti-patterns\nThese define how contributors (human or AI) must work within the project.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion .annex.sum file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**Barrel export aggregating orchestration module's public API: CommandRunner, ProgressReporter, runPool, PlanTracker, and trace utilities.**\n\n## Exports\n\n**Types:** FileTaskResult, RunSummary, ProgressEvent, CommandRunOptions from types.js; PoolOptions, TaskResult from pool.js; ITraceWriter, TraceEvent, TraceEventPayload from trace.js.\n\n**Classes:** CommandRunner; ProgressReporter, ProgressLog; PlanTracker.\n\n**Functions:** runPool (concurrency pool); createTraceWriter, cleanupOldTraces (trace management).\n\n## Module Dependencies\n\nAggregates from five internal modules: types.js, pool.js, progress.js, plan-tracker.js, trace.js, runner.js. Re-exports form single import surface for orchestration subsystem.\n\n## Usage Pattern\n\nPrimary consumers import CommandRunner, ProgressReporter, and runPool from this barrel. CommandRunner constructor accepts aiService and CommandRunOptions (concurrency parameter); executeGenerate(plan) returns Promise<RunSummary>.",
      "model": "claude-haiku-4-5-20251001",
      "inputTokens": 3,
      "outputTokens": 246,
      "cacheReadTokens": 16147,
      "cacheCreationTokens": 3129,
      "latencyMs": 21009,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/orchestration/index.ts",
          "sizeBytes": 2076
        }
      ]
    },
    {
      "timestamp": "2026-02-14T13:47:26.024Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: src/installer/uninstall.ts\n\n```typescript\n/**\n * Uninstall module for agents-reverse-engineer installer\n *\n * Handles removing installed command files, hooks, and hook registrations.\n * Mirrors the installation logic in operations.ts for clean reversal.\n */\n\nimport { existsSync, unlinkSync, readFileSync, writeFileSync, readdirSync, rmdirSync, rmSync } from 'node:fs';\nimport * as path from 'node:path';\nimport { parse, modify, applyEdits } from 'jsonc-parser';\nimport type { Runtime, Location, InstallerResult } from './types.js';\nimport { resolveInstallPath, getAllRuntimes, resolveCodexConfigPath } from './paths.js';\nimport {\n  getClaudeTemplates,\n  getCodexTemplates,\n  getOpenCodeTemplates,\n  getGeminiTemplates,\n} from '../integration/templates.js';\n\n/**\n * Session hook configuration for settings.json (matches operations.ts)\n */\ninterface SessionHook {\n  type: 'command';\n  command: string;\n}\n\ninterface HookEvent {\n  hooks: SessionHook[];\n}\n\ninterface SettingsJson {\n  hooks?: {\n    SessionStart?: HookEvent[];\n    SessionEnd?: HookEvent[];\n    PostToolUse?: HookEvent[];\n  };\n  permissions?: {\n    allow?: string[];\n    deny?: string[];\n  };\n  [key: string]: unknown;\n}\n\n/**\n * Hook definitions for ARE uninstall (includes legacy hooks for cleanup)\n */\ninterface HookDefinition {\n  event: 'SessionStart' | 'SessionEnd' | 'PostToolUse';\n  filename: string;\n}\n\nconst ARE_HOOKS: HookDefinition[] = [\n  // Current\n  { event: 'PostToolUse', filename: 'are-context-loader.js' },\n  // Legacy (for cleaning up old installations)\n  { event: 'SessionStart', filename: 'are-check-update.js' },\n  { event: 'SessionEnd', filename: 'are-session-end.js' },\n];\n\n/**\n * Plugin definitions for ARE OpenCode (must match operations.ts)\n */\nconst ARE_PLUGIN_FILENAMES = [\n  'are-check-update.js',\n];\n\n/**\n * Agent file created by OpenCode backend's ensureProjectConfig()\n * (must match OPENCODE_AGENT_NAME in src/ai/backends/opencode.ts)\n */\nconst OPENCODE_AGENT_FILENAME = 'are-summarizer.md';\n\n/**\n * Permissions to remove during uninstall (must match operations.ts)\n */\nconst ARE_PERMISSIONS = [\n  'Bash(npx are init*)',\n  'Bash(npx are discover*)',\n  'Bash(npx are generate*)',\n  'Bash(npx are update*)',\n  'Bash(npx are specify*)',\n  'Bash(npx are rebuild*)',\n  'Bash(npx are clean*)',\n  'Bash(rm -f .agents-reverse-engineer/progress.log*)',\n  'Bash(sleep *)',\n];\n\nconst CODEX_RULES_FILENAME = 'are.rules';\nconst LOCAL_CONTEXT_RULES_FILENAME = 'AGENTS.override.md';\nconst LOCAL_CONTEXT_RULES_MARKER =\n  '<!-- Generated by agents-reverse-engineer installer: local-context-rules -->';\n\nfunction getCodexRulesFilePath(location: Location, basePath: string): string {\n  const projectRoot = location === 'local' ? path.dirname(basePath) : undefined;\n  const codexConfigDir = resolveCodexConfigPath(location, projectRoot);\n  return path.join(codexConfigDir, 'rules', CODEX_RULES_FILENAME);\n}\n\nfunction getCodexContextRulesFilePath(location: Location, basePath: string): string {\n  if (location === 'local') {\n    // Local install targets the repository root so Codex loads it as project rules.\n    return path.join(path.dirname(basePath), LOCAL_CONTEXT_RULES_FILENAME);\n  }\n\n  // Global install targets Codex home config so rules apply across repositories.\n  const codexConfigDir = resolveCodexConfigPath('global');\n  return path.join(codexConfigDir, LOCAL_CONTEXT_RULES_FILENAME);\n}\n\n/**\n * Get templates for a specific runtime\n *\n * @param runtime - Target runtime (claude, codex, opencode, or gemini)\n * @returns Array of template objects for the runtime\n */\nfunction getTemplatesForRuntime(runtime: Exclude<Runtime, 'all'>) {\n  switch (runtime) {\n    case 'claude':\n      return getClaudeTemplates();\n    case 'codex':\n      return getCodexTemplates();\n    case 'opencode':\n      return getOpenCodeTemplates();\n    case 'gemini':\n      return getGeminiTemplates();\n  }\n}\n\n/**\n * Uninstall files for one or all runtimes\n *\n * If runtime is 'all', uninstalls from all supported runtimes.\n * Otherwise, uninstalls from the specified runtime only.\n *\n * @param runtime - Target runtime or 'all'\n * @param location - Installation location (global or local)\n * @param dryRun - If true, don't actually delete files\n * @returns Array of uninstallation results (one per runtime)\n */\nexport function uninstallFiles(\n  runtime: Runtime,\n  location: Location,\n  dryRun: boolean = false,\n): InstallerResult[] {\n  if (runtime === 'all') {\n    return getAllRuntimes().map((r) => uninstallFilesForRuntime(r, location, dryRun));\n  }\n  return [uninstallFilesForRuntime(runtime, location, dryRun)];\n}\n\n/**\n * Uninstall files for a specific runtime\n *\n * Removes command templates, hook files, and VERSION file from the installation directory.\n * Also unregisters hooks from settings.json for Claude global installs.\n *\n * @param runtime - Target runtime (claude, codex, opencode, or gemini)\n * @param location - Installation location (global or local)\n * @param dryRun - If true, don't actually delete files\n * @returns Uninstallation result with files deleted\n */\nfunction uninstallFilesForRuntime(\n  runtime: Exclude<Runtime, 'all'>,\n  location: Location,\n  dryRun: boolean,\n): InstallerResult {\n  const basePath = resolveInstallPath(runtime, location);\n  const templates = getTemplatesForRuntime(runtime);\n  const filesCreated: string[] = []; // In uninstall context, this tracks files deleted\n  const filesSkipped: string[] = []; // Files that didn't exist\n  const errors: string[] = [];\n\n  // Remove command templates\n  for (const template of templates) {\n    // Template path is relative (e.g., .claude/commands/are/generate.md)\n    // Extract the part after the runtime directory (e.g., commands/are/generate.md)\n    const relativePath = template.path.split('/').slice(1).join('/');\n    const fullPath = path.join(basePath, relativePath);\n\n    if (existsSync(fullPath)) {\n      if (!dryRun) {\n        try {\n          unlinkSync(fullPath);\n        } catch (err) {\n          errors.push(`Failed to delete ${fullPath}: ${err}`);\n          continue;\n        }\n      }\n      filesCreated.push(fullPath); // Track as \"deleted\"\n    } else {\n      filesSkipped.push(fullPath); // File didn't exist\n    }\n  }\n\n  // Remove hooks/plugins based on runtime\n  let hookUnregistered = false;\n  if (runtime === 'claude' || runtime === 'gemini') {\n    // Remove all ARE hook files\n    for (const hookDef of ARE_HOOKS) {\n      const hookPath = path.join(basePath, 'hooks', hookDef.filename);\n      if (existsSync(hookPath)) {\n        if (!dryRun) {\n          try {\n            unlinkSync(hookPath);\n          } catch (err) {\n            errors.push(`Failed to delete hook ${hookPath}: ${err}`);\n          }\n        }\n        if (!errors.some((e) => e.includes(hookPath))) {\n          filesCreated.push(hookPath);\n        }\n      }\n    }\n\n    // Unregister hooks from settings.json\n    hookUnregistered = unregisterHooks(basePath, runtime, dryRun);\n\n    // Unregister permissions from settings.json (Claude only)\n    if (runtime === 'claude') {\n      unregisterPermissions(basePath, dryRun);\n    }\n  } else if (runtime === 'codex') {\n    // Remove Codex rules file installed by ARE.\n    const rulesPath = getCodexRulesFilePath(location, basePath);\n    if (existsSync(rulesPath)) {\n      if (!dryRun) {\n        try {\n          unlinkSync(rulesPath);\n        } catch (err) {\n          errors.push(`Failed to delete Codex rules ${rulesPath}: ${err}`);\n        }\n      }\n      if (!errors.some((e) => e.includes(rulesPath))) {\n        filesCreated.push(rulesPath);\n      }\n    }\n\n    // Remove Codex context rules only if this installer created them.\n    const contextRulesPath = getCodexContextRulesFilePath(location, basePath);\n    if (existsSync(contextRulesPath)) {\n      let isManagedByAre = false;\n      try {\n        const content = readFileSync(contextRulesPath, 'utf-8');\n        isManagedByAre = content.includes(LOCAL_CONTEXT_RULES_MARKER);\n      } catch {\n        isManagedByAre = false;\n      }\n\n      if (isManagedByAre) {\n        if (!dryRun) {\n          try {\n            unlinkSync(contextRulesPath);\n          } catch (err) {\n            errors.push(`Failed to delete context rules ${contextRulesPath}: ${err}`);\n          }\n        }\n        if (!errors.some((e) => e.includes(contextRulesPath))) {\n          filesCreated.push(contextRulesPath);\n        }\n      }\n    }\n  } else if (runtime === 'opencode') {\n    // Remove all ARE plugin files\n    for (const pluginFilename of ARE_PLUGIN_FILENAMES) {\n      const pluginPath = path.join(basePath, 'plugins', pluginFilename);\n      if (existsSync(pluginPath)) {\n        if (!dryRun) {\n          try {\n            unlinkSync(pluginPath);\n          } catch (err) {\n            errors.push(`Failed to delete plugin ${pluginPath}: ${err}`);\n          }\n        }\n        if (!errors.some((e) => e.includes(pluginPath))) {\n          filesCreated.push(pluginPath);\n          hookUnregistered = true;\n        }\n      }\n    }\n\n    // Remove ARE agent file (created by OpenCode backend's ensureProjectConfig())\n    const agentPath = path.join(basePath, 'agents', OPENCODE_AGENT_FILENAME);\n    if (existsSync(agentPath)) {\n      if (!dryRun) {\n        try {\n          unlinkSync(agentPath);\n        } catch (err) {\n          errors.push(`Failed to delete agent ${agentPath}: ${err}`);\n        }\n      }\n      if (!errors.some((e) => e.includes(agentPath))) {\n        filesCreated.push(agentPath);\n      }\n    }\n  }\n\n  // Remove ARE-VERSION file if exists\n  const versionPath = path.join(basePath, 'ARE-VERSION');\n  if (existsSync(versionPath)) {\n    if (!dryRun) {\n      try {\n        unlinkSync(versionPath);\n      } catch (err) {\n        errors.push(`Failed to delete ARE-VERSION: ${err}`);\n      }\n    }\n    if (!errors.some((e) => e.includes('ARE-VERSION'))) {\n      filesCreated.push(versionPath);\n    }\n  }\n\n  // Try to clean up empty directories\n  if (!dryRun) {\n    if (runtime === 'claude' || runtime === 'codex') {\n      // Claude and Codex use skills format: clean up skills/are-* directories\n      const skillsDir = path.join(basePath, 'skills');\n      cleanupAreSkillDirs(skillsDir);\n      cleanupEmptyDirs(skillsDir);\n\n      if (runtime === 'codex') {\n        // Clean up empty .codex/rules tree if ARE installed it.\n        const codexRulesDir = path.dirname(getCodexRulesFilePath(location, basePath));\n        cleanupEmptyDirs(codexRulesDir);\n      }\n    } else if (runtime === 'gemini') {\n      // Gemini uses flat commands/ directory for TOML files\n      const commandsDir = path.join(basePath, 'commands');\n      cleanupEmptyDirs(commandsDir);\n      // Clean up legacy files from old installations\n      cleanupLegacyGeminiFiles(commandsDir);\n    } else {\n      // OpenCode uses commands format with flat .md files\n      const commandsDir = path.join(basePath, 'commands');\n      cleanupEmptyDirs(commandsDir);\n    }\n\n    // Clean up hooks/plugins directory if empty\n    if (runtime === 'claude' || runtime === 'gemini') {\n      const hooksDir = path.join(basePath, 'hooks');\n      cleanupEmptyDirs(hooksDir);\n    } else if (runtime === 'opencode') {\n      const pluginsDir = path.join(basePath, 'plugins');\n      cleanupEmptyDirs(pluginsDir);\n      const agentsDir = path.join(basePath, 'agents');\n      cleanupEmptyDirs(agentsDir);\n\n      // Clean up OpenCode plugin infrastructure files if no user content remains\n      // (package.json, node_modules, bun.lock, .gitignore created by OpenCode's plugin system)\n      if (location === 'local') {\n        cleanupOpenCodeInfrastructure(basePath);\n      }\n    }\n  }\n\n  return {\n    success: errors.length === 0,\n    runtime,\n    location,\n    filesCreated, // Actually files deleted in uninstall context\n    filesSkipped, // Files that didn't exist\n    errors,\n    hookRegistered: hookUnregistered, // Repurpose: true if hook was unregistered\n  };\n}\n\n/**\n * Gemini hook configuration (simpler format)\n */\ninterface GeminiHook {\n  name: string;\n  type: 'command';\n  command: string;\n}\n\ninterface GeminiSettingsJson {\n  hooks?: {\n    SessionStart?: GeminiHook[];\n    SessionEnd?: GeminiHook[];\n  };\n  [key: string]: unknown;\n}\n\n/**\n * Unregister ARE hooks from settings.json\n *\n * Removes all ARE hook entries from SessionStart, SessionEnd, and PostToolUse arrays.\n * Cleans up empty hooks structures. Handles both old and new hook paths.\n *\n * @param basePath - Base installation path (e.g., ~/.claude or ~/.gemini)\n * @param runtime - Target runtime (claude or gemini)\n * @param dryRun - If true, don't write changes\n * @returns true if any hook was removed, false if none found\n */\nexport function unregisterHooks(\n  basePath: string,\n  runtime: Exclude<Runtime, 'all'>,\n  dryRun: boolean,\n): boolean {\n  if (runtime === 'gemini') {\n    return unregisterGeminiHooks(basePath, dryRun);\n  }\n  return unregisterClaudeHooks(basePath, dryRun);\n}\n\n/**\n * Build hook command patterns for matching (includes legacy paths)\n */\nfunction getHookPatterns(runtimeDir: string): string[] {\n  const patterns: string[] = [];\n  for (const hookDef of ARE_HOOKS) {\n    // Current path format\n    patterns.push(`node ${runtimeDir}/hooks/${hookDef.filename}`);\n    // Legacy path format\n    patterns.push(`node hooks/${hookDef.filename}`);\n  }\n  return patterns;\n}\n\n/**\n * Unregister ARE hooks from Claude Code settings.json\n */\nfunction unregisterClaudeHooks(basePath: string, dryRun: boolean): boolean {\n  const settingsPath = path.join(basePath, 'settings.json');\n\n  // Settings file must exist\n  if (!existsSync(settingsPath)) {\n    return false;\n  }\n\n  // Load settings (JSONC-aware)\n  let content: string;\n  try {\n    content = readFileSync(settingsPath, 'utf-8');\n  } catch {\n    return false;\n  }\n\n  const settings = (parse(content) ?? {}) as SettingsJson;\n\n  if (!settings.hooks) {\n    return false;\n  }\n\n  const hookPatterns = getHookPatterns('.claude');\n  let removedAny = false;\n\n  // Process all hook event types (including legacy SessionEnd for cleanup)\n  for (const eventType of ['SessionStart', 'SessionEnd', 'PostToolUse'] as const) {\n    if (!settings.hooks[eventType]) {\n      continue;\n    }\n\n    const originalLength = settings.hooks[eventType]!.length;\n\n    settings.hooks[eventType] = settings.hooks[eventType]!.filter(\n      (event) => !event.hooks?.some((h) => hookPatterns.includes(h.command)),\n    );\n\n    if (settings.hooks[eventType]!.length < originalLength) {\n      removedAny = true;\n    }\n\n    // Clean up empty array\n    if (settings.hooks[eventType]!.length === 0) {\n      delete settings.hooks[eventType];\n    }\n  }\n\n  if (!removedAny) {\n    return false;\n  }\n\n  // Clean up empty hooks object\n  const hooksEmpty = Object.keys(settings.hooks).length === 0;\n\n  // Write updated settings preserving comments\n  if (!dryRun) {\n    const fmt = { formattingOptions: { tabSize: 2, insertSpaces: true } };\n    const updated = hooksEmpty\n      ? applyEdits(content, modify(content, ['hooks'], undefined, fmt))\n      : applyEdits(content, modify(content, ['hooks'], settings.hooks, fmt));\n    writeFileSync(settingsPath, updated, 'utf-8');\n  }\n\n  return true;\n}\n\n/**\n * Unregister ARE permissions from Claude Code settings.json\n *\n * Removes all ARE-related bash command permissions from the allow list.\n *\n * @param basePath - Base installation path (e.g., ~/.claude)\n * @param dryRun - If true, don't write changes\n * @returns true if any permissions were removed, false if none found\n */\nexport function unregisterPermissions(basePath: string, dryRun: boolean): boolean {\n  const settingsPath = path.join(basePath, 'settings.json');\n\n  // Settings file must exist\n  if (!existsSync(settingsPath)) {\n    return false;\n  }\n\n  // Load settings (JSONC-aware)\n  let content: string;\n  try {\n    content = readFileSync(settingsPath, 'utf-8');\n  } catch {\n    return false;\n  }\n\n  const settings = (parse(content) ?? {}) as SettingsJson;\n\n  // Check if permissions.allow exists\n  if (!settings.permissions?.allow) {\n    return false;\n  }\n\n  const originalLength = settings.permissions.allow.length;\n\n  // Remove all ARE permissions\n  settings.permissions.allow = settings.permissions.allow.filter(\n    (perm) => !ARE_PERMISSIONS.includes(perm),\n  );\n\n  // Check if we actually removed something\n  if (settings.permissions.allow.length === originalLength) {\n    return false;\n  }\n\n  // Clean up empty structures\n  if (settings.permissions.allow.length === 0) {\n    delete settings.permissions.allow;\n  }\n\n  const permsEmpty = !settings.permissions || Object.keys(settings.permissions).length === 0;\n\n  // Write updated settings preserving comments\n  if (!dryRun) {\n    const fmt = { formattingOptions: { tabSize: 2, insertSpaces: true } };\n    const updated = permsEmpty\n      ? applyEdits(content, modify(content, ['permissions'], undefined, fmt))\n      : applyEdits(content, modify(content, ['permissions'], settings.permissions, fmt));\n    writeFileSync(settingsPath, updated, 'utf-8');\n  }\n\n  return true;\n}\n\n/**\n * Unregister ARE hooks from Gemini CLI settings.json\n */\nfunction unregisterGeminiHooks(basePath: string, dryRun: boolean): boolean {\n  const settingsPath = path.join(basePath, 'settings.json');\n\n  // Settings file must exist\n  if (!existsSync(settingsPath)) {\n    return false;\n  }\n\n  // Load settings (JSONC-aware)\n  let content: string;\n  try {\n    content = readFileSync(settingsPath, 'utf-8');\n  } catch {\n    return false;\n  }\n\n  const settings = (parse(content) ?? {}) as GeminiSettingsJson;\n\n  if (!settings.hooks) {\n    return false;\n  }\n\n  const hookPatterns = getHookPatterns('.gemini');\n  let removedAny = false;\n\n  // Process both SessionStart and SessionEnd\n  for (const eventType of ['SessionStart', 'SessionEnd'] as const) {\n    if (!settings.hooks[eventType]) {\n      continue;\n    }\n\n    const originalLength = settings.hooks[eventType]!.length;\n\n    settings.hooks[eventType] = settings.hooks[eventType]!.filter(\n      (h) => !hookPatterns.includes(h.command),\n    );\n\n    if (settings.hooks[eventType]!.length < originalLength) {\n      removedAny = true;\n    }\n\n    // Clean up empty array\n    if (settings.hooks[eventType]!.length === 0) {\n      delete settings.hooks[eventType];\n    }\n  }\n\n  if (!removedAny) {\n    return false;\n  }\n\n  // Clean up empty hooks object\n  const hooksEmpty = Object.keys(settings.hooks).length === 0;\n\n  // Write updated settings preserving comments\n  if (!dryRun) {\n    const fmt = { formattingOptions: { tabSize: 2, insertSpaces: true } };\n    const updated = hooksEmpty\n      ? applyEdits(content, modify(content, ['hooks'], undefined, fmt))\n      : applyEdits(content, modify(content, ['hooks'], settings.hooks, fmt));\n    writeFileSync(settingsPath, updated, 'utf-8');\n  }\n\n  return true;\n}\n\n/**\n * Clean up ARE skill directories\n *\n * Removes all empty are-* skill directories from the skills folder.\n *\n * @param skillsDir - Path to the skills directory\n */\nfunction cleanupAreSkillDirs(skillsDir: string): void {\n  try {\n    if (!existsSync(skillsDir)) {\n      return;\n    }\n\n    const entries = readdirSync(skillsDir);\n    for (const entry of entries) {\n      // Only clean up are-* directories\n      if (entry.startsWith('are-')) {\n        const skillDir = path.join(skillsDir, entry);\n        cleanupEmptyDirs(skillDir);\n      }\n    }\n  } catch {\n    // Ignore errors\n  }\n}\n\n/**\n * Clean up empty directories recursively\n *\n * Removes a directory if it's empty, then tries parent directories.\n * Stops when hitting a non-empty directory or the runtime root.\n *\n * @param dirPath - Directory path to check and potentially remove\n */\nfunction cleanupEmptyDirs(dirPath: string): void {\n  try {\n    if (!existsSync(dirPath)) {\n      return;\n    }\n\n    const entries = readdirSync(dirPath);\n    if (entries.length === 0) {\n      rmdirSync(dirPath);\n\n      // Try parent directory (but don't go above runtime root)\n      const parent = path.dirname(dirPath);\n      // Stop at common runtime roots (.claude, .agents, .codex, .opencode, .gemini)\n      const baseName = path.basename(parent);\n      if (\n        baseName !== '.claude' &&\n        baseName !== '.agents' &&\n        baseName !== '.codex' &&\n        baseName !== '.opencode' &&\n        baseName !== '.gemini' &&\n        baseName !== '.config'\n      ) {\n        cleanupEmptyDirs(parent);\n      }\n    }\n  } catch {\n    // Ignore errors - directory might be in use or we don't have permissions\n  }\n}\n\n/**\n * Clean up legacy Gemini files from old installations\n *\n * Removes:\n * - Old are-*.md files from .gemini/commands/ (pre-TOML format)\n * - Old .toml files from .gemini/commands/are/ (pre-flat structure)\n *\n * @param commandsDir - Path to the commands directory\n */\nfunction cleanupLegacyGeminiFiles(commandsDir: string): void {\n  try {\n    if (!existsSync(commandsDir)) {\n      return;\n    }\n\n    const entries = readdirSync(commandsDir);\n    for (const entry of entries) {\n      // Remove legacy are-*.md files\n      if (entry.startsWith('are-') && entry.endsWith('.md')) {\n        const filePath = path.join(commandsDir, entry);\n        try {\n          unlinkSync(filePath);\n        } catch {\n          // Ignore errors\n        }\n      }\n    }\n\n    // Remove legacy nested are/ directory (old /are:* namespace structure)\n    const areDir = path.join(commandsDir, 'are');\n    if (existsSync(areDir)) {\n      const areEntries = readdirSync(areDir);\n      for (const entry of areEntries) {\n        if (entry.endsWith('.toml')) {\n          try {\n            unlinkSync(path.join(areDir, entry));\n          } catch {\n            // Ignore errors\n          }\n        }\n      }\n      cleanupEmptyDirs(areDir);\n    }\n  } catch {\n    // Ignore errors\n  }\n}\n\n/**\n * Clean up OpenCode plugin infrastructure files\n *\n * When ARE installs plugins to .opencode/, OpenCode's plugin system creates\n * infrastructure files (package.json, node_modules/, bun.lock, .gitignore).\n * After removing ARE files, these artifacts remain. This function removes them\n * if no other commands or plugins exist in the directory.\n *\n * Only safe for local installs — global config dirs may have other content.\n *\n * @param basePath - Path to the .opencode directory\n */\nfunction cleanupOpenCodeInfrastructure(basePath: string): void {\n  // Only clean up if no commands, plugins, or agents remain\n  const commandsDir = path.join(basePath, 'commands');\n  const pluginsDir = path.join(basePath, 'plugins');\n  const agentsDir = path.join(basePath, 'agents');\n\n  if (dirHasContent(commandsDir) || dirHasContent(pluginsDir) || dirHasContent(agentsDir)) {\n    return;\n  }\n\n  // Remove OpenCode plugin infrastructure files\n  const infraFiles = ['package.json', 'bun.lock', '.gitignore'];\n  for (const file of infraFiles) {\n    const filePath = path.join(basePath, file);\n    try {\n      if (existsSync(filePath)) {\n        unlinkSync(filePath);\n      }\n    } catch {\n      // Ignore errors\n    }\n  }\n\n  // Remove node_modules\n  const nodeModulesDir = path.join(basePath, 'node_modules');\n  if (existsSync(nodeModulesDir)) {\n    try {\n      rmSync(nodeModulesDir, { recursive: true, force: true });\n    } catch {\n      // Ignore errors\n    }\n  }\n\n  // Try to remove the .opencode directory itself if now empty\n  cleanupEmptyDirs(basePath);\n}\n\n/**\n * Check if a directory exists and has any entries\n *\n * @param dirPath - Directory path to check\n * @returns true if directory exists and is non-empty\n */\nfunction dirHasContent(dirPath: string): boolean {\n  try {\n    if (!existsSync(dirPath)) return false;\n    return readdirSync(dirPath).length > 0;\n  } catch {\n    return false;\n  }\n}\n\n/**\n * Configuration directory name (matches config/loader.ts)\n */\nconst CONFIG_DIR = '.agents-reverse-engineer';\n\n/**\n * Delete the .agents-reverse-engineer configuration folder\n *\n * Only applicable for local installations. Removes the entire folder\n * including configuration files and generation plans.\n *\n * @param location - Installation location (only 'local' triggers deletion)\n * @param dryRun - If true, don't actually delete\n * @returns true if folder was deleted, false if not found or not local\n */\nexport function deleteConfigFolder(location: Location, dryRun: boolean): boolean {\n  // Only delete for local installations\n  if (location !== 'local') {\n    return false;\n  }\n\n  const configPath = path.join(process.cwd(), CONFIG_DIR);\n\n  if (!existsSync(configPath)) {\n    return false;\n  }\n\n  if (!dryRun) {\n    try {\n      rmSync(configPath, { recursive: true, force: true });\n    } catch {\n      return false;\n    }\n  }\n\n  return true;\n}\n\n```\n\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n- Workflow & convention rules: if this file defines contribution guidelines, PR conventions, commit standards, testing mandates, tool usage requirements, approval workflows, AI agent behavioral instructions, or code conventions (naming standards, formatting rules, import ordering, linting policies), extract them as explicit, actionable rules. Sources: CONTRIBUTING.md, CI configs, PR templates, style guides, linter/formatter configs (.eslintrc, .prettierrc, .editorconfig), tsconfig strictness, README dev sections.\n\n\nTARGET LENGTH (MANDATORY):\n- Source file: 24378 characters\n- Target summary: ~2438 characters (10% compression)\n- Maximum: 2926 characters\n- Achieve this by: ultra-dense writing, minimal examples, single-sentence descriptions\n\nULTRA-COMPRESSION TECHNIQUES:\n- Use telegraphic style: \"exports foo, bar, baz\" not \"this file exports three functions\"\n- No code examples unless critical to understanding\n- One sentence per export maximum\n- Omit obvious parameter names (\"opts: Options\" not \"options: Options - configuration object\")\n- Use abbreviations: \"params\" not \"parameters\", \"config\" not \"configuration\"\n- Skip section if empty (don't write \"No special concerns\" or \"N/A\")\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nWORKFLOW & CONVENTION RULES (NEVER EXCLUDE):\n- Contribution requirements: branch naming, commit message formats, PR description templates\n- Testing mandates: required test types, coverage thresholds, \"never delete tests\" policies\n- Tool usage rules: required CLI commands, browser automation tools, linter/formatter requirements\n- Approval workflows: review requirements, CI checks that must pass, deployment gates\n- AI agent instructions: explicit directives for automated assistants (coding rules, prohibited actions)\n- Code conventions: naming standards (camelCase, PascalCase, snake_case for what), import ordering rules, file/folder naming patterns, formatting rules (indentation, quotes, semicolons), language idioms, TypeScript strictness policies, preferred patterns vs anti-patterns\nThese define how contributors (human or AI) must work within the project.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion .annex.sum file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**uninstall.ts reverses installation of ARE (agents-reverse-engineer) by removing command templates, hooks, plugins, permissions, and configuration across multiple AI runtimes (Claude, Codex, OpenCode, Gemini).**\n\n## Exports\n\n`uninstallFiles(runtime: Runtime, location: Location, dryRun?: boolean): InstallerResult[]` — Uninstalls files for specified runtime or 'all' runtimes, returns array of results.\n\n`unregisterHooks(basePath: string, runtime: Exclude<Runtime, 'all'>, dryRun: boolean): boolean` — Removes ARE hook entries from settings.json for Claude or Gemini, returns true if any hook was removed.\n\n`unregisterPermissions(basePath: string, dryRun: boolean): boolean` — Removes ARE Bash permissions from Claude Code settings.json, returns true if any removed.\n\n`deleteConfigFolder(location: Location, dryRun: boolean): boolean` — Deletes .agents-reverse-engineer folder for local installations, returns true if successful.\n\n## Hook & Plugin Registration Constants\n\n`ARE_HOOKS: HookDefinition[]` — Current and legacy hook definitions for ARE (PostToolUse: are-context-loader.js; legacy SessionStart: are-check-update.js; SessionEnd: are-session-end.js); used for cleanup across Claude/Gemini.\n\n`ARE_PLUGIN_FILENAMES: string[]` — OpenCode plugins to remove (are-check-update.js).\n\n`OPENCODE_AGENT_FILENAME = 'are-summarizer.md'` — Agent file created by OpenCode backend ensureProjectConfig(), matches OPENCODE_AGENT_NAME in src/ai/backends/opencode.ts.\n\n`ARE_PERMISSIONS: string[]` — Eight Bash command patterns for Claude permission removal (npx are init/discover/generate/update/specify/rebuild/clean, rm -f .agents-reverse-engineer/progress.log, sleep).\n\n## Configuration & File Path Handling\n\n`CODEX_RULES_FILENAME = 'are.rules'` — Codex rules file installed by ARE.\n\n`LOCAL_CONTEXT_RULES_FILENAME = 'AGENTS.override.md'` — Context rules file for Codex.\n\n`LOCAL_CONTEXT_RULES_MARKER = '<!-- Generated by agents-reverse-engineer installer: local-context-rules -->'` — Marker used to identify ARE-managed context rules files; only files with this marker are deleted during uninstall.\n\n`CONFIG_DIR = '.agents-reverse-engineer'` — Configuration directory name (matches config/loader.ts); deleted for local installs only.\n\n`getCodexRulesFilePath(location: Location, basePath: string): string` — Resolves Codex rules path (local → project root/rules/are.rules; global → Codex home).\n\n`getCodexContextRulesFilePath(location: Location, basePath: string): string` — Resolves Codex context rules path (local → repository root/AGENTS.override.md; global → Codex config/AGENTS.override.md).\n\n## Runtime-Specific Deletion Logic\n\n`getTemplatesForRuntime(runtime: Exclude<Runtime, 'all'>): Template[]` — Dispatches getClaudeTemplates(), getCodexTemplates(), getOpenCodeTemplates(), or getGeminiTemplates() based on runtime.\n\n`uninstallFilesForRuntime(runtime: Exclude<Runtime, 'all'>, location: Location, dryRun: boolean): InstallerResult` — Core uninstall function: removes command templates, hook files (Claude/Gemini), plugins (OpenCode), Codex rules/context-rules, ARE-VERSION; cleans empty directories; unregisters hooks/permissions; returns InstallerResult with filesCreated (actually deleted), filesSkipped, errors, hookRegistered (true if unregistered).\n\n## Hook Unregistration\n\n`unregisterClaudeHooks(basePath: string, dryRun: boolean): boolean` — Removes ARE hook entries from Claude's settings.json across SessionStart/SessionEnd/PostToolUse events using hook pattern matching, preserves JSONC comments via jsonc-parser.\n\n`unregisterGeminiHooks(basePath: string, dryRun: boolean): boolean` — Removes ARE hooks from Gemini's settings.json across SessionStart/SessionEnd events.\n\n`getHookPatterns(runtimeDir: string): string[]` — Generates hook command patterns for matching: \"node {runtimeDir}/hooks/{filename}\" (current) and \"node hooks/{filename}\" (legacy).\n\n## Directory & File Cleanup Utilities\n\n`cleanupAreSkillDirs(skillsDir: string): void` — Removes empty are-* skill subdirectories from Claude/Codex skills folder.\n\n`cleanupEmptyDirs(dirPath: string): void` — Recursively removes empty directories up the tree, stops at runtime roots (.claude, .codex, .opencode, .gemini, .agents, .config).\n\n`cleanupLegacyGeminiFiles(commandsDir: string): void` — Removes legacy are-*.md files (pre-TOML format) and .toml files in are/ subdirectory (pre-flat structure) from Gemini installations.\n\n`cleanupOpenCodeInfrastructure(basePath: string): void` — Removes OpenCode plugin infrastructure (package.json, bun.lock, .gitignore, node_modules/) only if no other commands/plugins/agents exist; only for local installs.\n\n`dirHasContent(dirPath: string): boolean` — Returns true if directory exists and is non-empty; used to guard OpenCode infrastructure cleanup.\n\n## Type Definitions\n\n`SessionHook` — { type: 'command'; command: string } — Hook configuration for Claude/Gemini.\n\n`HookEvent` — { hooks: SessionHook[] } — Event wrapper for hook arrays.\n\n`SettingsJson` — Partial schema: hooks (SessionStart/SessionEnd/PostToolUse arrays), permissions (allow/deny strings).\n\n`HookDefinition` — { event: 'SessionStart'|'SessionEnd'|'PostToolUse'; filename: string } — Hook metadata.\n\n`GeminiHook` — { name: string; type: 'command'; command: string } — Gemini-specific hook format.\n\n`GeminiSettingsJson` — Partial schema: hooks (SessionStart/SessionEnd arrays only; no PostToolUse).\n\n## Key Dependencies\n\n`jsonc-parser` — parse(), modify(), applyEdits() for reading/writing settings.json while preserving JSONC comments.\n\n`./paths.js` — resolveInstallPath(), getAllRuntimes(), resolveCodexConfigPath() for path resolution.\n\n`../integration/templates.js` — getClaudeTemplates(), getCodexTemplates(), getOpenCodeTemplates(), getGeminiTemplates() for template metadata.\n\n`node:fs` — existsSync, unlinkSync, readFileSync, writeFileSync, readdirSync, rmdirSync, rmSync.",
      "model": "claude-haiku-4-5-20251001",
      "inputTokens": 3,
      "outputTokens": 1674,
      "cacheReadTokens": 16147,
      "cacheCreationTokens": 10365,
      "latencyMs": 34767,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/installer/uninstall.ts",
          "sizeBytes": 24380
        }
      ]
    },
    {
      "timestamp": "2026-02-14T13:47:37.135Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: src/orchestration/plan-tracker.ts\n\n```typescript\n/**\n * Progress tracker that updates GENERATION-PLAN.md checkboxes during generation.\n *\n * Maintains the markdown content in memory for fast updates and serializes\n * disk writes via a promise chain to handle concurrent Phase 1 completions.\n *\n * @module\n */\n\nimport { writeFile, mkdir } from 'node:fs/promises';\nimport * as path from 'node:path';\nimport { CONFIG_DIR } from '../config/loader.js';\n\n/**\n * Tracks generation progress by ticking checkboxes in GENERATION-PLAN.md.\n *\n * Create one instance at the start of `executeGenerate()`, call `markDone()`\n * as tasks complete, and `flush()` before returning.\n */\nexport class PlanTracker {\n  private content: string;\n  private readonly planPath: string;\n  private writeQueue: Promise<void> = Promise.resolve();\n\n  constructor(projectRoot: string, initialMarkdown: string) {\n    this.planPath = path.join(projectRoot, CONFIG_DIR, 'GENERATION-PLAN.md');\n    this.content = initialMarkdown;\n  }\n\n  /** Write the initial plan file to disk. */\n  async initialize(): Promise<void> {\n    try {\n      await mkdir(path.dirname(this.planPath), { recursive: true });\n      await writeFile(this.planPath, this.content, 'utf8');\n    } catch {\n      // Non-critical — generation continues without tracking\n    }\n  }\n\n  /**\n   * Mark a task as done by replacing its checkbox.\n   *\n   * The caller must pass the exact path as it appears in the markdown:\n   * - File: `src/cli/init.ts`\n   * - Directory: `src/cli/AGENTS.md`  (caller appends `/AGENTS.md`)\n   * - Pointer: `CLAUDE.md`\n   */\n  markDone(itemPath: string): void {\n    const before = this.content;\n    this.content = this.content.replace(\n      `- [ ] \\`${itemPath}\\``,\n      `- [x] \\`${itemPath}\\``,\n    );\n\n    if (this.content === before) return; // no match — skip write\n\n    // Queue a serialized write so concurrent markDone() calls don't corrupt the file\n    this.writeQueue = this.writeQueue\n      .then(() => writeFile(this.planPath, this.content, 'utf8'))\n      .catch(() => {/* non-critical */});\n  }\n\n  /** Wait for all queued writes to finish. */\n  async flush(): Promise<void> {\n    await this.writeQueue;\n  }\n}\n\n```\n\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n- Workflow & convention rules: if this file defines contribution guidelines, PR conventions, commit standards, testing mandates, tool usage requirements, approval workflows, AI agent behavioral instructions, or code conventions (naming standards, formatting rules, import ordering, linting policies), extract them as explicit, actionable rules. Sources: CONTRIBUTING.md, CI configs, PR templates, style guides, linter/formatter configs (.eslintrc, .prettierrc, .editorconfig), tsconfig strictness, README dev sections.\n\n\nTARGET LENGTH (MANDATORY):\n- Source file: 2125 characters\n- Target summary: ~213 characters (10% compression)\n- Maximum: 256 characters\n- Achieve this by: ultra-dense writing, minimal examples, single-sentence descriptions\n\nULTRA-COMPRESSION TECHNIQUES:\n- Use telegraphic style: \"exports foo, bar, baz\" not \"this file exports three functions\"\n- No code examples unless critical to understanding\n- One sentence per export maximum\n- Omit obvious parameter names (\"opts: Options\" not \"options: Options - configuration object\")\n- Use abbreviations: \"params\" not \"parameters\", \"config\" not \"configuration\"\n- Skip section if empty (don't write \"No special concerns\" or \"N/A\")\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nWORKFLOW & CONVENTION RULES (NEVER EXCLUDE):\n- Contribution requirements: branch naming, commit message formats, PR description templates\n- Testing mandates: required test types, coverage thresholds, \"never delete tests\" policies\n- Tool usage rules: required CLI commands, browser automation tools, linter/formatter requirements\n- Approval workflows: review requirements, CI checks that must pass, deployment gates\n- AI agent instructions: explicit directives for automated assistants (coding rules, prohibited actions)\n- Code conventions: naming standards (camelCase, PascalCase, snake_case for what), import ordering rules, file/folder naming patterns, formatting rules (indentation, quotes, semicolons), language idioms, TypeScript strictness policies, preferred patterns vs anti-patterns\nThese define how contributors (human or AI) must work within the project.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion .annex.sum file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**PlanTracker maintains in-memory markdown content and serializes disk writes to update GENERATION-PLAN.md checkboxes during artifact generation.**\n\n## Exports\n\n`PlanTracker` class: Constructor takes `projectRoot: string, initialMarkdown: string`. Methods: `initialize(): Promise<void>` writes initial plan to disk; `markDone(itemPath: string): void` replaces `- [ ] \\`${itemPath}\\`` with `- [x] \\`${itemPath}\\`` in memory and queues serialized write; `flush(): Promise<void>` awaits completion of all queued writes.\n\n## Key Design\n\nUses Promise chain (`writeQueue`) to serialize concurrent `markDone()` calls and prevent file corruption. Disk writes are non-critical (caught silently); generation proceeds if file I/O fails. In-memory content updates are synchronous; disk serialization is asynchronous.\n\n## Path Convention\n\n`markDone()` expects exact markdown checkbox paths: files as `src/cli/init.ts`, directories as `src/cli/AGENTS.md` (caller appends `/AGENTS.md`), pointers as `CLAUDE.md`. Caller is responsible for formatting; `markDone()` performs literal string replacement on `- [ ] \\`${itemPath}\\``.\n\n## File Location\n\nPlan file location: `${projectRoot}/${CONFIG_DIR}/GENERATION-PLAN.md` where `CONFIG_DIR` imported from `../config/loader.js`.",
      "model": "claude-haiku-4-5-20251001",
      "inputTokens": 3,
      "outputTokens": 340,
      "cacheReadTokens": 16147,
      "cacheCreationTokens": 3382,
      "latencyMs": 24421,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/orchestration/plan-tracker.ts",
          "sizeBytes": 2129
        }
      ]
    },
    {
      "timestamp": "2026-02-14T13:47:39.581Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: src/orchestration/pool.ts\n\n```typescript\n/**\n * Iterator-based concurrency pool.\n *\n * Provides a zero-dependency concurrency limiter using the shared-iterator\n * worker pattern. Workers pull from a shared iterator so exactly N tasks\n * execute concurrently, with new tasks starting as previous ones complete.\n *\n * This avoids the \"batch\" anti-pattern where `Promise.all` on chunks of N\n * tasks idles workers while waiting for the slowest task in each batch.\n *\n * @module\n */\n\nimport type { ITraceWriter } from './trace.js';\n\n// ---------------------------------------------------------------------------\n// Types\n// ---------------------------------------------------------------------------\n\n/**\n * Options for the concurrency pool.\n */\nexport interface PoolOptions {\n  /** Maximum number of concurrent workers */\n  concurrency: number;\n  /** Stop pulling new tasks on first error */\n  failFast?: boolean;\n  /** Trace writer for concurrency debugging (no-op when tracing is off) */\n  tracer?: ITraceWriter;\n  /** Phase label for trace events (e.g., 'phase-1-files') */\n  phaseLabel?: string;\n  /** Labels for each task by index (e.g., file paths). Used in trace events. */\n  taskLabels?: string[];\n}\n\n/**\n * Result of a single task execution within the pool.\n *\n * Indexed by the task's position in the original array so callers\n * can correlate results back to their inputs.\n */\nexport interface TaskResult<T> {\n  /** Zero-based index of the task in the original array */\n  index: number;\n  /** Whether the task completed successfully */\n  success: boolean;\n  /** The resolved value (present when success is true) */\n  value?: T;\n  /** The error (present when success is false) */\n  error?: Error;\n}\n\n// ---------------------------------------------------------------------------\n// Pool implementation\n// ---------------------------------------------------------------------------\n\n/**\n * Run an array of async task factories through a concurrency-limited pool.\n *\n * Uses the shared-iterator pattern: all workers iterate over the same\n * `entries()` iterator, so each task is picked up by exactly one worker.\n * When a worker finishes a task, it immediately pulls the next one from\n * the iterator, keeping all worker slots busy.\n *\n * @typeParam T - The resolved type of each task\n * @param tasks - Array of zero-argument async functions to execute\n * @param options - Pool configuration (concurrency, failFast, tracing)\n * @param onComplete - Optional callback invoked after each task settles\n * @returns Array of results indexed by original task position (may be sparse if aborted)\n *\n * @example\n * ```typescript\n * const results = await runPool(\n *   urls.map(url => () => fetch(url).then(r => r.json())),\n *   { concurrency: 5, failFast: false },\n *   (result) => console.log(`Task ${result.index}: ${result.success}`),\n * );\n * ```\n */\nexport async function runPool<T>(\n  tasks: Array<() => Promise<T>>,\n  options: PoolOptions,\n  onComplete?: (result: TaskResult<T>) => void,\n): Promise<TaskResult<T>[]> {\n  const results: TaskResult<T>[] = [];\n\n  if (tasks.length === 0) {\n    return results;\n  }\n\n  const tracer = options.tracer;\n  const phase = options.phaseLabel ?? 'unknown';\n  const taskLabels = options.taskLabels;\n\n  // Shared abort flag -- workers check before pulling next task\n  let aborted = false;\n\n  // Active task counter for trace snapshots\n  let activeTasks = 0;\n\n  async function worker(\n    iterator: IterableIterator<[number, () => Promise<T>]>,\n    workerId: number,\n  ): Promise<void> {\n    let tasksExecuted = 0;\n\n    tracer?.emit({ type: 'worker:start', workerId, phase });\n\n    for (const [index, task] of iterator) {\n      if (aborted) break;\n\n      activeTasks++;\n      const taskStart = Date.now();\n      const label = taskLabels?.[index] ?? `task-${index}`;\n\n      tracer?.emit({\n        type: 'task:pickup',\n        workerId,\n        taskIndex: index,\n        taskLabel: label,\n        activeTasks,\n      });\n\n      try {\n        const value = await task();\n        activeTasks--;\n        tasksExecuted++;\n\n        tracer?.emit({\n          type: 'task:done',\n          workerId,\n          taskIndex: index,\n          taskLabel: label,\n          durationMs: Date.now() - taskStart,\n          success: true,\n          activeTasks,\n        });\n\n        const result: TaskResult<T> = { index, success: true, value };\n        results[index] = result;\n        onComplete?.(result);\n      } catch (err) {\n        activeTasks--;\n        tasksExecuted++;\n\n        const error = err instanceof Error ? err : new Error(String(err));\n\n        tracer?.emit({\n          type: 'task:done',\n          workerId,\n          taskIndex: index,\n          taskLabel: label,\n          durationMs: Date.now() - taskStart,\n          success: false,\n          error: error.message,\n          activeTasks,\n        });\n\n        const result: TaskResult<T> = { index, success: false, error };\n        results[index] = result;\n        onComplete?.(result);\n\n        if (options.failFast) {\n          aborted = true;\n          break;\n        }\n      }\n    }\n\n    tracer?.emit({ type: 'worker:end', workerId, phase, tasksExecuted });\n  }\n\n  // Spawn workers, capped at the number of available tasks\n  const effectiveConcurrency = Math.min(options.concurrency, tasks.length);\n  const iterator = tasks.entries();\n  const workers = Array.from({ length: effectiveConcurrency }, (_, workerId) =>\n    worker(iterator, workerId),\n  );\n\n  await Promise.allSettled(workers);\n\n  return results;\n}\n\n```\n\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n- Workflow & convention rules: if this file defines contribution guidelines, PR conventions, commit standards, testing mandates, tool usage requirements, approval workflows, AI agent behavioral instructions, or code conventions (naming standards, formatting rules, import ordering, linting policies), extract them as explicit, actionable rules. Sources: CONTRIBUTING.md, CI configs, PR templates, style guides, linter/formatter configs (.eslintrc, .prettierrc, .editorconfig), tsconfig strictness, README dev sections.\n\n\nTARGET LENGTH (MANDATORY):\n- Source file: 5458 characters\n- Target summary: ~546 characters (10% compression)\n- Maximum: 655 characters\n- Achieve this by: ultra-dense writing, minimal examples, single-sentence descriptions\n\nULTRA-COMPRESSION TECHNIQUES:\n- Use telegraphic style: \"exports foo, bar, baz\" not \"this file exports three functions\"\n- No code examples unless critical to understanding\n- One sentence per export maximum\n- Omit obvious parameter names (\"opts: Options\" not \"options: Options - configuration object\")\n- Use abbreviations: \"params\" not \"parameters\", \"config\" not \"configuration\"\n- Skip section if empty (don't write \"No special concerns\" or \"N/A\")\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nWORKFLOW & CONVENTION RULES (NEVER EXCLUDE):\n- Contribution requirements: branch naming, commit message formats, PR description templates\n- Testing mandates: required test types, coverage thresholds, \"never delete tests\" policies\n- Tool usage rules: required CLI commands, browser automation tools, linter/formatter requirements\n- Approval workflows: review requirements, CI checks that must pass, deployment gates\n- AI agent instructions: explicit directives for automated assistants (coding rules, prohibited actions)\n- Code conventions: naming standards (camelCase, PascalCase, snake_case for what), import ordering rules, file/folder naming patterns, formatting rules (indentation, quotes, semicolons), language idioms, TypeScript strictness policies, preferred patterns vs anti-patterns\nThese define how contributors (human or AI) must work within the project.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion .annex.sum file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**pool.ts exports a shared-iterator-based concurrency limiter (`runPool`, `PoolOptions`, `TaskResult`) that executes async task factories with bounded parallelism without batching idle periods.**\n\n## Exports\n\n`runPool<T>(tasks: Array<() => Promise<T>>, options: PoolOptions, onComplete?: (result: TaskResult<T>) => void): Promise<TaskResult<T>[]>` — executes task array through concurrency-limited pool using shared iterator; workers pull from same iterator so each task runs exactly once; returns sparse array indexed by original task position.\n\n`PoolOptions` — interface: `concurrency: number` (max concurrent workers), `failFast?: boolean` (abort on first error), `tracer?: ITraceWriter`, `phaseLabel?: string`, `taskLabels?: string[]`.\n\n`TaskResult<T>` — interface: `index: number`, `success: boolean`, `value?: T`, `error?: Error`.\n\n## Pattern & Architecture\n\nShared-iterator pattern: all workers iterate `tasks.entries()` concurrently, each pulling one `[index, task]` pair. Eliminates batch anti-pattern (Promise.all chunks) by keeping worker slots continuously filled as tasks complete. Requires zero external dependencies.\n\n## Concurrency Control\n\nEffective concurrency = `Math.min(options.concurrency, tasks.length)`. Abort flag `aborted` stops new task pulls on `failFast: true` or completion. `activeTasks` counter tracks in-flight work for tracing snapshots.\n\n## Error Handling\n\nCatch block normalizes non-Error throws via `err instanceof Error ? err : new Error(String(err))`. Result array indexed by task position remains sparse if aborted (results[index] assigned only on completion).\n\n## Tracing Events\n\nTracer emits: `worker:start` (workerId, phase), `task:pickup` (workerId, taskIndex, taskLabel, activeTasks), `task:done` (workerId, taskIndex, taskLabel, durationMs, success, error message, activeTasks), `worker:end` (workerId, phase, tasksExecuted). Task labels default to `task-${index}` if `taskLabels` not provided.",
      "model": "claude-haiku-4-5-20251001",
      "inputTokens": 3,
      "outputTokens": 515,
      "cacheReadTokens": 16147,
      "cacheCreationTokens": 4240,
      "latencyMs": 23562,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/orchestration/pool.ts",
          "sizeBytes": 5458
        }
      ]
    },
    {
      "timestamp": "2026-02-14T13:47:32.851Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: src/integration/templates.ts\n\n```typescript\n/**\n * Template generators for AI coding assistant integration files\n *\n * Generates command file templates for Claude Code, Codex, OpenCode, and Gemini CLI.\n */\n\nimport type { IntegrationTemplate } from './types.js';\n\n// =============================================================================\n// Shared Command Content\n// =============================================================================\n\nconst COMMANDS = {\n  generate: {\n    description: 'Generate AI-friendly documentation for the entire codebase',\n    argumentHint: '[path] [--dry-run] [--eval] [--concurrency N] [--fail-fast] [--debug] [--trace]',\n    content: `Generate comprehensive documentation for this codebase using agents-reverse-engineer.\n\n<execution>\nRun the generate command in the background and monitor progress in real time.\n\n## Steps\n\n1. **Display version**: Read \\`VERSION_FILE_PATH\\` and show the user: \\`agents-reverse-engineer vX.Y.Z\\`\n\n2. **Delete stale progress log** (prevents reading leftover data from a previous run):\n   \\`\\`\\`bash\n   rm -f .agents-reverse-engineer/progress.log\n   \\`\\`\\`\n\n3. **Run the generate command in the background** using \\`run_in_background: true\\`:\n   \\`\\`\\`bash\n   npx are generate BACKEND_FLAG $ARGUMENTS\n   \\`\\`\\`\n\n4. **Monitor progress by polling** \\`.agents-reverse-engineer/progress.log\\`:\n   - Wait ~15 seconds (use \\`sleep 15\\` in Bash), then use the **Read** tool to read \\`.agents-reverse-engineer/progress.log\\` (use the \\`offset\\` parameter to read only the last ~20 lines for long files)\n   - Show the user a brief progress update (e.g. \"32/96 files analyzed, ~12m remaining\")\n   - Check whether the background task has completed using \\`TaskOutput\\` with \\`block: false\\`\n   - Repeat until the background task finishes\n   - **Important**: Keep polling even if progress.log doesn't exist yet (the command takes a few seconds to start writing)\n\n5. **On completion**, read the full background task output and summarize:\n   - Number of files analyzed and any failures\n   - Number of directories documented\n   - Any inconsistency warnings from the quality report\n\nThis executes a two-phase pipeline:\n\n1. **File Analysis** (concurrent): Discovers files, applies filters, then analyzes each source file via AI and writes \\`.sum\\` summary files with YAML frontmatter (\\`content_hash\\`, \\`file_type\\`, \\`purpose\\`, \\`public_interface\\`, \\`dependencies\\`, \\`patterns\\`).\n\n2. **Directory Aggregation** (sequential): Generates \\`AGENTS.md\\` per directory in post-order traversal (deepest first, so child summaries feed into parents), and writes \\`CLAUDE.md\\` pointers.\n\n**Options:**\n- \\`--dry-run\\`: Preview the plan without making AI calls\n- \\`--eval\\`: Namespace output by backend.model for side-by-side comparison (e.g., \\`file.ts.claude.haiku.sum\\`, \\`AGENTS.claude.haiku.md\\`)\n- \\`--concurrency N\\`: Control number of parallel AI calls (default: auto)\n- \\`--fail-fast\\`: Stop on first file analysis failure\n- \\`--debug\\`: Show AI prompts and backend details\n- \\`--trace\\`: Enable concurrency tracing to \\`.agents-reverse-engineer/traces/\\`\n</execution>`,\n  },\n\n  update: {\n    description: 'Incrementally update documentation for changed files',\n    argumentHint: '[path] [--uncommitted] [--dry-run] [--eval] [--concurrency N] [--fail-fast] [--debug] [--trace]',\n    content: `Update documentation for files that changed since last run.\n\n<execution>\nRun the update command in the background and monitor progress in real time.\n\n## Steps\n\n1. **Display version**: Read \\`VERSION_FILE_PATH\\` and show the user: \\`agents-reverse-engineer vX.Y.Z\\`\n\n2. **Delete stale progress log** (prevents reading leftover data from a previous run):\n   \\`\\`\\`bash\n   rm -f .agents-reverse-engineer/progress.log\n   \\`\\`\\`\n\n3. **Run the update command in the background** using \\`run_in_background: true\\`:\n   \\`\\`\\`bash\n   npx are update BACKEND_FLAG $ARGUMENTS\n   \\`\\`\\`\n\n4. **Monitor progress by polling** \\`.agents-reverse-engineer/progress.log\\`:\n   - Wait ~15 seconds (use \\`sleep 15\\` in Bash), then use the **Read** tool to read \\`.agents-reverse-engineer/progress.log\\` (use the \\`offset\\` parameter to read only the last ~20 lines for long files)\n   - Show the user a brief progress update (e.g. \"12/30 files updated, ~5m remaining\")\n   - Check whether the background task has completed using \\`TaskOutput\\` with \\`block: false\\`\n   - Repeat until the background task finishes\n   - **Important**: Keep polling even if progress.log doesn't exist yet (the command takes a few seconds to start writing)\n\n5. **On completion**, read the full background task output and summarize:\n   - Files updated\n   - Files unchanged\n   - Any orphaned docs cleaned up\n\n**Options:**\n- \\`--uncommitted\\`: Include staged but uncommitted changes\n- \\`--dry-run\\`: Show what would be updated without writing\n- \\`--eval\\`: Namespace output by backend.model for side-by-side comparison\n- \\`--concurrency N\\`: Control number of parallel AI calls (default: auto)\n- \\`--fail-fast\\`: Stop on first file analysis failure\n- \\`--debug\\`: Show AI prompts and backend details\n- \\`--trace\\`: Enable concurrency tracing to \\`.agents-reverse-engineer/traces/\\`\n</execution>`,\n  },\n\n  init: {\n    description: 'Initialize agents-reverse-engineer configuration',\n    argumentHint: '',\n    content: `Initialize agents-reverse-engineer configuration in this project.\n\n<execution>\n1. **Display version**: Read \\`VERSION_FILE_PATH\\` and show the user: \\`agents-reverse-engineer vX.Y.Z\\`\n\n2. Run the agents-reverse-engineer init command:\n\n\\`\\`\\`bash\nnpx are init\n\\`\\`\\`\n\nThis creates \\`.agents-reverse-engineer/config.yaml\\` configuration file.\n</execution>`,\n  },\n\n  discover: {\n    description: 'Discover files in codebase',\n    argumentHint: '[path] [--debug] [--trace]',\n    content: `List files that would be analyzed for documentation.\n\n<execution>\n## STRICT RULES - VIOLATION IS FORBIDDEN\n\n1. Run ONLY this exact command: \\`npx are discover $ARGUMENTS\\`\n2. DO NOT add ANY flags the user did not explicitly type\n3. If user typed nothing after \\`COMMAND_PREFIXdiscover\\`, run with ZERO flags\n\n## Steps\n\n1. **Display version**: Read \\`VERSION_FILE_PATH\\` and show the user: \\`agents-reverse-engineer vX.Y.Z\\`\n\n2. **Delete stale progress log** (prevents reading leftover data from a previous run):\n   \\`\\`\\`bash\n   rm -f .agents-reverse-engineer/progress.log\n   \\`\\`\\`\n\n3. **Run the discover command in the background** using \\`run_in_background: true\\`:\n   \\`\\`\\`bash\n   npx are discover $ARGUMENTS\n   \\`\\`\\`\n\n4. **Monitor progress by polling** \\`.agents-reverse-engineer/progress.log\\`:\n   - Wait ~10 seconds (use \\`sleep 10\\` in Bash), then use the **Read** tool to read \\`.agents-reverse-engineer/progress.log\\` (use the \\`offset\\` parameter to read only the last ~20 lines for long files)\n   - Show the user a brief progress update\n   - Check whether the background task has completed using \\`TaskOutput\\` with \\`block: false\\`\n   - Repeat until the background task finishes\n   - **Important**: Keep polling even if progress.log doesn't exist yet (the command takes a few seconds to start writing)\n\n5. **On completion**, read the full background task output and report number of files found.\n\n6. **Review plan and suggest exclusions**:\n   - Read \\`.agents-reverse-engineer/GENERATION-PLAN.md\\` and \\`.agents-reverse-engineer/config.yaml\\`\n   - Scan the Phase 1 file list and classify files into these categories:\n     - **Test/spec files**: matches like \\`*.test.*\\`, \\`*.spec.*\\`, \\`__tests__/**\\`, \\`__mocks__/**\\`, \\`*.stories.*\\`, \\`*.story.*\\`\n     - **CI/CD configs**: \\`.github/workflows/*.yml\\`, \\`.gitlab-ci.yml\\`, \\`Jenkinsfile\\`, \\`.circleci/**\\`, \\`.travis.yml\\`\n     - **Tool configs**: \\`.eslintrc*\\`, \\`.prettierrc*\\`, \\`jest.config.*\\`, \\`.editorconfig\\`, \\`babel.config.*\\`, \\`webpack.config.*\\`, \\`vite.config.*\\`, \\`rollup.config.*\\`, \\`tsconfig*.json\\`, \\`.lintstagedrc*\\`, \\`.huskyrc*\\`, \\`.stylelintrc*\\`, \\`commitlint.config.*\\`\n     - **Migration files**: paths containing \\`migrations/\\` or matching \\`*.migration.*\\`\n     - **Fixture/snapshot files**: \\`__snapshots__/**\\`, \\`*.fixture.*\\`, \\`fixtures/**\\`, \\`test-data/**\\`, \\`testdata/**\\`\n     - **Type declarations**: \\`*.d.ts\\` (not source code, auto-generated or ambient)\n     - **Docker/infra**: \\`Dockerfile*\\`, \\`docker-compose*\\`, \\`*.Dockerfile\\`, \\`k8s/**\\`, \\`terraform/**\\`, \\`helm/**\\`\n   - For each category, count how many files from the plan match and list up to 3 example filenames\n   - **Skip categories with zero matches** — only present categories that have files in the plan\n   - Also skip patterns that are already in the current \\`config.yaml\\` exclude list\n   - Present the findings in a summary table showing category, file count, example files, and proposed glob patterns\n   - Ask the user which categories to exclude using \\`AskUserQuestion\\` with \\`multiSelect: true\\`\n   - For accepted categories, use the **Edit** tool to append the corresponding glob patterns to the \\`exclude.patterns\\` array in \\`.agents-reverse-engineer/config.yaml\\`\n   - After editing, briefly confirm what was added\n</execution>`,\n  },\n\n  clean: {\n    description: 'Delete all generated documentation artifacts (.sum, AGENTS.md, plan)',\n    argumentHint: '[path] [--dry-run]',\n    content: `Remove all documentation artifacts generated by agents-reverse-engineer.\n\n<execution>\n## STRICT RULES - VIOLATION IS FORBIDDEN\n\n1. Run ONLY this exact command: \\`npx are clean $ARGUMENTS\\`\n2. DO NOT add ANY flags the user did not explicitly type\n3. If user typed nothing after \\`COMMAND_PREFIXclean\\`, run with ZERO flags\n\n**Display version**: Read \\`VERSION_FILE_PATH\\` and show the user: \\`agents-reverse-engineer vX.Y.Z\\`\n\n\\`\\`\\`bash\nnpx are clean $ARGUMENTS\n\\`\\`\\`\n\nReport number of files deleted.\n</execution>`,\n  },\n\n  specify: {\n    description: 'Generate project specification from AGENTS.md docs',\n    argumentHint: '[path] [--dry-run] [--output <path>] [--multi-file] [--force] [--debug] [--trace]',\n    content: `Generate a project specification from existing AGENTS.md documentation.\n\n<execution>\nRun the specify command in the background and monitor progress in real time.\n\n## Steps\n\n1. **Display version**: Read \\`VERSION_FILE_PATH\\` and show the user: \\`agents-reverse-engineer vX.Y.Z\\`\n\n2. **Delete stale progress log** (prevents reading leftover data from a previous run):\n   \\`\\`\\`bash\n   rm -f .agents-reverse-engineer/progress.log\n   \\`\\`\\`\n\n3. **Run the specify command in the background** using \\`run_in_background: true\\`:\n   \\`\\`\\`bash\n   npx are specify BACKEND_FLAG $ARGUMENTS\n   \\`\\`\\`\n\n4. **Monitor progress by polling** \\`.agents-reverse-engineer/progress.log\\`:\n   - Wait ~15 seconds (use \\`sleep 15\\` in Bash), then use the **Read** tool to read \\`.agents-reverse-engineer/progress.log\\` (use the \\`offset\\` parameter to read only the last ~20 lines for long files)\n   - Show the user a brief progress update\n   - Check whether the background task has completed using \\`TaskOutput\\` with \\`block: false\\`\n   - Repeat until the background task finishes\n   - **Important**: Keep polling even if progress.log doesn't exist yet (the command takes a few seconds to start writing)\n\n5. **On completion**, read the full background task output and summarize:\n   - Number of AGENTS.md files collected\n   - Output file(s) written\n\nThis collects all AGENTS.md files, synthesizes them via AI, and writes a comprehensive project specification.\n\nIf no AGENTS.md files exist, it will auto-run \\`generate\\` first.\n\n**Options:**\n- \\`--dry-run\\`: Show input statistics without making AI calls\n- \\`--output <path>\\`: Custom output path (default: specs/SPEC.md)\n- \\`--multi-file\\`: Split specification into multiple files\n- \\`--force\\`: Overwrite existing specification\n- \\`--debug\\`: Show AI prompts and backend details\n- \\`--trace\\`: Enable concurrency tracing to \\`.agents-reverse-engineer/traces/\\`\n</execution>`,\n  },\n\n  rebuild: {\n    description: 'Reconstruct project from specification documents',\n    argumentHint: '[path] [--dry-run] [--output <path>] [--force] [--concurrency N] [--fail-fast] [--debug] [--trace]',\n    content: `Reconstruct a project from specification documents using agents-reverse-engineer.\n\n<execution>\nRun the rebuild command in the background and monitor progress in real time.\n\n## Steps\n\n1. **Display version**: Read \\`VERSION_FILE_PATH\\` and show the user: \\`agents-reverse-engineer vX.Y.Z\\`\n\n2. **Delete stale progress log** (prevents reading leftover data from a previous run):\n   \\`\\`\\`bash\n   rm -f .agents-reverse-engineer/progress.log\n   \\`\\`\\`\n\n3. **Run the rebuild command in the background** using \\`run_in_background: true\\`:\n   \\`\\`\\`bash\n   npx are rebuild BACKEND_FLAG $ARGUMENTS\n   \\`\\`\\`\n\n4. **Monitor progress by polling** \\`.agents-reverse-engineer/progress.log\\`:\n   - Wait ~15 seconds (use \\`sleep 15\\` in Bash), then use the **Read** tool to read \\`.agents-reverse-engineer/progress.log\\` (use the \\`offset\\` parameter to read only the last ~20 lines for long files)\n   - Show the user a brief progress update (e.g. \"4/12 rebuild units completed\")\n   - Check whether the background task has completed using \\`TaskOutput\\` with \\`block: false\\`\n   - Repeat until the background task finishes\n   - **Important**: Keep polling even if progress.log doesn't exist yet (the command takes a few seconds to start writing)\n\n5. **On completion**, read the full background task output and summarize:\n   - Number of rebuild units processed\n   - Files generated and output directory\n   - Any failures or partial completions\n\nThis reads spec files from \\`specs/\\`, partitions them into ordered rebuild units, and processes each via AI to generate source files.\n\n**Options:**\n- \\`--dry-run\\`: Show rebuild plan without making AI calls\n- \\`--output <path>\\`: Output directory (default: rebuild/)\n- \\`--force\\`: Wipe output directory and start fresh\n- \\`--concurrency N\\`: Control number of parallel AI calls (default: auto)\n- \\`--fail-fast\\`: Stop on first failure\n- \\`--debug\\`: Show AI prompts and backend details\n- \\`--trace\\`: Enable concurrency tracing to \\`.agents-reverse-engineer/traces/\\`\n\n**Exit codes:** 0 (success), 1 (partial failure), 2 (total failure)\n</execution>`,\n  },\n\n  help: {\n    description: 'Show available ARE commands and usage guide',\n    argumentHint: '',\n    // Content uses COMMAND_PREFIX placeholder, replaced per platform\n    content: `<objective>\nDisplay the complete ARE command reference.\n\n**First**: Read \\`VERSION_FILE_PATH\\` and show the user the version: \\`agents-reverse-engineer vX.Y.Z\\`\n\n**Then**: Output ONLY the reference content below. Do NOT add:\n- Project-specific analysis\n- Git status or file context\n- Next-step suggestions\n- Any commentary beyond the reference\n</objective>\n\n<reference>\n# agents-reverse-engineer (ARE) Command Reference\n\n**ARE** generates AI-friendly documentation for codebases, creating structured summaries optimized for AI assistants.\n\n## Quick Start\n\n1. \\`COMMAND_PREFIXinit\\` — Create configuration file\n2. \\`COMMAND_PREFIXgenerate\\` — Generate documentation for the codebase\n3. \\`COMMAND_PREFIXupdate\\` — Keep docs in sync after code changes\n\n## Commands Reference\n\n### \\`COMMAND_PREFIXinit\\`\nInitialize configuration in this project.\n\nCreates \\`.agents-reverse-engineer/config.yaml\\` with customizable settings.\n\n**Usage:** \\`COMMAND_PREFIXinit\\`\n**CLI:** \\`npx are init\\`\n\n---\n\n### \\`COMMAND_PREFIXdiscover\\`\nDiscover files that would be analyzed for documentation.\n\nShows included files, excluded files with reasons, and generates a \\`GENERATION-PLAN.md\\` execution plan.\n\n**Options:**\n| Flag | Description |\n|------|-------------|\n| \\`[path]\\` | Target directory (default: current directory) |\n| \\`--debug\\` | Show verbose debug output |\n| \\`--trace\\` | Enable concurrency tracing to \\`.agents-reverse-engineer/traces/\\` |\n**Usage:**\n- \\`COMMAND_PREFIXdiscover\\` — Discover files and generate execution plan\n\n**CLI:**\n\\`\\`\\`bash\nnpx are discover\nnpx are discover ./src\n\\`\\`\\`\n\n---\n\n### \\`COMMAND_PREFIXgenerate\\`\nGenerate comprehensive documentation for the codebase.\n\n**Options:**\n| Flag | Description |\n|------|-------------|\n| \\`[path]\\` | Target directory (default: current directory) |\n| \\`--concurrency N\\` | Number of concurrent AI calls (default: auto) |\n| \\`--dry-run\\` | Show what would be generated without writing |\n| \\`--eval\\` | Namespace output by backend.model for comparison |\n| \\`--fail-fast\\` | Stop on first file analysis failure |\n| \\`--debug\\` | Show AI prompts and backend details |\n| \\`--trace\\` | Enable concurrency tracing to \\`.agents-reverse-engineer/traces/\\` |\n**Usage:**\n- \\`COMMAND_PREFIXgenerate\\` — Generate docs\n- \\`COMMAND_PREFIXgenerate --dry-run\\` — Preview without writing\n- \\`COMMAND_PREFIXgenerate --eval\\` — Generate variant docs for comparison\n- \\`COMMAND_PREFIXgenerate --concurrency 3\\` — Limit parallel AI calls\n\n**CLI:**\n\\`\\`\\`bash\nnpx are generate\nnpx are generate --dry-run\nnpx are generate ./my-project --concurrency 3\nnpx are generate --debug --trace\n\\`\\`\\`\n\n**How it works:**\n1. Discovers files, applies filters, analyzes each file via concurrent AI calls, writes \\`.sum\\` summary files\n2. Generates \\`AGENTS.md\\` for each directory (post-order traversal) and writes \\`CLAUDE.md\\` pointers\n\n---\n\n### \\`COMMAND_PREFIXupdate\\`\nIncrementally update documentation for changed files.\n\n**Options:**\n| Flag | Description |\n|------|-------------|\n| \\`[path]\\` | Target directory (default: current directory) |\n| \\`--uncommitted\\` | Include staged but uncommitted changes |\n| \\`--dry-run\\` | Show what would be updated without writing |\n| \\`--eval\\` | Namespace output by backend.model for comparison |\n| \\`--concurrency N\\` | Number of concurrent AI calls (default: auto) |\n| \\`--fail-fast\\` | Stop on first file analysis failure |\n| \\`--debug\\` | Show AI prompts and backend details |\n| \\`--trace\\` | Enable concurrency tracing to \\`.agents-reverse-engineer/traces/\\` |\n**Usage:**\n- \\`COMMAND_PREFIXupdate\\` — Update docs for committed changes\n- \\`COMMAND_PREFIXupdate --uncommitted\\` — Include uncommitted changes\n\n**CLI:**\n\\`\\`\\`bash\nnpx are update\nnpx are update --uncommitted\nnpx are update --dry-run\nnpx are update ./my-project --concurrency 3\n\\`\\`\\`\n\n---\n\n### \\`COMMAND_PREFIXspecify\\`\nGenerate a project specification from AGENTS.md documentation.\n\nCollects all AGENTS.md files, synthesizes them via AI, and writes a comprehensive project specification. Auto-runs \\`generate\\` if no AGENTS.md files exist.\n\n**Options:**\n| Flag | Description |\n|------|-------------|\n| \\`[path]\\` | Target directory (default: current directory) |\n| \\`--output <path>\\` | Custom output path (default: specs/SPEC.md) |\n| \\`--multi-file\\` | Split specification into multiple files |\n| \\`--force\\` | Overwrite existing specification |\n| \\`--dry-run\\` | Show input statistics without making AI calls |\n| \\`--debug\\` | Show AI prompts and backend details |\n| \\`--trace\\` | Enable concurrency tracing to \\`.agents-reverse-engineer/traces/\\` |\n**Usage:**\n- \\`COMMAND_PREFIXspecify\\` — Generate specification\n- \\`COMMAND_PREFIXspecify --dry-run\\` — Preview without calling AI\n- \\`COMMAND_PREFIXspecify --output ./docs/spec.md --force\\` — Custom output path\n\n**CLI:**\n\\`\\`\\`bash\nnpx are specify\nnpx are specify --dry-run\nnpx are specify --output ./docs/spec.md --force\nnpx are specify --multi-file\n\\`\\`\\`\n\n---\n\n### \\`COMMAND_PREFIXrebuild\\`\nReconstruct a project from specification documents.\n\nReads spec files from \\`specs/\\`, partitions them into ordered rebuild units, processes each via AI, and writes generated source files to an output directory. Supports checkpoint-based session continuity for resumable long-running rebuilds.\n\n**Options:**\n| Flag | Description |\n|------|-------------|\n| \\`[path]\\` | Target directory (default: current directory) |\n| \\`--output <path>\\` | Output directory (default: rebuild/) |\n| \\`--force\\` | Wipe output directory and start fresh |\n| \\`--dry-run\\` | Show rebuild plan without making AI calls |\n| \\`--concurrency N\\` | Number of concurrent AI calls (default: auto) |\n| \\`--fail-fast\\` | Stop on first failure |\n| \\`--debug\\` | Show AI prompts and backend details |\n| \\`--trace\\` | Enable concurrency tracing to \\`.agents-reverse-engineer/traces/\\` |\n**Usage:**\n- \\`COMMAND_PREFIXrebuild --dry-run\\` — Preview rebuild plan\n- \\`COMMAND_PREFIXrebuild --output ./out --force\\` — Rebuild to custom directory\n\n**CLI:**\n\\`\\`\\`bash\nnpx are rebuild --dry-run\nnpx are rebuild --output ./out --force\nnpx are rebuild --concurrency 3\n\\`\\`\\`\n\n**How it works:**\n1. Reads all spec files from \\`specs/\\` directory\n2. Partitions specs into ordered rebuild units (from Build Plan phases or top-level headings)\n3. Processes units in order: sequentially between groups, concurrently within each group\n4. Accumulates context (export signatures) after each group for dependent phases\n5. Writes generated source files via \\`===FILE:===\\` delimited output parsing\n\n**Exit codes:** 0 (success), 1 (partial failure), 2 (total failure)\n\n---\n\n### \\`COMMAND_PREFIXclean\\`\nRemove all generated documentation artifacts.\n\n**Options:**\n| Flag | Description |\n|------|-------------|\n| \\`--dry-run\\` | Show what would be deleted without deleting |\n\n**What gets deleted:**\n- \\`.agents-reverse-engineer/GENERATION-PLAN.md\\`\n- All \\`*.sum\\` files (including eval variant \\`.sum\\` files)\n- All \\`AGENTS.md\\` and \\`AGENTS.*.md\\` variant files\n- Pointers: \\`CLAUDE.md\\`\n\n**Usage:**\n- \\`COMMAND_PREFIXclean --dry-run\\` — Preview deletions\n- \\`COMMAND_PREFIXclean\\` — Delete all artifacts\n\n**CLI:**\n\\`\\`\\`bash\nnpx are clean --dry-run\nnpx are clean\n\\`\\`\\`\n\n---\n\n### \\`COMMAND_PREFIXhelp\\`\nShow this command reference.\n\n## CLI Installation\n\nInstall ARE commands to your AI assistant:\n\n\\`\\`\\`bash\nnpx agents-reverse-engineer install              # Interactive mode\nnpx agents-reverse-engineer install --runtime claude -g  # Global Claude\nnpx agents-reverse-engineer install --runtime codex -g   # Global Codex\nnpx agents-reverse-engineer install --runtime claude -l  # Local project\nnpx agents-reverse-engineer install --runtime all -g     # All runtimes\n\\`\\`\\`\n\n**Install/Uninstall Options:**\n| Flag | Description |\n|------|-------------|\n| \\`--runtime <name>\\` | Target: \\`claude\\`, \\`codex\\`, \\`opencode\\`, \\`gemini\\`, \\`all\\` |\n| \\`-g, --global\\` | Install to global config directory |\n| \\`-l, --local\\` | Install to current project directory |\n| \\`--force\\` | Overwrite existing files (install only) |\n\n## Configuration\n\n**File:** \\`.agents-reverse-engineer/config.yaml\\`\n\n\\`\\`\\`yaml\n# Exclusion patterns\nexclude:\n  patterns:\n    - \"**/*.test.ts\"\n    - \"**/__mocks__/**\"\n  vendorDirs:\n    - node_modules\n    - dist\n    - .git\n  binaryExtensions:\n    - .png\n    - .jpg\n    - .pdf\n\n# Options\noptions:\n  followSymlinks: false\n  maxFileSize: 100000\n\n# Output settings\noutput:\n  colors: true\n\\`\\`\\`\n\n## Generated Files\n\n### Per Source File\n\n**\\`*.sum\\`** — File summaries with YAML frontmatter + detailed prose.\n\n\\`\\`\\`yaml\n---\nfile_type: service\ngenerated_at: 2025-01-15T10:30:00Z\ncontent_hash: abc123...\npurpose: Handles user authentication and session management\npublic_interface: [login(), logout(), refreshToken(), AuthService]\ndependencies: [express, jsonwebtoken, ./user-model]\npatterns: [singleton, factory, observer]\nrelated_files: [./types.ts, ./middleware.ts]\n---\n\n<300-500 word summary covering implementation, patterns, edge cases>\n\\`\\`\\`\n\n### Per Directory\n\n**\\`AGENTS.md\\`** — Directory overview synthesized from \\`.sum\\` files. Groups files by purpose and links to subdirectories.\n\n### Pointer Files\n\n| File | Purpose |\n|------|---------|\n| \\`CLAUDE.md\\` | Project entry point — imports root AGENTS.md |\n\n## Common Workflows\n\n**Initial documentation:**\n\\`\\`\\`\nCOMMAND_PREFIXinit\nCOMMAND_PREFIXgenerate\n\\`\\`\\`\n\n**After code changes:**\n\\`\\`\\`\nCOMMAND_PREFIXupdate\n\\`\\`\\`\n\n**Full regeneration:**\n\\`\\`\\`\nCOMMAND_PREFIXclean\nCOMMAND_PREFIXgenerate\n\\`\\`\\`\n\n**Preview before generating:**\n\\`\\`\\`\nCOMMAND_PREFIXdiscover                   # Check files and exclusions\nCOMMAND_PREFIXgenerate --dry-run         # Preview generation\n\\`\\`\\`\n\n## Tips\n\n- **Custom exclusions**: Edit \\`.agents-reverse-engineer/config.yaml\\` to skip files\n- **Context loading**: Install creates a hook that auto-loads AGENTS.md context when reading project files\n\n## Resources\n\n- **Repository:** https://github.com/GeoloeG-IsT/agents-reverse-engineer\n- **Update:** \\`npx are install --force\\`\n</reference>`,\n  },\n} as const;\n\n// =============================================================================\n// Platform-specific template generators\n// =============================================================================\n\ntype Platform = 'claude' | 'codex' | 'opencode' | 'gemini';\n\ninterface PlatformConfig {\n  commandPrefix: string; // /are- command prefix used in generated docs\n  pathPrefix: string; // .claude/skills/, .agents/skills/, .opencode/commands/, etc\n  filenameSeparator: string; // . or -\n  extraFrontmatter?: string; // e.g., \"agent: build\" for OpenCode\n  usesName: boolean; // Claude uses \"name:\" in frontmatter\n  versionFilePath: string; // .claude/ARE-VERSION, .agents/ARE-VERSION, etc.\n}\n\nconst PLATFORM_CONFIGS: Record<Platform, PlatformConfig> = {\n  claude: {\n    commandPrefix: '/are-',\n    pathPrefix: '.claude/skills/',\n    filenameSeparator: '.',\n    usesName: true,\n    versionFilePath: '.claude/ARE-VERSION',\n  },\n  codex: {\n    commandPrefix: '/are-',\n    pathPrefix: '.agents/skills/',\n    filenameSeparator: '.',\n    usesName: true,\n    versionFilePath: '.agents/ARE-VERSION',\n  },\n  opencode: {\n    commandPrefix: '/are-',\n    pathPrefix: '.opencode/commands/',\n    filenameSeparator: '-',\n    extraFrontmatter: 'agent: build',\n    usesName: false,\n    versionFilePath: '.opencode/ARE-VERSION',\n  },\n  gemini: {\n    commandPrefix: '/are-',\n    pathPrefix: '.gemini/commands/',\n    filenameSeparator: '-',\n    usesName: false,\n    versionFilePath: '.gemini/ARE-VERSION',\n  },\n};\n\nfunction buildFrontmatter(\n  platform: Platform,\n  commandName: string,\n  description: string,\n  argumentHint?: string,\n): string {\n  const config = PLATFORM_CONFIGS[platform];\n  const lines = ['---'];\n\n  if (config.usesName) {\n    lines.push(`name: are-${commandName}`);\n  }\n\n  lines.push(`description: ${description}`);\n\n  if (platform === 'codex' && argumentHint) {\n    lines.push(`argument-hint: ${JSON.stringify(argumentHint)}`);\n  }\n\n  if (config.extraFrontmatter) {\n    lines.push(config.extraFrontmatter);\n  }\n\n  lines.push('---');\n  return lines.join('\\n');\n}\n\n/**\n * Build TOML content for Gemini CLI commands\n *\n * Gemini uses TOML format with description and prompt fields.\n * See: https://geminicli.com/docs/cli/custom-commands/\n */\nfunction buildGeminiToml(\n  commandName: string,\n  command: (typeof COMMANDS)[keyof typeof COMMANDS]\n): string {\n  const config = PLATFORM_CONFIGS.gemini;\n  // Replace placeholders in content\n  const promptContent = command.content\n    .replace(/COMMAND_PREFIX/g, config.commandPrefix)\n    .replace(/VERSION_FILE_PATH/g, config.versionFilePath)\n    .replace(/BACKEND_FLAG/g, '--backend gemini');\n\n  // Build TOML content\n  // Use triple quotes for multi-line prompt\n  const lines = [`description = \"${command.description}\"`];\n\n  if (command.argumentHint) {\n    lines.push(`# Arguments: ${command.argumentHint}`);\n  }\n\n  lines.push(`prompt = \"\"\"`);\n  lines.push(promptContent);\n  lines.push(`\"\"\"`);\n\n  return lines.join('\\n');\n}\n\nfunction buildTemplate(\n  platform: Platform,\n  commandName: string,\n  command: (typeof COMMANDS)[keyof typeof COMMANDS]\n): IntegrationTemplate {\n  const config = PLATFORM_CONFIGS[platform];\n\n  // Platform-specific file naming:\n  // - Claude: .claude/skills/are-{command}/SKILL.md\n  // - Codex: .agents/skills/are-{command}/SKILL.md\n  // - OpenCode: .opencode/commands/are-{command}.md\n  // - Gemini: .gemini/commands/are-{command}.toml (TOML format)\n  if (platform === 'gemini') {\n    const filename = `are-${commandName}.toml`;\n    const path = `${config.pathPrefix}${filename}`;\n    const content = buildGeminiToml(commandName, command);\n\n    return {\n      filename,\n      path,\n      content: `${content}\\n`,\n    };\n  }\n\n  const usesSkillFile = platform === 'claude' || platform === 'codex';\n  const filename = usesSkillFile ? 'SKILL.md' : `are-${commandName}.md`;\n  const path =\n    usesSkillFile\n      ? `${config.pathPrefix}are-${commandName}/${filename}`\n      : `${config.pathPrefix}${filename}`;\n\n  const frontmatter = buildFrontmatter(\n    platform,\n    commandName,\n    command.description,\n    command.argumentHint,\n  );\n\n  // Replace placeholders in content\n  const content = command.content\n    .replace(/COMMAND_PREFIX/g, config.commandPrefix)\n    .replace(/VERSION_FILE_PATH/g, config.versionFilePath)\n    .replace(/BACKEND_FLAG/g, `--backend ${platform}`);\n\n  return {\n    filename,\n    path,\n    content: `${frontmatter}\\n\\n${content}\\n`,\n  };\n}\n\nfunction getTemplatesForPlatform(platform: Platform): IntegrationTemplate[] {\n  return Object.entries(COMMANDS).map(([name, command]) =>\n    buildTemplate(platform, name, command)\n  );\n}\n\n// =============================================================================\n// Public API\n// =============================================================================\n\n/**\n * Get Claude Code command file templates\n */\nexport function getClaudeTemplates(): IntegrationTemplate[] {\n  return getTemplatesForPlatform('claude');\n}\n\n/**\n * Get Codex command file templates\n */\nexport function getCodexTemplates(): IntegrationTemplate[] {\n  return getTemplatesForPlatform('codex');\n}\n\n/**\n * Get OpenCode command file templates\n */\nexport function getOpenCodeTemplates(): IntegrationTemplate[] {\n  return getTemplatesForPlatform('opencode');\n}\n\n/**\n * Get Gemini CLI command file templates\n */\nexport function getGeminiTemplates(): IntegrationTemplate[] {\n  return getTemplatesForPlatform('gemini');\n}\n\n```\n\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n- Workflow & convention rules: if this file defines contribution guidelines, PR conventions, commit standards, testing mandates, tool usage requirements, approval workflows, AI agent behavioral instructions, or code conventions (naming standards, formatting rules, import ordering, linting policies), extract them as explicit, actionable rules. Sources: CONTRIBUTING.md, CI configs, PR templates, style guides, linter/formatter configs (.eslintrc, .prettierrc, .editorconfig), tsconfig strictness, README dev sections.\n\n\nTARGET LENGTH (MANDATORY):\n- Source file: 29841 characters\n- Target summary: ~2984 characters (10% compression)\n- Maximum: 3581 characters\n- Achieve this by: ultra-dense writing, minimal examples, single-sentence descriptions\n\nULTRA-COMPRESSION TECHNIQUES:\n- Use telegraphic style: \"exports foo, bar, baz\" not \"this file exports three functions\"\n- No code examples unless critical to understanding\n- One sentence per export maximum\n- Omit obvious parameter names (\"opts: Options\" not \"options: Options - configuration object\")\n- Use abbreviations: \"params\" not \"parameters\", \"config\" not \"configuration\"\n- Skip section if empty (don't write \"No special concerns\" or \"N/A\")\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nWORKFLOW & CONVENTION RULES (NEVER EXCLUDE):\n- Contribution requirements: branch naming, commit message formats, PR description templates\n- Testing mandates: required test types, coverage thresholds, \"never delete tests\" policies\n- Tool usage rules: required CLI commands, browser automation tools, linter/formatter requirements\n- Approval workflows: review requirements, CI checks that must pass, deployment gates\n- AI agent instructions: explicit directives for automated assistants (coding rules, prohibited actions)\n- Code conventions: naming standards (camelCase, PascalCase, snake_case for what), import ordering rules, file/folder naming patterns, formatting rules (indentation, quotes, semicolons), language idioms, TypeScript strictness policies, preferred patterns vs anti-patterns\nThese define how contributors (human or AI) must work within the project.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion .annex.sum file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**src/integration/templates.ts generates command file templates for AI coding assistants (Claude Code, Codex, OpenCode, Gemini) implementing ARE (agents-reverse-engineer) CLI integration.**\n\n## Exports\n\n`getClaudeTemplates()` → `IntegrationTemplate[]` — Returns Claude Code `.claude/skills/are-{command}/SKILL.md` template files with markdown frontmatter (name, description, argument-hint).\n\n`getCodexTemplates()` → `IntegrationTemplate[]` — Returns Codex `.agents/skills/are-{command}/SKILL.md` template files with identical markdown structure.\n\n`getOpenCodeTemplates()` → `IntegrationTemplate[]` — Returns OpenCode `.opencode/commands/are-{command}.md` flat template files with markdown frontmatter (no skill subdirs, includes `agent: build` metadata).\n\n`getGeminiTemplates()` → `IntegrationTemplate[]` — Returns Gemini CLI `.gemini/commands/are-{command}.toml` files in TOML format (triple-quoted `prompt` field instead of markdown).\n\n## Command Registry & Content Generation\n\n`COMMANDS` object defines six command templates (generate, update, init, discover, clean, specify, rebuild, help), each with `description`, `argumentHint`, and `content` (instruction template with placeholder substitution). Content includes execution workflows: background process spawning, progress log polling (.agents-reverse-engineer/progress.log via offset reads), version display from VERSION_FILE_PATH, and task completion summarization.\n\n`buildFrontmatter(platform, commandName, description, argumentHint)` → string — Generates YAML frontmatter: `name: are-{command}` (Claude/Codex only), description, optional argument-hint (Codex only), optional extraFrontmatter (agent: build for OpenCode).\n\n`buildGeminiToml(commandName, command)` → string — Builds TOML syntax with description and triple-quoted prompt field; no YAML frontmatter.\n\n`buildTemplate(platform, commandName, command)` → `IntegrationTemplate` — Orchestrates platform-specific file generation: selects filename/path per config, assembles frontmatter (skipped for Gemini), replaces placeholders (`COMMAND_PREFIX`, `VERSION_FILE_PATH`, `BACKEND_FLAG` → platform-specific values), returns `{filename, path, content}`.\n\n## Platform Configuration\n\n`PLATFORM_CONFIGS: Record<Platform, PlatformConfig>` — Maps platform name to config: commandPrefix (`/are-` all platforms), pathPrefix (`.claude/skills/`, `.agents/skills/`, `.opencode/commands/`, `.gemini/commands/`), filenameSeparator (`.` for Claude/Codex, `-` for OpenCode/Gemini), usesName (true for Claude/Codex, false for OpenCode/Gemini), versionFilePath (`.claude/ARE-VERSION`, `.agents/ARE-VERSION`, `.opencode/ARE-VERSION`, `.gemini/ARE-VERSION`).\n\n## Placeholder Substitution Rules\n\nCommand content uses three placeholders replaced per platform:\n- `COMMAND_PREFIX` → `/are-` (all platforms)\n- `VERSION_FILE_PATH` → platform-specific path (e.g., `.claude/ARE-VERSION`)\n- `BACKEND_FLAG` → `--backend {claude|codex|opencode|gemini}`\n\n## Behavioral Contracts — ARE Command Execution Workflows\n\n### generate / update / specify / rebuild\n1. Display version: read VERSION_FILE_PATH, show \"agents-reverse-engineer vX.Y.Z\"\n2. Delete stale `.agents-reverse-engineer/progress.log` (prevents carryover from prior runs)\n3. Spawn background task: `npx are {command} BACKEND_FLAG $ARGUMENTS` with `run_in_background: true`\n4. Poll progress.log every 15 seconds: read with offset parameter (last ~20 lines), show brief update (e.g., \"32/96 files analyzed\"), check TaskOutput with `block: false`, continue until completion\n5. **Critical**: Keep polling even if progress.log doesn't exist yet (command takes seconds to start writing)\n6. On completion, summarize full background task output\n\n### discover\n**STRICT RULES — VIOLATION FORBIDDEN:**\n- Run ONLY `npx are discover $ARGUMENTS` — NO additional flags user didn't type\n- If user typed nothing after discover, run with ZERO flags\n- Polls progress.log (~10s wait), reads GENERATION-PLAN.md + config.yaml, classifies files by category (test/spec, CI/CD, tool configs, migrations, fixtures, type declarations, Docker), counts matches, suggests exclusions via AskUserQuestion with multiSelect: true, edits config.yaml with accepted glob patterns\n\n### clean\n**STRICT RULES — VIOLATION FORBIDDEN:**\n- Run ONLY `npx are clean $ARGUMENTS` — NO additional flags user didn't type\n- If user typed nothing, run with ZERO flags\n\n### discover file categorization rules\nMatch patterns: `*.test.*`, `*.spec.*`, `__tests__/**`, `__mocks__/**`, `*.stories.*` (test/spec); `.github/workflows/*.yml`, `.gitlab-ci.yml`, `Jenkinsfile`, `.circleci/**`, `.travis.yml` (CI/CD); `.eslintrc*`, `.prettierrc*`, `jest.config.*`, `.editorconfig`, `babel.config.*`, `webpack.config.*`, `vite.config.*`, `rollup.config.*`, `tsconfig*.json`, `.lintstagedrc*`, `.huskyrc*`, `.stylelintrc*`, `commitlint.config.*` (tool configs); `migrations/`, `*.migration.*` (migrations); `__snapshots__/**`, `*.fixture.*`, `fixtures/**`, `test-data/**`, `testdata/**` (fixtures); `*.d.ts` (type declarations); `Dockerfile*`, `docker-compose*`, `*.Dockerfile`, `k8s/**`, `terraform/**`, `helm/**` (Docker/infra). Present only categories with matches; skip if already in config.yaml exclude list.\n\n## File Structure Convention\n\n- Claude/Codex: hierarchical `.claude/skills/are-{command}/SKILL.md` (skill per subdirectory)\n- OpenCode/Gemini: flat command files (`.opencode/commands/are-{command}.md`, `.gemini/commands/are-{command}.toml`)",
      "model": "claude-haiku-4-5-20251001",
      "inputTokens": 3,
      "outputTokens": 1522,
      "cacheReadTokens": 16147,
      "cacheCreationTokens": 11730,
      "latencyMs": 33498,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/integration/templates.ts",
          "sizeBytes": 29883
        }
      ]
    },
    {
      "timestamp": "2026-02-14T13:47:40.939Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: src/orchestration/progress.ts\n\n```typescript\n/**\n * Streaming build-log progress reporter with ETA calculation.\n *\n * Outputs one line per event (start, done, fail, dir-done)\n * using `console.log` for atomic, non-corrupting concurrent output.\n * Each line shows progress counter, status, file path, timing, and\n * token counts using colored output via `picocolors`.\n *\n * ETA is computed via a moving average of the last 10 completion times,\n * displayed after 2 or more files have completed.\n *\n * Optionally mirrors all output to a plain-text progress log file\n * (`.agents-reverse-engineer/progress.log`) via {@link ProgressLog},\n * enabling `tail -f` monitoring when running inside buffered environments\n * (e.g. Claude Code's Bash tool).\n *\n * @module\n */\n\nimport { open, mkdir } from 'node:fs/promises';\nimport type { FileHandle } from 'node:fs/promises';\nimport * as path from 'node:path';\nimport pc from 'picocolors';\nimport type { RunSummary } from './types.js';\n\n// ---------------------------------------------------------------------------\n// ANSI stripping\n// ---------------------------------------------------------------------------\n\n/** Strip ANSI escape sequences from a string for plain-text output. */\nfunction stripAnsi(str: string): string {\n  // Matches all common ANSI escape codes (SGR, cursor, erase, etc.)\n  // eslint-disable-next-line no-control-regex\n  return str.replace(/\\x1b\\[[0-9;]*m/g, '');\n}\n\n// ---------------------------------------------------------------------------\n// ProgressLog\n// ---------------------------------------------------------------------------\n\n/** Relative path for the progress log file */\nconst PROGRESS_LOG_FILENAME = 'progress.log';\n\n/**\n * Plain-text progress log file writer.\n *\n * Mirrors console progress output to `.agents-reverse-engineer/progress.log`\n * without ANSI escape codes, enabling real-time monitoring via `tail -f`\n * when the CLI runs inside buffered environments (e.g. Claude Code).\n *\n * Uses promise-chain serialization (same pattern as {@link TraceWriter})\n * to handle concurrent writes from multiple pool workers safely.\n *\n * @example\n * ```typescript\n * const log = new ProgressLog('/project/.agents-reverse-engineer/progress.log');\n * log.write('=== ARE Generate (2026-02-09) ===');\n * log.write('[1/10] ANALYZING src/index.ts');\n * await log.finalize();\n * ```\n */\nexport class ProgressLog {\n  private writeQueue: Promise<void> = Promise.resolve();\n  private fd: FileHandle | null = null;\n\n  constructor(private readonly filePath: string) {}\n\n  /**\n   * Create a ProgressLog for a project root.\n   *\n   * @param projectRoot - Absolute path to the project root directory\n   * @returns A new ProgressLog instance\n   */\n  static create(projectRoot: string): ProgressLog {\n    return new ProgressLog(\n      path.join(projectRoot, '.agents-reverse-engineer', PROGRESS_LOG_FILENAME),\n    );\n  }\n\n  /**\n   * Append a line to the progress log file.\n   *\n   * On first call, creates the parent directory and opens the file\n   * in truncate mode ('w'). Subsequent writes append to the open handle.\n   * Write failures are silently swallowed (non-critical telemetry).\n   */\n  write(line: string): void {\n    this.writeQueue = this.writeQueue\n      .then(async () => {\n        if (!this.fd) {\n          await mkdir(path.dirname(this.filePath), { recursive: true });\n          this.fd = await open(this.filePath, 'w');\n        }\n        await this.fd.write(line + '\\n');\n      })\n      .catch(() => { /* non-critical -- progress log loss is acceptable */ });\n  }\n\n  /** Flush all pending writes and close the file handle. */\n  async finalize(): Promise<void> {\n    await this.writeQueue;\n    if (this.fd) {\n      await this.fd.close();\n      this.fd = null;\n    }\n  }\n}\n\n// ---------------------------------------------------------------------------\n// ProgressReporter\n// ---------------------------------------------------------------------------\n\n/**\n * Streaming build-log progress reporter.\n *\n * Create one instance per command run. Call the event methods as files\n * are processed. Call {@link printSummary} at the end of the run.\n *\n * @example\n * ```typescript\n * const reporter = new ProgressReporter(fileCount);\n * reporter.onFileStart('src/index.ts');\n * reporter.onFileDone('src/index.ts', 1200, 500, 300, 'sonnet');\n * reporter.printSummary(summary);\n * ```\n */\nexport class ProgressReporter {\n  /** Total number of file tasks in this run */\n  private readonly totalFiles: number;\n\n  /** Number of files that have started processing */\n  private started: number = 0;\n\n  /** Number of files completed successfully */\n  private completed: number = 0;\n\n  /** Number of files that failed */\n  private failed: number = 0;\n\n  /** Sliding window of recent completion durations for ETA */\n  private readonly completionTimes: number[] = [];\n\n  /** Maximum window size for ETA moving average */\n  private readonly windowSize: number = 10;\n\n  /** Total number of directory tasks in this run */\n  private totalDirectories: number = 0;\n\n  /** Number of directory tasks that have started */\n  private dirStarted: number = 0;\n\n  /** Number of directory tasks completed */\n  private dirCompleted: number = 0;\n\n  /** Sliding window of recent directory completion durations for ETA */\n  private readonly dirCompletionTimes: number[] = [];\n\n  /** Optional file-based progress log for tail -f monitoring */\n  private readonly progressLog: ProgressLog | null;\n\n  /**\n   * Create a new progress reporter.\n   *\n   * @param totalFiles - Total number of file tasks to process\n   * @param totalDirectories - Total number of directory tasks to process\n   * @param progressLog - Optional progress log for file-based output mirroring\n   */\n  constructor(totalFiles: number, totalDirectories: number = 0, progressLog?: ProgressLog) {\n    this.totalFiles = totalFiles;\n    this.totalDirectories = totalDirectories;\n    this.progressLog = progressLog ?? null;\n  }\n\n  /**\n   * Log the start of file analysis.\n   *\n   * Output format: `[X/Y] ANALYZING path`\n   *\n   * @param filePath - Relative path to the file being analyzed\n   */\n  onFileStart(filePath: string): void {\n    this.started++;\n    const line = `${pc.dim(`[${this.started}/${this.totalFiles}]`)} ${pc.cyan('ANALYZING')} ${filePath}`;\n    console.log(line);\n    this.progressLog?.write(stripAnsi(line));\n  }\n\n  /**\n   * Log the successful completion of file analysis.\n   *\n   * Output format: `[X/Y] DONE path Xs in/out tok ~Ns remaining`\n   *\n   * Records the completion time for ETA calculation.\n   *\n   * @param filePath - Relative path to the completed file\n   * @param durationMs - Wall-clock duration of the AI call\n   * @param tokensIn - Number of input tokens consumed (non-cached)\n   * @param tokensOut - Number of output tokens generated\n   * @param model - Model identifier used for this call\n   * @param cacheReadTokens - Number of cache read input tokens\n   */\n  onFileDone(\n    filePath: string,\n    durationMs: number,\n    tokensIn: number,\n    tokensOut: number,\n    model: string,\n    cacheReadTokens = 0,\n    cacheCreationTokens = 0,\n  ): void {\n    this.completed++;\n\n    // Record completion time for ETA\n    this.completionTimes.push(durationMs);\n    if (this.completionTimes.length > this.windowSize) {\n      this.completionTimes.shift();\n    }\n\n    const counter = pc.dim(`[${this.completed + this.failed}/${this.totalFiles}]`);\n    const time = pc.dim(`${(durationMs / 1000).toFixed(1)}s`);\n    // Total prompt size = non-cached + cache read + cache creation tokens\n    const totalIn = tokensIn + cacheReadTokens + cacheCreationTokens;\n    const tokens = pc.dim(`${totalIn}/${tokensOut} tok`);\n    const modelLabel = pc.dim(model);\n    const eta = this.formatETA();\n\n    const line = `${counter} ${pc.green('DONE')} ${filePath} ${time} ${tokens} ${modelLabel}${eta}`;\n    console.log(line);\n    this.progressLog?.write(stripAnsi(line));\n  }\n\n  /**\n   * Log a file analysis failure.\n   *\n   * Output format: `[X/Y] FAIL path error`\n   *\n   * @param filePath - Relative path to the failed file\n   * @param error - Error message describing the failure\n   */\n  onFileError(filePath: string, error: string): void {\n    this.failed++;\n\n    const line = `${pc.dim(`[${this.completed + this.failed}/${this.totalFiles}]`)} ${pc.red('FAIL')} ${filePath} ${pc.dim(error)}`;\n    console.log(line);\n    this.progressLog?.write(stripAnsi(line));\n  }\n\n  /**\n   * Log the start of directory AGENTS.md generation.\n   *\n   * Output format: `[dir X/Y] ANALYZING dirPath/AGENTS.md`\n   *\n   * @param dirPath - Path to the directory\n   */\n  onDirectoryStart(dirPath: string): void {\n    this.dirStarted++;\n    const line = `${pc.dim(`[dir ${this.dirStarted}/${this.totalDirectories}]`)} ${pc.cyan('ANALYZING')} ${dirPath}/AGENTS.md`;\n    console.log(line);\n    this.progressLog?.write(stripAnsi(line));\n  }\n\n  /**\n   * Log the completion of directory AGENTS.md generation.\n   *\n   * Output format: `[dir X/Y] DONE dirPath/AGENTS.md Xs in/out tok model ~ETA`\n   *\n   * @param dirPath - Path to the directory\n   * @param durationMs - Wall-clock duration of the AI call\n   * @param tokensIn - Number of input tokens consumed (non-cached)\n   * @param tokensOut - Number of output tokens generated\n   * @param model - Model identifier used for this call\n   * @param cacheReadTokens - Number of cache read input tokens\n   */\n  onDirectoryDone(\n    dirPath: string,\n    durationMs: number,\n    tokensIn: number,\n    tokensOut: number,\n    model: string,\n    cacheReadTokens = 0,\n    cacheCreationTokens = 0,\n  ): void {\n    this.dirCompleted++;\n\n    // Record completion time for directory ETA\n    this.dirCompletionTimes.push(durationMs);\n    if (this.dirCompletionTimes.length > this.windowSize) {\n      this.dirCompletionTimes.shift();\n    }\n\n    const counter = pc.dim(`[dir ${this.dirCompleted}/${this.totalDirectories}]`);\n    const time = pc.dim(`${(durationMs / 1000).toFixed(1)}s`);\n    // Total prompt size = non-cached + cache read + cache creation tokens\n    const totalIn = tokensIn + cacheReadTokens + cacheCreationTokens;\n    const tokens = pc.dim(`${totalIn}/${tokensOut} tok`);\n    const modelLabel = pc.dim(model);\n    const eta = this.formatDirectoryETA();\n\n    const line = `${counter} ${pc.blue('DONE')} ${dirPath}/AGENTS.md ${time} ${tokens} ${modelLabel}${eta}`;\n    console.log(line);\n    this.progressLog?.write(stripAnsi(line));\n  }\n\n  /**\n   * Print the end-of-run summary.\n   *\n   * Shows files processed, token counts, files read with unique dedup,\n   * time elapsed, errors, and retries.\n   *\n   * @param summary - Aggregated run summary\n   */\n  printSummary(summary: RunSummary): void {\n    const elapsed = (summary.totalDurationMs / 1000).toFixed(1);\n\n    const lines: string[] = [];\n    lines.push('');\n    lines.push(pc.bold('=== Run Summary ==='));\n    lines.push(`  ARE version:     ${summary.version}`);\n    lines.push(`  Files processed: ${pc.green(String(summary.filesProcessed))}`);\n    if (summary.filesFailed > 0) {\n      lines.push(`  Files failed:    ${pc.red(String(summary.filesFailed))}`);\n    }\n    if (summary.filesSkipped > 0) {\n      lines.push(`  Files skipped:   ${pc.yellow(String(summary.filesSkipped))}`);\n    }\n    if (summary.dirsProcessed != null && summary.dirsProcessed > 0) {\n      lines.push(`  Dirs processed:  ${pc.green(String(summary.dirsProcessed))}`);\n    }\n    if (summary.dirsFailed != null && summary.dirsFailed > 0) {\n      lines.push(`  Dirs failed:     ${pc.red(String(summary.dirsFailed))}`);\n    }\n    if (summary.dirsSkipped != null && summary.dirsSkipped > 0) {\n      lines.push(`  Dirs skipped:    ${pc.yellow(String(summary.dirsSkipped))}`);\n    }\n    lines.push(`  Total calls:     ${summary.totalCalls}`);\n    const totalIn = summary.totalInputTokens + summary.totalCacheReadTokens + summary.totalCacheCreationTokens;\n    lines.push(`  Tokens:          ${totalIn} in / ${summary.totalOutputTokens} out`);\n    if (summary.totalCacheReadTokens > 0) {\n      lines.push(`  Cache:           ${summary.totalCacheReadTokens} read / ${summary.totalCacheCreationTokens} created`);\n    }\n\n    if (summary.totalFilesRead > 0) {\n      lines.push(`  Files read:      ${summary.totalFilesRead} (${summary.uniqueFilesRead} unique)`);\n    }\n\n    lines.push(`  Total time:      ${elapsed}s`);\n    lines.push(`  Errors:          ${summary.errorCount}`);\n    if (summary.retryCount > 0) {\n      lines.push(`  Retries:         ${summary.retryCount}`);\n    }\n\n    for (const line of lines) {\n      console.log(line);\n      this.progressLog?.write(stripAnsi(line));\n    }\n  }\n\n  // -------------------------------------------------------------------------\n  // ETA calculation\n  // -------------------------------------------------------------------------\n\n  /**\n   * Compute and format the estimated time remaining.\n   *\n   * Uses a moving average of the last 10 completion times.\n   * Returns an empty string if fewer than 2 completions have occurred.\n   *\n   * @returns Formatted ETA string like ` ~12s remaining` or ` ~2m 30s remaining`\n   */\n  private formatETA(): string {\n    if (this.completionTimes.length < 2) return '';\n\n    const avg =\n      this.completionTimes.reduce((a, b) => a + b, 0) /\n      this.completionTimes.length;\n    const remaining = this.totalFiles - this.completed - this.failed;\n\n    if (remaining <= 0) return '';\n\n    const etaMs = avg * remaining;\n    const seconds = Math.round(etaMs / 1000);\n\n    if (seconds < 60) {\n      return pc.dim(` ~${seconds}s remaining`);\n    }\n\n    const minutes = Math.floor(seconds / 60);\n    const secs = seconds % 60;\n    return pc.dim(` ~${minutes}m ${secs}s remaining`);\n  }\n\n  /**\n   * Compute and format the estimated time remaining for directory tasks.\n   *\n   * Uses a moving average of the last 10 directory completion times.\n   * Returns an empty string if fewer than 2 completions have occurred.\n   */\n  private formatDirectoryETA(): string {\n    if (this.dirCompletionTimes.length < 2) return '';\n\n    const avg =\n      this.dirCompletionTimes.reduce((a, b) => a + b, 0) /\n      this.dirCompletionTimes.length;\n    const remaining = this.totalDirectories - this.dirCompleted;\n\n    if (remaining <= 0) return '';\n\n    const etaMs = avg * remaining;\n    const seconds = Math.round(etaMs / 1000);\n\n    if (seconds < 60) {\n      return pc.dim(` ~${seconds}s remaining`);\n    }\n\n    const minutes = Math.floor(seconds / 60);\n    const secs = seconds % 60;\n    return pc.dim(` ~${minutes}m ${secs}s remaining`);\n  }\n}\n\n```\n\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n- Workflow & convention rules: if this file defines contribution guidelines, PR conventions, commit standards, testing mandates, tool usage requirements, approval workflows, AI agent behavioral instructions, or code conventions (naming standards, formatting rules, import ordering, linting policies), extract them as explicit, actionable rules. Sources: CONTRIBUTING.md, CI configs, PR templates, style guides, linter/formatter configs (.eslintrc, .prettierrc, .editorconfig), tsconfig strictness, README dev sections.\n\n\nTARGET LENGTH (MANDATORY):\n- Source file: 14422 characters\n- Target summary: ~1442 characters (10% compression)\n- Maximum: 1730 characters\n- Achieve this by: ultra-dense writing, minimal examples, single-sentence descriptions\n\nULTRA-COMPRESSION TECHNIQUES:\n- Use telegraphic style: \"exports foo, bar, baz\" not \"this file exports three functions\"\n- No code examples unless critical to understanding\n- One sentence per export maximum\n- Omit obvious parameter names (\"opts: Options\" not \"options: Options - configuration object\")\n- Use abbreviations: \"params\" not \"parameters\", \"config\" not \"configuration\"\n- Skip section if empty (don't write \"No special concerns\" or \"N/A\")\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nWORKFLOW & CONVENTION RULES (NEVER EXCLUDE):\n- Contribution requirements: branch naming, commit message formats, PR description templates\n- Testing mandates: required test types, coverage thresholds, \"never delete tests\" policies\n- Tool usage rules: required CLI commands, browser automation tools, linter/formatter requirements\n- Approval workflows: review requirements, CI checks that must pass, deployment gates\n- AI agent instructions: explicit directives for automated assistants (coding rules, prohibited actions)\n- Code conventions: naming standards (camelCase, PascalCase, snake_case for what), import ordering rules, file/folder naming patterns, formatting rules (indentation, quotes, semicolons), language idioms, TypeScript strictness policies, preferred patterns vs anti-patterns\nThese define how contributors (human or AI) must work within the project.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion .annex.sum file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**progress.ts streams build-log progress to stdout and optional .agents-reverse-engineer/progress.log with ETA calculation via moving averages.**\n\n## Exports\n\n**ProgressLog** — Plain-text progress log file writer that mirrors console output to `.agents-reverse-engineer/progress.log` without ANSI codes for `tail -f` monitoring. Serializes concurrent writes via promise-chain queue. `static create(projectRoot: string): ProgressLog` creates instance for project root; `write(line: string): void` appends lines; `finalize(): Promise<void>` flushes and closes file handle.\n\n**ProgressReporter** — Streaming progress reporter for file and directory analysis. Constructor: `new ProgressReporter(totalFiles: number, totalDirectories?: number, progressLog?: ProgressLog)`. Methods:\n- `onFileStart(filePath: string): void` — logs `[X/Y] ANALYZING path`\n- `onFileDone(filePath: string, durationMs: number, tokensIn: number, tokensOut: number, model: string, cacheReadTokens?: number, cacheCreationTokens?: number): void` — logs `[X/Y] DONE path Xs in/out tok model ~ETA`\n- `onFileError(filePath: string, error: string): void` — logs `[X/Y] FAIL path error`\n- `onDirectoryStart(dirPath: string): void` — logs `[dir X/Y] ANALYZING dirPath/AGENTS.md`\n- `onDirectoryDone(dirPath: string, durationMs: number, tokensIn: number, tokensOut: number, model: string, cacheReadTokens?: number, cacheCreationTokens?: number): void` — logs `[dir X/Y] DONE dirPath/AGENTS.md Xs in/out tok model ~ETA`\n- `printSummary(summary: RunSummary): void` — outputs end-of-run statistics\n\n## Key Design Patterns\n\n**Moving-average ETA**: `completionTimes` and `dirCompletionTimes` sliding windows (max 10 elements) track recent durations; ETA calculated when ≥2 completions exist; formatted as seconds if <60, else as `Xm Ys remaining`.\n\n**Token accounting**: Total input = `tokensIn + cacheReadTokens + cacheCreationTokens` (cache components tracked separately in summary).\n\n**Promise-chain serialization**: ProgressLog uses `writeQueue: Promise<void>` to serialize file writes from concurrent workers (identical pattern to TraceWriter).\n\n**ANSI stripping**: `stripAnsi(str)` removes escape sequences matching `/\\x1b\\[[0-9;]*m/g` for plain-text log output.\n\n**File initialization**: ProgressLog parent directory created and file opened (truncate mode `'w'`) on first `write()` call; write errors silently swallowed (non-critical telemetry).\n\n## Dependencies\n\n- `node:fs/promises` (open, mkdir, FileHandle)\n- `node:path` (join, dirname)\n- `picocolors` (pc — colored console output: cyan for ANALYZING, green/blue for DONE, red for FAIL, dim for metadata)\n- `./types.js` (RunSummary — aggregated run statistics)\n\n## Output Formats\n\nFile analysis: `[X/Y] ANALYZING path` | `[X/Y] DONE path Xs in/out tok model [~ETA]` | `[X/Y] FAIL path error`\n\nDirectory analysis: `[dir X/Y] ANALYZING dirPath/AGENTS.md` | `[dir X/Y] DONE dirPath/AGENTS.md Xs in/out tok model [~ETA]`\n\nSummary: `=== Run Summary ===` followed by lines for version, files/dirs processed/failed/skipped, total calls, token counts (input/output/cache), files read (total/unique), elapsed time, errors, retries.\n\n## Constants\n\n**PROGRESS_LOG_FILENAME** = `'progress.log'` — filename relative to `.agents-reverse-engineer/` directory.",
      "model": "claude-haiku-4-5-20251001",
      "inputTokens": 3,
      "outputTokens": 979,
      "cacheReadTokens": 16147,
      "cacheCreationTokens": 7095,
      "latencyMs": 28028,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/orchestration/progress.ts",
          "sizeBytes": 14422
        }
      ]
    },
    {
      "timestamp": "2026-02-14T13:47:46.106Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: src/orchestration/trace.ts\n\n```typescript\n/**\n * Concurrency tracing system for debugging task/subprocess lifecycle.\n *\n * Produces append-only NDJSON trace files in `.agents-reverse-engineer/traces/`\n * when the `--trace` CLI flag is set. When disabled, the {@link NullTraceWriter}\n * ensures zero overhead -- every call site can unconditionally call `emit()`\n * without branching.\n *\n * Uses promise-chain serialization (same pattern as {@link PlanTracker}) to\n * handle concurrent writes from multiple pool workers safely.\n *\n * @module\n */\n\nimport { open, mkdir, readdir, unlink } from 'node:fs/promises';\nimport type { FileHandle } from 'node:fs/promises';\nimport * as path from 'node:path';\n\n// ---------------------------------------------------------------------------\n// Trace directory\n// ---------------------------------------------------------------------------\n\n/** Directory for trace files (relative to project root) */\nconst TRACES_DIR = '.agents-reverse-engineer/traces';\n\n// ---------------------------------------------------------------------------\n// Trace event types\n// ---------------------------------------------------------------------------\n\n/** Common fields present on every trace event */\ninterface TraceEventBase {\n  /** Monotonically increasing sequence number (per-run) */\n  seq: number;\n  /** ISO 8601 timestamp at event creation time */\n  ts: string;\n  /** process.pid of the Node.js parent process */\n  pid: number;\n  /** High-resolution elapsed time since run start (ms, fractional) */\n  elapsedMs: number;\n}\n\n/** Emitted when a phase begins execution */\ninterface PhaseStartEvent extends TraceEventBase {\n  type: 'phase:start';\n  phase: string;\n  taskCount: number;\n  concurrency: number;\n}\n\n/** Emitted when a phase completes */\ninterface PhaseEndEvent extends TraceEventBase {\n  type: 'phase:end';\n  phase: string;\n  durationMs: number;\n  tasksCompleted: number;\n  tasksFailed: number;\n}\n\n/** Emitted when a pool worker starts pulling from the shared iterator */\ninterface WorkerStartEvent extends TraceEventBase {\n  type: 'worker:start';\n  workerId: number;\n  phase: string;\n}\n\n/** Emitted when a pool worker exhausts the iterator or is aborted */\ninterface WorkerEndEvent extends TraceEventBase {\n  type: 'worker:end';\n  workerId: number;\n  phase: string;\n  tasksExecuted: number;\n}\n\n/** Emitted when a worker picks up a task from the iterator */\ninterface TaskPickupEvent extends TraceEventBase {\n  type: 'task:pickup';\n  workerId: number;\n  taskIndex: number;\n  taskLabel: string;\n  activeTasks: number;\n}\n\n/** Emitted when a task completes (success or failure) */\ninterface TaskDoneEvent extends TraceEventBase {\n  type: 'task:done';\n  workerId: number;\n  taskIndex: number;\n  taskLabel: string;\n  durationMs: number;\n  success: boolean;\n  error?: string;\n  activeTasks: number;\n}\n\n/** Emitted when a child process is spawned */\ninterface SubprocessSpawnEvent extends TraceEventBase {\n  type: 'subprocess:spawn';\n  childPid: number;\n  command: string;\n  taskLabel: string;\n}\n\n/** Emitted when a child process exits */\ninterface SubprocessExitEvent extends TraceEventBase {\n  type: 'subprocess:exit';\n  childPid: number;\n  command: string;\n  taskLabel: string;\n  exitCode: number;\n  signal: string | null;\n  durationMs: number;\n  timedOut: boolean;\n}\n\n/** Emitted before a retry attempt */\ninterface RetryEvent extends TraceEventBase {\n  type: 'retry';\n  attempt: number;\n  taskLabel: string;\n  errorCode: string;\n}\n\n/** Emitted when a non-pool task starts execution */\ninterface TaskStartEvent extends TraceEventBase {\n  type: 'task:start';\n  taskLabel: string;\n  phase: string;\n}\n\n/** Emitted when file discovery begins */\ninterface DiscoveryStartEvent extends TraceEventBase {\n  type: 'discovery:start';\n  targetPath: string;\n}\n\n/** Emitted when file discovery completes */\ninterface DiscoveryEndEvent extends TraceEventBase {\n  type: 'discovery:end';\n  filesIncluded: number;\n  filesExcluded: number;\n  durationMs: number;\n}\n\n/** Emitted when a filter is applied during discovery */\ninterface FilterAppliedEvent extends TraceEventBase {\n  type: 'filter:applied';\n  filterName: string;\n  filesMatched: number;\n  filesRejected: number;\n}\n\n/** Emitted when a generation/update plan is created */\ninterface PlanCreatedEvent extends TraceEventBase {\n  type: 'plan:created';\n  planType: 'generate' | 'update';\n  fileCount: number;\n  taskCount: number;\n}\n\n/** Emitted when configuration is loaded */\ninterface ConfigLoadedEvent extends TraceEventBase {\n  type: 'config:loaded';\n  configPath: string;\n  model: string;\n  concurrency: number;\n}\n\n/** Discriminated union of all trace event types */\nexport type TraceEvent =\n  | PhaseStartEvent\n  | PhaseEndEvent\n  | WorkerStartEvent\n  | WorkerEndEvent\n  | TaskPickupEvent\n  | TaskDoneEvent\n  | TaskStartEvent\n  | SubprocessSpawnEvent\n  | SubprocessExitEvent\n  | RetryEvent\n  | DiscoveryStartEvent\n  | DiscoveryEndEvent\n  | FilterAppliedEvent\n  | PlanCreatedEvent\n  | ConfigLoadedEvent;\n\n/** Keys auto-populated by the trace writer */\ntype BaseKeys = 'seq' | 'ts' | 'pid' | 'elapsedMs';\n\n/** Distributive Omit that works correctly across union members */\ntype DistributiveOmit<T, K extends PropertyKey> = T extends unknown ? Omit<T, K> : never;\n\n/** Event payload without auto-populated base fields */\nexport type TraceEventPayload = DistributiveOmit<TraceEvent, BaseKeys>;\n\n// ---------------------------------------------------------------------------\n// ITraceWriter interface\n// ---------------------------------------------------------------------------\n\n/**\n * Public interface for trace event emission.\n *\n * All consumers depend only on this interface, allowing the no-op\n * implementation to be swapped in when tracing is disabled.\n */\nexport interface ITraceWriter {\n  /** Emit a trace event. Base fields (seq, ts, pid, elapsedMs) are auto-populated. */\n  emit(event: TraceEventPayload): void;\n  /** Flush all pending writes and close the file handle. */\n  finalize(): Promise<void>;\n  /** Absolute path to the trace file (empty string for NullTraceWriter). */\n  readonly filePath: string;\n}\n\n// ---------------------------------------------------------------------------\n// NullTraceWriter (no-op)\n// ---------------------------------------------------------------------------\n\n/**\n * No-op trace writer. Returned when `--trace` is not set.\n * All methods are empty -- zero overhead at call sites.\n */\nclass NullTraceWriter implements ITraceWriter {\n  readonly filePath = '';\n  emit(): void { /* no-op */ }\n  async finalize(): Promise<void> { /* no-op */ }\n}\n\n// ---------------------------------------------------------------------------\n// TraceWriter (real implementation)\n// ---------------------------------------------------------------------------\n\n/**\n * Append-only NDJSON trace writer.\n *\n * Each `emit()` call serializes the event to a single JSON line and\n * enqueues a file append via a promise chain. This guarantees correct\n * ordering even when multiple pool workers emit concurrently.\n */\nclass TraceWriter implements ITraceWriter {\n  readonly filePath: string;\n\n  private seq = 0;\n  private readonly nodePid = process.pid;\n  private readonly startHr = process.hrtime.bigint();\n  private writeQueue: Promise<void> = Promise.resolve();\n  private fd: FileHandle | null = null;\n\n  constructor(filePath: string) {\n    this.filePath = filePath;\n  }\n\n  emit(partial: TraceEventPayload): void {\n    const event = {\n      ...partial,\n      seq: this.seq++,\n      ts: new Date().toISOString(),\n      pid: this.nodePid,\n      elapsedMs: Number(process.hrtime.bigint() - this.startHr) / 1_000_000,\n    };\n    const line = JSON.stringify(event) + '\\n';\n\n    this.writeQueue = this.writeQueue\n      .then(async () => {\n        if (!this.fd) {\n          await mkdir(path.dirname(this.filePath), { recursive: true });\n          this.fd = await open(this.filePath, 'a');\n        }\n        await this.fd.write(line);\n      })\n      .catch(() => { /* non-critical -- trace loss is acceptable */ });\n  }\n\n  async finalize(): Promise<void> {\n    await this.writeQueue;\n    if (this.fd) {\n      await this.fd.close();\n      this.fd = null;\n    }\n  }\n}\n\n// ---------------------------------------------------------------------------\n// Factory\n// ---------------------------------------------------------------------------\n\n/**\n * Create a trace writer.\n *\n * Returns a {@link NullTraceWriter} when `enabled` is false (zero overhead).\n * Otherwise returns a {@link TraceWriter} that appends NDJSON to\n * `.agents-reverse-engineer/traces/trace-{timestamp}.ndjson`.\n *\n * @param projectRoot - Absolute path to the project root directory\n * @param enabled - Whether tracing is enabled (typically from `--trace` flag)\n * @returns A trace writer instance\n */\nexport function createTraceWriter(projectRoot: string, enabled: boolean): ITraceWriter {\n  if (!enabled) return new NullTraceWriter();\n  const safeTimestamp = new Date().toISOString().replace(/[:.]/g, '-');\n  const filePath = path.join(projectRoot, TRACES_DIR, `trace-${safeTimestamp}.ndjson`);\n  return new TraceWriter(filePath);\n}\n\n// ---------------------------------------------------------------------------\n// Cleanup\n// ---------------------------------------------------------------------------\n\n/**\n * Remove old trace files, keeping only the most recent ones.\n *\n * Mirrors the pattern in `src/ai/telemetry/cleanup.ts`.\n *\n * @param projectRoot - Absolute path to the project root directory\n * @param keepCount - Number of most recent trace files to retain (default: 500)\n * @returns Number of files deleted\n */\nexport async function cleanupOldTraces(projectRoot: string, keepCount: number = 500): Promise<number> {\n  const tracesDir = path.join(projectRoot, TRACES_DIR);\n\n  let entries: string[];\n  try {\n    const allEntries = await readdir(tracesDir);\n    entries = allEntries.filter(\n      (name) => name.startsWith('trace-') && name.endsWith('.ndjson'),\n    );\n  } catch (error) {\n    if ((error as NodeJS.ErrnoException).code === 'ENOENT') {\n      return 0;\n    }\n    throw error;\n  }\n\n  // Sort newest first (ISO timestamps sort lexicographically)\n  entries.sort();\n  entries.reverse();\n\n  const toDelete = entries.slice(keepCount);\n\n  for (const filename of toDelete) {\n    await unlink(path.join(tracesDir, filename));\n  }\n\n  return toDelete.length;\n}\n\n```\n\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n- Workflow & convention rules: if this file defines contribution guidelines, PR conventions, commit standards, testing mandates, tool usage requirements, approval workflows, AI agent behavioral instructions, or code conventions (naming standards, formatting rules, import ordering, linting policies), extract them as explicit, actionable rules. Sources: CONTRIBUTING.md, CI configs, PR templates, style guides, linter/formatter configs (.eslintrc, .prettierrc, .editorconfig), tsconfig strictness, README dev sections.\n\n\nTARGET LENGTH (MANDATORY):\n- Source file: 10277 characters\n- Target summary: ~1028 characters (10% compression)\n- Maximum: 1234 characters\n- Achieve this by: ultra-dense writing, minimal examples, single-sentence descriptions\n\nULTRA-COMPRESSION TECHNIQUES:\n- Use telegraphic style: \"exports foo, bar, baz\" not \"this file exports three functions\"\n- No code examples unless critical to understanding\n- One sentence per export maximum\n- Omit obvious parameter names (\"opts: Options\" not \"options: Options - configuration object\")\n- Use abbreviations: \"params\" not \"parameters\", \"config\" not \"configuration\"\n- Skip section if empty (don't write \"No special concerns\" or \"N/A\")\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nWORKFLOW & CONVENTION RULES (NEVER EXCLUDE):\n- Contribution requirements: branch naming, commit message formats, PR description templates\n- Testing mandates: required test types, coverage thresholds, \"never delete tests\" policies\n- Tool usage rules: required CLI commands, browser automation tools, linter/formatter requirements\n- Approval workflows: review requirements, CI checks that must pass, deployment gates\n- AI agent instructions: explicit directives for automated assistants (coding rules, prohibited actions)\n- Code conventions: naming standards (camelCase, PascalCase, snake_case for what), import ordering rules, file/folder naming patterns, formatting rules (indentation, quotes, semicolons), language idioms, TypeScript strictness policies, preferred patterns vs anti-patterns\nThese define how contributors (human or AI) must work within the project.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion .annex.sum file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**trace.ts provides an append-only NDJSON tracing system for task/subprocess lifecycle debugging, with zero-overhead no-op mode when disabled.**\n\n## Exports\n\n`ITraceWriter` — interface for trace event emission with `emit(event: TraceEventPayload): void`, `finalize(): Promise<void>`, and read-only `filePath: string`.\n\n`TraceEvent` — discriminated union of 14 event types: `PhaseStartEvent`, `PhaseEndEvent`, `WorkerStartEvent`, `WorkerEndEvent`, `TaskPickupEvent`, `TaskDoneEvent`, `TaskStartEvent`, `SubprocessSpawnEvent`, `SubprocessExitEvent`, `RetryEvent`, `DiscoveryStartEvent`, `DiscoveryEndEvent`, `FilterAppliedEvent`, `PlanCreatedEvent`, `ConfigLoadedEvent`.\n\n`TraceEventPayload` — `TraceEvent` with auto-populated fields (`seq`, `ts`, `pid`, `elapsedMs`) omitted.\n\n`createTraceWriter(projectRoot: string, enabled: boolean): ITraceWriter` — factory returning `NullTraceWriter` (zero overhead) when disabled, else `TraceWriter` appending to `.agents-reverse-engineer/traces/trace-{ISO8601-timestamp}.ndjson`.\n\n`cleanupOldTraces(projectRoot: string, keepCount: number = 500): Promise<number>` — deletes oldest trace files beyond retention limit, returns count deleted, silently handles missing directory.\n\n## Architecture\n\n`TraceWriter` uses promise-chain serialization (`this.writeQueue`) to serialize concurrent writes from pool workers into correct NDJSON order; mirrors `PlanTracker` pattern. `NullTraceWriter` provides zero-cost abstraction (empty methods) when `--trace` flag absent. `TraceEventBase` auto-populates `seq` (monotonic per-run), `ts` (ISO 8601), `pid` (parent Node.js process), `elapsedMs` (high-resolution elapsed time since startup via `process.hrtime.bigint()`).\n\n## Behavioral Contracts\n\nTrace filename format: `trace-{ISO8601-timestamp}.ndjson` with colons and periods replaced by hyphens (e.g., `trace-2024-12-14T10-30-45-123Z.ndjson`). Each event serialized as single JSON line (`JSON.stringify(event) + '\\n'`). Directory created recursively if missing. Write errors silently ignored (trace loss acceptable). Timestamps use `new Date().toISOString()`. Cleanup sorts filenames lexicographically (ISO timestamps sort correctly) and keeps `keepCount` most recent entries.",
      "model": "claude-haiku-4-5-20251001",
      "inputTokens": 3,
      "outputTokens": 629,
      "cacheReadTokens": 16147,
      "cacheCreationTokens": 5607,
      "latencyMs": 23718,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/orchestration/trace.ts",
          "sizeBytes": 10277
        }
      ]
    },
    {
      "timestamp": "2026-02-14T13:47:47.349Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: src/orchestration/types.ts\n\n```typescript\n/**\n * Shared types for the orchestration module.\n *\n * These types are used across the concurrency pool, progress reporter,\n * and command runner to represent task results, run summaries, progress\n * events, and command options.\n *\n * @module\n */\n\nimport type { InconsistencyReport } from '../quality/index.js';\nimport type { ProgressLog } from './progress.js';\nimport type { ITraceWriter } from './trace.js';\n\n// ---------------------------------------------------------------------------\n// File task result\n// ---------------------------------------------------------------------------\n\n/**\n * Result of processing a single file through AI analysis.\n *\n * Produced by the command runner for each file task, carrying token\n * counts and timing data needed for the run summary.\n */\nexport interface FileTaskResult {\n  /** Relative path to the source file */\n  path: string;\n  /** Whether the AI call succeeded */\n  success: boolean;\n  /** Number of input tokens consumed (non-cached) */\n  tokensIn: number;\n  /** Number of output tokens generated */\n  tokensOut: number;\n  /** Number of cache read input tokens */\n  cacheReadTokens: number;\n  /** Number of cache creation input tokens */\n  cacheCreationTokens: number;\n  /** Wall-clock duration in milliseconds */\n  durationMs: number;\n  /** Model identifier used for this call */\n  model: string;\n  /** Error message if the call failed */\n  error?: string;\n}\n\n// ---------------------------------------------------------------------------\n// Run summary\n// ---------------------------------------------------------------------------\n\n/**\n * Aggregated summary of a command run.\n *\n * Produced at the end of a generate or update command execution,\n * combining per-file results into totals for display and telemetry.\n */\nexport interface RunSummary {\n  /** agents-reverse-engineer version that produced this run */\n  version: string;\n  /** Number of files that were successfully processed */\n  filesProcessed: number;\n  /** Number of files that failed processing */\n  filesFailed: number;\n  /** Number of files that were skipped (e.g., dry-run) */\n  filesSkipped: number;\n  /** Total number of AI calls made */\n  totalCalls: number;\n  /** Sum of input tokens across all calls */\n  totalInputTokens: number;\n  /** Sum of output tokens across all calls */\n  totalOutputTokens: number;\n  /** Sum of cache read tokens across all calls */\n  totalCacheReadTokens: number;\n  /** Sum of cache creation tokens across all calls */\n  totalCacheCreationTokens: number;\n  /** Total wall-clock duration in milliseconds */\n  totalDurationMs: number;\n  /** Number of errors encountered */\n  errorCount: number;\n  /** Number of retries that occurred */\n  retryCount: number;\n  /** Total file reads across all calls */\n  totalFilesRead: number;\n  /** Unique files read (deduped by path) */\n  uniqueFilesRead: number;\n  /** Number of code-vs-doc inconsistencies detected */\n  inconsistenciesCodeVsDoc?: number;\n  /** Number of code-vs-code inconsistencies detected */\n  inconsistenciesCodeVsCode?: number;\n  /** Number of directories that were successfully processed */\n  dirsProcessed?: number;\n  /** Number of directories that failed processing */\n  dirsFailed?: number;\n  /** Number of directories that were skipped (unchanged) */\n  dirsSkipped?: number;\n  /** Number of phantom path references detected in AGENTS.md files */\n  phantomPaths?: number;\n  /** Full inconsistency report (undefined if no checks ran) */\n  inconsistencyReport?: InconsistencyReport;\n}\n\n// ---------------------------------------------------------------------------\n// Progress events\n// ---------------------------------------------------------------------------\n\n/**\n * Event emitted by the command runner to the progress reporter.\n *\n * Each event type carries different optional fields:\n * - `start`: filePath, index, total\n * - `done`: filePath, index, total, durationMs, tokensIn, tokensOut, model\n * - `error`: filePath, index, total, error\n * - `dir-done`: filePath (directory path)\n */\nexport interface ProgressEvent {\n  /** Event type */\n  type: 'start' | 'done' | 'error' | 'dir-done';\n  /** File or directory path */\n  filePath: string;\n  /** Zero-based index of this task in the current phase */\n  index: number;\n  /** Total number of tasks in the current phase */\n  total: number;\n  /** Wall-clock duration in milliseconds (for 'done' events) */\n  durationMs?: number;\n  /** Input tokens consumed (for 'done' events) */\n  tokensIn?: number;\n  /** Output tokens generated (for 'done' events) */\n  tokensOut?: number;\n  /** Model identifier (for 'done' events) */\n  model?: string;\n  /** Error message (for 'error' events) */\n  error?: string;\n}\n\n// ---------------------------------------------------------------------------\n// Command run options\n// ---------------------------------------------------------------------------\n\n/**\n * Options that control how commands execute.\n *\n * These are populated from a combination of config file defaults\n * and CLI flag overrides.\n */\nexport interface CommandRunOptions {\n  /** Maximum number of concurrent AI calls */\n  concurrency: number;\n  /** Stop pulling new tasks on first error */\n  failFast?: boolean;\n  /** Show debug information (exact prompts sent) */\n  debug?: boolean;\n  /** List files that would be processed without executing */\n  dryRun?: boolean;\n  /** Trace writer for concurrency debugging (no-op when tracing is off) */\n  tracer?: ITraceWriter;\n  /** Progress log for file-based output mirroring (tail -f monitoring) */\n  progressLog?: ProgressLog;\n  /** Eval variant name for side-by-side model comparison (e.g., \"claude.haiku\") */\n  variant?: string;\n}\n\n```\n\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n- Workflow & convention rules: if this file defines contribution guidelines, PR conventions, commit standards, testing mandates, tool usage requirements, approval workflows, AI agent behavioral instructions, or code conventions (naming standards, formatting rules, import ordering, linting policies), extract them as explicit, actionable rules. Sources: CONTRIBUTING.md, CI configs, PR templates, style guides, linter/formatter configs (.eslintrc, .prettierrc, .editorconfig), tsconfig strictness, README dev sections.\n\n\nTARGET LENGTH (MANDATORY):\n- Source file: 5644 characters\n- Target summary: ~564 characters (10% compression)\n- Maximum: 677 characters\n- Achieve this by: ultra-dense writing, minimal examples, single-sentence descriptions\n\nULTRA-COMPRESSION TECHNIQUES:\n- Use telegraphic style: \"exports foo, bar, baz\" not \"this file exports three functions\"\n- No code examples unless critical to understanding\n- One sentence per export maximum\n- Omit obvious parameter names (\"opts: Options\" not \"options: Options - configuration object\")\n- Use abbreviations: \"params\" not \"parameters\", \"config\" not \"configuration\"\n- Skip section if empty (don't write \"No special concerns\" or \"N/A\")\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nWORKFLOW & CONVENTION RULES (NEVER EXCLUDE):\n- Contribution requirements: branch naming, commit message formats, PR description templates\n- Testing mandates: required test types, coverage thresholds, \"never delete tests\" policies\n- Tool usage rules: required CLI commands, browser automation tools, linter/formatter requirements\n- Approval workflows: review requirements, CI checks that must pass, deployment gates\n- AI agent instructions: explicit directives for automated assistants (coding rules, prohibited actions)\n- Code conventions: naming standards (camelCase, PascalCase, snake_case for what), import ordering rules, file/folder naming patterns, formatting rules (indentation, quotes, semicolons), language idioms, TypeScript strictness policies, preferred patterns vs anti-patterns\nThese define how contributors (human or AI) must work within the project.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion .annex.sum file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**Defines shared type interfaces for orchestration subsystem: task results, run summaries, progress events, and command options.**\n\n## Exported Types\n\n**FileTaskResult** — per-file AI analysis outcome with path, success flag, token counts (tokensIn, tokensOut, cacheReadTokens, cacheCreationTokens), durationMs, model, optional error.\n\n**RunSummary** — aggregated command execution stats: version, filesProcessed/filesFailed/filesSkipped, totalCalls, totalInputTokens/totalOutputTokens/totalCacheReadTokens/totalCacheCreationTokens, totalDurationMs, errorCount, retryCount, totalFilesRead, uniqueFilesRead, inconsistenciesCodeVsDoc, inconsistenciesCodeVsCode, dirsProcessed/dirsFailed/dirsSkipped, phantomPaths, inconsistencyReport: InconsistencyReport.\n\n**ProgressEvent** — emitted task updates with type: 'start'|'done'|'error'|'dir-done', filePath, index, total, optional durationMs, tokensIn, tokensOut, model, error; event-specific fields populate selectively (start: filePath/index/total; done: adds durationMs/tokensIn/tokensOut/model; error: adds error; dir-done: filePath only).\n\n**CommandRunOptions** — execution control params: concurrency (number), failFast, debug, dryRun (bool flags), tracer: ITraceWriter, progressLog: ProgressLog, variant (eval model name string).\n\n## Dependencies\n\nImports InconsistencyReport from '../quality/index.js', ProgressLog and ITraceWriter from sibling orchestration modules (progress.js, trace.js).",
      "model": "claude-haiku-4-5-20251001",
      "inputTokens": 3,
      "outputTokens": 413,
      "cacheReadTokens": 16147,
      "cacheCreationTokens": 4172,
      "latencyMs": 23300,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/orchestration/types.ts",
          "sizeBytes": 5644
        }
      ]
    },
    {
      "timestamp": "2026-02-14T13:47:50.377Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: src/output/logger.ts\n\n```typescript\n/**\n * Terminal logger for agents-reverse\n *\n * Provides colored output.\n * Output format follows CONTEXT.md human-readable specification.\n */\n\nimport pc from 'picocolors';\n\n/**\n * Logger interface for CLI output.\n */\nexport interface Logger {\n  /** Log an informational message */\n  info(message: string): void;\n\n  /** Log a discovered file */\n  file(path: string): void;\n\n  /** Log an excluded file with reason */\n  excluded(path: string, reason: string, filter: string): void;\n\n  /** Log discovery summary */\n  summary(included: number, excluded: number): void;\n\n  /** Log a warning message */\n  warn(message: string): void;\n\n  /** Log an error message */\n  error(message: string): void;\n}\n\n/**\n * Options for creating a logger instance.\n */\nexport interface LoggerOptions {\n  /**\n   * Use colors in terminal output.\n   * @default true\n   */\n  colors: boolean;\n}\n\n/**\n * Color functions type - either picocolors or identity functions\n */\ninterface ColorFunctions {\n  green: (s: string) => string;\n  dim: (s: string) => string;\n  red: (s: string) => string;\n  bold: (s: string) => string;\n  yellow: (s: string) => string;\n}\n\n/**\n * Identity function for no-color mode\n */\nconst identity = (s: string): string => s;\n\n/**\n * No-color formatter that returns strings unchanged\n */\nconst noColor: ColorFunctions = {\n  green: identity,\n  dim: identity,\n  red: identity,\n  bold: identity,\n  yellow: identity,\n};\n\n/**\n * Create a logger instance with the given options.\n *\n * Output format per CONTEXT.md (human-readable):\n * - file: green \"  +\" prefix + relative path\n * - excluded: dim \"  -\" prefix + path + reason (when shown)\n * - summary: bold count + dim excluded count\n * - warn: yellow \"Warning:\" prefix\n * - error: red \"Error:\" prefix\n *\n * @param options - Logger configuration\n * @returns Logger instance\n *\n * @example\n * ```typescript\n * const log = createLogger({ colors: true });\n *\n * log.file('src/index.ts');\n * log.summary(42, 10);\n * ```\n */\nexport function createLogger(options: LoggerOptions): Logger {\n  const c: ColorFunctions = options.colors ? pc : noColor;\n\n  return {\n    info(message: string): void {\n      console.log(message);\n    },\n\n    file(path: string): void {\n      console.log(c.green('  +') + ' ' + path);\n    },\n\n    excluded(path: string, reason: string, filter: string): void {\n      console.log(c.dim('  -') + ' ' + path + c.dim(` (${reason}: ${filter})`));\n    },\n\n    summary(included: number, excluded: number): void {\n      console.log(\n        c.bold(`\\nDiscovered ${included} files`) +\n          c.dim(` (${excluded} excluded)`)\n      );\n    },\n\n    warn(message: string): void {\n      console.warn(c.yellow('Warning: ') + message);\n    },\n\n    error(message: string): void {\n      console.error(c.red('Error: ') + message);\n    },\n  };\n}\n\n/**\n * Create a silent logger that produces no output.\n *\n * Useful for testing or programmatic usage.\n *\n * @returns Logger instance with all no-op methods\n */\nexport function createSilentLogger(): Logger {\n  const noop = (): void => {};\n  return {\n    info: noop,\n    file: noop,\n    excluded: noop,\n    summary: noop,\n    warn: noop,\n    error: noop,\n  };\n}\n\n```\n\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n- Workflow & convention rules: if this file defines contribution guidelines, PR conventions, commit standards, testing mandates, tool usage requirements, approval workflows, AI agent behavioral instructions, or code conventions (naming standards, formatting rules, import ordering, linting policies), extract them as explicit, actionable rules. Sources: CONTRIBUTING.md, CI configs, PR templates, style guides, linter/formatter configs (.eslintrc, .prettierrc, .editorconfig), tsconfig strictness, README dev sections.\n\n\nTARGET LENGTH (MANDATORY):\n- Source file: 3144 characters\n- Target summary: ~314 characters (10% compression)\n- Maximum: 377 characters\n- Achieve this by: ultra-dense writing, minimal examples, single-sentence descriptions\n\nULTRA-COMPRESSION TECHNIQUES:\n- Use telegraphic style: \"exports foo, bar, baz\" not \"this file exports three functions\"\n- No code examples unless critical to understanding\n- One sentence per export maximum\n- Omit obvious parameter names (\"opts: Options\" not \"options: Options - configuration object\")\n- Use abbreviations: \"params\" not \"parameters\", \"config\" not \"configuration\"\n- Skip section if empty (don't write \"No special concerns\" or \"N/A\")\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nWORKFLOW & CONVENTION RULES (NEVER EXCLUDE):\n- Contribution requirements: branch naming, commit message formats, PR description templates\n- Testing mandates: required test types, coverage thresholds, \"never delete tests\" policies\n- Tool usage rules: required CLI commands, browser automation tools, linter/formatter requirements\n- Approval workflows: review requirements, CI checks that must pass, deployment gates\n- AI agent instructions: explicit directives for automated assistants (coding rules, prohibited actions)\n- Code conventions: naming standards (camelCase, PascalCase, snake_case for what), import ordering rules, file/folder naming patterns, formatting rules (indentation, quotes, semicolons), language idioms, TypeScript strictness policies, preferred patterns vs anti-patterns\nThese define how contributors (human or AI) must work within the project.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion .annex.sum file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**logger.ts exports Logger interface and createLogger/createSilentLogger factories for colorized CLI output following CONTEXT.md format.**\n\n## Exports\n\n**Logger** interface: `info(message: string)`, `file(path: string)`, `excluded(path: string, reason: string, filter: string)`, `summary(included: number, excluded: number)`, `warn(message: string)`, `error(message: string)` — CLI output contract.\n\n**LoggerOptions** interface: `colors: boolean` (default true) — logger configuration.\n\n**createLogger(options: LoggerOptions): Logger** — returns logger with colored output via picocolors or identity functions when `options.colors=false`.\n\n**createSilentLogger(): Logger** — returns no-op logger for testing.\n\n## Output Format (CONTEXT.md Spec)\n\n- **file**: green `\"  +\"` prefix + relative path\n- **excluded**: dim `\"  -\"` prefix + path + dim `\"(${reason}: ${filter})\"` suffix\n- **summary**: bold `\"\\nDiscovered ${included} files\"` + dim `\"(${excluded} excluded)\"`\n- **warn**: yellow `\"Warning: \"` prefix + message\n- **error**: red `\"Error: \"` prefix + message\n\n## Dependencies\n\nImports `picocolors` (pc) for terminal colorization; falls back to identity functions when colors disabled.",
      "model": "claude-haiku-4-5-20251001",
      "inputTokens": 3,
      "outputTokens": 324,
      "cacheReadTokens": 16147,
      "cacheCreationTokens": 3712,
      "latencyMs": 22523,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/output/logger.ts",
          "sizeBytes": 3144
        }
      ]
    },
    {
      "timestamp": "2026-02-14T13:47:36.892Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: src/orchestration/orchestrator.ts\n\n```typescript\n/**\n * Documentation orchestrator\n *\n * Unified orchestrator for both generation and incremental update workflows.\n * Combines functionality from GenerationOrchestrator and UpdateOrchestrator.\n */\n\nimport { readFile } from 'node:fs/promises';\nimport * as path from 'node:path';\nimport type { Config } from '../config/schema.js';\nimport type { DiscoveryResult } from '../types/index.js';\nimport type { Logger } from '../core/logger.js';\nimport { nullLogger } from '../core/logger.js';\nimport type { ITraceWriter } from './trace.js';\n\n// Generation-specific imports\nimport { buildFilePrompt } from '../generation/prompts/index.js';\nimport { analyzeComplexity } from '../generation/complexity.js';\nimport type { ComplexityMetrics } from '../generation/complexity.js';\nimport { sumFileExists } from '../generation/writers/sum.js';\nimport { isGeneratedAgentsMd } from '../generation/writers/agents-md.js';\n\n// Update-specific imports\nimport {\n  isGitRepo,\n  getCurrentCommit,\n  computeContentHash,\n  type FileChange,\n} from '../change-detection/index.js';\nimport { cleanupOrphans, getAffectedDirectories } from '../update/orphan-cleaner.js';\nimport { readSumFile, getSumPath } from '../generation/writers/sum.js';\nimport type { UpdatePlanOptions, CleanupResult } from '../update/types.js';\nimport { discoverFiles as runDiscovery } from '../discovery/run.js';\n\n// ---------------------------------------------------------------------------\n// Interfaces\n// ---------------------------------------------------------------------------\n\n/**\n * A file prepared for analysis.\n */\nexport interface PreparedFile {\n  /** Absolute path to the file */\n  filePath: string;\n  /** Relative path from project root */\n  relativePath: string;\n  /** File content */\n  content: string;\n}\n\n/**\n * Analysis task for a file or directory.\n */\nexport interface AnalysisTask {\n  /** Type of task */\n  type: 'file' | 'directory';\n  /** File or directory path */\n  filePath: string;\n  /** System prompt (set for file tasks; directory prompts built at execution time) */\n  systemPrompt?: string;\n  /** User prompt (set for file tasks; directory prompts built at execution time) */\n  userPrompt?: string;\n  /** Directory info for directory tasks */\n  directoryInfo?: {\n    /** Paths of .sum files in this directory */\n    sumFiles: string[];\n    /** Number of files analyzed */\n    fileCount: number;\n  };\n}\n\n/**\n * Result of the generation planning process.\n */\nexport interface GenerationPlan {\n  /** Files to be analyzed (after skip filtering) */\n  files: PreparedFile[];\n  /** Analysis tasks to execute */\n  tasks: AnalysisTask[];\n  /** Complexity metrics */\n  complexity: ComplexityMetrics;\n  /** Compact project directory listing for bird's-eye context */\n  projectStructure?: string;\n  /** Files skipped due to existing .sum artifacts */\n  skippedFiles?: string[];\n  /** Directories skipped due to existing AGENTS.md with no dirty children */\n  skippedDirs?: string[];\n  /** All discovered files (before skip filtering, for directoryFileMap) */\n  allDiscoveredFiles?: PreparedFile[];\n}\n\n/**\n * Result of update preparation (before analysis).\n */\nexport interface UpdatePlan {\n  /** Files to analyze (added or modified) */\n  filesToAnalyze: FileChange[];\n  /** Pre-built analysis tasks with prompts (for unified execution) */\n  fileTasks: AnalysisTask[];\n  /** Files to skip (unchanged based on content hash) */\n  filesToSkip: string[];\n  /** Cleanup result (files to delete) */\n  cleanup: CleanupResult;\n  /** Directories that need AGENTS.md regeneration */\n  affectedDirs: string[];\n  /** Base commit (not used in frontmatter mode, kept for compatibility) */\n  baseCommit: string;\n  /** Current commit */\n  currentCommit: string;\n  /** Whether this is first run (no .sum files exist) */\n  isFirstRun: boolean;\n}\n\n// ---------------------------------------------------------------------------\n// Orchestrator\n// ---------------------------------------------------------------------------\n\n/**\n * Unified orchestrator for documentation generation and incremental updates.\n *\n * Provides methods for:\n * - Full project generation: createPlan() -> generates all docs\n * - Incremental updates: preparePlan() -> updates only changed files\n * - Shared task building: createFileTasks() -> builds prompts for both flows\n */\nexport class DocumentationOrchestrator {\n  private config: Config;\n  private projectRoot: string;\n  private tracer?: ITraceWriter;\n  private debug: boolean;\n  private logger: Logger;\n\n  constructor(\n    config: Config,\n    projectRoot: string,\n    options?: { tracer?: ITraceWriter; debug?: boolean; logger?: Logger }\n  ) {\n    this.config = config;\n    this.projectRoot = projectRoot;\n    this.tracer = options?.tracer;\n    this.debug = options?.debug ?? false;\n    this.logger = options?.logger ?? nullLogger;\n  }\n\n  // ===========================================================================\n  // GENERATION METHODS (for `are generate` command)\n  // ===========================================================================\n\n  /**\n   * Prepare files for analysis by reading content and detecting types.\n   */\n  async prepareFiles(discoveryResult: DiscoveryResult): Promise<PreparedFile[]> {\n    const prepared: PreparedFile[] = [];\n\n    for (let i = 0; i < discoveryResult.files.length; i++) {\n      const filePath = discoveryResult.files[i];\n      try {\n        const content = await readFile(filePath, 'utf-8');\n        const relativePath = path.relative(this.projectRoot, filePath);\n\n        prepared.push({\n          filePath,\n          relativePath,\n          content,\n        });\n      } catch {\n        // Skip files that can't be read (permission errors, etc.)\n        // Silently ignore - these files won't appear in the plan\n      }\n    }\n\n    return prepared;\n  }\n\n  /**\n   * Build a compact project structure listing from prepared files.\n   * Groups files by directory to give the AI bird's-eye context.\n   */\n  private buildProjectStructure(files: PreparedFile[]): string {\n    const byDir = new Map<string, string[]>();\n    for (const file of files) {\n      const dir = path.dirname(file.relativePath) || '.';\n      const group = byDir.get(dir) ?? [];\n      group.push(path.basename(file.relativePath));\n      byDir.set(dir, group);\n    }\n\n    const lines: string[] = [];\n    for (const [dir, dirFiles] of [...byDir.entries()].sort(([a], [b]) => a.localeCompare(b))) {\n      lines.push(`${dir}/`);\n      for (const f of dirFiles.sort()) {\n        lines.push(`  ${f}`);\n      }\n    }\n    return lines.join('\\n');\n  }\n\n  /**\n   * Filter prepared files, removing those that already have .sum artifacts.\n   */\n  async filterExistingFiles(files: PreparedFile[]): Promise<{\n    filesToProcess: PreparedFile[];\n    skippedFiles: string[];\n  }> {\n    const filesToProcess: PreparedFile[] = [];\n    const skippedFiles: string[] = [];\n\n    for (const file of files) {\n      const exists = await sumFileExists(file.filePath);\n      if (exists) {\n        skippedFiles.push(file.relativePath);\n      } else {\n        filesToProcess.push(file);\n      }\n    }\n\n    return { filesToProcess, skippedFiles };\n  }\n\n  /**\n   * Mark a directory and all its ancestors as needing regeneration.\n   */\n  private markDirtyWithAncestors(dir: string, dirtySet: Set<string>): void {\n    let current = dir;\n    while (true) {\n      dirtySet.add(current);\n      if (current === '.' || current === '') break;\n      const parent = path.dirname(current);\n      if (parent === current) break;\n      current = parent;\n    }\n  }\n\n  /**\n   * Filter directory tasks, keeping only directories that need regeneration.\n   *\n   * A directory needs regeneration if:\n   * - It has no generated AGENTS.md, OR\n   * - Any descendant file was processed in phase 1 (dirty propagation)\n   */\n  async filterExistingDirectories(\n    allFiles: PreparedFile[],\n    processedFiles: PreparedFile[],\n  ): Promise<{ dirsToProcess: Set<string>; skippedDirs: string[] }> {\n    // Directories that had files processed → dirty, must regenerate\n    const dirtyDirs = new Set<string>();\n    for (const file of processedFiles) {\n      this.markDirtyWithAncestors(path.dirname(file.relativePath), dirtyDirs);\n    }\n\n    // All directories from discovered files\n    const allDirs = new Set<string>();\n    for (const file of allFiles) {\n      allDirs.add(path.dirname(file.relativePath));\n    }\n\n    const dirsToProcess = new Set<string>();\n    const skippedDirs: string[] = [];\n\n    for (const dir of allDirs) {\n      if (dirtyDirs.has(dir)) {\n        dirsToProcess.add(dir);\n      } else {\n        // Check if generated AGENTS.md already exists\n        const agentsPath = path.join(this.projectRoot, dir, 'AGENTS.md');\n        const isGenerated = await isGeneratedAgentsMd(agentsPath);\n        if (isGenerated) {\n          skippedDirs.push(dir);\n        } else {\n          // No generated AGENTS.md → needs generation; propagate up\n          this.markDirtyWithAncestors(dir, dirsToProcess);\n        }\n      }\n    }\n\n    return { dirsToProcess, skippedDirs };\n  }\n\n  /**\n   * Create directory tasks for LLM-generated directory descriptions.\n   * These tasks run after all files in a directory are analyzed, allowing\n   * the LLM to synthesize a richer directory overview from the .sum files.\n   * Prompts are built at execution time by buildDirectoryPrompt().\n   */\n  createDirectoryTasks(files: PreparedFile[]): AnalysisTask[] {\n    const tasks: AnalysisTask[] = [];\n\n    // Group files by directory\n    const filesByDir = new Map<string, PreparedFile[]>();\n    for (const file of files) {\n      const dir = path.dirname(file.relativePath);\n      const dirFiles = filesByDir.get(dir) ?? [];\n      dirFiles.push(file);\n      filesByDir.set(dir, dirFiles);\n    }\n\n    // Create a directory task for each directory with analyzed files\n    for (const [dir, dirFiles] of Array.from(filesByDir.entries())) {\n      const sumFilePaths = dirFiles.map(f => `${f.relativePath}.sum`);\n\n      tasks.push({\n        type: 'directory',\n        filePath: dir || '.',\n        directoryInfo: {\n          sumFiles: sumFilePaths,\n          fileCount: dirFiles.length,\n        },\n      });\n    }\n\n    return tasks;\n  }\n\n  /**\n   * Create a complete generation plan.\n   *\n   * When `force` is false (default), files with existing `.sum` artifacts\n   * and directories with existing generated `AGENTS.md` are skipped.\n   * When `force` is true, all files and directories are processed.\n   */\n  async createPlan(\n    discoveryResult: DiscoveryResult,\n    options?: { force?: boolean },\n  ): Promise<GenerationPlan> {\n    const force = options?.force ?? false;\n    const planStartTime = process.hrtime.bigint();\n\n    // Emit phase start\n    this.tracer?.emit({\n      type: 'phase:start',\n      phase: 'plan-creation',\n      taskCount: discoveryResult.files.length,\n      concurrency: 1,\n    });\n\n    if (this.debug) {\n      this.logger.debug('[debug] Preparing files: reading and detecting types...');\n    }\n\n    const allFiles = await this.prepareFiles(discoveryResult);\n\n    // --- Skip filtering ---\n    let filesToProcess = allFiles;\n    let skippedFiles: string[] = [];\n    let skippedDirs: string[] = [];\n\n    if (!force) {\n      if (this.debug) {\n        this.logger.debug('[debug] Checking for existing .sum files...');\n      }\n      const fileFilter = await this.filterExistingFiles(allFiles);\n      filesToProcess = fileFilter.filesToProcess;\n      skippedFiles = fileFilter.skippedFiles;\n    }\n\n    if (this.debug) {\n      this.logger.debug('[debug] Analyzing complexity...');\n    }\n\n    const complexity = analyzeComplexity(\n      allFiles.map(f => f.filePath),\n      this.projectRoot\n    );\n\n    if (this.debug) {\n      this.logger.debug(`[debug] Complexity analysis: depth=${complexity.directoryDepth}`);\n    }\n\n    // Build project structure from ALL files (for bird's-eye context)\n    const projectStructure = this.buildProjectStructure(allFiles);\n\n    // Create file tasks only for files to process\n    const fileTasks = await this.createFileTasks(filesToProcess);\n\n    // --- Directory skip filtering ---\n    // Create directory tasks scoped to files being processed,\n    // but use allFiles for the full directory set so we can skip correctly\n    let dirTasks: AnalysisTask[];\n\n    if (!force) {\n      if (this.debug) {\n        this.logger.debug('[debug] Checking for existing AGENTS.md files...');\n      }\n      const dirFilter = await this.filterExistingDirectories(allFiles, filesToProcess);\n      skippedDirs = dirFilter.skippedDirs;\n\n      // Create directory tasks only for directories that need processing\n      // We use allFiles so the directory tasks know about ALL child .sum files\n      // (including pre-existing ones), but filter to only dirty directories\n      const allDirTasks = this.createDirectoryTasks(allFiles);\n      dirTasks = allDirTasks.filter(t => dirFilter.dirsToProcess.has(t.filePath));\n    } else {\n      dirTasks = this.createDirectoryTasks(allFiles);\n    }\n\n    const tasks = [...fileTasks, ...dirTasks];\n\n    if (this.debug) {\n      const skipMsg = skippedFiles.length > 0\n        ? `, ${skippedFiles.length} files skipped, ${skippedDirs.length} dirs skipped`\n        : '';\n      this.logger.debug(\n        `[debug] Generation plan: ${filesToProcess.length} files, ${tasks.length} tasks (${dirTasks.length} directories)${skipMsg}`\n      );\n    }\n\n    // Release file content from PreparedFile objects to free memory.\n    // Content has already been embedded into task prompts by createFileTasks()\n    // and is no longer needed. The runner re-reads files from disk.\n    for (const file of filesToProcess) {\n      (file as { content: string }).content = '';\n    }\n    for (const file of allFiles) {\n      (file as { content: string }).content = '';\n    }\n\n    const plan: GenerationPlan = {\n      files: filesToProcess,\n      tasks,\n      complexity,\n      projectStructure,\n      skippedFiles: skippedFiles.length > 0 ? skippedFiles : undefined,\n      skippedDirs: skippedDirs.length > 0 ? skippedDirs : undefined,\n      allDiscoveredFiles: allFiles.length !== filesToProcess.length ? allFiles : undefined,\n    };\n\n    // Emit plan created event\n    this.tracer?.emit({\n      type: 'plan:created',\n      planType: 'generate',\n      fileCount: filesToProcess.length,\n      taskCount: tasks.length,\n    });\n\n    // Emit phase end\n    const planEndTime = process.hrtime.bigint();\n    const planDurationMs = Number(planEndTime - planStartTime) / 1_000_000;\n    this.tracer?.emit({\n      type: 'phase:end',\n      phase: 'plan-creation',\n      durationMs: planDurationMs,\n      tasksCompleted: 1,\n      tasksFailed: 0,\n    });\n\n    return plan;\n  }\n\n  // ===========================================================================\n  // UPDATE METHODS (for `are update` command)\n  // ===========================================================================\n\n  /**\n   * Close resources (no-op in frontmatter mode, kept for API compatibility).\n   */\n  close(): void {\n    // No database to close in frontmatter mode\n  }\n\n  /**\n   * Check prerequisites for update.\n   *\n   * @throws Error if not in a git repository\n   */\n  async checkPrerequisites(): Promise<void> {\n    const isRepo = await isGitRepo(this.projectRoot);\n    if (!isRepo) {\n      throw new Error(\n        `Not a git repository: ${this.projectRoot}\\n` +\n        'The update command requires a git repository for change detection.'\n      );\n    }\n  }\n\n  /**\n   * Discover all source files in the project.\n   */\n  private async discoverFiles(): Promise<string[]> {\n    const filterResult = await runDiscovery(this.projectRoot, this.config, {\n      tracer: this.tracer,\n      debug: this.debug,\n    });\n\n    // Walker returns absolute paths; convert to relative for consistent usage\n    return filterResult.included.map((f: string) => path.relative(this.projectRoot, f));\n  }\n\n  /**\n   * Prepare update plan without executing analysis.\n   *\n   * Uses frontmatter-based change detection:\n   * - Reads content_hash from each .sum file\n   * - Compares with current file content hash\n   * - Files with mismatched hashes need re-analysis\n   *\n   * @param options - Update options\n   * @returns Update plan with files to analyze and cleanup actions\n   */\n  async preparePlan(options: UpdatePlanOptions = {}): Promise<UpdatePlan> {\n    const planStartTime = process.hrtime.bigint();\n\n    // Emit phase start\n    this.tracer?.emit({\n      type: 'phase:start',\n      phase: 'update-plan-creation',\n      taskCount: 0, // Will be determined after discovery\n      concurrency: 1,\n    });\n\n    if (this.debug) {\n      this.logger.debug('[debug] Creating update plan with change detection...');\n    }\n\n    await this.checkPrerequisites();\n\n    // Get current commit for reference\n    const currentCommit = await getCurrentCommit(this.projectRoot);\n\n    if (this.debug) {\n      this.logger.debug(`[debug] Git commit: ${currentCommit.slice(0, 7)}`);\n    }\n\n    // Discover all source files\n    if (this.debug) {\n      this.logger.debug('[debug] Discovering files...');\n    }\n\n    const allFiles = await this.discoverFiles();\n\n    const filesToAnalyze: FileChange[] = [];\n    const filesToSkip: string[] = [];\n    const deletedOrRenamed: FileChange[] = [];\n\n    // Track which .sum files we've seen (to detect orphans)\n    const seenSumFiles = new Set<string>();\n\n    // Check each file against its .sum file\n    const variant = options.variant;\n    for (const relativePath of allFiles) {\n      const filePath = path.join(this.projectRoot, relativePath);\n      const sumPath = getSumPath(filePath, variant);\n      seenSumFiles.add(sumPath);\n\n      try {\n        // Read existing .sum file\n        const sumContent = await readSumFile(sumPath);\n\n        if (!sumContent) {\n          // No .sum file exists - file needs analysis\n          filesToAnalyze.push({ path: relativePath, status: 'added' });\n          continue;\n        }\n\n        // Compare content hashes\n        const currentHash = await computeContentHash(filePath);\n        const storedHash = sumContent.contentHash;\n\n        if (!storedHash || storedHash !== currentHash) {\n          // Hash mismatch or no hash stored - file needs re-analysis\n          filesToAnalyze.push({ path: relativePath, status: 'modified' });\n        } else {\n          // Hash matches - skip this file\n          filesToSkip.push(relativePath);\n        }\n      } catch {\n        // Error reading file - skip it\n        filesToSkip.push(relativePath);\n      }\n    }\n\n    // Cleanup orphans (deleted files whose .sum files still exist)\n    const cleanup = await cleanupOrphans(\n      this.projectRoot,\n      deletedOrRenamed,\n      options.dryRun ?? false,\n      variant,\n    );\n\n    // Get directories affected by changes (for AGENTS.md regeneration)\n    // Sort by depth descending (deepest first) so children are processed before parents\n    const affectedDirs = Array.from(getAffectedDirectories(filesToAnalyze))\n      .sort((a, b) => {\n        const depthA = a === '.' ? 0 : a.split(path.sep).length;\n        const depthB = b === '.' ? 0 : b.split(path.sep).length;\n        return depthB - depthA;\n      });\n\n    if (this.debug) {\n      this.logger.debug(\n        `[debug] Change detection: ${filesToAnalyze.length} changed, ${filesToSkip.length} unchanged, ${cleanup.deletedSumFiles.length} orphaned`\n      );\n      this.logger.debug(`[debug] Affected directories: ${affectedDirs.length}`);\n    }\n\n    // Emit plan created event\n    this.tracer?.emit({\n      type: 'plan:created',\n      planType: 'update',\n      fileCount: filesToAnalyze.length,\n      taskCount: filesToAnalyze.length + affectedDirs.length, // File tasks + dir regen tasks\n    });\n\n    // Emit phase end\n    const planEndTime = process.hrtime.bigint();\n    const planDurationMs = Number(planEndTime - planStartTime) / 1_000_000;\n    this.tracer?.emit({\n      type: 'phase:end',\n      phase: 'update-plan-creation',\n      durationMs: planDurationMs,\n      tasksCompleted: 1,\n      tasksFailed: 0,\n    });\n\n    // Determine if this is first run (no files to skip means no existing .sum files)\n    const isFirstRun = filesToSkip.length === 0 && filesToAnalyze.length > 0;\n\n    // Build analysis tasks with prompts for files that need re-analysis\n    // (Skip if dry run or first run - no point building tasks we won't execute)\n    const fileTasks = options.dryRun || isFirstRun\n      ? []\n      : await this.createFileTasks(filesToAnalyze, variant);\n\n    return {\n      filesToAnalyze,\n      fileTasks,\n      filesToSkip,\n      cleanup,\n      affectedDirs,\n      baseCommit: currentCommit, // Not used in frontmatter mode\n      currentCommit,\n      isFirstRun,\n    };\n  }\n\n  /**\n   * Record file analyzed (no-op in frontmatter mode - hash is stored in .sum file).\n   * Kept for API compatibility.\n   */\n  async recordFileAnalyzed(\n    _relativePath: string,\n    _contentHash: string,\n    _currentCommit: string\n  ): Promise<void> {\n    // No-op: content hash is stored in .sum file frontmatter\n  }\n\n  /**\n   * Remove file from state (no-op in frontmatter mode).\n   * Kept for API compatibility.\n   */\n  async removeFileState(_relativePath: string): Promise<void> {\n    // No-op: .sum file cleanup is handled separately\n  }\n\n  /**\n   * Record a completed update run (no-op in frontmatter mode).\n   * Kept for API compatibility.\n   */\n  async recordRun(\n    _commitHash: string,\n    _filesAnalyzed: number,\n    _filesSkipped: number\n  ): Promise<number> {\n    // No-op: no run history in frontmatter mode\n    return 0;\n  }\n\n  /**\n   * Get last run information (not available in frontmatter mode).\n   * Kept for API compatibility.\n   */\n  async getLastRun(): Promise<undefined> {\n    // No run history in frontmatter mode\n    return undefined;\n  }\n\n  /**\n   * Check if this is the first run.\n   * In frontmatter mode, checks if any .sum files exist.\n   */\n  async isFirstRun(): Promise<boolean> {\n    const plan = await this.preparePlan({ dryRun: true });\n    return plan.isFirstRun;\n  }\n\n  // ===========================================================================\n  // SHARED METHODS (used by both generate and update)\n  // ===========================================================================\n\n  /**\n   * Create analysis tasks for files.\n   * Pre-builds prompts with optional existing .sum content for incremental updates.\n   *\n   * Used by both generate (PreparedFile[]) and update (FileChange[]) workflows.\n   *\n   * @param files - Files to create tasks for (PreparedFile[] or FileChange[])\n   * @returns Array of analysis tasks with pre-built prompts\n   */\n  async createFileTasks(\n    files: PreparedFile[] | FileChange[],\n    variant?: string,\n  ): Promise<AnalysisTask[]> {\n    const tasks: AnalysisTask[] = [];\n\n    for (const file of files) {\n      // Handle both PreparedFile (generate) and FileChange (update)\n      const isFileChange = 'path' in file && 'status' in file;\n      const filePath = isFileChange ? (file as FileChange).path : (file as PreparedFile).relativePath;\n      const absolutePath = isFileChange\n        ? path.join(this.projectRoot, (file as FileChange).path)\n        : (file as PreparedFile).filePath;\n\n      // Read content (already loaded for PreparedFile, need to load for FileChange)\n      const content = isFileChange\n        ? await readFile(absolutePath, 'utf-8')\n        : (file as PreparedFile).content;\n\n      // For updates, read existing .sum for incremental update context\n      const existingSumContent = isFileChange\n        ? await readSumFile(getSumPath(absolutePath, variant))\n        : undefined;\n\n      // Build prompt (same logic for both, with optional existingSum for updates)\n      const prompt = buildFilePrompt({\n        filePath,\n        content,\n        existingSum: existingSumContent?.summary,\n        sourceFileSize: content.length,\n        compressionRatio: this.config.generation.compressionRatio,\n      }, this.debug);\n\n      tasks.push({\n        type: 'file',\n        filePath,\n        systemPrompt: prompt.system,\n        userPrompt: prompt.user,\n      });\n    }\n\n    return tasks;\n  }\n}\n\n// ---------------------------------------------------------------------------\n// Factory functions\n// ---------------------------------------------------------------------------\n\n/**\n * Create a documentation orchestrator.\n *\n * This is the unified orchestrator for both generation and update workflows.\n */\nexport function createOrchestrator(\n  config: Config,\n  projectRoot: string,\n  options?: { tracer?: ITraceWriter; debug?: boolean; logger?: Logger }\n): DocumentationOrchestrator {\n  return new DocumentationOrchestrator(config, projectRoot, options);\n}\n\n/**\n * Alias for createOrchestrator (for update command compatibility).\n */\nexport function createUpdateOrchestrator(\n  config: Config,\n  projectRoot: string,\n  options?: { tracer?: ITraceWriter; debug?: boolean; logger?: Logger }\n): DocumentationOrchestrator {\n  return new DocumentationOrchestrator(config, projectRoot, options);\n}\n\n```\n\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n- Workflow & convention rules: if this file defines contribution guidelines, PR conventions, commit standards, testing mandates, tool usage requirements, approval workflows, AI agent behavioral instructions, or code conventions (naming standards, formatting rules, import ordering, linting policies), extract them as explicit, actionable rules. Sources: CONTRIBUTING.md, CI configs, PR templates, style guides, linter/formatter configs (.eslintrc, .prettierrc, .editorconfig), tsconfig strictness, README dev sections.\n\n\nTARGET LENGTH (MANDATORY):\n- Source file: 24881 characters\n- Target summary: ~2488 characters (10% compression)\n- Maximum: 2986 characters\n- Achieve this by: ultra-dense writing, minimal examples, single-sentence descriptions\n\nULTRA-COMPRESSION TECHNIQUES:\n- Use telegraphic style: \"exports foo, bar, baz\" not \"this file exports three functions\"\n- No code examples unless critical to understanding\n- One sentence per export maximum\n- Omit obvious parameter names (\"opts: Options\" not \"options: Options - configuration object\")\n- Use abbreviations: \"params\" not \"parameters\", \"config\" not \"configuration\"\n- Skip section if empty (don't write \"No special concerns\" or \"N/A\")\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nWORKFLOW & CONVENTION RULES (NEVER EXCLUDE):\n- Contribution requirements: branch naming, commit message formats, PR description templates\n- Testing mandates: required test types, coverage thresholds, \"never delete tests\" policies\n- Tool usage rules: required CLI commands, browser automation tools, linter/formatter requirements\n- Approval workflows: review requirements, CI checks that must pass, deployment gates\n- AI agent instructions: explicit directives for automated assistants (coding rules, prohibited actions)\n- Code conventions: naming standards (camelCase, PascalCase, snake_case for what), import ordering rules, file/folder naming patterns, formatting rules (indentation, quotes, semicolons), language idioms, TypeScript strictness policies, preferred patterns vs anti-patterns\nThese define how contributors (human or AI) must work within the project.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion .annex.sum file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**DocumentationOrchestrator unifies generation and incremental update workflows for AI-friendly documentation, providing two main execution paths (full project analysis vs. change-detection-driven updates) with shared task-building logic.**\n\n## Exported Types\n\n`PreparedFile` — file with absolute path, relative path, and content string for generation workflow.\n\n`AnalysisTask` — task object (type: 'file' | 'directory') with filePath, optional systemPrompt/userPrompt for file tasks, optional directoryInfo (sumFiles array, fileCount) for directory tasks.\n\n`GenerationPlan` — plan result containing files[], tasks[], ComplexityMetrics, optional projectStructure string, optional skippedFiles/skippedDirs arrays, optional allDiscoveredFiles[].\n\n`UpdatePlan` — plan result containing filesToAnalyze (FileChange[]), fileTasks (AnalysisTask[]), filesToSkip[], cleanup (CleanupResult), affectedDirs[], baseCommit, currentCommit, isFirstRun (boolean).\n\n## Exported Classes\n\n`DocumentationOrchestrator` — unified orchestrator with generation methods (prepareFiles, filterExistingFiles, filterExistingDirectories, createDirectoryTasks, createPlan), update methods (checkPrerequisites, preparePlan, recordFileAnalyzed, removeFileState, recordRun, getLastRun, isFirstRun), and shared method (createFileTasks). Constructor accepts Config, projectRoot string, and optional options: { tracer?: ITraceWriter; debug?: boolean; logger?: Logger }.\n\n## Exported Factory Functions\n\n`createOrchestrator(config: Config, projectRoot: string, options?: {...}): DocumentationOrchestrator` — factory for generation and update workflows.\n\n`createUpdateOrchestrator(config: Config, projectRoot: string, options?: {...}): DocumentationOrchestrator` — alias for update command compatibility.\n\n## Generation Workflow (are generate)\n\n`createPlan(discoveryResult: DiscoveryResult, options?: { force?: boolean }): Promise<GenerationPlan>` — orchestrates full project documentation generation. Calls prepareFiles() to load all discovered files, optionally filters by existing .sum artifacts (unless force=true), analyzes complexity via analyzeComplexity(), builds projectStructure string via buildProjectStructure(), creates fileTasks via createFileTasks(), filters directories via filterExistingDirectories() and createDirectoryTasks(), combines file and directory tasks. Emits tracer events: phase:start (plan-creation), plan:created (planType:'generate'), phase:end. Memory optimization: clears file.content after embedding into prompts.\n\n`prepareFiles(discoveryResult: DiscoveryResult): Promise<PreparedFile[]>` — reads file contents from disk, silently skips files with read errors, returns PreparedFile array with filePath, relativePath, content.\n\n`buildProjectStructure(files: PreparedFile[]): string` — groups files by directory (relative paths), returns newline-delimited listing with directory headers and indented filenames for bird's-eye context.\n\n`filterExistingFiles(files: PreparedFile[]): Promise<{ filesToProcess, skippedFiles[] }>` — checks sumFileExists() for each file; skipped files still appear in projectStructure for context but not in analysis tasks.\n\n`filterExistingDirectories(allFiles: PreparedFile[], processedFiles: PreparedFile[]): Promise<{ dirsToProcess: Set<string>, skippedDirs[] }>` — propagates dirty state from processed files up to ancestors via markDirtyWithAncestors(); checks isGeneratedAgentsMd() for unprocessed directories; returns only directories needing regeneration.\n\n`createDirectoryTasks(files: PreparedFile[]): AnalysisTask[]` — groups PreparedFile[] by directory, creates one directory task per directory with sumFiles array (constructed as relativePath.sum) and fileCount.\n\n## Update Workflow (are update)\n\n`preparePlan(options?: UpdatePlanOptions): Promise<UpdatePlan>` — change-detection-driven planning using frontmatter hashes. Calls checkPrerequisites() (throws if not git repo), getCurrentCommit(), discoverFiles(), iterates each file to read .sum via readSumFile(), computes currentHash via computeContentHash(), compares with stored content_hash from .sum frontmatter. Files with missing/mismatched hashes marked as 'added' or 'modified' status. Calls cleanupOrphans() for deleted files, getAffectedDirectories() for AGENTS.md regeneration. Emits tracer events: phase:start (update-plan-creation), plan:created (planType:'update'), phase:end. Sets isFirstRun=true if filesToSkip.length===0 and filesToAnalyze.length>0.\n\n`checkPrerequisites(): Promise<void>` — throws Error if not in git repository (required for change detection).\n\n`discoverFiles(): Promise<string[]>` — calls runDiscovery() from ../discovery/run.js, returns relative paths.\n\n`recordFileAnalyzed, removeFileState, recordRun, getLastRun` — no-op methods; frontmatter stores hashes directly in .sum files, not in database.\n\n`isFirstRun(): Promise<boolean>` — calls preparePlan({ dryRun: true }) and checks plan.isFirstRun.\n\n## Shared Task Creation\n\n`createFileTasks(files: PreparedFile[] | FileChange[], variant?: string): Promise<AnalysisTask[]>` — polymorphic: handles both PreparedFile (generation) and FileChange (update) arrays. For each file, reads content (cached for PreparedFile, loaded from disk for FileChange), optionally reads existing .sum via readSumFile(getSumPath(absolutePath, variant)) for incremental context, calls buildFilePrompt({ filePath, content, existingSum?: sumContent.summary, sourceFileSize: content.length, compressionRatio: config.generation.compressionRatio }, debug) to generate prompts, returns AnalysisTask[] with type:'file', filePath, systemPrompt, userPrompt.\n\n## Integration Points\n\n- Imports from ../generation/prompts (buildFilePrompt), ../generation/complexity (analyzeComplexity, ComplexityMetrics), ../generation/writers (sumFileExists, isGeneratedAgentsMd, readSumFile, getSumPath).\n- Imports from ../change-detection (isGitRepo, getCurrentCommit, computeContentHash, FileChange).\n- Imports from ../update (cleanupOrphans, getAffectedDirectories, UpdatePlanOptions, CleanupResult).\n- Imports from ../discovery/run (discoverFiles as runDiscovery).\n- Uses Config type for generation.compressionRatio setting.\n- Accepts optional ITraceWriter for event emission (phase:start, phase:end, plan:created).\n- Accepts optional Logger (defaults to nullLogger) for debug output.\n\n## State & Lifecycle\n\nConstructor stores config, projectRoot, tracer, debug flag, logger. close() is no-op (no database). Frontmatter-based hash storage in .sum files eliminates need for persistent state tracking between runs.\n\n## Debug Tracing\n\nWhen debug=true, logs via logger.debug() with [debug] prefix at: file preparation, .sum check, complexity analysis, AGENTS.md check, plan summary. Tracer emits structured events throughout plan creation (phase:start, plan:created, phase:end with durationMs, tasksCompleted, tasksFailed).",
      "model": "claude-haiku-4-5-20251001",
      "inputTokens": 3,
      "outputTokens": 1770,
      "cacheReadTokens": 16147,
      "cacheCreationTokens": 9778,
      "latencyMs": 36302,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/orchestration/orchestrator.ts",
          "sizeBytes": 24885
        }
      ]
    },
    {
      "timestamp": "2026-02-14T13:47:51.333Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: src/quality/index.ts\n\n```typescript\n/**\n * Public API for the quality analysis module.\n *\n * Re-exports all quality analysis types and functions from the\n * inconsistency detection and density validation submodules.\n *\n * @module\n */\n\n// ---------------------------------------------------------------------------\n// Shared types\n// ---------------------------------------------------------------------------\n\nexport type {\n  InconsistencySeverity,\n  CodeDocInconsistency,\n  CodeCodeInconsistency,\n  PhantomPathInconsistency,\n  Inconsistency,\n  InconsistencyReport,\n} from './types.js';\n\n// ---------------------------------------------------------------------------\n// Inconsistency detection: code-vs-doc\n// ---------------------------------------------------------------------------\n\nexport { extractExports, checkCodeVsDoc } from './inconsistency/code-vs-doc.js';\n\n// ---------------------------------------------------------------------------\n// Inconsistency detection: code-vs-code\n// ---------------------------------------------------------------------------\n\nexport { checkCodeVsCode } from './inconsistency/code-vs-code.js';\n\n// ---------------------------------------------------------------------------\n// Inconsistency reporting\n// ---------------------------------------------------------------------------\n\nexport { buildInconsistencyReport, formatReportForCli, formatReportAsMarkdown } from './inconsistency/reporter.js';\n\n// ---------------------------------------------------------------------------\n// Phantom path detection\n// ---------------------------------------------------------------------------\n\nexport { checkPhantomPaths } from './phantom-paths/index.js';\n\n\n```\n\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n- Workflow & convention rules: if this file defines contribution guidelines, PR conventions, commit standards, testing mandates, tool usage requirements, approval workflows, AI agent behavioral instructions, or code conventions (naming standards, formatting rules, import ordering, linting policies), extract them as explicit, actionable rules. Sources: CONTRIBUTING.md, CI configs, PR templates, style guides, linter/formatter configs (.eslintrc, .prettierrc, .editorconfig), tsconfig strictness, README dev sections.\n\n\nTARGET LENGTH (MANDATORY):\n- Source file: 1652 characters\n- Target summary: ~165 characters (10% compression)\n- Maximum: 198 characters\n- Achieve this by: ultra-dense writing, minimal examples, single-sentence descriptions\n\nULTRA-COMPRESSION TECHNIQUES:\n- Use telegraphic style: \"exports foo, bar, baz\" not \"this file exports three functions\"\n- No code examples unless critical to understanding\n- One sentence per export maximum\n- Omit obvious parameter names (\"opts: Options\" not \"options: Options - configuration object\")\n- Use abbreviations: \"params\" not \"parameters\", \"config\" not \"configuration\"\n- Skip section if empty (don't write \"No special concerns\" or \"N/A\")\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nWORKFLOW & CONVENTION RULES (NEVER EXCLUDE):\n- Contribution requirements: branch naming, commit message formats, PR description templates\n- Testing mandates: required test types, coverage thresholds, \"never delete tests\" policies\n- Tool usage rules: required CLI commands, browser automation tools, linter/formatter requirements\n- Approval workflows: review requirements, CI checks that must pass, deployment gates\n- AI agent instructions: explicit directives for automated assistants (coding rules, prohibited actions)\n- Code conventions: naming standards (camelCase, PascalCase, snake_case for what), import ordering rules, file/folder naming patterns, formatting rules (indentation, quotes, semicolons), language idioms, TypeScript strictness policies, preferred patterns vs anti-patterns\nThese define how contributors (human or AI) must work within the project.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion .annex.sum file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**src/quality/index.ts is the public API barrel export for quality analysis modules (inconsistency detection, density validation, phantom path detection).**\n\n## Exported Types\n`InconsistencySeverity`, `CodeDocInconsistency`, `CodeCodeInconsistency`, `PhantomPathInconsistency`, `Inconsistency`, `InconsistencyReport` — inconsistency classification and report structures.\n\n## Exported Functions\n`extractExports()`—extracts code exports for comparison; `checkCodeVsDoc()` — detects code-documentation mismatches; `checkCodeVsCode()` — detects code-to-code inconsistencies; `buildInconsistencyReport()` — constructs inconsistency reports; `formatReportForCli()`, `formatReportAsMarkdown()` — report formatting; `checkPhantomPaths()` — detects phantom path references.\n\n## Module Organization\nRe-exports from `./inconsistency/code-vs-doc.js`, `./inconsistency/code-vs-code.js`, `./inconsistency/reporter.js`, `./phantom-paths/index.js`; types from `./types.js`. Barrel pattern centralizes quality analysis API.",
      "model": "claude-haiku-4-5-20251001",
      "inputTokens": 3,
      "outputTokens": 288,
      "cacheReadTokens": 16147,
      "cacheCreationTokens": 3040,
      "latencyMs": 22883,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/quality/index.ts",
          "sizeBytes": 1652
        }
      ]
    },
    {
      "timestamp": "2026-02-14T13:47:51.681Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: src/quality/types.ts\n\n```typescript\n/**\n * Shared types for quality analysis: inconsistency detection and density measurement.\n */\n\n/** Severity level for detected inconsistencies. */\nexport type InconsistencySeverity = 'info' | 'warning' | 'error';\n\n/**\n * Inconsistency between .sum documentation and source code.\n * Detected by comparing exported symbols against .sum content.\n */\nexport interface CodeDocInconsistency {\n  type: 'code-vs-doc';\n  severity: InconsistencySeverity;\n  /** Path to the source file */\n  filePath: string;\n  /** Path to the corresponding .sum file */\n  sumPath: string;\n  description: string;\n  details: {\n    /** Symbols exported in source but not mentioned in .sum */\n    missingFromDoc: string[];\n    /** Symbols mentioned in .sum but not found in source */\n    missingFromCode: string[];\n    /** Purpose statement that contradicts observable behavior */\n    purposeMismatch?: string;\n  };\n}\n\n/**\n * Inconsistency across multiple source files.\n * Detected by comparing patterns and exports across files.\n */\nexport interface CodeCodeInconsistency {\n  type: 'code-vs-code';\n  severity: InconsistencySeverity;\n  /** Paths to the conflicting files */\n  files: string[];\n  description: string;\n  /** Pattern that was detected (e.g. 'duplicate-export') */\n  pattern: string;\n}\n\n/**\n * Path reference in generated documentation that doesn't resolve to a real file/directory.\n */\nexport interface PhantomPathInconsistency {\n  type: 'phantom-path';\n  severity: InconsistencySeverity;\n  /** Path to the AGENTS.md file containing the phantom reference */\n  agentsMdPath: string;\n  description: string;\n  details: {\n    /** The phantom path as written in the document */\n    referencedPath: string;\n    /** What it was resolved against (project root or AGENTS.md location) */\n    resolvedTo: string;\n    /** The line of text containing the phantom reference */\n    context: string;\n  };\n}\n\n/** Union of all inconsistency types. */\nexport type Inconsistency = CodeDocInconsistency | CodeCodeInconsistency | PhantomPathInconsistency;\n\n/** Structured report produced by inconsistency analysis. */\nexport interface InconsistencyReport {\n  /** Run metadata */\n  metadata: {\n    timestamp: string;\n    projectRoot: string;\n    filesChecked: number;\n    durationMs: number;\n  };\n  /** All detected inconsistencies */\n  issues: Inconsistency[];\n  /** Summary counts */\n  summary: {\n    total: number;\n    codeVsDoc: number;\n    codeVsCode: number;\n    phantomPaths: number;\n    errors: number;\n    warnings: number;\n    info: number;\n  };\n}\n\n```\n\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n- Workflow & convention rules: if this file defines contribution guidelines, PR conventions, commit standards, testing mandates, tool usage requirements, approval workflows, AI agent behavioral instructions, or code conventions (naming standards, formatting rules, import ordering, linting policies), extract them as explicit, actionable rules. Sources: CONTRIBUTING.md, CI configs, PR templates, style guides, linter/formatter configs (.eslintrc, .prettierrc, .editorconfig), tsconfig strictness, README dev sections.\n\n\nTARGET LENGTH (MANDATORY):\n- Source file: 2518 characters\n- Target summary: ~252 characters (10% compression)\n- Maximum: 302 characters\n- Achieve this by: ultra-dense writing, minimal examples, single-sentence descriptions\n\nULTRA-COMPRESSION TECHNIQUES:\n- Use telegraphic style: \"exports foo, bar, baz\" not \"this file exports three functions\"\n- No code examples unless critical to understanding\n- One sentence per export maximum\n- Omit obvious parameter names (\"opts: Options\" not \"options: Options - configuration object\")\n- Use abbreviations: \"params\" not \"parameters\", \"config\" not \"configuration\"\n- Skip section if empty (don't write \"No special concerns\" or \"N/A\")\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nWORKFLOW & CONVENTION RULES (NEVER EXCLUDE):\n- Contribution requirements: branch naming, commit message formats, PR description templates\n- Testing mandates: required test types, coverage thresholds, \"never delete tests\" policies\n- Tool usage rules: required CLI commands, browser automation tools, linter/formatter requirements\n- Approval workflows: review requirements, CI checks that must pass, deployment gates\n- AI agent instructions: explicit directives for automated assistants (coding rules, prohibited actions)\n- Code conventions: naming standards (camelCase, PascalCase, snake_case for what), import ordering rules, file/folder naming patterns, formatting rules (indentation, quotes, semicolons), language idioms, TypeScript strictness policies, preferred patterns vs anti-patterns\nThese define how contributors (human or AI) must work within the project.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion .annex.sum file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**types.ts defines inconsistency detection types for quality analysis: CodeDocInconsistency, CodeCodeInconsistency, PhantomPathInconsistency, and InconsistencyReport.**\n\n## Exported Types\n\n`InconsistencySeverity` — union type: `'info' | 'warning' | 'error'`.\n\n`CodeDocInconsistency` — interface tracking .sum/source mismatches; fields: type `'code-vs-doc'`, severity, filePath, sumPath, description, details (missingFromDoc[], missingFromCode[], purposeMismatch?).\n\n`CodeCodeInconsistency` — interface for cross-file pattern conflicts; fields: type `'code-vs-code'`, severity, files[], description, pattern string.\n\n`PhantomPathInconsistency` — interface for invalid doc references; fields: type `'phantom-path'`, severity, agentsMdPath, description, details (referencedPath, resolvedTo, context).\n\n`Inconsistency` — union: CodeDocInconsistency | CodeCodeInconsistency | PhantomPathInconsistency.\n\n`InconsistencyReport` — aggregated findings; fields: metadata (timestamp, projectRoot, filesChecked, durationMs), issues[], summary (total, codeVsDoc, codeVsCode, phantomPaths, errors, warnings, info counts).",
      "model": "claude-haiku-4-5-20251001",
      "inputTokens": 3,
      "outputTokens": 327,
      "cacheReadTokens": 16147,
      "cacheCreationTokens": 3454,
      "latencyMs": 23738,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/quality/types.ts",
          "sizeBytes": 2518
        }
      ]
    },
    {
      "timestamp": "2026-02-14T13:47:51.978Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: src/rebuild/checkpoint.ts\n\n```typescript\n/**\n * Checkpoint manager for rebuild session continuity.\n *\n * Persists per-module completion status and spec file hashes inside the\n * output directory. Uses promise-chain write serialization (same pattern\n * as PlanTracker) to handle concurrent pool worker updates safely.\n *\n * @module\n */\n\nimport { writeFile, readFile, mkdir, rm } from 'node:fs/promises';\nimport * as path from 'node:path';\nimport { computeContentHashFromString } from '../change-detection/index.js';\nimport { RebuildCheckpointSchema, type RebuildCheckpoint } from './types.js';\nimport { getVersion } from '../version.js';\n\n/**\n * Manages rebuild checkpoint state for session continuity.\n *\n * Create via the static `load()` or `createFresh()` factory methods.\n * Call `markDone()` / `markFailed()` as modules complete, and `flush()`\n * before returning to ensure all writes finish.\n */\nexport class CheckpointManager {\n  private data: RebuildCheckpoint;\n  private readonly checkpointPath: string;\n  private writeQueue: Promise<void> = Promise.resolve();\n\n  constructor(outputDir: string, initialData: RebuildCheckpoint) {\n    this.checkpointPath = path.join(outputDir, '.rebuild-checkpoint');\n    this.data = initialData;\n  }\n\n  /**\n   * Load an existing checkpoint or create a new one.\n   *\n   * If a checkpoint file exists and is valid, checks for spec drift by\n   * comparing stored hashes against current spec file content hashes.\n   * Returns `isResume: true` only if the checkpoint is valid and specs\n   * haven't changed.\n   *\n   * @param outputDir - Absolute path to the output directory\n   * @param specFiles - Current spec files with content for drift detection\n   * @param unitNames - Names of all rebuild units (for fresh checkpoint initialization)\n   * @returns CheckpointManager instance and whether this is a resume\n   */\n  static async load(\n    outputDir: string,\n    specFiles: Array<{ relativePath: string; content: string }>,\n    unitNames: string[],\n  ): Promise<{ manager: CheckpointManager; isResume: boolean }> {\n    const checkpointPath = path.join(outputDir, '.rebuild-checkpoint');\n\n    let raw: string;\n    try {\n      raw = await readFile(checkpointPath, 'utf-8');\n    } catch {\n      // No checkpoint file -- create fresh\n      const manager = CheckpointManager.createFresh(outputDir, specFiles, unitNames);\n      return { manager, isResume: false };\n    }\n\n    // Parse and validate\n    let parsed: unknown;\n    try {\n      parsed = JSON.parse(raw);\n    } catch {\n      // Corrupted JSON -- create fresh\n      const manager = CheckpointManager.createFresh(outputDir, specFiles, unitNames);\n      return { manager, isResume: false };\n    }\n\n    const result = RebuildCheckpointSchema.safeParse(parsed);\n    if (!result.success) {\n      // Schema validation failed -- create fresh\n      const manager = CheckpointManager.createFresh(outputDir, specFiles, unitNames);\n      return { manager, isResume: false };\n    }\n\n    const checkpoint = result.data;\n\n    // Check for spec drift\n    const currentHashes: Record<string, string> = {};\n    for (const spec of specFiles) {\n      currentHashes[spec.relativePath] = computeContentHashFromString(spec.content);\n    }\n\n    // Compare hash counts (files added or removed)\n    const storedPaths = Object.keys(checkpoint.specHashes);\n    const currentPaths = Object.keys(currentHashes);\n    if (storedPaths.length !== currentPaths.length) {\n      const manager = CheckpointManager.createFresh(outputDir, specFiles, unitNames);\n      return { manager, isResume: false };\n    }\n\n    // Compare individual hashes\n    for (const specPath of currentPaths) {\n      if (checkpoint.specHashes[specPath] !== currentHashes[specPath]) {\n        const manager = CheckpointManager.createFresh(outputDir, specFiles, unitNames);\n        return { manager, isResume: false };\n      }\n    }\n\n    // Valid checkpoint, no drift -- resume\n    const manager = new CheckpointManager(outputDir, checkpoint);\n    return { manager, isResume: true };\n  }\n\n  /**\n   * Create a fresh checkpoint with all modules set to pending.\n   *\n   * @param outputDir - Absolute path to the output directory\n   * @param specFiles - Spec files with content for hash computation\n   * @param unitNames - Names of all rebuild units\n   * @returns New CheckpointManager instance\n   */\n  static createFresh(\n    outputDir: string,\n    specFiles: Array<{ relativePath: string; content: string }>,\n    unitNames: string[],\n  ): CheckpointManager {\n    const specHashes: Record<string, string> = {};\n    for (const spec of specFiles) {\n      specHashes[spec.relativePath] = computeContentHashFromString(spec.content);\n    }\n\n    const modules: RebuildCheckpoint['modules'] = {};\n    for (const name of unitNames) {\n      modules[name] = { status: 'pending' };\n    }\n\n    const now = new Date().toISOString();\n    const data: RebuildCheckpoint = {\n      version: getVersion(),\n      createdAt: now,\n      updatedAt: now,\n      outputDir,\n      specHashes,\n      modules,\n    };\n\n    return new CheckpointManager(outputDir, data);\n  }\n\n  /**\n   * Mark a unit as successfully completed.\n   *\n   * Queues a serialized write to the checkpoint file.\n   */\n  markDone(unitName: string, filesWritten: string[]): void {\n    this.data.modules[unitName] = {\n      status: 'done',\n      completedAt: new Date().toISOString(),\n      filesWritten,\n    };\n    this.data.updatedAt = new Date().toISOString();\n    this.queueWrite();\n  }\n\n  /**\n   * Mark a unit as failed.\n   *\n   * Queues a serialized write to the checkpoint file.\n   */\n  markFailed(unitName: string, error: string): void {\n    this.data.modules[unitName] = {\n      status: 'failed',\n      error,\n    };\n    this.data.updatedAt = new Date().toISOString();\n    this.queueWrite();\n  }\n\n  /**\n   * Get names of units that are pending or failed (eligible for execution).\n   */\n  getPendingUnits(): string[] {\n    return Object.entries(this.data.modules)\n      .filter(([, mod]) => mod.status === 'pending' || mod.status === 'failed')\n      .map(([name]) => name);\n  }\n\n  /**\n   * Check if a unit has been completed.\n   */\n  isDone(unitName: string): boolean {\n    return this.data.modules[unitName]?.status === 'done';\n  }\n\n  /**\n   * Wait for all queued writes to finish.\n   */\n  async flush(): Promise<void> {\n    await this.writeQueue;\n  }\n\n  /**\n   * Create the output directory if needed and write the initial checkpoint file.\n   */\n  async initialize(): Promise<void> {\n    try {\n      await mkdir(path.dirname(this.checkpointPath), { recursive: true });\n      await writeFile(\n        this.checkpointPath,\n        JSON.stringify(this.data, null, 2),\n        'utf-8',\n      );\n    } catch {\n      // Non-critical -- rebuild continues without persistent checkpoint\n    }\n  }\n\n  /**\n   * Return the current checkpoint data (for dry-run display or inspection).\n   */\n  getData(): RebuildCheckpoint {\n    return this.data;\n  }\n\n  /**\n   * Queue a serialized write to prevent file corruption from concurrent calls.\n   *\n   * Follows the PlanTracker promise-chain pattern.\n   */\n  private queueWrite(): void {\n    this.writeQueue = this.writeQueue\n      .then(() =>\n        writeFile(\n          this.checkpointPath,\n          JSON.stringify(this.data, null, 2),\n          'utf-8',\n        ),\n      )\n      .catch(() => {\n        /* non-critical */\n      });\n  }\n}\n\n```\n\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n- Workflow & convention rules: if this file defines contribution guidelines, PR conventions, commit standards, testing mandates, tool usage requirements, approval workflows, AI agent behavioral instructions, or code conventions (naming standards, formatting rules, import ordering, linting policies), extract them as explicit, actionable rules. Sources: CONTRIBUTING.md, CI configs, PR templates, style guides, linter/formatter configs (.eslintrc, .prettierrc, .editorconfig), tsconfig strictness, README dev sections.\n\n\nTARGET LENGTH (MANDATORY):\n- Source file: 7304 characters\n- Target summary: ~730 characters (10% compression)\n- Maximum: 876 characters\n- Achieve this by: ultra-dense writing, minimal examples, single-sentence descriptions\n\nULTRA-COMPRESSION TECHNIQUES:\n- Use telegraphic style: \"exports foo, bar, baz\" not \"this file exports three functions\"\n- No code examples unless critical to understanding\n- One sentence per export maximum\n- Omit obvious parameter names (\"opts: Options\" not \"options: Options - configuration object\")\n- Use abbreviations: \"params\" not \"parameters\", \"config\" not \"configuration\"\n- Skip section if empty (don't write \"No special concerns\" or \"N/A\")\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nWORKFLOW & CONVENTION RULES (NEVER EXCLUDE):\n- Contribution requirements: branch naming, commit message formats, PR description templates\n- Testing mandates: required test types, coverage thresholds, \"never delete tests\" policies\n- Tool usage rules: required CLI commands, browser automation tools, linter/formatter requirements\n- Approval workflows: review requirements, CI checks that must pass, deployment gates\n- AI agent instructions: explicit directives for automated assistants (coding rules, prohibited actions)\n- Code conventions: naming standards (camelCase, PascalCase, snake_case for what), import ordering rules, file/folder naming patterns, formatting rules (indentation, quotes, semicolons), language idioms, TypeScript strictness policies, preferred patterns vs anti-patterns\nThese define how contributors (human or AI) must work within the project.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion .annex.sum file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**CheckpointManager persists per-module rebuild progress and spec file hashes to enable session resumption after interruption.**\n\n## Exports\n\n`CheckpointManager` — class managing rebuild state persistence; factory methods `load(outputDir, specFiles, unitNames): Promise<{manager, isResume}>` and `createFresh(outputDir, specFiles, unitNames): CheckpointManager`; instance methods `markDone(unitName, filesWritten): void`, `markFailed(unitName, error): void`, `getPendingUnits(): string[]`, `isDone(unitName): boolean`, `flush(): Promise<void>`, `initialize(): Promise<void>`, `getData(): RebuildCheckpoint`.\n\n## State & Persistence\n\nStores `RebuildCheckpoint` containing `version`, `createdAt`, `updatedAt`, `outputDir`, `specHashes` (Map<relativePath, contentHash>), and `modules` (Map<unitName, {status: 'pending'|'done'|'failed', completedAt?, filesWritten?, error?}>). Writes to `.rebuild-checkpoint` file in output directory using promise-chain serialization (`writeQueue`) to prevent corruption from concurrent worker pool updates.\n\n## Resume Logic\n\n`load()` performs three-stage drift detection: (1) validates checkpoint file existence and JSON parse, (2) validates schema via `RebuildCheckpointSchema.safeParse()`, (3) compares current spec file content hashes (`computeContentHashFromString`) against stored hashes; returns `isResume: true` only if checkpoint valid and all spec hashes match exactly (file count and content). Any mismatch triggers fresh checkpoint.\n\n## Error Handling\n\nCheckpoint write failures silently suppress (rebuild continues without persistence); JSON parse, schema validation, and file I/O errors in `load()` trigger automatic `createFresh()` fallback; `initialize()` mkdir/writeFile errors swallowed as non-critical.",
      "model": "claude-haiku-4-5-20251001",
      "inputTokens": 3,
      "outputTokens": 449,
      "cacheReadTokens": 16147,
      "cacheCreationTokens": 4845,
      "latencyMs": 24018,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/rebuild/checkpoint.ts",
          "sizeBytes": 7304
        }
      ]
    },
    {
      "timestamp": "2026-02-14T13:47:55.867Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: src/rebuild/index.ts\n\n```typescript\n/**\n * Rebuild module barrel export.\n *\n * Re-exports all public types, schemas, functions, and classes\n * for the rebuild pipeline.\n *\n * @module\n */\n\nexport type {\n  RebuildCheckpoint,\n  RebuildUnit,\n  RebuildPlan,\n  RebuildResult,\n} from './types.js';\n\nexport { RebuildCheckpointSchema } from './types.js';\n\nexport { readSpecFiles, partitionSpec } from './spec-reader.js';\n\nexport { parseModuleOutput } from './output-parser.js';\n\nexport { CheckpointManager } from './checkpoint.js';\n\nexport { REBUILD_SYSTEM_PROMPT, buildRebuildPrompt } from './prompts.js';\n\nexport { executeRebuild, type RebuildExecutionOptions } from './orchestrator.js';\n\n```\n\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n- Workflow & convention rules: if this file defines contribution guidelines, PR conventions, commit standards, testing mandates, tool usage requirements, approval workflows, AI agent behavioral instructions, or code conventions (naming standards, formatting rules, import ordering, linting policies), extract them as explicit, actionable rules. Sources: CONTRIBUTING.md, CI configs, PR templates, style guides, linter/formatter configs (.eslintrc, .prettierrc, .editorconfig), tsconfig strictness, README dev sections.\n\n\nTARGET LENGTH (MANDATORY):\n- Source file: 645 characters\n- Target summary: ~65 characters (10% compression)\n- Maximum: 78 characters\n- Achieve this by: ultra-dense writing, minimal examples, single-sentence descriptions\n\nULTRA-COMPRESSION TECHNIQUES:\n- Use telegraphic style: \"exports foo, bar, baz\" not \"this file exports three functions\"\n- No code examples unless critical to understanding\n- One sentence per export maximum\n- Omit obvious parameter names (\"opts: Options\" not \"options: Options - configuration object\")\n- Use abbreviations: \"params\" not \"parameters\", \"config\" not \"configuration\"\n- Skip section if empty (don't write \"No special concerns\" or \"N/A\")\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nWORKFLOW & CONVENTION RULES (NEVER EXCLUDE):\n- Contribution requirements: branch naming, commit message formats, PR description templates\n- Testing mandates: required test types, coverage thresholds, \"never delete tests\" policies\n- Tool usage rules: required CLI commands, browser automation tools, linter/formatter requirements\n- Approval workflows: review requirements, CI checks that must pass, deployment gates\n- AI agent instructions: explicit directives for automated assistants (coding rules, prohibited actions)\n- Code conventions: naming standards (camelCase, PascalCase, snake_case for what), import ordering rules, file/folder naming patterns, formatting rules (indentation, quotes, semicolons), language idioms, TypeScript strictness policies, preferred patterns vs anti-patterns\nThese define how contributors (human or AI) must work within the project.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion .annex.sum file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**src/rebuild/index.ts exports types, schemas, and functions for the rebuild pipeline (RebuildCheckpoint, RebuildUnit, RebuildPlan, RebuildResult, RebuildCheckpointSchema, readSpecFiles, partitionSpec, parseModuleOutput, CheckpointManager, REBUILD_SYSTEM_PROMPT, buildRebuildPrompt, executeRebuild).**\n\n## Exported Types\n`RebuildCheckpoint`, `RebuildUnit`, `RebuildPlan`, `RebuildResult` — domain types for rebuild state and execution.\n\n## Exported Constants & Schemas\n`RebuildCheckpointSchema` — schema for validating checkpoint structures; `REBUILD_SYSTEM_PROMPT` — system prompt for rebuild orchestration.\n\n## Exported Functions\n`readSpecFiles(...)` / `partitionSpec(...)` — spec file I/O and partitioning from spec-reader.js; `parseModuleOutput(...)` — parses module generation output; `buildRebuildPrompt(...)` — constructs rebuild prompts; `executeRebuild(..., opts: RebuildExecutionOptions)` — orchestrates rebuild execution.\n\n## Exported Classes\n`CheckpointManager` — manages rebuild checkpoint persistence and recovery.",
      "model": "claude-haiku-4-5-20251001",
      "inputTokens": 3,
      "outputTokens": 291,
      "cacheReadTokens": 16147,
      "cacheCreationTokens": 2865,
      "latencyMs": 21857,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/rebuild/index.ts",
          "sizeBytes": 645
        }
      ]
    },
    {
      "timestamp": "2026-02-14T13:47:44.329Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: src/orchestration/runner.ts\n\n```typescript\n/**\n * Two-phase command runner for AI-driven documentation generation.\n *\n * Wires together {@link AIService}, {@link ExecutionPlan}, the concurrency\n * pool, and the progress reporter into a cohesive execution engine.\n *\n * The two execution phases match the {@link ExecutionPlan} dependency graph:\n * 1. **File analysis** -- concurrent AI calls with configurable parallelism\n * 2. **Directory docs** -- concurrent per depth level, post-order AGENTS.md + companion CLAUDE.md generation\n *\n * @module\n */\n\nimport * as path from 'node:path';\nimport { readFile } from 'node:fs/promises';\nimport type { AIService } from '../ai/index.js';\nimport type { AIResponse } from '../ai/types.js';\nimport type { ExecutionPlan, ExecutionTask } from '../generation/executor.js';\nimport { writeSumFile, readSumFile, writeAnnexFile, getSumPath } from '../generation/writers/sum.js';\nimport type { SumFileContent } from '../generation/writers/sum.js';\nimport { writeAgentsMd, writeAgentsMdHub } from '../generation/writers/agents-md.js';\nimport { writeClaudeMdPointer } from '../generation/writers/claude-md.js';\nimport { computeContentHashFromString } from '../change-detection/index.js';\nimport { buildDirectoryPrompt } from '../generation/prompts/index.js';\nimport type { Config } from '../config/schema.js';\nimport {\n  checkCodeVsDoc,\n  checkCodeVsCode,\n  checkPhantomPaths,\n  buildInconsistencyReport,\n  formatReportForCli,\n} from '../quality/index.js';\nimport type { Inconsistency } from '../quality/index.js';\nimport { formatExecutionPlanAsMarkdown } from '../generation/executor.js';\nimport { runPool } from './pool.js';\nimport { PlanTracker } from './plan-tracker.js';\nimport { ProgressReporter } from './progress.js';\nimport type { ITraceWriter } from './trace.js';\nimport type {\n  FileTaskResult,\n  RunSummary,\n  CommandRunOptions,\n} from './types.js';\nimport { getVersion } from '../version.js';\n\n// ---------------------------------------------------------------------------\n// CommandRunner\n// ---------------------------------------------------------------------------\n\n/**\n * Orchestrates AI-driven documentation generation.\n *\n * Create one instance per command invocation. The runner holds references\n * to the AI service and run options, then executes plans or file lists\n * through the two-phase pipeline (file analysis, then directory aggregation).\n *\n * @example\n * ```typescript\n * const runner = new CommandRunner(aiService, {\n *   concurrency: 5,\n *   failFast: false,\n * });\n *\n * const summary = await runner.executeGenerate(plan);\n * console.log(`Processed ${summary.filesProcessed} files`);\n * ```\n */\nexport class CommandRunner {\n  /** AI service instance for making calls */\n  private readonly aiService: AIService;\n\n  /** Command execution options */\n  private readonly options: CommandRunOptions;\n\n  /** Trace writer for concurrency debugging */\n  private readonly tracer: ITraceWriter | undefined;\n\n  /** Eval variant name for side-by-side model comparison */\n  private readonly variant: string | undefined;\n\n  /**\n   * Create a new command runner.\n   *\n   * @param aiService - The AI service instance (should be created per CLI run)\n   * @param options - Execution options (concurrency, failFast, etc.)\n   */\n  constructor(aiService: AIService, options: CommandRunOptions) {\n    this.aiService = aiService;\n    this.options = options;\n    this.tracer = options.tracer;\n    this.variant = options.variant;\n\n    // Wire the tracer into the AI service for subprocess/retry events\n    if (this.tracer) {\n      this.aiService.setTracer(this.tracer);\n    }\n  }\n\n  /** Progress log instance (if provided via options) for ProgressReporter mirroring */\n  private get progressLog() { return this.options.progressLog; }\n\n  /**\n   * Execute the `generate` command using a pre-built execution plan.\n   *\n   * Runs two phases:\n   * 1. File tasks concurrently through the pool\n   * 2. Directory AGENTS.md + companion CLAUDE.md generation (post-order)\n   *\n   * @param plan - The execution plan from the generation orchestrator\n   * @returns Aggregated run summary\n   */\n  async executeGenerate(\n    plan: ExecutionPlan,\n    options?: { skippedFiles?: number; skippedDirs?: number },\n  ): Promise<RunSummary> {\n    const reporter = new ProgressReporter(plan.fileTasks.length, plan.directoryTasks.length, this.progressLog);\n\n    // Initialize plan tracker (writes GENERATION-PLAN.md with checkboxes)\n    const planTracker = new PlanTracker(\n      plan.projectRoot,\n      formatExecutionPlanAsMarkdown(plan),\n    );\n    await planTracker.initialize();\n\n    const runStart = Date.now();\n    let filesProcessed = 0;\n    let filesFailed = 0;\n\n    // -------------------------------------------------------------------\n    // Pre-Phase 1: Cache old .sum content for stale documentation detection\n    // Throttled to avoid opening too many file descriptors at once.\n    // -------------------------------------------------------------------\n\n    const prePhase1Start = Date.now();\n    this.tracer?.emit({\n      type: 'phase:start',\n      phase: 'pre-phase-1-cache',\n      taskCount: plan.fileTasks.length,\n      concurrency: 20,\n    });\n\n    const oldSumCache = new Map<string, SumFileContent>();\n    const sumReadTasks = plan.fileTasks.map(\n      (task) => async () => {\n        try {\n          const existing = await readSumFile(getSumPath(task.absolutePath, this.variant));\n          if (existing) {\n            oldSumCache.set(task.path, existing);\n          }\n        } catch {\n          // No old .sum to compare -- skip\n        }\n      },\n    );\n    await runPool(sumReadTasks, {\n      concurrency: 20,\n      tracer: this.tracer,\n      phaseLabel: 'pre-phase-1-cache',\n      taskLabels: plan.fileTasks.map(t => t.path),\n    });\n\n    this.tracer?.emit({\n      type: 'phase:end',\n      phase: 'pre-phase-1-cache',\n      durationMs: Date.now() - prePhase1Start,\n      tasksCompleted: plan.fileTasks.length,\n      tasksFailed: 0,\n    });\n\n    // -------------------------------------------------------------------\n    // Phase 1: File analysis (concurrent)\n    // -------------------------------------------------------------------\n\n    const phase1Start = Date.now();\n    this.tracer?.emit({\n      type: 'phase:start',\n      phase: 'phase-1-files',\n      taskCount: plan.fileTasks.length,\n      concurrency: this.options.concurrency,\n    });\n\n    // Cache source content during Phase 1, reused for inconsistency detection\n    const sourceContentCache = new Map<string, string>();\n\n    const fileTasks = plan.fileTasks.map(\n      (task: ExecutionTask, taskIndex: number) => async (): Promise<FileTaskResult> => {\n        reporter.onFileStart(task.path);\n\n        const callStart = Date.now();\n\n        // Read the source file\n        const sourceContent = await readFile(task.absolutePath, 'utf-8');\n        sourceContentCache.set(task.path, sourceContent);\n\n        // Call AI with the task's prompts\n        const response: AIResponse = await this.aiService.call({\n          prompt: task.userPrompt,\n          systemPrompt: task.systemPrompt,\n          taskLabel: task.path,\n        });\n\n        // Track file size for telemetry (from in-memory content, avoids stat syscall)\n        this.aiService.addFilesReadToLastEntry([{\n          path: task.path,\n          sizeBytes: Buffer.byteLength(sourceContent, 'utf-8'),\n        }]);\n\n        // Compute content hash from already-loaded content (avoids second readFile)\n        const contentHash = computeContentHashFromString(sourceContent);\n\n        // Build .sum file content\n        const cleanedText = stripPreamble(response.text);\n        const sumContent: SumFileContent = {\n          summary: cleanedText,\n          metadata: {\n            purpose: extractPurpose(cleanedText),\n          },\n          generatedAt: new Date().toISOString(),\n          contentHash,\n        };\n\n        // Write .sum file\n        await writeSumFile(task.absolutePath, sumContent, this.variant);\n\n        // Write annex file if LLM identified reproduction-critical constants\n        if (cleanedText.includes('## Annex References')) {\n          await writeAnnexFile(task.absolutePath, sourceContent, this.variant);\n        }\n\n        const durationMs = Date.now() - callStart;\n\n        return {\n          path: task.path,\n          success: true,\n          tokensIn: response.inputTokens,\n          tokensOut: response.outputTokens,\n          cacheReadTokens: response.cacheReadTokens,\n          cacheCreationTokens: response.cacheCreationTokens,\n          durationMs,\n          model: response.model,\n        };\n      },\n    );\n\n    const poolResults = await runPool(\n      fileTasks,\n      {\n        concurrency: this.options.concurrency,\n        failFast: this.options.failFast,\n        tracer: this.tracer,\n        phaseLabel: 'phase-1-files',\n        taskLabels: plan.fileTasks.map(t => t.path),\n      },\n      (result) => {\n        if (result.success && result.value) {\n          const v = result.value;\n          filesProcessed++;\n          reporter.onFileDone(v.path, v.durationMs, v.tokensIn, v.tokensOut, v.model, v.cacheReadTokens, v.cacheCreationTokens);\n          planTracker.markDone(v.path);\n        } else {\n          filesFailed++;\n          const errorMsg = result.error?.message ?? 'Unknown error';\n          const taskPath = plan.fileTasks[result.index]?.path ?? `task-${result.index}`;\n          reporter.onFileError(taskPath, errorMsg);\n        }\n      },\n    );\n\n    this.tracer?.emit({\n      type: 'phase:end',\n      phase: 'phase-1-files',\n      durationMs: Date.now() - phase1Start,\n      tasksCompleted: filesProcessed,\n      tasksFailed: filesFailed,\n    });\n\n    // -------------------------------------------------------------------\n    // Post-Phase 1: Inconsistency detection (non-throwing)\n    // -------------------------------------------------------------------\n\n    let inconsistenciesCodeVsDoc = 0;\n    let inconsistenciesCodeVsCode = 0;\n    let inconsistencyReport: import('../quality/index.js').InconsistencyReport | undefined;\n\n    try {\n      const inconsistencyStart = Date.now();\n      const allIssues: Inconsistency[] = [];\n\n      // Collect successfully processed file paths from pool results\n      const processedPaths: string[] = [];\n      for (const result of poolResults) {\n        if (result.success && result.value) {\n          processedPaths.push(result.value.path);\n        }\n      }\n\n      // Group files by directory\n      const dirGroups = new Map<string, string[]>();\n      for (const filePath of processedPaths) {\n        const dir = path.dirname(filePath);\n        const group = dirGroups.get(dir);\n        if (group) {\n          group.push(filePath);\n        } else {\n          dirGroups.set(dir, [filePath]);\n        }\n      }\n\n      // Run checks per directory group (throttled to avoid excessive parallel I/O)\n      const dirEntries = Array.from(dirGroups.entries());\n\n      this.tracer?.emit({\n        type: 'phase:start',\n        phase: 'post-phase-1-quality',\n        taskCount: dirEntries.length,\n        concurrency: 10,\n      });\n\n      const dirCheckResults: Inconsistency[][] = [];\n      const dirCheckTasks = dirEntries.map(\n        ([, groupPaths], groupIndex) => async () => {\n          const dirIssues: Inconsistency[] = [];\n          const filesForCodeVsCode: Array<{ path: string; content: string }> = [];\n\n          // Process files within this group sequentially to limit I/O\n          for (const filePath of groupPaths) {\n            const absoluteFilePath = `${plan.projectRoot}/${filePath}`;\n\n            // Use cached content from Phase 1 (avoids re-read)\n            let sourceContent = sourceContentCache.get(filePath);\n            if (!sourceContent) {\n              try {\n                sourceContent = await readFile(absoluteFilePath, 'utf-8');\n              } catch {\n                continue; // File unreadable, skip\n              }\n            }\n\n            filesForCodeVsCode.push({ path: filePath, content: sourceContent });\n\n            // Old-doc check: detects stale documentation\n            const oldSum = oldSumCache.get(filePath);\n            if (oldSum) {\n              const oldIssue = checkCodeVsDoc(sourceContent, oldSum, filePath);\n              if (oldIssue) {\n                oldIssue.description += ' (stale documentation)';\n                dirIssues.push(oldIssue);\n              }\n            }\n\n            // New-doc check: detects LLM omissions in freshly generated .sum\n            try {\n              const newSum = await readSumFile(getSumPath(absoluteFilePath, this.variant));\n              if (newSum) {\n                const newIssue = checkCodeVsDoc(sourceContent, newSum, filePath);\n                if (newIssue) {\n                  dirIssues.push(newIssue);\n                }\n              }\n            } catch {\n              // Freshly written .sum unreadable -- skip\n            }\n          }\n\n          // Code-vs-code check scoped to this directory group\n          const codeIssues = checkCodeVsCode(filesForCodeVsCode);\n          dirIssues.push(...codeIssues);\n\n          dirCheckResults[groupIndex] = dirIssues;\n        },\n      );\n      await runPool(dirCheckTasks, {\n        concurrency: 10,\n        tracer: this.tracer,\n        phaseLabel: 'post-phase-1-quality',\n        taskLabels: dirEntries.map(([dirPath]) => dirPath),\n      });\n\n      this.tracer?.emit({\n        type: 'phase:end',\n        phase: 'post-phase-1-quality',\n        durationMs: Date.now() - inconsistencyStart,\n        tasksCompleted: dirEntries.length,\n        tasksFailed: 0,\n      });\n\n      const allIssuesFlat = dirCheckResults.filter(Boolean).flat();\n      allIssues.push(...allIssuesFlat);\n\n      // Release cached source content to free memory\n      sourceContentCache.clear();\n\n      if (allIssues.length > 0) {\n        const report = buildInconsistencyReport(allIssues, {\n          projectRoot: plan.projectRoot,\n          filesChecked: processedPaths.length,\n          durationMs: Date.now() - inconsistencyStart,\n        });\n\n        inconsistenciesCodeVsDoc = report.summary.codeVsDoc;\n        inconsistenciesCodeVsCode = report.summary.codeVsCode;\n        inconsistencyReport = report;\n\n        console.error(formatReportForCli(report));\n      }\n    } catch (err) {\n      // Inconsistency detection must not break the pipeline\n      console.error(`[quality] Inconsistency detection failed: ${err instanceof Error ? err.message : String(err)}`);\n    }\n\n    // -------------------------------------------------------------------\n    // Phase 2: Directory docs (concurrent per depth level, post-order)\n    // -------------------------------------------------------------------\n\n    // Build set of directories in the plan (for filtering in buildDirectoryPrompt)\n    const knownDirs = new Set(plan.directoryTasks.map(t => t.path));\n\n    // Group directory tasks by depth so same-depth dirs run in parallel\n    // while maintaining post-order (children before parents)\n    const dirsByDepth = new Map<number, typeof plan.directoryTasks>();\n    for (const dirTask of plan.directoryTasks) {\n      const depth = (dirTask.metadata.depth as number) ?? 0;\n      const group = dirsByDepth.get(depth);\n      if (group) {\n        group.push(dirTask);\n      } else {\n        dirsByDepth.set(depth, [dirTask]);\n      }\n    }\n\n    // Process depth levels in descending order (deepest first = post-order)\n    const depthLevels = Array.from(dirsByDepth.keys()).sort((a, b) => b - a);\n    let dirsProcessed = 0;\n    let dirsFailed = 0;\n\n    for (const depth of depthLevels) {\n      const dirsAtDepth = dirsByDepth.get(depth)!;\n      const phaseLabel = `phase-2-dirs-depth-${depth}`;\n      const dirConcurrency = Math.min(this.options.concurrency, dirsAtDepth.length);\n\n      const phase2Start = Date.now();\n      this.tracer?.emit({\n        type: 'phase:start',\n        phase: phaseLabel,\n        taskCount: dirsAtDepth.length,\n        concurrency: dirConcurrency,\n      });\n\n      const dirTasks = dirsAtDepth.map(\n        (dirTask) => async () => {\n          reporter.onDirectoryStart(dirTask.path);\n          const dirCallStart = Date.now();\n          const prompt = await buildDirectoryPrompt(dirTask.absolutePath, plan.projectRoot, this.options.debug, knownDirs, plan.projectStructure, undefined, undefined, this.variant);\n          const dirResponse: AIResponse = await this.aiService.call({\n            prompt: prompt.user,\n            systemPrompt: prompt.system,\n            taskLabel: `${dirTask.path}/AGENTS.md`,\n          });\n          await writeAgentsMd(dirTask.absolutePath, plan.projectRoot, dirResponse.text, this.variant);\n          if (this.variant) {\n            await writeAgentsMdHub(dirTask.absolutePath, this.variant);\n          }\n          await writeClaudeMdPointer(dirTask.absolutePath);\n          const dirDurationMs = Date.now() - dirCallStart;\n          reporter.onDirectoryDone(\n            dirTask.path,\n            dirDurationMs,\n            dirResponse.inputTokens,\n            dirResponse.outputTokens,\n            dirResponse.model,\n            dirResponse.cacheReadTokens,\n            dirResponse.cacheCreationTokens,\n          );\n          planTracker.markDone(`${dirTask.path}/AGENTS.md`);\n        },\n      );\n\n      const phase2Results = await runPool(dirTasks, {\n        concurrency: dirConcurrency,\n        failFast: this.options.failFast,\n        tracer: this.tracer,\n        phaseLabel,\n        taskLabels: dirsAtDepth.map(t => t.path),\n      });\n\n      const phase2Succeeded = phase2Results.filter(r => r.success).length;\n      const phase2Failed = phase2Results.filter(r => !r.success).length;\n      dirsProcessed += phase2Succeeded;\n      dirsFailed += phase2Failed;\n\n      this.tracer?.emit({\n        type: 'phase:end',\n        phase: phaseLabel,\n        durationMs: Date.now() - phase2Start,\n        tasksCompleted: phase2Succeeded,\n        tasksFailed: phase2Failed,\n      });\n    }\n\n    // -------------------------------------------------------------------\n    // Post-Phase 2: Phantom path validation (non-throwing)\n    // -------------------------------------------------------------------\n\n    let phantomPathCount = 0;\n    const phantomAgentsFilename = this.variant ? `AGENTS.${this.variant}.md` : 'AGENTS.md';\n    try {\n      const phantomIssues: Inconsistency[] = [];\n      for (const dirTask of plan.directoryTasks) {\n        const agentsMdPath = path.join(dirTask.absolutePath, phantomAgentsFilename);\n        try {\n          const content = await readFile(agentsMdPath, 'utf-8');\n          const issues = checkPhantomPaths(agentsMdPath, content, plan.projectRoot);\n          phantomIssues.push(...issues);\n        } catch {\n          // AGENTS.md not yet written or read error — skip\n        }\n      }\n      phantomPathCount = phantomIssues.length;\n\n      if (phantomIssues.length > 0) {\n        const phantomReport = buildInconsistencyReport(phantomIssues, {\n          projectRoot: plan.projectRoot,\n          filesChecked: plan.directoryTasks.length,\n          durationMs: 0,\n        });\n        console.error(formatReportForCli(phantomReport));\n      }\n    } catch (err) {\n      console.error(`[quality] Phantom path validation failed: ${err instanceof Error ? err.message : String(err)}`);\n    }\n\n    // Ensure all plan tracker writes are flushed\n    await planTracker.flush();\n\n    // -------------------------------------------------------------------\n    // Build and print summary\n    // -------------------------------------------------------------------\n\n    const aiSummary = this.aiService.getSummary();\n    const totalDurationMs = Date.now() - runStart;\n\n    const summary: RunSummary = {\n      version: getVersion(),\n      filesProcessed,\n      filesFailed,\n      filesSkipped: options?.skippedFiles ?? 0,\n      dirsProcessed,\n      dirsFailed,\n      dirsSkipped: options?.skippedDirs ?? 0,\n      totalCalls: aiSummary.totalCalls,\n      totalInputTokens: aiSummary.totalInputTokens,\n      totalOutputTokens: aiSummary.totalOutputTokens,\n      totalCacheReadTokens: aiSummary.totalCacheReadTokens,\n      totalCacheCreationTokens: aiSummary.totalCacheCreationTokens,\n      totalDurationMs,\n      errorCount: aiSummary.errorCount,\n      retryCount: 0,\n      totalFilesRead: aiSummary.totalFilesRead,\n      uniqueFilesRead: aiSummary.uniqueFilesRead,\n      inconsistenciesCodeVsDoc,\n      inconsistenciesCodeVsCode,\n      phantomPaths: phantomPathCount,\n      inconsistencyReport,\n    };\n\n    reporter.printSummary(summary);\n\n    return summary;\n  }\n\n  /**\n   * Execute the `update` command for a set of changed files.\n   *\n   * Runs only Phase 1 (file analysis) for the specified files. Does NOT\n   * generate directory or root documents -- the update command handles\n   * AGENTS.md regeneration itself based on which directories were affected.\n   *\n   * @param filesToAnalyze - Array of changed files to re-analyze\n   * @param projectRoot - Absolute path to the project root\n   * @param config - Project configuration for prompt building\n   * @returns Aggregated run summary\n   */\n  async executeUpdate(\n    fileTasks: import('../orchestration/orchestrator.js').AnalysisTask[],\n    projectRoot: string,\n    config: Config,\n  ): Promise<RunSummary> {\n    const reporter = new ProgressReporter(fileTasks.length, 0, this.progressLog);\n\n    const runStart = Date.now();\n    let filesProcessed = 0;\n    let filesFailed = 0;\n\n    // -------------------------------------------------------------------\n    // Phase 1: File analysis (concurrent)\n    // -------------------------------------------------------------------\n\n    const phase1Start = Date.now();\n    this.tracer?.emit({\n      type: 'phase:start',\n      phase: 'update-phase-1-files',\n      taskCount: fileTasks.length,\n      concurrency: this.options.concurrency,\n    });\n\n    // Cache source content during update, reused for inconsistency detection\n    const updateSourceCache = new Map<string, string>();\n\n    const updateTasks = fileTasks.map(\n      (task: import('../orchestration/orchestrator.js').AnalysisTask, fileIndex: number) => async (): Promise<FileTaskResult> => {\n        reporter.onFileStart(task.filePath);\n\n        const callStart = Date.now();\n        const absolutePath = `${projectRoot}/${task.filePath}`;\n\n        // Read the source file\n        const sourceContent = await readFile(absolutePath, 'utf-8');\n        updateSourceCache.set(task.filePath, sourceContent);\n\n        // Call AI with pre-built prompts from task\n        const response: AIResponse = await this.aiService.call({\n          prompt: task.userPrompt!,\n          systemPrompt: task.systemPrompt!,\n          taskLabel: task.filePath,\n        });\n\n        // Track file size for telemetry (from in-memory content, avoids stat syscall)\n        this.aiService.addFilesReadToLastEntry([{\n          path: task.filePath,\n          sizeBytes: Buffer.byteLength(sourceContent, 'utf-8'),\n        }]);\n\n        // Compute content hash from already-loaded content (avoids second readFile)\n        const contentHash = computeContentHashFromString(sourceContent);\n\n        // Build .sum file content\n        const cleanedText = stripPreamble(response.text);\n        const sumContent: SumFileContent = {\n          summary: cleanedText,\n          metadata: {\n            purpose: extractPurpose(cleanedText),\n          },\n          generatedAt: new Date().toISOString(),\n          contentHash,\n        };\n\n        // Write .sum file\n        await writeSumFile(absolutePath, sumContent, this.variant);\n\n        // Write annex file if LLM identified reproduction-critical constants\n        if (cleanedText.includes('## Annex References')) {\n          await writeAnnexFile(absolutePath, sourceContent, this.variant);\n        }\n\n        const durationMs = Date.now() - callStart;\n\n        return {\n          path: task.filePath,\n          success: true,\n          tokensIn: response.inputTokens,\n          tokensOut: response.outputTokens,\n          cacheReadTokens: response.cacheReadTokens,\n          cacheCreationTokens: response.cacheCreationTokens,\n          durationMs,\n          model: response.model,\n        };\n      },\n    );\n\n    const poolResults = await runPool(\n      updateTasks,\n      {\n        concurrency: this.options.concurrency,\n        failFast: this.options.failFast,\n        tracer: this.tracer,\n        phaseLabel: 'update-phase-1-files',\n        taskLabels: fileTasks.map(t => t.filePath),\n      },\n      (result) => {\n        if (result.success && result.value) {\n          const v = result.value;\n          filesProcessed++;\n          reporter.onFileDone(v.path, v.durationMs, v.tokensIn, v.tokensOut, v.model, v.cacheReadTokens, v.cacheCreationTokens);\n        } else {\n          filesFailed++;\n          const errorMsg = result.error?.message ?? 'Unknown error';\n          const filePath = fileTasks[result.index]?.filePath ?? `file-${result.index}`;\n          reporter.onFileError(filePath, errorMsg);\n        }\n      },\n    );\n\n    this.tracer?.emit({\n      type: 'phase:end',\n      phase: 'update-phase-1-files',\n      durationMs: Date.now() - phase1Start,\n      tasksCompleted: filesProcessed,\n      tasksFailed: filesFailed,\n    });\n\n    // -------------------------------------------------------------------\n    // Post-analysis: Inconsistency detection (non-throwing)\n    // -------------------------------------------------------------------\n\n    let updateInconsistenciesCodeVsDoc = 0;\n    let updateInconsistenciesCodeVsCode = 0;\n    let updateInconsistencyReport: import('../quality/index.js').InconsistencyReport | undefined;\n\n    try {\n      const inconsistencyStart = Date.now();\n      const allIssues: Inconsistency[] = [];\n\n      // Collect successfully processed file paths\n      const processedPaths: string[] = [];\n      for (const result of poolResults) {\n        if (result.success && result.value) {\n          processedPaths.push(result.value.path);\n        }\n      }\n\n      // Group files by directory\n      const dirGroups = new Map<string, string[]>();\n      for (const filePath of processedPaths) {\n        const dir = path.dirname(filePath);\n        const group = dirGroups.get(dir);\n        if (group) {\n          group.push(filePath);\n        } else {\n          dirGroups.set(dir, [filePath]);\n        }\n      }\n\n      // Run checks per directory group (throttled to avoid excessive parallel I/O)\n      const updateDirEntries = Array.from(dirGroups.entries());\n\n      this.tracer?.emit({\n        type: 'phase:start',\n        phase: 'update-post-phase-1-quality',\n        taskCount: updateDirEntries.length,\n        concurrency: 10,\n      });\n\n      const updateDirResults: Inconsistency[][] = [];\n      const updateDirCheckTasks = updateDirEntries.map(\n        ([, groupPaths], groupIndex) => async () => {\n          const dirIssues: Inconsistency[] = [];\n          const filesForCodeVsCode: Array<{ path: string; content: string }> = [];\n\n          for (const filePath of groupPaths) {\n            const absoluteFilePath = `${projectRoot}/${filePath}`;\n\n            // Use cached content from update phase (avoids re-read)\n            let sourceContent = updateSourceCache.get(filePath);\n            if (!sourceContent) {\n              try {\n                sourceContent = await readFile(absoluteFilePath, 'utf-8');\n              } catch {\n                continue;\n              }\n            }\n\n            filesForCodeVsCode.push({ path: filePath, content: sourceContent });\n\n            // New-doc check: detects LLM omissions in freshly generated .sum\n            try {\n              const newSum = await readSumFile(getSumPath(absoluteFilePath, this.variant));\n              if (newSum) {\n                const newIssue = checkCodeVsDoc(sourceContent, newSum, filePath);\n                if (newIssue) {\n                  dirIssues.push(newIssue);\n                }\n              }\n            } catch {\n              // .sum unreadable -- skip\n            }\n          }\n\n          // Code-vs-code check scoped to this directory group\n          const codeIssues = checkCodeVsCode(filesForCodeVsCode);\n          dirIssues.push(...codeIssues);\n\n          updateDirResults[groupIndex] = dirIssues;\n        },\n      );\n      await runPool(updateDirCheckTasks, {\n        concurrency: 10,\n        tracer: this.tracer,\n        phaseLabel: 'update-post-phase-1-quality',\n        taskLabels: updateDirEntries.map(([dirPath]) => dirPath),\n      });\n\n      this.tracer?.emit({\n        type: 'phase:end',\n        phase: 'update-post-phase-1-quality',\n        durationMs: Date.now() - inconsistencyStart,\n        tasksCompleted: updateDirEntries.length,\n        tasksFailed: 0,\n      });\n\n      const allIssuesFlat = updateDirResults.filter(Boolean).flat();\n      allIssues.push(...allIssuesFlat);\n\n      // Release cached source content to free memory\n      updateSourceCache.clear();\n\n      if (allIssues.length > 0) {\n        const report = buildInconsistencyReport(allIssues, {\n          projectRoot,\n          filesChecked: processedPaths.length,\n          durationMs: Date.now() - inconsistencyStart,\n        });\n\n        updateInconsistenciesCodeVsDoc = report.summary.codeVsDoc;\n        updateInconsistenciesCodeVsCode = report.summary.codeVsCode;\n        updateInconsistencyReport = report;\n\n        console.error(formatReportForCli(report));\n      }\n    } catch (err) {\n      console.error(`[quality] Inconsistency detection failed: ${err instanceof Error ? err.message : String(err)}`);\n    }\n\n    // Build summary (caller is responsible for printing after dir regen)\n    const aiSummary = this.aiService.getSummary();\n    const totalDurationMs = Date.now() - runStart;\n\n    const summary: RunSummary = {\n      version: getVersion(),\n      filesProcessed,\n      filesFailed,\n      filesSkipped: 0,\n      totalCalls: aiSummary.totalCalls,\n      totalInputTokens: aiSummary.totalInputTokens,\n      totalOutputTokens: aiSummary.totalOutputTokens,\n      totalCacheReadTokens: aiSummary.totalCacheReadTokens,\n      totalCacheCreationTokens: aiSummary.totalCacheCreationTokens,\n      totalDurationMs,\n      errorCount: aiSummary.errorCount,\n      retryCount: 0,\n      totalFilesRead: aiSummary.totalFilesRead,\n      uniqueFilesRead: aiSummary.uniqueFilesRead,\n      inconsistenciesCodeVsDoc: updateInconsistenciesCodeVsDoc,\n      inconsistenciesCodeVsCode: updateInconsistenciesCodeVsCode,\n      inconsistencyReport: updateInconsistencyReport,\n    };\n\n    return summary;\n  }\n}\n\n// ---------------------------------------------------------------------------\n// Helpers\n// ---------------------------------------------------------------------------\n\n/**\n * Strip LLM preamble from response text.\n * Detects common preamble patterns and removes everything before the actual content.\n */\nfunction stripPreamble(responseText: string): string {\n  // Pattern 1: Content after a --- separator (LLM uses --- before real content)\n  const separatorIndex = responseText.indexOf('\\n---\\n');\n  if (separatorIndex >= 0 && separatorIndex < 500) {\n    const afterSeparator = responseText.slice(separatorIndex + 5).trim();\n    if (afterSeparator.length > 0) {\n      return afterSeparator;\n    }\n  }\n\n  // Pattern 2: Content starts with a bold purpose line (**)\n  const boldMatch = responseText.match(/^[\\s\\S]{0,500}?(\\*\\*[A-Z])/);\n  if (boldMatch && boldMatch.index !== undefined) {\n    const before = responseText.slice(0, boldMatch.index).trim();\n    // Only strip if what comes before looks like preamble (no identifiers, short)\n    if (before.length > 0 && before.length < 300 && !before.includes('##')) {\n      return responseText.slice(boldMatch.index);\n    }\n  }\n\n  return responseText;\n}\n\nconst PREAMBLE_PREFIXES = [\n  'now i', 'perfect', 'based on', 'let me', 'here is', 'i\\'ll', 'i will',\n  'great', 'okay', 'sure', 'certainly', 'alright',\n];\n\n/**\n * Extract the purpose from AI response text.\n *\n * Skips lines that look like LLM preamble, markdown headers, or separators.\n * Falls back to empty string if the response is empty.\n *\n * @param responseText - The AI-generated summary text\n * @returns A single-line purpose string\n */\nfunction extractPurpose(responseText: string): string {\n  const lines = responseText.split('\\n');\n  for (const line of lines) {\n    const trimmed = line.trim();\n    if (!trimmed || trimmed.startsWith('#') || trimmed === '---') continue;\n\n    // Skip lines that look like LLM preamble\n    const lower = trimmed.toLowerCase();\n    if (PREAMBLE_PREFIXES.some(p => lower.startsWith(p))) continue;\n\n    // Strip bold markdown wrapper if present\n    const purpose = trimmed.replace(/^\\*\\*(.+)\\*\\*$/, '$1');\n    return purpose.length > 120 ? purpose.slice(0, 117) + '...' : purpose;\n  }\n  return '';\n}\n\n```\n\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n- Workflow & convention rules: if this file defines contribution guidelines, PR conventions, commit standards, testing mandates, tool usage requirements, approval workflows, AI agent behavioral instructions, or code conventions (naming standards, formatting rules, import ordering, linting policies), extract them as explicit, actionable rules. Sources: CONTRIBUTING.md, CI configs, PR templates, style guides, linter/formatter configs (.eslintrc, .prettierrc, .editorconfig), tsconfig strictness, README dev sections.\n\n\nTARGET LENGTH (MANDATORY):\n- Source file: 32608 characters\n- Target summary: ~3261 characters (10% compression)\n- Maximum: 3913 characters\n- Achieve this by: ultra-dense writing, minimal examples, single-sentence descriptions\n\nULTRA-COMPRESSION TECHNIQUES:\n- Use telegraphic style: \"exports foo, bar, baz\" not \"this file exports three functions\"\n- No code examples unless critical to understanding\n- One sentence per export maximum\n- Omit obvious parameter names (\"opts: Options\" not \"options: Options - configuration object\")\n- Use abbreviations: \"params\" not \"parameters\", \"config\" not \"configuration\"\n- Skip section if empty (don't write \"No special concerns\" or \"N/A\")\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nWORKFLOW & CONVENTION RULES (NEVER EXCLUDE):\n- Contribution requirements: branch naming, commit message formats, PR description templates\n- Testing mandates: required test types, coverage thresholds, \"never delete tests\" policies\n- Tool usage rules: required CLI commands, browser automation tools, linter/formatter requirements\n- Approval workflows: review requirements, CI checks that must pass, deployment gates\n- AI agent instructions: explicit directives for automated assistants (coding rules, prohibited actions)\n- Code conventions: naming standards (camelCase, PascalCase, snake_case for what), import ordering rules, file/folder naming patterns, formatting rules (indentation, quotes, semicolons), language idioms, TypeScript strictness policies, preferred patterns vs anti-patterns\nThese define how contributors (human or AI) must work within the project.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion .annex.sum file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**CommandRunner orchestrates two-phase AI-driven documentation generation: concurrent file analysis producing .sum files, then post-order directory aggregation generating AGENTS.md and CLAUDE.md.**\n\n## Exports\n\n`CommandRunner` class with constructor `(aiService: AIService, options: CommandRunOptions)` holding AIService reference, execution options, tracer, and variant field.\n\n`executeGenerate(plan: ExecutionPlan, options?: { skippedFiles?: number; skippedDirs?: number }): Promise<RunSummary>` runs three sub-phases: pre-phase-1-cache (reads old .sum files via `readSumFile`, `getSumPath`), phase-1-files (concurrent file analysis calling `aiService.call()` with `userPrompt`/`systemPrompt`, writes outputs via `writeSumFile`, `writeAnnexFile`), post-phase-1-quality (inconsistency detection via `checkCodeVsDoc`, `checkCodeVsCode`, `buildInconsistencyReport`, `formatReportForCli`), phase-2-dirs (post-order by depth using `buildDirectoryPrompt`, `writeAgentsMd`, `writeAgentsMdHub`, `writeClaudeMdPointer`), post-phase-2 phantom validation via `checkPhantomPaths`.\n\n`executeUpdate(fileTasks: AnalysisTask[], projectRoot: string, config: Config): Promise<RunSummary>` runs Phase 1 only (file analysis) for changed files; does not regenerate AGENTS.md or root docs.\n\nHelper `stripPreamble(responseText: string): string` removes LLM preamble before `\\n---\\n` separator (if at offset <500) or text before opening bold marker `**[A-Z]` (if preceding text <300 chars and lacks `##`).\n\nHelper `extractPurpose(responseText: string): string` extracts single-line purpose by skipping lines matching `PREAMBLE_PREFIXES` (array: 'now i', 'perfect', 'based on', 'let me', 'here is', 'i\\'ll', 'i will', 'great', 'okay', 'sure', 'certainly', 'alright'), markdown headers (`#`), or `---`, stripping bold wrapper, truncating >120 chars to 117 + '...'.\n\n## Concurrency & Pooling\n\nRunner delegates task execution to `runPool(tasks, { concurrency, failFast, tracer, phaseLabel, taskLabels }, resultCallback)` with configurable parallelism: phase-1-files uses `options.concurrency`, pre-phase-1-cache throttles to 20 descriptors, post-phase-1-quality (directory checks) to 10, phase-2 per-depth to min(concurrency, dirCount).\n\nTracer emits `{ type: 'phase:start'|'phase:end', phase, taskCount?, concurrency?, durationMs?, tasksCompleted?, tasksFailed? }` for observability.\n\n## State Management & Caching\n\n`oldSumCache: Map<string, SumFileContent>` caches pre-existing .sum files for stale-doc detection via `checkCodeVsDoc(sourceContent, oldSum, filePath)`.\n\n`sourceContentCache: Map<string, string>` stores Phase 1 file reads to avoid redundant I/O in post-phase-1-quality; cleared after inconsistency detection.\n\n`updateSourceCache: Map<string, string>` analogous cache for executeUpdate.\n\n`planTracker: PlanTracker` tracks progress via `planTracker.markDone(path)` and `planTracker.flush()`, writing GENERATION-PLAN.md with checkboxes.\n\n## Phase 2 (Directory Aggregation) Ordering\n\nGroups directory tasks by `metadata.depth`, processes depth levels in descending order (highest depth first) ensuring post-order traversal (children before parents). Constructs `knownDirs: Set<string>` from plan for `buildDirectoryPrompt` filtering.\n\n## Output & Metrics\n\nReturns `RunSummary` containing: filesProcessed, filesFailed, filesSkipped, dirsProcessed, dirsFailed, dirsSkipped, totalCalls, totalInputTokens, totalOutputTokens, totalCacheReadTokens, totalCacheCreationTokens, totalDurationMs, errorCount, retryCount, totalFilesRead, uniqueFilesRead, inconsistenciesCodeVsDoc, inconsistenciesCodeVsCode, phantomPaths, inconsistencyReport.\n\n`ProgressReporter` instance logs via `onFileStart()`, `onFileDone(path, durationMs, tokensIn, tokensOut, model, cacheReadTokens, cacheCreationTokens)`, `onFileError()`, `onDirectoryStart()`, `onDirectoryDone()`, `printSummary(summary)`.\n\n## Annex & Variant Support\n\nDetects reproduction-critical constants by checking if cleaned text includes `'## Annex References'`, triggering `writeAnnexFile(absolutePath, sourceContent, variant)`.\n\nVariant mode (eval/A-B testing) supported via optional `variant: string` parameter; affects .sum path via `getSumPath(path, variant)`, AGENTS.md filename (`AGENTS.${variant}.md`), and triggers `writeAgentsMdHub(dirPath, variant)` after `writeAgentsMd()`.\n\n## Quality & Error Handling\n\nNon-throwing inconsistency detection and phantom validation wrap in try-catch, logging errors and continuing pipeline. Groups files by directory for quality checks to throttle I/O. Scales inconsistency detection to 10 concurrent directory checks.",
      "model": "claude-haiku-4-5-20251001",
      "inputTokens": 3,
      "outputTokens": 1345,
      "cacheReadTokens": 16147,
      "cacheCreationTokens": 12011,
      "latencyMs": 33413,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/orchestration/runner.ts",
          "sizeBytes": 32610
        }
      ]
    },
    {
      "timestamp": "2026-02-14T13:48:00.823Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: src/rebuild/output-parser.ts\n\n```typescript\n/**\n * AI output parser for multi-file rebuild responses.\n *\n * Extracts individual files from AI-generated responses using\n * `===FILE: path===` / `===END_FILE===` delimiters, with a fallback\n * to markdown fenced code blocks with file path annotations.\n *\n * @module\n */\n\n/**\n * Parse multi-file AI output into a Map of file paths to contents.\n *\n * Primary format: `===FILE: path===` / `===END_FILE===` delimiters.\n * Fallback format: Markdown fenced code blocks with `language:path` annotation.\n *\n * File paths are trimmed. File content is NOT trimmed (preserves indentation).\n * Returns an empty Map if neither format matches (caller handles error case).\n *\n * @param responseText - Raw AI response text\n * @returns Map of relative file paths to file contents\n */\nexport function parseModuleOutput(responseText: string): Map<string, string> {\n  // Primary: ===FILE: path=== / ===END_FILE=== delimiters\n  const files = parseDelimiterFormat(responseText);\n  if (files.size > 0) return files;\n\n  // Fallback: markdown fenced code blocks with file path annotation\n  return parseFencedBlockFormat(responseText);\n}\n\n/**\n * Parse `===FILE: path===` / `===END_FILE===` delimited output.\n *\n * Uses a line-by-line state machine requiring delimiters at column 0\n * (start of line) to avoid matching delimiter text embedded inside\n * generated source code (string literals, JSDoc, prompt templates).\n */\nfunction parseDelimiterFormat(text: string): Map<string, string> {\n  const files = new Map<string, string>();\n  const START_RE = /^===FILE:\\s*(.+?)===$/;\n  const END_RE = /^===END_FILE===$/;\n\n  const lines = text.split('\\n');\n  let currentPath: string | null = null;\n  let contentLines: string[] = [];\n\n  for (const line of lines) {\n    if (currentPath === null) {\n      // Looking for a file start delimiter\n      const startMatch = START_RE.exec(line);\n      if (startMatch) {\n        currentPath = startMatch[1].trim();\n        contentLines = [];\n      }\n      // Ignore non-delimiter lines outside of file blocks\n    } else {\n      // Inside a file block — check for end delimiter\n      if (END_RE.test(line)) {\n        files.set(currentPath, contentLines.join('\\n'));\n        currentPath = null;\n        contentLines = [];\n      } else {\n        contentLines.push(line);\n      }\n    }\n  }\n\n  // Handle unclosed file block (AI forgot ===END_FILE===)\n  if (currentPath !== null && contentLines.length > 0) {\n    files.set(currentPath, contentLines.join('\\n'));\n  }\n\n  return files;\n}\n\n/**\n * Parse markdown fenced code blocks with file path annotations.\n *\n * Matches blocks like:\n * ```language:path/to/file\n * content\n * ```\n */\nfunction parseFencedBlockFormat(text: string): Map<string, string> {\n  const files = new Map<string, string>();\n  const pattern = /```\\w*:([^\\n]+)\\n([\\s\\S]*?)```/g;\n\n  let match: RegExpExecArray | null;\n  while ((match = pattern.exec(text)) !== null) {\n    const filePath = match[1].trim();\n    const content = match[2];\n    files.set(filePath, content);\n  }\n\n  return files;\n}\n\n```\n\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n- Workflow & convention rules: if this file defines contribution guidelines, PR conventions, commit standards, testing mandates, tool usage requirements, approval workflows, AI agent behavioral instructions, or code conventions (naming standards, formatting rules, import ordering, linting policies), extract them as explicit, actionable rules. Sources: CONTRIBUTING.md, CI configs, PR templates, style guides, linter/formatter configs (.eslintrc, .prettierrc, .editorconfig), tsconfig strictness, README dev sections.\n\n\nTARGET LENGTH (MANDATORY):\n- Source file: 3011 characters\n- Target summary: ~301 characters (10% compression)\n- Maximum: 361 characters\n- Achieve this by: ultra-dense writing, minimal examples, single-sentence descriptions\n\nULTRA-COMPRESSION TECHNIQUES:\n- Use telegraphic style: \"exports foo, bar, baz\" not \"this file exports three functions\"\n- No code examples unless critical to understanding\n- One sentence per export maximum\n- Omit obvious parameter names (\"opts: Options\" not \"options: Options - configuration object\")\n- Use abbreviations: \"params\" not \"parameters\", \"config\" not \"configuration\"\n- Skip section if empty (don't write \"No special concerns\" or \"N/A\")\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nWORKFLOW & CONVENTION RULES (NEVER EXCLUDE):\n- Contribution requirements: branch naming, commit message formats, PR description templates\n- Testing mandates: required test types, coverage thresholds, \"never delete tests\" policies\n- Tool usage rules: required CLI commands, browser automation tools, linter/formatter requirements\n- Approval workflows: review requirements, CI checks that must pass, deployment gates\n- AI agent instructions: explicit directives for automated assistants (coding rules, prohibited actions)\n- Code conventions: naming standards (camelCase, PascalCase, snake_case for what), import ordering rules, file/folder naming patterns, formatting rules (indentation, quotes, semicolons), language idioms, TypeScript strictness policies, preferred patterns vs anti-patterns\nThese define how contributors (human or AI) must work within the project.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion .annex.sum file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**output-parser.ts parses AI-generated multi-file rebuild responses using delimiter and markdown fenced-block formats into a Map<string, string> of file paths to contents.**\n\n## Exports\n\n`parseModuleOutput(responseText: string): Map<string, string>` — primary entry point; tries delimiter format first, falls back to fenced-block format; returns empty Map if neither matches.\n\n## Parsing Formats\n\n**Delimiter format (primary):** `===FILE: path===` / `===END_FILE===` delimiters at line start; state machine via `parseDelimiterFormat()` with regex patterns `^===FILE:\\s*(.+?)===$/` and `^===END_FILE===$`; handles unclosed blocks by flushing remaining content.\n\n**Fenced-block format (fallback):** markdown code blocks matching `` ```language:path\\n...``` `` via pattern `/```\\w*:([^\\n]+)\\n([\\s\\S]*?)```/g` in `parseFencedBlockFormat()`.\n\n## Behavior\n\nFile paths are trimmed; content preserves indentation (not trimmed). Delimiter regex requires column-0 matches to avoid false positives in string literals/JSDoc. Unclosed `===FILE:` blocks auto-flush on EOF.",
      "model": "claude-haiku-4-5-20251001",
      "inputTokens": 3,
      "outputTokens": 298,
      "cacheReadTokens": 16147,
      "cacheCreationTokens": 3649,
      "latencyMs": 21788,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/rebuild/output-parser.ts",
          "sizeBytes": 3013
        }
      ]
    },
    {
      "timestamp": "2026-02-14T13:48:01.575Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: src/rebuild/prompts.ts\n\n```typescript\n/**\n * Prompt templates for AI-driven project reconstruction.\n *\n * Provides the system prompt that instructs the AI to generate source files\n * using `===FILE:===` / `===END_FILE===` delimiters, and a per-unit user\n * prompt builder that combines the full spec, current phase, and already-built\n * context.\n *\n * @module\n */\n\nimport type { RebuildUnit } from './types.js';\n\n/**\n * System prompt for AI-driven project reconstruction.\n *\n * Instructs the model to emit source files using `===FILE: path===` /\n * `===END_FILE===` delimiters with production-quality code that follows\n * the spec's architecture and type definitions.\n */\nexport const REBUILD_SYSTEM_PROMPT = `You reconstruct source code from a project specification.\n\nTASK:\nGenerate all source files for the described module/phase. The code must be complete, compilable, and production-ready.\n\nOUTPUT FORMAT:\nUse this exact delimiter format for EVERY file:\n\n===FILE: relative/path.ext===\n[file content]\n===END_FILE===\n\nGenerate ONLY the file content between delimiters. No markdown fencing, no commentary, no explanations outside the file delimiters.\n\nQUALITY:\n- Code must compile. Use exact type names, function signatures, and constants from the spec.\n- Follow the architecture and patterns described in the specification.\n- Imports must reference real modules described in the spec.\n- Generate production code only (no tests, no stubs, no placeholders).\n- Do not invent features not in the spec.\n- Do not add comments explaining what the spec says — write the code the spec describes.\n\nSTRICT COMPLIANCE:\n- When the specification defines exact names for functions, methods, types, classes, or constants, you MUST use those exact names. Do not invent synonyms (e.g., if the spec says done(), do not write reportSuccess()).\n- Pay close attention to the \"Interfaces for This Phase\" section in the current phase — it contains the exact signatures you must implement.\n- When \"Already Built\" context shows an exported symbol, import and use it. Do not redefine it.\n\nDELIMITER RULES:\n- ===FILE: and ===END_FILE=== MUST appear on their own line with NO leading whitespace.\n- If your generated code contains the literal text \"===FILE:\" (e.g., in a parser or template), ensure it is indented or inside a string — never at column 0.\n\nCONTEXT AWARENESS:\nWhen \"Already Built\" context is provided, import from those modules and use their exported types/functions. Do not redefine types that already exist in built modules.\nWhen \"Already Built\" context provides a function or method signature, your code MUST call it using the exact name shown. Match the API precisely.`;\n\n/**\n * Build the system + user prompt pair for a single rebuild unit.\n *\n * The user prompt includes:\n * 1. Full specification for reference\n * 2. Current phase/module to build\n * 3. Already-built context (exported signatures from prior groups)\n * 4. Output format reminder\n *\n * @param unit - The rebuild unit to generate code for\n * @param fullSpec - Concatenated content of all spec files\n * @param builtContext - Exported type signatures from previously built modules\n * @returns Prompt pair with system and user strings\n */\nexport function buildRebuildPrompt(\n  unit: RebuildUnit,\n  fullSpec: string,\n  builtContext: string | undefined,\n): { system: string; user: string } {\n  const sections: string[] = [\n    'Reconstruct the following module from this specification.',\n    '',\n    '## Full Specification',\n    '',\n    fullSpec,\n    '',\n    '## Current Phase',\n    '',\n    'Build the module described in this phase:',\n    '',\n    unit.specContent,\n  ];\n\n  if (builtContext) {\n    sections.push(\n      '',\n      '## Already Built',\n      '',\n      'The following modules have been built. Import from them as needed:',\n      '',\n      builtContext,\n    );\n  }\n\n  sections.push(\n    '',\n    '## Output Format',\n    '',\n    'Emit each file using:',\n    '===FILE: path/to/file.ext===',\n    '[content]',\n    '===END_FILE===',\n    '',\n    'Generate ALL files needed for this phase. Use relative paths from the project root.',\n  );\n\n  return {\n    system: REBUILD_SYSTEM_PROMPT,\n    user: sections.join('\\n'),\n  };\n}\n\n```\n\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n- Workflow & convention rules: if this file defines contribution guidelines, PR conventions, commit standards, testing mandates, tool usage requirements, approval workflows, AI agent behavioral instructions, or code conventions (naming standards, formatting rules, import ordering, linting policies), extract them as explicit, actionable rules. Sources: CONTRIBUTING.md, CI configs, PR templates, style guides, linter/formatter configs (.eslintrc, .prettierrc, .editorconfig), tsconfig strictness, README dev sections.\n\n\nTARGET LENGTH (MANDATORY):\n- Source file: 4139 characters\n- Target summary: ~414 characters (10% compression)\n- Maximum: 497 characters\n- Achieve this by: ultra-dense writing, minimal examples, single-sentence descriptions\n\nULTRA-COMPRESSION TECHNIQUES:\n- Use telegraphic style: \"exports foo, bar, baz\" not \"this file exports three functions\"\n- No code examples unless critical to understanding\n- One sentence per export maximum\n- Omit obvious parameter names (\"opts: Options\" not \"options: Options - configuration object\")\n- Use abbreviations: \"params\" not \"parameters\", \"config\" not \"configuration\"\n- Skip section if empty (don't write \"No special concerns\" or \"N/A\")\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nWORKFLOW & CONVENTION RULES (NEVER EXCLUDE):\n- Contribution requirements: branch naming, commit message formats, PR description templates\n- Testing mandates: required test types, coverage thresholds, \"never delete tests\" policies\n- Tool usage rules: required CLI commands, browser automation tools, linter/formatter requirements\n- Approval workflows: review requirements, CI checks that must pass, deployment gates\n- AI agent instructions: explicit directives for automated assistants (coding rules, prohibited actions)\n- Code conventions: naming standards (camelCase, PascalCase, snake_case for what), import ordering rules, file/folder naming patterns, formatting rules (indentation, quotes, semicolons), language idioms, TypeScript strictness policies, preferred patterns vs anti-patterns\nThese define how contributors (human or AI) must work within the project.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion .annex.sum file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**prompts.ts provides system and user prompt templates for AI-driven source code reconstruction from project specifications.**\n\n## Exports\n\n`REBUILD_SYSTEM_PROMPT` (string) — system message instructing AI to generate complete, compilable source files using `===FILE: path===` / `===END_FILE===` delimiters with strict adherence to spec signatures, type names, and architecture; forbids markdown fencing, tests, stubs, placeholders, and feature invention.\n\n`buildRebuildPrompt(unit: RebuildUnit, fullSpec: string, builtContext?: string): {system: string; user: string}` — assembles system + user prompt pair combining full specification, current phase content, previously-built module exports, and output format instructions for single reconstruction unit.\n\n## Behavioral Contracts\n\n**File delimiter format** (verbatim, column 0):\n```\n===FILE: relative/path.ext===\n[file content]\n===END_FILE===\n```\nNo markdown fencing, no content outside delimiters, no leading whitespace on delimiter lines.\n\n**Strict naming compliance**: exported function/type/class/constant names from spec must be used exactly as written; synonyms forbidden (e.g., `done()` not `reportSuccess()`).\n\n**Import requirements**: when \"Already Built\" context shows exported symbols, code must import and use them rather than redefine; signatures must match exactly.\n\n**Code quality mandate**: production-ready only (no tests, comments, stubs, placeholders); must compile; must follow spec architecture; must use real module names from spec; forbidden: feature invention, internal explanations of spec.\n\n## Integration\n\nDepends on `RebuildUnit` type from `./types.js`. Used by AI reconstruction pipeline to generate complete source trees from specification documents with deterministic, spec-compliant output.",
      "model": "claude-haiku-4-5-20251001",
      "inputTokens": 3,
      "outputTokens": 412,
      "cacheReadTokens": 16147,
      "cacheCreationTokens": 3835,
      "latencyMs": 23114,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/rebuild/prompts.ts",
          "sizeBytes": 4145
        }
      ]
    },
    {
      "timestamp": "2026-02-14T13:47:57.808Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: src/rebuild/orchestrator.ts\n\n```typescript\n/**\n * Rebuild execution orchestrator.\n *\n * Standalone async function that wires together the spec reader, checkpoint\n * manager, AI service, concurrency pool, and progress reporter into a\n * working rebuild pipeline. Processes rebuild units grouped by order value:\n * all units in a group run concurrently via runPool, and groups execute\n * sequentially to respect ordering dependencies.\n *\n * After each order group completes, exported type signatures are extracted\n * from the generated files and accumulated as context for subsequent groups.\n *\n * @module\n */\n\nimport * as path from 'node:path';\nimport { writeFile, mkdir, readFile, rm } from 'node:fs/promises';\nimport type { AIService } from '../ai/index.js';\nimport type { AIResponse } from '../ai/types.js';\nimport { runPool, ProgressReporter, type ProgressLog, type ITraceWriter } from '../orchestration/index.js';\nimport { CheckpointManager } from './checkpoint.js';\nimport { readSpecFiles, partitionSpec } from './spec-reader.js';\nimport { parseModuleOutput } from './output-parser.js';\nimport type { RebuildUnit, RebuildResult } from './types.js';\nimport { buildRebuildPrompt } from './prompts.js';\n\n// ---------------------------------------------------------------------------\n// Options\n// ---------------------------------------------------------------------------\n\n/**\n * Options for the rebuild execution pipeline.\n */\nexport interface RebuildExecutionOptions {\n  /** Absolute path to the output directory */\n  outputDir: string;\n  /** Maximum concurrent AI calls within each order group */\n  concurrency: number;\n  /** Stop on first failure */\n  failFast?: boolean;\n  /** Wipe output directory and start fresh */\n  force?: boolean;\n  /** Enable verbose debug logging */\n  debug?: boolean;\n  /** Trace writer for concurrency debugging */\n  tracer?: ITraceWriter;\n  /** Progress log for tail -f monitoring */\n  progressLog?: ProgressLog;\n}\n\n// ---------------------------------------------------------------------------\n// Context accumulation\n// ---------------------------------------------------------------------------\n\n/** Default character limit before truncating older group context */\nconst BUILT_CONTEXT_LIMIT = 100_000;\n\n/** Number of lines to keep from truncated files (typically imports + type declarations) */\nconst TRUNCATED_HEAD_LINES = 20;\n\n// ---------------------------------------------------------------------------\n// executeRebuild\n// ---------------------------------------------------------------------------\n\n/**\n * Execute the rebuild pipeline.\n *\n * Reads spec files, partitions into units, loads/creates checkpoint,\n * processes units grouped by order value (sequential groups, concurrent\n * within each group), accumulates built context, and returns summary.\n *\n * @param aiService - Configured AI service instance\n * @param projectRoot - Absolute path to the project root\n * @param options - Rebuild execution options\n * @returns Summary with counts of processed, failed, and skipped modules\n */\nexport async function executeRebuild(\n  aiService: AIService,\n  projectRoot: string,\n  options: RebuildExecutionOptions,\n): Promise<{ modulesProcessed: number; modulesFailed: number; modulesSkipped: number }> {\n  const { outputDir, concurrency, tracer, progressLog } = options;\n\n  // 1. Read specs\n  const specFiles = await readSpecFiles(projectRoot);\n\n  // 2. Partition into units\n  const units = partitionSpec(specFiles);\n\n  if (options.debug) {\n    console.error(`[debug] Rebuild units (${units.length}):`);\n    for (const unit of units) {\n      console.error(`[debug]   order=${unit.order} name=\"${unit.name}\"`);\n    }\n  }\n\n  // 3. Handle --force: wipe output directory\n  if (options.force) {\n    await rm(outputDir, { recursive: true, force: true });\n  }\n\n  // Ensure output directory exists\n  await mkdir(outputDir, { recursive: true });\n\n  // 4. Load/create checkpoint\n  const unitNames = units.map((u) => u.name);\n  const { manager: checkpoint, isResume } = await CheckpointManager.load(\n    outputDir,\n    specFiles,\n    unitNames,\n  );\n\n  if (isResume) {\n    const pending = checkpoint.getPendingUnits();\n    const done = unitNames.length - pending.length;\n    console.log(`Resuming from checkpoint: ${done} of ${unitNames.length} modules already complete`);\n    progressLog?.write(`Resuming from checkpoint: ${done} of ${unitNames.length} modules already complete`);\n  }\n\n  // 5. Initialize checkpoint (write to disk)\n  await checkpoint.initialize();\n\n  // 6. Filter to pending units\n  let modulesSkipped = 0;\n  const pendingUnits: RebuildUnit[] = [];\n  for (const unit of units) {\n    if (checkpoint.isDone(unit.name)) {\n      modulesSkipped++;\n    } else {\n      pendingUnits.push(unit);\n    }\n  }\n\n  if (pendingUnits.length === 0) {\n    console.log('All modules already complete. Nothing to rebuild.');\n    progressLog?.write('All modules already complete. Nothing to rebuild.');\n    return { modulesProcessed: 0, modulesFailed: 0, modulesSkipped };\n  }\n\n  // 7. Create progress reporter\n  const reporter = new ProgressReporter(pendingUnits.length, 0, progressLog);\n\n  // 8. Concatenate full spec for prompt context\n  const fullSpec = specFiles.map((f) => f.content).join('\\n\\n');\n\n  // 9. Context accumulator for export signatures\n  let builtContext = '';\n\n  // 10. Group units by order value\n  const orderGroups = new Map<number, RebuildUnit[]>();\n  for (const unit of pendingUnits) {\n    const group = orderGroups.get(unit.order) ?? [];\n    group.push(unit);\n    orderGroups.set(unit.order, group);\n  }\n\n  // Sort order keys ascending\n  const sortedOrders = [...orderGroups.keys()].sort((a, b) => a - b);\n\n  let modulesProcessed = 0;\n  let modulesFailed = 0;\n\n  // 11. Process each order group sequentially\n  for (const orderValue of sortedOrders) {\n    const groupUnits = orderGroups.get(orderValue)!;\n\n    tracer?.emit({\n      type: 'phase:start',\n      phase: `rebuild-order-${orderValue}`,\n      taskCount: groupUnits.length,\n      concurrency,\n    });\n\n    const groupStart = Date.now();\n    const filesWrittenInGroup: string[] = [];\n\n    // Create pool tasks for this group\n    const groupTasks = groupUnits.map((unit) => async (): Promise<RebuildResult> => {\n      reporter.onFileStart(unit.name);\n      const callStart = Date.now();\n\n      const prompt = buildRebuildPrompt(unit, fullSpec, builtContext || undefined);\n      const response: AIResponse = await aiService.call({\n        prompt: prompt.user,\n        systemPrompt: prompt.system,\n        taskLabel: `rebuild:${unit.name}`,\n      });\n\n      // Parse files from response\n      const files = parseModuleOutput(response.text);\n      if (files.size === 0) {\n        throw new Error(\n          `AI produced no files for unit \"${unit.name}\". Response may have used unexpected format.`,\n        );\n      }\n\n      // Write files to output directory\n      const filesWritten: string[] = [];\n      for (const [filePath, content] of files) {\n        const absolutePath = path.join(outputDir, filePath);\n        await mkdir(path.dirname(absolutePath), { recursive: true });\n        await writeFile(absolutePath, content, 'utf-8');\n        filesWritten.push(filePath);\n      }\n\n      // Update checkpoint\n      checkpoint.markDone(unit.name, filesWritten);\n\n      const durationMs = Date.now() - callStart;\n      return {\n        unitName: unit.name,\n        success: true,\n        filesWritten,\n        tokensIn: response.inputTokens,\n        tokensOut: response.outputTokens,\n        cacheReadTokens: response.cacheReadTokens,\n        cacheCreationTokens: response.cacheCreationTokens,\n        durationMs,\n        model: response.model,\n      };\n    });\n\n    // 12. Run pool with onComplete callback\n    await runPool(\n      groupTasks,\n      {\n        concurrency,\n        failFast: options.failFast,\n        tracer,\n        phaseLabel: `rebuild-order-${orderValue}`,\n        taskLabels: groupUnits.map((u) => u.name),\n      },\n      (result) => {\n        if (result.success && result.value) {\n          const v = result.value;\n          modulesProcessed++;\n          filesWrittenInGroup.push(...v.filesWritten);\n          reporter.onFileDone(\n            v.unitName,\n            v.durationMs,\n            v.tokensIn,\n            v.tokensOut,\n            v.model,\n            v.cacheReadTokens,\n            v.cacheCreationTokens,\n          );\n        } else {\n          modulesFailed++;\n          const errorMsg = result.error?.message ?? 'Unknown error';\n          const unitName = groupUnits[result.index]?.name ?? `unit-${result.index}`;\n          checkpoint.markFailed(unitName, errorMsg);\n          reporter.onFileError(unitName, errorMsg);\n        }\n      },\n    );\n\n    tracer?.emit({\n      type: 'phase:end',\n      phase: `rebuild-order-${orderValue}`,\n      durationMs: Date.now() - groupStart,\n      tasksCompleted: modulesProcessed,\n      tasksFailed: modulesFailed,\n    });\n\n    // 13. After group completes, accumulate full file content as context\n    for (const filePath of filesWrittenInGroup) {\n      // Skip non-source files (configs, docs)\n      if (filePath.endsWith('.md') || filePath.endsWith('.json') || filePath.endsWith('.yml')) continue;\n      try {\n        const content = await readFile(path.join(outputDir, filePath), 'utf-8');\n        builtContext += `\\n// === ${filePath} ===\\n${content}\\n`;\n      } catch {\n        // Non-critical: skip unreadable files\n      }\n    }\n\n    // Truncate older context if it exceeds the limit\n    if (builtContext.length > BUILT_CONTEXT_LIMIT) {\n      const sections = builtContext.split(/\\n\\/\\/ === /);\n      // Keep the first empty section prefix, truncate older sections\n      const recentCount = Math.max(1, Math.floor(sections.length / 2));\n      const olderSections = sections.slice(1, sections.length - recentCount);\n      const recentSections = sections.slice(sections.length - recentCount);\n\n      let truncated = '';\n      for (const section of olderSections) {\n        const lines = section.split('\\n');\n        const head = lines.slice(0, TRUNCATED_HEAD_LINES).join('\\n');\n        truncated += `\\n// === ${head}\\n// ... (truncated)\\n`;\n      }\n      for (const section of recentSections) {\n        truncated += `\\n// === ${section}`;\n      }\n      builtContext = truncated;\n\n      if (options.debug) {\n        console.error(`[debug] Built context truncated: keeping last ${recentCount} groups in full`);\n      }\n    }\n  }\n\n  // 14. Flush checkpoint\n  await checkpoint.flush();\n\n  // 15. Return summary\n  return { modulesProcessed, modulesFailed, modulesSkipped };\n}\n\n```\n\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n- Workflow & convention rules: if this file defines contribution guidelines, PR conventions, commit standards, testing mandates, tool usage requirements, approval workflows, AI agent behavioral instructions, or code conventions (naming standards, formatting rules, import ordering, linting policies), extract them as explicit, actionable rules. Sources: CONTRIBUTING.md, CI configs, PR templates, style guides, linter/formatter configs (.eslintrc, .prettierrc, .editorconfig), tsconfig strictness, README dev sections.\n\n\nTARGET LENGTH (MANDATORY):\n- Source file: 10509 characters\n- Target summary: ~1051 characters (10% compression)\n- Maximum: 1261 characters\n- Achieve this by: ultra-dense writing, minimal examples, single-sentence descriptions\n\nULTRA-COMPRESSION TECHNIQUES:\n- Use telegraphic style: \"exports foo, bar, baz\" not \"this file exports three functions\"\n- No code examples unless critical to understanding\n- One sentence per export maximum\n- Omit obvious parameter names (\"opts: Options\" not \"options: Options - configuration object\")\n- Use abbreviations: \"params\" not \"parameters\", \"config\" not \"configuration\"\n- Skip section if empty (don't write \"No special concerns\" or \"N/A\")\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nWORKFLOW & CONVENTION RULES (NEVER EXCLUDE):\n- Contribution requirements: branch naming, commit message formats, PR description templates\n- Testing mandates: required test types, coverage thresholds, \"never delete tests\" policies\n- Tool usage rules: required CLI commands, browser automation tools, linter/formatter requirements\n- Approval workflows: review requirements, CI checks that must pass, deployment gates\n- AI agent instructions: explicit directives for automated assistants (coding rules, prohibited actions)\n- Code conventions: naming standards (camelCase, PascalCase, snake_case for what), import ordering rules, file/folder naming patterns, formatting rules (indentation, quotes, semicolons), language idioms, TypeScript strictness policies, preferred patterns vs anti-patterns\nThese define how contributors (human or AI) must work within the project.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion .annex.sum file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**orchestrator.ts orchestrates rebuild execution by grouping rebuild units by order, processing groups sequentially with concurrent units within each group, accumulating export context between groups.**\n\n## Exports\n\n`executeRebuild(aiService: AIService, projectRoot: string, options: RebuildExecutionOptions): Promise<{ modulesProcessed: number; modulesFailed: number; modulesSkipped: number }>` — Main entry point; reads spec files, partitions into RebuildUnits, loads/creates CheckpointManager, processes order groups sequentially (each group runs concurrently), accumulates export signatures from generated files as context for downstream groups, returns module counts.\n\n`RebuildExecutionOptions` — Configuration interface: `outputDir` (absolute output path), `concurrency` (max concurrent AI calls per group), `failFast?` (bool), `force?` (bool, wipe output), `debug?` (bool), `tracer?` (ITraceWriter), `progressLog?` (ProgressLog).\n\n## Pipeline Architecture\n\nReads spec files via `readSpecFiles`, partitions via `partitionSpec`, creates/resumes CheckpointManager via `CheckpointManager.load`, groups pending units by `order` field, sorts groups ascending, executes sequentially. Within each group, creates task functions (async) and dispatches via `runPool` with concurrency limit. After group completes, reads written files (excluding .md/.json/.yml), extracts content, accumulates in `builtContext` string for prompt context in subsequent groups.\n\n## Context Accumulation\n\n`builtContext` string accumulates full file content from generated modules. Truncation strategy: when length exceeds `BUILT_CONTEXT_LIMIT` (100,000 chars), splits on section markers (`// === [filePath] ===`), keeps recent half-groups in full, truncates older sections to `TRUNCATED_HEAD_LINES` (20 lines, typically imports/type declarations) with ellipsis.\n\n## Prompt & AI Integration\n\nBuilds prompt via `buildRebuildPrompt(unit, fullSpec, builtContext || undefined)`, calls `aiService.call({ prompt, systemPrompt, taskLabel })`, parses response text via `parseModuleOutput` (returns `Map<filePath, content>`), writes files to `outputDir` with parent directory creation, updates checkpoint via `checkpoint.markDone(name, filesWritten)`.\n\n## Checkpoint & Resumption\n\n`CheckpointManager.load(outputDir, specFiles, unitNames)` returns manager and `isResume` flag. On resume, skips units where `checkpoint.isDone(name)` returns true. Failed units marked via `checkpoint.markFailed(name, errorMsg)`. Final `checkpoint.flush()` persists state to disk.\n\n## Error Handling & Progress\n\nFailures collected in `modulesFailed` counter; `failFast` option stops on first error. `ProgressReporter` tracks via `onFileStart/onFileDone/onFileError`, receives token counts from `AIResponse`. Tracer emits phase events: `phase:start` (order group, task count, concurrency), `phase:end` (duration, task counts).",
      "model": "claude-haiku-4-5-20251001",
      "inputTokens": 3,
      "outputTokens": 732,
      "cacheReadTokens": 0,
      "cacheCreationTokens": 21749,
      "latencyMs": 27268,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/rebuild/orchestrator.ts",
          "sizeBytes": 10509
        }
      ]
    },
    {
      "timestamp": "2026-02-14T13:48:06.365Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: src/rebuild/types.ts\n\n```typescript\n/**\n * Types for the rebuild module.\n *\n * Provides Zod-validated checkpoint schema and interfaces for rebuild\n * units, plans, and results.\n *\n * @module\n */\n\nimport { z } from 'zod';\n\n/**\n * Zod schema for the rebuild checkpoint file.\n *\n * Validated when reading from disk to detect corruption or version mismatch.\n */\nexport const RebuildCheckpointSchema = z.object({\n  version: z.string(),\n  createdAt: z.string(),\n  updatedAt: z.string(),\n  outputDir: z.string(),\n  specHashes: z.record(z.string()),\n  modules: z.record(z.object({\n    status: z.enum(['pending', 'done', 'failed']),\n    completedAt: z.string().optional(),\n    error: z.string().optional(),\n    filesWritten: z.array(z.string()).optional(),\n  })),\n});\n\n/**\n * Checkpoint state persisted to `.rebuild-checkpoint` inside the output directory.\n *\n * Tracks per-module completion status and spec file hashes for drift detection.\n */\nexport type RebuildCheckpoint = z.infer<typeof RebuildCheckpointSchema>;\n\n/**\n * A single rebuild unit representing one AI call.\n *\n * Each unit produces all files for a logical module/phase of the project.\n */\nexport interface RebuildUnit {\n  /** Unit name derived from spec section heading */\n  name: string;\n  /** The spec section content for this unit */\n  specContent: string;\n  /** Execution order from Build Plan phase numbering */\n  order: number;\n}\n\n/**\n * The full rebuild plan computed from spec files before execution.\n */\nexport interface RebuildPlan {\n  /** Spec files read from specs/ directory */\n  specFiles: Array<{ relativePath: string; content: string }>;\n  /** Ordered rebuild units extracted from spec content */\n  units: RebuildUnit[];\n  /** Output directory for rebuilt project */\n  outputDir: string;\n}\n\n/**\n * Result of rebuilding a single unit.\n */\nexport interface RebuildResult {\n  /** Name of the rebuild unit */\n  unitName: string;\n  /** Whether the rebuild succeeded */\n  success: boolean;\n  /** Relative paths of files written within the output directory */\n  filesWritten: string[];\n  /** Input tokens consumed */\n  tokensIn: number;\n  /** Output tokens produced */\n  tokensOut: number;\n  /** Cache read tokens */\n  cacheReadTokens: number;\n  /** Cache creation tokens */\n  cacheCreationTokens: number;\n  /** Duration in milliseconds */\n  durationMs: number;\n  /** Model used for generation */\n  model: string;\n  /** Error message if rebuild failed */\n  error?: string;\n}\n\n```\n\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n- Workflow & convention rules: if this file defines contribution guidelines, PR conventions, commit standards, testing mandates, tool usage requirements, approval workflows, AI agent behavioral instructions, or code conventions (naming standards, formatting rules, import ordering, linting policies), extract them as explicit, actionable rules. Sources: CONTRIBUTING.md, CI configs, PR templates, style guides, linter/formatter configs (.eslintrc, .prettierrc, .editorconfig), tsconfig strictness, README dev sections.\n\n\nTARGET LENGTH (MANDATORY):\n- Source file: 2406 characters\n- Target summary: ~241 characters (10% compression)\n- Maximum: 289 characters\n- Achieve this by: ultra-dense writing, minimal examples, single-sentence descriptions\n\nULTRA-COMPRESSION TECHNIQUES:\n- Use telegraphic style: \"exports foo, bar, baz\" not \"this file exports three functions\"\n- No code examples unless critical to understanding\n- One sentence per export maximum\n- Omit obvious parameter names (\"opts: Options\" not \"options: Options - configuration object\")\n- Use abbreviations: \"params\" not \"parameters\", \"config\" not \"configuration\"\n- Skip section if empty (don't write \"No special concerns\" or \"N/A\")\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nWORKFLOW & CONVENTION RULES (NEVER EXCLUDE):\n- Contribution requirements: branch naming, commit message formats, PR description templates\n- Testing mandates: required test types, coverage thresholds, \"never delete tests\" policies\n- Tool usage rules: required CLI commands, browser automation tools, linter/formatter requirements\n- Approval workflows: review requirements, CI checks that must pass, deployment gates\n- AI agent instructions: explicit directives for automated assistants (coding rules, prohibited actions)\n- Code conventions: naming standards (camelCase, PascalCase, snake_case for what), import ordering rules, file/folder naming patterns, formatting rules (indentation, quotes, semicolons), language idioms, TypeScript strictness policies, preferred patterns vs anti-patterns\nThese define how contributors (human or AI) must work within the project.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion .annex.sum file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**types.ts defines Zod-validated schemas and TypeScript types for rebuild checkpoint persistence, units, plans, and per-module results.**\n\n## Exported Types & Schemas\n\n`RebuildCheckpointSchema` — Zod object schema validating checkpoint file structure: `version` (string), `createdAt`/`updatedAt` (ISO strings), `outputDir` (string), `specHashes` (record of string hashes), `modules` (record with `status` enum `['pending', 'done', 'failed']`, optional `completedAt`/`error`/`filesWritten`).\n\n`RebuildCheckpoint` — Type inferred from `RebuildCheckpointSchema`; persists to `.rebuild-checkpoint` file in output directory to track per-module completion status and spec file hashes for drift detection.\n\n`RebuildUnit` — Interface with `name` (derived from spec section heading), `specContent` (string), `order` (number from Build Plan phase numbering); represents single AI call producing all files for one logical module/phase.\n\n`RebuildPlan` — Interface with `specFiles` (array of `{relativePath, content}`), `units` (ordered `RebuildUnit[]`), `outputDir` (string); computed from spec files before execution.\n\n`RebuildResult` — Interface tracking single unit rebuild outcome: `unitName`, `success` (boolean), `filesWritten` (relative paths), `tokensIn`/`tokensOut`/`cacheReadTokens`/`cacheCreationTokens`, `durationMs`, `model`, optional `error`.\n\n## Checkpoint Persistence Contract\n\nCheckpoint file `.rebuild-checkpoint` stores version-stamped state for corruption/mismatch detection on disk read; `specHashes` enables drift detection across rebuilds.",
      "model": "claude-haiku-4-5-20251001",
      "inputTokens": 3,
      "outputTokens": 419,
      "cacheReadTokens": 16147,
      "cacheCreationTokens": 3427,
      "latencyMs": 21160,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/rebuild/types.ts",
          "sizeBytes": 2406
        }
      ]
    },
    {
      "timestamp": "2026-02-14T13:48:09.072Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: src/specify/index.ts\n\n```typescript\nexport { buildSpecPrompt } from './prompts.js';\nexport type { SpecPrompt } from './prompts.js';\n\nexport { writeSpec, SpecExistsError } from './writer.js';\nexport type { WriteSpecOptions } from './writer.js';\n\n```\n\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n- Workflow & convention rules: if this file defines contribution guidelines, PR conventions, commit standards, testing mandates, tool usage requirements, approval workflows, AI agent behavioral instructions, or code conventions (naming standards, formatting rules, import ordering, linting policies), extract them as explicit, actionable rules. Sources: CONTRIBUTING.md, CI configs, PR templates, style guides, linter/formatter configs (.eslintrc, .prettierrc, .editorconfig), tsconfig strictness, README dev sections.\n\n\nTARGET LENGTH (MANDATORY):\n- Source file: 208 characters\n- Target summary: ~21 characters (10% compression)\n- Maximum: 25 characters\n- Achieve this by: ultra-dense writing, minimal examples, single-sentence descriptions\n\nULTRA-COMPRESSION TECHNIQUES:\n- Use telegraphic style: \"exports foo, bar, baz\" not \"this file exports three functions\"\n- No code examples unless critical to understanding\n- One sentence per export maximum\n- Omit obvious parameter names (\"opts: Options\" not \"options: Options - configuration object\")\n- Use abbreviations: \"params\" not \"parameters\", \"config\" not \"configuration\"\n- Skip section if empty (don't write \"No special concerns\" or \"N/A\")\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nWORKFLOW & CONVENTION RULES (NEVER EXCLUDE):\n- Contribution requirements: branch naming, commit message formats, PR description templates\n- Testing mandates: required test types, coverage thresholds, \"never delete tests\" policies\n- Tool usage rules: required CLI commands, browser automation tools, linter/formatter requirements\n- Approval workflows: review requirements, CI checks that must pass, deployment gates\n- AI agent instructions: explicit directives for automated assistants (coding rules, prohibited actions)\n- Code conventions: naming standards (camelCase, PascalCase, snake_case for what), import ordering rules, file/folder naming patterns, formatting rules (indentation, quotes, semicolons), language idioms, TypeScript strictness policies, preferred patterns vs anti-patterns\nThese define how contributors (human or AI) must work within the project.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion .annex.sum file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**`index.ts` re-exports specification generation and writing utilities from `prompts.js` and `writer.js` modules.**\n\n## Exported Functions\n- `buildSpecPrompt()` — generates specification prompt from `prompts.js`\n- `writeSpec()` — writes specification file from `writer.js`\n\n## Exported Types\n- `SpecPrompt` — specification prompt type from `prompts.js`\n- `WriteSpecOptions` — options type for `writeSpec()` from `writer.js`\n\n## Exported Errors\n- `SpecExistsError` — error class from `writer.js` for existing specification conflicts",
      "model": "claude-haiku-4-5-20251001",
      "inputTokens": 3,
      "outputTokens": 149,
      "cacheReadTokens": 16147,
      "cacheCreationTokens": 2792,
      "latencyMs": 22451,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/specify/index.ts",
          "sizeBytes": 208
        }
      ]
    },
    {
      "timestamp": "2026-02-14T13:48:12.907Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: src/types/index.ts\n\n```typescript\n/**\n * Shared types used across modules\n */\n\n/**\n * Information about an excluded file\n */\nexport interface ExcludedFile {\n  /** Absolute or relative path to the excluded file */\n  path: string;\n  /** Reason for exclusion (e.g., \"gitignore pattern\", \"binary file\", \"vendor directory\") */\n  reason: string;\n}\n\n/**\n * Result of the file discovery process\n */\nexport interface DiscoveryResult {\n  /** List of file paths that should be analyzed */\n  files: string[];\n  /** List of files that were excluded with reasons */\n  excluded: ExcludedFile[];\n}\n\n/**\n * Statistics about the discovery process\n */\nexport interface DiscoveryStats {\n  /** Total number of files found */\n  totalFiles: number;\n  /** Number of files included for analysis */\n  includedFiles: number;\n  /** Number of files excluded */\n  excludedFiles: number;\n  /** Breakdown of exclusion reasons */\n  exclusionReasons: Record<string, number>;\n}\n\n```\n\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n- Workflow & convention rules: if this file defines contribution guidelines, PR conventions, commit standards, testing mandates, tool usage requirements, approval workflows, AI agent behavioral instructions, or code conventions (naming standards, formatting rules, import ordering, linting policies), extract them as explicit, actionable rules. Sources: CONTRIBUTING.md, CI configs, PR templates, style guides, linter/formatter configs (.eslintrc, .prettierrc, .editorconfig), tsconfig strictness, README dev sections.\n\n\nTARGET LENGTH (MANDATORY):\n- Source file: 909 characters\n- Target summary: ~91 characters (10% compression)\n- Maximum: 109 characters\n- Achieve this by: ultra-dense writing, minimal examples, single-sentence descriptions\n\nULTRA-COMPRESSION TECHNIQUES:\n- Use telegraphic style: \"exports foo, bar, baz\" not \"this file exports three functions\"\n- No code examples unless critical to understanding\n- One sentence per export maximum\n- Omit obvious parameter names (\"opts: Options\" not \"options: Options - configuration object\")\n- Use abbreviations: \"params\" not \"parameters\", \"config\" not \"configuration\"\n- Skip section if empty (don't write \"No special concerns\" or \"N/A\")\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nWORKFLOW & CONVENTION RULES (NEVER EXCLUDE):\n- Contribution requirements: branch naming, commit message formats, PR description templates\n- Testing mandates: required test types, coverage thresholds, \"never delete tests\" policies\n- Tool usage rules: required CLI commands, browser automation tools, linter/formatter requirements\n- Approval workflows: review requirements, CI checks that must pass, deployment gates\n- AI agent instructions: explicit directives for automated assistants (coding rules, prohibited actions)\n- Code conventions: naming standards (camelCase, PascalCase, snake_case for what), import ordering rules, file/folder naming patterns, formatting rules (indentation, quotes, semicolons), language idioms, TypeScript strictness policies, preferred patterns vs anti-patterns\nThese define how contributors (human or AI) must work within the project.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion .annex.sum file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**src/types/index.ts exports ExcludedFile, DiscoveryResult, DiscoveryStats interfaces for file discovery metadata and statistics.**\n\n## Exported Types\n\n- `ExcludedFile` — interface with `path: string` (absolute or relative file path) and `reason: string` (exclusion cause: \"gitignore pattern\", \"binary file\", \"vendor directory\")\n- `DiscoveryResult` — interface with `files: string[]` (analyzed file paths) and `excluded: ExcludedFile[]` (excluded files with reasons)\n- `DiscoveryStats` — interface with `totalFiles: number`, `includedFiles: number`, `excludedFiles: number`, `exclusionReasons: Record<string, number>` (exclusion reason breakdown)",
      "model": "claude-haiku-4-5-20251001",
      "inputTokens": 3,
      "outputTokens": 182,
      "cacheReadTokens": 16147,
      "cacheCreationTokens": 2981,
      "latencyMs": 20234,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/types/index.ts",
          "sizeBytes": 909
        }
      ]
    },
    {
      "timestamp": "2026-02-14T13:48:03.163Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: src/rebuild/spec-reader.ts\n\n```typescript\n/**\n * Spec file reader and partitioner for the rebuild module.\n *\n * Reads spec files from `specs/` and partitions them into rebuild units\n * based on the Build Plan section or top-level headings.\n *\n * @module\n */\n\nimport * as path from 'node:path';\nimport { readdir, readFile } from 'node:fs/promises';\nimport type { RebuildUnit } from './types.js';\n\n/**\n * Read all `.md` spec files from the `specs/` directory.\n *\n * Files are returned sorted alphabetically by filename.\n * Throws a descriptive error if `specs/` doesn't exist or has no `.md` files.\n *\n * @param projectRoot - Absolute path to the project root\n * @returns Array of spec file objects with relative path and content\n */\nexport async function readSpecFiles(\n  projectRoot: string,\n): Promise<Array<{ relativePath: string; content: string }>> {\n  const specsDir = path.join(projectRoot, 'specs');\n\n  let entries: string[];\n  try {\n    entries = await readdir(specsDir);\n  } catch {\n    throw new Error(\n      'No spec files found in specs/. Run \"are specify\" first.',\n    );\n  }\n\n  const mdFiles = entries.filter((e) => e.endsWith('.md')).sort();\n\n  if (mdFiles.length === 0) {\n    throw new Error(\n      'No spec files found in specs/. Run \"are specify\" first.',\n    );\n  }\n\n  const results: Array<{ relativePath: string; content: string }> = [];\n  for (const file of mdFiles) {\n    const filePath = path.join(specsDir, file);\n    const content = await readFile(filePath, 'utf-8');\n    results.push({ relativePath: `specs/${file}`, content });\n  }\n\n  return results;\n}\n\n/**\n * Partition spec content into ordered rebuild units.\n *\n * Strategy:\n * 1. Concatenate all spec file contents\n * 2. Look for a \"Build Plan\" section with phase headings\n * 3. Each phase becomes a RebuildUnit with context from Architecture and Public API Surface\n * 4. Falls back to splitting on top-level `## ` headings if no Build Plan found\n *\n * Throws a descriptive error if no rebuild units can be extracted.\n * Logs a warning and skips units with empty content.\n *\n * @param specFiles - Spec files from readSpecFiles()\n * @returns Ordered array of rebuild units\n */\nexport function partitionSpec(\n  specFiles: Array<{ relativePath: string; content: string }>,\n): RebuildUnit[] {\n  const fullContent = specFiles.map((f) => f.content).join('\\n\\n');\n\n  // Try Build Plan strategy first\n  let units = extractFromBuildPlan(fullContent);\n\n  // Fall back to top-level headings\n  if (units.length === 0) {\n    units = extractFromTopLevelHeadings(fullContent);\n  }\n\n  // Validate: must have at least one unit\n  if (units.length === 0) {\n    throw new Error(\n      'Could not extract rebuild units from spec files. Expected either a \"## Build Plan\" section with \"### Phase N:\" subsections, or top-level \"## \" headings. Check your spec file format.',\n    );\n  }\n\n  // Filter out empty units with warning\n  const validUnits: RebuildUnit[] = [];\n  for (const unit of units) {\n    if (!unit.specContent.trim()) {\n      console.error(`[warn] Skipping empty spec section: \"${unit.name}\"`);\n      continue;\n    }\n    validUnits.push(unit);\n  }\n\n  // Re-validate after filtering\n  if (validUnits.length === 0) {\n    throw new Error(\n      'Could not extract rebuild units from spec files. Expected either a \"## Build Plan\" section with \"### Phase N:\" subsections, or top-level \"## \" headings. Check your spec file format.',\n    );\n  }\n\n  return validUnits.sort((a, b) => a.order - b.order);\n}\n\n/**\n * Extract rebuild units from the Build Plan section.\n *\n * Looks for `## 9. Build Plan` or `## Build Plan`, then extracts\n * `### Phase N:` subsections. Each phase gets context from the\n * Architecture and Public API Surface sections.\n */\nfunction extractFromBuildPlan(fullContent: string): RebuildUnit[] {\n  // Find the Build Plan section\n  const buildPlanMatch = fullContent.match(\n    /^(## (?:\\d+\\.\\s*)?Build Plan)\\s*$/m,\n  );\n  if (!buildPlanMatch) return [];\n\n  const buildPlanStart = buildPlanMatch.index!;\n\n  // Find the end of the Build Plan section (next ## heading or end of content)\n  const afterBuildPlan = fullContent.slice(\n    buildPlanStart + buildPlanMatch[0].length,\n  );\n  const nextH2Match = afterBuildPlan.match(/^## /m);\n  const buildPlanContent = nextH2Match\n    ? afterBuildPlan.slice(0, nextH2Match.index)\n    : afterBuildPlan;\n\n  // Extract Architecture section for context (always included in full)\n  const architectureContent = extractSection(fullContent, 'Architecture');\n\n  // Extract Public API Surface section for targeted injection\n  const apiContent = extractSection(fullContent, 'Public API Surface');\n  const apiSubsections = apiContent ? extractSubsections(apiContent) : new Map<string, string>();\n\n  // Extract Data Structures and Behavioral Contracts for targeted injection\n  const dataStructuresContent = extractSection(fullContent, 'Data Structures');\n  const dataSubsections = dataStructuresContent ? extractSubsections(dataStructuresContent) : new Map<string, string>();\n\n  const behavioralContent = extractSection(fullContent, 'Behavioral Contracts');\n  const behavioralSubsections = behavioralContent ? extractSubsections(behavioralContent) : new Map<string, string>();\n\n  // Extract File Manifest for per-phase file list injection\n  const manifestContent = extractSection(fullContent, 'File Manifest');\n\n  // Check if any phase has Defines:/Consumes: lists (Change 2 format)\n  const hasDefinesConsumes = buildPlanContent.match(/^\\*\\*Defines:\\*\\*|^Defines:/m) !== null;\n\n  // Extract phase subsections (### Phase N: ...)\n  const phasePattern = /^### Phase (\\d+):\\s*(.+)$/gm;\n  const phases: Array<{ number: number; name: string; startIndex: number }> = [];\n\n  let phaseMatch: RegExpExecArray | null;\n  while ((phaseMatch = phasePattern.exec(buildPlanContent)) !== null) {\n    phases.push({\n      number: parseInt(phaseMatch[1], 10),\n      name: phaseMatch[2].trim(),\n      startIndex: phaseMatch.index,\n    });\n  }\n\n  if (phases.length === 0) return [];\n\n  // Extract content for each phase with targeted API injection\n  const units: RebuildUnit[] = [];\n  for (let i = 0; i < phases.length; i++) {\n    const phase = phases[i];\n    const contentStart = phase.startIndex;\n    const contentEnd = i + 1 < phases.length\n      ? phases[i + 1].startIndex\n      : buildPlanContent.length;\n    const phaseContent = buildPlanContent.slice(contentStart, contentEnd).trim();\n\n    // Build context prefix per phase\n    const contextParts: string[] = [];\n    if (architectureContent) {\n      contextParts.push(`## Architecture\\n\\n${architectureContent}`);\n    }\n\n    if (hasDefinesConsumes && apiSubsections.size > 0) {\n      // Targeted injection: only relevant API subsections\n      const relevantApi = findRelevantSubsections(phaseContent, apiSubsections);\n      if (relevantApi) {\n        contextParts.push(`## Interfaces for This Phase\\n\\n${relevantApi}`);\n      }\n      const relevantData = findRelevantSubsections(phaseContent, dataSubsections);\n      if (relevantData) {\n        contextParts.push(`## Data Structures for This Phase\\n\\n${relevantData}`);\n      }\n      const relevantBehavior = findRelevantSubsections(phaseContent, behavioralSubsections);\n      if (relevantBehavior) {\n        contextParts.push(`## Behavioral Contracts for This Phase\\n\\n${relevantBehavior}`);\n      }\n      if (manifestContent) {\n        const phaseFileEntries = extractManifestEntriesForPhase(manifestContent, phaseContent);\n        if (phaseFileEntries) {\n          contextParts.push(`## Files to Generate in This Phase\\n\\n${phaseFileEntries}`);\n        }\n      }\n    } else {\n      // Graceful degradation: full API Surface for older specs without Defines:/Consumes:\n      if (apiContent) {\n        contextParts.push(`## Public API Surface\\n\\n${apiContent}`);\n      }\n    }\n\n    const contextPrefix = contextParts.length > 0\n      ? contextParts.join('\\n\\n') + '\\n\\n'\n      : '';\n\n    units.push({\n      name: `Phase ${phase.number}: ${phase.name}`,\n      specContent: contextPrefix + phaseContent,\n      order: phase.number,\n    });\n  }\n\n  return units;\n}\n\n/**\n * Fallback: extract rebuild units from top-level `## ` headings.\n *\n * Each `## ` section becomes a unit with order matching its position.\n */\nfunction extractFromTopLevelHeadings(fullContent: string): RebuildUnit[] {\n  const headingPattern = /^## (.+)$/gm;\n  const headings: Array<{ name: string; startIndex: number }> = [];\n\n  let headingMatch: RegExpExecArray | null;\n  while ((headingMatch = headingPattern.exec(fullContent)) !== null) {\n    headings.push({\n      name: headingMatch[1].trim(),\n      startIndex: headingMatch.index,\n    });\n  }\n\n  if (headings.length === 0) return [];\n\n  const units: RebuildUnit[] = [];\n  for (let i = 0; i < headings.length; i++) {\n    const heading = headings[i];\n    const contentStart = heading.startIndex;\n    const contentEnd = i + 1 < headings.length\n      ? headings[i + 1].startIndex\n      : fullContent.length;\n    const sectionContent = fullContent.slice(contentStart, contentEnd).trim();\n\n    units.push({\n      name: heading.name,\n      specContent: sectionContent,\n      order: i + 1,\n    });\n  }\n\n  return units;\n}\n\n/**\n * Parse a section's content into subsections keyed by `### ` headings.\n *\n * @param sectionContent - Content of a spec section (without the `## ` heading)\n * @returns Map from heading text to full subsection content (including the heading)\n */\nfunction extractSubsections(sectionContent: string): Map<string, string> {\n  const result = new Map<string, string>();\n  const pattern = /^### (.+)$/gm;\n  const headings: Array<{ key: string; startIndex: number }> = [];\n\n  let match: RegExpExecArray | null;\n  while ((match = pattern.exec(sectionContent)) !== null) {\n    headings.push({ key: match[1].trim(), startIndex: match.index });\n  }\n\n  for (let i = 0; i < headings.length; i++) {\n    const heading = headings[i];\n    const end = i + 1 < headings.length\n      ? headings[i + 1].startIndex\n      : sectionContent.length;\n    result.set(heading.key, sectionContent.slice(heading.startIndex, end).trim());\n  }\n\n  return result;\n}\n\n/**\n * Find subsections relevant to a phase based on its content.\n *\n * Matches using:\n * 1. \"Defines:\" and \"Consumes:\" lists (exact symbol names)\n * 2. File paths and module names mentioned in the phase's task list\n *\n * @param phaseContent - Raw text of the phase section\n * @param subsections - Map of subsection heading → content\n * @returns Concatenated matching subsections, or null if none match\n */\nfunction findRelevantSubsections(\n  phaseContent: string,\n  subsections: Map<string, string>,\n): string | null {\n  if (subsections.size === 0) return null;\n\n  // Extract words from Defines: and Consumes: lines\n  const definesMatch = phaseContent.match(/(?:\\*\\*Defines:\\*\\*|^Defines:)\\s*(.+)/m);\n  const consumesMatch = phaseContent.match(/(?:\\*\\*Consumes:\\*\\*|^Consumes:)\\s*(.+)/m);\n\n  const keywords: string[] = [];\n  if (definesMatch) {\n    // Split on commas and semicolons, extract identifiers\n    keywords.push(...definesMatch[1].split(/[,;]/).map((s) => s.trim()).filter(Boolean));\n  }\n  if (consumesMatch) {\n    keywords.push(...consumesMatch[1].split(/[,;]/).map((s) => s.trim()).filter(Boolean));\n  }\n\n  // Also extract file paths and module-like references from the phase text\n  const pathRefs = phaseContent.match(/\\b(?:src\\/[\\w\\-./]+|[\\w-]+\\.(?:ts|js|py|rs|go))\\b/g);\n  if (pathRefs) {\n    keywords.push(...pathRefs);\n  }\n\n  if (keywords.length === 0) {\n    // No structured references found — return all subsections as fallback\n    return [...subsections.values()].join('\\n\\n');\n  }\n\n  // Lowercase the keywords for case-insensitive matching\n  const lowerKeywords = keywords.map((k) => k.toLowerCase());\n\n  const matched: string[] = [];\n  for (const [key, content] of subsections) {\n    const lowerKey = key.toLowerCase();\n    // Check if any keyword appears in the subsection heading\n    const isRelevant = lowerKeywords.some((kw) =>\n      lowerKey.includes(kw) || kw.includes(lowerKey) ||\n      // Fuzzy: check individual words from the keyword against the key\n      kw.split(/[\\s/()]+/).some((word) => word.length > 3 && lowerKey.includes(word)),\n    );\n    if (isRelevant) {\n      matched.push(content);\n    }\n  }\n\n  return matched.length > 0 ? matched.join('\\n\\n') : null;\n}\n\n/**\n * Extract file manifest entries relevant to a specific phase.\n *\n * Matches manifest lines against the phase's \"Defines:\" list symbols.\n * Returns null if no \"Defines:\" list or no matching manifest lines found.\n *\n * @param manifestContent - Full File Manifest section content\n * @param phaseContent - Raw text of the phase section\n * @returns Matching manifest lines, or null if none match\n */\nfunction extractManifestEntriesForPhase(\n  manifestContent: string,\n  phaseContent: string,\n): string | null {\n  // Extract \"Defines:\" list from phase\n  const definesMatch = phaseContent.match(/(?:\\*\\*Defines:\\*\\*|^Defines:)\\s*(.+)/m);\n  if (!definesMatch) return null;\n\n  const symbols = definesMatch[1].split(/[,;]/).map(s => s.trim()).filter(Boolean);\n  if (symbols.length === 0) return null;\n\n  // Find manifest lines that mention any defined symbol\n  const lines = manifestContent.split('\\n');\n  const matches = lines.filter(line =>\n    symbols.some(sym => line.includes(sym)),\n  );\n\n  return matches.length > 0 ? matches.join('\\n') : null;\n}\n\n/**\n * Extract a named section's content from the full spec.\n *\n * Looks for `## N. Name` or `## Name` and returns content up to next `## `.\n */\nfunction extractSection(\n  fullContent: string,\n  sectionName: string,\n): string | null {\n  const pattern = new RegExp(\n    `^## (?:\\\\d+\\\\.\\\\s*)?${sectionName}\\\\s*Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: src/rebuild/spec-reader.ts\n\n```typescript\n,\n    'm',\n  );\n  const match = fullContent.match(pattern);\n  if (!match) return null;\n\n  const sectionStart = match.index! + match[0].length;\n  const afterSection = fullContent.slice(sectionStart);\n  const nextH2 = afterSection.match(/^## /m);\n  const content = nextH2\n    ? afterSection.slice(0, nextH2.index)\n    : afterSection;\n\n  return content.trim() || null;\n}\n\n```\n\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n- Workflow & convention rules: if this file defines contribution guidelines, PR conventions, commit standards, testing mandates, tool usage requirements, approval workflows, AI agent behavioral instructions, or code conventions (naming standards, formatting rules, import ordering, linting policies), extract them as explicit, actionable rules. Sources: CONTRIBUTING.md, CI configs, PR templates, style guides, linter/formatter configs (.eslintrc, .prettierrc, .editorconfig), tsconfig strictness, README dev sections.\n\n\nTARGET LENGTH (MANDATORY):\n- Source file: 13952 characters\n- Target summary: ~1395 characters (10% compression)\n- Maximum: 1674 characters\n- Achieve this by: ultra-dense writing, minimal examples, single-sentence descriptions\n\nULTRA-COMPRESSION TECHNIQUES:\n- Use telegraphic style: \"exports foo, bar, baz\" not \"this file exports three functions\"\n- No code examples unless critical to understanding\n- One sentence per export maximum\n- Omit obvious parameter names (\"opts: Options\" not \"options: Options - configuration object\")\n- Use abbreviations: \"params\" not \"parameters\", \"config\" not \"configuration\"\n- Skip section if empty (don't write \"No special concerns\" or \"N/A\")\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nWORKFLOW & CONVENTION RULES (NEVER EXCLUDE):\n- Contribution requirements: branch naming, commit message formats, PR description templates\n- Testing mandates: required test types, coverage thresholds, \"never delete tests\" policies\n- Tool usage rules: required CLI commands, browser automation tools, linter/formatter requirements\n- Approval workflows: review requirements, CI checks that must pass, deployment gates\n- AI agent instructions: explicit directives for automated assistants (coding rules, prohibited actions)\n- Code conventions: naming standards (camelCase, PascalCase, snake_case for what), import ordering rules, file/folder naming patterns, formatting rules (indentation, quotes, semicolons), language idioms, TypeScript strictness policies, preferred patterns vs anti-patterns\nThese define how contributors (human or AI) must work within the project.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion .annex.sum file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**spec-reader.ts reads and partitions spec files from `specs/` directory into ordered rebuild units based on Build Plan sections or top-level headings.**\n\n## Exported Functions\n\n`readSpecFiles(projectRoot: string): Promise<Array<{ relativePath: string; content: string }>>` — reads all `.md` files from `specs/` directory, sorted alphabetically; throws `'No spec files found in specs/. Run \"are specify\" first.'` if directory missing or empty.\n\n`partitionSpec(specFiles: Array<{ relativePath: string; content: string }>): RebuildUnit[]` — partitions spec content into ordered `RebuildUnit` array using dual strategy: (1) extract from `## Build Plan` section with `### Phase N:` subsections, or (2) fall back to `## ` top-level headings; injects contextual sections (Architecture, Public API Surface, Data Structures, Behavioral Contracts, File Manifest) per-phase when `Defines:`/`Consumes:` metadata present; filters empty units with warning; throws if no units extracted.\n\n## Internal Extraction Strategy\n\n`extractFromBuildPlan(fullContent: string): RebuildUnit[]` — extracts phases from `## (?:\\d+\\.\\s*)?Build Plan` section, detects `### Phase (\\d+):` subsections, injects Architecture section unconditionally, performs targeted subsection injection (API, Data Structures, Behavioral Contracts, File Manifest) when `Defines:`/`Consumes:` format detected, gracefully degrades to full Public API Surface for older specs.\n\n`extractFromTopLevelHeadings(fullContent: string): RebuildUnit[]` — fallback parser extracting `## ` sections as units ordered by position.\n\n`extractSubsections(sectionContent: string): Map<string, string>` — parses section into `Map<heading, content>` keyed by `### ` headings; includes heading in each subsection value.\n\n`findRelevantSubsections(phaseContent: string, subsections: Map<string, string>): string | null` — matches subsections by extracting `Defines:`/`Consumes:` lists and file paths (`src/[\\w\\-./]+`, `[\\w-]+\\.(ts|js|py|rs|go)`), performs case-insensitive substring and fuzzy word matching (3+ char words) against subsection keys; returns null if no matches or falls back to all subsections if no structured references found.\n\n`extractManifestEntriesForPhase(manifestContent: string, phaseContent: string): string | null` — extracts `Defines:` symbols from phase, filters manifest lines containing any symbol, returns null if no `Defines:` or matches.\n\n`extractSection(fullContent: string, sectionName: string): string | null` — extracts section by regex `## (?:\\d+\\.\\s*)?${sectionName}` through next `## ` boundary; returns null if not found.\n\n## RebuildUnit Contract\n\n`RebuildUnit` type contains: `name: string`, `specContent: string` (with injected context prefix), `order: number` (phase number or position).\n\n## Error Handling & Validation\n\nBoth extraction strategies require ≥1 unit post-filtering; consistent error message specifies expected format: `\"Expected either a \\\"## Build Plan\\\" section with \\\"### Phase N:\\\" subsections, or top-level \\\"## \\\" headings.\"` Empty units logged as `[warn] Skipping empty spec section: \"{unit.name}\"`.\n\n## Regex Patterns\n\nBuild Plan section: `/^(## (?:\\d+\\.\\s*)?Build Plan)\\s*$/m`\nPhase extraction: `/^### Phase (\\d+):\\s*(.+)$/gm`\nTop-level heading: `/^## (.+)$/gm`\nSubsection heading: `/^### (.+)$/gm`\nDefines/Consumes: `/(?:\\*\\*Defines:\\*\\*|^Defines:)\\s*(.+)/m` and `/(?:\\*\\*Consumes:\\*\\*|^Consumes:)\\s*(.+)/m`\nFile path references: `/\\b(?:src\\/[\\w\\-./]+|[\\w-]+\\.(?:ts|js|py|rs|go))\\b/g`",
      "model": "claude-haiku-4-5-20251001",
      "inputTokens": 3,
      "outputTokens": 1020,
      "cacheReadTokens": 16147,
      "cacheCreationTokens": 7106,
      "latencyMs": 30550,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/rebuild/spec-reader.ts",
          "sizeBytes": 13956
        }
      ]
    },
    {
      "timestamp": "2026-02-14T13:48:10.672Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: src/specify/writer.ts\n\n```typescript\nimport { writeFile, mkdir, access } from 'node:fs/promises';\nimport { constants } from 'node:fs';\nimport * as path from 'node:path';\n\n/**\n * Options controlling spec output writing behavior.\n */\nexport interface WriteSpecOptions {\n  /** Full path to the output file (e.g., /project/specs/SPEC.md). */\n  outputPath: string;\n  /** Overwrite existing files without error. */\n  force: boolean;\n  /** Split AI output into multiple files by top-level `# ` headings. */\n  multiFile: boolean;\n}\n\n/**\n * Thrown when writeSpec() detects existing file(s) and force=false.\n * Callers should catch this and present a user-friendly message.\n */\nexport class SpecExistsError extends Error {\n  /** Paths of the files that already exist. */\n  readonly paths: string[];\n\n  constructor(paths: string[]) {\n    const list = paths.map((p) => `  - ${p}`).join('\\n');\n    super(`Spec file(s) already exist:\\n${list}\\nUse --force to overwrite.`);\n    this.name = 'SpecExistsError';\n    this.paths = paths;\n  }\n}\n\n/**\n * Check whether a file exists at the given path.\n */\nasync function fileExists(filePath: string): Promise<boolean> {\n  try {\n    await access(filePath, constants.F_OK);\n    return true;\n  } catch {\n    return false;\n  }\n}\n\n/**\n * Sanitize a heading string into a filename-safe slug.\n *\n * Lowercases, replaces whitespace with hyphens, strips non-alphanumeric\n * characters (except hyphens), collapses consecutive hyphens, and trims\n * leading/trailing hyphens.\n */\nfunction slugify(heading: string): string {\n  return heading\n    .toLowerCase()\n    .replace(/\\s+/g, '-')\n    .replace(/[^a-z0-9-]/g, '')\n    .replace(/-+/g, '-')\n    .replace(/^-|-$/g, '');\n}\n\n/**\n * Split markdown content on top-level `# ` headings into named sections.\n *\n * Returns an array of `{ filename, content }` pairs. Any content before\n * the first `# ` heading is placed into `00-preamble.md`.\n */\nfunction splitByHeadings(content: string): Array<{ filename: string; content: string }> {\n  const sections: Array<{ filename: string; content: string }> = [];\n\n  // Split on lines that start with exactly \"# \" (top-level heading)\n  const parts = content.split(/^(?=# )/m);\n\n  for (const part of parts) {\n    const trimmed = part.trimEnd();\n    if (!trimmed) continue;\n\n    const headingMatch = trimmed.match(/^# (.+)/);\n    if (headingMatch) {\n      const slug = slugify(headingMatch[1]);\n      const filename = slug ? `${slug}.md` : '00-preamble.md';\n      sections.push({ filename, content: trimmed + '\\n' });\n    } else {\n      // Content before the first heading\n      sections.push({ filename: '00-preamble.md', content: trimmed + '\\n' });\n    }\n  }\n\n  return sections;\n}\n\n/**\n * Write spec output to disk with overwrite protection.\n *\n * In single-file mode, writes content directly to `outputPath`.\n * In multi-file mode, splits content on top-level `# ` headings and\n * writes each section to a separate file in the directory of `outputPath`.\n *\n * @param content - The full AI-generated spec markdown\n * @param options - Output path, force flag, and multi-file mode\n * @returns Array of absolute paths to all written files\n * @throws SpecExistsError if files exist and force=false\n */\nexport async function writeSpec(content: string, options: WriteSpecOptions): Promise<string[]> {\n  const { outputPath, force, multiFile } = options;\n\n  if (!multiFile) {\n    // Single-file mode\n    if (!force && await fileExists(outputPath)) {\n      throw new SpecExistsError([outputPath]);\n    }\n\n    await mkdir(path.dirname(outputPath), { recursive: true });\n    await writeFile(outputPath, content, 'utf-8');\n    return [outputPath];\n  }\n\n  // Multi-file mode: split on top-level headings\n  const outputDir = path.dirname(outputPath);\n  const sections = splitByHeadings(content);\n\n  // Check all target files for existence before writing any\n  if (!force) {\n    const conflicts: string[] = [];\n    for (const section of sections) {\n      const filePath = path.join(outputDir, section.filename);\n      if (await fileExists(filePath)) {\n        conflicts.push(filePath);\n      }\n    }\n    if (conflicts.length > 0) {\n      throw new SpecExistsError(conflicts);\n    }\n  }\n\n  // Create output directory and write all sections\n  await mkdir(outputDir, { recursive: true });\n\n  const writtenPaths: string[] = [];\n  for (const section of sections) {\n    const filePath = path.join(outputDir, section.filename);\n    await writeFile(filePath, section.content, 'utf-8');\n    writtenPaths.push(filePath);\n  }\n\n  return writtenPaths;\n}\n\n```\n\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n- Workflow & convention rules: if this file defines contribution guidelines, PR conventions, commit standards, testing mandates, tool usage requirements, approval workflows, AI agent behavioral instructions, or code conventions (naming standards, formatting rules, import ordering, linting policies), extract them as explicit, actionable rules. Sources: CONTRIBUTING.md, CI configs, PR templates, style guides, linter/formatter configs (.eslintrc, .prettierrc, .editorconfig), tsconfig strictness, README dev sections.\n\n\nTARGET LENGTH (MANDATORY):\n- Source file: 4490 characters\n- Target summary: ~449 characters (10% compression)\n- Maximum: 539 characters\n- Achieve this by: ultra-dense writing, minimal examples, single-sentence descriptions\n\nULTRA-COMPRESSION TECHNIQUES:\n- Use telegraphic style: \"exports foo, bar, baz\" not \"this file exports three functions\"\n- No code examples unless critical to understanding\n- One sentence per export maximum\n- Omit obvious parameter names (\"opts: Options\" not \"options: Options - configuration object\")\n- Use abbreviations: \"params\" not \"parameters\", \"config\" not \"configuration\"\n- Skip section if empty (don't write \"No special concerns\" or \"N/A\")\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nWORKFLOW & CONVENTION RULES (NEVER EXCLUDE):\n- Contribution requirements: branch naming, commit message formats, PR description templates\n- Testing mandates: required test types, coverage thresholds, \"never delete tests\" policies\n- Tool usage rules: required CLI commands, browser automation tools, linter/formatter requirements\n- Approval workflows: review requirements, CI checks that must pass, deployment gates\n- AI agent instructions: explicit directives for automated assistants (coding rules, prohibited actions)\n- Code conventions: naming standards (camelCase, PascalCase, snake_case for what), import ordering rules, file/folder naming patterns, formatting rules (indentation, quotes, semicolons), language idioms, TypeScript strictness policies, preferred patterns vs anti-patterns\nThese define how contributors (human or AI) must work within the project.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion .annex.sum file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**writer.ts exports spec file writing utilities with overwrite protection and multi-file splitting on markdown headings.**\n\n## Exports\n\n**WriteSpecOptions** interface: `{ outputPath: string, force: boolean, multiFile: boolean }` — controls spec output path, overwrite behavior, and heading-based file splitting.\n\n**SpecExistsError** class: extends Error with `paths: string[]` property — thrown when target files exist and `force=false`; message lists conflicting paths with instructions to use `--force`.\n\n**writeSpec**(content: string, options: WriteSpecOptions): Promise<string[]> — writes AI-generated markdown to disk; single-file mode writes directly to `outputPath`; multi-file mode splits on `^# ` regex and writes sections to separate files in `outputPath` directory; throws SpecExistsError on conflicts unless `force=true`; returns array of written absolute paths.\n\n## Internal Utilities\n\n**fileExists**(filePath: string): Promise<boolean> — checks existence via `fs.access()` with `constants.F_OK`.\n\n**slugify**(heading: string): string — converts heading to filename-safe slug via lowercase, whitespace→hyphens (`/\\s+/g`), strip non-alphanumeric (`/[^a-z0-9-]/g`), collapse hyphens (`/-+/g`), trim edges (`/^-|-$/g`).\n\n**splitByHeadings**(content: string): Array<{ filename, content }> — splits markdown on `^(?=# )/m` regex; preamble (pre-first-heading content) → `00-preamble.md`; each section named `[slug].md` from heading text or `00-preamble.md` if slug empty.\n\n## Behavior & Design\n\nSingle-file mode writes content as-is; multi-file mode requires heading-based splitting and directory structure. Conflict detection checks all target paths before any writes (atomic validation). Output uses UTF-8 encoding. Parent directories created recursively via `mkdir(..., { recursive: true })`.",
      "model": "claude-haiku-4-5-20251001",
      "inputTokens": 3,
      "outputTokens": 480,
      "cacheReadTokens": 16147,
      "cacheCreationTokens": 4111,
      "latencyMs": 24558,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/specify/writer.ts",
          "sizeBytes": 4490
        }
      ]
    },
    {
      "timestamp": "2026-02-14T13:48:09.843Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: src/specify/prompts.ts\n\n```typescript\nimport type { AgentsDocs } from '../generation/collector.js';\n\n/**\n * Prompt pair for spec generation: system instructions and user content.\n */\nexport interface SpecPrompt {\n  system: string;\n  user: string;\n}\n\n/**\n * System prompt for AI-driven specification synthesis.\n *\n * Enforces conceptual grouping by concern, prohibits folder-mirroring\n * and exact file path prescription, and targets AI agent consumption.\n */\nexport const SPEC_SYSTEM_PROMPT = `You produce software specifications from documentation.\n\nTASK:\nGenerate a comprehensive specification document from the provided AGENTS.md content. The specification must contain enough detail for an AI agent to reconstruct the entire project from scratch without seeing the original source code.\n\nAUDIENCE: AI agents (LLMs) — use structured, precise, instruction-oriented language. Every statement should be actionable.\n\nORGANIZATION (MANDATORY):\nGroup content by CONCERN, not by directory structure. Use these conceptual sections in order:\n\n1. Project Overview — purpose, core value proposition, problem solved, technology stack with versions\n2. Architecture — system design, module boundaries, data flow patterns, key design decisions and their rationale\n3. Public API Surface — all exported interfaces, function signatures with full parameter and return types, type definitions, error contracts\n4. Data Structures & State — key types, schemas, config objects, state management patterns, serialization formats\n5. Configuration — all config options with types, defaults, validation rules, environment variables\n6. Dependencies — each external dependency with exact version and rationale for inclusion\n7. Behavioral Contracts — Split into two subsections:\n   a. Runtime Behavior: error handling strategies (exact error types/codes and when thrown), retry logic (formulas, delay values), concurrency model, lifecycle hooks, resource management\n   b. Implementation Contracts: every regex pattern used for parsing/validation/extraction (verbatim in backticks), every format string and output template (exact structure with examples), every magic constant and sentinel value with its meaning, every environment variable with expected values, every file format specification (YAML schemas, NDJSON structures). These are reproduction-critical — an AI agent needs them to rebuild the system with identical observable behavior.\n8. Test Contracts — what each module's tests should verify: scenarios, edge cases, expected behaviors, error conditions\n9. Build Plan — phased implementation sequence with explicit interface contracts per phase:\n   - Each phase MUST include a \"Defines:\" list naming the exact types, interfaces, classes, and functions this phase must export (use the exact names from section 3 Public API Surface)\n   - Each phase MUST include a \"Consumes:\" list naming the exact types and functions from earlier phases that this phase imports\n   - Include dependency ordering and implementation tasks as before\n10. Prompt Templates & System Instructions — every AI prompt template, system prompt, and user prompt template used by the system. Reproduce the FULL text verbatim from annex files or AGENTS.md content. Organize by pipeline phase or functional area. Include placeholder syntax exactly as defined (e.g., {{FILE_PATH}}). These are reproduction-critical — without them, a rebuilder cannot produce functionally equivalent AI output.\n11. IDE Integration & Installer — command templates per platform, platform configuration objects (path prefixes, filename conventions, frontmatter formats), installer permission lists, hook definitions and their activation status. Reproduce template content verbatim from annex files or AGENTS.md content.\n12. File Manifest — exhaustive list of every source file the project contains:\n    - For each file: relative path, module it belongs to, and the public exports it provides\n    - Group by directory\n    - Include stub/placeholder files explicitly (mark them as stubs)\n    - Include type-only files (files that export only types/interfaces)\n    - This section ensures no files are missed during rebuild\n\nRULES:\n- Describe MODULE BOUNDARIES and their interfaces — not file paths or directory layouts\n- Use exact function, type, and constant names as they appear in the documentation\n- Include FULL type signatures for all public APIs (parameters, return types, generics)\n- Do NOT prescribe exact filenames or file paths — describe what each module does and exports\n- Do NOT mirror the project's folder structure in your section organization\n- Do NOT use directory names as section headings\n- Include version numbers for ALL external dependencies\n- The Build Plan MUST list implementation phases with explicit dependency ordering\n- Each Build Plan phase must state what it depends on and what it enables\n- Build Plan phases MUST cross-reference the Public API Surface: every type/function in the API Surface section must appear in exactly one phase's \"Defines:\" list\n- Behavioral Contracts must specify exact error types/codes and when they are thrown\n- Behavioral Contracts MUST include verbatim regex patterns, format strings, and magic constants from the source documents — do NOT paraphrase regex patterns into prose descriptions\n- When multiple modules reference the same constant or pattern, consolidate into a single definition with cross-references to the modules that use it\n- The File Manifest MUST list every source file. Each Build Plan phase MUST reference which File Manifest entries it produces. A file missing from both is a spec defect.\n\nREPRODUCTION-CRITICAL CONTENT (MANDATORY):\nThe source documents may include annex files containing full verbatim source code\nfor reproduction-critical modules (prompt templates, configuration defaults, IDE\ntemplates, installer configs). These are provided as fenced code blocks.\n\nFor ALL reproduction-critical content:\n- Reproduce the FULL content verbatim in the appropriate spec section (10 or 11)\n- Do NOT summarize, paraphrase, abbreviate, or \"improve\" the text\n- Use fenced code blocks to preserve formatting\n- If content contains placeholder syntax ({{TOKEN}}), preserve it exactly\n- If no annex files or reproduction-critical sections are provided, omit sections 10-11\n\nOUTPUT: Raw markdown. No preamble. No meta-commentary. No \"Here is...\" or \"I've generated...\" prefix.`;\n\n/**\n * Build the system + user prompt pair for spec generation.\n *\n * Injects all collected AGENTS.md content with section delimiters.\n *\n * @param docs - Collected AGENTS.md documents from collectAgentsDocs()\n * @returns SpecPrompt with system and user prompt strings\n */\nexport function buildSpecPrompt(docs: AgentsDocs, annexFiles?: AgentsDocs): SpecPrompt {\n  const agentsSections = docs.map(\n    (doc) => `### ${doc.relativePath}\\n\\n${doc.content}`,\n  );\n\n  const userSections: string[] = [\n    'Generate a comprehensive project specification from the following documentation.',\n    '',\n    `## AGENTS.md Files (${docs.length} directories)`,\n    '',\n    ...agentsSections,\n  ];\n\n  if (annexFiles && annexFiles.length > 0) {\n    const annexSections = annexFiles.map(\n      (doc) => `### ${doc.relativePath}\\n\\n${doc.content}`,\n    );\n    userSections.push(\n      '',\n      `## Annex Files (${annexFiles.length} reproduction-critical source files)`,\n      '',\n      ...annexSections,\n    );\n  }\n\n  userSections.push(\n    '',\n    '## Output Requirements',\n    '',\n    'The specification MUST include these sections in order:',\n    '1. Project Overview (purpose, value, tech stack)',\n    '2. Architecture (module boundaries, data flow, design decisions)',\n    '3. Public API Surface (all exported interfaces, full type signatures)',\n    '4. Data Structures & State (types, schemas, config objects)',\n    '5. Configuration (options, types, defaults, validation)',\n    '6. Dependencies (each with version and rationale)',\n    '7. Behavioral Contracts (error handling, concurrency, lifecycle, PLUS verbatim regex patterns, format specs, magic constants, templates)',\n    '8. Test Contracts (per-module test scenarios and edge cases)',\n    '9. Build Plan (phased implementation order with dependencies, each phase listing \"Defines:\" and \"Consumes:\" with exact names from API Surface)',\n    '10. Prompt Templates & System Instructions (FULL verbatim text from annex content)',\n    '11. IDE Integration & Installer (command templates, platform configs, permission lists — all verbatim from annex content)',\n    '12. File Manifest (every source file with path, module, and exports)',\n    '',\n    'Sections 10 and 11 MUST reproduce annex content verbatim.',\n    'Do NOT summarize prompt templates or IDE templates into prose descriptions.',\n    '',\n    'Output ONLY the markdown content. No preamble.',\n  );\n\n  return {\n    system: SPEC_SYSTEM_PROMPT,\n    user: userSections.join('\\n'),\n  };\n}\n\n```\n\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n- Workflow & convention rules: if this file defines contribution guidelines, PR conventions, commit standards, testing mandates, tool usage requirements, approval workflows, AI agent behavioral instructions, or code conventions (naming standards, formatting rules, import ordering, linting policies), extract them as explicit, actionable rules. Sources: CONTRIBUTING.md, CI configs, PR templates, style guides, linter/formatter configs (.eslintrc, .prettierrc, .editorconfig), tsconfig strictness, README dev sections.\n\n\nTARGET LENGTH (MANDATORY):\n- Source file: 8826 characters\n- Target summary: ~883 characters (10% compression)\n- Maximum: 1060 characters\n- Achieve this by: ultra-dense writing, minimal examples, single-sentence descriptions\n\nULTRA-COMPRESSION TECHNIQUES:\n- Use telegraphic style: \"exports foo, bar, baz\" not \"this file exports three functions\"\n- No code examples unless critical to understanding\n- One sentence per export maximum\n- Omit obvious parameter names (\"opts: Options\" not \"options: Options - configuration object\")\n- Use abbreviations: \"params\" not \"parameters\", \"config\" not \"configuration\"\n- Skip section if empty (don't write \"No special concerns\" or \"N/A\")\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nWORKFLOW & CONVENTION RULES (NEVER EXCLUDE):\n- Contribution requirements: branch naming, commit message formats, PR description templates\n- Testing mandates: required test types, coverage thresholds, \"never delete tests\" policies\n- Tool usage rules: required CLI commands, browser automation tools, linter/formatter requirements\n- Approval workflows: review requirements, CI checks that must pass, deployment gates\n- AI agent instructions: explicit directives for automated assistants (coding rules, prohibited actions)\n- Code conventions: naming standards (camelCase, PascalCase, snake_case for what), import ordering rules, file/folder naming patterns, formatting rules (indentation, quotes, semicolons), language idioms, TypeScript strictness policies, preferred patterns vs anti-patterns\nThese define how contributors (human or AI) must work within the project.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion .annex.sum file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**prompts.ts constructs system + user prompt pairs for AI-driven specification generation from AGENTS.md documentation.**\n\n## Exports\n\n`SpecPrompt` — interface with `system: string` and `user: string` fields for prompt pairs.\n\n`SPEC_SYSTEM_PROMPT` — system prompt defining specification synthesis rules for AI agents.\n\n`buildSpecPrompt(docs: AgentsDocs, annexFiles?: AgentsDocs): SpecPrompt` — assembles user prompt by injecting AGENTS.md sections and optional annex files with structured output requirements.\n\n## Specification Schema (SPEC_SYSTEM_PROMPT)\n\nMandates 12-section specification structure for AI reconstruction:\n1. Project Overview (purpose, value proposition, tech stack with versions)\n2. Architecture (module boundaries, data flow, design decisions)\n3. Public API Surface (exported interfaces, full type signatures)\n4. Data Structures & State (types, schemas, config objects, state patterns)\n5. Configuration (options, types, defaults, validation, environment variables)\n6. Dependencies (exact versions, rationale)\n7. Behavioral Contracts (error handling, retry logic, concurrency, lifecycle; verbatim regex patterns in backticks, format strings, magic constants, environment variable names, file format specs)\n8. Test Contracts (per-module scenarios, edge cases, error conditions)\n9. Build Plan (phased implementation with explicit \"Defines:\" and \"Consumes:\" lists cross-referencing Public API Surface section)\n10. Prompt Templates & System Instructions (full verbatim text from annex files, placeholder syntax preserved)\n11. IDE Integration & Installer (command templates, platform configs, permission lists — all verbatim)\n12. File Manifest (exhaustive source file list with relative path, module, public exports)\n\n## Prompts Composition Rules\n\n`SPEC_SYSTEM_PROMPT` enforces:\n- Conceptual grouping by concern, not directory structure\n- No folder-mirroring or exact file path prescription\n- Module boundaries and interfaces described, not file paths\n- Full type signatures for all public APIs\n- Exact function, type, constant names from documentation\n- All external dependency versions included\n- Build Plan phases must cross-reference Public API Surface; each phase lists \"Defines:\" and \"Consumes:\" with exact names\n- Behavioral Contracts must specify exact error types/codes and when thrown\n- Regex patterns, format strings, magic constants reproduced verbatim in backticks\n- Sections 10–11 require verbatim annex reproduction (no paraphrasing/summarization)\n- File Manifest entries must align with Build Plan phases (each file listed in manifest, each phase references which files it produces)\n\n`buildSpecPrompt()` constructs user prompt with:\n- Header: \"Generate a comprehensive project specification from the following documentation\"\n- Section: \"## AGENTS.md Files (N directories)\" with docs mapped as `### {relativePath}\\n\\n{content}`\n- Optional section: \"## Annex Files (M reproduction-critical source files)\" with annexFiles mapped identically\n- Section: \"## Output Requirements\" listing 12-section mandate and verbatim reproduction requirements\n- Footer: \"Output ONLY the markdown content. No preamble.\"",
      "model": "claude-haiku-4-5-20251001",
      "inputTokens": 3,
      "outputTokens": 737,
      "cacheReadTokens": 16147,
      "cacheCreationTokens": 4777,
      "latencyMs": 26803,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/specify/prompts.ts",
          "sizeBytes": 8864
        }
      ]
    },
    {
      "timestamp": "2026-02-14T13:48:13.217Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: src/update/index.ts\n\n```typescript\n/**\n * Update module\n *\n * Provides incremental documentation update functionality.\n * Coordinates state management, change detection, and orphan cleanup.\n */\nexport {\n  DocumentationOrchestrator as UpdateOrchestrator,\n  createUpdateOrchestrator,\n  type UpdatePlan,\n} from '../orchestration/orchestrator.js';\n\nexport {\n  cleanupOrphans,\n  cleanupEmptyDirectoryDocs,\n  getAffectedDirectories,\n} from './orphan-cleaner.js';\n\nexport type {\n  UpdatePlanOptions,\n  UpdateResult,\n  UpdateProgress,\n  CleanupResult,\n} from './types.js';\n\n```\n\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n- Workflow & convention rules: if this file defines contribution guidelines, PR conventions, commit standards, testing mandates, tool usage requirements, approval workflows, AI agent behavioral instructions, or code conventions (naming standards, formatting rules, import ordering, linting policies), extract them as explicit, actionable rules. Sources: CONTRIBUTING.md, CI configs, PR templates, style guides, linter/formatter configs (.eslintrc, .prettierrc, .editorconfig), tsconfig strictness, README dev sections.\n\n\nTARGET LENGTH (MANDATORY):\n- Source file: 530 characters\n- Target summary: ~53 characters (10% compression)\n- Maximum: 64 characters\n- Achieve this by: ultra-dense writing, minimal examples, single-sentence descriptions\n\nULTRA-COMPRESSION TECHNIQUES:\n- Use telegraphic style: \"exports foo, bar, baz\" not \"this file exports three functions\"\n- No code examples unless critical to understanding\n- One sentence per export maximum\n- Omit obvious parameter names (\"opts: Options\" not \"options: Options - configuration object\")\n- Use abbreviations: \"params\" not \"parameters\", \"config\" not \"configuration\"\n- Skip section if empty (don't write \"No special concerns\" or \"N/A\")\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nWORKFLOW & CONVENTION RULES (NEVER EXCLUDE):\n- Contribution requirements: branch naming, commit message formats, PR description templates\n- Testing mandates: required test types, coverage thresholds, \"never delete tests\" policies\n- Tool usage rules: required CLI commands, browser automation tools, linter/formatter requirements\n- Approval workflows: review requirements, CI checks that must pass, deployment gates\n- AI agent instructions: explicit directives for automated assistants (coding rules, prohibited actions)\n- Code conventions: naming standards (camelCase, PascalCase, snake_case for what), import ordering rules, file/folder naming patterns, formatting rules (indentation, quotes, semicolons), language idioms, TypeScript strictness policies, preferred patterns vs anti-patterns\nThese define how contributors (human or AI) must work within the project.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion .annex.sum file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**src/update/index.ts re-exports orchestration and cleanup utilities for incremental documentation updates.**\n\n## Exports\n\n**Orchestrator**: `DocumentationOrchestrator` (aliased `UpdateOrchestrator`), `createUpdateOrchestrator()` — re-exported from `../orchestration/orchestrator.js` for state management and change detection.\n\n**Cleanup**: `cleanupOrphans()`, `cleanupEmptyDirectoryDocs()`, `getAffectedDirectories()` — orphan removal and affected-path detection from `./orphan-cleaner.js`.\n\n**Types**: `UpdatePlanOptions`, `UpdateResult`, `UpdateProgress`, `CleanupResult` — re-exported from `./types.js` for plan configuration, execution results, progress tracking, and cleanup outcome shapes.",
      "model": "claude-haiku-4-5-20251001",
      "inputTokens": 3,
      "outputTokens": 188,
      "cacheReadTokens": 16147,
      "cacheCreationTokens": 2893,
      "latencyMs": 23781,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/update/index.ts",
          "sizeBytes": 530
        }
      ]
    },
    {
      "timestamp": "2026-02-14T13:48:15.431Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: src/update/types.ts\n\n```typescript\n/**\n * Types for incremental update workflow\n */\nimport type { FileChange } from '../change-detection/types.js';\n\n/**\n * Result of orphan cleanup\n */\nexport interface CleanupResult {\n  /** .sum files that were deleted */\n  deletedSumFiles: string[];\n  /** AGENTS.md files that were deleted (from empty directories) */\n  deletedAgentsMd: string[];\n}\n\n/**\n * Options for the update command\n */\nexport interface UpdatePlanOptions {\n  /** Include uncommitted changes (staged + working directory) */\n  includeUncommitted?: boolean;\n  /** Dry run - show what would change without making changes */\n  dryRun?: boolean;\n  /** Eval variant (e.g., \"claude.haiku\") — checks variant-specific .sum files */\n  variant?: string;\n}\n\n/**\n * Result of an update run\n */\nexport interface UpdateResult {\n  /** Files that were analyzed (added or modified) */\n  analyzedFiles: string[];\n  /** Files that were skipped (unchanged) */\n  skippedFiles: string[];\n  /** Cleanup result (deleted .sum and AGENTS.md files) */\n  cleanup: CleanupResult;\n  /** Directories whose AGENTS.md was regenerated */\n  regeneratedDirs: string[];\n  /** Git commit hash at start of update */\n  baseCommit: string;\n  /** Git commit hash at end of update */\n  currentCommit: string;\n  /** Whether this was a dry run */\n  dryRun: boolean;\n}\n\n/**\n * Progress callback for update operations\n */\nexport interface UpdateProgress {\n  /** Called when a file is about to be processed */\n  onFileStart?: (path: string, status: 'analyzing' | 'skipping') => void;\n  /** Called when a file is done processing */\n  onFileDone?: (path: string, status: 'analyzed' | 'skipped' | 'error') => void;\n  /** Called when cleanup deletes a file */\n  onCleanup?: (path: string, type: 'sum' | 'agents-md') => void;\n  /** Called when a directory AGENTS.md is regenerated */\n  onDirRegenerate?: (path: string) => void;\n}\n\n```\n\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n- Workflow & convention rules: if this file defines contribution guidelines, PR conventions, commit standards, testing mandates, tool usage requirements, approval workflows, AI agent behavioral instructions, or code conventions (naming standards, formatting rules, import ordering, linting policies), extract them as explicit, actionable rules. Sources: CONTRIBUTING.md, CI configs, PR templates, style guides, linter/formatter configs (.eslintrc, .prettierrc, .editorconfig), tsconfig strictness, README dev sections.\n\n\nTARGET LENGTH (MANDATORY):\n- Source file: 1847 characters\n- Target summary: ~185 characters (10% compression)\n- Maximum: 222 characters\n- Achieve this by: ultra-dense writing, minimal examples, single-sentence descriptions\n\nULTRA-COMPRESSION TECHNIQUES:\n- Use telegraphic style: \"exports foo, bar, baz\" not \"this file exports three functions\"\n- No code examples unless critical to understanding\n- One sentence per export maximum\n- Omit obvious parameter names (\"opts: Options\" not \"options: Options - configuration object\")\n- Use abbreviations: \"params\" not \"parameters\", \"config\" not \"configuration\"\n- Skip section if empty (don't write \"No special concerns\" or \"N/A\")\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nWORKFLOW & CONVENTION RULES (NEVER EXCLUDE):\n- Contribution requirements: branch naming, commit message formats, PR description templates\n- Testing mandates: required test types, coverage thresholds, \"never delete tests\" policies\n- Tool usage rules: required CLI commands, browser automation tools, linter/formatter requirements\n- Approval workflows: review requirements, CI checks that must pass, deployment gates\n- AI agent instructions: explicit directives for automated assistants (coding rules, prohibited actions)\n- Code conventions: naming standards (camelCase, PascalCase, snake_case for what), import ordering rules, file/folder naming patterns, formatting rules (indentation, quotes, semicolons), language idioms, TypeScript strictness policies, preferred patterns vs anti-patterns\nThese define how contributors (human or AI) must work within the project.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion .annex.sum file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**update/types.ts defines type contracts for incremental documentation updates, tracking file changes, cleanup operations, and progress callbacks.**\n\n## Exports\n\n**CleanupResult** — object shape: `deletedSumFiles: string[]`, `deletedAgentsMd: string[]`; tracks orphaned documentation artifacts removed during updates.\n\n**UpdatePlanOptions** — object shape: `includeUncommitted?: boolean`, `dryRun?: boolean`, `variant?: string`; configures update scope (uncommitted changes, dry-run mode, variant-specific .sum files).\n\n**UpdateResult** — object shape: `analyzedFiles: string[]`, `skippedFiles: string[]`, `cleanup: CleanupResult`, `regeneratedDirs: string[]`, `baseCommit: string`, `currentCommit: string`, `dryRun: boolean`; aggregates update operation outcomes including file tallies, deletions, git state.\n\n**UpdateProgress** — callback interface: `onFileStart?(path, status: 'analyzing'|'skipping')`, `onFileDone?(path, status: 'analyzed'|'skipped'|'error')`, `onCleanup?(path, type: 'sum'|'agents-md')`, `onDirRegenerate?(path)`; enables progress instrumentation during incremental updates.\n\n## Dependencies\n\nImports `FileChange` from `change-detection/types.js`; consumer workflows likely use this to correlate detected changes with update operations.",
      "model": "claude-haiku-4-5-20251001",
      "inputTokens": 3,
      "outputTokens": 340,
      "cacheReadTokens": 17646,
      "cacheCreationTokens": 1779,
      "latencyMs": 23241,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/update/types.ts",
          "sizeBytes": 1849
        }
      ]
    },
    {
      "timestamp": "2026-02-14T13:48:14.242Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: src/update/orphan-cleaner.ts\n\n```typescript\n/**\n * Orphan cleanup for stale .sum and AGENTS.md files\n *\n * Handles:\n * - Deleting .sum files when source files are deleted\n * - Deleting .sum files for renamed files (at old path)\n * - Deleting AGENTS.md from directories with no remaining source files\n */\nimport { unlink, readdir, stat } from 'node:fs/promises';\nimport * as path from 'node:path';\nimport type { FileChange } from '../change-detection/types.js';\nimport type { CleanupResult } from './types.js';\nimport { getSumPath, getAnnexPath } from '../generation/writers/sum.js';\n\n/**\n * Files to ignore when checking if a directory has source files.\n * These are generated files, not source files.\n */\nconst GENERATED_FILES = new Set([\n  'AGENTS.md',\n  'CLAUDE.md',\n]);\n\n/**\n * Clean up orphaned .sum files and empty AGENTS.md files.\n *\n * @param projectRoot - Absolute path to project root\n * @param changes - List of file changes (deleted and renamed files need cleanup)\n * @param dryRun - If true, don't actually delete files\n * @returns Cleanup result with lists of deleted files\n */\nexport async function cleanupOrphans(\n  projectRoot: string,\n  changes: FileChange[],\n  dryRun: boolean = false,\n  variant?: string,\n): Promise<CleanupResult> {\n  const result: CleanupResult = {\n    deletedSumFiles: [],\n    deletedAgentsMd: [],\n  };\n\n  // Collect paths that need .sum cleanup\n  const pathsToClean: string[] = [];\n\n  for (const change of changes) {\n    if (change.status === 'deleted') {\n      pathsToClean.push(change.path);\n    } else if (change.status === 'renamed' && change.oldPath) {\n      // For renames, clean up the old path's .sum\n      pathsToClean.push(change.oldPath);\n    }\n  }\n\n  // Delete .sum and .annex.sum files for deleted/renamed source files\n  for (const relativePath of pathsToClean) {\n    const absoluteFilePath = path.join(projectRoot, relativePath);\n    const sumPath = getSumPath(absoluteFilePath, variant);\n    const deleted = await deleteIfExists(sumPath, dryRun);\n    if (deleted) {\n      result.deletedSumFiles.push(path.relative(projectRoot, sumPath));\n    }\n    const annexPath = getAnnexPath(absoluteFilePath, variant);\n    const annexDeleted = await deleteIfExists(annexPath, dryRun);\n    if (annexDeleted) {\n      result.deletedSumFiles.push(path.relative(projectRoot, annexPath));\n    }\n  }\n\n  // Collect affected directories for AGENTS.md cleanup\n  const affectedDirs = new Set<string>();\n  for (const relativePath of pathsToClean) {\n    const dir = path.dirname(relativePath);\n    if (dir && dir !== '.') {\n      affectedDirs.add(dir);\n    }\n  }\n\n  // Check each affected directory for empty AGENTS.md\n  const agentsFilename = variant ? `AGENTS.${variant}.md` : 'AGENTS.md';\n  for (const dir of affectedDirs) {\n    const dirPath = path.join(projectRoot, dir);\n    const cleaned = await cleanupEmptyDirectoryDocs(dirPath, dryRun, agentsFilename);\n    if (cleaned) {\n      result.deletedAgentsMd.push(path.join(dir, agentsFilename));\n    }\n  }\n\n  return result;\n}\n\n/**\n * Delete a file if it exists.\n *\n * @returns true if file was deleted (or would be in dry run)\n */\nasync function deleteIfExists(filePath: string, dryRun: boolean): Promise<boolean> {\n  try {\n    await stat(filePath);\n    if (!dryRun) {\n      await unlink(filePath);\n    }\n    return true;\n  } catch {\n    // File doesn't exist\n    return false;\n  }\n}\n\n/**\n * Check if a directory has any source files remaining.\n * If not, delete its AGENTS.md.\n *\n * @param dirPath - Absolute path to directory\n * @param dryRun - If true, don't actually delete\n * @returns true if AGENTS.md was deleted\n */\nexport async function cleanupEmptyDirectoryDocs(\n  dirPath: string,\n  dryRun: boolean = false,\n  agentsFilename: string = 'AGENTS.md',\n): Promise<boolean> {\n  try {\n    const entries = await readdir(dirPath);\n\n    // Check if directory has any source files\n    // Source files are: not .sum files, not generated docs, not hidden files\n    const hasSourceFiles = entries.some(entry => {\n      // Skip hidden files\n      if (entry.startsWith('.')) return false;\n      // Skip .sum files (includes .annex.sum and variant .sum)\n      if (entry.endsWith('.sum')) return false;\n      // Skip known generated files\n      if (GENERATED_FILES.has(entry)) return false;\n      // Skip variant AGENTS files (AGENTS.*.md)\n      if (entry.startsWith('AGENTS.') && entry.endsWith('.md')) return false;\n      // Everything else counts as a source file\n      return true;\n    });\n\n    if (!hasSourceFiles) {\n      const agentsPath = path.join(dirPath, agentsFilename);\n      return await deleteIfExists(agentsPath, dryRun);\n    }\n\n    return false;\n  } catch {\n    // Directory doesn't exist or can't be read\n    return false;\n  }\n}\n\n/**\n * Get list of directories that should have their AGENTS.md regenerated.\n *\n * Includes parent directories of changed files up to project root.\n *\n * @param changes - List of file changes (non-deleted files)\n * @returns Set of relative directory paths\n */\nexport function getAffectedDirectories(changes: FileChange[]): Set<string> {\n  const dirs = new Set<string>();\n\n  for (const change of changes) {\n    // Skip deleted files - they don't affect directory docs\n    if (change.status === 'deleted') continue;\n\n    // Add all parent directories up to root\n    let dir = path.dirname(change.path);\n    while (dir && dir !== '.' && !path.isAbsolute(dir)) {\n      dirs.add(dir);\n      dir = path.dirname(dir);\n    }\n    // Include root directory\n    dirs.add('.');\n  }\n\n  return dirs;\n}\n\n```\n\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n- Workflow & convention rules: if this file defines contribution guidelines, PR conventions, commit standards, testing mandates, tool usage requirements, approval workflows, AI agent behavioral instructions, or code conventions (naming standards, formatting rules, import ordering, linting policies), extract them as explicit, actionable rules. Sources: CONTRIBUTING.md, CI configs, PR templates, style guides, linter/formatter configs (.eslintrc, .prettierrc, .editorconfig), tsconfig strictness, README dev sections.\n\n\nTARGET LENGTH (MANDATORY):\n- Source file: 5468 characters\n- Target summary: ~547 characters (10% compression)\n- Maximum: 656 characters\n- Achieve this by: ultra-dense writing, minimal examples, single-sentence descriptions\n\nULTRA-COMPRESSION TECHNIQUES:\n- Use telegraphic style: \"exports foo, bar, baz\" not \"this file exports three functions\"\n- No code examples unless critical to understanding\n- One sentence per export maximum\n- Omit obvious parameter names (\"opts: Options\" not \"options: Options - configuration object\")\n- Use abbreviations: \"params\" not \"parameters\", \"config\" not \"configuration\"\n- Skip section if empty (don't write \"No special concerns\" or \"N/A\")\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nWORKFLOW & CONVENTION RULES (NEVER EXCLUDE):\n- Contribution requirements: branch naming, commit message formats, PR description templates\n- Testing mandates: required test types, coverage thresholds, \"never delete tests\" policies\n- Tool usage rules: required CLI commands, browser automation tools, linter/formatter requirements\n- Approval workflows: review requirements, CI checks that must pass, deployment gates\n- AI agent instructions: explicit directives for automated assistants (coding rules, prohibited actions)\n- Code conventions: naming standards (camelCase, PascalCase, snake_case for what), import ordering rules, file/folder naming patterns, formatting rules (indentation, quotes, semicolons), language idioms, TypeScript strictness policies, preferred patterns vs anti-patterns\nThese define how contributors (human or AI) must work within the project.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion .annex.sum file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**orphan-cleaner.ts manages cleanup of stale .sum/.annex.sum files and AGENTS.md when source files are deleted or renamed.**\n\n## Exports\n\n`cleanupOrphans(projectRoot, changes, dryRun?, variant?)` → `Promise<CleanupResult>`: Deletes .sum/.annex.sum files for deleted/renamed sources; removes AGENTS.md from directories with no source files remaining.\n\n`cleanupEmptyDirectoryDocs(dirPath, dryRun?, agentsFilename?)` → `Promise<boolean>`: Deletes AGENTS.md/.variant.md if directory contains no source files.\n\n`getAffectedDirectories(changes)` → `Set<string>`: Returns all parent directory paths (up to root) for non-deleted file changes; used to identify which AGENTS.md files need regeneration.\n\n## Key Dependencies\n\n`getSumPath`, `getAnnexPath` from `../generation/writers/sum.js`: Compute paths for .sum and .annex.sum documentation files.\n\n`FileChange` type: Tracks file status (`deleted`, `renamed`) and paths.\n\n## Design Patterns\n\nCleanup operates in two phases: (1) delete .sum/.annex.sum for deleted/renamed sources, (2) prune empty directory documentation. Supports dry-run mode via `dryRun` parameter. Variant support allows per-variant .sum/.md files via optional `variant` parameter (e.g., AGENTS.v2.md).\n\n## Behavioral Contracts\n\n**GENERATED_FILES Set**: `['AGENTS.md', 'CLAUDE.md']` — constant file names ignored as source files when determining if directory is empty.\n\n**Source file detection** in `cleanupEmptyDirectoryDocs`: Counts as source if: not hidden (no leading dot), not `.sum` file, not in GENERATED_FILES, not AGENTS.*.md pattern. Deletes AGENTS.md only if zero source files remain.\n\n**Dry-run semantics**: `dryRun=true` skips actual `unlink()` calls but returns `true` to indicate what would be deleted.",
      "model": "claude-haiku-4-5-20251001",
      "inputTokens": 3,
      "outputTokens": 496,
      "cacheReadTokens": 16147,
      "cacheCreationTokens": 4423,
      "latencyMs": 25532,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/update/orphan-cleaner.ts",
          "sizeBytes": 5468
        }
      ]
    },
    {
      "timestamp": "2026-02-14T13:48:17.819Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: hooks/opencode-are-check-update.js\n\n```javascript\n#!/usr/bin/env node\n/**\n * ARE Update Check Plugin for OpenCode\n *\n * Checks for ARE updates in background, writes result to cache.\n * OpenCode equivalent of the Claude/Gemini SessionStart hook.\n *\n * Cache file: ~/.config/opencode/cache/are-update-check.json\n */\n\nimport { existsSync, mkdirSync, readFileSync, writeFileSync } from 'fs';\nimport { homedir } from 'os';\nimport { join } from 'path';\nimport { spawn } from 'child_process';\n\nexport const AreCheckUpdate = async (ctx) => {\n  return {\n    event: async ({ event }) => {\n      // Only trigger on session creation\n      if (event.type !== 'session.created') return;\n\n      const homeDir = homedir();\n      const cwd = process.cwd();\n      const cacheDir = join(homeDir, '.config', 'opencode', 'cache');\n      const cacheFile = join(cacheDir, 'are-update-check.json');\n      const npmCacheDir = join(cacheDir, 'npm-cache');\n\n      // ARE-VERSION file locations (check project first, then global)\n      const projectVersionFile = join(cwd, '.opencode', 'ARE-VERSION');\n      const globalVersionFile = join(homeDir, '.config', 'opencode', 'ARE-VERSION');\n\n      // Ensure cache directory exists\n      if (!existsSync(cacheDir)) {\n        mkdirSync(cacheDir, { recursive: true });\n      }\n      if (!existsSync(npmCacheDir)) {\n        mkdirSync(npmCacheDir, { recursive: true });\n      }\n\n      // Run check in background (spawn background process)\n      const child = spawn(process.execPath, ['-e', `\n        const fs = require('fs');\n        const { execSync } = require('child_process');\n\n        const cacheFile = ${JSON.stringify(cacheFile)};\n        const projectVersionFile = ${JSON.stringify(projectVersionFile)};\n        const globalVersionFile = ${JSON.stringify(globalVersionFile)};\n\n        // Check project directory first (local install), then global\n        let installed = '0.0.0';\n        try {\n          if (fs.existsSync(projectVersionFile)) {\n            installed = fs.readFileSync(projectVersionFile, 'utf8').trim();\n          } else if (fs.existsSync(globalVersionFile)) {\n            installed = fs.readFileSync(globalVersionFile, 'utf8').trim();\n          }\n        } catch (e) {}\n\n        let latest = null;\n        try {\n          latest = execSync('npm view agents-reverse-engineer version', { encoding: 'utf8', timeout: 10000, windowsHide: true }).trim();\n        } catch (e) {}\n\n        const result = {\n          update_available: latest && installed !== latest,\n          installed,\n          latest: latest || 'unknown',\n          checked: Math.floor(Date.now() / 1000)\n        };\n\n        fs.writeFileSync(cacheFile, JSON.stringify(result));\n      `], {\n        stdio: 'ignore',\n        detached: true,\n        windowsHide: true,\n        env: {\n          ...process.env,\n        },\n      });\n\n      child.unref();\n    },\n  };\n};\n\n```\n\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n- Workflow & convention rules: if this file defines contribution guidelines, PR conventions, commit standards, testing mandates, tool usage requirements, approval workflows, AI agent behavioral instructions, or code conventions (naming standards, formatting rules, import ordering, linting policies), extract them as explicit, actionable rules. Sources: CONTRIBUTING.md, CI configs, PR templates, style guides, linter/formatter configs (.eslintrc, .prettierrc, .editorconfig), tsconfig strictness, README dev sections.\n\n\nTARGET LENGTH (MANDATORY):\n- Source file: 2816 characters\n- Target summary: ~282 characters (10% compression)\n- Maximum: 338 characters\n- Achieve this by: ultra-dense writing, minimal examples, single-sentence descriptions\n\nULTRA-COMPRESSION TECHNIQUES:\n- Use telegraphic style: \"exports foo, bar, baz\" not \"this file exports three functions\"\n- No code examples unless critical to understanding\n- One sentence per export maximum\n- Omit obvious parameter names (\"opts: Options\" not \"options: Options - configuration object\")\n- Use abbreviations: \"params\" not \"parameters\", \"config\" not \"configuration\"\n- Skip section if empty (don't write \"No special concerns\" or \"N/A\")\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nWORKFLOW & CONVENTION RULES (NEVER EXCLUDE):\n- Contribution requirements: branch naming, commit message formats, PR description templates\n- Testing mandates: required test types, coverage thresholds, \"never delete tests\" policies\n- Tool usage rules: required CLI commands, browser automation tools, linter/formatter requirements\n- Approval workflows: review requirements, CI checks that must pass, deployment gates\n- AI agent instructions: explicit directives for automated assistants (coding rules, prohibited actions)\n- Code conventions: naming standards (camelCase, PascalCase, snake_case for what), import ordering rules, file/folder naming patterns, formatting rules (indentation, quotes, semicolons), language idioms, TypeScript strictness policies, preferred patterns vs anti-patterns\nThese define how contributors (human or AI) must work within the project.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion .annex.sum file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**hooks/opencode-are-check-update.js exports AreCheckUpdate, an OpenCode session hook that spawns a background process to check for agents-reverse-engineer updates and cache the result.**\n\n## Exported Interface\n\n- `AreCheckUpdate(ctx)` — async function returning event handler; triggers on `session.created` event type to initiate background update check.\n\n## Cache & Version File Paths\n\n- Cache directory: `~/.config/opencode/cache/`\n- Cache file: `~/.config/opencode/cache/are-update-check.json`\n- npm cache directory: `~/.config/opencode/cache/npm-cache/`\n- Project version file: `.opencode/ARE-VERSION` (checked first)\n- Global version file: `~/.config/opencode/ARE-VERSION` (fallback)\n\n## Background Process Output\n\nSpawned child writes `are-update-check.json` with structure: `{update_available: boolean, installed: string, latest: string, checked: number (unix timestamp)}`. Installed version defaults to `'0.0.0'` if neither version file exists; latest defaults to `'unknown'` if npm registry query fails (10s timeout). Process spawned detached with `stdio: 'ignore'` and `child.unref()` to prevent parent blocking.\n\n## Integration\n\nEvent handler pattern: checks `event.type === 'session.created'` before executing. Uses `spawn(process.execPath)` with inline `-e` script to avoid blocking; inherits `process.env` in child. Filesystem operations use `fs.existsSync()`, `fs.readFileSync()`, `fs.writeFileSync()`, `mkdirSync({recursive: true})`.",
      "model": "claude-haiku-4-5-20251001",
      "inputTokens": 3,
      "outputTokens": 399,
      "cacheReadTokens": 16147,
      "cacheCreationTokens": 3559,
      "latencyMs": 23084,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "hooks/opencode-are-check-update.js",
          "sizeBytes": 2816
        }
      ]
    },
    {
      "timestamp": "2026-02-14T13:48:17.764Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: hooks/are-context-loader.js\n\n```javascript\n#!/usr/bin/env node\n\n/**\n * ARE Context Loader Hook (PostToolUse → Read)\n *\n * Fires after every Read tool call. When the read file's directory contains\n * an AGENTS.md generated by ARE, and it hasn't been loaded yet this session,\n * outputs its content as additionalContext for Claude.\n *\n * Deduplicates per session via temp file keyed by session_id.\n * Skips the project root (already loaded via CLAUDE.md → @AGENTS.md).\n */\n\nimport fs from 'node:fs';\nimport path from 'node:path';\nimport os from 'node:os';\n\nlet input = '';\nprocess.stdin.setEncoding('utf8');\nprocess.stdin.on('data', (chunk) => { input += chunk; });\nprocess.stdin.on('end', () => {\n  try {\n    main(JSON.parse(input));\n  } catch {\n    // Malformed input — exit silently\n  }\n});\n\nfunction main(data) {\n  const filePath = data.tool_input && data.tool_input.file_path;\n  if (!filePath) return;\n\n  const projectRoot = data.cwd || '';\n\n  // Only trigger for files within the project\n  if (projectRoot && !filePath.startsWith(projectRoot)) return;\n\n  const startDir = path.dirname(path.resolve(filePath));\n\n  // Skip project root — already loaded via CLAUDE.md → @AGENTS.md\n  if (startDir === projectRoot) return;\n\n  // Deduplicate per session\n  const sessionId = data.session_id || 'default';\n  const dedupeDir = path.join(os.tmpdir(), 'are-context-loader');\n  const dedupeFile = path.join(dedupeDir, sessionId + '.json');\n\n  let seen = {};\n  try {\n    if (fs.existsSync(dedupeFile)) {\n      seen = JSON.parse(fs.readFileSync(dedupeFile, 'utf8'));\n    }\n  } catch {}\n\n  // Walk from the file's directory up to (but not including) project root,\n  // collecting all unseen ARE-generated AGENTS.md files\n  const parts = [];\n  let dir = startDir;\n  while (dir && dir !== projectRoot && dir !== path.dirname(dir)) {\n    if (!seen[dir]) {\n      const agentsMd = path.join(dir, 'AGENTS.md');\n      if (fs.existsSync(agentsMd)) {\n        try {\n          const content = fs.readFileSync(agentsMd, 'utf8');\n          if (content.includes('Generated by agents-reverse-engineer')) {\n            const relDir = projectRoot ? path.relative(projectRoot, dir) : dir;\n            parts.push('## ARE Context: ' + relDir + '/AGENTS.md\\n\\n' + content);\n            seen[dir] = true;\n          }\n        } catch {}\n      }\n    }\n    dir = path.dirname(dir);\n  }\n\n  if (parts.length === 0) return;\n\n  // Persist dedup state\n  try {\n    fs.mkdirSync(dedupeDir, { recursive: true });\n    fs.writeFileSync(dedupeFile, JSON.stringify(seen));\n  } catch {}\n\n  // Output structured JSON — additionalContext is injected into Claude's conversation\n  // Reverse so parent context appears before child (root-to-leaf order)\n  process.stdout.write(JSON.stringify({\n    suppressOutput: false,\n    hookSpecificOutput: {\n      hookEventName: 'PostToolUse',\n      additionalContext: parts.reverse().join('\\n\\n'),\n    },\n  }));\n}\n\n```\n\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n- Workflow & convention rules: if this file defines contribution guidelines, PR conventions, commit standards, testing mandates, tool usage requirements, approval workflows, AI agent behavioral instructions, or code conventions (naming standards, formatting rules, import ordering, linting policies), extract them as explicit, actionable rules. Sources: CONTRIBUTING.md, CI configs, PR templates, style guides, linter/formatter configs (.eslintrc, .prettierrc, .editorconfig), tsconfig strictness, README dev sections.\n\n\nTARGET LENGTH (MANDATORY):\n- Source file: 2856 characters\n- Target summary: ~286 characters (10% compression)\n- Maximum: 343 characters\n- Achieve this by: ultra-dense writing, minimal examples, single-sentence descriptions\n\nULTRA-COMPRESSION TECHNIQUES:\n- Use telegraphic style: \"exports foo, bar, baz\" not \"this file exports three functions\"\n- No code examples unless critical to understanding\n- One sentence per export maximum\n- Omit obvious parameter names (\"opts: Options\" not \"options: Options - configuration object\")\n- Use abbreviations: \"params\" not \"parameters\", \"config\" not \"configuration\"\n- Skip section if empty (don't write \"No special concerns\" or \"N/A\")\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nWORKFLOW & CONVENTION RULES (NEVER EXCLUDE):\n- Contribution requirements: branch naming, commit message formats, PR description templates\n- Testing mandates: required test types, coverage thresholds, \"never delete tests\" policies\n- Tool usage rules: required CLI commands, browser automation tools, linter/formatter requirements\n- Approval workflows: review requirements, CI checks that must pass, deployment gates\n- AI agent instructions: explicit directives for automated assistants (coding rules, prohibited actions)\n- Code conventions: naming standards (camelCase, PascalCase, snake_case for what), import ordering rules, file/folder naming patterns, formatting rules (indentation, quotes, semicolons), language idioms, TypeScript strictness policies, preferred patterns vs anti-patterns\nThese define how contributors (human or AI) must work within the project.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion .annex.sum file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**are-context-loader.js auto-injects AGENTS.md documentation into Claude's context after Read tool calls, walking the file's directory tree upward to collect generated documentation files.**\n\n## Exports & Hooks\n\n`main(data)` — processes PostToolUse hook data; extracts `file_path` from `data.tool_input`, walks ancestry from file directory toward project root, outputs collected AGENTS.md files as JSON to stdout with `additionalContext` for Claude's conversation.\n\n## Deduplication & Session Management\n\nUses session-scoped temp file (`~/.tmp/are-context-loader/{session_id}.json`) keyed by `data.session_id` to track which directories' AGENTS.md have been loaded, preventing redundant context injection across multiple Read calls in the same session.\n\n## Directory Traversal & Filtering\n\nWalks from `path.dirname(filePath)` upward, skipping project root (already loaded via CLAUDE.md), stopping at filesystem boundary; collects only files matching `AGENTS.md` with marker string `'Generated by agents-reverse-engineer'`.\n\n## Output Format\n\nOutputs JSON with structure `{suppressOutput: false, hookSpecificOutput: {hookEventName: 'PostToolUse', additionalContext: string}}` to stdout; reverses collected parts to ensure root-to-leaf ordering (parent context before child).\n\n## Input & Constraints\n\nReads stdin as JSON containing `tool_input.file_path`, `cwd` (project root), `session_id`; silently exits on malformed input or missing `file_path`; returns early if file outside project tree or at project root.",
      "model": "claude-haiku-4-5-20251001",
      "inputTokens": 3,
      "outputTokens": 376,
      "cacheReadTokens": 16147,
      "cacheCreationTokens": 3645,
      "latencyMs": 23719,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "hooks/are-context-loader.js",
          "sizeBytes": 2868
        }
      ]
    },
    {
      "timestamp": "2026-02-14T13:48:16.050Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: hooks/are-check-update.js\n\n```javascript\n#!/usr/bin/env node\n/**\n * ARE Update Check Hook\n *\n * Checks for ARE updates in background, writes result to cache.\n * Called by SessionStart hook - runs once per session.\n *\n * Cache file: ~/.claude/cache/are-update-check.json\n */\n\nimport { existsSync, mkdirSync, readFileSync, writeFileSync } from 'fs';\nimport { homedir } from 'os';\nimport { join } from 'path';\nimport { spawn, execSync } from 'child_process';\n\nconst homeDir = homedir();\nconst cwd = process.cwd();\nconst cacheDir = join(homeDir, '.claude', 'cache');\nconst cacheFile = join(cacheDir, 'are-update-check.json');\nconst npmCacheDir = join(cacheDir, 'npm-cache');\n\n// ARE-VERSION file locations (check project first, then global)\nconst projectVersionFile = join(cwd, '.claude', 'ARE-VERSION');\nconst globalVersionFile = join(homeDir, '.claude', 'ARE-VERSION');\n\n// Ensure cache directory exists\nif (!existsSync(cacheDir)) {\n  mkdirSync(cacheDir, { recursive: true });\n}\nif (!existsSync(npmCacheDir)) {\n  mkdirSync(npmCacheDir, { recursive: true });\n}\n\n// Run check in background (spawn background process)\nconst child = spawn(process.execPath, ['-e', `\n  const fs = require('fs');\n  const { execSync } = require('child_process');\n\n  const cacheFile = ${JSON.stringify(cacheFile)};\n  const projectVersionFile = ${JSON.stringify(projectVersionFile)};\n  const globalVersionFile = ${JSON.stringify(globalVersionFile)};\n\n  // Check project directory first (local install), then global\n  let installed = '0.0.0';\n  try {\n    if (fs.existsSync(projectVersionFile)) {\n      installed = fs.readFileSync(projectVersionFile, 'utf8').trim();\n    } else if (fs.existsSync(globalVersionFile)) {\n      installed = fs.readFileSync(globalVersionFile, 'utf8').trim();\n    }\n  } catch (e) {}\n\n  let latest = null;\n  try {\n    latest = execSync('npm view agents-reverse-engineer version', { encoding: 'utf8', timeout: 10000, windowsHide: true }).trim();\n  } catch (e) {}\n\n  const result = {\n    update_available: latest && installed !== latest,\n    installed,\n    latest: latest || 'unknown',\n    checked: Math.floor(Date.now() / 1000)\n  };\n\n  fs.writeFileSync(cacheFile, JSON.stringify(result));\n`], {\n  stdio: 'ignore',\n  detached: true,\n  windowsHide: true,\n  env: {\n    ...process.env,\n  },\n});\n\nchild.unref();\n\n```\n\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n- Workflow & convention rules: if this file defines contribution guidelines, PR conventions, commit standards, testing mandates, tool usage requirements, approval workflows, AI agent behavioral instructions, or code conventions (naming standards, formatting rules, import ordering, linting policies), extract them as explicit, actionable rules. Sources: CONTRIBUTING.md, CI configs, PR templates, style guides, linter/formatter configs (.eslintrc, .prettierrc, .editorconfig), tsconfig strictness, README dev sections.\n\n\nTARGET LENGTH (MANDATORY):\n- Source file: 2261 characters\n- Target summary: ~226 characters (10% compression)\n- Maximum: 271 characters\n- Achieve this by: ultra-dense writing, minimal examples, single-sentence descriptions\n\nULTRA-COMPRESSION TECHNIQUES:\n- Use telegraphic style: \"exports foo, bar, baz\" not \"this file exports three functions\"\n- No code examples unless critical to understanding\n- One sentence per export maximum\n- Omit obvious parameter names (\"opts: Options\" not \"options: Options - configuration object\")\n- Use abbreviations: \"params\" not \"parameters\", \"config\" not \"configuration\"\n- Skip section if empty (don't write \"No special concerns\" or \"N/A\")\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nWORKFLOW & CONVENTION RULES (NEVER EXCLUDE):\n- Contribution requirements: branch naming, commit message formats, PR description templates\n- Testing mandates: required test types, coverage thresholds, \"never delete tests\" policies\n- Tool usage rules: required CLI commands, browser automation tools, linter/formatter requirements\n- Approval workflows: review requirements, CI checks that must pass, deployment gates\n- AI agent instructions: explicit directives for automated assistants (coding rules, prohibited actions)\n- Code conventions: naming standards (camelCase, PascalCase, snake_case for what), import ordering rules, file/folder naming patterns, formatting rules (indentation, quotes, semicolons), language idioms, TypeScript strictness policies, preferred patterns vs anti-patterns\nThese define how contributors (human or AI) must work within the project.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion .annex.sum file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**are-check-update.js spawns background process to check for ARE (agents-reverse-engineer) npm package updates and caches result to ~/.claude/cache/are-update-check.json.**\n\n## Execution Model\nRuns as SessionStart hook invoked once per session. Spawns detached child process with `stdio: 'ignore'` and `detached: true` to avoid blocking CLI startup; parent process immediately unrefs child and exits.\n\n## Cache Location & Structure\nCache file: `~/.claude/cache/are-update-check.json`. Cache dir created recursively if missing. Result object contains: `update_available` (boolean), `installed` (version string), `latest` (version string or \"unknown\"), `checked` (Unix timestamp).\n\n## Version Detection Logic\nChecks ARE-VERSION file in project-local `.claude/` directory first, then falls back to global `~/.claude/ARE-VERSION`. Defaults to \"0.0.0\" if neither exists. Spawned child reads these paths via `fs.existsSync()` and `fs.readFileSync()`.\n\n## npm Lookup\nSpawned process runs `npm view agents-reverse-engineer version` with 10-second timeout. Sets `windowsHide: true` to suppress cmd window on Windows. Captures stdout, trims whitespace. Falls back to \"unknown\" on any error (network, timeout, npm not found).\n\n## Directory Structure\nParent creates `~/.claude/cache/` and `~/.claude/cache/npm-cache/` (both with `{ recursive: true }`). Exports none; hook-only entry point.",
      "model": "claude-haiku-4-5-20251001",
      "inputTokens": 3,
      "outputTokens": 368,
      "cacheReadTokens": 16147,
      "cacheCreationTokens": 3461,
      "latencyMs": 25848,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "hooks/are-check-update.js",
          "sizeBytes": 2261
        }
      ]
    },
    {
      "timestamp": "2026-02-14T13:48:22.647Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: scripts/analyze-agents-compression.sh\n\n```text\n#!/bin/bash\n# Analyze the compression ratio of AGENTS.md files compared to their source documents\n\necho \"Analyzing AGENTS.md aggregation efficiency...\"\necho \"==============================================\"\necho \"\"\n\ntotal_input_size=0\ntotal_output_size=0\nfile_count=0\nratios=()\n\n# Find all AGENTS.md files (excluding the root one for now)\nwhile IFS= read -r agents_file; do\n  # Get the directory containing this AGENTS.md\n  dir=$(dirname \"$agents_file\")\n\n  # Skip if this is the root AGENTS.md (we'll handle it separately)\n  if [ \"$dir\" = \".\" ]; then\n    continue\n  fi\n\n  # Calculate total size of input documents\n  input_size=0\n  sum_count=0\n  subdir_count=0\n\n  # 1. Sum up all .sum files in this directory (not recursive)\n  while IFS= read -r sum_file; do\n    size=$(stat -f%z \"$sum_file\" 2>/dev/null || stat -c%s \"$sum_file\" 2>/dev/null)\n    input_size=$((input_size + size))\n    sum_count=$((sum_count + 1))\n  done < <(find \"$dir\" -maxdepth 1 -name \"*.sum\" -type f 2>/dev/null)\n\n  # 2. Sum up all AGENTS.md files in immediate subdirectories\n  while IFS= read -r subdir; do\n    subagents=\"$subdir/AGENTS.md\"\n    if [ -f \"$subagents\" ]; then\n      size=$(stat -f%z \"$subagents\" 2>/dev/null || stat -c%s \"$subagents\" 2>/dev/null)\n      input_size=$((input_size + size))\n      subdir_count=$((subdir_count + 1))\n    fi\n  done < <(find \"$dir\" -maxdepth 1 -mindepth 1 -type d 2>/dev/null)\n\n  # Get the size of this AGENTS.md\n  output_size=$(stat -f%z \"$agents_file\" 2>/dev/null || stat -c%s \"$agents_file\" 2>/dev/null)\n\n  # Calculate ratio if we have input\n  if [ \"$input_size\" -gt 0 ]; then\n    ratio=$(awk \"BEGIN {printf \\\"%.2f\\\", ($output_size / $input_size) * 100}\")\n    ratios+=(\"$ratio\")\n\n    # Accumulate totals\n    total_input_size=$((total_input_size + input_size))\n    total_output_size=$((total_output_size + output_size))\n    file_count=$((file_count + 1))\n\n    # Format directory name\n    dir_display=\"$dir\"\n    if [ ${#dir_display} -gt 50 ]; then\n      dir_display=\"...${dir_display: -47}\"\n    fi\n\n    # Show details for all files (or filter by criteria)\n    printf \"%-52s %7s → %7s (%6.2f%%) [%2d .sum, %2d subdirs]\\n\" \\\n      \"$dir_display\" \\\n      \"$(numfmt --to=iec --suffix=B $input_size 2>/dev/null || echo \"${input_size}B\")\" \\\n      \"$(numfmt --to=iec --suffix=B $output_size 2>/dev/null || echo \"${output_size}B\")\" \\\n      \"$ratio\" \\\n      \"$sum_count\" \\\n      \"$subdir_count\"\n  fi\ndone < <(find . -name \"AGENTS.md\" -type f | sort)\n\n# Now handle the root AGENTS.md separately\nif [ -f \"./AGENTS.md\" ]; then\n  echo \"\"\n  echo \"--- Root AGENTS.md ---\"\n\n  root_input=0\n  root_sum_count=0\n  root_subdir_count=0\n\n  # Sum direct .sum files in root\n  while IFS= read -r sum_file; do\n    size=$(stat -f%z \"$sum_file\" 2>/dev/null || stat -c%s \"$sum_file\" 2>/dev/null)\n    root_input=$((root_input + size))\n    root_sum_count=$((root_sum_count + 1))\n  done < <(find . -maxdepth 1 -name \"*.sum\" -type f 2>/dev/null)\n\n  # Sum AGENTS.md from immediate subdirectories\n  while IFS= read -r subdir; do\n    subagents=\"$subdir/AGENTS.md\"\n    if [ -f \"$subagents\" ]; then\n      size=$(stat -f%z \"$subagents\" 2>/dev/null || stat -c%s \"$subagents\" 2>/dev/null)\n      root_input=$((root_input + size))\n      root_subdir_count=$((root_subdir_count + 1))\n    fi\n  done < <(find . -maxdepth 1 -mindepth 1 -type d 2>/dev/null | grep -v \"^\\./\\.\")\n\n  root_output=$(stat -f%z \"./AGENTS.md\" 2>/dev/null || stat -c%s \"./AGENTS.md\" 2>/dev/null)\n\n  if [ \"$root_input\" -gt 0 ]; then\n    root_ratio=$(awk \"BEGIN {printf \\\"%.2f\\\", ($root_output / $root_input) * 100}\")\n\n    printf \"%-52s %7s → %7s (%6.2f%%) [%2d .sum, %2d subdirs]\\n\" \\\n      \".\" \\\n      \"$(numfmt --to=iec --suffix=B $root_input 2>/dev/null || echo \"${root_input}B\")\" \\\n      \"$(numfmt --to=iec --suffix=B $root_output 2>/dev/null || echo \"${root_output}B\")\" \\\n      \"$root_ratio\" \\\n      \"$root_sum_count\" \\\n      \"$root_subdir_count\"\n\n    # Add to totals\n    total_input_size=$((total_input_size + root_input))\n    total_output_size=$((total_output_size + root_output))\n    file_count=$((file_count + 1))\n    ratios+=(\"$root_ratio\")\n  fi\nfi\n\necho \"\"\necho \"==============================================\"\necho \"Summary Statistics\"\necho \"==============================================\"\necho \"AGENTS.md files analyzed: $file_count\"\necho \"Total input size: $(numfmt --to=iec-i --suffix=B $total_input_size 2>/dev/null || echo \"$total_input_size bytes\")\"\necho \"Total output size: $(numfmt --to=iec-i --suffix=B $total_output_size 2>/dev/null || echo \"$total_output_size bytes\")\"\n\n# Calculate overall average ratio\nif [ \"$file_count\" -gt 0 ] && [ \"$total_input_size\" -gt 0 ]; then\n  overall_ratio=$(awk \"BEGIN {printf \\\"%.2f\\\", ($total_output_size / $total_input_size) * 100}\")\n  compression_ratio=$(awk \"BEGIN {printf \\\"%.2f\\\", ($total_input_size / $total_output_size)}\")\n  space_saved=$((total_input_size - total_output_size))\n\n  echo \"Overall aggregation ratio: $overall_ratio%\"\n  echo \"Compression factor: ${compression_ratio}x\"\n  echo \"Space efficiency: $(numfmt --to=iec-i --suffix=B $space_saved 2>/dev/null || echo \"$space_saved bytes\") saved\"\n  echo \"\"\n\n  # Calculate mean ratio\n  sum_ratios=0\n  for r in \"${ratios[@]}\"; do\n    sum_ratios=$(awk \"BEGIN {print $sum_ratios + $r}\")\n  done\n  mean_ratio=$(awk \"BEGIN {printf \\\"%.2f\\\", $sum_ratios / $file_count}\")\n  echo \"Mean ratio: $mean_ratio%\"\n\n  # Calculate median\n  IFS=\n```\n\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.\\n' sorted_ratios=($(sort -n <<<\"${ratios[*]}\"))\n  unset IFS\n  mid=$((file_count / 2))\n  if [ $((file_count % 2)) -eq 0 ]; then\n    median=$(awk \"BEGIN {printf \\\"%.2f\\\", (${sorted_ratios[$mid-1]} + ${sorted_ratios[$mid]}) / 2}\")\n  else\n    median=\"${sorted_ratios[$mid]}\"\n  fi\n  echo \"Median ratio: $median%\"\n\n  # Find min and max\n  min_ratio=\"${sorted_ratios[0]}\"\n  max_ratio=\"${sorted_ratios[$((file_count-1))]}\"\n  echo \"Min ratio: $min_ratio%\"\n  echo \"Max ratio: $max_ratio%\"\n\n  echo \"\"\n  echo \"==============================================\"\n  echo \"Interpretation\"\n  echo \"==============================================\"\n\n  if (( $(echo \"$overall_ratio < 50\" | bc -l) )); then\n    echo \"✓ EXCELLENT: Strong aggregation/summarization (>2x compression)\"\n  elif (( $(echo \"$overall_ratio < 70\" | bc -l) )); then\n    echo \"✓ GOOD: Moderate aggregation (~1.5x compression)\"\n  elif (( $(echo \"$overall_ratio < 90\" | bc -l) )); then\n    echo \"~ ACCEPTABLE: Light aggregation (<1.2x compression)\"\n  else\n    echo \"⚠ MINIMAL: Very little compression (nearly 1:1)\"\n  fi\n\n  echo \"\"\n  echo \"The aggregation process reduces ${file_count} AGENTS.md files from\"\n  echo \"their combined input sources by a factor of ${compression_ratio}x.\"\n  echo \"==============================================\"\nfi\n\n```\n\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n- Workflow & convention rules: if this file defines contribution guidelines, PR conventions, commit standards, testing mandates, tool usage requirements, approval workflows, AI agent behavioral instructions, or code conventions (naming standards, formatting rules, import ordering, linting policies), extract them as explicit, actionable rules. Sources: CONTRIBUTING.md, CI configs, PR templates, style guides, linter/formatter configs (.eslintrc, .prettierrc, .editorconfig), tsconfig strictness, README dev sections.\n\n\nTARGET LENGTH (MANDATORY):\n- Source file: 6664 characters\n- Target summary: ~666 characters (10% compression)\n- Maximum: 799 characters\n- Achieve this by: ultra-dense writing, minimal examples, single-sentence descriptions\n\nULTRA-COMPRESSION TECHNIQUES:\n- Use telegraphic style: \"exports foo, bar, baz\" not \"this file exports three functions\"\n- No code examples unless critical to understanding\n- One sentence per export maximum\n- Omit obvious parameter names (\"opts: Options\" not \"options: Options - configuration object\")\n- Use abbreviations: \"params\" not \"parameters\", \"config\" not \"configuration\"\n- Skip section if empty (don't write \"No special concerns\" or \"N/A\")\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nWORKFLOW & CONVENTION RULES (NEVER EXCLUDE):\n- Contribution requirements: branch naming, commit message formats, PR description templates\n- Testing mandates: required test types, coverage thresholds, \"never delete tests\" policies\n- Tool usage rules: required CLI commands, browser automation tools, linter/formatter requirements\n- Approval workflows: review requirements, CI checks that must pass, deployment gates\n- AI agent instructions: explicit directives for automated assistants (coding rules, prohibited actions)\n- Code conventions: naming standards (camelCase, PascalCase, snake_case for what), import ordering rules, file/folder naming patterns, formatting rules (indentation, quotes, semicolons), language idioms, TypeScript strictness policies, preferred patterns vs anti-patterns\nThese define how contributors (human or AI) must work within the project.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion .annex.sum file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**analyze-agents-compression.sh measures compression efficiency of AGENTS.md aggregation by computing input→output ratios across the documentation tree.**\n\n## Role\nAudit script for the Agents Reverse Engineer (ARE) pipeline. Validates that AGENTS.md files effectively compress their input sources (.sum files and nested AGENTS.md from subdirectories).\n\n## Output Metrics\nComputes per-directory ratio = (AGENTS.md size / input size) × 100%, then aggregates:\n- Individual compression ratios for each AGENTS.md\n- Overall aggregation ratio, compression factor, space saved\n- Mean, median, min, max ratios across all files\n- Interpretation: <50% = EXCELLENT (>2x), <70% = GOOD (~1.5x), <90% = ACCEPTABLE (<1.2x), ≥90% = MINIMAL\n\n## Input Calculation\nFor each AGENTS.md at depth N:\n- Sums all .sum files in same directory (maxdepth 1)\n- Sums AGENTS.md from immediate subdirectories (maxdepth 1)\n- Root AGENTS.md processed separately, excluding hidden subdirs (grep filter: `!= \"^\\./\\.\"`\n\n## File Discovery\n`find . -name \"AGENTS.md\" -type f | sort` locates all AGENTS.md candidates; root handled specially to avoid double-counting.\n\n## Size Retrieval\nPortable across macOS/Linux: `stat -f%z` (BSD) fallback to `stat -c%s` (GNU); `numfmt --to=iec` for human-readable bytes, fallback to raw bytes.\n\n## Ratio Computation\nUses `awk \"BEGIN {printf \"%.2f\", (output / input) * 100}\"` for 2-decimal percentage; median computed via sorted array with even-count averaging.",
      "model": "claude-haiku-4-5-20251001",
      "inputTokens": 3,
      "outputTokens": 429,
      "cacheReadTokens": 16147,
      "cacheCreationTokens": 5239,
      "latencyMs": 34673,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "scripts/analyze-agents-compression.sh",
          "sizeBytes": 6674
        }
      ]
    },
    {
      "timestamp": "2026-02-14T13:48:27.550Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: src/version.ts\n\n```typescript\nimport { readFileSync } from 'node:fs';\nimport { dirname, join } from 'node:path';\nimport { fileURLToPath } from 'node:url';\n\n/**\n * Get package version from package.json.\n */\nexport function getVersion(): string {\n  try {\n    const __dirname = dirname(fileURLToPath(import.meta.url));\n    const packagePath = join(__dirname, '..', 'package.json');\n    const packageJson = JSON.parse(readFileSync(packagePath, 'utf-8'));\n    return packageJson.version || 'unknown';\n  } catch {\n    return 'unknown';\n  }\n}\n\n```\n\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n- Workflow & convention rules: if this file defines contribution guidelines, PR conventions, commit standards, testing mandates, tool usage requirements, approval workflows, AI agent behavioral instructions, or code conventions (naming standards, formatting rules, import ordering, linting policies), extract them as explicit, actionable rules. Sources: CONTRIBUTING.md, CI configs, PR templates, style guides, linter/formatter configs (.eslintrc, .prettierrc, .editorconfig), tsconfig strictness, README dev sections.\n\n\nTARGET LENGTH (MANDATORY):\n- Source file: 506 characters\n- Target summary: ~51 characters (10% compression)\n- Maximum: 61 characters\n- Achieve this by: ultra-dense writing, minimal examples, single-sentence descriptions\n\nULTRA-COMPRESSION TECHNIQUES:\n- Use telegraphic style: \"exports foo, bar, baz\" not \"this file exports three functions\"\n- No code examples unless critical to understanding\n- One sentence per export maximum\n- Omit obvious parameter names (\"opts: Options\" not \"options: Options - configuration object\")\n- Use abbreviations: \"params\" not \"parameters\", \"config\" not \"configuration\"\n- Skip section if empty (don't write \"No special concerns\" or \"N/A\")\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nWORKFLOW & CONVENTION RULES (NEVER EXCLUDE):\n- Contribution requirements: branch naming, commit message formats, PR description templates\n- Testing mandates: required test types, coverage thresholds, \"never delete tests\" policies\n- Tool usage rules: required CLI commands, browser automation tools, linter/formatter requirements\n- Approval workflows: review requirements, CI checks that must pass, deployment gates\n- AI agent instructions: explicit directives for automated assistants (coding rules, prohibited actions)\n- Code conventions: naming standards (camelCase, PascalCase, snake_case for what), import ordering rules, file/folder naming patterns, formatting rules (indentation, quotes, semicolons), language idioms, TypeScript strictness policies, preferred patterns vs anti-patterns\nThese define how contributors (human or AI) must work within the project.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion .annex.sum file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**src/version.ts exports getVersion() to retrieve package.json version string, falling back to 'unknown' on read/parse errors.**\n\n## Exports\n- `getVersion(): string` — reads package.json from parent directory, returns `version` field or 'unknown' if file missing/malformed\n\n## Key Details\n- Uses ESM introspection: `import.meta.url` → `fileURLToPath()` → `dirname()` to resolve __dirname\n- Path construction: `join(__dirname, '..', 'package.json')` assumes package.json at project root\n- Silent error handling: any fs/parse exception returns 'unknown' sentinel value\n- Return type always string (never null/undefined)",
      "model": "claude-haiku-4-5-20251001",
      "inputTokens": 3,
      "outputTokens": 165,
      "cacheReadTokens": 16147,
      "cacheCreationTokens": 2887,
      "latencyMs": 19577,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "src/version.ts",
          "sizeBytes": 506
        }
      ]
    },
    {
      "timestamp": "2026-02-14T13:48:24.743Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: scripts/analyze-sum-ratios.sh\n\n```text\n#!/bin/bash\n# Analyze the ratio of .sum file sizes compared to their source files\n\necho \"Analyzing .sum file size ratios...\"\necho \"==================================\"\necho \"\"\n\ntotal_source_size=0\ntotal_sum_size=0\nfile_count=0\nratios=()\n\n# Find all .sum files and analyze them\nwhile IFS= read -r sum_file; do\n  # Get the source file path (remove .sum extension)\n  source_file=\"${sum_file%.sum}\"\n\n  # Check if source file exists\n  if [ -f \"$source_file\" ]; then\n    # Get file sizes in bytes\n    source_size=$(stat -f%z \"$source_file\" 2>/dev/null || stat -c%s \"$source_file\" 2>/dev/null)\n    sum_size=$(stat -f%z \"$sum_file\" 2>/dev/null || stat -c%s \"$sum_file\" 2>/dev/null)\n\n    # Calculate ratio (sum / source) as percentage\n    if [ \"$source_size\" -gt 0 ]; then\n      ratio=$(awk \"BEGIN {printf \\\"%.2f\\\", ($sum_size / $source_size) * 100}\")\n      ratios+=(\"$ratio\")\n\n      # Accumulate totals\n      total_source_size=$((total_source_size + source_size))\n      total_sum_size=$((total_sum_size + sum_size))\n      file_count=$((file_count + 1))\n\n      # Show individual file stats (only first 10 and outliers)\n      if [ \"$file_count\" -le 10 ] || (( $(echo \"$ratio > 50\" | bc -l) )) || (( $(echo \"$ratio < 10\" | bc -l) )); then\n        printf \"%-60s %8d bytes -> %8d bytes (%6.2f%%)\\n\" \\\n          \"$(basename \"$source_file\")\" \"$source_size\" \"$sum_size\" \"$ratio\"\n      fi\n    fi\n  fi\ndone < <(find . -name \"*.sum\" -not -path \"*/node_modules/*\" -not -path \"*/.git/*\")\n\necho \"\"\necho \"==================================\"\necho \"Summary Statistics\"\necho \"==================================\"\necho \"Files analyzed: $file_count\"\necho \"Total source size: $(numfmt --to=iec-i --suffix=B $total_source_size 2>/dev/null || echo \"$total_source_size bytes\")\"\necho \"Total .sum size: $(numfmt --to=iec-i --suffix=B $total_sum_size 2>/dev/null || echo \"$total_sum_size bytes\")\"\n\n# Calculate overall average ratio\nif [ \"$file_count\" -gt 0 ] && [ \"$total_source_size\" -gt 0 ]; then\n  overall_ratio=$(awk \"BEGIN {printf \\\"%.2f\\\", ($total_sum_size / $total_source_size) * 100}\")\n  echo \"Overall ratio: $overall_ratio%\"\n  echo \"\"\n\n  # Calculate mean ratio\n  sum_ratios=0\n  for r in \"${ratios[@]}\"; do\n    sum_ratios=$(awk \"BEGIN {print $sum_ratios + $r}\")\n  done\n  mean_ratio=$(awk \"BEGIN {printf \\\"%.2f\\\", $sum_ratios / $file_count}\")\n  echo \"Mean ratio: $mean_ratio%\"\n\n  # Calculate median\n  IFS=\n```\n\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.\\n' sorted_ratios=($(sort -n <<<\"${ratios[*]}\"))\n  unset IFS\n  mid=$((file_count / 2))\n  if [ $((file_count % 2)) -eq 0 ]; then\n    median=$(awk \"BEGIN {printf \\\"%.2f\\\", (${sorted_ratios[$mid-1]} + ${sorted_ratios[$mid]}) / 2}\")\n  else\n    median=\"${sorted_ratios[$mid]}\"\n  fi\n  echo \"Median ratio: $median%\"\n\n  # Find min and max\n  min_ratio=\"${sorted_ratios[0]}\"\n  max_ratio=\"${sorted_ratios[$((file_count-1))]}\"\n  echo \"Min ratio: $min_ratio%\"\n  echo \"Max ratio: $max_ratio%\"\n\n  echo \"\"\n  echo \"==================================\"\n  echo \"Target Configuration: 25%\"\n  echo \"Actual Performance: $overall_ratio%\"\n\n  if (( $(echo \"$overall_ratio < 25\" | bc -l) )); then\n    echo \"✓ PASS: .sum files are more compact than configured target\"\n  elif (( $(echo \"$overall_ratio < 30\" | bc -l) )); then\n    echo \"~ ACCEPTABLE: .sum files are slightly above target\"\n  else\n    echo \"✗ ABOVE TARGET: .sum files are larger than configured\"\n  fi\n  echo \"==================================\"\nfi\n\n```\n\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n- Workflow & convention rules: if this file defines contribution guidelines, PR conventions, commit standards, testing mandates, tool usage requirements, approval workflows, AI agent behavioral instructions, or code conventions (naming standards, formatting rules, import ordering, linting policies), extract them as explicit, actionable rules. Sources: CONTRIBUTING.md, CI configs, PR templates, style guides, linter/formatter configs (.eslintrc, .prettierrc, .editorconfig), tsconfig strictness, README dev sections.\n\n\nTARGET LENGTH (MANDATORY):\n- Source file: 3360 characters\n- Target summary: ~336 characters (10% compression)\n- Maximum: 403 characters\n- Achieve this by: ultra-dense writing, minimal examples, single-sentence descriptions\n\nULTRA-COMPRESSION TECHNIQUES:\n- Use telegraphic style: \"exports foo, bar, baz\" not \"this file exports three functions\"\n- No code examples unless critical to understanding\n- One sentence per export maximum\n- Omit obvious parameter names (\"opts: Options\" not \"options: Options - configuration object\")\n- Use abbreviations: \"params\" not \"parameters\", \"config\" not \"configuration\"\n- Skip section if empty (don't write \"No special concerns\" or \"N/A\")\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nWORKFLOW & CONVENTION RULES (NEVER EXCLUDE):\n- Contribution requirements: branch naming, commit message formats, PR description templates\n- Testing mandates: required test types, coverage thresholds, \"never delete tests\" policies\n- Tool usage rules: required CLI commands, browser automation tools, linter/formatter requirements\n- Approval workflows: review requirements, CI checks that must pass, deployment gates\n- AI agent instructions: explicit directives for automated assistants (coding rules, prohibited actions)\n- Code conventions: naming standards (camelCase, PascalCase, snake_case for what), import ordering rules, file/folder naming patterns, formatting rules (indentation, quotes, semicolons), language idioms, TypeScript strictness policies, preferred patterns vs anti-patterns\nThese define how contributors (human or AI) must work within the project.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion .annex.sum file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**analyze-sum-ratios.sh measures compression efficiency of .sum documentation files against source files, reporting aggregate statistics and per-file ratios.**\n\n## Purpose\nAnalyzes all .sum files in the codebase to verify they meet the target 10% compression ratio (configured as 25% threshold for acceptance), calculating mean/median/min/max ratios and flagging outliers.\n\n## Execution Model\nBash script; finds .sum files via `find . -name \"*.sum\" -not -path \"*/node_modules/*\" -not -path \"*/.git/*\"`, pairs each with source file (removes .sum extension), computes `(sum_size / source_size) * 100` ratio using `awk`, accumulates totals.\n\n## Key Metrics & Thresholds\n- **Target ratio**: 25% (configured threshold; script aims for 10%)\n- **Outlier display threshold**: ratio > 50% or < 10% (shown regardless of file count limit)\n- **First 10 files**: always displayed\n- **Output columns**: filename, source bytes, .sum bytes, ratio percentage\n- **Pass criteria**: overall_ratio < 25% (PASS); < 30% (ACCEPTABLE); ≥ 30% (ABOVE TARGET)\n\n## Statistics Calculated\n- `file_count`: total .sum files processed\n- `total_source_size`, `total_sum_size`: cumulative byte counts\n- `overall_ratio`: `(total_sum_size / total_source_size) * 100`\n- `mean_ratio`: arithmetic mean of individual file ratios\n- `median`: middle value of sorted ratios array\n- `min_ratio`, `max_ratio`: extremes\n\n## Platform Compatibility\nUses conditional stat syntax: `stat -f%z` (BSD/macOS) fallback `stat -c%s` (GNU/Linux); `numfmt --to=iec-i --suffix=B` for human-readable sizes (graceful fallback to bytes).",
      "model": "claude-haiku-4-5-20251001",
      "inputTokens": 3,
      "outputTokens": 460,
      "cacheReadTokens": 16147,
      "cacheCreationTokens": 3983,
      "latencyMs": 22835,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "scripts/analyze-sum-ratios.sh",
          "sizeBytes": 3364
        }
      ]
    },
    {
      "timestamp": "2026-02-14T13:48:25.096Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: scripts/build-hooks.js\n\n```javascript\n#!/usr/bin/env node\n/**\n * Build Hooks Script\n *\n * Copies hook source files from hooks/ to hooks/dist/ for npm bundling.\n * Run via: npm run build:hooks\n * Called automatically during: npm run prepublishOnly\n */\n\nimport { copyFileSync, existsSync, mkdirSync, readdirSync } from 'fs';\nimport { join, dirname } from 'path';\nimport { fileURLToPath } from 'url';\n\nconst __filename = fileURLToPath(import.meta.url);\nconst __dirname = dirname(__filename);\nconst projectRoot = join(__dirname, '..');\n\nconst HOOKS_SRC = join(projectRoot, 'hooks');\nconst HOOKS_DIST = join(projectRoot, 'hooks', 'dist');\n\n// Ensure dist directory exists\nif (!existsSync(HOOKS_DIST)) {\n  mkdirSync(HOOKS_DIST, { recursive: true });\n}\n\n// Copy all .js files from hooks/ to hooks/dist/\nconst hookFiles = readdirSync(HOOKS_SRC).filter(\n  (f) => f.endsWith('.js') && f !== 'dist'\n);\n\nconsole.log('Building hooks...');\nfor (const file of hookFiles) {\n  const src = join(HOOKS_SRC, file);\n  const dest = join(HOOKS_DIST, file);\n  copyFileSync(src, dest);\n  console.log(`  Copied: ${file} -> hooks/dist/${file}`);\n}\n\nconsole.log(`Done. ${hookFiles.length} hook(s) built.`);\n\n```\n\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n- Workflow & convention rules: if this file defines contribution guidelines, PR conventions, commit standards, testing mandates, tool usage requirements, approval workflows, AI agent behavioral instructions, or code conventions (naming standards, formatting rules, import ordering, linting policies), extract them as explicit, actionable rules. Sources: CONTRIBUTING.md, CI configs, PR templates, style guides, linter/formatter configs (.eslintrc, .prettierrc, .editorconfig), tsconfig strictness, README dev sections.\n\n\nTARGET LENGTH (MANDATORY):\n- Source file: 1141 characters\n- Target summary: ~114 characters (10% compression)\n- Maximum: 137 characters\n- Achieve this by: ultra-dense writing, minimal examples, single-sentence descriptions\n\nULTRA-COMPRESSION TECHNIQUES:\n- Use telegraphic style: \"exports foo, bar, baz\" not \"this file exports three functions\"\n- No code examples unless critical to understanding\n- One sentence per export maximum\n- Omit obvious parameter names (\"opts: Options\" not \"options: Options - configuration object\")\n- Use abbreviations: \"params\" not \"parameters\", \"config\" not \"configuration\"\n- Skip section if empty (don't write \"No special concerns\" or \"N/A\")\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nWORKFLOW & CONVENTION RULES (NEVER EXCLUDE):\n- Contribution requirements: branch naming, commit message formats, PR description templates\n- Testing mandates: required test types, coverage thresholds, \"never delete tests\" policies\n- Tool usage rules: required CLI commands, browser automation tools, linter/formatter requirements\n- Approval workflows: review requirements, CI checks that must pass, deployment gates\n- AI agent instructions: explicit directives for automated assistants (coding rules, prohibited actions)\n- Code conventions: naming standards (camelCase, PascalCase, snake_case for what), import ordering rules, file/folder naming patterns, formatting rules (indentation, quotes, semicolons), language idioms, TypeScript strictness policies, preferred patterns vs anti-patterns\nThese define how contributors (human or AI) must work within the project.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion .annex.sum file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**build-hooks.js copies .js files from hooks/ to hooks/dist/ for npm bundling, invoked via npm run build:hooks.**\n\n## Execution Context\nEntry point script (shebang: `#!/usr/bin/env node`). Runs as npm lifecycle hook during `prepublishOnly`. Uses ES modules with `import.meta.url` to compute `__dirname` and `__filename` equivalents; resolves `projectRoot` via `join(__dirname, '..')`.\n\n## Directory Structure & Paths\n- `HOOKS_SRC = join(projectRoot, 'hooks')` — source directory containing hook implementations\n- `HOOKS_DIST = join(projectRoot, 'hooks', 'dist')` — destination directory for npm bundle\n- Creates `HOOKS_DIST` with `mkdirSync(HOOKS_DIST, { recursive: true })` if absent\n\n## Build Algorithm\nFilters `readdirSync(HOOKS_SRC)` by `.js` file extension, excludes `'dist'` folder. Iterates hookFiles, copies each via `copyFileSync(src, dest)`. Logs progress per file and summary count.\n\n## Console Output Format\n```\nBuilding hooks...\n  Copied: [filename] -> hooks/dist/[filename]\nDone. N hook(s) built.\n```",
      "model": "claude-haiku-4-5-20251001",
      "inputTokens": 3,
      "outputTokens": 307,
      "cacheReadTokens": 16147,
      "cacheCreationTokens": 3121,
      "latencyMs": 22767,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "scripts/build-hooks.js",
          "sizeBytes": 1141
        }
      ]
    },
    {
      "timestamp": "2026-02-14T13:48:31.534Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: .npmrc\n\n```text\n# Use a project-local cache to avoid global ~/.npm permission issues (EACCES)\n# and keep ARE command execution reproducible across environments.\n# Issue with codex only ...\ncache=.agents-reverse-engineer/.npm-cache\n\n```\n\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n- Workflow & convention rules: if this file defines contribution guidelines, PR conventions, commit standards, testing mandates, tool usage requirements, approval workflows, AI agent behavioral instructions, or code conventions (naming standards, formatting rules, import ordering, linting policies), extract them as explicit, actionable rules. Sources: CONTRIBUTING.md, CI configs, PR templates, style guides, linter/formatter configs (.eslintrc, .prettierrc, .editorconfig), tsconfig strictness, README dev sections.\n\n\nTARGET LENGTH (MANDATORY):\n- Source file: 215 characters\n- Target summary: ~22 characters (10% compression)\n- Maximum: 26 characters\n- Achieve this by: ultra-dense writing, minimal examples, single-sentence descriptions\n\nULTRA-COMPRESSION TECHNIQUES:\n- Use telegraphic style: \"exports foo, bar, baz\" not \"this file exports three functions\"\n- No code examples unless critical to understanding\n- One sentence per export maximum\n- Omit obvious parameter names (\"opts: Options\" not \"options: Options - configuration object\")\n- Use abbreviations: \"params\" not \"parameters\", \"config\" not \"configuration\"\n- Skip section if empty (don't write \"No special concerns\" or \"N/A\")\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nWORKFLOW & CONVENTION RULES (NEVER EXCLUDE):\n- Contribution requirements: branch naming, commit message formats, PR description templates\n- Testing mandates: required test types, coverage thresholds, \"never delete tests\" policies\n- Tool usage rules: required CLI commands, browser automation tools, linter/formatter requirements\n- Approval workflows: review requirements, CI checks that must pass, deployment gates\n- AI agent instructions: explicit directives for automated assistants (coding rules, prohibited actions)\n- Code conventions: naming standards (camelCase, PascalCase, snake_case for what), import ordering rules, file/folder naming patterns, formatting rules (indentation, quotes, semicolons), language idioms, TypeScript strictness policies, preferred patterns vs anti-patterns\nThese define how contributors (human or AI) must work within the project.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion .annex.sum file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**`.npmrc` configures npm to use a project-local cache directory (`.agents-reverse-engineer/.npm-cache`) to avoid EACCES permission errors and ensure reproducible command execution across environments.**\n\n## Configuration\n\nCache directory: `.agents-reverse-engineer/.npm-cache` (local cache prevents global `~/.npm` permission conflicts and maintains environment reproducibility for ARE command execution).",
      "model": "claude-haiku-4-5-20251001",
      "inputTokens": 3,
      "outputTokens": 88,
      "cacheReadTokens": 16147,
      "cacheCreationTokens": 2778,
      "latencyMs": 19233,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": ".npmrc",
          "sizeBytes": 215
        }
      ]
    },
    {
      "timestamp": "2026-02-14T13:48:33.162Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: CONTRIBUTING.md\n\n```markdown\n# Contributing to agents-reverse-engineer\n\nThanks for your interest in contributing! Here's how to get started.\n\n## Development Setup\n\n```bash\ngit clone https://github.com/GeoloeG-IsT/agents-reverse-engineer.git\ncd agents-reverse-engineer\nnpm install\nnpm run build\n```\n\n### Running in Development\n\n```bash\nnpm run dev\n```\n\nUses `tsx watch` for live reload during development.\n\n### Building\n\n```bash\nnpm run build\n```\n\nCompiles TypeScript to `dist/` via `tsc`.\n\n## Project Structure\n\n```\nsrc/\n  cli/          # CLI entry point and command handlers\n  config/       # Configuration schema and loading\n  discovery/    # File discovery and plan generation\n  generation/   # Documentation generation (AI-driven)\n  orchestration/# Concurrency pool, tracing, process management\n  ai/           # AI service abstraction and subprocess management\n```\n\n## Making Changes\n\n1. **Fork** the repo and create a branch from `main`\n2. Make your changes\n3. Ensure `npm run build` succeeds\n4. Submit a pull request\n\n## Reporting Bugs\n\nOpen an issue at [GitHub Issues](https://github.com/GeoloeG-IsT/agents-reverse-engineer/issues) with:\n\n- Steps to reproduce\n- Expected vs actual behavior\n- Your environment (OS, Node version, AI runtime)\n\n## Feature Requests\n\nOpen an issue with the `enhancement` label describing your use case and proposed solution.\n\n## License\n\nBy contributing, you agree that your contributions will be licensed under the MIT License.\n\n```\n\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n- Workflow & convention rules: if this file defines contribution guidelines, PR conventions, commit standards, testing mandates, tool usage requirements, approval workflows, AI agent behavioral instructions, or code conventions (naming standards, formatting rules, import ordering, linting policies), extract them as explicit, actionable rules. Sources: CONTRIBUTING.md, CI configs, PR templates, style guides, linter/formatter configs (.eslintrc, .prettierrc, .editorconfig), tsconfig strictness, README dev sections.\n\n\nTARGET LENGTH (MANDATORY):\n- Source file: 1436 characters\n- Target summary: ~144 characters (10% compression)\n- Maximum: 173 characters\n- Achieve this by: ultra-dense writing, minimal examples, single-sentence descriptions\n\nULTRA-COMPRESSION TECHNIQUES:\n- Use telegraphic style: \"exports foo, bar, baz\" not \"this file exports three functions\"\n- No code examples unless critical to understanding\n- One sentence per export maximum\n- Omit obvious parameter names (\"opts: Options\" not \"options: Options - configuration object\")\n- Use abbreviations: \"params\" not \"parameters\", \"config\" not \"configuration\"\n- Skip section if empty (don't write \"No special concerns\" or \"N/A\")\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nWORKFLOW & CONVENTION RULES (NEVER EXCLUDE):\n- Contribution requirements: branch naming, commit message formats, PR description templates\n- Testing mandates: required test types, coverage thresholds, \"never delete tests\" policies\n- Tool usage rules: required CLI commands, browser automation tools, linter/formatter requirements\n- Approval workflows: review requirements, CI checks that must pass, deployment gates\n- AI agent instructions: explicit directives for automated assistants (coding rules, prohibited actions)\n- Code conventions: naming standards (camelCase, PascalCase, snake_case for what), import ordering rules, file/folder naming patterns, formatting rules (indentation, quotes, semicolons), language idioms, TypeScript strictness policies, preferred patterns vs anti-patterns\nThese define how contributors (human or AI) must work within the project.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion .annex.sum file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**CONTRIBUTING.md defines contribution guidelines, development setup, project structure, and licensing terms for agents-reverse-engineer.**\n\n## Development Workflow\n\nFork from `main`, run `npm install`, `npm run build` (TypeScript → `dist/`), and `npm run dev` (`tsx watch`). PR submission required; `npm run build` must succeed.\n\n## Project Structure\n\n`src/cli/` (CLI entry point), `src/config/` (schema/loading), `src/discovery/` (file discovery), `src/generation/` (AI docs), `src/orchestration/` (concurrency/tracing), `src/ai/` (AI service/subprocess).\n\n## Issue Requirements\n\nBugs: steps to reproduce, expected vs actual behavior, environment (OS, Node version, AI runtime). Features: `enhancement` label, use case, proposed solution.\n\n## Licensing\n\nContributions licensed under MIT License.",
      "model": "claude-haiku-4-5-20251001",
      "inputTokens": 3,
      "outputTokens": 206,
      "cacheReadTokens": 16147,
      "cacheCreationTokens": 3088,
      "latencyMs": 17942,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "CONTRIBUTING.md",
          "sizeBytes": 1436
        }
      ]
    },
    {
      "timestamp": "2026-02-14T13:48:35.244Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: LICENSE\n\n```text\nMIT License\n\nCopyright (c) 2026 GeoloeG-IsT\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n\n```\n\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n- Workflow & convention rules: if this file defines contribution guidelines, PR conventions, commit standards, testing mandates, tool usage requirements, approval workflows, AI agent behavioral instructions, or code conventions (naming standards, formatting rules, import ordering, linting policies), extract them as explicit, actionable rules. Sources: CONTRIBUTING.md, CI configs, PR templates, style guides, linter/formatter configs (.eslintrc, .prettierrc, .editorconfig), tsconfig strictness, README dev sections.\n\n\nTARGET LENGTH (MANDATORY):\n- Source file: 1068 characters\n- Target summary: ~107 characters (10% compression)\n- Maximum: 128 characters\n- Achieve this by: ultra-dense writing, minimal examples, single-sentence descriptions\n\nULTRA-COMPRESSION TECHNIQUES:\n- Use telegraphic style: \"exports foo, bar, baz\" not \"this file exports three functions\"\n- No code examples unless critical to understanding\n- One sentence per export maximum\n- Omit obvious parameter names (\"opts: Options\" not \"options: Options - configuration object\")\n- Use abbreviations: \"params\" not \"parameters\", \"config\" not \"configuration\"\n- Skip section if empty (don't write \"No special concerns\" or \"N/A\")\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nWORKFLOW & CONVENTION RULES (NEVER EXCLUDE):\n- Contribution requirements: branch naming, commit message formats, PR description templates\n- Testing mandates: required test types, coverage thresholds, \"never delete tests\" policies\n- Tool usage rules: required CLI commands, browser automation tools, linter/formatter requirements\n- Approval workflows: review requirements, CI checks that must pass, deployment gates\n- AI agent instructions: explicit directives for automated assistants (coding rules, prohibited actions)\n- Code conventions: naming standards (camelCase, PascalCase, snake_case for what), import ordering rules, file/folder naming patterns, formatting rules (indentation, quotes, semicolons), language idioms, TypeScript strictness policies, preferred patterns vs anti-patterns\nThese define how contributors (human or AI) must work within the project.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion .annex.sum file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**LICENSE establishes MIT licensing terms for GeoloeG-IsT software (2026), permitting unrestricted use, modification, distribution, and sublicensing under copyright and warranty disclaimers.**\n\n## License Terms\n\nMIT License grants permission to obtain, use, copy, modify, merge, publish, distribute, sublicense, and sell copies with conditions: (1) copyright notice and license text must be included in all copies; (2) software provided \"as-is\" without warranty of any kind; (3) authors/copyright holders not liable for claims, damages, or liability arising from use, contract, tort, or otherwise.\n\n## Copyright Attribution\n\nCopyright holder: GeoloeG-IsT. Year: 2026.",
      "model": "claude-haiku-4-5-20251001",
      "inputTokens": 3,
      "outputTokens": 160,
      "cacheReadTokens": 16147,
      "cacheCreationTokens": 2956,
      "latencyMs": 16623,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "LICENSE",
          "sizeBytes": 1068
        }
      ]
    },
    {
      "timestamp": "2026-02-14T13:48:33.734Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: LANGUAGES-MANIFEST.md\n\n```markdown\n# Package Manifest Files by Language\n\n| Language | Manifest File(s) | Package Manager | Supported |\n|----------|------------------|-----------------|-----------|\n| JavaScript/TypeScript | `package.json` | npm, yarn, pnpm | ✓ |\n| Python | `requirements.txt`, `pyproject.toml`, `setup.py`, `Pipfile` | pip, poetry, pipenv | ✓ |\n| Ruby | `Gemfile` | bundler | |\n| Go | `go.mod` | go modules | ✓ |\n| Rust | `Cargo.toml` | cargo | ✓ |\n| Java | `pom.xml` (Maven), `build.gradle` (Gradle) | Maven, Gradle | |\n| Kotlin | `build.gradle.kts`, `build.gradle` | Gradle | |\n| C#/.NET | `*.csproj`, `packages.config`, `*.fsproj` | NuGet | |\n| PHP | `composer.json` | Composer | |\n| Swift | `Package.swift` | Swift PM | |\n| Elixir | `mix.exs` | Mix | |\n| Erlang | `rebar.config` | rebar3 | |\n| Scala | `build.sbt` | sbt | |\n| Clojure | `deps.edn`, `project.clj` | clj, Leiningen | |\n| Haskell | `package.yaml`, `*.cabal`, `stack.yaml` | cabal, stack | |\n| Dart/Flutter | `pubspec.yaml` | pub | |\n| Lua | `*.rockspec` | LuaRocks | |\n| R | `DESCRIPTION` | CRAN | |\n| Julia | `Project.toml` | Pkg | |\n| Zig | `build.zig.zon` | zig | |\n| Nim | `*.nimble` | nimble | |\n| OCaml | `dune-project`, `*.opam` | dune, opam | |\n| C/C++ | `CMakeLists.txt`, `conanfile.txt`, `vcpkg.json` | CMake, Conan, vcpkg | |\n\n```\n\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n- Workflow & convention rules: if this file defines contribution guidelines, PR conventions, commit standards, testing mandates, tool usage requirements, approval workflows, AI agent behavioral instructions, or code conventions (naming standards, formatting rules, import ordering, linting policies), extract them as explicit, actionable rules. Sources: CONTRIBUTING.md, CI configs, PR templates, style guides, linter/formatter configs (.eslintrc, .prettierrc, .editorconfig), tsconfig strictness, README dev sections.\n\n\nTARGET LENGTH (MANDATORY):\n- Source file: 1285 characters\n- Target summary: ~129 characters (10% compression)\n- Maximum: 155 characters\n- Achieve this by: ultra-dense writing, minimal examples, single-sentence descriptions\n\nULTRA-COMPRESSION TECHNIQUES:\n- Use telegraphic style: \"exports foo, bar, baz\" not \"this file exports three functions\"\n- No code examples unless critical to understanding\n- One sentence per export maximum\n- Omit obvious parameter names (\"opts: Options\" not \"options: Options - configuration object\")\n- Use abbreviations: \"params\" not \"parameters\", \"config\" not \"configuration\"\n- Skip section if empty (don't write \"No special concerns\" or \"N/A\")\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nWORKFLOW & CONVENTION RULES (NEVER EXCLUDE):\n- Contribution requirements: branch naming, commit message formats, PR description templates\n- Testing mandates: required test types, coverage thresholds, \"never delete tests\" policies\n- Tool usage rules: required CLI commands, browser automation tools, linter/formatter requirements\n- Approval workflows: review requirements, CI checks that must pass, deployment gates\n- AI agent instructions: explicit directives for automated assistants (coding rules, prohibited actions)\n- Code conventions: naming standards (camelCase, PascalCase, snake_case for what), import ordering rules, file/folder naming patterns, formatting rules (indentation, quotes, semicolons), language idioms, TypeScript strictness policies, preferred patterns vs anti-patterns\nThese define how contributors (human or AI) must work within the project.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion .annex.sum file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**LANGUAGES-MANIFEST.md is a reference table mapping programming languages to their package manifest files, package managers, and support status.**\n\n## Content Structure\n\nReference table with 5 columns: Language, Manifest File(s), Package Manager, Supported (checkmark).\n\n## Language Coverage\n\nSupports 23 languages with manifest mappings:\n- **JavaScript/TypeScript**: `package.json` (npm, yarn, pnpm) — marked supported ✓\n- **Python**: `requirements.txt`, `pyproject.toml`, `setup.py`, `Pipfile` (pip, poetry, pipenv) — marked supported ✓\n- **Go**: `go.mod` (go modules) — marked supported ✓\n- **Rust**: `Cargo.toml` (cargo) — marked supported ✓\n- **Ruby**, **Java**, **Kotlin**, **C#/.NET**, **PHP**, **Swift**, **Elixir**, **Erlang**, **Scala**, **Clojure**, **Haskell**, **Dart/Flutter**, **Lua**, **R**, **Julia**, **Zig**, **Nim**, **OCaml**, **C/C++**: mapped to manifest files and package managers; support status not indicated (unmarked rows)\n\n## Usage Context\n\nReference for dependency analysis tools, package resolution, multi-language project scanning, and CI/CD integration. Marks JavaScript, Python, Go, Rust as currently supported; others listed for future implementation or informational purposes.",
      "model": "claude-haiku-4-5-20251001",
      "inputTokens": 3,
      "outputTokens": 342,
      "cacheReadTokens": 16147,
      "cacheCreationTokens": 3267,
      "latencyMs": 19624,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "LANGUAGES-MANIFEST.md",
          "sizeBytes": 1293
        }
      ]
    },
    {
      "timestamp": "2026-02-14T13:48:37.015Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: package.json\n\n```json\n{\n  \"name\": \"agents-reverse-engineer\",\n  \"version\": \"0.9.10\",\n  \"description\": \"CLI tool for reverse-engineering codebase documentation for AI agents\",\n  \"type\": \"module\",\n  \"bin\": {\n    \"agents-reverse-engineer\": \"dist/cli/index.js\",\n    \"are\": \"dist/cli/index.js\"\n  },\n  \"scripts\": {\n    \"build\": \"tsc\",\n    \"build:hooks\": \"node scripts/build-hooks.js\",\n    \"prepack\": \"rm -f LICENSE.sum README.md.sum\",\n    \"prepublishOnly\": \"npm run build && npm run build:hooks\",\n    \"dev\": \"tsx watch src/cli/index.ts\"\n  },\n  \"dependencies\": {\n    \"fast-glob\": \"^3.3.3\",\n    \"ignore\": \"^7.0.3\",\n    \"isbinaryfile\": \"^5.0.4\",\n    \"jsonc-parser\": \"^3.3.1\",\n    \"picocolors\": \"^1.1.1\",\n    \"simple-git\": \"^3.27.0\",\n    \"yaml\": \"^2.7.0\",\n    \"zod\": \"^3.24.1\"\n  },\n  \"devDependencies\": {\n    \"@types/node\": \"^22.10.7\",\n    \"marked\": \"^15.0.12\",\n    \"tsx\": \"^4.19.2\",\n    \"typescript\": \"^5.7.3\"\n  },\n  \"engines\": {\n    \"node\": \">=18.0.0\"\n  },\n  \"keywords\": [\n    \"documentation\",\n    \"codebase\",\n    \"ai\",\n    \"agents\",\n    \"reverse-engineering\",\n    \"claude-code\",\n    \"codex\",\n    \"gemini-cli\",\n    \"opencode\",\n    \"agents-md\",\n    \"code-documentation\",\n    \"ai-documentation\",\n    \"codebase-analysis\",\n    \"developer-tools\",\n    \"cli\",\n    \"llm\"\n  ],\n  \"author\": \"GeoloeG-IsT\",\n  \"license\": \"MIT\",\n  \"exports\": {\n    \".\": \"./dist/cli/index.js\",\n    \"./core\": {\n      \"types\": \"./dist/core/index.d.ts\",\n      \"default\": \"./dist/core/index.js\"\n    }\n  },\n  \"main\": \"dist/cli/index.js\",\n  \"files\": [\n    \"dist\",\n    \"hooks/dist\",\n    \"README.md\",\n    \"LICENSE\"\n  ],\n  \"repository\": {\n    \"type\": \"git\",\n    \"url\": \"git+https://github.com/GeoloeG-IsT/agents-reverse-engineer.git\"\n  },\n  \"bugs\": {\n    \"url\": \"https://github.com/GeoloeG-IsT/agents-reverse-engineer/issues\"\n  },\n  \"homepage\": \"https://github.com/GeoloeG-IsT/agents-reverse-engineer#readme\"\n}\n\n```\n\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n- Workflow & convention rules: if this file defines contribution guidelines, PR conventions, commit standards, testing mandates, tool usage requirements, approval workflows, AI agent behavioral instructions, or code conventions (naming standards, formatting rules, import ordering, linting policies), extract them as explicit, actionable rules. Sources: CONTRIBUTING.md, CI configs, PR templates, style guides, linter/formatter configs (.eslintrc, .prettierrc, .editorconfig), tsconfig strictness, README dev sections.\n\n\nTARGET LENGTH (MANDATORY):\n- Source file: 1839 characters\n- Target summary: ~184 characters (10% compression)\n- Maximum: 221 characters\n- Achieve this by: ultra-dense writing, minimal examples, single-sentence descriptions\n\nULTRA-COMPRESSION TECHNIQUES:\n- Use telegraphic style: \"exports foo, bar, baz\" not \"this file exports three functions\"\n- No code examples unless critical to understanding\n- One sentence per export maximum\n- Omit obvious parameter names (\"opts: Options\" not \"options: Options - configuration object\")\n- Use abbreviations: \"params\" not \"parameters\", \"config\" not \"configuration\"\n- Skip section if empty (don't write \"No special concerns\" or \"N/A\")\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nWORKFLOW & CONVENTION RULES (NEVER EXCLUDE):\n- Contribution requirements: branch naming, commit message formats, PR description templates\n- Testing mandates: required test types, coverage thresholds, \"never delete tests\" policies\n- Tool usage rules: required CLI commands, browser automation tools, linter/formatter requirements\n- Approval workflows: review requirements, CI checks that must pass, deployment gates\n- AI agent instructions: explicit directives for automated assistants (coding rules, prohibited actions)\n- Code conventions: naming standards (camelCase, PascalCase, snake_case for what), import ordering rules, file/folder naming patterns, formatting rules (indentation, quotes, semicolons), language idioms, TypeScript strictness policies, preferred patterns vs anti-patterns\nThese define how contributors (human or AI) must work within the project.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion .annex.sum file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**package.json defines CLI entry points, dependencies, and publish configuration for agents-reverse-engineer, a documentation generation tool for AI agent codebases.**\n\n## Entry Points\nDual CLI commands: `agents-reverse-engineer` and `are` both resolve to `dist/cli/index.js`. ESM module with exports: default CLI at `.`, core library at `./core` with TypeScript definitions.\n\n## Dependencies\n**Runtime:** fast-glob (file globbing), ignore (gitignore parsing), isbinaryfile (binary detection), jsonc-parser (JSON with comments), picocolors (terminal colors), simple-git (git operations), yaml (YAML parsing), zod (schema validation).\n**Dev:** @types/node ≥22.10.7, marked (markdown parsing), tsx (TypeScript execution), typescript ≥5.7.3.\n\n## Build & Publish Scripts\n`build` → tsc compilation; `build:hooks` → node scripts/build-hooks.js; `prepublishOnly` runs build + build:hooks; `prepack` removes .sum artifacts before npm packaging; `dev` → tsx watch for live development.\n\n## Distribution & Publishing\nNode ≥18.0.0 required. Published files: dist/, hooks/dist/, README.md, LICENSE. Prepack removes LICENSE.sum and README.md.sum to prevent documentation artifact leakage in published package.",
      "model": "claude-haiku-4-5-20251001",
      "inputTokens": 3,
      "outputTokens": 317,
      "cacheReadTokens": 16147,
      "cacheCreationTokens": 3480,
      "latencyMs": 18000,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "package.json",
          "sizeBytes": 1839
        }
      ]
    },
    {
      "timestamp": "2026-02-14T13:48:38.694Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: tsconfig.json\n\n```json\n{\n  \"compilerOptions\": {\n    \"target\": \"ES2022\",\n    \"module\": \"NodeNext\",\n    \"moduleResolution\": \"NodeNext\",\n    \"lib\": [\"ES2022\"],\n    \"outDir\": \"dist\",\n    \"rootDir\": \"src\",\n    \"strict\": true,\n    \"esModuleInterop\": true,\n    \"skipLibCheck\": true,\n    \"forceConsistentCasingInFileNames\": true,\n    \"declaration\": true,\n    \"declarationMap\": true,\n    \"sourceMap\": true,\n    \"resolveJsonModule\": true,\n    \"isolatedModules\": true\n  },\n  \"include\": [\"src/**/*\"],\n  \"exclude\": [\"node_modules\", \"dist\"]\n}\n\n```\n\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n- Workflow & convention rules: if this file defines contribution guidelines, PR conventions, commit standards, testing mandates, tool usage requirements, approval workflows, AI agent behavioral instructions, or code conventions (naming standards, formatting rules, import ordering, linting policies), extract them as explicit, actionable rules. Sources: CONTRIBUTING.md, CI configs, PR templates, style guides, linter/formatter configs (.eslintrc, .prettierrc, .editorconfig), tsconfig strictness, README dev sections.\n\n\nTARGET LENGTH (MANDATORY):\n- Source file: 506 characters\n- Target summary: ~51 characters (10% compression)\n- Maximum: 61 characters\n- Achieve this by: ultra-dense writing, minimal examples, single-sentence descriptions\n\nULTRA-COMPRESSION TECHNIQUES:\n- Use telegraphic style: \"exports foo, bar, baz\" not \"this file exports three functions\"\n- No code examples unless critical to understanding\n- One sentence per export maximum\n- Omit obvious parameter names (\"opts: Options\" not \"options: Options - configuration object\")\n- Use abbreviations: \"params\" not \"parameters\", \"config\" not \"configuration\"\n- Skip section if empty (don't write \"No special concerns\" or \"N/A\")\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nWORKFLOW & CONVENTION RULES (NEVER EXCLUDE):\n- Contribution requirements: branch naming, commit message formats, PR description templates\n- Testing mandates: required test types, coverage thresholds, \"never delete tests\" policies\n- Tool usage rules: required CLI commands, browser automation tools, linter/formatter requirements\n- Approval workflows: review requirements, CI checks that must pass, deployment gates\n- AI agent instructions: explicit directives for automated assistants (coding rules, prohibited actions)\n- Code conventions: naming standards (camelCase, PascalCase, snake_case for what), import ordering rules, file/folder naming patterns, formatting rules (indentation, quotes, semicolons), language idioms, TypeScript strictness policies, preferred patterns vs anti-patterns\nThese define how contributors (human or AI) must work within the project.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion .annex.sum file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**tsconfig.json defines TypeScript compilation target ES2022 with strict type checking, module resolution via NodeNext, and output to dist/ from src/.**\n\n## Compiler Configuration\n\n- `target: \"ES2022\"` — output JavaScript for ES2022 standard\n- `module: \"NodeNext\"` — emit ES modules with Node.js interop\n- `moduleResolution: \"NodeNext\"` — resolve imports using Node.js algorithm\n- `lib: [\"ES2022\"]` — include ES2022 type definitions\n- `outDir: \"dist\"` — compiled output directory\n- `rootDir: \"src\"` — source root directory\n\n## Type Safety & Strictness\n\n- `strict: true` — enforce all strict type-checking options (noImplicitAny, strictNullChecks, strictFunctionTypes, etc.)\n- `isolatedModules: true` — each file transpiles independently; require explicit imports/exports\n- `skipLibCheck: true` — skip type checking of declaration files\n\n## Output & Source Maps\n\n- `declaration: true` — emit `.d.ts` type definition files\n- `declarationMap: true` — emit source maps for declarations\n- `sourceMap: true` — emit `.js.map` files for debugging\n- `resolveJsonModule: true` — allow importing JSON files as modules\n\n## Compatibility & File Handling\n\n- `esModuleInterop: true` — add helpers for CommonJS/ES module compatibility\n- `forceConsistentCasingInFileNames: true` — error on case-insensitive file name mismatches\n\n## File Inclusion/Exclusion\n\n- `include: [\"src/**/*\"]` — compile all files under src/\n- `exclude: [\"node_modules\", \"dist\"]` — skip dependencies and build output",
      "model": "claude-haiku-4-5-20251001",
      "inputTokens": 3,
      "outputTokens": 410,
      "cacheReadTokens": 17645,
      "cacheCreationTokens": 1410,
      "latencyMs": 16569,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "tsconfig.json",
          "sizeBytes": 506
        }
      ]
    },
    {
      "timestamp": "2026-02-14T13:48:36.658Z",
      "prompt": "Analyze this source file and generate a summary that captures what an AI coding assistant needs to know to work with this file effectively.\n\nFile: README.md\n\n```markdown\n<div align=\"center\">\n\n# AGENTS REVERSE ENGINEER (ARE)\n\n**Reverse engineer your codebase into AI-friendly documentation.**\n\n**Generate `.sum` files and `AGENTS.md` for Claude Code, OpenCode, and any AI assistant that supports `AGENTS.md`.**\n\n[![npm version](https://img.shields.io/npm/v/agents-reverse-engineer?style=for-the-badge&logo=npm&logoColor=white&color=CB3837)](https://www.npmjs.com/package/agents-reverse-engineer)\n[![npm downloads](https://img.shields.io/npm/dw/agents-reverse-engineer?style=for-the-badge&logo=npm&logoColor=white&color=CB3837)](https://www.npmjs.com/package/agents-reverse-engineer)\n[![GitHub stars](https://img.shields.io/github/stars/GeoloeG-IsT/agents-reverse-engineer?style=for-the-badge&logo=github&color=181717)](https://github.com/GeoloeG-IsT/agents-reverse-engineer)\n[![Last commit](https://img.shields.io/github/last-commit/GeoloeG-IsT/agents-reverse-engineer?style=for-the-badge&logo=github&color=181717)](https://github.com/GeoloeG-IsT/agents-reverse-engineer/commits)\n[![License](https://img.shields.io/badge/license-MIT-blue?style=for-the-badge)](LICENSE)\n\n<br>\n\n```bash\nnpx agents-reverse-engineer@latest\n```\n\n**Interactive installer with runtime and location selection.**\n\n**Works on Mac, Windows, and Linux.**\n\n<br>\n\n_\"Finally, my AI assistant actually understands my codebase structure.\"_\n\n_\"No more explaining the same architecture in every conversation.\"_\n\n<br>\n\n[Why This Exists](#why-this-exists) · [How It Works](#how-it-works) · [Commands](#commands) · [Generated Docs](#generated-documentation)\n\n<br>\n\n### Works with\n\n[<img src=\"https://img.shields.io/badge/Claude_Code-F97316?style=for-the-badge&logo=anthropic&logoColor=white\" alt=\"Claude Code\">](https://claude.ai/claude-code)\n[<img src=\"https://img.shields.io/badge/Gemini_CLI-4285F4?style=for-the-badge&logo=google&logoColor=white\" alt=\"Gemini CLI\">](https://github.com/google-gemini/gemini-cli)\n[<img src=\"https://img.shields.io/badge/OpenCode-000000?style=for-the-badge&logo=github&logoColor=white\" alt=\"OpenCode\">](https://github.com/anomalyco/opencode)\n[<img src=\"https://img.shields.io/badge/Any_AGENTS.md_tool-6B7280?style=for-the-badge\" alt=\"Any AGENTS.md tool\">](#)\n\n</div>\n\n---\n\n## Why This Exists\n\nAI coding assistants are powerful, but they don't know your codebase. Every session starts fresh. You explain the same architecture, the same patterns, the same file locations — over and over.\n\n**agents-reverse-engineer** fixes that. It generates documentation that AI assistants actually read:\n\n- **`.sum` files** — Per-file summaries with purpose, exports, dependencies\n- **`AGENTS.md`** — Per-directory overviews with file organization (standard format)\n- **`CLAUDE.md`** / **`GEMINI.md`** / **`OPENCODE.md`** — Runtime-specific project entry points\n\nThe result: Your AI assistant understands your codebase from the first message.\n\n---\n\n## Who This Is For\n\nDevelopers using AI coding assistants (Claude Code, Codex, OpenCode, Gemini CLI, or any tool supporting `AGENTS.md`) who want their assistant to actually understand their project structure — without manually writing documentation or repeating context every session.\n\n---\n\n## Getting Started\n\n### 1. Install Commands\n\n```bash\nnpx agents-reverse-engineer@latest\n```\n\nThe interactive installer prompts you to:\n\n1. **Select runtime** — Claude Code, Codex, OpenCode, Gemini CLI, or all\n2. **Select location** — Global (`~/.claude/`, `~/.agents/`, etc.) or local (`./.claude/`, `./.agents/`, etc.)\n\nThis installs:\n\n- **Commands** — `/are-init`, `/are-discover`, `/are-generate`, `/are-update`, `/are-specify`, `/are-clean`\n- **Codex context rules** — local install writes `./AGENTS.override.md`; global install writes `~/.codex/AGENTS.override.md` with lazy AGENTS hierarchy loading guidance\n\n### 2. Initialize Configuration\n\nAfter installation, create the configuration file in your AI assistant:\n\n```bash\n/are-init\n```\n\nThis creates `.agents-reverse-engineer/config.yaml` with default settings.\n\n### 3. Generate Documentation\n\nIn your AI assistant:\n\n```\n/are-discover\n/are-generate\n```\n\nThe assistant creates the plan and generates all documentation.\n\n### Non-Interactive Installation\n\n```bash\n# Install for Claude Code globally\nnpx agents-reverse-engineer@latest --runtime claude -g\n\n# Install for Codex globally\nnpx agents-reverse-engineer@latest --runtime codex -g\n\n# Install for all runtimes locally\nnpx agents-reverse-engineer@latest --runtime all -l\n```\n\n### Uninstall\n\n```bash\nnpx agents-reverse-engineer@latest uninstall\n```\n\nRemoves:\n- Command files (`/are-*` commands)\n- ARE permissions from settings.json\n- `.agents-reverse-engineer` folder (local installs only)\n\nUse `--runtime` and `-g`/`-l` flags for specific targets.\n\n### Checking Version\n\n```bash\nnpx agents-reverse-engineer@latest --version\n```\n\n---\n\n## How It Works\n\n### 1. Install Commands\n\n```bash\nnpx agents-reverse-engineer@latest\n```\n\nInteractive installer installs commands and hooks for your chosen runtime(s).\n\n**Runtimes:** Claude Code, Codex, OpenCode, Gemini CLI (or all at once)\n\n---\n\n### 2. Initialize Configuration\n\n```\n/are-init\n```\n\nCreates `.agents-reverse-engineer/config.yaml` with exclusion patterns and options.\n\n---\n\n### 3. Discover & Plan\n\n```\n/are-discover\n```\n\nScans your codebase (respecting `.gitignore`), detects file types, and creates `GENERATION-PLAN.md` with all files to analyze.\n\nUses **post-order traversal** — deepest directories first, so child documentation exists before parent directories are documented.\n\n---\n\n### 4. Generate (in your AI assistant)\n\n```\n/are-generate\n```\n\nYour AI assistant executes the plan:\n\n1. **File Analysis** — Creates `.sum` file for each source file\n2. **Directory Docs** — Creates `AGENTS.md` and `CLAUDE.md` pointer for each directory\n\n---\n\n### 5. Update Incrementally\n\n```\n/are-update\n```\n\nOnly regenerates documentation for files that changed since last run.\n\n---\n\n### 6. Generate Specification\n\n```\n/are-specify\n```\n\nSynthesizes all AGENTS.md documentation into a single project specification document (`specs/SPEC.md`). Use `--multi-file` to split into separate files, or `--dry-run` to preview without calling the AI.\n\n---\n\n## Commands\n\n| Command                         | Description                      |\n| ------------------------------- | -------------------------------- |\n| `are`                           | Interactive installer (default)  |\n| `are install`                   | Install with prompts             |\n| `are install --runtime <rt> -g` | Install to runtime globally      |\n| `are install --runtime <rt> -l` | Install to runtime locally       |\n| `are uninstall`                 | Uninstall (remove files/hooks)   |\n| `are init`                      | Create configuration file        |\n| `are discover`                  | Scan files and create GENERATION-PLAN.md |\n| `are generate`                  | Generate all documentation       |\n| `are update`                    | Update changed files only        |\n| `are specify`                   | Generate project specification   |\n| `are rebuild`                   | Reconstruct project from specs   |\n| `are clean`                     | Remove all generated docs        |\n\n**Runtimes:** `claude`, `codex`, `opencode`, `gemini`, `all`\n\n### General CLI Options\n\n| Flag                | Description                                              | Applies to                          |\n| ------------------- | -------------------------------------------------------- | ----------------------------------- |\n| `--model <name>`    | AI model to use (e.g., sonnet, opus, haiku)              | generate, update, specify, rebuild  |\n| `--backend <name>`  | AI backend (claude, codex, gemini, opencode, auto)       | generate, update, specify, rebuild  |\n| `--concurrency <n>` | Number of concurrent AI calls (default: auto)            | generate, update, rebuild           |\n| `--dry-run`         | Show plan without writing files                          | generate, update, specify, rebuild, clean |\n| `--force`           | Overwrite existing files                                 | init, install, generate, specify, rebuild |\n| `--fail-fast`       | Stop on first file analysis failure                      | generate, update, rebuild           |\n| `--show-excluded`   | Show excluded files during discovery                     | discover                            |\n| `--uncommitted`     | Include uncommitted changes                              | update                              |\n| `--eval`            | Namespace output by backend.model for comparison         | generate, update                    |\n| `--debug`           | Show AI prompts and backend details                      | discover, generate, update, specify, rebuild |\n| `--trace`           | Enable concurrency tracing (.agents-reverse-engineer/traces/) | generate, update, specify, rebuild |\n\n### AI Assistant Commands\n\n| Command         | Description                    | Supported Runtimes       |\n| --------------- | ------------------------------ | ------------------------ |\n| `/are-init`     | Initialize config and commands | Claude, Codex, OpenCode, Gemini |\n| `/are-discover` | Rediscover and regenerate plan | Claude, Codex, OpenCode, Gemini |\n| `/are-generate` | Generate all documentation     | Claude, Codex, OpenCode, Gemini |\n| `/are-update`   | Update changed files only      | Claude, Codex, OpenCode, Gemini |\n| `/are-specify`  | Generate project specification | Claude, Codex, OpenCode, Gemini |\n| `/are-rebuild`  | Reconstruct project from specs | Claude, Codex, OpenCode, Gemini |\n| `/are-clean`    | Remove all generated docs      | Claude, Codex, OpenCode, Gemini |\n\n---\n\n## Generated Documentation\n\n### `.sum` Files (Per File)\n\n```yaml\n---\nfile_type: service\ngenerated_at: 2026-01-30T12:00:00Z\n---\n\n## Purpose\nHandles user authentication via JWT tokens.\n\n## Public Interface\n- `authenticate(token: string): User`\n- `generateToken(user: User): string`\n\n## Dependencies\n- jsonwebtoken: Token signing/verification\n- ./user-repository: User data access\n\n## Implementation Notes\nTokens expire after 24 hours. Refresh handled by client.\n```\n\n### `AGENTS.md` (Per Directory)\n\nDirectory overview with:\n\n- Description of the directory's role\n- Files grouped by purpose (Types, Services, Utils, etc.)\n- Subdirectories with brief descriptions\n\n### Pointer Files\n\n- **`CLAUDE.md`** — Imports `AGENTS.md` for Claude Code (auto-loaded per directory)\n\n### Root Overview\n\n- **`AGENTS.md`** — Root directory overview (universal format)\n\n---\n\n## Configuration\n\nEdit `.agents-reverse-engineer/config.yaml`:\n\n```yaml\n# File and directory exclusions\nexclude:\n  patterns: []              # Custom glob patterns (e.g., [\"*.log\", \"temp/**\"])\n  vendorDirs:               # Directories to skip\n    - node_modules\n    - dist\n    - .git\n  binaryExtensions:         # File types to skip\n    - .png\n    - .jpg\n    - .pdf\n\n# Discovery options\noptions:\n  followSymlinks: false     # Follow symbolic links during traversal\n  maxFileSize: 1048576      # Max file size in bytes (1MB default)\n\n# Output formatting\noutput:\n  colors: true              # Use colors in terminal output\n\n# AI service configuration\nai:\n  backend: auto             # Backend: 'claude', 'codex', 'gemini', 'opencode', 'auto'\n  model: sonnet             # Model identifier (backend-specific)\n  timeoutMs: 300000         # Subprocess timeout in ms (5 minutes)\n  maxRetries: 3             # Max retries for transient errors\n  concurrency: 10           # Parallel AI calls (1-20, auto-detected from CPU/RAM)\n\n  telemetry:\n    keepRuns: 50            # Number of run logs to keep\n```\n\n### Key Config Options\n\n**Concurrency (`ai.concurrency`)**\n- Default: Auto-detected from CPU cores and available memory\n- Range: `1-20`\n- Lower values recommended for resource-constrained environments\n- Higher values speed up generation but use more memory\n\n**Timeout (`ai.timeoutMs`)**\n- Default: `300000` (5 minutes)\n- AI subprocess timeout for each file analysis\n- Increase for very large files or slow connections\n\n---\n\n## Requirements\n\n- **Node.js 18+**\n- **AI Coding Assistant** — One of:\n  - [Claude Code](https://claude.ai/claude-code) (full support)\n  - [Gemini CLI](https://github.com/google-gemini/gemini-cli) (full support)\n  - [OpenCode](https://github.com/opencode-ai/opencode) (AGENTS.md supported)\n  - Any assistant supporting `AGENTS.md` format\n\n---\n\n## Contributing\n\nContributions are welcome! Whether it's bug reports, feature requests, or pull requests — all input is valued.\n\nSee [CONTRIBUTING.md](CONTRIBUTING.md) for development setup and guidelines.\n\n---\n\n## License\n\nMIT\n\n```\n\nLead with a single bold purpose statement: **[FileName] does X.**\nThen use ## headings to organize the remaining content.\nEvery file MUST include at minimum:\n- A purpose statement (first line, bold)\n- Exported symbols with signatures (under any appropriate heading)\nChoose additional sections based on file content.",
      "systemPrompt": "You are analyzing source code to generate documentation for AI coding assistants.\n\nTASK:\nAnalyze the file and produce a dense, identifier-rich summary. Choose the documentation topics most relevant to THIS specific file. Do not follow a fixed template — adapt your sections to what matters most.\n\nConsider topics such as (choose what applies):\n- What this file IS (its role in the project)\n- Public interface: exported functions, classes, types, constants with signatures\n- Key algorithms, data structures, or state management\n- Integration points and coupling with other modules\n- Configuration, environment, or runtime requirements\n- Error handling strategies or validation boundaries\n- Concurrency, lifecycle, or resource management concerns\n- Domain-specific patterns (middleware chains, event handlers, schema definitions, factories)\n- Behavioral contracts: verbatim regex patterns, format strings, output templates, magic constants, sentinel values, error code strings, environment variable names\n- Workflow & convention rules: if this file defines contribution guidelines, PR conventions, commit standards, testing mandates, tool usage requirements, approval workflows, AI agent behavioral instructions, or code conventions (naming standards, formatting rules, import ordering, linting policies), extract them as explicit, actionable rules. Sources: CONTRIBUTING.md, CI configs, PR templates, style guides, linter/formatter configs (.eslintrc, .prettierrc, .editorconfig), tsconfig strictness, README dev sections.\n\n\nTARGET LENGTH (MANDATORY):\n- Source file: 12587 characters\n- Target summary: ~1259 characters (10% compression)\n- Maximum: 1511 characters\n- Achieve this by: ultra-dense writing, minimal examples, single-sentence descriptions\n\nULTRA-COMPRESSION TECHNIQUES:\n- Use telegraphic style: \"exports foo, bar, baz\" not \"this file exports three functions\"\n- No code examples unless critical to understanding\n- One sentence per export maximum\n- Omit obvious parameter names (\"opts: Options\" not \"options: Options - configuration object\")\n- Use abbreviations: \"params\" not \"parameters\", \"config\" not \"configuration\"\n- Skip section if empty (don't write \"No special concerns\" or \"N/A\")\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this file\", \"this module\", \"provides\", \"responsible for\", \"is used to\", \"basically\", \"essentially\", \"provides functionality for\"\n- Use the pattern: \"[ExportName] does X\" not \"The ExportName function is responsible for doing X\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Compress descriptions: \"parses YAML frontmatter from .sum files\" not \"responsible for the parsing of YAML-style frontmatter...\"\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- All exported function/class/type/const names MUST appear in the summary exactly as written in source\n- Key parameter types and return types MUST be mentioned\n- Preserve exact casing of identifiers (e.g., buildAgentsMd, not \"build agents md\")\n- Missing any exported identifier is a failure\n\nWHAT TO INCLUDE:\n- All exported function/class/type/const names\n- Parameter types and return types for public functions\n- Key dependencies and what they're used for\n- Notable design patterns (name them explicitly: \"Strategy pattern\", \"Builder pattern\", etc.)\n- Only critical TODOs (security, breaking issues)\n\nWHAT TO EXCLUDE:\n- Control flow minutiae (loop structures, variable naming, temporary state)\n- Generic descriptions without identifiers\n- Filler phrases and transitions\n\nBEHAVIORAL CONTRACTS (NEVER EXCLUDE):\n- Regex patterns for parsing/validation/extraction — include the full pattern verbatim in backticks\n- Format strings, output templates, serialization structures — show exact format\n- Magic constants, sentinel values, numeric thresholds (timeouts, buffer sizes, retry counts)\n- Prompt text or template strings that control AI/LLM behavior\n- Error message patterns and error code strings used for matching\n- Environment variable names and their expected values\n- File format specifications (YAML frontmatter schemas, NDJSON line formats)\nThese define observable behavior that must be reproduced exactly.\n\nWORKFLOW & CONVENTION RULES (NEVER EXCLUDE):\n- Contribution requirements: branch naming, commit message formats, PR description templates\n- Testing mandates: required test types, coverage thresholds, \"never delete tests\" policies\n- Tool usage rules: required CLI commands, browser automation tools, linter/formatter requirements\n- Approval workflows: review requirements, CI checks that must pass, deployment gates\n- AI agent instructions: explicit directives for automated assistants (coding rules, prohibited actions)\n- Code conventions: naming standards (camelCase, PascalCase, snake_case for what), import ordering rules, file/folder naming patterns, formatting rules (indentation, quotes, semicolons), language idioms, TypeScript strictness policies, preferred patterns vs anti-patterns\nThese define how contributors (human or AI) must work within the project.\n\nREPRODUCTION-CRITICAL CONTENT (ANNEX OVERFLOW):\nSome files exist primarily to define large string constants, prompt templates,\nconfiguration arrays, default value sets, or command/IDE template content.\nFor these files:\n- Write a CONCISE summary following the standard density and length rules\n- List each constant/export by name with a one-line description of its role\n- Do NOT attempt to reproduce multi-line string constants verbatim in the summary\n- Instead, end the summary with a dedicated ## Annex References section listing\n  each reproduction-critical constant:\n\n  ## Annex References\n  - `FILE_SYSTEM_PROMPT` — system prompt for file analysis (250 lines)\n  - `DIRECTORY_SYSTEM_PROMPT` — system prompt for AGENTS.md generation (150 lines)\n\n  The pipeline will extract the actual constant values from source code and write\n  them to a companion .annex.sum file automatically. Your job is to IDENTIFY which\n  constants are reproduction-critical, not to reproduce them inline.\n\nFor files that are primarily logic (functions, classes, algorithms), ignore this\nsection — it does not apply.\n\nOUTPUT FORMAT (MANDATORY):\n- Start your response DIRECTLY with the purpose statement — a single bold line: **Purpose statement here.**\n- Do NOT include any preamble, thinking, or meta-commentary before the purpose statement\n- Do NOT say \"Here is...\", \"Now I'll...\", \"Based on my analysis...\", \"Let me create...\", \"Perfect.\"\n- Your response IS the documentation — not a message about the documentation",
      "response": "**README.md documents the agents-reverse-engineer (ARE) CLI tool, which generates AI-friendly `.sum` files and `AGENTS.md` documentation from codebases.**\n\n## Public Commands\n\nCLI: `are install`, `are init`, `are discover`, `are generate`, `are update`, `are specify`, `are rebuild`, `are clean`, `are uninstall`, `are --version`.\n\nAI Assistant: `/are-init`, `/are-discover`, `/are-generate`, `/are-update`, `/are-specify`, `/are-rebuild`, `/are-clean`.\n\n## Key Runtime Targets\n\nInstalls for Claude Code, Codex, OpenCode, Gemini CLI, or all runtimes. Supports global (`~/.claude/`, `~/.agents/`) or local (`./.claude/`, `./.agents/`) placement via `--runtime` and `-g`/`-l` flags.\n\n## Configuration Schema\n\n`.agents-reverse-engineer/config.yaml` controls exclusions (`patterns`, `vendorDirs`, `binaryExtensions`), discovery options (`followSymlinks`, `maxFileSize: 1048576`), output formatting (`colors: true`), and AI service settings (`backend: auto`, `model: sonnet`, `timeoutMs: 300000`, `maxRetries: 3`, `concurrency: 10`, `telemetry.keepRuns: 50`).\n\n## CLI Flags\n\nGeneral: `--model <name>`, `--backend <name>`, `--concurrency <n>`, `--dry-run`, `--force`, `--fail-fast`, `--debug`, `--trace`, `--eval`. Command-specific: `--show-excluded` (discover), `--uncommitted` (update).\n\n## Output Artifacts\n\n`.sum` files per source file with `file_type`, `generated_at`, purpose, public interface, dependencies; `AGENTS.md` per directory with file groupings; `CLAUDE.md` pointer files; root `AGENTS.md` project overview; optional `specs/SPEC.md` from `/are-specify`.\n\n## Workflow: Post-Order Traversal\n\n`/are-discover` scans codebase respecting `.gitignore`, generates `GENERATION-PLAN.md` with post-order traversal (deepest directories first). `/are-generate` executes plan creating `.sum` files then `AGENTS.md`. `/are-update` regenerates only changed files. `/are-specify --multi-file` splits spec into separate files; `--dry-run` previews without AI calls.\n\n## Requirements\n\nNode.js 18+. Supports Claude Code, Gemini CLI, OpenCode, or any `AGENTS.md`-compatible assistant.",
      "model": "claude-haiku-4-5-20251001",
      "inputTokens": 3,
      "outputTokens": 624,
      "cacheReadTokens": 0,
      "cacheCreationTokens": 22333,
      "latencyMs": 21277,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": [
        {
          "path": "README.md",
          "sizeBytes": 12622
        }
      ]
    },
    {
      "timestamp": "2026-02-14T13:48:57.989Z",
      "prompt": "Generate AGENTS.md for directory: \"src/quality/phantom-paths\" (phantom-paths)\n\n## File Summaries (2 files)\n\n### index.ts\n**Purpose:** index.ts re-exports checkPhantomPaths validator for phantom path detection.\n\n**index.ts re-exports checkPhantomPaths validator for phantom path detection.**\n\n## Exports\n\n- `checkPhantomPaths` — validator function from './validator.js'\n### validator.ts\n**Purpose:** validator.ts detects invalid path references in AGENTS.md documentation files by matching regex patterns against cont...\n\n**validator.ts detects invalid path references in AGENTS.md documentation files by matching regex patterns against content and verifying file existence.**\n\n## Exports\n\n`checkPhantomPaths(agentsMdPath: string, content: string, projectRoot: string): PhantomPathInconsistency[]` — extracts path-like strings from AGENTS.md using regex patterns, resolves them relative to file location or project root, and returns array of unresolved paths with context.\n\n## Path Detection Patterns\n\n`PATH_PATTERNS` — three RegExp instances matching: markdown links `\\[(?:[^\\]]*)\\]\\((\\.[^)]+)\\)/g`, backtick-quoted paths `` /`((?:src\\/|\\.\\.?\\/)[^`]+\\.[a-z]{1,4})`/g ``, and prose references `/(?:from|in|by|via|see)\\s+`?(src\\/[\\w\\-./]+)`?/gi`.\n\n## Validation Rules\n\n`SKIP_PATTERNS` — six RegExp instances excluding node_modules, .git/, URLs starting with `https?:`, template placeholders `{{`, template literals `${`, glob wildcards `*`, and brace expansion `{a,b,c}` from validation.\n\n## Resolution Strategy\n\nPaths resolved in order: (1) relative to AGENTS.md directory, (2) relative to projectRoot for src/ references, (3) with .js→.ts substitution for TypeScript import conventions. Deduplicates via `seen` Set to avoid duplicate issue reports.\n\n## Issue Structure\n\nReturns `PhantomPathInconsistency[]` objects with type `'phantom-path'`, severity `'warning'`, relative agentsMdPath, rawPath that failed resolution, absolute resolvedTo path, and 120-char context line snippet.\n\n## Import Map (verified — use these exact paths)\n\nvalidator.ts:\n  ../types.js → PhantomPathInconsistency (type)\n\n## Project Directory Structure\n\n<project-structure>\n./\n  .npmrc\n  CONTRIBUTING.md\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  pages.yml\n  publish.yml\nhooks/\n  are-check-update.js\n  are-context-loader.js\n  opencode-are-check-update.js\nscripts/\n  analyze-agents-compression.sh\n  analyze-sum-ratios.sh\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  codex.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/providers/\n  subprocess.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  rebuild.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/core/\n  index.ts\n  logger.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  claude-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  orchestrator.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/rebuild/\n  checkpoint.ts\n  index.ts\n  orchestrator.ts\n  output-parser.ts\n  prompts.ts\n  spec-reader.ts\n  types.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>",
      "systemPrompt": "You are generating an AGENTS.md file — a directory-level overview for AI coding assistants.\n\nCRITICAL: Output ONLY the raw markdown content. No code fences, no preamble, no explanations, no conversational text. Your entire response IS the AGENTS.md file content.\n\nOUTPUT FORMAT:\n- First line MUST be exactly: <!-- Generated by agents-reverse-engineer -->\n- Use a # heading with the directory name\n- Write a one-paragraph purpose statement for the directory\nusage\nADAPTIVE SECTIONS:\nAnalyze the directory contents and choose the most relevant sections. Do NOT use a fixed template. Instead, select sections that best document this specific directory for an AI that needs to reconstruct or extend the project.\n\nConsider these section types (choose what applies):\n- **Contents**: Group files by purpose/category under ## headings. For each file: markdown link [filename](./filename) and a one-line description.\n- **Subdirectories**: If subdirectories exist, list them with links [dirname/](./dirname/) and brief summaries.\n- **Architecture / Data Flow**: If files form a pipeline, request/response chain, or layered architecture, document it.\n- **Stack**: If this is a package root (has package.json, Cargo.toml, go.mod, etc.), document the technology stack, key scripts, and entry points.\n- **Structure**: If the directory layout follows a convention (feature-sliced, domain-driven, MVC, etc.), document it.\n- **Patterns**: If files share recurring design patterns (factory, strategy, middleware, barrel re-export), name and document them.\n- **Configuration**: If the directory contains config files, schemas, or environment definitions, document the config surface area.\n- **API Surface**: If the directory exports a public API (barrel index, route definitions, SDK), document the interface contract.\n- **File Relationships**: How files collaborate, depend on each other, or share state.\n- **Behavioral Contracts**: If files contain regex patterns, format specifications, magic constants, or template strings that define observable behavior, collect them in a dedicated section. Preserve verbatim patterns from file summaries — do NOT paraphrase regex into prose. This section is MANDATORY when file summaries contain behavioral artifacts.\n- **Reproduction-Critical Constants**: If file summaries reference annex files (via ## Annex References sections), list them with links. Example: \"Full prompt template text: [templates.annex.sum](./prompts/templates.annex.sum)\". Do NOT reproduce annex content in AGENTS.md — just link to it.\n- **Workflow & Conventions**: If file summaries contain contribution guidelines, PR conventions, commit standards, testing mandates, tool usage rules, AI agent instructions, or code conventions (naming, formatting, linting), collect them into actionable directives. Source from CONTRIBUTING.md, CI configs, PR templates, linter/formatter configs, README dev sections. Present as concrete rules (\"commits must use conventionalcommits format\", \"use camelCase for functions, PascalCase for types\") not vague descriptions (\"the project has guidelines\").\n\nChoose any relevant sections or create your own based on the directory contents. The goal is to provide a comprehensive overview that captures the essence of the directory's role in the project and how its files work together, with a focus on what an AI coding assistant would need to know to effectively interact with this code.\n\nSCOPE:\n- AGENTS.md is a NAVIGATIONAL INDEX — help an AI find the right file quickly\n- Focus on: what each file does, how files relate, directory-level patterns\n- Do NOT reproduce full architecture sections — those belong in the root CLAUDE.md\n\nPATH ACCURACY (MANDATORY):\n- When referencing files or modules outside this directory, use ONLY paths from the \"Import Map\" section\n- Do NOT invent, rename, or guess module paths — if a path isn't in the Import Map, don't reference it\n- Use the exact directory names from \"Project Directory Structure\" — do NOT rename directories\n  (e.g., if the directory is called \"cli\", write \"src/cli/\", NOT \"src/commands/\")\n- Cross-module references must use the specifier format from actual import statements\n  (e.g., \"../generation/writers/sum.js\", NOT \"../fs/sum-file.js\")\n- If you are unsure about a path, omit the cross-reference rather than guessing\n\nCONSISTENCY (MANDATORY):\n- Do not contradict yourself within the same document\n- If you describe a technique (e.g., \"regex-based\"), do not call it something else later (e.g., \"AST-based\")\n- When stating version numbers, engines, or config fields, use ONLY values present in the file summaries\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this directory\", \"this module\", \"provides\", \"responsible for\", \"is used to\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Per-file descriptions in Contents sections: 1-2 sentences maximum. Reference key symbols but do not reproduce full summaries.\n- Behavioral contracts (regex patterns, format specs, constants) belong in a separate Behavioral Contracts section, not in per-file descriptions.\n- Annex file references: link to .annex.sum files, do not inline their content. One line per annex reference.\n- Subdirectory descriptions: 1-2 sentences maximum. Capture the directory's role, not its full contents.\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- Key exported symbols from file summaries MUST appear in the directory overview\n- Preserve exact casing of identifiers\n\nUSER NOTES:\n- If \"User Notes\" are provided in the prompt, they contain user-defined instructions that will be automatically prepended to your output\n- Do NOT repeat or paraphrase user notes in your generated content — they are included separately\n- You may reference information from user notes for context",
      "response": "<!-- Generated by agents-reverse-engineer -->\n\n# src/quality/phantom-paths\n\nDetects and reports invalid path references in AGENTS.md documentation by matching three classes of path patterns (markdown links, backtick-quoted paths, prose references) and verifying file existence via regex-based extraction and filesystem resolution.\n\n## Contents\n\n- [index.ts](./index.ts) — Re-exports `checkPhantomPaths` validator.\n- [validator.ts](./validator.ts) — Implements `checkPhantomPaths(agentsMdPath, content, projectRoot)` core validation logic.\n\n## Exports\n\n`checkPhantomPaths(agentsMdPath: string, content: string, projectRoot: string)` returns `PhantomPathInconsistency[]` array.\n\n## Path Detection Patterns\n\nThree `RegExp` instances in `PATH_PATTERNS` match:\n\n- Markdown links: `\\[(?:[^\\]]*)\\]\\((\\.[^)]+)\\)/g`\n- Backtick-quoted paths: `` /`((?:src\\/|\\.\\.?\\/)[^`]+\\.[a-z]{1,4})`/g ``\n- Prose references (from/in/by/via/see clauses): `/(?:from|in|by|via|see)\\s+`?(src\\/[\\w\\-./]+)`?/gi`\n\n## Validation Rules\n\n`SKIP_PATTERNS` — six RegExp instances reject node_modules, .git/, `https?:` URLs, template placeholders `{{`, template literals `${`, glob wildcards `*`, and brace expansion syntax `{a,b,c}`.\n\n## Resolution Strategy\n\nPaths resolved in sequence: (1) relative to AGENTS.md directory, (2) relative to `projectRoot` for src/ references, (3) with `.js`→`.ts` substitution for TypeScript import conventions. Deduplicates via `seen` Set.\n\n## Issue Structure\n\n`PhantomPathInconsistency` objects contain `type: 'phantom-path'`, `severity: 'warning'`, relative `agentsMdPath`, unresolved `rawPath`, absolute `resolvedTo` filesystem path, and 120-character context snippet.",
      "model": "claude-haiku-4-5-20251001",
      "inputTokens": 3,
      "outputTokens": 519,
      "cacheReadTokens": 17512,
      "cacheCreationTokens": 2505,
      "latencyMs": 14899,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": []
    },
    {
      "timestamp": "2026-02-14T13:48:57.993Z",
      "prompt": "Generate AGENTS.md for directory: \"src/ai/telemetry\" (telemetry)\n\n## File Summaries (3 files)\n\n### cleanup.ts\n**Purpose:** cleanup.ts removes old telemetry log files from `.agents-reverse-engineer/logs/`, retaining only the N most recent.\n\n**cleanup.ts removes old telemetry log files from `.agents-reverse-engineer/logs/`, retaining only the N most recent.**\n\n## Exports\n\n`cleanupOldLogs(projectRoot: string, keepCount: number): Promise<number>` — deletes log files matching `run-*.json` pattern beyond `keepCount` threshold, returns count of deleted files; returns 0 if logs directory missing.\n\n## Constants\n\n`LOGS_DIR = '.agents-reverse-engineer/logs'` — relative path to telemetry logs directory from project root.\n\n## Behavior\n\n- Filters entries by pattern `run-*.json`\n- Sorts lexicographically (newest-first via reverse); ISO timestamp filenames ensure correct chronological ordering\n- Slices to `keepCount` most recent, deletes remainder\n- Handles `ENOENT` gracefully (returns 0); re-throws other errors\n### logger.ts\n**Purpose:** TelemetryLogger accumulates per-call telemetry data during CLI runs and produces aggregate RunLog summaries.\n\n**TelemetryLogger accumulates per-call telemetry data during CLI runs and produces aggregate RunLog summaries.**\n\n## Exports\n\n`TelemetryLogger` class: in-memory accumulator initialized with `runId: string, backend: string, model: string, command: string`. Public properties: `runId`, `startTime` (ISO 8601), `backend`, `model`, `command` (all readonly). Methods:\n- `addEntry(entry: TelemetryEntry): void` — appends call telemetry\n- `getEntries(): readonly TelemetryEntry[]` — returns immutable entry array\n- `setFilesReadOnLastEntry(filesRead: FileRead[]): void` — patches final entry's filesRead\n- `getSummary(): RunLog['summary']` — computes aggregates (totalCalls, totalInputTokens, totalOutputTokens, totalCacheReadTokens, totalCacheCreationTokens, totalDurationMs, errorCount, totalFilesRead, uniqueFilesRead)\n- `toRunLog(): RunLog` — finalizes run with `endTime`, entries array, summary; call once at run completion\n\n## Design\n\nSingle-instance logger per CLI invocation. Entries accumulated as TelemetryEntry objects; summaries computed fresh on each `getSummary()` call (not cached). File tracking via `setFilesReadOnLastEntry()` deferred until after command runner execution. `toRunLog()` snapshot captures current ISO time as `endTime`.\n\n## Dependencies\n\nImports `TelemetryEntry, RunLog, FileRead` from `../types.js`.\n### run-log.ts\n**Purpose:** writeRunLog writes completed RunLog telemetry to disk as pretty-printed JSON in .agents-reverse-engineer/logs/ with s...\n\n**writeRunLog writes completed RunLog telemetry to disk as pretty-printed JSON in .agents-reverse-engineer/logs/ with sanitized filenames.**\n\n## Exports\n\n`writeRunLog(projectRoot: string, runLog: RunLog): Promise<string>` — creates logs directory if missing, generates filename from runLog.command, runLog.backend, runLog.model, runLog.startTime with `:` and `.` replaced by `-`, writes JSON with 2-space indentation, returns absolute file path.\n\n## Constants\n\n`LOGS_DIR = '.agents-reverse-engineer/logs'` — telemetry log directory relative to project root.\n\n## Dependencies\n\n`RunLog` type imported from `../types.js`; uses `node:fs/promises` (mkdir, writeFile) and `node:path` (join).\n\n## Behavioral Contracts\n\nFilename format: `run-{safeCommand}-{safeBackend}-{safeModel}-{safeTimestamp}.json` where safeCommand/backend/model are lowercase with `[^a-z0-9]` replaced by `-`, safeTimestamp is ISO string with `[:.]/g` replaced by `-`. JSON serialized with `null` replacer and `2` space indent. Example output: `run-generate-claude-sonnet-2026-02-07T12-00-00-000Z.json`.\n\n## Import Map (verified — use these exact paths)\n\nlogger.ts:\n  ../types.js → TelemetryEntry, RunLog, FileRead (type)\n\nrun-log.ts:\n  ../types.js → RunLog (type)\n\n## Project Directory Structure\n\n<project-structure>\n./\n  .npmrc\n  CONTRIBUTING.md\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  pages.yml\n  publish.yml\nhooks/\n  are-check-update.js\n  are-context-loader.js\n  opencode-are-check-update.js\nscripts/\n  analyze-agents-compression.sh\n  analyze-sum-ratios.sh\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  codex.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/providers/\n  subprocess.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  rebuild.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/core/\n  index.ts\n  logger.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  claude-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  orchestrator.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/rebuild/\n  checkpoint.ts\n  index.ts\n  orchestrator.ts\n  output-parser.ts\n  prompts.ts\n  spec-reader.ts\n  types.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>",
      "systemPrompt": "You are generating an AGENTS.md file — a directory-level overview for AI coding assistants.\n\nCRITICAL: Output ONLY the raw markdown content. No code fences, no preamble, no explanations, no conversational text. Your entire response IS the AGENTS.md file content.\n\nOUTPUT FORMAT:\n- First line MUST be exactly: <!-- Generated by agents-reverse-engineer -->\n- Use a # heading with the directory name\n- Write a one-paragraph purpose statement for the directory\nusage\nADAPTIVE SECTIONS:\nAnalyze the directory contents and choose the most relevant sections. Do NOT use a fixed template. Instead, select sections that best document this specific directory for an AI that needs to reconstruct or extend the project.\n\nConsider these section types (choose what applies):\n- **Contents**: Group files by purpose/category under ## headings. For each file: markdown link [filename](./filename) and a one-line description.\n- **Subdirectories**: If subdirectories exist, list them with links [dirname/](./dirname/) and brief summaries.\n- **Architecture / Data Flow**: If files form a pipeline, request/response chain, or layered architecture, document it.\n- **Stack**: If this is a package root (has package.json, Cargo.toml, go.mod, etc.), document the technology stack, key scripts, and entry points.\n- **Structure**: If the directory layout follows a convention (feature-sliced, domain-driven, MVC, etc.), document it.\n- **Patterns**: If files share recurring design patterns (factory, strategy, middleware, barrel re-export), name and document them.\n- **Configuration**: If the directory contains config files, schemas, or environment definitions, document the config surface area.\n- **API Surface**: If the directory exports a public API (barrel index, route definitions, SDK), document the interface contract.\n- **File Relationships**: How files collaborate, depend on each other, or share state.\n- **Behavioral Contracts**: If files contain regex patterns, format specifications, magic constants, or template strings that define observable behavior, collect them in a dedicated section. Preserve verbatim patterns from file summaries — do NOT paraphrase regex into prose. This section is MANDATORY when file summaries contain behavioral artifacts.\n- **Reproduction-Critical Constants**: If file summaries reference annex files (via ## Annex References sections), list them with links. Example: \"Full prompt template text: [templates.annex.sum](./prompts/templates.annex.sum)\". Do NOT reproduce annex content in AGENTS.md — just link to it.\n- **Workflow & Conventions**: If file summaries contain contribution guidelines, PR conventions, commit standards, testing mandates, tool usage rules, AI agent instructions, or code conventions (naming, formatting, linting), collect them into actionable directives. Source from CONTRIBUTING.md, CI configs, PR templates, linter/formatter configs, README dev sections. Present as concrete rules (\"commits must use conventionalcommits format\", \"use camelCase for functions, PascalCase for types\") not vague descriptions (\"the project has guidelines\").\n\nChoose any relevant sections or create your own based on the directory contents. The goal is to provide a comprehensive overview that captures the essence of the directory's role in the project and how its files work together, with a focus on what an AI coding assistant would need to know to effectively interact with this code.\n\nSCOPE:\n- AGENTS.md is a NAVIGATIONAL INDEX — help an AI find the right file quickly\n- Focus on: what each file does, how files relate, directory-level patterns\n- Do NOT reproduce full architecture sections — those belong in the root CLAUDE.md\n\nPATH ACCURACY (MANDATORY):\n- When referencing files or modules outside this directory, use ONLY paths from the \"Import Map\" section\n- Do NOT invent, rename, or guess module paths — if a path isn't in the Import Map, don't reference it\n- Use the exact directory names from \"Project Directory Structure\" — do NOT rename directories\n  (e.g., if the directory is called \"cli\", write \"src/cli/\", NOT \"src/commands/\")\n- Cross-module references must use the specifier format from actual import statements\n  (e.g., \"../generation/writers/sum.js\", NOT \"../fs/sum-file.js\")\n- If you are unsure about a path, omit the cross-reference rather than guessing\n\nCONSISTENCY (MANDATORY):\n- Do not contradict yourself within the same document\n- If you describe a technique (e.g., \"regex-based\"), do not call it something else later (e.g., \"AST-based\")\n- When stating version numbers, engines, or config fields, use ONLY values present in the file summaries\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this directory\", \"this module\", \"provides\", \"responsible for\", \"is used to\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Per-file descriptions in Contents sections: 1-2 sentences maximum. Reference key symbols but do not reproduce full summaries.\n- Behavioral contracts (regex patterns, format specs, constants) belong in a separate Behavioral Contracts section, not in per-file descriptions.\n- Annex file references: link to .annex.sum files, do not inline their content. One line per annex reference.\n- Subdirectory descriptions: 1-2 sentences maximum. Capture the directory's role, not its full contents.\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- Key exported symbols from file summaries MUST appear in the directory overview\n- Preserve exact casing of identifiers\n\nUSER NOTES:\n- If \"User Notes\" are provided in the prompt, they contain user-defined instructions that will be automatically prepended to your output\n- Do NOT repeat or paraphrase user notes in your generated content — they are included separately\n- You may reference information from user notes for context",
      "response": "<!-- Generated by agents-reverse-engineer -->\n# src/ai/telemetry\n\nAccumulates and persists telemetry data from AI service calls during CLI execution, providing cleanup and storage for run logs with aggregated metrics.\n\n## Contents\n\n**Telemetry accumulation:**\n- [logger.ts](./logger.ts) — `TelemetryLogger` class accumulates `TelemetryEntry` objects and computes aggregate `RunLog` summaries (token counts, durations, error counts, file tracking).\n\n**Telemetry persistence:**\n- [run-log.ts](./run-log.ts) — `writeRunLog()` serializes completed `RunLog` to disk as pretty-printed JSON in `.agents-reverse-engineer/logs/` with sanitized filenames.\n\n**Telemetry maintenance:**\n- [cleanup.ts](./cleanup.ts) — `cleanupOldLogs()` removes old `run-*.json` log files, retaining only the N most recent by lexicographic (ISO timestamp) ordering.\n\n## Data Flow\n\n1. During CLI execution: `TelemetryLogger` instance accumulates calls via `addEntry(entry: TelemetryEntry)`\n2. File reads deferred: `setFilesReadOnLastEntry(filesRead: FileRead[])` patches final entry after command completion\n3. Run completion: `toRunLog()` snapshots current state with `endTime` and aggregates via `getSummary()`\n4. Persistence: `writeRunLog()` writes `RunLog` JSON to disk with `startTime`-derived filename\n5. Maintenance: `cleanupOldLogs()` prunes logs directory to retain only `keepCount` most recent runs\n\n## Behavioral Contracts\n\n**Filename sanitization** (`run-log.ts`): Replaces non-alphanumeric characters in command, backend, model with `-`, and `:` / `.` in ISO timestamp with `-`. Pattern: `run-{safeCommand}-{safeBackend}-{safeModel}-{safeTimestamp}.json`. Example: `run-generate-claude-sonnet-2026-02-07T12-00-00-000Z.json`.\n\n**Log retention** (`cleanup.ts`): Filters entries by glob `run-*.json`, sorts lexicographically (newest first via reverse), slices to `keepCount`, deletes remainder. ISO timestamp filenames guarantee correct chronological ordering.\n\n**Aggregation** (`logger.ts`): `getSummary()` computes fresh aggregates on each call (not cached): `totalCalls`, `totalInputTokens`, `totalOutputTokens`, `totalCacheReadTokens`, `totalCacheCreationTokens`, `totalDurationMs`, `errorCount`, `totalFilesRead`, `uniqueFilesRead`.\n\n## Configuration\n\n- `LOGS_DIR` constant: `.agents-reverse-engineer/logs` — relative path from project root for all telemetry operations.\n- Directory auto-created by `writeRunLog()` if missing; `cleanupOldLogs()` returns 0 if directory absent (ENOENT handled gracefully).",
      "model": "claude-haiku-4-5-20251001",
      "inputTokens": 3,
      "outputTokens": 721,
      "cacheReadTokens": 17512,
      "cacheCreationTokens": 3016,
      "latencyMs": 17825,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": []
    },
    {
      "timestamp": "2026-02-14T13:48:57.984Z",
      "prompt": "Generate AGENTS.md for directory: \"src/ai/providers\" (providers)\n\n## File Summaries (1 files)\n\n### subprocess.ts\n**Purpose:** SubprocessProvider wraps an AIBackend to spawn CLI subprocesses implementing the AIProvider interface, preserving leg...\n\n**SubprocessProvider wraps an AIBackend to spawn CLI subprocesses implementing the AIProvider interface, preserving legacy CLI behavior with timeout enforcement, rate-limit detection, and structured logging.**\n\n## Exports\n\n`SubprocessProvider` — class implementing `AIProvider`; constructor takes `AIBackend` and `SubprocessProviderOptions`, exposes `backend` property, `call(options: AICallOptions): Promise<AIResponse>`, `setTracer(tracer: ITraceWriter): void`, `setSubprocessLogDir(dir: string): void`.\n\n`SubprocessProviderOptions` — interface with `timeoutMs: number` (required), `debug?: boolean`, `logger?: Logger`, `tracer?: ITraceWriter | null`.\n\n## Rate-Limit Detection\n\n`RATE_LIMIT_PATTERNS` array: `['rate limit', '429', 'too many requests', 'overloaded']`. `isRateLimitStderr(stderr: string): boolean` performs case-insensitive substring match against patterns; if matched, `call()` throws `AIServiceError('RATE_LIMIT', ...)`.\n\n## Error Handling\n\n`call()` distinguishes three error modes: timeout (after `timeoutMs` elapsed) throws `AIServiceError('TIMEOUT', 'Subprocess timed out')`; non-zero exit with rate-limit stderr throws `AIServiceError('RATE_LIMIT', ...)`; other non-zero exit throws `AIServiceError('SUBPROCESS_ERROR', ...)`; parse failures throw `AIServiceError('PARSE_ERROR', ...)`. All error messages truncate stderr/stdout to first 200–500 chars.\n\n## Subprocess Invocation\n\n`call()` invokes `runSubprocess(this.backend.cliCommand, args, {timeoutMs, input, onSpawn})` where args come from `backend.buildArgs(options)` and input defaults to `options.prompt` or uses `backend.composeStdinInput(options)`. Emits `subprocess:spawn` trace with `{type, childPid, command, taskLabel}` on spawn and `subprocess:exit` trace with `{type, childPid, command, taskLabel, exitCode, signal, durationMs, timedOut}` on exit.\n\n## Debug & Tracing\n\nWhen `debug: true`, logs pre-spawn memory snapshot (`heapUsed`, `rss` via `formatBytes()`) and post-exit exit code/duration to `logger.debug()`. `tracer` (optional `ITraceWriter`) receives structured subprocess lifecycle events. `activeCount` tracks concurrent subprocesses for memory profiling context.\n\n## Subprocess Output Logging\n\n`setSubprocessLogDir(dir: string)` enables file-based logging. `enqueueSubprocessLog()` writes per-subprocess logs asynchronously with serialized `mkdir`/`writeFile` to prevent concurrent races. Log format: `task`, `pid`, `command`, `exit`, `signal`, `duration`, `timed_out`, then `--- stdout ---` and `--- stderr ---` sections. Failures silently ignored (non-critical).\n\n## Memory & Concurrency\n\n`logWriteQueue: Promise<void>` serializes all log writes via promise chaining; each write awaits `mkdir(dir, {recursive: true})` then `writeFile()`. Sanitizes taskLabel for filenames: replaces `/` with `--`, strips non-alphanumeric except `.`, `_`, `-`.\n\n## Import Map (verified — use these exact paths)\n\nsubprocess.ts:\n  ../types.js → AIProvider, AIBackend, AICallOptions, AIResponse (type)\n  ../types.js → AIServiceError\n  ../subprocess.js → runSubprocess\n  ../types.js → SubprocessResult (type)\n  ../../orchestration/trace.js → ITraceWriter (type)\n  ../../core/logger.js → Logger (type)\n  ../../core/logger.js → nullLogger\n\n## Project Directory Structure\n\n<project-structure>\n./\n  .npmrc\n  CONTRIBUTING.md\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  pages.yml\n  publish.yml\nhooks/\n  are-check-update.js\n  are-context-loader.js\n  opencode-are-check-update.js\nscripts/\n  analyze-agents-compression.sh\n  analyze-sum-ratios.sh\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  codex.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/providers/\n  subprocess.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  rebuild.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/core/\n  index.ts\n  logger.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  claude-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  orchestrator.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/rebuild/\n  checkpoint.ts\n  index.ts\n  orchestrator.ts\n  output-parser.ts\n  prompts.ts\n  spec-reader.ts\n  types.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>",
      "systemPrompt": "You are generating an AGENTS.md file — a directory-level overview for AI coding assistants.\n\nCRITICAL: Output ONLY the raw markdown content. No code fences, no preamble, no explanations, no conversational text. Your entire response IS the AGENTS.md file content.\n\nOUTPUT FORMAT:\n- First line MUST be exactly: <!-- Generated by agents-reverse-engineer -->\n- Use a # heading with the directory name\n- Write a one-paragraph purpose statement for the directory\nusage\nADAPTIVE SECTIONS:\nAnalyze the directory contents and choose the most relevant sections. Do NOT use a fixed template. Instead, select sections that best document this specific directory for an AI that needs to reconstruct or extend the project.\n\nConsider these section types (choose what applies):\n- **Contents**: Group files by purpose/category under ## headings. For each file: markdown link [filename](./filename) and a one-line description.\n- **Subdirectories**: If subdirectories exist, list them with links [dirname/](./dirname/) and brief summaries.\n- **Architecture / Data Flow**: If files form a pipeline, request/response chain, or layered architecture, document it.\n- **Stack**: If this is a package root (has package.json, Cargo.toml, go.mod, etc.), document the technology stack, key scripts, and entry points.\n- **Structure**: If the directory layout follows a convention (feature-sliced, domain-driven, MVC, etc.), document it.\n- **Patterns**: If files share recurring design patterns (factory, strategy, middleware, barrel re-export), name and document them.\n- **Configuration**: If the directory contains config files, schemas, or environment definitions, document the config surface area.\n- **API Surface**: If the directory exports a public API (barrel index, route definitions, SDK), document the interface contract.\n- **File Relationships**: How files collaborate, depend on each other, or share state.\n- **Behavioral Contracts**: If files contain regex patterns, format specifications, magic constants, or template strings that define observable behavior, collect them in a dedicated section. Preserve verbatim patterns from file summaries — do NOT paraphrase regex into prose. This section is MANDATORY when file summaries contain behavioral artifacts.\n- **Reproduction-Critical Constants**: If file summaries reference annex files (via ## Annex References sections), list them with links. Example: \"Full prompt template text: [templates.annex.sum](./prompts/templates.annex.sum)\". Do NOT reproduce annex content in AGENTS.md — just link to it.\n- **Workflow & Conventions**: If file summaries contain contribution guidelines, PR conventions, commit standards, testing mandates, tool usage rules, AI agent instructions, or code conventions (naming, formatting, linting), collect them into actionable directives. Source from CONTRIBUTING.md, CI configs, PR templates, linter/formatter configs, README dev sections. Present as concrete rules (\"commits must use conventionalcommits format\", \"use camelCase for functions, PascalCase for types\") not vague descriptions (\"the project has guidelines\").\n\nChoose any relevant sections or create your own based on the directory contents. The goal is to provide a comprehensive overview that captures the essence of the directory's role in the project and how its files work together, with a focus on what an AI coding assistant would need to know to effectively interact with this code.\n\nSCOPE:\n- AGENTS.md is a NAVIGATIONAL INDEX — help an AI find the right file quickly\n- Focus on: what each file does, how files relate, directory-level patterns\n- Do NOT reproduce full architecture sections — those belong in the root CLAUDE.md\n\nPATH ACCURACY (MANDATORY):\n- When referencing files or modules outside this directory, use ONLY paths from the \"Import Map\" section\n- Do NOT invent, rename, or guess module paths — if a path isn't in the Import Map, don't reference it\n- Use the exact directory names from \"Project Directory Structure\" — do NOT rename directories\n  (e.g., if the directory is called \"cli\", write \"src/cli/\", NOT \"src/commands/\")\n- Cross-module references must use the specifier format from actual import statements\n  (e.g., \"../generation/writers/sum.js\", NOT \"../fs/sum-file.js\")\n- If you are unsure about a path, omit the cross-reference rather than guessing\n\nCONSISTENCY (MANDATORY):\n- Do not contradict yourself within the same document\n- If you describe a technique (e.g., \"regex-based\"), do not call it something else later (e.g., \"AST-based\")\n- When stating version numbers, engines, or config fields, use ONLY values present in the file summaries\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this directory\", \"this module\", \"provides\", \"responsible for\", \"is used to\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Per-file descriptions in Contents sections: 1-2 sentences maximum. Reference key symbols but do not reproduce full summaries.\n- Behavioral contracts (regex patterns, format specs, constants) belong in a separate Behavioral Contracts section, not in per-file descriptions.\n- Annex file references: link to .annex.sum files, do not inline their content. One line per annex reference.\n- Subdirectory descriptions: 1-2 sentences maximum. Capture the directory's role, not its full contents.\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- Key exported symbols from file summaries MUST appear in the directory overview\n- Preserve exact casing of identifiers\n\nUSER NOTES:\n- If \"User Notes\" are provided in the prompt, they contain user-defined instructions that will be automatically prepended to your output\n- Do NOT repeat or paraphrase user notes in your generated content — they are included separately\n- You may reference information from user notes for context",
      "response": "<!-- Generated by agents-reverse-engineer -->\n\n# src/ai/providers\n\nThis directory contains provider implementations that wrap AI backends into a consistent interface. Currently hosts `SubprocessProvider`, which adapts CLI-based backends to the `AIProvider` contract with subprocess lifecycle management, timeout enforcement, and rate-limit detection.\n\n## Contents\n\n- [subprocess.ts](./subprocess.ts) – `SubprocessProvider` class wrapping `AIBackend` instances; spawns subprocess with `call(options: AICallOptions)`, detects rate limits via `isRateLimitStderr()`, enforces timeouts, and writes structured logs asynchronously via `setSubprocessLogDir()`.\n\n## Architecture\n\n`SubprocessProvider` implements `AIProvider` by delegating to `AIBackend` (injected via constructor). The call flow is: `call(AICallOptions)` → build args via `backend.buildArgs()` → invoke `runSubprocess()` with timeout enforcement → parse `SubprocessResult` → throw on error or return `AIResponse`. Rate-limit detection intercepts stderr before error propagation.\n\n## Rate-Limit Detection & Error Handling\n\nRate limits are identified by `RATE_LIMIT_PATTERNS` (case-insensitive substring matches: `'rate limit'`, `'429'`, `'too many requests'`, `'overloaded'`). `isRateLimitStderr()` returns true if any pattern matches; matching triggers `AIServiceError('RATE_LIMIT', ...)`. Non-rate-limit errors throw `AIServiceError` with `'TIMEOUT'`, `'SUBPROCESS_ERROR'`, or `'PARSE_ERROR'` codes. Stderr/stdout truncated to 200–500 characters in error messages.\n\n## Subprocess Lifecycle & Tracing\n\n`call()` emits two structured trace events via optional `tracer: ITraceWriter`:\n- `subprocess:spawn` with `{type, childPid, command, taskLabel}`\n- `subprocess:exit` with `{type, childPid, command, taskLabel, exitCode, signal, durationMs, timedOut}`\n\nDebug mode (`debug: true`) logs heap snapshots (`heapUsed`, `rss`) before spawn and exit code/duration after exit. `activeCount` tracks concurrent subprocess count for memory profiling.\n\n## Logging & File I/O\n\n`setSubprocessLogDir(dir: string)` enables per-subprocess log files. Logs are queued and written serially via `logWriteQueue: Promise<void>` (chained promises prevent concurrent writes). Log format: metadata section (`task`, `pid`, `command`, `exit`, `signal`, `duration`, `timed_out`), then `--- stdout ---` and `--- stderr ---` sections. Task labels are sanitized for filenames (replace `/` with `--`, strip non-alphanumeric except `.`, `_`, `-`). Write failures are silently ignored.\n\n## Configuration\n\n`SubprocessProviderOptions` defines: `timeoutMs: number` (required; passed to `runSubprocess`), `debug?: boolean`, `logger?: Logger`, `tracer?: ITraceWriter | null`. Timeout enforcement is critical for preventing hung subprocesses.",
      "model": "claude-haiku-4-5-20251001",
      "inputTokens": 3,
      "outputTokens": 730,
      "cacheReadTokens": 16147,
      "cacheCreationTokens": 4258,
      "latencyMs": 18537,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": []
    },
    {
      "timestamp": "2026-02-14T13:48:57.996Z",
      "prompt": "Generate AGENTS.md for directory: \"src/quality/inconsistency\" (inconsistency)\n\n## File Summaries (3 files)\n\n### code-vs-code.ts\n**Purpose:** code-vs-code.ts detects duplicate symbol exports across co-located source files via heuristic analysis.\n\n**code-vs-code.ts detects duplicate symbol exports across co-located source files via heuristic analysis.**\n\n## Exports\n\n`checkCodeVsCode(files: Array<{ path: string; content: string }>): CodeCodeInconsistency[]` — scans files for symbols exported from multiple paths, returns inconsistencies with `type: 'code-vs-code'`, `severity: 'warning'`, `pattern: 'duplicate-export'`.\n\n## Dependencies & Integration\n\nCalls `extractExports(content: string)` from `code-vs-doc.js` to parse export names; returns `CodeCodeCodeInconsistency[]` typed from `../types.js`. Caller must scope input per-directory to avoid cross-module false positives.\n\n## Algorithm & Data Structure\n\nBuilds `Map<string, string[]>` tracking export names to file paths, flags entries with `paths.length > 1`; heuristic-only, no AI/LLM calls.\n\n## Behavioral Contract\n\nInconsistency objects: `{type: 'code-vs-code', severity: 'warning', files: paths[], description: 'Symbol \"[name]\" exported from [count] files', pattern: 'duplicate-export'}`.\n### code-vs-doc.ts\n**Purpose:** code-vs-doc.ts detects drift between TypeScript/JavaScript exports and .sum documentation via heuristic symbol matching.\n\n**code-vs-doc.ts detects drift between TypeScript/JavaScript exports and .sum documentation via heuristic symbol matching.**\n\n## Exports\n\n`extractExports(sourceContent: string): string[]` — extracts all named/default export identifiers from source using regex `/^[ \\t]*export\\s+(?:default\\s+)?(?:function|class|const|let|var|type|interface|enum)\\s+(\\w+)/gm`, matching function/class/const/let/var/type/interface/enum declarations, ignoring re-exports and comments.\n\n`checkCodeVsDoc(sourceContent: string, sumContent: SumFileContent, filePath: string): CodeDocInconsistency | null` — compares source exports against SumFileContent.summary text; returns CodeDocInconsistency with type='code-vs-doc', severity='warning', listing missingFromDoc (exports absent from sum text) and empty missingFromCode, or null if consistent.\n\n## Detection Logic\n\nCase-sensitive substring matching: exports in source are flagged if not found via `.includes()` in sumContent.summary. Reports only when missingFromDoc.length > 0; description format is \"Documentation out of sync: N exports undocumented\".\n### reporter.ts\n**Purpose:** reporter.ts builds structured inconsistency reports and formats them as CLI and Markdown output.\n\n**reporter.ts builds structured inconsistency reports and formats them as CLI and Markdown output.**\n\n## Exports\n\n`buildInconsistencyReport(issues: Inconsistency[], metadata: { projectRoot: string; filesChecked: number; durationMs: number }): InconsistencyReport` — aggregates issues into typed report with summary counts (codeVsDoc, codeVsCode, phantomPaths) and metadata (timestamp ISO, projectRoot, filesChecked, durationMs).\n\n`formatReportForCli(report: InconsistencyReport): string` — formats report as plain-text CLI output with severity tags `[ERROR]`, `[WARN]`, `[INFO]` and location info (filePath for code-vs-doc, agentsMdPath+referencedPath for phantom-path, files array for code-vs-code).\n\n`formatReportAsMarkdown(report: InconsistencyReport): string` — formats report as GitHub Markdown table with columns Severity, Type, Description, Location; handles empty issue list.\n\n## Summary Structure\n\nInconsistencyReport contains: `metadata` (timestamp, projectRoot, filesChecked, durationMs), `issues` (Inconsistency[] array), `summary` (total, codeVsDoc, codeVsCode, phantomPaths, errors, warnings, info counts).\n\n## Issue Types & Severity\n\nInconsistency types: `'code-vs-doc'`, `'code-vs-code'`, `'phantom-path'`. Severities: `'error'`, `'warning'`, `'info'`. Each issue carries filePath, description, and type-specific properties (files array for code-vs-code; agentsMdPath + details.referencedPath for phantom-path).\n\n## Import Map (verified — use these exact paths)\n\ncode-vs-code.ts:\n  ../types.js → CodeCodeInconsistency (type)\n\ncode-vs-doc.ts:\n  ../../generation/writers/sum.js → SumFileContent (type)\n  ../types.js → CodeDocInconsistency (type)\n\nreporter.ts:\n  ../types.js → Inconsistency, InconsistencyReport (type)\n\n## Project Directory Structure\n\n<project-structure>\n./\n  .npmrc\n  CONTRIBUTING.md\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  pages.yml\n  publish.yml\nhooks/\n  are-check-update.js\n  are-context-loader.js\n  opencode-are-check-update.js\nscripts/\n  analyze-agents-compression.sh\n  analyze-sum-ratios.sh\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  codex.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/providers/\n  subprocess.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  rebuild.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/core/\n  index.ts\n  logger.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  claude-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  orchestrator.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/rebuild/\n  checkpoint.ts\n  index.ts\n  orchestrator.ts\n  output-parser.ts\n  prompts.ts\n  spec-reader.ts\n  types.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>",
      "systemPrompt": "You are generating an AGENTS.md file — a directory-level overview for AI coding assistants.\n\nCRITICAL: Output ONLY the raw markdown content. No code fences, no preamble, no explanations, no conversational text. Your entire response IS the AGENTS.md file content.\n\nOUTPUT FORMAT:\n- First line MUST be exactly: <!-- Generated by agents-reverse-engineer -->\n- Use a # heading with the directory name\n- Write a one-paragraph purpose statement for the directory\nusage\nADAPTIVE SECTIONS:\nAnalyze the directory contents and choose the most relevant sections. Do NOT use a fixed template. Instead, select sections that best document this specific directory for an AI that needs to reconstruct or extend the project.\n\nConsider these section types (choose what applies):\n- **Contents**: Group files by purpose/category under ## headings. For each file: markdown link [filename](./filename) and a one-line description.\n- **Subdirectories**: If subdirectories exist, list them with links [dirname/](./dirname/) and brief summaries.\n- **Architecture / Data Flow**: If files form a pipeline, request/response chain, or layered architecture, document it.\n- **Stack**: If this is a package root (has package.json, Cargo.toml, go.mod, etc.), document the technology stack, key scripts, and entry points.\n- **Structure**: If the directory layout follows a convention (feature-sliced, domain-driven, MVC, etc.), document it.\n- **Patterns**: If files share recurring design patterns (factory, strategy, middleware, barrel re-export), name and document them.\n- **Configuration**: If the directory contains config files, schemas, or environment definitions, document the config surface area.\n- **API Surface**: If the directory exports a public API (barrel index, route definitions, SDK), document the interface contract.\n- **File Relationships**: How files collaborate, depend on each other, or share state.\n- **Behavioral Contracts**: If files contain regex patterns, format specifications, magic constants, or template strings that define observable behavior, collect them in a dedicated section. Preserve verbatim patterns from file summaries — do NOT paraphrase regex into prose. This section is MANDATORY when file summaries contain behavioral artifacts.\n- **Reproduction-Critical Constants**: If file summaries reference annex files (via ## Annex References sections), list them with links. Example: \"Full prompt template text: [templates.annex.sum](./prompts/templates.annex.sum)\". Do NOT reproduce annex content in AGENTS.md — just link to it.\n- **Workflow & Conventions**: If file summaries contain contribution guidelines, PR conventions, commit standards, testing mandates, tool usage rules, AI agent instructions, or code conventions (naming, formatting, linting), collect them into actionable directives. Source from CONTRIBUTING.md, CI configs, PR templates, linter/formatter configs, README dev sections. Present as concrete rules (\"commits must use conventionalcommits format\", \"use camelCase for functions, PascalCase for types\") not vague descriptions (\"the project has guidelines\").\n\nChoose any relevant sections or create your own based on the directory contents. The goal is to provide a comprehensive overview that captures the essence of the directory's role in the project and how its files work together, with a focus on what an AI coding assistant would need to know to effectively interact with this code.\n\nSCOPE:\n- AGENTS.md is a NAVIGATIONAL INDEX — help an AI find the right file quickly\n- Focus on: what each file does, how files relate, directory-level patterns\n- Do NOT reproduce full architecture sections — those belong in the root CLAUDE.md\n\nPATH ACCURACY (MANDATORY):\n- When referencing files or modules outside this directory, use ONLY paths from the \"Import Map\" section\n- Do NOT invent, rename, or guess module paths — if a path isn't in the Import Map, don't reference it\n- Use the exact directory names from \"Project Directory Structure\" — do NOT rename directories\n  (e.g., if the directory is called \"cli\", write \"src/cli/\", NOT \"src/commands/\")\n- Cross-module references must use the specifier format from actual import statements\n  (e.g., \"../generation/writers/sum.js\", NOT \"../fs/sum-file.js\")\n- If you are unsure about a path, omit the cross-reference rather than guessing\n\nCONSISTENCY (MANDATORY):\n- Do not contradict yourself within the same document\n- If you describe a technique (e.g., \"regex-based\"), do not call it something else later (e.g., \"AST-based\")\n- When stating version numbers, engines, or config fields, use ONLY values present in the file summaries\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this directory\", \"this module\", \"provides\", \"responsible for\", \"is used to\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Per-file descriptions in Contents sections: 1-2 sentences maximum. Reference key symbols but do not reproduce full summaries.\n- Behavioral contracts (regex patterns, format specs, constants) belong in a separate Behavioral Contracts section, not in per-file descriptions.\n- Annex file references: link to .annex.sum files, do not inline their content. One line per annex reference.\n- Subdirectory descriptions: 1-2 sentences maximum. Capture the directory's role, not its full contents.\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- Key exported symbols from file summaries MUST appear in the directory overview\n- Preserve exact casing of identifiers\n\nUSER NOTES:\n- If \"User Notes\" are provided in the prompt, they contain user-defined instructions that will be automatically prepended to your output\n- Do NOT repeat or paraphrase user notes in your generated content — they are included separately\n- You may reference information from user notes for context",
      "response": "<!-- Generated by agents-reverse-engineer -->\n\n# src/quality/inconsistency\n\nDetects and reports inconsistencies between code exports and documentation, and duplicate symbol exports across files, via heuristic analysis and structured reporting.\n\n## Contents\n\n**Export Detection & Comparison**\n\n[code-vs-doc.ts](./code-vs-doc.ts) — Compares TypeScript/JavaScript exports against .sum documentation; exports `extractExports(sourceContent: string): string[]` using regex `/^[ \\t]*export\\s+(?:default\\s+)?(?:function|class|const|let|var|type|interface|enum)\\s+(\\w+)/gm` and `checkCodeVsDoc(sourceContent: string, sumContent: SumFileContent, filePath: string): CodeDocInconsistency | null` for case-sensitive substring matching against `sumContent.summary`.\n\n[code-vs-code.ts](./code-vs-code.ts) — Detects duplicate symbol exports across co-located source files; exports `checkCodeVsCode(files: Array<{ path: string; content: string }>): CodeCodeInconsistency[]` via `Map<string, string[]>` heuristic tracking export names to file paths.\n\n**Reporting & Formatting**\n\n[reporter.ts](./reporter.ts) — Aggregates inconsistencies into typed reports and formats output; exports `buildInconsistencyReport(issues: Inconsistency[], metadata: {...}): InconsistencyReport`, `formatReportForCli(report: InconsistencyReport): string` with severity tags `[ERROR]`, `[WARN]`, `[INFO]`, and `formatReportAsMarkdown(report: InconsistencyReport): string` for GitHub Markdown tables.\n\n## Detection Pipeline\n\n`extractExports()` parses named/default exports from source via regex, matching function/class/const/let/var/type/interface/enum declarations and ignoring re-exports. `checkCodeVsDoc()` flags exports absent from `SumFileContent.summary` text using `.includes()`. `checkCodeVsCode()` builds export-to-paths map and flags entries with `paths.length > 1`. `buildInconsistencyReport()` aggregates results with counts (codeVsDoc, codeVsCode, phantomPaths) and metadata (timestamp ISO, projectRoot, filesChecked, durationMs). CLI and Markdown formatters render severity, type, description, and location (filePath for code-vs-doc; files array for code-vs-code).\n\n## Behavioral Contracts\n\n**Export Regex Pattern**\n```\n/^[ \\t]*export\\s+(?:default\\s+)?(?:function|class|const|let|var|type|interface|enum)\\s+(\\w+)/gm\n```\nMatches leading spaces/tabs, `export` keyword, optional `default`, keyword class (function/class/const/let/var/type/interface/enum), and identifier capture. Re-exports and comments ignored.\n\n**Inconsistency Objects**\n\ncode-vs-doc: `{type: 'code-vs-doc', severity: 'warning', filePath, description: 'Documentation out of sync: N exports undocumented', missingFromDoc: string[], missingFromCode: []}`\n\ncode-vs-code: `{type: 'code-vs-code', severity: 'warning', files: string[], description: 'Symbol \"[name]\" exported from [count] files', pattern: 'duplicate-export'}`\n\nphantom-path: `{type: 'phantom-path', severity: 'error'|'warning'|'info', agentsMdPath, filePath, description, details: {referencedPath, ...}}`\n\n**Report Summary Structure**\n\n`InconsistencyReport` contains `metadata` (timestamp ISO 8601, projectRoot, filesChecked, durationMs), `issues` (Inconsistency[] array), `summary` (total, codeVsDoc, codeVsCode, phantomPaths, errors, warnings, info counts).\n\n## Type Dependencies\n\nImports from `../types.js`: `CodeCodeInconsistency`, `CodeDocInconsistency`, `Inconsistency`, `InconsistencyReport`.\n\nImports from `../../generation/writers/sum.js`: `SumFileContent`.",
      "model": "claude-haiku-4-5-20251001",
      "inputTokens": 3,
      "outputTokens": 1016,
      "cacheReadTokens": 17512,
      "cacheCreationTokens": 3147,
      "latencyMs": 20635,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": []
    },
    {
      "timestamp": "2026-02-14T13:48:58.032Z",
      "prompt": "Generate AGENTS.md for directory: \"src/discovery/filters\" (filters)\n\n## File Summaries (5 files)\n\n### binary.ts\n**Purpose:** binary.ts exports a binary file filter with extension-based fast-path detection and fallback content analysis via isb...\n\n**binary.ts exports a binary file filter with extension-based fast-path detection and fallback content analysis via isbinaryfile.**\n\n## Exports\n\n**BINARY_EXTENSIONS**: `Set<string>` — Pre-computed set of 65 binary file extensions (images, archives, executables, media, documents, fonts, compiled code, databases) for O(1) exclusion without I/O.\n\n**BinaryFilterOptions**: Interface with `maxFileSize?: number` (default 1MB=1048576 bytes) and `additionalExtensions?: string[]` for runtime extension registration.\n\n**createBinaryFilter(options: BinaryFilterOptions): FileFilter** — Returns FileFilter object with `name: 'binary'` and async `shouldExclude(absolutePath): Promise<boolean>`.\n\n## Detection Strategy\n\nTwo-phase approach: (1) fast-path extension check via `path.extname().toLowerCase()` against binaryExtensions Set; (2) slow-path content analysis via `isBinaryFile(absolutePath)` for unknown extensions. Size threshold enforced: `stats.size > maxFileSize` returns true. File I/O errors caught; unreadable files excluded.\n\n## Integration\n\nImplements FileFilter contract from `../types.js`. Depends on `isbinaryfile` npm package for content detection, `node:fs/promises` for stat, `node:path` for extension parsing.\n### custom.ts\n**Purpose:** custom.ts exports createCustomFilter() to build a FileFilter using gitignore-style pattern matching via the ignore li...\n\n**custom.ts exports createCustomFilter() to build a FileFilter using gitignore-style pattern matching via the ignore library.**\n\n## Exports\n\n`createCustomFilter(patterns: string[], root: string): FileFilter` — constructs a FileFilter with `name: 'custom'` and `shouldExclude(absolutePath: string): boolean` method that checks relative paths against gitignore patterns.\n\n## Pattern Matching Behavior\n\nPatterns use gitignore syntax (e.g., `*.log`, `tmp/**`, `secret.txt`). Absolute paths are converted to relative paths via `path.relative()` from the normalized root directory. Paths starting with `..` or empty strings are never excluded. Empty pattern arrays return `false` (pass all files). Pattern matching delegates to `ignore().add(patterns).ignores(relativePath)`.\n\n## Dependencies\n\nImports `ignore` library (Ignore type), Node's `path` module, and `FileFilter` interface from parent types module.\n### gitignore.ts\n**Purpose:** gitignore.ts exports createGitignoreFilter, a function that creates a FileFilter wrapping the ignore library to exclu...\n\n**gitignore.ts exports createGitignoreFilter, a function that creates a FileFilter wrapping the ignore library to exclude paths matching .gitignore patterns.**\n\n## Exports\n\n`createGitignoreFilter(root: string): Promise<FileFilter>` — Async factory returning FileFilter with `name: 'gitignore'` and `shouldExclude(absolutePath: string): boolean` method. Loads .gitignore from root directory; silently continues if file missing.\n\n## Dependencies\n\nUses `ignore` library (type `Ignore`) for pattern matching; `fs.readFile` to load .gitignore content; `path.resolve` and `path.relative` for path normalization.\n\n## Critical Behavior\n\n`shouldExclude()` converts absolutePath to relative path via `path.relative(normalizedRoot, absolutePath)`. Returns false if relativePath is empty or starts with `'..'` (outside root). Ignores library requires relative paths; file-only walker means no trailing-slash directory handling applied.\n### index.ts\n**Purpose:** src/discovery/filters/index.ts orchestrates filter chain execution for file discovery, re-exporting filter creators a...\n\n**src/discovery/filters/index.ts orchestrates filter chain execution for file discovery, re-exporting filter creators and providing applyFilters to process files through ordered filters with bounded concurrency and trace recording.**\n\n## Exports\n\n`createGitignoreFilter`, `createVendorFilter` (with `DEFAULT_VENDOR_DIRS`), `createBinaryFilter` (with `BINARY_EXTENSIONS`, `BinaryFilterOptions`), `createCustomFilter` — filter factory functions re-exported from submodules.\n\n`applyFilters(files: string[], filters: FileFilter[], options?: { tracer?: ITraceWriter; debug?: boolean; logger?: Logger }): Promise<FilterResult>` — applies ordered filter chain to file array, short-circuits at first exclusion, returns `{ included: string[], excluded: ExcludedFile[] }`.\n\n## Concurrency & Performance\n\nProcesses files with bounded concurrency (`CONCURRENCY = 30`) via worker pool from shared iterator to prevent file descriptor exhaustion (critical for binary content I/O). Order preserved via index-based sorting post-execution.\n\n## Filter Chain Semantics\n\nFilters evaluated per-file in order; first `shouldExclude()` returning truthy stops further checks. Excluded files record structure: `{ path, filter: filterName, reason: \"Excluded by [name] filter\" }`. Stats tracked per filter (matched/rejected) for trace emission.\n\n## Trace & Debug\n\nEmits `filter:applied` trace events with `filterName`, `filesMatched`, `filesRejected`. Debug logging via optional `logger` (defaults `nullLogger`).\n### vendor.ts\n**Purpose:** vendor.ts exports a vendor directory filter (createVendorFilter, DEFAULT_VENDOR_DIRS) excluding third-party code from...\n\n**vendor.ts exports a vendor directory filter (createVendorFilter, DEFAULT_VENDOR_DIRS) excluding third-party code from file discovery.**\n\n## Exports\n\n`DEFAULT_VENDOR_DIRS`: readonly array of vendor directory names ('node_modules', 'vendor', '.git', 'dist', 'build', '__pycache__', '.next', 'venv', '.venv', 'target') — directories globally excluded from discovery.\n\n`createVendorFilter(vendorDirs: string[]): FileFilter` — returns filter with `name: 'vendor'` and `shouldExclude(absolutePath: string): boolean` method; filters split input into single-segment directory names (matched anywhere in path via split-and-check) and multi-segment path patterns (matched via substring inclusion after OS-specific path separator normalization).\n\n## Filter Logic\n\n`shouldExclude` splits path by `path.sep`, checks each segment against `singleSegments` Set, then checks full path against all `pathPatterns` via `absolutePath.includes(pattern)`. Returns true if any match found.\n\n## Pattern Matching Behavior\n\nSingle-segment inputs ('node_modules') match as directory names anywhere in path hierarchy; multi-segment inputs ('apps/vendor', '.agents/skills') match as literal path sequences after normalization to OS separators.\n\n## Import Map (verified — use these exact paths)\n\nbinary.ts:\n  ../types.js → FileFilter (type)\n\ncustom.ts:\n  ../types.js → FileFilter (type)\n\ngitignore.ts:\n  ../types.js → FileFilter (type)\n\nindex.ts:\n  ../types.js → FileFilter, FilterResult, ExcludedFile (type)\n  ../../orchestration/trace.js → ITraceWriter (type)\n  ../../core/logger.js → Logger (type)\n  ../../core/logger.js → nullLogger\n\nvendor.ts:\n  ../types.js → FileFilter (type)\n\n## Project Directory Structure\n\n<project-structure>\n./\n  .npmrc\n  CONTRIBUTING.md\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  pages.yml\n  publish.yml\nhooks/\n  are-check-update.js\n  are-context-loader.js\n  opencode-are-check-update.js\nscripts/\n  analyze-agents-compression.sh\n  analyze-sum-ratios.sh\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  codex.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/providers/\n  subprocess.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  rebuild.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/core/\n  index.ts\n  logger.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  claude-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  orchestrator.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/rebuild/\n  checkpoint.ts\n  index.ts\n  orchestrator.ts\n  output-parser.ts\n  prompts.ts\n  spec-reader.ts\n  types.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>",
      "systemPrompt": "You are generating an AGENTS.md file — a directory-level overview for AI coding assistants.\n\nCRITICAL: Output ONLY the raw markdown content. No code fences, no preamble, no explanations, no conversational text. Your entire response IS the AGENTS.md file content.\n\nOUTPUT FORMAT:\n- First line MUST be exactly: <!-- Generated by agents-reverse-engineer -->\n- Use a # heading with the directory name\n- Write a one-paragraph purpose statement for the directory\nusage\nADAPTIVE SECTIONS:\nAnalyze the directory contents and choose the most relevant sections. Do NOT use a fixed template. Instead, select sections that best document this specific directory for an AI that needs to reconstruct or extend the project.\n\nConsider these section types (choose what applies):\n- **Contents**: Group files by purpose/category under ## headings. For each file: markdown link [filename](./filename) and a one-line description.\n- **Subdirectories**: If subdirectories exist, list them with links [dirname/](./dirname/) and brief summaries.\n- **Architecture / Data Flow**: If files form a pipeline, request/response chain, or layered architecture, document it.\n- **Stack**: If this is a package root (has package.json, Cargo.toml, go.mod, etc.), document the technology stack, key scripts, and entry points.\n- **Structure**: If the directory layout follows a convention (feature-sliced, domain-driven, MVC, etc.), document it.\n- **Patterns**: If files share recurring design patterns (factory, strategy, middleware, barrel re-export), name and document them.\n- **Configuration**: If the directory contains config files, schemas, or environment definitions, document the config surface area.\n- **API Surface**: If the directory exports a public API (barrel index, route definitions, SDK), document the interface contract.\n- **File Relationships**: How files collaborate, depend on each other, or share state.\n- **Behavioral Contracts**: If files contain regex patterns, format specifications, magic constants, or template strings that define observable behavior, collect them in a dedicated section. Preserve verbatim patterns from file summaries — do NOT paraphrase regex into prose. This section is MANDATORY when file summaries contain behavioral artifacts.\n- **Reproduction-Critical Constants**: If file summaries reference annex files (via ## Annex References sections), list them with links. Example: \"Full prompt template text: [templates.annex.sum](./prompts/templates.annex.sum)\". Do NOT reproduce annex content in AGENTS.md — just link to it.\n- **Workflow & Conventions**: If file summaries contain contribution guidelines, PR conventions, commit standards, testing mandates, tool usage rules, AI agent instructions, or code conventions (naming, formatting, linting), collect them into actionable directives. Source from CONTRIBUTING.md, CI configs, PR templates, linter/formatter configs, README dev sections. Present as concrete rules (\"commits must use conventionalcommits format\", \"use camelCase for functions, PascalCase for types\") not vague descriptions (\"the project has guidelines\").\n\nChoose any relevant sections or create your own based on the directory contents. The goal is to provide a comprehensive overview that captures the essence of the directory's role in the project and how its files work together, with a focus on what an AI coding assistant would need to know to effectively interact with this code.\n\nSCOPE:\n- AGENTS.md is a NAVIGATIONAL INDEX — help an AI find the right file quickly\n- Focus on: what each file does, how files relate, directory-level patterns\n- Do NOT reproduce full architecture sections — those belong in the root CLAUDE.md\n\nPATH ACCURACY (MANDATORY):\n- When referencing files or modules outside this directory, use ONLY paths from the \"Import Map\" section\n- Do NOT invent, rename, or guess module paths — if a path isn't in the Import Map, don't reference it\n- Use the exact directory names from \"Project Directory Structure\" — do NOT rename directories\n  (e.g., if the directory is called \"cli\", write \"src/cli/\", NOT \"src/commands/\")\n- Cross-module references must use the specifier format from actual import statements\n  (e.g., \"../generation/writers/sum.js\", NOT \"../fs/sum-file.js\")\n- If you are unsure about a path, omit the cross-reference rather than guessing\n\nCONSISTENCY (MANDATORY):\n- Do not contradict yourself within the same document\n- If you describe a technique (e.g., \"regex-based\"), do not call it something else later (e.g., \"AST-based\")\n- When stating version numbers, engines, or config fields, use ONLY values present in the file summaries\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this directory\", \"this module\", \"provides\", \"responsible for\", \"is used to\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Per-file descriptions in Contents sections: 1-2 sentences maximum. Reference key symbols but do not reproduce full summaries.\n- Behavioral contracts (regex patterns, format specs, constants) belong in a separate Behavioral Contracts section, not in per-file descriptions.\n- Annex file references: link to .annex.sum files, do not inline their content. One line per annex reference.\n- Subdirectory descriptions: 1-2 sentences maximum. Capture the directory's role, not its full contents.\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- Key exported symbols from file summaries MUST appear in the directory overview\n- Preserve exact casing of identifiers\n\nUSER NOTES:\n- If \"User Notes\" are provided in the prompt, they contain user-defined instructions that will be automatically prepended to your output\n- Do NOT repeat or paraphrase user notes in your generated content — they are included separately\n- You may reference information from user notes for context",
      "response": "<!-- Generated by agents-reverse-engineer -->\n\n# src/discovery/filters\n\nThe filters directory implements a composable chain of file exclusion filters for the discovery phase, combining gitignore patterns, vendor directory detection, binary file identification, and custom patterns into a unified filtering pipeline with bounded concurrency and trace recording.\n\n## Contents\n\n**Filter Factories**\n\n- [binary.ts](./binary.ts) — Two-phase binary detection via `BINARY_EXTENSIONS` Set (O(1) extension check) and `isBinaryfile` content analysis for unknown types; exports `createBinaryFilter(options: BinaryFilterOptions): FileFilter` with configurable `maxFileSize` threshold.\n\n- [vendor.ts](./vendor.ts) — Excludes third-party code via `DEFAULT_VENDOR_DIRS` (node_modules, dist, build, etc.); exports `createVendorFilter(vendorDirs: string[]): FileFilter` with dual-mode matching for single-segment directory names and multi-segment path patterns.\n\n- [gitignore.ts](./gitignore.ts) — Loads `.gitignore` from root directory and wraps `ignore` library patterns; exports async `createGitignoreFilter(root: string): Promise<FileFilter>` that converts absolute paths to relative paths for pattern matching.\n\n- [custom.ts](./custom.ts) — Applies caller-provided gitignore-style patterns via `ignore` library; exports `createCustomFilter(patterns: string[], root: string): FileFilter` for runtime pattern registration.\n\n- [index.ts](./index.ts) — Orchestrates filter chain execution with `applyFilters(files: string[], filters: FileFilter[], options?): Promise<FilterResult>` applying filters in order with short-circuit semantics, bounded concurrency (`CONCURRENCY = 30`), and trace emission via `ITraceWriter`.\n\n## Filter Chain Architecture\n\n`applyFilters()` processes file arrays through an ordered filter chain where each `FileFilter` has `name: string` and `shouldExclude(absolutePath): Promise<boolean> | boolean`. Evaluation stops at first truthy exclusion per file; results returned as `{ included: string[], excluded: ExcludedFile[] }` with per-filter statistics tracked for trace events (`filter:applied`).\n\nConcurrency bounded via worker pool from shared iterator to prevent file descriptor exhaustion during binary content I/O (critical for `isBinaryFile` checks in `createBinaryFilter`).\n\n## Behavioral Contracts\n\n**Path Normalization:**\n- Absolute paths → relative paths via `path.relative(normalizedRoot, absolutePath)`\n- Relative paths starting with `'..'` or empty strings treated as outside-root and never excluded\n- OS-specific path separator normalization in vendor filter via `path.sep`\n\n**Extension Detection (binary.ts):**\n- Case-insensitive via `path.extname().toLowerCase()`\n- 65 binary extensions pre-computed into `BINARY_EXTENSIONS` Set for O(1) lookup\n\n**Vendor Directory Matching (vendor.ts):**\n- Single-segment names ('node_modules') matched anywhere in path hierarchy via `absolutePath.split(path.sep)` segment checking\n- Multi-segment patterns ('apps/vendor') matched via substring inclusion after OS normalization\n\n**Pattern Matching (gitignore, custom):**\n- Empty pattern arrays return `false` (pass all files)\n- Missing .gitignore file silently handled (no error thrown)\n\n## File Relationships\n\n- `binary.ts`, `vendor.ts`, `gitignore.ts`, `custom.ts` each implement `FileFilter` interface from `../types.js`\n- `index.ts` imports and re-exports all four filter factories, then chains their execution via `applyFilters()`\n- `applyFilters()` consumes filter `name` field for trace event attribution and exclusion reason recording\n- Trace events emitted to optional `ITraceWriter` (from `../../orchestration/trace.js`) with per-filter match counts\n- Optional `logger` parameter (type `Logger` from `../../core/logger.js`) defaults to `nullLogger` for debug output",
      "model": "claude-haiku-4-5-20251001",
      "inputTokens": 3,
      "outputTokens": 954,
      "cacheReadTokens": 17512,
      "cacheCreationTokens": 3726,
      "latencyMs": 21010,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": []
    },
    {
      "timestamp": "2026-02-14T13:48:58.004Z",
      "prompt": "Generate AGENTS.md for directory: \"src/generation/prompts\" (prompts)\n\n## File Summaries (4 files)\n\n### builder.ts\n**Purpose:** builder.ts constructs LLM prompts for file and directory analysis with language detection, compression rules, and con...\n\n**builder.ts constructs LLM prompts for file and directory analysis with language detection, compression rules, and context assembly.**\n\n## Exports\n\n`detectLanguage(filePath: string): string` — maps file extensions to syntax highlighting language codes (`.ts`→`typescript`, `.py`→`python`, etc.; defaults to `text`).\n\n`buildFilePrompt(context: PromptContext, debug?: boolean, logger?: Logger): {system: string; user: string}` — assembles system and user prompts for single-file analysis. Substitutes `{{FILE_PATH}}`, `{{CONTENT}}`, `{{LANG}}` in `FILE_USER_PROMPT`; appends optional `context.contextFiles` as \"Related Files\" section; injects aggressive compression rules when `compressionRatio < 0.5` and `sourceFileSize > 0`, calculating target size and max size; uses `FILE_UPDATE_SYSTEM_PROMPT` if `context.existingSum` exists (incremental mode), otherwise `FILE_SYSTEM_PROMPT`; appends existing summary under \"Existing Summary\" header for updates.\n\n`buildDirectoryPrompt(dirPath: string, projectRoot: string, debug?: boolean, knownDirs?: Set<string>, projectStructure?: string, existingAgentsMd?: string, logger?: Logger, variant?: string): Promise<{system: string; user: string}>` — async; generates directory-level `AGENTS.md` prompt by: reading `.sum` file summaries in parallel via `readSumFile()`, extracting `metadata.purpose`; collecting child `AGENTS.md`/`AGENTS.{variant}.md` from subdirectories; scanning for `AGENTS.local.md` or user-authored `AGENTS.md` (detected by absence of `GENERATED_MARKER_PREFIX`), preserving as \"User Notes\"; detecting manifest files (`package.json`, `Cargo.toml`, `go.mod`, `pyproject.toml`, `pom.xml`, `build.gradle`, `Gemfile`, `composer.json`, `CMakeLists.txt`, `Makefile`); extracting actual imports from source files (`.ts|tsx|js|jsx|py|go|rs|java|kt`) via `extractDirectoryImports()` and formatting with `formatImportMap()`; scanning for `.annex.sum` files; assembling user sections: directory path, file summaries, import map, project structure, annex files, subdirectories, manifest hints, local content; returns `DIRECTORY_UPDATE_SYSTEM_PROMPT` if `existingAgentsMd` provided, else `DIRECTORY_SYSTEM_PROMPT`.\n\n## Dependencies\n\nImports system prompts (`FILE_SYSTEM_PROMPT`, `FILE_USER_PROMPT`, `FILE_UPDATE_SYSTEM_PROMPT`, `DIRECTORY_SYSTEM_PROMPT`, `DIRECTORY_UPDATE_SYSTEM_PROMPT`) from `./templates.js`; reads `.sum` files via `readSumFile(getSumPath())` from `../writers/sum.js`; detects generated markers via `GENERATED_MARKER_PREFIX` from `../writers/agents-md.js`; extracts and formats imports via `extractDirectoryImports(dirPath, sourceFileNames)` and `formatImportMap(fileImports)` from `../../imports/index.js`; uses `Logger` type and `nullLogger` singleton from `../../core/logger.js`.\n\n## Behavioral Contracts\n\n**Compression thresholds:** Aggressive compression rules injected when `compressionRatio < 0.5` and `sourceFileSize > 0`; target size = `Math.round(sourceFileSize * ratio)`; max size = `Math.round(targetSize * 1.2)`.\n\n**Template substitution patterns:** `{{FILE_PATH}}`, `{{CONTENT}}`, `{{LANG}}` replaced via `.replace(/\\{\\{...\\}\\}/g, ...)` in `FILE_USER_PROMPT`.\n\n**Generated marker detection:** `GENERATED_MARKER_PREFIX` substring check distinguishes generated vs. user-authored `AGENTS.md`.\n\n**Directory manifest indicators:** `package.json`, `Cargo.toml`, `go.mod`, `pyproject.toml`, `pom.xml`, `build.gradle`, `Gemfile`, `composer.json`, `CMakeLists.txt`, `Makefile` signal project roots.\n\n**Incremental update signatures:** `FILE_UPDATE_SYSTEM_PROMPT` used when `context.existingSum` truthy; `DIRECTORY_UPDATE_SYSTEM_PROMPT` used when `existingAgentsMd` truthy; existing content appended under \"Existing Summary\"/\"Existing AGENTS.md\" headers with instruction to \"preserve stable content, modify only what changed.\"\n### index.ts\n**Purpose:** index.ts is a barrel file aggregating prompt building utilities and type definitions for documentation generation.\n\n**index.ts is a barrel file aggregating prompt building utilities and type definitions for documentation generation.**\n\n## Exports\n\nRe-exports from `./types.js`:\n- `PromptContext` — type for prompt execution context\n- `SUMMARY_GUIDELINES` — constant defining summary generation rules\n\nRe-exports from `./builder.js`:\n- `buildFilePrompt(...)` — constructs prompts for single-file analysis\n- `buildDirectoryPrompt(...)` — constructs prompts for directory/multi-file analysis\n- `detectLanguage(...)` — identifies programming language from file content or extension\n\n## Module Structure\n\nPublic API surface for `src/generation/prompts/` — all consumers import from this index rather than submodules. Depends on `types.js` (type definitions, guidelines constants) and `builder.js` (prompt generation logic).\n### templates.ts\n**Purpose:** templates.ts exports four system prompts that orchestrate AI-driven code documentation generation across file and dir...\n\n**templates.ts exports four system prompts that orchestrate AI-driven code documentation generation across file and directory analysis workflows.**\n\n## Exported Constants\n\n`FILE_SYSTEM_PROMPT` — System prompt instructing AI to analyze individual source files and produce dense, identifier-rich summaries (19,318 chars). Mandates density rules (every sentence references specific identifiers), anchor term preservation (exact function/class/type names), behavioral contract extraction (regex patterns, format strings, magic constants, error codes, environment variables), and workflow/convention rules (contribution guidelines, testing mandates, code conventions, AI agent directives). Defines reproduction-critical content handling via Annex References sections for large string constants.\n\n`FILE_USER_PROMPT` — User-facing prompt template for file analysis (templated with `{{FILE_PATH}}` and `{{CONTENT}}`). Requires purpose statement as first bold line followed by ## headings organizing exported symbols with signatures; mandates minimum sections for purpose and exported symbols.\n\n`DIRECTORY_SYSTEM_PROMPT` — System prompt for AGENTS.md generation at directory scope (used by buildDirectoryPrompt() in builder.ts). Outputs raw markdown only. Mandates first line comment `<!-- Generated by agents-reverse-engineer -->`, adaptive section selection (Contents, Subdirectories, Architecture, Stack, Structure, Patterns, Configuration, API Surface, File Relationships, Behavioral Contracts, Reproduction-Critical Constants, Workflow & Conventions). Enforces path accuracy via Import Map and Project Directory Structure, density rules, and anchor term preservation. Collects behavioral contracts (regex patterns, format specs, constants) in dedicated section with verbatim preservation.\n\n`DIRECTORY_UPDATE_SYSTEM_PROMPT` — System prompt for incremental AGENTS.md updates when directory contents change. Preserves existing structure/headings for unchanged files, modifies only affected entries, adds/removes files as needed. Mandates verbatim preservation of behavioral contracts and reproduction-critical constants unless source summaries show changes. Outputs raw markdown with `<!-- Generated by agents-reverse-engineer -->` header.\n\n`FILE_UPDATE_SYSTEM_PROMPT` — System prompt for incremental file summary updates when source code changes. Preserves existing structure and phrasing for unchanged code, modifies only affected sections, updates signatures/types to match current source. Mandates verbatim preservation of regex patterns, format strings, magic constants, and template content unless source changed them.\n### types.ts\n**Purpose:** types.ts defines TypeScript interfaces and guidelines for prompt context and summary generation across the documentat...\n\n**types.ts defines TypeScript interfaces and guidelines for prompt context and summary generation across the documentation pipeline.**\n\n## Exports\n\n`PromptContext` interface: context object with `filePath` (string), `content` (string), `contextFiles` (optional array of {path, content}), `existingSum` (optional string for incremental updates), `sourceFileSize` (optional number), `compressionRatio` (optional number 0.1-1.0).\n\n`SUMMARY_GUIDELINES` constant: configuration object with `targetLength` ({min: 300, max: 500}), `include` (array of 8 required documentation topics), `exclude` (array of 3 content exclusion rules).\n\n## Design\n\nType-only module establishing contracts for prompt builders consuming PromptContext and enforcing documentation standards via immutable SUMMARY_GUIDELINES constant.\n\n## Import Map (verified — use these exact paths)\n\nbuilder.ts:\n  ../writers/sum.js → readSumFile, getSumPath\n  ../writers/agents-md.js → GENERATED_MARKER_PREFIX\n  ../../imports/index.js → extractDirectoryImports, formatImportMap\n  ../../core/logger.js → Logger (type)\n  ../../core/logger.js → nullLogger\n\n## Project Directory Structure\n\n<project-structure>\n./\n  .npmrc\n  CONTRIBUTING.md\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  pages.yml\n  publish.yml\nhooks/\n  are-check-update.js\n  are-context-loader.js\n  opencode-are-check-update.js\nscripts/\n  analyze-agents-compression.sh\n  analyze-sum-ratios.sh\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  codex.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/providers/\n  subprocess.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  rebuild.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/core/\n  index.ts\n  logger.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  claude-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  orchestrator.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/rebuild/\n  checkpoint.ts\n  index.ts\n  orchestrator.ts\n  output-parser.ts\n  prompts.ts\n  spec-reader.ts\n  types.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>",
      "systemPrompt": "You are generating an AGENTS.md file — a directory-level overview for AI coding assistants.\n\nCRITICAL: Output ONLY the raw markdown content. No code fences, no preamble, no explanations, no conversational text. Your entire response IS the AGENTS.md file content.\n\nOUTPUT FORMAT:\n- First line MUST be exactly: <!-- Generated by agents-reverse-engineer -->\n- Use a # heading with the directory name\n- Write a one-paragraph purpose statement for the directory\nusage\nADAPTIVE SECTIONS:\nAnalyze the directory contents and choose the most relevant sections. Do NOT use a fixed template. Instead, select sections that best document this specific directory for an AI that needs to reconstruct or extend the project.\n\nConsider these section types (choose what applies):\n- **Contents**: Group files by purpose/category under ## headings. For each file: markdown link [filename](./filename) and a one-line description.\n- **Subdirectories**: If subdirectories exist, list them with links [dirname/](./dirname/) and brief summaries.\n- **Architecture / Data Flow**: If files form a pipeline, request/response chain, or layered architecture, document it.\n- **Stack**: If this is a package root (has package.json, Cargo.toml, go.mod, etc.), document the technology stack, key scripts, and entry points.\n- **Structure**: If the directory layout follows a convention (feature-sliced, domain-driven, MVC, etc.), document it.\n- **Patterns**: If files share recurring design patterns (factory, strategy, middleware, barrel re-export), name and document them.\n- **Configuration**: If the directory contains config files, schemas, or environment definitions, document the config surface area.\n- **API Surface**: If the directory exports a public API (barrel index, route definitions, SDK), document the interface contract.\n- **File Relationships**: How files collaborate, depend on each other, or share state.\n- **Behavioral Contracts**: If files contain regex patterns, format specifications, magic constants, or template strings that define observable behavior, collect them in a dedicated section. Preserve verbatim patterns from file summaries — do NOT paraphrase regex into prose. This section is MANDATORY when file summaries contain behavioral artifacts.\n- **Reproduction-Critical Constants**: If file summaries reference annex files (via ## Annex References sections), list them with links. Example: \"Full prompt template text: [templates.annex.sum](./prompts/templates.annex.sum)\". Do NOT reproduce annex content in AGENTS.md — just link to it.\n- **Workflow & Conventions**: If file summaries contain contribution guidelines, PR conventions, commit standards, testing mandates, tool usage rules, AI agent instructions, or code conventions (naming, formatting, linting), collect them into actionable directives. Source from CONTRIBUTING.md, CI configs, PR templates, linter/formatter configs, README dev sections. Present as concrete rules (\"commits must use conventionalcommits format\", \"use camelCase for functions, PascalCase for types\") not vague descriptions (\"the project has guidelines\").\n\nChoose any relevant sections or create your own based on the directory contents. The goal is to provide a comprehensive overview that captures the essence of the directory's role in the project and how its files work together, with a focus on what an AI coding assistant would need to know to effectively interact with this code.\n\nSCOPE:\n- AGENTS.md is a NAVIGATIONAL INDEX — help an AI find the right file quickly\n- Focus on: what each file does, how files relate, directory-level patterns\n- Do NOT reproduce full architecture sections — those belong in the root CLAUDE.md\n\nPATH ACCURACY (MANDATORY):\n- When referencing files or modules outside this directory, use ONLY paths from the \"Import Map\" section\n- Do NOT invent, rename, or guess module paths — if a path isn't in the Import Map, don't reference it\n- Use the exact directory names from \"Project Directory Structure\" — do NOT rename directories\n  (e.g., if the directory is called \"cli\", write \"src/cli/\", NOT \"src/commands/\")\n- Cross-module references must use the specifier format from actual import statements\n  (e.g., \"../generation/writers/sum.js\", NOT \"../fs/sum-file.js\")\n- If you are unsure about a path, omit the cross-reference rather than guessing\n\nCONSISTENCY (MANDATORY):\n- Do not contradict yourself within the same document\n- If you describe a technique (e.g., \"regex-based\"), do not call it something else later (e.g., \"AST-based\")\n- When stating version numbers, engines, or config fields, use ONLY values present in the file summaries\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this directory\", \"this module\", \"provides\", \"responsible for\", \"is used to\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Per-file descriptions in Contents sections: 1-2 sentences maximum. Reference key symbols but do not reproduce full summaries.\n- Behavioral contracts (regex patterns, format specs, constants) belong in a separate Behavioral Contracts section, not in per-file descriptions.\n- Annex file references: link to .annex.sum files, do not inline their content. One line per annex reference.\n- Subdirectory descriptions: 1-2 sentences maximum. Capture the directory's role, not its full contents.\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- Key exported symbols from file summaries MUST appear in the directory overview\n- Preserve exact casing of identifiers\n\nUSER NOTES:\n- If \"User Notes\" are provided in the prompt, they contain user-defined instructions that will be automatically prepended to your output\n- Do NOT repeat or paraphrase user notes in your generated content — they are included separately\n- You may reference information from user notes for context",
      "response": "<!-- Generated by agents-reverse-engineer -->\n\n# src/generation/prompts\n\nThis directory provides prompt construction utilities for AI-driven documentation generation, handling both single-file summaries and directory-level AGENTS.md synthesis with language detection, compression rules, and incremental update workflows.\n\n## Contents\n\n**Prompt Construction**\n- [builder.ts](./builder.ts) — `detectLanguage()` maps file extensions to syntax highlighting codes; `buildFilePrompt()` assembles system/user prompts for single-file analysis with template substitution (`{{FILE_PATH}}`, `{{CONTENT}}`, `{{LANG}}`), optional context files, and aggressive compression injection when `compressionRatio < 0.5`; `buildDirectoryPrompt()` generates AGENTS.md prompts by reading `.sum` file summaries, extracting imports via `extractDirectoryImports()`, detecting `.annex.sum` files and manifest indicators (`package.json`, `Cargo.toml`, etc.), preserving user-authored content via `GENERATED_MARKER_PREFIX` detection, and returning update system prompts when `existingAgentsMd` is provided.\n\n**Type Definitions & Guidelines**\n- [types.ts](./types.ts) — `PromptContext` interface with `filePath`, `content`, `contextFiles`, `existingSum`, `sourceFileSize`, `compressionRatio`; `SUMMARY_GUIDELINES` constant specifying `targetLength` ({min: 300, max: 500}) and required documentation topics.\n\n**System Prompts**\n- [templates.ts](./templates.ts) — `FILE_SYSTEM_PROMPT` (19,318 chars) instructs AI to produce dense, identifier-rich summaries with density rules, anchor term preservation, behavioral contract extraction, and annex handling; `FILE_USER_PROMPT` template with substitution slots; `DIRECTORY_SYSTEM_PROMPT` orchestrates AGENTS.md generation with adaptive section selection and density enforcement; `DIRECTORY_UPDATE_SYSTEM_PROMPT` and `FILE_UPDATE_SYSTEM_PROMPT` preserve existing content while modifying only changed sections.\n\n**Public API**\n- [index.ts](./index.ts) — Barrel re-exporting `PromptContext`, `SUMMARY_GUIDELINES`, `buildFilePrompt()`, `buildDirectoryPrompt()`, and `detectLanguage()` for consumers in `src/generation/`.\n\n## Architecture\n\n`buildFilePrompt()` consumes `PromptContext` and optional debug/logger parameters, substituting templates and conditionally injecting compression rules. `buildDirectoryPrompt()` orchestrates a multi-step pipeline: reads parallel `.sum` files via `readSumFile(getSumPath())`, extracts directory imports from source files (`.ts|tsx|js|jsx|py|go|rs|java|kt`) via `extractDirectoryImports(dirPath, sourceFileNames)`, formats imports with `formatImportMap()`, detects user-authored `AGENTS.md` by checking absence of `GENERATED_MARKER_PREFIX`, scans for `.annex.sum` files and manifest files, collects child subdirectory `AGENTS.md`/`AGENTS.{variant}.md` via `readSumFile()`, and assembles user section with directory path, file summaries, import map, project structure, annex files, subdirectories, and manifest hints. Incremental updates (`FILE_UPDATE_SYSTEM_PROMPT`, `DIRECTORY_UPDATE_SYSTEM_PROMPT`) instruct the AI to preserve stable content and modify only affected entries.\n\n## Behavioral Contracts\n\n**Compression injection:** When `compressionRatio < 0.5` AND `sourceFileSize > 0`, calculate `targetSize = Math.round(sourceFileSize * ratio)` and `maxSize = Math.round(targetSize * 1.2)`, then append aggressive compression rules to user prompt.\n\n**Template substitution:** Patterns `{{FILE_PATH}}`, `{{CONTENT}}`, `{{LANG}}` replaced via `.replace(/\\{\\{...\\}\\}/g, ...)` in `FILE_USER_PROMPT`.\n\n**Generated marker detection:** `GENERATED_MARKER_PREFIX` substring presence distinguishes generated AGENTS.md from user-authored versions; absence signals user-maintained content requiring preservation.\n\n**Manifest file indicators:** Presence of `package.json`, `Cargo.toml`, `go.mod`, `pyproject.toml`, `pom.xml`, `build.gradle`, `Gemfile`, `composer.json`, `CMakeLists.txt`, or `Makefile` signals project root/manifest hints included in directory prompt.\n\n**Incremental update mode activation:** `FILE_UPDATE_SYSTEM_PROMPT` triggered when `context.existingSum` truthy; `DIRECTORY_UPDATE_SYSTEM_PROMPT` triggered when `existingAgentsMd` truthy; existing content appended under \"Existing Summary\"/\"Existing AGENTS.md\" headers with explicit instruction to preserve stable sections.\n\n## Reproduction-Critical Constants\n\nFull template text: [templates.annex.sum](./templates.annex.sum)",
      "model": "claude-haiku-4-5-20251001",
      "inputTokens": 3,
      "outputTokens": 1177,
      "cacheReadTokens": 17512,
      "cacheCreationTokens": 4208,
      "latencyMs": 23295,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": []
    },
    {
      "timestamp": "2026-02-14T13:48:58.008Z",
      "prompt": "Generate AGENTS.md for directory: \"src/generation/writers\" (writers)\n\n## File Summaries (4 files)\n\n### agents-md.ts\n**Purpose:** agents-md.ts manages AGENTS.md file generation and preservation, handling marker-based detection, user-content preser...\n\n**agents-md.ts manages AGENTS.md file generation and preservation, handling marker-based detection, user-content preservation via AGENTS.local.md, and multi-variant file writing.**\n\n## Exports\n\n`GENERATED_MARKER_PREFIX` — constant `'<!-- Generated by agents-reverse-engineer'` for version-agnostic marker detection.\n\n`GENERATED_MARKER` — constant `${GENERATED_MARKER_PREFIX} v${version} -->` for version-specific file identification.\n\n`isGeneratedAgentsMd(filePath: string): Promise<boolean>` — returns true if file contains GENERATED_MARKER_PREFIX (catches readFile errors as false).\n\n`writeAgentsMd(dirPath, _projectRoot, content, variant?): Promise<string>` — writes AGENTS.md with LLM content; if variant provided, writes to AGENTS.${variant}.md directly; otherwise preserves user-authored AGENTS.md by renaming to AGENTS.local.md and prepending `@AGENTS.local.md` reference above generated content; strips GENERATED_MARKER_PREFIX from input content before writing.\n\n`writeAgentsMdHub(dirPath, activeVariant): Promise<string>` — writes hub AGENTS.md referencing active variant via `@AGENTS.${activeVariant}.md`; preserves user content as AGENTS.local.md with `@AGENTS.local.md` reference.\n\n## Key Behaviors\n\nBoth `writeAgentsMd` and `writeAgentsMdHub` follow identical user-preservation workflow: (1) check existing AGENTS.md for GENERATED_MARKER_PREFIX, (2) if absent, rename to AGENTS.local.md, (3) check for pre-existing AGENTS.local.md from prior runs, (4) prepend `@AGENTS.local.md` to generated content if local content exists. All file writes include GENERATED_MARKER with version. Marker stripping removes leading `GENERATED_MARKER_PREFIX` + content up to `-->` + leading newlines from LLM input.\n### claude-md.ts\n**Purpose:** writeClaudeMdPointer() generates CLAUDE.md with @-references to AGENTS.md and optional CLAUDE.local.md, preserving us...\n\n**writeClaudeMdPointer() generates CLAUDE.md with @-references to AGENTS.md and optional CLAUDE.local.md, preserving user content.**\n\n## Exports\n- `writeClaudeMdPointer(dirAbsolutePath: string): Promise<string>` — creates/updates CLAUDE.md at path, returns claudeMdPath\n\n## Dependencies\n- `GENERATED_MARKER`, `GENERATED_MARKER_PREFIX` from `./agents-md.js` — sentinel strings for distinguishing AI-generated vs user-authored files\n- Node fs/promises: `readFile`, `rename`, `writeFile`; fs: `existsSync`; path module\n\n## Workflow\n1. Checks existing CLAUDE.md for `GENERATED_MARKER_PREFIX`; if absent (user-authored), renames to CLAUDE.local.md\n2. Detects CLAUDE.local.md from current or prior runs via `existsSync`\n3. Builds lines array: [GENERATED_MARKER, '', optional '@CLAUDE.local.md', '@AGENTS.md', '']\n4. Writes joined content to claudeMdPath with utf-8 encoding\n\n## Behavioral Contracts\n- File paths: `CLAUDE.md`, `CLAUDE.local.md` (sibling in dirAbsolutePath)\n- Content format: newline-joined lines; @-references must be exact identifiers (`@CLAUDE.local.md`, `@AGENTS.md`)\n- Detection rule: user content identified by absence of GENERATED_MARKER_PREFIX substring\n- Idempotent: CLAUDE.local.md existence checked twice (after rename attempt, then independent check) to handle both fresh and repeat runs\n### index.ts\n**Purpose:** src/generation/writers/index.ts re-exports documentation generation writers for .sum files, AGENTS.md, and Claude.md ...\n\n**src/generation/writers/index.ts re-exports documentation generation writers for .sum files, AGENTS.md, and Claude.md pointers.**\n\n## Exports\n\n- `writeSumFile(content: SumFileContent): void` — writes AI-friendly documentation to .sum file\n- `readSumFile(path: string): SumFileContent` — parses .sum file content\n- `getSumPath(filePath: string): string` — resolves .sum output path\n- `sumFileExists(path: string): boolean` — checks .sum file existence\n- `getAnnexPath(filePath: string): string` — resolves .annex.sum output path for reproduction-critical constants\n- `writeAnnexFile(path: string, content: string): void` — writes annex file with large constant definitions\n- `writeAgentsMd(agents: Agent[]): void` — generates AGENTS.md hub documentation\n- `writeAgentsMdHub(config: HubConfig): void` — generates AGENTS.md with hub-level metadata\n- `writeClaudeMdPointer(pointer: ClaudePointer): void` — writes Claude.md navigation pointer\n\n## Architecture\n\nAggregation barrel: centralizes all writer module exports (sum.js, agents-md.js, claude-md.js) for unified import paths.\n### sum.ts\n**Purpose:** sum.ts serializes AI documentation into YAML-frontmatter .sum files with structural parsing and SHA-256 change detect...\n\n**sum.ts serializes AI documentation into YAML-frontmatter .sum files with structural parsing and SHA-256 change detection.**\n\n## Exported Functions\n\n`readSumFile(sumPath: string): Promise<SumFileContent | null>` — reads and parses .sum file, returns null on missing/invalid file.\n\n`writeSumFile(sourcePath: string, content: SumFileContent, variant?: string): Promise<string>` — writes .sum alongside source file with optional variant suffix (e.g., `file.ts.claude.haiku.sum`); creates parent directories; returns written path.\n\n`getSumPath(sourcePath: string, variant?: string): string` — computes .sum filename: `foo.ts` → `foo.ts.sum` or `foo.ts.${variant}.sum`.\n\n`sumFileExists(sourcePath: string, variant?: string): Promise<boolean>` — checks .sum file existence by attempting read.\n\n`writeAnnexFile(sourcePath: string, sourceContent: string, variant?: string): Promise<string>` — writes reproduction-critical source to `foo.annex.sum` (or `foo.annex.${variant}.sum`); wraps content in markdown code block with header referencing original file.\n\n`getAnnexPath(sourcePath: string, variant?: string): string` — computes annex filename with extension stripped: `foo.ts` → `foo.annex.sum` or `foo.annex.${variant}.sum`.\n\n## Exported Types\n\n`SumFileContent` — struct with `summary: string` (main content), `metadata: SummaryMetadata`, `generatedAt: string` (timestamp), `contentHash: string` (SHA-256 for change detection).\n\n## Frontmatter Format\n\n.sum files use YAML frontmatter delimited by `---\\n...\\n---\\n`. Required fields: `generated_at`, `content_hash`, `purpose`. Optional fields: `critical_todos`, `related_files`. Arrays use inline `[a, b, c]` for ≤3 short items; multi-line `- item` format otherwise.\n\n## Parsing Patterns\n\n`parseYamlArray(frontmatter: string, key: string): string[]` — extracts YAML arrays with regex `/key:\\s*\\[([^\\]]*)\\]/` (inline) or `/key:\\s*\\n((?:\\s+-\\s+.+\\n?)+)/m` (multi-line); strips whitespace and quotes.\n\n`parseSumFile(content: string): SumFileContent | null` — extracts frontmatter via `/^---\\n([\\s\\S]*?)\\n---\\n/`; parses `generated_at`, `content_hash`, `purpose` fields; populates optional `criticalTodos`, `relatedFiles` in metadata; returns null on invalid structure.\n\n## Dependencies\n\nImports `writeFile`, `readFile`, `mkdir` from `node:fs/promises`; `path` module for file operations; `SummaryMetadata` type from `../types.js`; `GENERATED_MARKER` string from `./agents-md.js` for annex headers.\n\n## Import Map (verified — use these exact paths)\n\nagents-md.ts:\n  ../../version.js → getVersion\n\nsum.ts:\n  ../types.js → SummaryMetadata (type)\n\n## Project Directory Structure\n\n<project-structure>\n./\n  .npmrc\n  CONTRIBUTING.md\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  pages.yml\n  publish.yml\nhooks/\n  are-check-update.js\n  are-context-loader.js\n  opencode-are-check-update.js\nscripts/\n  analyze-agents-compression.sh\n  analyze-sum-ratios.sh\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  codex.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/providers/\n  subprocess.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  rebuild.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/core/\n  index.ts\n  logger.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  claude-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  orchestrator.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/rebuild/\n  checkpoint.ts\n  index.ts\n  orchestrator.ts\n  output-parser.ts\n  prompts.ts\n  spec-reader.ts\n  types.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>",
      "systemPrompt": "You are generating an AGENTS.md file — a directory-level overview for AI coding assistants.\n\nCRITICAL: Output ONLY the raw markdown content. No code fences, no preamble, no explanations, no conversational text. Your entire response IS the AGENTS.md file content.\n\nOUTPUT FORMAT:\n- First line MUST be exactly: <!-- Generated by agents-reverse-engineer -->\n- Use a # heading with the directory name\n- Write a one-paragraph purpose statement for the directory\nusage\nADAPTIVE SECTIONS:\nAnalyze the directory contents and choose the most relevant sections. Do NOT use a fixed template. Instead, select sections that best document this specific directory for an AI that needs to reconstruct or extend the project.\n\nConsider these section types (choose what applies):\n- **Contents**: Group files by purpose/category under ## headings. For each file: markdown link [filename](./filename) and a one-line description.\n- **Subdirectories**: If subdirectories exist, list them with links [dirname/](./dirname/) and brief summaries.\n- **Architecture / Data Flow**: If files form a pipeline, request/response chain, or layered architecture, document it.\n- **Stack**: If this is a package root (has package.json, Cargo.toml, go.mod, etc.), document the technology stack, key scripts, and entry points.\n- **Structure**: If the directory layout follows a convention (feature-sliced, domain-driven, MVC, etc.), document it.\n- **Patterns**: If files share recurring design patterns (factory, strategy, middleware, barrel re-export), name and document them.\n- **Configuration**: If the directory contains config files, schemas, or environment definitions, document the config surface area.\n- **API Surface**: If the directory exports a public API (barrel index, route definitions, SDK), document the interface contract.\n- **File Relationships**: How files collaborate, depend on each other, or share state.\n- **Behavioral Contracts**: If files contain regex patterns, format specifications, magic constants, or template strings that define observable behavior, collect them in a dedicated section. Preserve verbatim patterns from file summaries — do NOT paraphrase regex into prose. This section is MANDATORY when file summaries contain behavioral artifacts.\n- **Reproduction-Critical Constants**: If file summaries reference annex files (via ## Annex References sections), list them with links. Example: \"Full prompt template text: [templates.annex.sum](./prompts/templates.annex.sum)\". Do NOT reproduce annex content in AGENTS.md — just link to it.\n- **Workflow & Conventions**: If file summaries contain contribution guidelines, PR conventions, commit standards, testing mandates, tool usage rules, AI agent instructions, or code conventions (naming, formatting, linting), collect them into actionable directives. Source from CONTRIBUTING.md, CI configs, PR templates, linter/formatter configs, README dev sections. Present as concrete rules (\"commits must use conventionalcommits format\", \"use camelCase for functions, PascalCase for types\") not vague descriptions (\"the project has guidelines\").\n\nChoose any relevant sections or create your own based on the directory contents. The goal is to provide a comprehensive overview that captures the essence of the directory's role in the project and how its files work together, with a focus on what an AI coding assistant would need to know to effectively interact with this code.\n\nSCOPE:\n- AGENTS.md is a NAVIGATIONAL INDEX — help an AI find the right file quickly\n- Focus on: what each file does, how files relate, directory-level patterns\n- Do NOT reproduce full architecture sections — those belong in the root CLAUDE.md\n\nPATH ACCURACY (MANDATORY):\n- When referencing files or modules outside this directory, use ONLY paths from the \"Import Map\" section\n- Do NOT invent, rename, or guess module paths — if a path isn't in the Import Map, don't reference it\n- Use the exact directory names from \"Project Directory Structure\" — do NOT rename directories\n  (e.g., if the directory is called \"cli\", write \"src/cli/\", NOT \"src/commands/\")\n- Cross-module references must use the specifier format from actual import statements\n  (e.g., \"../generation/writers/sum.js\", NOT \"../fs/sum-file.js\")\n- If you are unsure about a path, omit the cross-reference rather than guessing\n\nCONSISTENCY (MANDATORY):\n- Do not contradict yourself within the same document\n- If you describe a technique (e.g., \"regex-based\"), do not call it something else later (e.g., \"AST-based\")\n- When stating version numbers, engines, or config fields, use ONLY values present in the file summaries\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this directory\", \"this module\", \"provides\", \"responsible for\", \"is used to\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Per-file descriptions in Contents sections: 1-2 sentences maximum. Reference key symbols but do not reproduce full summaries.\n- Behavioral contracts (regex patterns, format specs, constants) belong in a separate Behavioral Contracts section, not in per-file descriptions.\n- Annex file references: link to .annex.sum files, do not inline their content. One line per annex reference.\n- Subdirectory descriptions: 1-2 sentences maximum. Capture the directory's role, not its full contents.\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- Key exported symbols from file summaries MUST appear in the directory overview\n- Preserve exact casing of identifiers\n\nUSER NOTES:\n- If \"User Notes\" are provided in the prompt, they contain user-defined instructions that will be automatically prepended to your output\n- Do NOT repeat or paraphrase user notes in your generated content — they are included separately\n- You may reference information from user notes for context",
      "response": "<!-- Generated by agents-reverse-engineer -->\n\n# src/generation/writers\n\nAggregates file writers for .sum documentation, AGENTS.md variants, and CLAUDE.md pointers. Handles marker-based user content preservation, version tracking, and annex file generation for reproduction-critical constants.\n\n## Contents\n\n### Sum File Generation\n[sum.ts](./sum.ts) — Serializes source documentation into YAML-frontmatter .sum files with SHA-256 change detection. Exports `readSumFile()`, `writeSumFile()`, `getSumPath()`, `sumFileExists()`, `writeAnnexFile()`, `getAnnexPath()`. Uses `SumFileContent` type with `summary`, `metadata`, `generatedAt`, `contentHash` fields. Frontmatter parsing extracts `generated_at`, `content_hash`, `purpose`, and optional `critical_todos`, `related_files` via regex-based YAML array extraction.\n\n### AGENTS.md Writers\n[agents-md.ts](./agents-md.ts) — Manages AGENTS.md file generation with marker-based user content preservation. Exports `GENERATED_MARKER_PREFIX`, `GENERATED_MARKER`, `isGeneratedAgentsMd()`, `writeAgentsMd()`, `writeAgentsMdHub()`. Detects user-authored content by checking for `GENERATED_MARKER_PREFIX` substring; renames existing AGENTS.md to AGENTS.local.md and prepends `@AGENTS.local.md` reference if user content detected. Supports variant suffix (e.g., AGENTS.claude.haiku.md) and hub mode with active variant references.\n\n### CLAUDE.md Pointer\n[claude-md.ts](./claude-md.ts) — Generates CLAUDE.md with @-references to AGENTS.md and optional CLAUDE.local.md. Exports `writeClaudeMdPointer()`. Detects and preserves user-authored CLAUDE.md by renaming to CLAUDE.local.md, then writes hub file with `@CLAUDE.local.md` and `@AGENTS.md` references using identical marker and preservation logic as agents-md.ts.\n\n### Barrel Export\n[index.ts](./index.ts) — Re-exports `writeSumFile()`, `readSumFile()`, `getSumPath()`, `sumFileExists()`, `getAnnexFile()`, `writeAnnexFile()`, `writeAgentsMd()`, `writeAgentsMdHub()`, `writeClaudeMdPointer()` from aggregated writer modules.\n\n## Behavioral Contracts\n\n**Marker Format**: `GENERATED_MARKER_PREFIX` = `'<!-- Generated by agents-reverse-engineer'`; `GENERATED_MARKER` = `${GENERATED_MARKER_PREFIX} v${version} -->`\n\n**User Content Detection**: Files lacking `GENERATED_MARKER_PREFIX` substring are treated as user-authored and preserved via rename to `.local.md` variant.\n\n**File Naming Patterns**: \n- Sum files: `foo.ts.sum` or `foo.ts.${variant}.sum`\n- Annex files: `foo.annex.sum` or `foo.annex.${variant}.sum`\n- AGENTS variants: `AGENTS.${variant}.md` or hub `AGENTS.md`\n- CLAUDE variants: `CLAUDE.md` and optional `CLAUDE.local.md`\n\n**YAML Frontmatter Arrays**: Inline format `[a, b, c]` for ≤3 short items; multi-line `- item` format otherwise. Parsing uses regex `/key:\\s*\\[([^\\]]*)\\]/` (inline) and `/key:\\s*\\n((?:\\s+-\\s+.+\\n?)+)/m` (multi-line).\n\n**Content Preservation Workflow**: (1) Check existing file for GENERATED_MARKER_PREFIX, (2) if absent, rename to `.local.md`, (3) check for pre-existing `.local.md` from prior runs, (4) prepend `@${localFile}` reference to generated content if local content exists. Marker stripping removes leading `GENERATED_MARKER_PREFIX` + content up to `-->` + leading newlines from LLM input.\n\n**Hub Reference Format**: @-references use exact identifiers: `@AGENTS.${variant}.md`, `@AGENTS.local.md`, `@CLAUDE.local.md`, `@AGENTS.md`.\n\n## Architecture\n\n**Layered Writer Stack**: sum.ts (lowest level, YAML serialization) → agents-md.ts, claude-md.ts (mid-level, variant/hub logic) → index.ts (export aggregation).\n\n**Marker-Based Preservation**: All writers (sum.ts via annex headers, agents-md.ts, claude-md.ts) use `GENERATED_MARKER` constant from agents-md.ts to identify AI-generated files and trigger user-content rename-and-reference workflows. Prevents accidental overwrites of user contributions.\n\n**Variant Support**: sum.ts and agents-md.ts support optional `variant` parameter for multi-model documentation (e.g., `file.ts.claude.haiku.sum`, `AGENTS.claude.haiku.md`); hub mode writes version-agnostic files that reference the active variant.",
      "model": "claude-haiku-4-5-20251001",
      "inputTokens": 3,
      "outputTokens": 1245,
      "cacheReadTokens": 17512,
      "cacheCreationTokens": 4064,
      "latencyMs": 24657,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": []
    },
    {
      "timestamp": "2026-02-14T13:48:58.000Z",
      "prompt": "Generate AGENTS.md for directory: \"src/ai/backends\" (backends)\n\n## File Summaries (4 files)\n\n### claude.ts\n**Purpose:** claude.ts implements the AIBackend interface for the Claude Code CLI, handling argument construction, JSON response p...\n\n**claude.ts implements the AIBackend interface for the Claude Code CLI, handling argument construction, JSON response parsing (legacy and NDJSON formats), and PATH availability detection.**\n\n## Exports\n\n`isCommandOnPath(command: string): Promise<boolean>` — detects if a command exists in `process.env.PATH`, accounting for Windows `PATHEXT` extensions; uses `fs.stat()` instead of execute-bit checks for cross-platform compatibility.\n\n`ClaudeBackend` class implements `AIBackend` with four public methods:\n- `isAvailable(): Promise<boolean>` — checks `claude` CLI availability via `isCommandOnPath()`\n- `buildArgs(options: AICallOptions): string[]` — constructs CLI args: `-p --output-format json --no-session-persistence --allowedTools Read Write` plus optional `--model`, `--system-prompt`, `--max-turns`\n- `parseResponse(stdout: string, durationMs: number, exitCode: number): AIResponse` — validates JSON against `ClaudeResponseSchema` (Zod), extracts model name from first `modelUsage` key, returns normalized `AIResponse` with token usage and cost\n- `getInstallInstructions(): string` — returns npm install command for `@anthropic-ai/claude-code`\n\n## Response Parsing\n\n`ClaudeResponseSchema` (Zod object schema) validates Claude CLI output with `.passthrough()` on nested objects to tolerate future CLI field additions. Schema requires: `type: 'result'`, `subtype: 'success'|'error'`, `is_error: boolean`, durations, `result: string`, `session_id`, `total_cost_usd`, `usage` (input/output/cache tokens), `modelUsage` object map.\n\n`extractResultJson(stdout: string): string | undefined` (private) handles three CLI output formats in priority order: JSON array `[{system}, {assistant}, {result}]` (CLI ≥ 2.1.38), NDJSON newline-delimited objects, legacy single JSON object. Returns stringified result object or undefined if extraction fails.\n\n## Error Handling\n\n`parseResponse()` throws `AIServiceError` with code `'PARSE_ERROR'` if result JSON is missing (first 200 chars logged) or schema validation fails (error message included).\n\n## Design Notes\n\nPrompt is NOT included in CLI args; it goes to stdin via subprocess wrapper. `--no-session-persistence` disables disk session saving. `--allowedTools Read Write` pre-approves only essential tools, avoiding `bypassPermissions` block when running as root.\n### codex.ts\n**Purpose:** CodexBackend implements AIBackend interface for Codex CLI adapter, parsing JSONL event streams with multi-stage text ...\n\n**CodexBackend implements AIBackend interface for Codex CLI adapter, parsing JSONL event streams with multi-stage text extraction and token accounting fallbacks.**\n\n## Exports\n\n`CodexBackend` class implements `AIBackend` with properties `name='codex'`, `cliCommand='codex'`; methods: `isAvailable(): Promise<boolean>`, `buildArgs(AICallOptions): string[]`, `composeStdinInput(AICallOptions): string`, `parseResponse(stdout: string, durationMs: number, exitCode: number): AIResponse`, `getInstallInstructions(): string`.\n\n## Key Functions & Types\n\n`CodexUsageTotals` interface: `inputTokens`, `outputTokens`, `cacheReadTokens`, `cacheCreationTokens` (all numbers).\n\n`composePromptWithSystem(AICallOptions)`: wraps `systemPrompt` in `<system-instructions>` tags when present; preserves prompt field.\n\n`extractAssistantTextFromItem(unknown)`: filters `type==='agent_message'` items; extracts `text` field and `content[]` array elements where `type` is `'text'` or `'output_text'`; returns trimmed string array.\n\n`extractUsageFromTurnCompleted(unknown)`: normalizes polymorphic token field names (`input_tokens`, `inputTokens`, `cached_input_tokens`, `cache_read_input_tokens`, `cacheReadInputTokens`, `cache_creation_input_tokens`, `cacheCreationInputTokens`, `output_tokens`, `outputTokens`); subtracts `cacheRead` from `rawInput` to isolate non-cached input tokens.\n\n`collectText(unknown, string[], shouldSkipObject?)`: recursively traverses JSON objects/arrays extracting string values from `text` fields; skips objects where `shouldSkipObject` returns true.\n\n`shouldSkipTextObject(Record)`: returns true for `type==='reasoning'` or `type.endsWith('.reasoning')`.\n\n`uniq(string[])`: deduplicates array preserving insertion order.\n\n## CLI Integration\n\n`buildArgs()` produces: `['-a', 'never', 'exec', '--json', '--skip-git-repo-check', '--ephemeral', '--color', 'never']` plus optional `['--model', model]` and stdin mode flag `'-'`.\n\n`composeStdinInput()` delegates to `composePromptWithSystem()`.\n\n## Response Parsing Strategy\n\n`parseResponse()` implements cascading fallback chain:\n1. **Preferred path (JSONL structured)**: parse each line as JSON; process `type==='item.completed'` via `extractAssistantTextFromItem()`; accumulate `type==='turn.completed'` usage; return `AIResponse` with `raw.format='jsonl'`.\n2. **Fallback (JSONL compatibility)**: if structured extraction yields empty text, recursively `collectText()` across all parsed events with `shouldSkipTextObject()` filter; return `raw.format='jsonl-fallback'`.\n3. **Fallback (plain text)**: if no JSON lines parsed, return `stdout.trim()` as text; return `raw.format='text'`.\n4. **Error**: throw `AIServiceError('PARSE_ERROR', ...)` if all paths fail.\n\nSkips event types: `'error'`, `'thread.started'`, `'turn.started'`, anything ending in `'.error'` or `'.failed'`.\n\nReturns `AIResponse` with `text`, `model` (tracked from direct `obj.model` field), `{input,output,cacheRead,cacheCreation}Tokens`, `durationMs`, `exitCode`, `raw` object.\n\n## Type Guards\n\nHelper functions: `asRecord()` returns typed `Record<string, unknown>` or null; `asString()` returns `string | undefined`; `asNumber()` returns `number | undefined` (finite check).\n### gemini.ts\n**Purpose:** GeminiBackend is a stub implementation of AIBackend for the Gemini CLI, demonstrating the extension pattern while def...\n\n**GeminiBackend is a stub implementation of AIBackend for the Gemini CLI, demonstrating the extension pattern while deferring full implementation until Gemini's JSON output stabilizes.**\n\n## Exports\n\n`GeminiBackend` class implements `AIBackend`:\n- `name = 'gemini'` property\n- `cliCommand = 'gemini'` property\n- `isAvailable(): Promise<boolean>` — checks if `gemini` CLI is on PATH via `isCommandOnPath()`\n- `buildArgs(_options: AICallOptions): string[]` — returns `['-p', '--output-format', 'json']`\n- `parseResponse(_stdout: string, _durationMs: number, _exitCode: number): AIResponse` — throws `AIServiceError` with code `'SUBPROCESS_ERROR'` and message `'Gemini backend is not yet implemented. Use Claude backend.'`\n- `getInstallInstructions(): string` — returns multiline instructions: `'Gemini CLI (experimental):\\n  npm install -g @anthropic-ai/gemini-cli\\n  https://github.com/google-gemini/gemini-cli'`\n\n## Key Dependencies\n\nImports `AIBackend`, `AICallOptions`, `AIResponse` types and `AIServiceError` from `../types.js`; imports `isCommandOnPath` utility from `./claude.js`.\n\n## Behavioral Contract\n\n- `parseResponse()` unconditionally throws `AIServiceError('SUBPROCESS_ERROR', 'Gemini backend is not yet implemented. Use Claude backend.')`\n- `buildArgs()` ignores options parameter and returns fixed argument array for Gemini CLI\n- Implementation deferred pending Gemini CLI JSON output stability (documented in RESEARCH.md Open Question 2)\n### opencode.ts\n**Purpose:** OpenCodeBackend class implements AIBackend for OpenCode CLI, parsing NDJSON streaming output, aggregating token usage...\n\n**OpenCodeBackend class implements AIBackend for OpenCode CLI, parsing NDJSON streaming output, aggregating token usage across agentic turns, calculating cost from tokens when unprovided, and extracting response text.**\n\n## Exports\n\n- `OpenCodeBackend` — class implementing `AIBackend` interface with methods: `isAvailable()` → `Promise<boolean>`, `buildArgs(options: AICallOptions)` → `string[]`, `composeStdinInput(options: AICallOptions)` → `string`, `ensureProjectConfig(projectRoot: string)` → `Promise<void>`, `parseResponse(stdout: string, durationMs: number, exitCode: number)` → `AIResponse`\n\n## NDJSON Event Schemas\n\nZod schemas validate OpenCode's streaming NDJSON format: `OpenCodeTokensSchema` (token breakdown with `total`, `input`, `output`, `reasoning`, cache `read`/`write`), `OpenCodeStepFinishSchema` (`type: 'step_finish'` events containing per-turn token counts and cost), `OpenCodeTextSchema` (`type: 'text'` events carrying `part.text` response chunks). Other lifecycle events (`step_start`, `tool_use`, `tool_result`) are emitted but ignored.\n\n## Cost Calculation\n\nWhen OpenCode omits cost, `calculateCostFromTokens(inputTokens, outputTokens, cacheReadTokens, cacheWriteTokens)` → `number` uses Anthropic Claude Sonnet pricing: `INPUT_COST_PER_MTOK = 15`, `OUTPUT_COST_PER_MTOK = 75`, `CACHE_WRITE_COST_PER_MTOK = 18.75`, `CACHE_READ_COST_PER_MTOK = 1.50` (all per million tokens in USD).\n\n## Model Name Mapping\n\n`MODEL_ALIASES: Record<string, string>` maps short names to OpenCode's `provider/model` format: `'sonnet'` → `'anthropic/claude-sonnet-4-5'`, `'opus'` → `'anthropic/claude-opus-4-6'`, `'haiku'` → `'anthropic/claude-haiku-4-5'`. `resolveModelForOpenCode(model: string)` → `string` passes through identifiers containing `/` unchanged, looks up others in aliases.\n\n## OpenCode Agent Config\n\n`OPENCODE_AGENT_NAME = 'are-summarizer'` (used in `.opencode/agents/` directory and `--agent` CLI flag). `OPENCODE_AGENT_CONTENT` — markdown agent file disabling all tools (`tools: {\"*\": false}`) and limiting to `steps: 5` to force single-turn non-agentic mode. Expects ARE system prompt injected via `<system-instructions>` XML tags in stdin; agent file instructs model to follow these tags exactly, output only raw content without preamble/meta-commentary.\n\n## Response Parsing Strategy\n\n`parseResponse(stdout, durationMs, exitCode)` splits stdout by newlines, parses each as JSON (skipping malformed lines), aggregates: (1) all `text` event chunks via `part.text` concatenation, (2) all `step_finish` event token counts across turns, (3) cost from aggregated tokens if OpenCode provided zero. Special handling: strips \"MAXIMUM STEPS REACHED\" marker if substantial content (≥100 chars) remains; throws `AIServiceError` with code `'PARSE_ERROR'` if no text found or marker leaves <100 chars. Returns `AIResponse` with aggregated metrics in `raw` field including `numTurns`, `reasoningTokens`, `calculatedCost`.\n\n## Interface Compliance\n\nProperties: `name = 'opencode'`, `cliCommand = 'opencode'`. `buildArgs(options: AICallOptions)` returns `['run', '--format', 'json', '--agent', OPENCODE_AGENT_NAME]` plus `--model <resolved>` if specified. OpenCode lacks `--max-turns`, `--allowedTools`, `--system-prompt` equivalents — mitigated via agent `steps: 5`, `tools: {\"*\": false}`, and `<system-instructions>` XML wrapping. `ensureProjectConfig(projectRoot)` writes agent file to `.opencode/agents/are-summarizer.md` with `mkdir -p` recursion, always overwrites. `getInstallInstructions()` returns curl command and URL.\n\n## Import Map (verified — use these exact paths)\n\nclaude.ts:\n  ../types.js → AIBackend, AICallOptions, AIResponse (type)\n  ../types.js → AIServiceError\n\ncodex.ts:\n  ../types.js → AIBackend, AICallOptions, AIResponse (type)\n  ../types.js → AIServiceError\n\ngemini.ts:\n  ../types.js → AIBackend, AICallOptions, AIResponse (type)\n  ../types.js → AIServiceError\n\nopencode.ts:\n  ../types.js → AIBackend, AICallOptions, AIResponse (type)\n  ../types.js → AIServiceError\n\n## Project Directory Structure\n\n<project-structure>\n./\n  .npmrc\n  CONTRIBUTING.md\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  pages.yml\n  publish.yml\nhooks/\n  are-check-update.js\n  are-context-loader.js\n  opencode-are-check-update.js\nscripts/\n  analyze-agents-compression.sh\n  analyze-sum-ratios.sh\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  codex.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/providers/\n  subprocess.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  rebuild.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/core/\n  index.ts\n  logger.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  claude-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  orchestrator.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/rebuild/\n  checkpoint.ts\n  index.ts\n  orchestrator.ts\n  output-parser.ts\n  prompts.ts\n  spec-reader.ts\n  types.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>",
      "systemPrompt": "You are generating an AGENTS.md file — a directory-level overview for AI coding assistants.\n\nCRITICAL: Output ONLY the raw markdown content. No code fences, no preamble, no explanations, no conversational text. Your entire response IS the AGENTS.md file content.\n\nOUTPUT FORMAT:\n- First line MUST be exactly: <!-- Generated by agents-reverse-engineer -->\n- Use a # heading with the directory name\n- Write a one-paragraph purpose statement for the directory\nusage\nADAPTIVE SECTIONS:\nAnalyze the directory contents and choose the most relevant sections. Do NOT use a fixed template. Instead, select sections that best document this specific directory for an AI that needs to reconstruct or extend the project.\n\nConsider these section types (choose what applies):\n- **Contents**: Group files by purpose/category under ## headings. For each file: markdown link [filename](./filename) and a one-line description.\n- **Subdirectories**: If subdirectories exist, list them with links [dirname/](./dirname/) and brief summaries.\n- **Architecture / Data Flow**: If files form a pipeline, request/response chain, or layered architecture, document it.\n- **Stack**: If this is a package root (has package.json, Cargo.toml, go.mod, etc.), document the technology stack, key scripts, and entry points.\n- **Structure**: If the directory layout follows a convention (feature-sliced, domain-driven, MVC, etc.), document it.\n- **Patterns**: If files share recurring design patterns (factory, strategy, middleware, barrel re-export), name and document them.\n- **Configuration**: If the directory contains config files, schemas, or environment definitions, document the config surface area.\n- **API Surface**: If the directory exports a public API (barrel index, route definitions, SDK), document the interface contract.\n- **File Relationships**: How files collaborate, depend on each other, or share state.\n- **Behavioral Contracts**: If files contain regex patterns, format specifications, magic constants, or template strings that define observable behavior, collect them in a dedicated section. Preserve verbatim patterns from file summaries — do NOT paraphrase regex into prose. This section is MANDATORY when file summaries contain behavioral artifacts.\n- **Reproduction-Critical Constants**: If file summaries reference annex files (via ## Annex References sections), list them with links. Example: \"Full prompt template text: [templates.annex.sum](./prompts/templates.annex.sum)\". Do NOT reproduce annex content in AGENTS.md — just link to it.\n- **Workflow & Conventions**: If file summaries contain contribution guidelines, PR conventions, commit standards, testing mandates, tool usage rules, AI agent instructions, or code conventions (naming, formatting, linting), collect them into actionable directives. Source from CONTRIBUTING.md, CI configs, PR templates, linter/formatter configs, README dev sections. Present as concrete rules (\"commits must use conventionalcommits format\", \"use camelCase for functions, PascalCase for types\") not vague descriptions (\"the project has guidelines\").\n\nChoose any relevant sections or create your own based on the directory contents. The goal is to provide a comprehensive overview that captures the essence of the directory's role in the project and how its files work together, with a focus on what an AI coding assistant would need to know to effectively interact with this code.\n\nSCOPE:\n- AGENTS.md is a NAVIGATIONAL INDEX — help an AI find the right file quickly\n- Focus on: what each file does, how files relate, directory-level patterns\n- Do NOT reproduce full architecture sections — those belong in the root CLAUDE.md\n\nPATH ACCURACY (MANDATORY):\n- When referencing files or modules outside this directory, use ONLY paths from the \"Import Map\" section\n- Do NOT invent, rename, or guess module paths — if a path isn't in the Import Map, don't reference it\n- Use the exact directory names from \"Project Directory Structure\" — do NOT rename directories\n  (e.g., if the directory is called \"cli\", write \"src/cli/\", NOT \"src/commands/\")\n- Cross-module references must use the specifier format from actual import statements\n  (e.g., \"../generation/writers/sum.js\", NOT \"../fs/sum-file.js\")\n- If you are unsure about a path, omit the cross-reference rather than guessing\n\nCONSISTENCY (MANDATORY):\n- Do not contradict yourself within the same document\n- If you describe a technique (e.g., \"regex-based\"), do not call it something else later (e.g., \"AST-based\")\n- When stating version numbers, engines, or config fields, use ONLY values present in the file summaries\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this directory\", \"this module\", \"provides\", \"responsible for\", \"is used to\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Per-file descriptions in Contents sections: 1-2 sentences maximum. Reference key symbols but do not reproduce full summaries.\n- Behavioral contracts (regex patterns, format specs, constants) belong in a separate Behavioral Contracts section, not in per-file descriptions.\n- Annex file references: link to .annex.sum files, do not inline their content. One line per annex reference.\n- Subdirectory descriptions: 1-2 sentences maximum. Capture the directory's role, not its full contents.\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- Key exported symbols from file summaries MUST appear in the directory overview\n- Preserve exact casing of identifiers\n\nUSER NOTES:\n- If \"User Notes\" are provided in the prompt, they contain user-defined instructions that will be automatically prepended to your output\n- Do NOT repeat or paraphrase user notes in your generated content — they are included separately\n- You may reference information from user notes for context",
      "response": "<!-- Generated by agents-reverse-engineer -->\n\n# src/ai/backends\n\nThe backends directory contains multiple AIBackend implementations that adapt different CLI tools (Claude, Codex, Gemini, OpenCode) to a unified interface for AIResponse parsing and argument construction.\n\n## Contents\n\n### Backend Implementations\n\n[claude.ts](./claude.ts) — Implements `ClaudeBackend` with response parsing for legacy JSON and NDJSON formats, token extraction from `modelUsage` map, and PATH detection via `isCommandOnPath()`; supports `--model`, `--system-prompt`, `--max-turns` options and requires `--allowedTools Read Write` for root execution.\n\n[codex.ts](./codex.ts) — Implements `CodexBackend` with polymorphic JSONL event stream parsing, text extraction from `agent_message` items with fallback to recursive `collectText()`, token normalization across field naming variants, and usage aggregation across turns.\n\n[gemini.ts](./gemini.ts) — Implements `GeminiBackend` as a stub that throws `AIServiceError('SUBPROCESS_ERROR')` from `parseResponse()`, deferring full implementation until Gemini CLI JSON output stabilizes; imports `isCommandOnPath()` from `claude.ts`.\n\n[opencode.ts](./opencode.ts) — Implements `OpenCodeBackend` with NDJSON streaming parsing, cost calculation from tokens using Anthropic Claude Sonnet pricing constants, model alias mapping for short names, agent config injection via `.opencode/agents/are-summarizer.md`, and handling of \"MAXIMUM STEPS REACHED\" markers.\n\n## Response Parsing Architecture\n\nEach backend parses distinct CLI output formats into a unified `AIResponse` type:\n\n- **Claude**: `ClaudeResponseSchema` (Zod) validates JSON object with `type: 'result'`, `subtype`, `is_error`, token counts in `usage` and `modelUsage` map (first key = model name), cost in `total_cost_usd`.\n- **Codex**: Multi-stage JSONL fallback chain—preferred path extracts `agent_message` items via `extractAssistantTextFromItem()`; fallback recursively collects text with `shouldSkipTextObject()` filter for reasoning fields; final fallback returns raw stdout.\n- **OpenCode**: NDJSON schema validation via `OpenCodeStepFinishSchema` and `OpenCodeTextSchema`; aggregates token counts across `step_finish` events, calculates cost via `calculateCostFromTokens()` if unprovided, concatenates text chunks from `text` events.\n\n## Cost & Token Handling\n\n**Claude**: extracts `input_tokens`, `output_tokens`, `cache_read_tokens`, `cache_creation_tokens` from `usage` object and `total_cost_usd`.\n\n**Codex**: normalizes field names (`inputTokens`, `input_tokens`, `cached_input_tokens`, etc.) and isolates non-cached input by subtracting `cacheRead` from raw input; returns `{ inputTokens, outputTokens, cacheReadTokens, cacheCreationTokens }`.\n\n**OpenCode**: pricing constants `INPUT_COST_PER_MTOK = 15`, `OUTPUT_COST_PER_MTOK = 75`, `CACHE_WRITE_COST_PER_MTOK = 18.75`, `CACHE_READ_COST_PER_MTOK = 1.50` (USD per million tokens) used by `calculateCostFromTokens()` when OpenCode provides zero cost.\n\n## CLI Argument Construction\n\n- **Claude**: `buildArgs()` → `['-p', '--output-format', 'json', '--no-session-persistence', '--allowedTools', 'Read', 'Write']` plus `--model`, `--system-prompt`, `--max-turns` if present.\n- **Codex**: `buildArgs()` → `['-a', 'never', 'exec', '--json', '--skip-git-repo-check', '--ephemeral', '--color', 'never']` plus optional `--model` and stdin flag `'-'`.\n- **OpenCode**: `buildArgs()` → `['run', '--format', 'json', '--agent', 'are-summarizer']` plus resolved model via `MODEL_ALIASES` (e.g., `'sonnet'` → `'anthropic/claude-sonnet-4-5'`).\n\n## OpenCode Agent Configuration\n\n`OPENCODE_AGENT_NAME = 'are-summarizer'` and `OPENCODE_AGENT_CONTENT` markdown disables all tools (`tools: {\"*\": false}`) and enforces single-turn execution with `steps: 5`. `ensureProjectConfig(projectRoot)` writes agent file to `.opencode/agents/are-summarizer.md` with parent directory creation; `composeStdinInput()` wraps system prompt in `<system-instructions>` XML tags for agent to follow exactly without output preamble.\n\n## Error Handling\n\nAll backends throw `AIServiceError`:\n- **Claude**: `PARSE_ERROR` if result JSON missing or schema validation fails (first 200 chars logged).\n- **Codex**: `PARSE_ERROR` if all extraction paths fail (malformed JSON, no text found).\n- **Gemini**: `SUBPROCESS_ERROR` unconditionally from `parseResponse()`.\n- **OpenCode**: `PARSE_ERROR` if no text found or \"MAXIMUM STEPS REACHED\" marker leaves <100 chars content; throws on malformed NDJSON without validation.\n\n## Behavioral Contracts\n\nClaude's `isCommandOnPath()` detects executables cross-platform via `fs.stat()` instead of execute-bit checks; accounts for Windows `PATHEXT` extensions.\n\nCodex's `extractUsageFromTurnCompleted()` deduplicates polymorphic token field names and normalizes cache terminology (`cached_input_tokens` → `cacheReadTokens`, etc.).\n\nOpenCode filters NDJSON events by type: `step_finish`, `text` are processed; `step_start`, `tool_use`, `tool_result`, `error` are skipped.",
      "model": "claude-haiku-4-5-20251001",
      "inputTokens": 3,
      "outputTokens": 1451,
      "cacheReadTokens": 16147,
      "cacheCreationTokens": 6639,
      "latencyMs": 26247,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": []
    },
    {
      "timestamp": "2026-02-14T13:49:24.278Z",
      "prompt": "Generate AGENTS.md for directory: \"src/output\" (output)\n\n## File Summaries (1 files)\n\n### logger.ts\n**Purpose:** logger.ts exports Logger interface and createLogger/createSilentLogger factories for colorized CLI output following C...\n\n**logger.ts exports Logger interface and createLogger/createSilentLogger factories for colorized CLI output following CONTEXT.md format.**\n\n## Exports\n\n**Logger** interface: `info(message: string)`, `file(path: string)`, `excluded(path: string, reason: string, filter: string)`, `summary(included: number, excluded: number)`, `warn(message: string)`, `error(message: string)` — CLI output contract.\n\n**LoggerOptions** interface: `colors: boolean` (default true) — logger configuration.\n\n**createLogger(options: LoggerOptions): Logger** — returns logger with colored output via picocolors or identity functions when `options.colors=false`.\n\n**createSilentLogger(): Logger** — returns no-op logger for testing.\n\n## Output Format (CONTEXT.md Spec)\n\n- **file**: green `\"  +\"` prefix + relative path\n- **excluded**: dim `\"  -\"` prefix + path + dim `\"(${reason}: ${filter})\"` suffix\n- **summary**: bold `\"\\nDiscovered ${included} files\"` + dim `\"(${excluded} excluded)\"`\n- **warn**: yellow `\"Warning: \"` prefix + message\n- **error**: red `\"Error: \"` prefix + message\n\n## Dependencies\n\nImports `picocolors` (pc) for terminal colorization; falls back to identity functions when colors disabled.\n\n## Project Directory Structure\n\n<project-structure>\n./\n  .npmrc\n  CONTRIBUTING.md\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  pages.yml\n  publish.yml\nhooks/\n  are-check-update.js\n  are-context-loader.js\n  opencode-are-check-update.js\nscripts/\n  analyze-agents-compression.sh\n  analyze-sum-ratios.sh\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  codex.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/providers/\n  subprocess.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  rebuild.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/core/\n  index.ts\n  logger.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  claude-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  orchestrator.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/rebuild/\n  checkpoint.ts\n  index.ts\n  orchestrator.ts\n  output-parser.ts\n  prompts.ts\n  spec-reader.ts\n  types.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>",
      "systemPrompt": "You are generating an AGENTS.md file — a directory-level overview for AI coding assistants.\n\nCRITICAL: Output ONLY the raw markdown content. No code fences, no preamble, no explanations, no conversational text. Your entire response IS the AGENTS.md file content.\n\nOUTPUT FORMAT:\n- First line MUST be exactly: <!-- Generated by agents-reverse-engineer -->\n- Use a # heading with the directory name\n- Write a one-paragraph purpose statement for the directory\nusage\nADAPTIVE SECTIONS:\nAnalyze the directory contents and choose the most relevant sections. Do NOT use a fixed template. Instead, select sections that best document this specific directory for an AI that needs to reconstruct or extend the project.\n\nConsider these section types (choose what applies):\n- **Contents**: Group files by purpose/category under ## headings. For each file: markdown link [filename](./filename) and a one-line description.\n- **Subdirectories**: If subdirectories exist, list them with links [dirname/](./dirname/) and brief summaries.\n- **Architecture / Data Flow**: If files form a pipeline, request/response chain, or layered architecture, document it.\n- **Stack**: If this is a package root (has package.json, Cargo.toml, go.mod, etc.), document the technology stack, key scripts, and entry points.\n- **Structure**: If the directory layout follows a convention (feature-sliced, domain-driven, MVC, etc.), document it.\n- **Patterns**: If files share recurring design patterns (factory, strategy, middleware, barrel re-export), name and document them.\n- **Configuration**: If the directory contains config files, schemas, or environment definitions, document the config surface area.\n- **API Surface**: If the directory exports a public API (barrel index, route definitions, SDK), document the interface contract.\n- **File Relationships**: How files collaborate, depend on each other, or share state.\n- **Behavioral Contracts**: If files contain regex patterns, format specifications, magic constants, or template strings that define observable behavior, collect them in a dedicated section. Preserve verbatim patterns from file summaries — do NOT paraphrase regex into prose. This section is MANDATORY when file summaries contain behavioral artifacts.\n- **Reproduction-Critical Constants**: If file summaries reference annex files (via ## Annex References sections), list them with links. Example: \"Full prompt template text: [templates.annex.sum](./prompts/templates.annex.sum)\". Do NOT reproduce annex content in AGENTS.md — just link to it.\n- **Workflow & Conventions**: If file summaries contain contribution guidelines, PR conventions, commit standards, testing mandates, tool usage rules, AI agent instructions, or code conventions (naming, formatting, linting), collect them into actionable directives. Source from CONTRIBUTING.md, CI configs, PR templates, linter/formatter configs, README dev sections. Present as concrete rules (\"commits must use conventionalcommits format\", \"use camelCase for functions, PascalCase for types\") not vague descriptions (\"the project has guidelines\").\n\nChoose any relevant sections or create your own based on the directory contents. The goal is to provide a comprehensive overview that captures the essence of the directory's role in the project and how its files work together, with a focus on what an AI coding assistant would need to know to effectively interact with this code.\n\nSCOPE:\n- AGENTS.md is a NAVIGATIONAL INDEX — help an AI find the right file quickly\n- Focus on: what each file does, how files relate, directory-level patterns\n- Do NOT reproduce full architecture sections — those belong in the root CLAUDE.md\n\nPATH ACCURACY (MANDATORY):\n- When referencing files or modules outside this directory, use ONLY paths from the \"Import Map\" section\n- Do NOT invent, rename, or guess module paths — if a path isn't in the Import Map, don't reference it\n- Use the exact directory names from \"Project Directory Structure\" — do NOT rename directories\n  (e.g., if the directory is called \"cli\", write \"src/cli/\", NOT \"src/commands/\")\n- Cross-module references must use the specifier format from actual import statements\n  (e.g., \"../generation/writers/sum.js\", NOT \"../fs/sum-file.js\")\n- If you are unsure about a path, omit the cross-reference rather than guessing\n\nCONSISTENCY (MANDATORY):\n- Do not contradict yourself within the same document\n- If you describe a technique (e.g., \"regex-based\"), do not call it something else later (e.g., \"AST-based\")\n- When stating version numbers, engines, or config fields, use ONLY values present in the file summaries\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this directory\", \"this module\", \"provides\", \"responsible for\", \"is used to\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Per-file descriptions in Contents sections: 1-2 sentences maximum. Reference key symbols but do not reproduce full summaries.\n- Behavioral contracts (regex patterns, format specs, constants) belong in a separate Behavioral Contracts section, not in per-file descriptions.\n- Annex file references: link to .annex.sum files, do not inline their content. One line per annex reference.\n- Subdirectory descriptions: 1-2 sentences maximum. Capture the directory's role, not its full contents.\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- Key exported symbols from file summaries MUST appear in the directory overview\n- Preserve exact casing of identifiers\n\nUSER NOTES:\n- If \"User Notes\" are provided in the prompt, they contain user-defined instructions that will be automatically prepended to your output\n- Do NOT repeat or paraphrase user notes in your generated content — they are included separately\n- You may reference information from user notes for context",
      "response": "<!-- Generated by agents-reverse-engineer -->\n\n# src/output\n\nCLI output abstraction for colorized logging with format specifications. Exports `Logger` interface and factory functions (`createLogger`, `createSilentLogger`) that produce formatted output matching CONTEXT.md conventions.\n\n## Contents\n\n- [logger.ts](./logger.ts) — `Logger` interface with `info`, `file`, `excluded`, `summary`, `warn`, `error` methods; `createLogger(options)` and `createSilentLogger()` factories using `picocolors` for optional terminal colorization.\n\n## Logger Interface & Factories\n\n**Logger** contract (6 methods):\n- `info(message: string)` — plain text output\n- `file(path: string)` — green `\"  +\"` prefix + relative path\n- `excluded(path: string, reason: string, filter: string)` — dim `\"  -\"` prefix + path + dim suffix `\"(${reason}: ${filter})\"`\n- `summary(included: number, excluded: number)` — bold discovery count + dim exclusion count\n- `warn(message: string)` — yellow `\"Warning: \"` prefix\n- `error(message: string)` — red `\"Error: \"` prefix\n\n**createLogger(options: LoggerOptions)** — returns `Logger` with colored output; respects `options.colors` (default `true`) to toggle `picocolors` or identity functions.\n\n**createSilentLogger()** — returns no-op `Logger` for test/quiet modes.\n\n## Configuration\n\n**LoggerOptions** interface: single field `colors: boolean` controls terminal colorization behavior.",
      "model": "claude-haiku-4-5-20251001",
      "inputTokens": 3,
      "outputTokens": 378,
      "cacheReadTokens": 17512,
      "cacheCreationTokens": 2302,
      "latencyMs": 17853,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": []
    },
    {
      "timestamp": "2026-02-14T13:49:24.269Z",
      "prompt": "Generate AGENTS.md for directory: \".github/workflows\" (workflows)\n\n## File Summaries (2 files)\n\n### pages.yml\n**Purpose:** pages.yml defines CI/CD workflow triggering on main branch pushes to site/ or blog/ directories, building with Node 2...\n\n**pages.yml defines CI/CD workflow triggering on main branch pushes to site/ or blog/ directories, building with Node 20 and site/build.mjs, then deploying to GitHub Pages.**\n\n## Trigger Configuration\n- **Push events**: branches=[main], paths=[site/**, blog/**]\n- **Manual trigger**: workflow_dispatch enabled\n- Concurrent workflow cancellation: group=pages, cancel-in-progress=true\n\n## Permissions\n- contents: read\n- pages: write\n- id-token: write\n\n## Build Job (ubuntu-latest)\nExecutes sequentially: checkout@v4 → setup-node@v4 (node-version='20') → npm ci → node site/build.mjs → upload-pages-artifact@v3 (path: site)\n\n## Deploy Job\nDepends on: build. Runs on ubuntu-latest in github-pages environment. Executes actions/deploy-pages@v4, outputs page_url to steps.deployment.outputs.page_url.\n\n## Build Artifact\n- Source: site/ directory\n- Upload action: actions/upload-pages-artifact@v3\n- Triggers: post-build before deploy\n### publish.yml\n**Purpose:** publish.yml automates npm package publishing on GitHub release events using Node.js 20 with provenance attestation.\n\n**publish.yml automates npm package publishing on GitHub release events using Node.js 20 with provenance attestation.**\n\n## Trigger Events\n\nPublishes on `release.types: [published]` (manual release creation) or `workflow_dispatch` (manual trigger).\n\n## Execution Environment\n\nRuns on `ubuntu-latest` with Node.js 20 configured for `https://registry.npmjs.org` registry. Permissions: `contents: read`, `id-token: write` (required for npm provenance).\n\n## Build & Publish Pipeline\n\n1. `actions/checkout@v4` — clone repository\n2. `actions/setup-node@v4` — configure Node.js 20 and npm registry\n3. `npm ci` — install dependencies from lockfile\n4. `npm run build` — execute build script\n5. `npm publish --provenance --access public` — publish with provenance attestation and public access, consuming `NODE_AUTH_TOKEN` secret\n\n## Secrets & Authentication\n\n`NODE_AUTH_TOKEN` (required) — npm authentication token from repository secrets; enables publishing without interactive auth.\n\n## Project Directory Structure\n\n<project-structure>\n./\n  .npmrc\n  CONTRIBUTING.md\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  pages.yml\n  publish.yml\nhooks/\n  are-check-update.js\n  are-context-loader.js\n  opencode-are-check-update.js\nscripts/\n  analyze-agents-compression.sh\n  analyze-sum-ratios.sh\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  codex.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/providers/\n  subprocess.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  rebuild.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/core/\n  index.ts\n  logger.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  claude-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  orchestrator.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/rebuild/\n  checkpoint.ts\n  index.ts\n  orchestrator.ts\n  output-parser.ts\n  prompts.ts\n  spec-reader.ts\n  types.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>",
      "systemPrompt": "You are generating an AGENTS.md file — a directory-level overview for AI coding assistants.\n\nCRITICAL: Output ONLY the raw markdown content. No code fences, no preamble, no explanations, no conversational text. Your entire response IS the AGENTS.md file content.\n\nOUTPUT FORMAT:\n- First line MUST be exactly: <!-- Generated by agents-reverse-engineer -->\n- Use a # heading with the directory name\n- Write a one-paragraph purpose statement for the directory\nusage\nADAPTIVE SECTIONS:\nAnalyze the directory contents and choose the most relevant sections. Do NOT use a fixed template. Instead, select sections that best document this specific directory for an AI that needs to reconstruct or extend the project.\n\nConsider these section types (choose what applies):\n- **Contents**: Group files by purpose/category under ## headings. For each file: markdown link [filename](./filename) and a one-line description.\n- **Subdirectories**: If subdirectories exist, list them with links [dirname/](./dirname/) and brief summaries.\n- **Architecture / Data Flow**: If files form a pipeline, request/response chain, or layered architecture, document it.\n- **Stack**: If this is a package root (has package.json, Cargo.toml, go.mod, etc.), document the technology stack, key scripts, and entry points.\n- **Structure**: If the directory layout follows a convention (feature-sliced, domain-driven, MVC, etc.), document it.\n- **Patterns**: If files share recurring design patterns (factory, strategy, middleware, barrel re-export), name and document them.\n- **Configuration**: If the directory contains config files, schemas, or environment definitions, document the config surface area.\n- **API Surface**: If the directory exports a public API (barrel index, route definitions, SDK), document the interface contract.\n- **File Relationships**: How files collaborate, depend on each other, or share state.\n- **Behavioral Contracts**: If files contain regex patterns, format specifications, magic constants, or template strings that define observable behavior, collect them in a dedicated section. Preserve verbatim patterns from file summaries — do NOT paraphrase regex into prose. This section is MANDATORY when file summaries contain behavioral artifacts.\n- **Reproduction-Critical Constants**: If file summaries reference annex files (via ## Annex References sections), list them with links. Example: \"Full prompt template text: [templates.annex.sum](./prompts/templates.annex.sum)\". Do NOT reproduce annex content in AGENTS.md — just link to it.\n- **Workflow & Conventions**: If file summaries contain contribution guidelines, PR conventions, commit standards, testing mandates, tool usage rules, AI agent instructions, or code conventions (naming, formatting, linting), collect them into actionable directives. Source from CONTRIBUTING.md, CI configs, PR templates, linter/formatter configs, README dev sections. Present as concrete rules (\"commits must use conventionalcommits format\", \"use camelCase for functions, PascalCase for types\") not vague descriptions (\"the project has guidelines\").\n\nChoose any relevant sections or create your own based on the directory contents. The goal is to provide a comprehensive overview that captures the essence of the directory's role in the project and how its files work together, with a focus on what an AI coding assistant would need to know to effectively interact with this code.\n\nSCOPE:\n- AGENTS.md is a NAVIGATIONAL INDEX — help an AI find the right file quickly\n- Focus on: what each file does, how files relate, directory-level patterns\n- Do NOT reproduce full architecture sections — those belong in the root CLAUDE.md\n\nPATH ACCURACY (MANDATORY):\n- When referencing files or modules outside this directory, use ONLY paths from the \"Import Map\" section\n- Do NOT invent, rename, or guess module paths — if a path isn't in the Import Map, don't reference it\n- Use the exact directory names from \"Project Directory Structure\" — do NOT rename directories\n  (e.g., if the directory is called \"cli\", write \"src/cli/\", NOT \"src/commands/\")\n- Cross-module references must use the specifier format from actual import statements\n  (e.g., \"../generation/writers/sum.js\", NOT \"../fs/sum-file.js\")\n- If you are unsure about a path, omit the cross-reference rather than guessing\n\nCONSISTENCY (MANDATORY):\n- Do not contradict yourself within the same document\n- If you describe a technique (e.g., \"regex-based\"), do not call it something else later (e.g., \"AST-based\")\n- When stating version numbers, engines, or config fields, use ONLY values present in the file summaries\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this directory\", \"this module\", \"provides\", \"responsible for\", \"is used to\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Per-file descriptions in Contents sections: 1-2 sentences maximum. Reference key symbols but do not reproduce full summaries.\n- Behavioral contracts (regex patterns, format specs, constants) belong in a separate Behavioral Contracts section, not in per-file descriptions.\n- Annex file references: link to .annex.sum files, do not inline their content. One line per annex reference.\n- Subdirectory descriptions: 1-2 sentences maximum. Capture the directory's role, not its full contents.\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- Key exported symbols from file summaries MUST appear in the directory overview\n- Preserve exact casing of identifiers\n\nUSER NOTES:\n- If \"User Notes\" are provided in the prompt, they contain user-defined instructions that will be automatically prepended to your output\n- Do NOT repeat or paraphrase user notes in your generated content — they are included separately\n- You may reference information from user notes for context",
      "response": "<!-- Generated by agents-reverse-engineer -->\n\n# .github/workflows\n\nThis directory contains GitHub Actions CI/CD workflows that automate site deployment and npm package publishing. These workflows define the release pipeline, handling documentation site builds to GitHub Pages and authenticated npm package distribution with provenance attestation.\n\n## Workflows\n\n### [pages.yml](./pages.yml)\n\nBuilds and deploys documentation site to GitHub Pages on pushes to `main` affecting `site/**` or `blog/**` paths. Runs `site/build.mjs` with Node 20, uploads `site/` artifact via `actions/upload-pages-artifact@v3`, and deploys via `actions/deploy-pages@v4`. Supports manual triggers with `workflow_dispatch` and cancels concurrent runs via group=pages.\n\n**Trigger:** `push` branches=[main], paths=[site/**, blog/**]; `workflow_dispatch`\n\n**Permissions:** contents:read, pages:write, id-token:write\n\n**Build:** `checkout@v4` → `setup-node@v4` (node-version='20') → `npm ci` → `node site/build.mjs`\n\n### [publish.yml](./publish.yml)\n\nPublishes npm package on GitHub release creation or manual dispatch with Node 20 provenance attestation. Executes `npm run build`, then `npm publish --provenance --access public` using `NODE_AUTH_TOKEN` secret for authentication to `https://registry.npmjs.org`.\n\n**Trigger:** `release` types=[published]; `workflow_dispatch`\n\n**Permissions:** contents:read, id-token:write (provenance requirement)\n\n**Pipeline:** `checkout@v4` → `setup-node@v4` (node-version='20') → `npm ci` → `npm run build` → `npm publish --provenance --access public`\n\n**Secrets:** NODE_AUTH_TOKEN (npm registry authentication)",
      "model": "claude-haiku-4-5-20251001",
      "inputTokens": 3,
      "outputTokens": 436,
      "cacheReadTokens": 17512,
      "cacheCreationTokens": 2539,
      "latencyMs": 22720,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": []
    },
    {
      "timestamp": "2026-02-14T13:49:24.292Z",
      "prompt": "Generate AGENTS.md for directory: \"src/change-detection\" (change-detection)\n\n## File Summaries (3 files)\n\n### detector.ts\n**Purpose:** detector.ts implements git change detection and content hashing for tracking file modifications and computing integri...\n\n**detector.ts implements git change detection and content hashing for tracking file modifications and computing integrity checksums.**\n\n## Exports\n\n`isGitRepo(projectRoot: string): Promise<boolean>` — checks if projectRoot is a git repository via `git.checkIsRepo()`.\n\n`getCurrentCommit(projectRoot: string): Promise<string>` — returns current HEAD commit hash from `git revparse HEAD`, trimmed.\n\n`getChangedFiles(projectRoot: string, baseCommit: string, options?: ChangeDetectionOptions): Promise<ChangeDetectionResult>` — detects file changes between baseCommit and HEAD using `git diff --name-status -M`; optionally includes uncommitted changes (staged, modified, deleted, untracked) when `options.includeUncommitted` is true; parses diff output mapping status codes (A=added, M=modified, D=deleted, R*=renamed) to FileChange objects with `path`, `status`, and optional `oldPath`.\n\n`computeContentHash(filePath: string): Promise<string>` — computes SHA-256 hex digest of file content via `readFile()` + `createHash('sha256')`.\n\n`computeContentHashFromString(content: string): string` — computes SHA-256 hex digest synchronously from string content; avoids redundant disk reads.\n\n## Key Behaviors\n\nRename detection uses `-M` flag with 50% similarity threshold; status prefix R followed by percentage (R100, R95, etc.) maps old/new paths from tab-separated parts[1] and parts[last].\n\nUncommitted change tracking deduplicates across `status.modified`, `status.deleted`, `status.not_added`, `status.staged` to avoid duplicate FileChange entries for same path.\n\nDiff output format: `STATUS\\tFILE` (or `STATUS\\tOLD\\tNEW` for renames); lines filtered for non-empty and split on tab.\n\n## Dependencies\n\n`simple-git` for git operations (`checkIsRepo()`, `revparse()`, `diff()`, `status()`); `node:crypto.createHash()` for SHA-256; `node:fs/promises.readFile()` for file I/O.\n### index.ts\n**Purpose:** src/change-detection/index.ts is a barrel export module aggregating git-based change detection utilities and types.\n\n**src/change-detection/index.ts is a barrel export module aggregating git-based change detection utilities and types.**\n\n## Exported Functions\n- `isGitRepo()` — detects if directory is git repository\n- `getCurrentCommit()` — retrieves current HEAD commit\n- `getChangedFiles()` — returns added, modified, deleted, renamed files\n- `computeContentHash()` — hashes file content\n- `computeContentHashFromString()` — hashes string content\n\n## Exported Types\n- `ChangeType` — enum/union for file change classification\n- `FileChange` — object structure for individual file changes\n- `ChangeDetectionResult` — aggregated change detection output\n- `ChangeDetectionOptions` — configuration params for change detection\n\n## Architecture\nRe-exports from `./detector.js` (functions) and `./types.js` (types); serves as public API surface for incremental update workflows.\n### types.ts\n**Purpose:** types.ts defines TypeScript interfaces and types for git change detection operations.\n\n**types.ts defines TypeScript interfaces and types for git change detection operations.**\n\n## Exported Types\n\n`ChangeType` — union of `'added' | 'modified' | 'deleted' | 'renamed'` for file operation classification.\n\n`FileChange` — interface with `path: string`, `status: ChangeType`, `oldPath?: string`; represents single file change from git diff.\n\n`ChangeDetectionResult` — interface with `currentCommit: string`, `baseCommit: string`, `changes: FileChange[]`, `includesUncommitted: boolean`; encapsulates change detection output.\n\n`ChangeDetectionOptions` — interface with `includeUncommitted?: boolean`; controls whether to include staged and working directory changes in detection.\n\n## Project Directory Structure\n\n<project-structure>\n./\n  .npmrc\n  CONTRIBUTING.md\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  pages.yml\n  publish.yml\nhooks/\n  are-check-update.js\n  are-context-loader.js\n  opencode-are-check-update.js\nscripts/\n  analyze-agents-compression.sh\n  analyze-sum-ratios.sh\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  codex.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/providers/\n  subprocess.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  rebuild.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/core/\n  index.ts\n  logger.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  claude-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  orchestrator.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/rebuild/\n  checkpoint.ts\n  index.ts\n  orchestrator.ts\n  output-parser.ts\n  prompts.ts\n  spec-reader.ts\n  types.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>",
      "systemPrompt": "You are generating an AGENTS.md file — a directory-level overview for AI coding assistants.\n\nCRITICAL: Output ONLY the raw markdown content. No code fences, no preamble, no explanations, no conversational text. Your entire response IS the AGENTS.md file content.\n\nOUTPUT FORMAT:\n- First line MUST be exactly: <!-- Generated by agents-reverse-engineer -->\n- Use a # heading with the directory name\n- Write a one-paragraph purpose statement for the directory\nusage\nADAPTIVE SECTIONS:\nAnalyze the directory contents and choose the most relevant sections. Do NOT use a fixed template. Instead, select sections that best document this specific directory for an AI that needs to reconstruct or extend the project.\n\nConsider these section types (choose what applies):\n- **Contents**: Group files by purpose/category under ## headings. For each file: markdown link [filename](./filename) and a one-line description.\n- **Subdirectories**: If subdirectories exist, list them with links [dirname/](./dirname/) and brief summaries.\n- **Architecture / Data Flow**: If files form a pipeline, request/response chain, or layered architecture, document it.\n- **Stack**: If this is a package root (has package.json, Cargo.toml, go.mod, etc.), document the technology stack, key scripts, and entry points.\n- **Structure**: If the directory layout follows a convention (feature-sliced, domain-driven, MVC, etc.), document it.\n- **Patterns**: If files share recurring design patterns (factory, strategy, middleware, barrel re-export), name and document them.\n- **Configuration**: If the directory contains config files, schemas, or environment definitions, document the config surface area.\n- **API Surface**: If the directory exports a public API (barrel index, route definitions, SDK), document the interface contract.\n- **File Relationships**: How files collaborate, depend on each other, or share state.\n- **Behavioral Contracts**: If files contain regex patterns, format specifications, magic constants, or template strings that define observable behavior, collect them in a dedicated section. Preserve verbatim patterns from file summaries — do NOT paraphrase regex into prose. This section is MANDATORY when file summaries contain behavioral artifacts.\n- **Reproduction-Critical Constants**: If file summaries reference annex files (via ## Annex References sections), list them with links. Example: \"Full prompt template text: [templates.annex.sum](./prompts/templates.annex.sum)\". Do NOT reproduce annex content in AGENTS.md — just link to it.\n- **Workflow & Conventions**: If file summaries contain contribution guidelines, PR conventions, commit standards, testing mandates, tool usage rules, AI agent instructions, or code conventions (naming, formatting, linting), collect them into actionable directives. Source from CONTRIBUTING.md, CI configs, PR templates, linter/formatter configs, README dev sections. Present as concrete rules (\"commits must use conventionalcommits format\", \"use camelCase for functions, PascalCase for types\") not vague descriptions (\"the project has guidelines\").\n\nChoose any relevant sections or create your own based on the directory contents. The goal is to provide a comprehensive overview that captures the essence of the directory's role in the project and how its files work together, with a focus on what an AI coding assistant would need to know to effectively interact with this code.\n\nSCOPE:\n- AGENTS.md is a NAVIGATIONAL INDEX — help an AI find the right file quickly\n- Focus on: what each file does, how files relate, directory-level patterns\n- Do NOT reproduce full architecture sections — those belong in the root CLAUDE.md\n\nPATH ACCURACY (MANDATORY):\n- When referencing files or modules outside this directory, use ONLY paths from the \"Import Map\" section\n- Do NOT invent, rename, or guess module paths — if a path isn't in the Import Map, don't reference it\n- Use the exact directory names from \"Project Directory Structure\" — do NOT rename directories\n  (e.g., if the directory is called \"cli\", write \"src/cli/\", NOT \"src/commands/\")\n- Cross-module references must use the specifier format from actual import statements\n  (e.g., \"../generation/writers/sum.js\", NOT \"../fs/sum-file.js\")\n- If you are unsure about a path, omit the cross-reference rather than guessing\n\nCONSISTENCY (MANDATORY):\n- Do not contradict yourself within the same document\n- If you describe a technique (e.g., \"regex-based\"), do not call it something else later (e.g., \"AST-based\")\n- When stating version numbers, engines, or config fields, use ONLY values present in the file summaries\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this directory\", \"this module\", \"provides\", \"responsible for\", \"is used to\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Per-file descriptions in Contents sections: 1-2 sentences maximum. Reference key symbols but do not reproduce full summaries.\n- Behavioral contracts (regex patterns, format specs, constants) belong in a separate Behavioral Contracts section, not in per-file descriptions.\n- Annex file references: link to .annex.sum files, do not inline their content. One line per annex reference.\n- Subdirectory descriptions: 1-2 sentences maximum. Capture the directory's role, not its full contents.\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- Key exported symbols from file summaries MUST appear in the directory overview\n- Preserve exact casing of identifiers\n\nUSER NOTES:\n- If \"User Notes\" are provided in the prompt, they contain user-defined instructions that will be automatically prepended to your output\n- Do NOT repeat or paraphrase user notes in your generated content — they are included separately\n- You may reference information from user notes for context",
      "response": "<!-- Generated by agents-reverse-engineer -->\n\n# src/change-detection\n\nThe change-detection directory provides git-based file modification tracking and integrity verification for the agents-reverse-engineer system. It detects added, modified, deleted, and renamed files between commits, supports uncommitted changes detection, and computes SHA-256 content hashes for cache validation and change verification in incremental documentation workflows.\n\n## Contents\n\n### Functions\n- [detector.ts](./detector.ts) — `isGitRepo()`, `getCurrentCommit()`, `getChangedFiles()`, `computeContentHash()`, `computeContentHashFromString()`; implements git diff parsing with rename detection (`-M` flag, 50% similarity threshold) and optional uncommitted change tracking (deduplicates across staged, modified, deleted, untracked).\n- [index.ts](./index.ts) — Barrel re-export of detector and types; public API surface for incremental update workflows.\n\n### Types\n- [types.ts](./types.ts) — `ChangeType` (added|modified|deleted|renamed), `FileChange` (path, status, oldPath?), `ChangeDetectionResult` (currentCommit, baseCommit, changes[], includesUncommitted), `ChangeDetectionOptions` (includeUncommitted?).\n\n## Data Flow\n\n`getChangedFiles(projectRoot, baseCommit, options)` → `git diff --name-status -M baseCommit HEAD` (parses status codes: A, M, D, R* with old/new paths) + optional `git status` (stage/working tree) → deduplicates and maps to `FileChange[]` → returns `ChangeDetectionResult`.\n\nUncommitted tracking merges `status.modified`, `status.deleted`, `status.not_added`, `status.staged`, filtering duplicates by path to avoid redundant entries.\n\n## Behavioral Contracts\n\n**Rename detection:** Status prefix `R` followed by percentage (R100, R95, etc.); old/new paths extracted from tab-separated parts[1] and parts[last].\n\n**Diff output format:** `STATUS\\tFILE` (or `STATUS\\tOLD\\tNEW` for renames); non-empty lines split on tab character.\n\n**Content hash algorithm:** SHA-256 hex digest via `crypto.createHash('sha256')` on file buffer or string.\n\n## Dependencies\n\n`simple-git` (checkIsRepo, revparse, diff, status); `node:crypto.createHash()`; `node:fs/promises.readFile()`.",
      "model": "claude-haiku-4-5-20251001",
      "inputTokens": 3,
      "outputTokens": 589,
      "cacheReadTokens": 17512,
      "cacheCreationTokens": 2932,
      "latencyMs": 24940,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": []
    },
    {
      "timestamp": "2026-02-14T13:49:24.300Z",
      "prompt": "Generate AGENTS.md for directory: \"src/imports\" (imports)\n\n## File Summaries (3 files)\n\n### extractor.ts\n**Purpose:** extractor.ts parses TypeScript/JavaScript import statements from source files and formats them for LLM consumption.\n\n**extractor.ts parses TypeScript/JavaScript import statements from source files and formats them for LLM consumption.**\n\n## Exports\n\n- `extractImports(sourceContent: string): ImportEntry[]` — extracts import entries from source content using regex matching.\n- `extractDirectoryImports(dirPath: string, fileNames: string[]): Promise<FileImports[]>` — reads and classifies imports across multiple files, filtering to first 100 lines per file; categorizes as internal (`./`) or external (`../`); skips bare npm packages and `node:` builtins.\n- `formatImportMap(fileImports: FileImports[]): string` — formats FileImports array as structured text block for LLM prompts, grouping by file with `specifier → symbols` format and `(type)` tag for type-only imports.\n\n## Behavioral Contracts\n\n**IMPORT_REGEX** pattern (global, multiline):\n```\n/^import\\s+(type\\s+)?(?:\\{([^}]*)\\}|(\\*\\s+as\\s+\\w+)|(\\w+))\\s+from\\s+['\"]([^'\"]+)['\"]/gm\n```\nCaptures: (1) `type` keyword, (2) named symbols `{...}`, (3) namespace `* as name`, (4) default import, (5) module specifier.\n\n**Import extraction logic**: parses Group 2 (named) via `.split(',').map(s => s.trim().replace(/\\s+as\\s+\\w+/, ''))` to strip aliasing; Group 3 (namespace) via `.replace('* as ', '')` to isolate name; Group 4 (default) used as-is.\n\n**Filter rules**: `extractDirectoryImports` skips specifiers not starting with `.` or `..` (excludes npm packages and `node:` builtins); classifies internal as `./`, external as `../`.\n\n**Performance optimization**: reads only first 100 lines per file (imports at top).\n\n**Format output template**: `{fileName}:\\n  {specifier} → {symbols}{typeTag}\\n\\n` with `typeTag = ' (type)'` for type-only imports.\n### index.ts\n**Purpose:** src/imports/index.ts re-exports import extraction utilities and type definitions from submodules.\n\n**src/imports/index.ts re-exports import extraction utilities and type definitions from submodules.**\n\n## Exports\n\n- `extractImports` — extracts imports from individual files\n- `extractDirectoryImports` — extracts imports from directory trees\n- `formatImportMap` — formats import maps for output\n- `ImportEntry` — type for individual import entries\n- `FileImports` — type for file-level import collections\n\n## Module Structure\n\nBarrel export aggregating `./extractor.js` (functions) and `./types.js` (types) into single public API surface.\n### types.ts\n**Purpose:** types.ts defines TypeScript interfaces for representing parsed import statements and file-level import collections.\n\n**types.ts defines TypeScript interfaces for representing parsed import statements and file-level import collections.**\n\n## Exported Types\n\n`ImportEntry` — represents single import statement with `specifier: string`, `symbols: string[]`, `typeOnly: boolean`.\n\n`FileImports` — represents all imports from source file with `fileName: string`, `externalImports: ImportEntry[]`, `internalImports: ImportEntry[]`.\n\n## Design\n\nSeparates external vs internal imports by origin (other modules vs same directory). Type-only imports flagged via `typeOnly` boolean. Symbols captured as string array to preserve import names.\n\n## Project Directory Structure\n\n<project-structure>\n./\n  .npmrc\n  CONTRIBUTING.md\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  pages.yml\n  publish.yml\nhooks/\n  are-check-update.js\n  are-context-loader.js\n  opencode-are-check-update.js\nscripts/\n  analyze-agents-compression.sh\n  analyze-sum-ratios.sh\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  codex.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/providers/\n  subprocess.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  rebuild.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/core/\n  index.ts\n  logger.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  claude-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  orchestrator.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/rebuild/\n  checkpoint.ts\n  index.ts\n  orchestrator.ts\n  output-parser.ts\n  prompts.ts\n  spec-reader.ts\n  types.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>",
      "systemPrompt": "You are generating an AGENTS.md file — a directory-level overview for AI coding assistants.\n\nCRITICAL: Output ONLY the raw markdown content. No code fences, no preamble, no explanations, no conversational text. Your entire response IS the AGENTS.md file content.\n\nOUTPUT FORMAT:\n- First line MUST be exactly: <!-- Generated by agents-reverse-engineer -->\n- Use a # heading with the directory name\n- Write a one-paragraph purpose statement for the directory\nusage\nADAPTIVE SECTIONS:\nAnalyze the directory contents and choose the most relevant sections. Do NOT use a fixed template. Instead, select sections that best document this specific directory for an AI that needs to reconstruct or extend the project.\n\nConsider these section types (choose what applies):\n- **Contents**: Group files by purpose/category under ## headings. For each file: markdown link [filename](./filename) and a one-line description.\n- **Subdirectories**: If subdirectories exist, list them with links [dirname/](./dirname/) and brief summaries.\n- **Architecture / Data Flow**: If files form a pipeline, request/response chain, or layered architecture, document it.\n- **Stack**: If this is a package root (has package.json, Cargo.toml, go.mod, etc.), document the technology stack, key scripts, and entry points.\n- **Structure**: If the directory layout follows a convention (feature-sliced, domain-driven, MVC, etc.), document it.\n- **Patterns**: If files share recurring design patterns (factory, strategy, middleware, barrel re-export), name and document them.\n- **Configuration**: If the directory contains config files, schemas, or environment definitions, document the config surface area.\n- **API Surface**: If the directory exports a public API (barrel index, route definitions, SDK), document the interface contract.\n- **File Relationships**: How files collaborate, depend on each other, or share state.\n- **Behavioral Contracts**: If files contain regex patterns, format specifications, magic constants, or template strings that define observable behavior, collect them in a dedicated section. Preserve verbatim patterns from file summaries — do NOT paraphrase regex into prose. This section is MANDATORY when file summaries contain behavioral artifacts.\n- **Reproduction-Critical Constants**: If file summaries reference annex files (via ## Annex References sections), list them with links. Example: \"Full prompt template text: [templates.annex.sum](./prompts/templates.annex.sum)\". Do NOT reproduce annex content in AGENTS.md — just link to it.\n- **Workflow & Conventions**: If file summaries contain contribution guidelines, PR conventions, commit standards, testing mandates, tool usage rules, AI agent instructions, or code conventions (naming, formatting, linting), collect them into actionable directives. Source from CONTRIBUTING.md, CI configs, PR templates, linter/formatter configs, README dev sections. Present as concrete rules (\"commits must use conventionalcommits format\", \"use camelCase for functions, PascalCase for types\") not vague descriptions (\"the project has guidelines\").\n\nChoose any relevant sections or create your own based on the directory contents. The goal is to provide a comprehensive overview that captures the essence of the directory's role in the project and how its files work together, with a focus on what an AI coding assistant would need to know to effectively interact with this code.\n\nSCOPE:\n- AGENTS.md is a NAVIGATIONAL INDEX — help an AI find the right file quickly\n- Focus on: what each file does, how files relate, directory-level patterns\n- Do NOT reproduce full architecture sections — those belong in the root CLAUDE.md\n\nPATH ACCURACY (MANDATORY):\n- When referencing files or modules outside this directory, use ONLY paths from the \"Import Map\" section\n- Do NOT invent, rename, or guess module paths — if a path isn't in the Import Map, don't reference it\n- Use the exact directory names from \"Project Directory Structure\" — do NOT rename directories\n  (e.g., if the directory is called \"cli\", write \"src/cli/\", NOT \"src/commands/\")\n- Cross-module references must use the specifier format from actual import statements\n  (e.g., \"../generation/writers/sum.js\", NOT \"../fs/sum-file.js\")\n- If you are unsure about a path, omit the cross-reference rather than guessing\n\nCONSISTENCY (MANDATORY):\n- Do not contradict yourself within the same document\n- If you describe a technique (e.g., \"regex-based\"), do not call it something else later (e.g., \"AST-based\")\n- When stating version numbers, engines, or config fields, use ONLY values present in the file summaries\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this directory\", \"this module\", \"provides\", \"responsible for\", \"is used to\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Per-file descriptions in Contents sections: 1-2 sentences maximum. Reference key symbols but do not reproduce full summaries.\n- Behavioral contracts (regex patterns, format specs, constants) belong in a separate Behavioral Contracts section, not in per-file descriptions.\n- Annex file references: link to .annex.sum files, do not inline their content. One line per annex reference.\n- Subdirectory descriptions: 1-2 sentences maximum. Capture the directory's role, not its full contents.\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- Key exported symbols from file summaries MUST appear in the directory overview\n- Preserve exact casing of identifiers\n\nUSER NOTES:\n- If \"User Notes\" are provided in the prompt, they contain user-defined instructions that will be automatically prepended to your output\n- Do NOT repeat or paraphrase user notes in your generated content — they are included separately\n- You may reference information from user notes for context",
      "response": "<!-- Generated by agents-reverse-engineer -->\n\n# src/imports\n\nThe imports directory exports utilities for parsing TypeScript/JavaScript import statements from source files and formatting import maps for LLM consumption. It provides regex-based extraction across file trees with classification of internal vs. external imports.\n\n## Contents\n\n### Core Functions\n- [extractor.ts](./extractor.ts) — `extractImports()` parses individual files via IMPORT_REGEX; `extractDirectoryImports()` reads directories filtering to first 100 lines per file; `formatImportMap()` outputs structured text for LLM prompts with `specifier → symbols` grouping and `(type)` tags.\n- [index.ts](./index.ts) — Barrel export aggregating `extractImports`, `extractDirectoryImports`, `formatImportMap`, `ImportEntry`, and `FileImports`.\n- [types.ts](./types.ts) — `ImportEntry` (specifier, symbols[], typeOnly flag) and `FileImports` (fileName, externalImports[], internalImports[]) interfaces.\n\n## Behavioral Contracts\n\n**IMPORT_REGEX** (global, multiline):\n```\n/^import\\s+(type\\s+)?(?:\\{([^}]*)\\}|(\\*\\s+as\\s+\\w+)|(\\w+))\\s+from\\s+['\"]([^'\"]+)['\"]/gm\n```\nCaptures type keyword, named imports `{...}`, namespace `* as name`, default import, and module specifier from lines starting with `import`.\n\n**Symbol parsing**: named imports via `.split(',').map(s => s.trim().replace(/\\s+as\\s+\\w+/, ''))` strips aliases; namespace via `.replace('* as ', '')` isolates name; default used as-is.\n\n**Filter rules**: `extractDirectoryImports` skips specifiers not starting with `.` or `..` (excludes npm packages and `node:` builtins); classifies `./` as internal, `../` as external.\n\n**Performance**: reads only first 100 lines per file (imports clustered at top).\n\n**Format template**: `{fileName}:\\n  {specifier} → {symbols}{typeTag}\\n\\n` with `typeTag = ' (type)'` for type-only imports.\n\n## Type Surface\n\n- **ImportEntry**: `{ specifier: string; symbols: string[]; typeOnly: boolean }`\n- **FileImports**: `{ fileName: string; externalImports: ImportEntry[]; internalImports: ImportEntry[] }`",
      "model": "claude-haiku-4-5-20251001",
      "inputTokens": 3,
      "outputTokens": 598,
      "cacheReadTokens": 17512,
      "cacheCreationTokens": 2827,
      "latencyMs": 25269,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": []
    },
    {
      "timestamp": "2026-02-14T13:49:24.320Z",
      "prompt": "Generate AGENTS.md for directory: \"src/discovery\" (discovery)\n\n## File Summaries (3 files)\n\n### run.ts\n**Purpose:** run.ts orchestrates the file discovery pipeline by composing directory walking with sequential filter application to ...\n\n**run.ts orchestrates the file discovery pipeline by composing directory walking with sequential filter application to identify includable vs. excludable files.**\n\n## Exports\n\n`discoverFiles(root: string, config: DiscoveryConfig, options?: DiscoverFilesOptions): Promise<FilterResult>` — walks directory tree from `root`, instantiates four filters (gitignore, vendor, binary, custom) in strict order, applies them sequentially via `applyFilters()`, returns `FilterResult` with included/excluded file lists and filter attribution.\n\n`DiscoveryConfig` interface — config subset with `exclude.vendorDirs[]`, `exclude.binaryExtensions[]`, `exclude.patterns[]`, `options.maxFileSize`, `options.followSymlinks`.\n\n`DiscoverFilesOptions` interface — optional `tracer?: ITraceWriter`, `debug?: boolean` for pipeline instrumentation.\n\n## Dependencies & Integration\n\nImports `walkDirectory` from `./walker.js` (directory traversal); imports `applyFilters`, `createGitignoreFilter`, `createVendorFilter`, `createBinaryFilter`, `createCustomFilter` from `./filters/index.js` (filter composition); imports `FilterResult` from `./types.js` (return type); imports `ITraceWriter` from `../orchestration/trace.js` (trace instrumentation interface).\n\n## Filter Pipeline Architecture\n\n`discoverFiles()` instantiates filters in fixed order: gitignore (async) → vendor → binary → custom. All four filters are composed into array and passed to `applyFilters()`. Binary filter receives `maxFileSize` and `additionalExtensions` config tuple. Custom filter receives patterns array and root path. Gitignore filter is async-created; others are synchronous.\n### types.ts\n**Purpose:** src/discovery/types.ts defines core interfaces for the file discovery pipeline: FileFilter, ExcludedFile, FilterResul...\n\n**src/discovery/types.ts defines core interfaces for the file discovery pipeline: FileFilter, ExcludedFile, FilterResult, WalkerOptions.**\n\n## Exported Types\n\n`FileFilter` — filter interface with `name: string` and `shouldExclude(path: string, stats?: Stats): Promise<boolean> | boolean`. Determines file inclusion/exclusion in discovery chain.\n\n`ExcludedFile` — record with `path: string`, `reason: string`, `filter: string`. Tracks excluded files with reasoning.\n\n`FilterResult` — result object with `included: string[]` (passed files) and `excluded: ExcludedFile[]` (rejected files with reasons).\n\n`WalkerOptions` — directory walker config: `cwd: string` (root path, required), `followSymlinks?: boolean` (default: false), `dot?: boolean` (default: true, include dotfiles).\n\n## Key Design\n\nFileFilter uses asynchronous-first pattern: `shouldExclude()` returns `Promise<boolean> | boolean`, allowing both sync and async filters (e.g., size checks, I/O operations). Filter name field required for logging excluded file sources.\n\n## Defaults & Conventions\n\n`followSymlinks` defaults to false per CONTEXT.md symlink exclusion policy. `dot` defaults to true to capture dotfiles for analysis (configuration files, git metadata).\n### walker.ts\n**Purpose:** walker.ts exports walkDirectory() to traverse directories and collect absolute file paths via fast-glob.\n\n**walker.ts exports walkDirectory() to traverse directories and collect absolute file paths via fast-glob.**\n\n## Exported Symbols\n\n- `walkDirectory(options: WalkerOptions): Promise<string[]>` — Returns all files in directory tree as absolute paths; filtering (gitignore, binary, vendor) applied separately via filter chain.\n\n## Configuration & Behavior\n\n`walkDirectory` accepts `WalkerOptions` and passes to `fg.glob('**/*', {...})` with:\n- `cwd` from options (working directory)\n- `absolute: true` (returns absolute paths)\n- `onlyFiles: true` (excludes directories)\n- `dot: options.dot ?? true` (includes dotfiles by default)\n- `followSymlinks: options.followSymlinks ?? false` (no symlink following by default)\n- `suppressErrors: true` (ignores permission errors per RESEARCH.md)\n- `ignore: ['**/.git/**']` (always excludes .git for performance)\n\n## Design Pattern\n\nSeparation of concerns: walker only collects files; all filtering (gitignore, binary detection, vendor directories, custom patterns) delegated to filter chain module.\n\n## Import Map (verified — use these exact paths)\n\nrun.ts:\n  ../orchestration/trace.js → ITraceWriter (type)\n\n## Project Directory Structure\n\n<project-structure>\n./\n  .npmrc\n  CONTRIBUTING.md\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  pages.yml\n  publish.yml\nhooks/\n  are-check-update.js\n  are-context-loader.js\n  opencode-are-check-update.js\nscripts/\n  analyze-agents-compression.sh\n  analyze-sum-ratios.sh\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  codex.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/providers/\n  subprocess.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  rebuild.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/core/\n  index.ts\n  logger.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  claude-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  orchestrator.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/rebuild/\n  checkpoint.ts\n  index.ts\n  orchestrator.ts\n  output-parser.ts\n  prompts.ts\n  spec-reader.ts\n  types.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\n\n## Subdirectories\n\n### filters/\n<!-- Generated by agents-reverse-engineer v0.9.10 -->\n\n# src/discovery/filters\n\nThe filters directory implements a composable chain of file exclusion filters for the discovery phase, combining gitignore patterns, vendor directory detection, binary file identification, and custom patterns into a unified filtering pipeline with bounded concurrency and trace recording.\n\n## Contents\n\n**Filter Factories**\n\n- [binary.ts](./binary.ts) — Two-phase binary detection via `BINARY_EXTENSIONS` Set (O(1) extension check) and `isBinaryfile` content analysis for unknown types; exports `createBinaryFilter(options: BinaryFilterOptions): FileFilter` with configurable `maxFileSize` threshold.\n\n- [vendor.ts](./vendor.ts) — Excludes third-party code via `DEFAULT_VENDOR_DIRS` (node_modules, dist, build, etc.); exports `createVendorFilter(vendorDirs: string[]): FileFilter` with dual-mode matching for single-segment directory names and multi-segment path patterns.\n\n- [gitignore.ts](./gitignore.ts) — Loads `.gitignore` from root directory and wraps `ignore` library patterns; exports async `createGitignoreFilter(root: string): Promise<FileFilter>` that converts absolute paths to relative paths for pattern matching.\n\n- [custom.ts](./custom.ts) — Applies caller-provided gitignore-style patterns via `ignore` library; exports `createCustomFilter(patterns: string[], root: string): FileFilter` for runtime pattern registration.\n\n- [index.ts](./index.ts) — Orchestrates filter chain execution with `applyFilters(files: string[], filters: FileFilter[], options?): Promise<FilterResult>` applying filters in order with short-circuit semantics, bounded concurrency (`CONCURRENCY = 30`), and trace emission via `ITraceWriter`.\n\n## Filter Chain Architecture\n\n`applyFilters()` processes file arrays through an ordered filter chain where each `FileFilter` has `name: string` and `shouldExclude(absolutePath): Promise<boolean> | boolean`. Evaluation stops at first truthy exclusion per file; results returned as `{ included: string[], excluded: ExcludedFile[] }` with per-filter statistics tracked for trace events (`filter:applied`).\n\nConcurrency bounded via worker pool from shared iterator to prevent file descriptor exhaustion during binary content I/O (critical for `isBinaryFile` checks in `createBinaryFilter`).\n\n## Behavioral Contracts\n\n**Path Normalization:**\n- Absolute paths → relative paths via `path.relative(normalizedRoot, absolutePath)`\n- Relative paths starting with `'..'` or empty strings treated as outside-root and never excluded\n- OS-specific path separator normalization in vendor filter via `path.sep`\n\n**Extension Detection (binary.ts):**\n- Case-insensitive via `path.extname().toLowerCase()`\n- 65 binary extensions pre-computed into `BINARY_EXTENSIONS` Set for O(1) lookup\n\n**Vendor Directory Matching (vendor.ts):**\n- Single-segment names ('node_modules') matched anywhere in path hierarchy via `absolutePath.split(path.sep)` segment checking\n- Multi-segment patterns ('apps/vendor') matched via substring inclusion after OS normalization\n\n**Pattern Matching (gitignore, custom):**\n- Empty pattern arrays return `false` (pass all files)\n- Missing .gitignore file silently handled (no error thrown)\n\n## File Relationships\n\n- `binary.ts`, `vendor.ts`, `gitignore.ts`, `custom.ts` each implement `FileFilter` interface from `../types.js`\n- `index.ts` imports and re-exports all four filter factories, then chains their execution via `applyFilters()`\n- `applyFilters()` consumes filter `name` field for trace event attribution and exclusion reason recording\n- Trace events emitted to optional `ITraceWriter` (from `../../orchestration/trace.js`) with per-filter match counts\n- Optional `logger` parameter (type `Logger` from `../../core/logger.js`) defaults to `nullLogger` for debug output",
      "systemPrompt": "You are generating an AGENTS.md file — a directory-level overview for AI coding assistants.\n\nCRITICAL: Output ONLY the raw markdown content. No code fences, no preamble, no explanations, no conversational text. Your entire response IS the AGENTS.md file content.\n\nOUTPUT FORMAT:\n- First line MUST be exactly: <!-- Generated by agents-reverse-engineer -->\n- Use a # heading with the directory name\n- Write a one-paragraph purpose statement for the directory\nusage\nADAPTIVE SECTIONS:\nAnalyze the directory contents and choose the most relevant sections. Do NOT use a fixed template. Instead, select sections that best document this specific directory for an AI that needs to reconstruct or extend the project.\n\nConsider these section types (choose what applies):\n- **Contents**: Group files by purpose/category under ## headings. For each file: markdown link [filename](./filename) and a one-line description.\n- **Subdirectories**: If subdirectories exist, list them with links [dirname/](./dirname/) and brief summaries.\n- **Architecture / Data Flow**: If files form a pipeline, request/response chain, or layered architecture, document it.\n- **Stack**: If this is a package root (has package.json, Cargo.toml, go.mod, etc.), document the technology stack, key scripts, and entry points.\n- **Structure**: If the directory layout follows a convention (feature-sliced, domain-driven, MVC, etc.), document it.\n- **Patterns**: If files share recurring design patterns (factory, strategy, middleware, barrel re-export), name and document them.\n- **Configuration**: If the directory contains config files, schemas, or environment definitions, document the config surface area.\n- **API Surface**: If the directory exports a public API (barrel index, route definitions, SDK), document the interface contract.\n- **File Relationships**: How files collaborate, depend on each other, or share state.\n- **Behavioral Contracts**: If files contain regex patterns, format specifications, magic constants, or template strings that define observable behavior, collect them in a dedicated section. Preserve verbatim patterns from file summaries — do NOT paraphrase regex into prose. This section is MANDATORY when file summaries contain behavioral artifacts.\n- **Reproduction-Critical Constants**: If file summaries reference annex files (via ## Annex References sections), list them with links. Example: \"Full prompt template text: [templates.annex.sum](./prompts/templates.annex.sum)\". Do NOT reproduce annex content in AGENTS.md — just link to it.\n- **Workflow & Conventions**: If file summaries contain contribution guidelines, PR conventions, commit standards, testing mandates, tool usage rules, AI agent instructions, or code conventions (naming, formatting, linting), collect them into actionable directives. Source from CONTRIBUTING.md, CI configs, PR templates, linter/formatter configs, README dev sections. Present as concrete rules (\"commits must use conventionalcommits format\", \"use camelCase for functions, PascalCase for types\") not vague descriptions (\"the project has guidelines\").\n\nChoose any relevant sections or create your own based on the directory contents. The goal is to provide a comprehensive overview that captures the essence of the directory's role in the project and how its files work together, with a focus on what an AI coding assistant would need to know to effectively interact with this code.\n\nSCOPE:\n- AGENTS.md is a NAVIGATIONAL INDEX — help an AI find the right file quickly\n- Focus on: what each file does, how files relate, directory-level patterns\n- Do NOT reproduce full architecture sections — those belong in the root CLAUDE.md\n\nPATH ACCURACY (MANDATORY):\n- When referencing files or modules outside this directory, use ONLY paths from the \"Import Map\" section\n- Do NOT invent, rename, or guess module paths — if a path isn't in the Import Map, don't reference it\n- Use the exact directory names from \"Project Directory Structure\" — do NOT rename directories\n  (e.g., if the directory is called \"cli\", write \"src/cli/\", NOT \"src/commands/\")\n- Cross-module references must use the specifier format from actual import statements\n  (e.g., \"../generation/writers/sum.js\", NOT \"../fs/sum-file.js\")\n- If you are unsure about a path, omit the cross-reference rather than guessing\n\nCONSISTENCY (MANDATORY):\n- Do not contradict yourself within the same document\n- If you describe a technique (e.g., \"regex-based\"), do not call it something else later (e.g., \"AST-based\")\n- When stating version numbers, engines, or config fields, use ONLY values present in the file summaries\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this directory\", \"this module\", \"provides\", \"responsible for\", \"is used to\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Per-file descriptions in Contents sections: 1-2 sentences maximum. Reference key symbols but do not reproduce full summaries.\n- Behavioral contracts (regex patterns, format specs, constants) belong in a separate Behavioral Contracts section, not in per-file descriptions.\n- Annex file references: link to .annex.sum files, do not inline their content. One line per annex reference.\n- Subdirectory descriptions: 1-2 sentences maximum. Capture the directory's role, not its full contents.\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- Key exported symbols from file summaries MUST appear in the directory overview\n- Preserve exact casing of identifiers\n\nUSER NOTES:\n- If \"User Notes\" are provided in the prompt, they contain user-defined instructions that will be automatically prepended to your output\n- Do NOT repeat or paraphrase user notes in your generated content — they are included separately\n- You may reference information from user notes for context",
      "response": "<!-- Generated by agents-reverse-engineer -->\n\n# src/discovery\n\nThe discovery directory orchestrates file enumeration and exclusion, walking directory trees and filtering results through a composable chain of gitignore, vendor, binary, and custom pattern filters with trace instrumentation and bounded concurrency.\n\n## Contents\n\n**Core Pipeline**\n\n- [run.ts](./run.ts) — Entry point: `discoverFiles(root, config, options)` walks directory via `walkDirectory()`, instantiates filters in order (gitignore → vendor → binary → custom), chains them through `applyFilters()`, returns `FilterResult` with included/excluded files and filter attribution.\n\n- [walker.ts](./walker.ts) — Low-level traversal: `walkDirectory(options: WalkerOptions)` delegates to `fast-glob` with `absolute: true`, `onlyFiles: true`, `dot: true` (default), `followSymlinks: false` (default), `suppressErrors: true`, and hardcoded `.git/**` exclusion; separation of concerns keeps walker filter-agnostic.\n\n- [types.ts](./types.ts) — Interfaces: `FileFilter` (async-first `shouldExclude()` returning `Promise<boolean> | boolean`), `ExcludedFile` (path + reason + filter name), `FilterResult` (included/excluded arrays), `WalkerOptions` (cwd, followSymlinks, dot).\n\n## Subdirectories\n\n- [filters/](./filters/) — Filter chain factories (`createBinaryFilter`, `createVendorFilter`, `createGitignoreFilter`, `createCustomFilter`) and composition: `applyFilters(files, filters, options)` orchestrates ordered evaluation with short-circuit semantics, bounded concurrency (CONCURRENCY=30), and trace emission.\n\n## Filter Pipeline Architecture\n\n`discoverFiles()` constructs filter array in strict order: async `createGitignoreFilter(root)` → sync `createVendorFilter(vendorDirs)` → sync `createBinaryFilter(maxFileSize)` → sync `createCustomFilter(patterns, root)`. All filters passed to `applyFilters()` which evaluates per-file in order, stops at first exclusion, records reason and filter name, emits trace events (`filter:applied` with per-filter statistics).\n\nBinary filter receives `maxFileSize` and `additionalExtensions` config tuple for two-phase detection (extension Set lookup, then `isBinaryFile` content analysis). Custom filter receives patterns array and root path for gitignore-style pattern application via `ignore` library.\n\n## Configuration Surface\n\n`DiscoveryConfig` interface: `exclude.vendorDirs[]`, `exclude.binaryExtensions[]`, `exclude.patterns[]`, `options.maxFileSize`, `options.followSymlinks`.\n\n`DiscoverFilesOptions` interface: optional `tracer?: ITraceWriter`, `debug?: boolean` for pipeline instrumentation.\n\n## Key Design Patterns\n\n**Separation of Concerns:** Walker collects all files (no filtering); filters applied separately via composable chain.\n\n**Async-First Filters:** `FileFilter.shouldExclude()` returns `Promise<boolean> | boolean`, allowing sync filters (gitignore, vendor, binary extension check) and async filters (binary content I/O via `isBinaryFile`).\n\n**Trace Attribution:** Each `ExcludedFile` record includes `filter: string` (e.g., \"gitignore\", \"vendor\", \"binary\") for provenance tracking; `applyFilters()` emits `filter:applied` trace events with per-filter match counts.\n\n**Bounded Concurrency:** Worker pool via shared iterator prevents file descriptor exhaustion during binary content checks (`isBinaryFile` I/O); CONCURRENCY=30 configurable in `filters/index.ts`.\n\n## Behavioral Contracts\n\n**Path Normalization:**\n- Absolute paths → relative via `path.relative(normalizedRoot, absolutePath)` for pattern matching\n- Relative paths starting with `'..'` or empty treated as outside-root (never excluded)\n- OS-specific `path.sep` normalization in vendor filter for cross-platform matching\n\n**Binary Extension Detection:**\n- Case-insensitive via `path.extname().toLowerCase()`\n- 65 pre-computed binary extensions in `BINARY_EXTENSIONS` Set (O(1) lookup)\n\n**Vendor Directory Matching:**\n- Single-segment names ('node_modules') matched anywhere in hierarchy via `absolutePath.split(path.sep)` segment checking\n- Multi-segment patterns ('apps/vendor') matched via substring inclusion after normalization\n\n**Pattern Matching (gitignore, custom):**\n- Empty pattern arrays return `false` (pass all files)\n- Missing .gitignore silently handled (no error)\n\n## File Relationships\n\n- `run.ts` orchestrates: `walkDirectory()` → `applyFilters([gitignore, vendor, binary, custom])`\n- `walker.ts` produces file list; filters consume it sequentially\n- `filters/index.ts` chains four factories and exports `applyFilters()` orchestrator\n- All filters implement `FileFilter` interface from `types.ts`\n- Trace events routed to optional `ITraceWriter` (from `../orchestration/trace.js`)",
      "model": "claude-haiku-4-5-20251001",
      "inputTokens": 3,
      "outputTokens": 1221,
      "cacheReadTokens": 17512,
      "cacheCreationTokens": 4029,
      "latencyMs": 29170,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": []
    },
    {
      "timestamp": "2026-02-14T13:49:24.295Z",
      "prompt": "Generate AGENTS.md for directory: \"src/config\" (config)\n\n## File Summaries (3 files)\n\n### defaults.ts\n**Purpose:** src/config/defaults.ts exports default configuration values and concurrency calculation for agents-reverse-engineer.\n\n**src/config/defaults.ts exports default configuration values and concurrency calculation for agents-reverse-engineer.**\n\n## Exports\n\n`getDefaultConcurrency()` → number: Computes concurrency by clamping `cores * CONCURRENCY_MULTIPLIER` (5) against MIN_CONCURRENCY (2), MAX_CONCURRENCY (20), and memory cap derived from `(totalMemGB * MEMORY_FRACTION) / SUBPROCESS_HEAP_GB` (50% of RAM ÷ 0.512 GB per subprocess).\n\n`DEFAULT_VENDOR_DIRS` → readonly string[]: Directories to exclude: `node_modules`, `vendor`, `.git`, `dist`, `build`, `__pycache__`, `.next`, `venv`, `.venv`, `target`, `.cargo`, `.gradle`, `.agents-reverse-engineer`, `.agents`, `.planning`, `.claude`, `.codex`, `.opencode`, `.gemini`.\n\n`DEFAULT_EXCLUDE_PATTERNS` → readonly string[]: Gitignore-syntax patterns excluding AI assistant docs (AGENTS.md, CLAUDE.md, OPENCODE.md, GEMINI.md, variants with `AGENTS.*.md`), lock files (package-lock.json, yarn.lock, pnpm-lock.yaml, Cargo.lock, poetry.lock, go.sum, etc.), dotfiles (.env, .gitignore, .gitattributes), logs (*.log), and summaries (*.sum, **/SKILL.md).\n\n`DEFAULT_BINARY_EXTENSIONS` → readonly string[]: Image (`.png`, `.jpg`, `.gif`, `.webp`), archive (`.zip`, `.tar`, `.gz`), executable (`.exe`, `.dll`, `.so`, `.dylib`), media (`.mp3`, `.mp4`), document (`.pdf`), font (`.woff`, `.ttf`), and compiled (`.class`, `.pyc`) extensions.\n\n`DEFAULT_MAX_FILE_SIZE` → 1048576: 1 MB file size limit.\n\n`DEFAULT_COMPRESSION_RATIO` → 0.25: Target 25% compression for .sum files.\n\n`DEFAULT_CONFIG` → object: Merges all defaults with structure: `exclude.patterns`, `exclude.vendorDirs`, `exclude.binaryExtensions`, `options.followSymlinks` (false), `options.maxFileSize`, `output.colors` (true), `generation.compressionRatio`.\n\n## Constants & Thresholds\n\nCONCURRENCY_MULTIPLIER = 5; MIN_CONCURRENCY = 2; MAX_CONCURRENCY = 20; SUBPROCESS_HEAP_GB = 0.512 GB; MEMORY_FRACTION = 0.5.\n### loader.ts\n**Purpose:** loader.ts loads and validates agent-reverse configuration from `.agents-reverse-engineer/config.yaml`, returning defa...\n\n**loader.ts loads and validates agent-reverse configuration from `.agents-reverse-engineer/config.yaml`, returning defaults when absent.**\n\n## Exports\n\n`findProjectRoot(startDir: string): Promise<string>` — walks directory tree upward to locate `.agents-reverse-engineer/` directory; returns containing directory or startDir if not found.\n\n`ConfigError` — extends Error with `filePath` and optional `cause` properties for configuration parsing/validation failures.\n\n`loadConfig(root: string, options?: {tracer?: ITraceWriter; debug?: boolean; logger?: Logger}): Promise<Config>` — parses and validates config via ConfigSchema; emits `config:loaded` trace event with model, concurrency, configPath; returns Config object or defaults if file missing; throws ConfigError on parse/validation failure.\n\n`configExists(root: string): Promise<boolean>` — checks existence of config.yaml in `.agents-reverse-engineer/`.\n\n`writeDefaultConfig(root: string): Promise<void>` — creates `.agents-reverse-engineer/` directory and writes config.yaml template with formatted comments and inline defaults.\n\n`yamlScalar(value: string): string` — escapes and quotes YAML scalar values containing special chars `*{}\\[\\]?,:#&!|>'\"%@`` ` to prevent misinterpretation.\n\n## Constants\n\n`CONFIG_DIR` = `'.agents-reverse-engineer'` — configuration directory name.\n\n`CONFIG_FILE` = `'config.yaml'` — configuration filename.\n\n## Configuration File Structure\n\nGenerated config includes sections: `exclude` (patterns, vendorDirs, binaryExtensions), `options` (followSymlinks, maxFileSize), `output` (colors), `generation` (compressionRatio: 0.1–1.0), `ai` (backend, model, timeoutMs, maxRetries, concurrency, telemetry.keepRuns).\n\n## Integration\n\nDependencies: ConfigSchema and Config from schema.js; DEFAULT_VENDOR_DIRS, DEFAULT_BINARY_EXTENSIONS, DEFAULT_MAX_FILE_SIZE, DEFAULT_EXCLUDE_PATTERNS, getDefaultConcurrency from defaults.js; ITraceWriter for trace event emission; Logger for debug output. Node fs promises (readFile, writeFile, mkdir, access) and yaml (parse, stringify).\n\n## Error Handling\n\nFile-not-found (ENOENT) triggers default config return; ZodError validation issues formatted as indented path:message list in ConfigError; YAML parse errors wrapped in ConfigError with source error preserved.\n### schema.ts\n**Purpose:** src/config/schema.ts defines Zod validation schemas for .agents-reverse/config.yaml with sensible defaults and export...\n\n**src/config/schema.ts defines Zod validation schemas for .agents-reverse/config.yaml with sensible defaults and exports Config type inferred from ConfigSchema.**\n\n## Exports\n\n- `ConfigSchema` — Zod object schema validating root config; all fields default to empty objects or DEFAULT_* values.\n- `Config` — TypeScript type inferred from ConfigSchema via `z.infer<typeof ConfigSchema>`.\n- `ExcludeSchema`, `ExcludeConfig` — Schema and type for exclude section (patterns, vendorDirs, binaryExtensions arrays).\n- `OptionsSchema`, `OptionsConfig` — Schema and type for options section (followSymlinks: boolean, maxFileSize: number).\n- `OutputSchema`, `OutputConfig` — Schema and type for output section (colors: boolean).\n- `GenerationSchema`, `GenerationConfig` — Schema and type for generation section (compressionRatio: 0.1–1.0, activeVariant: optional string).\n- `AISchema`, `AIConfig` — Schema and type for AI service section (backend enum, model string, timeoutMs, maxRetries, concurrency 1–20, telemetry.keepRuns).\n\n## Configuration Contracts\n\n**compressionRatio** defaults to 0.25 (25% of source); range 0.1–1.0 controls .sum file verbosity; 0.1 = aggressive compression, 0.5 = verbose.\n\n**backend** enum: `['claude', 'codex', 'gemini', 'opencode', 'auto']`; 'auto' detects from PATH.\n\n**concurrency** bounds: 1–20; auto-detected via `getDefaultConcurrency()` if omitted.\n\n**timeoutMs** default: `300_000` (300 seconds for subprocess execution).\n\n**maxRetries** default: 3 for transient errors.\n\n**telemetry.keepRuns** default: 50 (retained run logs on disk).\n\n## Dependencies\n\nImports `DEFAULT_VENDOR_DIRS`, `DEFAULT_BINARY_EXTENSIONS`, `DEFAULT_MAX_FILE_SIZE`, `DEFAULT_EXCLUDE_PATTERNS`, `getDefaultConcurrency` from `./defaults.js`.\n\n## Import Map (verified — use these exact paths)\n\nloader.ts:\n  ../core/logger.js → Logger (type)\n  ../core/logger.js → nullLogger\n  ../orchestration/trace.js → ITraceWriter (type)\n\n## Project Directory Structure\n\n<project-structure>\n./\n  .npmrc\n  CONTRIBUTING.md\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  pages.yml\n  publish.yml\nhooks/\n  are-check-update.js\n  are-context-loader.js\n  opencode-are-check-update.js\nscripts/\n  analyze-agents-compression.sh\n  analyze-sum-ratios.sh\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  codex.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/providers/\n  subprocess.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  rebuild.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/core/\n  index.ts\n  logger.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  claude-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  orchestrator.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/rebuild/\n  checkpoint.ts\n  index.ts\n  orchestrator.ts\n  output-parser.ts\n  prompts.ts\n  spec-reader.ts\n  types.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>",
      "systemPrompt": "You are generating an AGENTS.md file — a directory-level overview for AI coding assistants.\n\nCRITICAL: Output ONLY the raw markdown content. No code fences, no preamble, no explanations, no conversational text. Your entire response IS the AGENTS.md file content.\n\nOUTPUT FORMAT:\n- First line MUST be exactly: <!-- Generated by agents-reverse-engineer -->\n- Use a # heading with the directory name\n- Write a one-paragraph purpose statement for the directory\nusage\nADAPTIVE SECTIONS:\nAnalyze the directory contents and choose the most relevant sections. Do NOT use a fixed template. Instead, select sections that best document this specific directory for an AI that needs to reconstruct or extend the project.\n\nConsider these section types (choose what applies):\n- **Contents**: Group files by purpose/category under ## headings. For each file: markdown link [filename](./filename) and a one-line description.\n- **Subdirectories**: If subdirectories exist, list them with links [dirname/](./dirname/) and brief summaries.\n- **Architecture / Data Flow**: If files form a pipeline, request/response chain, or layered architecture, document it.\n- **Stack**: If this is a package root (has package.json, Cargo.toml, go.mod, etc.), document the technology stack, key scripts, and entry points.\n- **Structure**: If the directory layout follows a convention (feature-sliced, domain-driven, MVC, etc.), document it.\n- **Patterns**: If files share recurring design patterns (factory, strategy, middleware, barrel re-export), name and document them.\n- **Configuration**: If the directory contains config files, schemas, or environment definitions, document the config surface area.\n- **API Surface**: If the directory exports a public API (barrel index, route definitions, SDK), document the interface contract.\n- **File Relationships**: How files collaborate, depend on each other, or share state.\n- **Behavioral Contracts**: If files contain regex patterns, format specifications, magic constants, or template strings that define observable behavior, collect them in a dedicated section. Preserve verbatim patterns from file summaries — do NOT paraphrase regex into prose. This section is MANDATORY when file summaries contain behavioral artifacts.\n- **Reproduction-Critical Constants**: If file summaries reference annex files (via ## Annex References sections), list them with links. Example: \"Full prompt template text: [templates.annex.sum](./prompts/templates.annex.sum)\". Do NOT reproduce annex content in AGENTS.md — just link to it.\n- **Workflow & Conventions**: If file summaries contain contribution guidelines, PR conventions, commit standards, testing mandates, tool usage rules, AI agent instructions, or code conventions (naming, formatting, linting), collect them into actionable directives. Source from CONTRIBUTING.md, CI configs, PR templates, linter/formatter configs, README dev sections. Present as concrete rules (\"commits must use conventionalcommits format\", \"use camelCase for functions, PascalCase for types\") not vague descriptions (\"the project has guidelines\").\n\nChoose any relevant sections or create your own based on the directory contents. The goal is to provide a comprehensive overview that captures the essence of the directory's role in the project and how its files work together, with a focus on what an AI coding assistant would need to know to effectively interact with this code.\n\nSCOPE:\n- AGENTS.md is a NAVIGATIONAL INDEX — help an AI find the right file quickly\n- Focus on: what each file does, how files relate, directory-level patterns\n- Do NOT reproduce full architecture sections — those belong in the root CLAUDE.md\n\nPATH ACCURACY (MANDATORY):\n- When referencing files or modules outside this directory, use ONLY paths from the \"Import Map\" section\n- Do NOT invent, rename, or guess module paths — if a path isn't in the Import Map, don't reference it\n- Use the exact directory names from \"Project Directory Structure\" — do NOT rename directories\n  (e.g., if the directory is called \"cli\", write \"src/cli/\", NOT \"src/commands/\")\n- Cross-module references must use the specifier format from actual import statements\n  (e.g., \"../generation/writers/sum.js\", NOT \"../fs/sum-file.js\")\n- If you are unsure about a path, omit the cross-reference rather than guessing\n\nCONSISTENCY (MANDATORY):\n- Do not contradict yourself within the same document\n- If you describe a technique (e.g., \"regex-based\"), do not call it something else later (e.g., \"AST-based\")\n- When stating version numbers, engines, or config fields, use ONLY values present in the file summaries\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this directory\", \"this module\", \"provides\", \"responsible for\", \"is used to\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Per-file descriptions in Contents sections: 1-2 sentences maximum. Reference key symbols but do not reproduce full summaries.\n- Behavioral contracts (regex patterns, format specs, constants) belong in a separate Behavioral Contracts section, not in per-file descriptions.\n- Annex file references: link to .annex.sum files, do not inline their content. One line per annex reference.\n- Subdirectory descriptions: 1-2 sentences maximum. Capture the directory's role, not its full contents.\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- Key exported symbols from file summaries MUST appear in the directory overview\n- Preserve exact casing of identifiers\n\nUSER NOTES:\n- If \"User Notes\" are provided in the prompt, they contain user-defined instructions that will be automatically prepended to your output\n- Do NOT repeat or paraphrase user notes in your generated content — they are included separately\n- You may reference information from user notes for context",
      "response": "<!-- Generated by agents-reverse-engineer -->\n# src/config\n\nConfiguration management for agents-reverse-engineer: loading, validating, and providing default values for the tool's runtime behavior, file exclusion patterns, binary detection, AI service settings, and concurrency limits.\n\n## Contents\n\n### Defaults\n[defaults.ts](./defaults.ts) exports `getDefaultConcurrency()` (cores × CONCURRENCY_MULTIPLIER clamped to MIN_CONCURRENCY=2, MAX_CONCURRENCY=20, with memory cap from totalMemGB × 0.5 ÷ 0.512 GB/subprocess), `DEFAULT_VENDOR_DIRS`, `DEFAULT_EXCLUDE_PATTERNS` (gitignore syntax), `DEFAULT_BINARY_EXTENSIONS`, size/compression limits, and a merged `DEFAULT_CONFIG` object.\n\n### Schema & Validation\n[schema.ts](./schema.ts) defines Zod validation schemas: `ConfigSchema` (root), `ExcludeSchema`, `OptionsSchema`, `OutputSchema`, `GenerationSchema` (compressionRatio 0.1–1.0), and `AISchema` (backend enum, model, timeoutMs=300s, maxRetries=3, concurrency 1–20, telemetry.keepRuns=50); exports `Config` type inferred from ConfigSchema.\n\n### Runtime Loading\n[loader.ts](./loader.ts) provides `loadConfig(root, options?)` to parse `.agents-reverse-engineer/config.yaml` via ConfigSchema with fallback to defaults, `findProjectRoot(startDir)` to locate `.agents-reverse-engineer/`, `configExists(root)`, and `writeDefaultConfig(root)` to scaffold config.yaml; throws `ConfigError` (extends Error with filePath and cause) on parse/validation failure.\n\n## Configuration Contracts\n\n- **compressionRatio**: 0.1–1.0; controls .sum file verbosity (0.1 = aggressive, 0.5 = verbose).\n- **backend**: enum `['claude', 'codex', 'gemini', 'opencode', 'auto']`; 'auto' detects from PATH.\n- **concurrency**: 1–20; auto-detected via `getDefaultConcurrency()` if omitted.\n- **timeoutMs**: default 300,000 (5 min for subprocess execution).\n- **maxRetries**: default 3 for transient errors.\n- **telemetry.keepRuns**: default 50 retained run logs.\n\n## Constants & Thresholds\n\n- CONCURRENCY_MULTIPLIER = 5; MIN_CONCURRENCY = 2; MAX_CONCURRENCY = 20.\n- SUBPROCESS_HEAP_GB = 0.512; MEMORY_FRACTION = 0.5.\n- DEFAULT_MAX_FILE_SIZE = 1048576 (1 MB); DEFAULT_COMPRESSION_RATIO = 0.25.\n- CONFIG_DIR = `.agents-reverse-engineer`; CONFIG_FILE = `config.yaml`.\n\n## Integration Points\n\n- Consumed by `src/cli/` commands (init, generate, update, discover, rebuild, specify).\n- Provides defaults to `src/discovery/` (file filters via exclude patterns/binary extensions/vendor dirs).\n- Supplies AI backend config to `src/ai/` (backend enum, concurrency, timeoutMs, maxRetries).\n- Trace events (`config:loaded`) emitted to `src/orchestration/trace.js` via ITraceWriter.",
      "model": "claude-haiku-4-5-20251001",
      "inputTokens": 3,
      "outputTokens": 817,
      "cacheReadTokens": 17512,
      "cacheCreationTokens": 3836,
      "latencyMs": 30000,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": []
    },
    {
      "timestamp": "2026-02-14T13:49:24.286Z",
      "prompt": "Generate AGENTS.md for directory: \"src/core\" (core)\n\n## File Summaries (2 files)\n\n### index.ts\n**Purpose:** index.ts is the public programmatic API entrypoint for agents-reverse-engineer, aggregating 40+ exports across loggin...\n\n**index.ts is the public programmatic API entrypoint for agents-reverse-engineer, aggregating 40+ exports across logging, AI services, file discovery, documentation generation, quality checking, and configuration modules.**\n\n## Exports\n\n### Logger\n- `nullLogger` — no-op logger\n- `consoleLogger` — stdout logger  \n- `Logger` — type\n\n### AI Services\n- `AIService` — main AI orchestration class\n- `AIServiceOptions`, `AIProvider`, `AICallOptions`, `AIResponse`, `AIBackend`, `RetryOptions`, `AIServiceErrorCode` — types\n- `AIServiceError` — error class\n- `withRetry(fn, opts): Promise` — wraps function with retry logic\n- `DEFAULT_RETRY_OPTIONS` — constant\n- `SubprocessProvider` — spawns subprocess for external LLM calls\n- `SubprocessProviderOptions` — type\n\n### Discovery\n- `discoverFiles(opts): Promise<File[]>` — finds files matching filters\n- `walkDirectory(path, opts): AsyncIterable<FilePath>` — traverses directory tree\n- `applyFilters(files, filters): FilterResult[]` — applies inclusion/exclusion rules\n- `FileFilter`, `FilterResult`, `WalkerOptions` — types\n\n### Prompt Generation\n- `buildFilePrompt(file, context): string` — constructs single-file analysis prompt\n- `buildDirectoryPrompt(dir, context): string` — constructs multi-file analysis prompt\n- `detectLanguage(path): Language` — identifies programming language\n- `PromptContext` — type\n\n### Documentation Writers\n- `writeSumFile(path, content): void` — writes .sum summary file\n- `readSumFile(path): SumFileContent` — parses .sum file\n- `getSumPath(filePath): string` — derives .sum path from source\n- `sumFileExists(path): boolean` — checks if .sum exists\n- `getAnnexPath(path): string` — derives .annex path\n- `writeAnnexFile(path, content): void` — writes .annex reference file\n- `SumFileContent` — type\n- `writeAgentsMd(dir, summaries): void` — generates AGENTS.md hub\n- `writeAgentsMdHub(dir, summaries): void` — AGENTS.md variant\n- `isGeneratedAgentsMd(path): boolean` — detects generated marker\n- `writeClaudeMdPointer(dir, target): void` — creates .claude.md redirect\n\n### Generation Orchestration\n- `DocumentationOrchestrator` / `GenerationOrchestrator` / `UpdateOrchestrator` — aliases for main orchestration engine\n- `GenerationPlan`, `PreparedFile`, `AnalysisTask`, `UpdatePlan` — types\n- `buildExecutionPlan(files, opts): ExecutionPlan` — sequences analysis tasks\n- `formatExecutionPlanAsMarkdown(plan): string` — renders plan as MD\n- `ExecutionPlan`, `ExecutionTask` — types\n\n### Quality Checks\n- `extractExports(sourceCode): Export[]` — parses function/class/const declarations\n- `checkCodeVsDoc(source, doc): Inconsistency[]` — compares code exports to documentation\n- `checkCodeVsCode(before, after): CodeCodeInconsistency[]` — detects breaking changes\n- `checkPhantomPaths(summaries, rootDir): PhantomPathInconsistency[]` — validates referenced file paths\n- `buildInconsistencyReport(issues): InconsistencyReport` — aggregates quality findings\n- `formatReportForCli(report): string` — CLI output format\n- `formatReportAsMarkdown(report): string` — MD output format\n- `Inconsistency`, `InconsistencyReport`, `InconsistencySeverity`, `CodeDocInconsistency`, `CodeCodeInconsistency`, `PhantomPathInconsistency` — types\n\n### Task Pooling\n- `runPool(tasks, opts): Promise<TaskResult[]>` — concurrent task execution\n- `PoolOptions`, `TaskResult` — types\n\n### Change Detection\n- `computeContentHash(filePath): string` — SHA256 file hash\n- `computeContentHashFromString(content): string` — SHA256 string hash\n- `isGitRepo(dir): boolean` — detects git repository\n- `getCurrentCommit(): string` — retrieves HEAD SHA\n- `getChangedFiles(since, filter): FileChange[]` — lists modified files since commit\n- `FileChange`, `ChangeDetectionResult` — types\n\n### Configuration\n- `loadConfig(path?): Config` — parses .are.json or discovers defaults\n- `findProjectRoot(startPath?): string` — locates project root via marker files\n- `ConfigSchema` — Zod schema for validation\n- `Config` — type\n- `getDefaultConcurrency(): number` — CPU-aware default worker count\n- `DEFAULT_EXCLUDE_PATTERNS` — constant string array (node_modules, .git, dist, etc.)\n- `DEFAULT_VENDOR_DIRS` — constant string array\n\n### Imports\n- `extractImports(sourceCode): ImportMap` — parses ES module imports\n- `extractDirectoryImports(dir): ImportMap` — aggregates imports across directory\n- `formatImportMap(map): string` — renders import analysis\n\n## Context\nRe-exports are from `../` paths (ai/, discovery/, generation/, orchestration/, quality/, change-detection/, config/, imports/) — this aggregation module enables single-line imports for library consumers. Marked `@beta` until v1.0.0; no CLI-only dependencies (process, ora, picocolors) in public API.\n### logger.ts\n**Purpose:** logger.ts defines a minimal Logger interface and default implementations for decoupled debug/warn/error output across...\n\n**logger.ts defines a minimal Logger interface and default implementations for decoupled debug/warn/error output across core library modules.**\n\n## Exports\n\n`Logger` interface: contract with `debug(message: string)`, `warn(message: string)`, `error(message: string)` methods.\n\n`nullLogger`: Logger implementation with no-op methods; default when no logger injected.\n\n`consoleLogger`: Logger implementation routing all methods to `console.error(msg)` for stderr output.\n\n## Design\n\nDependency injection pattern: core modules accept optional `Logger` parameter to decouple from direct `console` calls, enabling consumers to inject custom backends or silence output entirely. CLI code passes `consoleLogger` to preserve existing behavior.\n\n## Project Directory Structure\n\n<project-structure>\n./\n  .npmrc\n  CONTRIBUTING.md\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  pages.yml\n  publish.yml\nhooks/\n  are-check-update.js\n  are-context-loader.js\n  opencode-are-check-update.js\nscripts/\n  analyze-agents-compression.sh\n  analyze-sum-ratios.sh\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  codex.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/providers/\n  subprocess.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  rebuild.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/core/\n  index.ts\n  logger.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  claude-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  orchestrator.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/rebuild/\n  checkpoint.ts\n  index.ts\n  orchestrator.ts\n  output-parser.ts\n  prompts.ts\n  spec-reader.ts\n  types.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>",
      "systemPrompt": "You are generating an AGENTS.md file — a directory-level overview for AI coding assistants.\n\nCRITICAL: Output ONLY the raw markdown content. No code fences, no preamble, no explanations, no conversational text. Your entire response IS the AGENTS.md file content.\n\nOUTPUT FORMAT:\n- First line MUST be exactly: <!-- Generated by agents-reverse-engineer -->\n- Use a # heading with the directory name\n- Write a one-paragraph purpose statement for the directory\nusage\nADAPTIVE SECTIONS:\nAnalyze the directory contents and choose the most relevant sections. Do NOT use a fixed template. Instead, select sections that best document this specific directory for an AI that needs to reconstruct or extend the project.\n\nConsider these section types (choose what applies):\n- **Contents**: Group files by purpose/category under ## headings. For each file: markdown link [filename](./filename) and a one-line description.\n- **Subdirectories**: If subdirectories exist, list them with links [dirname/](./dirname/) and brief summaries.\n- **Architecture / Data Flow**: If files form a pipeline, request/response chain, or layered architecture, document it.\n- **Stack**: If this is a package root (has package.json, Cargo.toml, go.mod, etc.), document the technology stack, key scripts, and entry points.\n- **Structure**: If the directory layout follows a convention (feature-sliced, domain-driven, MVC, etc.), document it.\n- **Patterns**: If files share recurring design patterns (factory, strategy, middleware, barrel re-export), name and document them.\n- **Configuration**: If the directory contains config files, schemas, or environment definitions, document the config surface area.\n- **API Surface**: If the directory exports a public API (barrel index, route definitions, SDK), document the interface contract.\n- **File Relationships**: How files collaborate, depend on each other, or share state.\n- **Behavioral Contracts**: If files contain regex patterns, format specifications, magic constants, or template strings that define observable behavior, collect them in a dedicated section. Preserve verbatim patterns from file summaries — do NOT paraphrase regex into prose. This section is MANDATORY when file summaries contain behavioral artifacts.\n- **Reproduction-Critical Constants**: If file summaries reference annex files (via ## Annex References sections), list them with links. Example: \"Full prompt template text: [templates.annex.sum](./prompts/templates.annex.sum)\". Do NOT reproduce annex content in AGENTS.md — just link to it.\n- **Workflow & Conventions**: If file summaries contain contribution guidelines, PR conventions, commit standards, testing mandates, tool usage rules, AI agent instructions, or code conventions (naming, formatting, linting), collect them into actionable directives. Source from CONTRIBUTING.md, CI configs, PR templates, linter/formatter configs, README dev sections. Present as concrete rules (\"commits must use conventionalcommits format\", \"use camelCase for functions, PascalCase for types\") not vague descriptions (\"the project has guidelines\").\n\nChoose any relevant sections or create your own based on the directory contents. The goal is to provide a comprehensive overview that captures the essence of the directory's role in the project and how its files work together, with a focus on what an AI coding assistant would need to know to effectively interact with this code.\n\nSCOPE:\n- AGENTS.md is a NAVIGATIONAL INDEX — help an AI find the right file quickly\n- Focus on: what each file does, how files relate, directory-level patterns\n- Do NOT reproduce full architecture sections — those belong in the root CLAUDE.md\n\nPATH ACCURACY (MANDATORY):\n- When referencing files or modules outside this directory, use ONLY paths from the \"Import Map\" section\n- Do NOT invent, rename, or guess module paths — if a path isn't in the Import Map, don't reference it\n- Use the exact directory names from \"Project Directory Structure\" — do NOT rename directories\n  (e.g., if the directory is called \"cli\", write \"src/cli/\", NOT \"src/commands/\")\n- Cross-module references must use the specifier format from actual import statements\n  (e.g., \"../generation/writers/sum.js\", NOT \"../fs/sum-file.js\")\n- If you are unsure about a path, omit the cross-reference rather than guessing\n\nCONSISTENCY (MANDATORY):\n- Do not contradict yourself within the same document\n- If you describe a technique (e.g., \"regex-based\"), do not call it something else later (e.g., \"AST-based\")\n- When stating version numbers, engines, or config fields, use ONLY values present in the file summaries\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this directory\", \"this module\", \"provides\", \"responsible for\", \"is used to\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Per-file descriptions in Contents sections: 1-2 sentences maximum. Reference key symbols but do not reproduce full summaries.\n- Behavioral contracts (regex patterns, format specs, constants) belong in a separate Behavioral Contracts section, not in per-file descriptions.\n- Annex file references: link to .annex.sum files, do not inline their content. One line per annex reference.\n- Subdirectory descriptions: 1-2 sentences maximum. Capture the directory's role, not its full contents.\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- Key exported symbols from file summaries MUST appear in the directory overview\n- Preserve exact casing of identifiers\n\nUSER NOTES:\n- If \"User Notes\" are provided in the prompt, they contain user-defined instructions that will be automatically prepended to your output\n- Do NOT repeat or paraphrase user notes in your generated content — they are included separately\n- You may reference information from user notes for context",
      "response": "<!-- Generated by agents-reverse-engineer -->\n\n# src/core\n\nThe core module serves as the unified public API for agents-reverse-engineer, aggregating 40+ exports across logging, AI services, file discovery, documentation generation, quality checking, and orchestration — enabling single-import access to the entire library.\n\n## Contents\n\n### [index.ts](./index.ts)\nRe-exports 40+ symbols from logging (`nullLogger`, `consoleLogger`, `Logger`), AI services (`AIService`, `withRetry`, `SubprocessProvider`), discovery (`discoverFiles`, `walkDirectory`, `applyFilters`), prompt generation (`buildFilePrompt`, `buildDirectoryPrompt`, `detectLanguage`), documentation writers (`writeSumFile`, `readSumFile`, `writeAgentsMd`), orchestration (`GenerationOrchestrator`, `buildExecutionPlan`), quality checks (`extractExports`, `checkCodeVsDoc`, `checkPhantomPaths`), change detection (`computeContentHash`, `getChangedFiles`), configuration (`loadConfig`, `findProjectRoot`, `DEFAULT_EXCLUDE_PATTERNS`), and imports (`extractImports`, `formatImportMap`). Module aggregates seven layers of functionality from sibling `../` paths (ai/, discovery/, generation/, orchestration/, quality/, change-detection/, config/, imports/) with no CLI-only dependencies in public API.\n\n### [logger.ts](./logger.ts)\nDefines `Logger` interface with `debug()`, `warn()`, `error()` methods, plus `nullLogger` (no-op) and `consoleLogger` (stderr) implementations. Enables dependency injection across core modules for decoupled output control.\n\n## API Layers\n\n**Logging**: `Logger` interface + `nullLogger`, `consoleLogger` implementations.\n\n**AI Services**: `AIService` orchestrator, `SubprocessProvider` for external LLM calls, `withRetry()` wrapper with `RetryOptions` and `DEFAULT_RETRY_OPTIONS`.\n\n**Discovery**: `discoverFiles()`, `walkDirectory()`, `applyFilters()` for file enumeration with `FileFilter` rules.\n\n**Prompts**: `buildFilePrompt()`, `buildDirectoryPrompt()`, `detectLanguage()` with `PromptContext`.\n\n**Writers**: `writeSumFile()`, `readSumFile()`, `writeAgentsMd()`, `writeAnnexFile()`, `writeClaudeMdPointer()` for documentation output.\n\n**Orchestration**: `GenerationOrchestrator`, `buildExecutionPlan()`, `formatExecutionPlanAsMarkdown()` for multi-file analysis pipelines.\n\n**Quality**: `extractExports()`, `checkCodeVsDoc()`, `checkCodeVsCode()`, `checkPhantomPaths()`, `buildInconsistencyReport()` for documentation validation.\n\n**Change Detection**: `computeContentHash()`, `isGitRepo()`, `getCurrentCommit()`, `getChangedFiles()` for differential analysis.\n\n**Configuration**: `loadConfig()`, `findProjectRoot()`, `getDefaultConcurrency()`, `DEFAULT_EXCLUDE_PATTERNS` for environment setup.\n\n**Imports**: `extractImports()`, `extractDirectoryImports()`, `formatImportMap()` for dependency analysis.\n\n## Design Pattern\n\nSingle aggregation point: consumers import from `@agents-reverse-engineer/core` (or `../core`) instead of navigating sibling module paths. Marked `@beta` until v1.0.0; API is stable but subject to refinement. Dependency injection (Logger parameter) decouples output behavior from library logic.",
      "model": "claude-haiku-4-5-20251001",
      "inputTokens": 3,
      "outputTokens": 842,
      "cacheReadTokens": 0,
      "cacheCreationTokens": 21044,
      "latencyMs": 30770,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": []
    },
    {
      "timestamp": "2026-02-14T13:49:47.072Z",
      "prompt": "Generate AGENTS.md for directory: \"src/types\" (types)\n\n## File Summaries (1 files)\n\n### index.ts\n**Purpose:** src/types/index.ts exports ExcludedFile, DiscoveryResult, DiscoveryStats interfaces for file discovery metadata and s...\n\n**src/types/index.ts exports ExcludedFile, DiscoveryResult, DiscoveryStats interfaces for file discovery metadata and statistics.**\n\n## Exported Types\n\n- `ExcludedFile` — interface with `path: string` (absolute or relative file path) and `reason: string` (exclusion cause: \"gitignore pattern\", \"binary file\", \"vendor directory\")\n- `DiscoveryResult` — interface with `files: string[]` (analyzed file paths) and `excluded: ExcludedFile[]` (excluded files with reasons)\n- `DiscoveryStats` — interface with `totalFiles: number`, `includedFiles: number`, `excludedFiles: number`, `exclusionReasons: Record<string, number>` (exclusion reason breakdown)\n\n## Project Directory Structure\n\n<project-structure>\n./\n  .npmrc\n  CONTRIBUTING.md\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  pages.yml\n  publish.yml\nhooks/\n  are-check-update.js\n  are-context-loader.js\n  opencode-are-check-update.js\nscripts/\n  analyze-agents-compression.sh\n  analyze-sum-ratios.sh\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  codex.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/providers/\n  subprocess.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  rebuild.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/core/\n  index.ts\n  logger.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  claude-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  orchestrator.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/rebuild/\n  checkpoint.ts\n  index.ts\n  orchestrator.ts\n  output-parser.ts\n  prompts.ts\n  spec-reader.ts\n  types.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>",
      "systemPrompt": "You are generating an AGENTS.md file — a directory-level overview for AI coding assistants.\n\nCRITICAL: Output ONLY the raw markdown content. No code fences, no preamble, no explanations, no conversational text. Your entire response IS the AGENTS.md file content.\n\nOUTPUT FORMAT:\n- First line MUST be exactly: <!-- Generated by agents-reverse-engineer -->\n- Use a # heading with the directory name\n- Write a one-paragraph purpose statement for the directory\nusage\nADAPTIVE SECTIONS:\nAnalyze the directory contents and choose the most relevant sections. Do NOT use a fixed template. Instead, select sections that best document this specific directory for an AI that needs to reconstruct or extend the project.\n\nConsider these section types (choose what applies):\n- **Contents**: Group files by purpose/category under ## headings. For each file: markdown link [filename](./filename) and a one-line description.\n- **Subdirectories**: If subdirectories exist, list them with links [dirname/](./dirname/) and brief summaries.\n- **Architecture / Data Flow**: If files form a pipeline, request/response chain, or layered architecture, document it.\n- **Stack**: If this is a package root (has package.json, Cargo.toml, go.mod, etc.), document the technology stack, key scripts, and entry points.\n- **Structure**: If the directory layout follows a convention (feature-sliced, domain-driven, MVC, etc.), document it.\n- **Patterns**: If files share recurring design patterns (factory, strategy, middleware, barrel re-export), name and document them.\n- **Configuration**: If the directory contains config files, schemas, or environment definitions, document the config surface area.\n- **API Surface**: If the directory exports a public API (barrel index, route definitions, SDK), document the interface contract.\n- **File Relationships**: How files collaborate, depend on each other, or share state.\n- **Behavioral Contracts**: If files contain regex patterns, format specifications, magic constants, or template strings that define observable behavior, collect them in a dedicated section. Preserve verbatim patterns from file summaries — do NOT paraphrase regex into prose. This section is MANDATORY when file summaries contain behavioral artifacts.\n- **Reproduction-Critical Constants**: If file summaries reference annex files (via ## Annex References sections), list them with links. Example: \"Full prompt template text: [templates.annex.sum](./prompts/templates.annex.sum)\". Do NOT reproduce annex content in AGENTS.md — just link to it.\n- **Workflow & Conventions**: If file summaries contain contribution guidelines, PR conventions, commit standards, testing mandates, tool usage rules, AI agent instructions, or code conventions (naming, formatting, linting), collect them into actionable directives. Source from CONTRIBUTING.md, CI configs, PR templates, linter/formatter configs, README dev sections. Present as concrete rules (\"commits must use conventionalcommits format\", \"use camelCase for functions, PascalCase for types\") not vague descriptions (\"the project has guidelines\").\n\nChoose any relevant sections or create your own based on the directory contents. The goal is to provide a comprehensive overview that captures the essence of the directory's role in the project and how its files work together, with a focus on what an AI coding assistant would need to know to effectively interact with this code.\n\nSCOPE:\n- AGENTS.md is a NAVIGATIONAL INDEX — help an AI find the right file quickly\n- Focus on: what each file does, how files relate, directory-level patterns\n- Do NOT reproduce full architecture sections — those belong in the root CLAUDE.md\n\nPATH ACCURACY (MANDATORY):\n- When referencing files or modules outside this directory, use ONLY paths from the \"Import Map\" section\n- Do NOT invent, rename, or guess module paths — if a path isn't in the Import Map, don't reference it\n- Use the exact directory names from \"Project Directory Structure\" — do NOT rename directories\n  (e.g., if the directory is called \"cli\", write \"src/cli/\", NOT \"src/commands/\")\n- Cross-module references must use the specifier format from actual import statements\n  (e.g., \"../generation/writers/sum.js\", NOT \"../fs/sum-file.js\")\n- If you are unsure about a path, omit the cross-reference rather than guessing\n\nCONSISTENCY (MANDATORY):\n- Do not contradict yourself within the same document\n- If you describe a technique (e.g., \"regex-based\"), do not call it something else later (e.g., \"AST-based\")\n- When stating version numbers, engines, or config fields, use ONLY values present in the file summaries\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this directory\", \"this module\", \"provides\", \"responsible for\", \"is used to\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Per-file descriptions in Contents sections: 1-2 sentences maximum. Reference key symbols but do not reproduce full summaries.\n- Behavioral contracts (regex patterns, format specs, constants) belong in a separate Behavioral Contracts section, not in per-file descriptions.\n- Annex file references: link to .annex.sum files, do not inline their content. One line per annex reference.\n- Subdirectory descriptions: 1-2 sentences maximum. Capture the directory's role, not its full contents.\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- Key exported symbols from file summaries MUST appear in the directory overview\n- Preserve exact casing of identifiers\n\nUSER NOTES:\n- If \"User Notes\" are provided in the prompt, they contain user-defined instructions that will be automatically prepended to your output\n- Do NOT repeat or paraphrase user notes in your generated content — they are included separately\n- You may reference information from user notes for context",
      "response": "<!-- Generated by agents-reverse-engineer -->\n\n# src/types\n\nCentralized export point for file discovery metadata interfaces used across the codebase to represent discovered files, exclusion information, and discovery statistics.\n\n## Contents\n\n- [index.ts](./index.ts) — Exports `ExcludedFile`, `DiscoveryResult`, and `DiscoveryStats` interfaces for discovery metadata.\n\n## Exported Types\n\n**ExcludedFile**\n- `path: string` — absolute or relative file path\n- `reason: string` — exclusion cause: \"gitignore pattern\", \"binary file\", or \"vendor directory\"\n\n**DiscoveryResult**\n- `files: string[]` — analyzed file paths\n- `excluded: ExcludedFile[]` — excluded files with reasons\n\n**DiscoveryStats**\n- `totalFiles: number` — total files encountered\n- `includedFiles: number` — files included in analysis\n- `excludedFiles: number` — files excluded from analysis\n- `exclusionReasons: Record<string, number>` — exclusion reason breakdown by count\n\n## Type Relationships\n\n`ExcludedFile` and `DiscoveryResult` are consumed by src/discovery/run.ts and src/cli/discover.ts to represent discovery output. `DiscoveryStats` aggregates exclusion metadata across src/discovery/filters/ (binary.ts, custom.ts, gitignore.ts, vendor.ts) for discovery reporting.",
      "model": "claude-haiku-4-5-20251001",
      "inputTokens": 3,
      "outputTokens": 338,
      "cacheReadTokens": 0,
      "cacheCreationTokens": 19612,
      "latencyMs": 12824,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": []
    },
    {
      "timestamp": "2026-02-14T13:49:24.332Z",
      "prompt": "Generate AGENTS.md for directory: \"src/integration\" (integration)\n\n## File Summaries (4 files)\n\n### detect.ts\n**Purpose:** detect.ts detects which AI coding assistant environments (Claude Code, OpenCode, Aider) are present in a project by c...\n\n**detect.ts detects which AI coding assistant environments (Claude Code, OpenCode, Aider) are present in a project by checking for config directories and files.**\n\n## Exports\n\n`detectEnvironments(projectRoot: string): DetectedEnvironment[]` — scans projectRoot for .claude/.opencode/.aider config artifacts; returns array of detected environments with type and configDir.\n\n`hasEnvironment(projectRoot: string, type: EnvironmentType): boolean` — returns true if specified EnvironmentType is present.\n\n## Detection Rules\n\nClaude Code: `.claude/` directory OR `CLAUDE.md` file present.\n\nOpenCode: `.opencode/` directory present.\n\nAider: `.aider.conf.yml` file OR `.aider/` directory present.\n\n## Dependencies\n\n`existsSync` from `node:fs` — filesystem checks; `path` module for directory joining; `DetectedEnvironment`, `EnvironmentType` from `./types.js`.\n\n## Data Structure\n\nDetectedEnvironment objects contain: `type` (environment identifier), `configDir` (string like `.claude`), `detected` (boolean true).\n### generate.ts\n**Purpose:** generate.ts generates integration command files and hooks for detected AI assistant environments (Claude, OpenCode, G...\n\n**generate.ts generates integration command files and hooks for detected AI assistant environments (Claude, OpenCode, Gemini, Aider).**\n\n## Exports\n\n`generateIntegrationFiles(projectRoot: string, options?: GenerateOptions): Promise<IntegrationResult[]>` — Creates environment-specific template files and Claude session-end hooks; respects skip-if-exists unless `force: true`; supports `dryRun` mode and `environment` override to bypass auto-detection.\n\n## Types & Interfaces\n\n`GenerateOptions` — config with `dryRun` (no-op mode), `force` (overwrite existing), `environment` (EnvironmentType override for specific env).\n\n## Key Dependencies\n\n- `detectEnvironments(projectRoot)` from ./detect.js — returns array of detected environments with config dirs\n- `getClaudeTemplates()`, `getOpenCodeTemplates()`, `getGeminiTemplates()` from ./templates.js — environment-specific template arrays with `path` and `content` fields\n- `readBundledHook('are-session-end.js')` — reads pre-built hook from `hooks/dist/`; throws if missing\n\n## Environment Mapping\n\nConfig directory names: `claude` → `.claude`, `opencode` → `.opencode`, `aider` → `.aider`, `gemini` → `.gemini`.\n\n## Workflow\n\nFor each detected environment: (1) fetch templates via `getTemplatesForEnvironment(type)`; (2) write files unless they exist (or `force` overrides); (3) for `claude`, additionally write `.claude/hooks/are-session-end.js` from bundled hook. Returns `IntegrationResult[]` with `filesCreated`, `filesSkipped` per environment.\n\n## Critical Path Resolution\n\n`getBundledHookPath()` resolves from `dist/integration/` → `../../hooks/dist/[hookName]`; `readBundledHook()` validates existence before reading; `ensureDir()` creates parent directories recursively with `mkdirSync({ recursive: true })`.\n### templates.ts\n**Purpose:** src/integration/templates.ts generates command file templates for AI coding assistants (Claude Code, Codex, OpenCode,...\n\n**src/integration/templates.ts generates command file templates for AI coding assistants (Claude Code, Codex, OpenCode, Gemini) implementing ARE (agents-reverse-engineer) CLI integration.**\n\n## Exports\n\n`getClaudeTemplates()` → `IntegrationTemplate[]` — Returns Claude Code `.claude/skills/are-{command}/SKILL.md` template files with markdown frontmatter (name, description, argument-hint).\n\n`getCodexTemplates()` → `IntegrationTemplate[]` — Returns Codex `.agents/skills/are-{command}/SKILL.md` template files with identical markdown structure.\n\n`getOpenCodeTemplates()` → `IntegrationTemplate[]` — Returns OpenCode `.opencode/commands/are-{command}.md` flat template files with markdown frontmatter (no skill subdirs, includes `agent: build` metadata).\n\n`getGeminiTemplates()` → `IntegrationTemplate[]` — Returns Gemini CLI `.gemini/commands/are-{command}.toml` files in TOML format (triple-quoted `prompt` field instead of markdown).\n\n## Command Registry & Content Generation\n\n`COMMANDS` object defines six command templates (generate, update, init, discover, clean, specify, rebuild, help), each with `description`, `argumentHint`, and `content` (instruction template with placeholder substitution). Content includes execution workflows: background process spawning, progress log polling (.agents-reverse-engineer/progress.log via offset reads), version display from VERSION_FILE_PATH, and task completion summarization.\n\n`buildFrontmatter(platform, commandName, description, argumentHint)` → string — Generates YAML frontmatter: `name: are-{command}` (Claude/Codex only), description, optional argument-hint (Codex only), optional extraFrontmatter (agent: build for OpenCode).\n\n`buildGeminiToml(commandName, command)` → string — Builds TOML syntax with description and triple-quoted prompt field; no YAML frontmatter.\n\n`buildTemplate(platform, commandName, command)` → `IntegrationTemplate` — Orchestrates platform-specific file generation: selects filename/path per config, assembles frontmatter (skipped for Gemini), replaces placeholders (`COMMAND_PREFIX`, `VERSION_FILE_PATH`, `BACKEND_FLAG` → platform-specific values), returns `{filename, path, content}`.\n\n## Platform Configuration\n\n`PLATFORM_CONFIGS: Record<Platform, PlatformConfig>` — Maps platform name to config: commandPrefix (`/are-` all platforms), pathPrefix (`.claude/skills/`, `.agents/skills/`, `.opencode/commands/`, `.gemini/commands/`), filenameSeparator (`.` for Claude/Codex, `-` for OpenCode/Gemini), usesName (true for Claude/Codex, false for OpenCode/Gemini), versionFilePath (`.claude/ARE-VERSION`, `.agents/ARE-VERSION`, `.opencode/ARE-VERSION`, `.gemini/ARE-VERSION`).\n\n## Placeholder Substitution Rules\n\nCommand content uses three placeholders replaced per platform:\n- `COMMAND_PREFIX` → `/are-` (all platforms)\n- `VERSION_FILE_PATH` → platform-specific path (e.g., `.claude/ARE-VERSION`)\n- `BACKEND_FLAG` → `--backend {claude|codex|opencode|gemini}`\n\n## Behavioral Contracts — ARE Command Execution Workflows\n\n### generate / update / specify / rebuild\n1. Display version: read VERSION_FILE_PATH, show \"agents-reverse-engineer vX.Y.Z\"\n2. Delete stale `.agents-reverse-engineer/progress.log` (prevents carryover from prior runs)\n3. Spawn background task: `npx are {command} BACKEND_FLAG $ARGUMENTS` with `run_in_background: true`\n4. Poll progress.log every 15 seconds: read with offset parameter (last ~20 lines), show brief update (e.g., \"32/96 files analyzed\"), check TaskOutput with `block: false`, continue until completion\n5. **Critical**: Keep polling even if progress.log doesn't exist yet (command takes seconds to start writing)\n6. On completion, summarize full background task output\n\n### discover\n**STRICT RULES — VIOLATION FORBIDDEN:**\n- Run ONLY `npx are discover $ARGUMENTS` — NO additional flags user didn't type\n- If user typed nothing after discover, run with ZERO flags\n- Polls progress.log (~10s wait), reads GENERATION-PLAN.md + config.yaml, classifies files by category (test/spec, CI/CD, tool configs, migrations, fixtures, type declarations, Docker), counts matches, suggests exclusions via AskUserQuestion with multiSelect: true, edits config.yaml with accepted glob patterns\n\n### clean\n**STRICT RULES — VIOLATION FORBIDDEN:**\n- Run ONLY `npx are clean $ARGUMENTS` — NO additional flags user didn't type\n- If user typed nothing, run with ZERO flags\n\n### discover file categorization rules\nMatch patterns: `*.test.*`, `*.spec.*`, `__tests__/**`, `__mocks__/**`, `*.stories.*` (test/spec); `.github/workflows/*.yml`, `.gitlab-ci.yml`, `Jenkinsfile`, `.circleci/**`, `.travis.yml` (CI/CD); `.eslintrc*`, `.prettierrc*`, `jest.config.*`, `.editorconfig`, `babel.config.*`, `webpack.config.*`, `vite.config.*`, `rollup.config.*`, `tsconfig*.json`, `.lintstagedrc*`, `.huskyrc*`, `.stylelintrc*`, `commitlint.config.*` (tool configs); `migrations/`, `*.migration.*` (migrations); `__snapshots__/**`, `*.fixture.*`, `fixtures/**`, `test-data/**`, `testdata/**` (fixtures); `*.d.ts` (type declarations); `Dockerfile*`, `docker-compose*`, `*.Dockerfile`, `k8s/**`, `terraform/**`, `helm/**` (Docker/infra). Present only categories with matches; skip if already in config.yaml exclude list.\n\n## File Structure Convention\n\n- Claude/Codex: hierarchical `.claude/skills/are-{command}/SKILL.md` (skill per subdirectory)\n- OpenCode/Gemini: flat command files (`.opencode/commands/are-{command}.md`, `.gemini/commands/are-{command}.toml`)\n### types.ts\n**Purpose:** src/integration/types.ts defines type system for detecting AI coding assistant environments and generating integratio...\n\n**src/integration/types.ts defines type system for detecting AI coding assistant environments and generating integration templates.**\n\n## Exported Types\n\n- `EnvironmentType` — union type: `'claude' | 'opencode' | 'aider' | 'gemini'`\n- `DetectedEnvironment` — interface with `type: EnvironmentType`, `configDir: string`, `detected: boolean`\n- `IntegrationTemplate` — interface with `filename: string`, `path: string`, `content: string`\n- `IntegrationResult` — interface with `environment: EnvironmentType`, `filesCreated: string[]`, `filesSkipped: string[]`\n\n## Design Pattern\n\nDiscriminated union architecture: `EnvironmentType` acts as discriminant for environment detection workflows; `DetectedEnvironment` represents detection state; `IntegrationTemplate` encapsulates file generation specs; `IntegrationResult` captures creation outcomes.\n\n## Project Directory Structure\n\n<project-structure>\n./\n  .npmrc\n  CONTRIBUTING.md\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  pages.yml\n  publish.yml\nhooks/\n  are-check-update.js\n  are-context-loader.js\n  opencode-are-check-update.js\nscripts/\n  analyze-agents-compression.sh\n  analyze-sum-ratios.sh\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  codex.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/providers/\n  subprocess.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  rebuild.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/core/\n  index.ts\n  logger.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  claude-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  orchestrator.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/rebuild/\n  checkpoint.ts\n  index.ts\n  orchestrator.ts\n  output-parser.ts\n  prompts.ts\n  spec-reader.ts\n  types.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>",
      "systemPrompt": "You are generating an AGENTS.md file — a directory-level overview for AI coding assistants.\n\nCRITICAL: Output ONLY the raw markdown content. No code fences, no preamble, no explanations, no conversational text. Your entire response IS the AGENTS.md file content.\n\nOUTPUT FORMAT:\n- First line MUST be exactly: <!-- Generated by agents-reverse-engineer -->\n- Use a # heading with the directory name\n- Write a one-paragraph purpose statement for the directory\nusage\nADAPTIVE SECTIONS:\nAnalyze the directory contents and choose the most relevant sections. Do NOT use a fixed template. Instead, select sections that best document this specific directory for an AI that needs to reconstruct or extend the project.\n\nConsider these section types (choose what applies):\n- **Contents**: Group files by purpose/category under ## headings. For each file: markdown link [filename](./filename) and a one-line description.\n- **Subdirectories**: If subdirectories exist, list them with links [dirname/](./dirname/) and brief summaries.\n- **Architecture / Data Flow**: If files form a pipeline, request/response chain, or layered architecture, document it.\n- **Stack**: If this is a package root (has package.json, Cargo.toml, go.mod, etc.), document the technology stack, key scripts, and entry points.\n- **Structure**: If the directory layout follows a convention (feature-sliced, domain-driven, MVC, etc.), document it.\n- **Patterns**: If files share recurring design patterns (factory, strategy, middleware, barrel re-export), name and document them.\n- **Configuration**: If the directory contains config files, schemas, or environment definitions, document the config surface area.\n- **API Surface**: If the directory exports a public API (barrel index, route definitions, SDK), document the interface contract.\n- **File Relationships**: How files collaborate, depend on each other, or share state.\n- **Behavioral Contracts**: If files contain regex patterns, format specifications, magic constants, or template strings that define observable behavior, collect them in a dedicated section. Preserve verbatim patterns from file summaries — do NOT paraphrase regex into prose. This section is MANDATORY when file summaries contain behavioral artifacts.\n- **Reproduction-Critical Constants**: If file summaries reference annex files (via ## Annex References sections), list them with links. Example: \"Full prompt template text: [templates.annex.sum](./prompts/templates.annex.sum)\". Do NOT reproduce annex content in AGENTS.md — just link to it.\n- **Workflow & Conventions**: If file summaries contain contribution guidelines, PR conventions, commit standards, testing mandates, tool usage rules, AI agent instructions, or code conventions (naming, formatting, linting), collect them into actionable directives. Source from CONTRIBUTING.md, CI configs, PR templates, linter/formatter configs, README dev sections. Present as concrete rules (\"commits must use conventionalcommits format\", \"use camelCase for functions, PascalCase for types\") not vague descriptions (\"the project has guidelines\").\n\nChoose any relevant sections or create your own based on the directory contents. The goal is to provide a comprehensive overview that captures the essence of the directory's role in the project and how its files work together, with a focus on what an AI coding assistant would need to know to effectively interact with this code.\n\nSCOPE:\n- AGENTS.md is a NAVIGATIONAL INDEX — help an AI find the right file quickly\n- Focus on: what each file does, how files relate, directory-level patterns\n- Do NOT reproduce full architecture sections — those belong in the root CLAUDE.md\n\nPATH ACCURACY (MANDATORY):\n- When referencing files or modules outside this directory, use ONLY paths from the \"Import Map\" section\n- Do NOT invent, rename, or guess module paths — if a path isn't in the Import Map, don't reference it\n- Use the exact directory names from \"Project Directory Structure\" — do NOT rename directories\n  (e.g., if the directory is called \"cli\", write \"src/cli/\", NOT \"src/commands/\")\n- Cross-module references must use the specifier format from actual import statements\n  (e.g., \"../generation/writers/sum.js\", NOT \"../fs/sum-file.js\")\n- If you are unsure about a path, omit the cross-reference rather than guessing\n\nCONSISTENCY (MANDATORY):\n- Do not contradict yourself within the same document\n- If you describe a technique (e.g., \"regex-based\"), do not call it something else later (e.g., \"AST-based\")\n- When stating version numbers, engines, or config fields, use ONLY values present in the file summaries\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this directory\", \"this module\", \"provides\", \"responsible for\", \"is used to\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Per-file descriptions in Contents sections: 1-2 sentences maximum. Reference key symbols but do not reproduce full summaries.\n- Behavioral contracts (regex patterns, format specs, constants) belong in a separate Behavioral Contracts section, not in per-file descriptions.\n- Annex file references: link to .annex.sum files, do not inline their content. One line per annex reference.\n- Subdirectory descriptions: 1-2 sentences maximum. Capture the directory's role, not its full contents.\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- Key exported symbols from file summaries MUST appear in the directory overview\n- Preserve exact casing of identifiers\n\nUSER NOTES:\n- If \"User Notes\" are provided in the prompt, they contain user-defined instructions that will be automatically prepended to your output\n- Do NOT repeat or paraphrase user notes in your generated content — they are included separately\n- You may reference information from user notes for context",
      "response": "<!-- Generated by agents-reverse-engineer -->\n\n# src/integration\n\nDetects AI coding assistant environments (Claude Code, OpenCode, Aider, Gemini) and generates environment-specific integration command templates for ARE (agents-reverse-engineer) CLI execution.\n\n## Contents\n\n**Detection & Environment Discovery**\n\n- [detect.ts](./detect.ts) — `detectEnvironments(projectRoot)` scans for `.claude/`, `.opencode/`, `.aider/`, `.gemini/` config directories and returns `DetectedEnvironment[]` array; `hasEnvironment(projectRoot, type)` checks for specific `EnvironmentType`.\n\n**Template Generation**\n\n- [templates.ts](./templates.ts) — `getClaudeTemplates()`, `getOpenCodeTemplates()`, `getGeminiTemplates()` return `IntegrationTemplate[]` with platform-specific file paths and markdown/TOML frontmatter; `buildTemplate(platform, commandName, command)` assembles templates with placeholder substitution (`COMMAND_PREFIX`, `VERSION_FILE_PATH`, `BACKEND_FLAG`).\n\n**Integration Orchestration**\n\n- [generate.ts](./generate.ts) — `generateIntegrationFiles(projectRoot, options)` creates environment-specific command files and Claude session-end hooks; calls `detectEnvironments()`, fetches templates via `getTemplatesForEnvironment(type)`, writes files with `mkdirSync({ recursive: true })`, and copies bundled hook from `hooks/dist/are-session-end.js` for Claude environments.\n\n**Type System**\n\n- [types.ts](./types.ts) — Exports `EnvironmentType` (`'claude' | 'opencode' | 'aider' | 'gemini'`), `DetectedEnvironment`, `IntegrationTemplate` (`filename`, `path`, `content`), `IntegrationResult` (`filesCreated`, `filesSkipped` per environment).\n\n## Environment Detection\n\nClaude Code: `.claude/` directory OR `CLAUDE.md` file.\n\nOpenCode: `.opencode/` directory.\n\nAider: `.aider.conf.yml` file OR `.aider/` directory.\n\nGemini: `.gemini/` directory (inferred from template generation).\n\n## File Generation Workflow\n\n1. `detectEnvironments(projectRoot)` returns array of detected environments\n2. For each environment, `getTemplatesForEnvironment(type)` retrieves platform-specific templates\n3. Files are written to environment-specific config paths unless they exist (skip unless `force: true` in `GenerateOptions`)\n4. For Claude, additionally writes `.claude/hooks/are-session-end.js` from bundled hook (`readBundledHook('are-session-end.js')`)\n5. Returns `IntegrationResult[]` with `filesCreated` and `filesSkipped` per environment\n\n## Platform Configuration & Path Structure\n\n| Platform | Config Dir | Command Path | File Format |\n|----------|-----------|---|---|\n| Claude Code | `.claude/` | `.claude/skills/are-{command}/SKILL.md` | Markdown w/ YAML frontmatter |\n| OpenCode | `.opencode/` | `.opencode/commands/are-{command}.md` | Markdown w/ YAML frontmatter |\n| Gemini | `.gemini/` | `.gemini/commands/are-{command}.toml` | TOML (no YAML) |\n| Aider | `.aider/` | (detection only; no template generation) | N/A |\n\n## Behavioral Contracts — ARE Command Execution Workflows\n\n**generate / update / specify / rebuild:**\n1. Display version from `VERSION_FILE_PATH` (platform-specific: `.claude/ARE-VERSION`, `.opencode/ARE-VERSION`, `.gemini/ARE-VERSION`)\n2. Delete stale `.agents-reverse-engineer/progress.log`\n3. Spawn background task: `npx are {command} --backend {platform} $ARGUMENTS` with `run_in_background: true`\n4. Poll progress.log every 15 seconds; read last ~20 lines; continue until completion (tolerates missing log during startup)\n5. Summarize full background task output on completion\n\n**discover:**\n- Run ONLY `npx are discover $ARGUMENTS` (no extra flags)\n- Poll progress.log (~10s wait), read `GENERATION-PLAN.md` + `config.yaml`\n- Classify files: test/spec (`*.test.*`, `*.spec.*`, `__tests__/**`), CI/CD (`.github/workflows/*.yml`, `.gitlab-ci.yml`), tool configs (`.eslintrc*`, `tsconfig*.json`, `jest.config.*`), migrations (`migrations/`, `*.migration.*`), fixtures (`__snapshots__/**`, `*.fixture.*`), type declarations (`*.d.ts`), Docker/infra (`Dockerfile*`, `k8s/**`)\n- Present only categories with matches; skip if already in `config.yaml` exclude list\n- Suggest exclusions via `AskUserQuestion` with `multiSelect: true`; edit `config.yaml` with accepted glob patterns\n\n**clean:**\n- Run ONLY `npx are clean $ARGUMENTS` (no extra flags)\n\n## Command Registry\n\n`COMMANDS` object defines six commands: `generate`, `update`, `init`, `discover`, `clean`, `specify`, `rebuild`, `help` — each with `description`, `argumentHint`, and `content` (instruction template with placeholder substitution).\n\n## Placeholder Substitution\n\n- `COMMAND_PREFIX` → `/are-` (all platforms)\n- `VERSION_FILE_PATH` → platform-specific path\n- `BACKEND_FLAG` → `--backend {claude|opencode|gemini}`\n\n## Critical Dependencies\n\n- `detectEnvironments()` from [detect.ts](./detect.ts) — returns array of detected environments with config dirs\n- `getClaudeTemplates()`, `getOpenCodeTemplates()`, `getGeminiTemplates()` from [templates.ts](./templates.ts) — environment-specific template arrays\n- `readBundledHook('are-session-end.js')` — reads pre-built hook from `hooks/dist/`; validates existence before reading\n- `existsSync()` from `node:fs` — filesystem checks\n- `mkdirSync({ recursive: true })` — creates parent directories",
      "model": "claude-haiku-4-5-20251001",
      "inputTokens": 3,
      "outputTokens": 1491,
      "cacheReadTokens": 17512,
      "cacheCreationTokens": 4555,
      "latencyMs": 38004,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": []
    },
    {
      "timestamp": "2026-02-14T13:49:24.394Z",
      "prompt": "Generate AGENTS.md for directory: \"src/rebuild\" (rebuild)\n\n## File Summaries (7 files)\n\n### checkpoint.ts\n**Purpose:** CheckpointManager persists per-module rebuild progress and spec file hashes to enable session resumption after interr...\n\n**CheckpointManager persists per-module rebuild progress and spec file hashes to enable session resumption after interruption.**\n\n## Exports\n\n`CheckpointManager` — class managing rebuild state persistence; factory methods `load(outputDir, specFiles, unitNames): Promise<{manager, isResume}>` and `createFresh(outputDir, specFiles, unitNames): CheckpointManager`; instance methods `markDone(unitName, filesWritten): void`, `markFailed(unitName, error): void`, `getPendingUnits(): string[]`, `isDone(unitName): boolean`, `flush(): Promise<void>`, `initialize(): Promise<void>`, `getData(): RebuildCheckpoint`.\n\n## State & Persistence\n\nStores `RebuildCheckpoint` containing `version`, `createdAt`, `updatedAt`, `outputDir`, `specHashes` (Map<relativePath, contentHash>), and `modules` (Map<unitName, {status: 'pending'|'done'|'failed', completedAt?, filesWritten?, error?}>). Writes to `.rebuild-checkpoint` file in output directory using promise-chain serialization (`writeQueue`) to prevent corruption from concurrent worker pool updates.\n\n## Resume Logic\n\n`load()` performs three-stage drift detection: (1) validates checkpoint file existence and JSON parse, (2) validates schema via `RebuildCheckpointSchema.safeParse()`, (3) compares current spec file content hashes (`computeContentHashFromString`) against stored hashes; returns `isResume: true` only if checkpoint valid and all spec hashes match exactly (file count and content). Any mismatch triggers fresh checkpoint.\n\n## Error Handling\n\nCheckpoint write failures silently suppress (rebuild continues without persistence); JSON parse, schema validation, and file I/O errors in `load()` trigger automatic `createFresh()` fallback; `initialize()` mkdir/writeFile errors swallowed as non-critical.\n### index.ts\n**Purpose:** src/rebuild/index.ts exports types, schemas, and functions for the rebuild pipeline (RebuildCheckpoint, RebuildUnit, ...\n\n**src/rebuild/index.ts exports types, schemas, and functions for the rebuild pipeline (RebuildCheckpoint, RebuildUnit, RebuildPlan, RebuildResult, RebuildCheckpointSchema, readSpecFiles, partitionSpec, parseModuleOutput, CheckpointManager, REBUILD_SYSTEM_PROMPT, buildRebuildPrompt, executeRebuild).**\n\n## Exported Types\n`RebuildCheckpoint`, `RebuildUnit`, `RebuildPlan`, `RebuildResult` — domain types for rebuild state and execution.\n\n## Exported Constants & Schemas\n`RebuildCheckpointSchema` — schema for validating checkpoint structures; `REBUILD_SYSTEM_PROMPT` — system prompt for rebuild orchestration.\n\n## Exported Functions\n`readSpecFiles(...)` / `partitionSpec(...)` — spec file I/O and partitioning from spec-reader.js; `parseModuleOutput(...)` — parses module generation output; `buildRebuildPrompt(...)` — constructs rebuild prompts; `executeRebuild(..., opts: RebuildExecutionOptions)` — orchestrates rebuild execution.\n\n## Exported Classes\n`CheckpointManager` — manages rebuild checkpoint persistence and recovery.\n### orchestrator.ts\n**Purpose:** orchestrator.ts orchestrates rebuild execution by grouping rebuild units by order, processing groups sequentially wit...\n\n**orchestrator.ts orchestrates rebuild execution by grouping rebuild units by order, processing groups sequentially with concurrent units within each group, accumulating export context between groups.**\n\n## Exports\n\n`executeRebuild(aiService: AIService, projectRoot: string, options: RebuildExecutionOptions): Promise<{ modulesProcessed: number; modulesFailed: number; modulesSkipped: number }>` — Main entry point; reads spec files, partitions into RebuildUnits, loads/creates CheckpointManager, processes order groups sequentially (each group runs concurrently), accumulates export signatures from generated files as context for downstream groups, returns module counts.\n\n`RebuildExecutionOptions` — Configuration interface: `outputDir` (absolute output path), `concurrency` (max concurrent AI calls per group), `failFast?` (bool), `force?` (bool, wipe output), `debug?` (bool), `tracer?` (ITraceWriter), `progressLog?` (ProgressLog).\n\n## Pipeline Architecture\n\nReads spec files via `readSpecFiles`, partitions via `partitionSpec`, creates/resumes CheckpointManager via `CheckpointManager.load`, groups pending units by `order` field, sorts groups ascending, executes sequentially. Within each group, creates task functions (async) and dispatches via `runPool` with concurrency limit. After group completes, reads written files (excluding .md/.json/.yml), extracts content, accumulates in `builtContext` string for prompt context in subsequent groups.\n\n## Context Accumulation\n\n`builtContext` string accumulates full file content from generated modules. Truncation strategy: when length exceeds `BUILT_CONTEXT_LIMIT` (100,000 chars), splits on section markers (`// === [filePath] ===`), keeps recent half-groups in full, truncates older sections to `TRUNCATED_HEAD_LINES` (20 lines, typically imports/type declarations) with ellipsis.\n\n## Prompt & AI Integration\n\nBuilds prompt via `buildRebuildPrompt(unit, fullSpec, builtContext || undefined)`, calls `aiService.call({ prompt, systemPrompt, taskLabel })`, parses response text via `parseModuleOutput` (returns `Map<filePath, content>`), writes files to `outputDir` with parent directory creation, updates checkpoint via `checkpoint.markDone(name, filesWritten)`.\n\n## Checkpoint & Resumption\n\n`CheckpointManager.load(outputDir, specFiles, unitNames)` returns manager and `isResume` flag. On resume, skips units where `checkpoint.isDone(name)` returns true. Failed units marked via `checkpoint.markFailed(name, errorMsg)`. Final `checkpoint.flush()` persists state to disk.\n\n## Error Handling & Progress\n\nFailures collected in `modulesFailed` counter; `failFast` option stops on first error. `ProgressReporter` tracks via `onFileStart/onFileDone/onFileError`, receives token counts from `AIResponse`. Tracer emits phase events: `phase:start` (order group, task count, concurrency), `phase:end` (duration, task counts).\n### output-parser.ts\n**Purpose:** output-parser.ts parses AI-generated multi-file rebuild responses using delimiter and markdown fenced-block formats i...\n\n**output-parser.ts parses AI-generated multi-file rebuild responses using delimiter and markdown fenced-block formats into a Map<string, string> of file paths to contents.**\n\n## Exports\n\n`parseModuleOutput(responseText: string): Map<string, string>` — primary entry point; tries delimiter format first, falls back to fenced-block format; returns empty Map if neither matches.\n\n## Parsing Formats\n\n**Delimiter format (primary):** `===FILE: path===` / `===END_FILE===` delimiters at line start; state machine via `parseDelimiterFormat()` with regex patterns `^===FILE:\\s*(.+?)===$/` and `^===END_FILE===$`; handles unclosed blocks by flushing remaining content.\n\n**Fenced-block format (fallback):** markdown code blocks matching `` ```language:path\\n...``` `` via pattern `/```\\w*:([^\\n]+)\\n([\\s\\S]*?)```/g` in `parseFencedBlockFormat()`.\n\n## Behavior\n\nFile paths are trimmed; content preserves indentation (not trimmed). Delimiter regex requires column-0 matches to avoid false positives in string literals/JSDoc. Unclosed `===FILE:` blocks auto-flush on EOF.\n### prompts.ts\n**Purpose:** prompts.ts provides system and user prompt templates for AI-driven source code reconstruction from project specificat...\n\n**prompts.ts provides system and user prompt templates for AI-driven source code reconstruction from project specifications.**\n\n## Exports\n\n`REBUILD_SYSTEM_PROMPT` (string) — system message instructing AI to generate complete, compilable source files using `===FILE: path===` / `===END_FILE===` delimiters with strict adherence to spec signatures, type names, and architecture; forbids markdown fencing, tests, stubs, placeholders, and feature invention.\n\n`buildRebuildPrompt(unit: RebuildUnit, fullSpec: string, builtContext?: string): {system: string; user: string}` — assembles system + user prompt pair combining full specification, current phase content, previously-built module exports, and output format instructions for single reconstruction unit.\n\n## Behavioral Contracts\n\n**File delimiter format** (verbatim, column 0):\n```\n===FILE: relative/path.ext===\n[file content]\n===END_FILE===\n```\nNo markdown fencing, no content outside delimiters, no leading whitespace on delimiter lines.\n\n**Strict naming compliance**: exported function/type/class/constant names from spec must be used exactly as written; synonyms forbidden (e.g., `done()` not `reportSuccess()`).\n\n**Import requirements**: when \"Already Built\" context shows exported symbols, code must import and use them rather than redefine; signatures must match exactly.\n\n**Code quality mandate**: production-ready only (no tests, comments, stubs, placeholders); must compile; must follow spec architecture; must use real module names from spec; forbidden: feature invention, internal explanations of spec.\n\n## Integration\n\nDepends on `RebuildUnit` type from `./types.js`. Used by AI reconstruction pipeline to generate complete source trees from specification documents with deterministic, spec-compliant output.\n### spec-reader.ts\n**Purpose:** spec-reader.ts reads and partitions spec files from `specs/` directory into ordered rebuild units based on Build Plan...\n\n**spec-reader.ts reads and partitions spec files from `specs/` directory into ordered rebuild units based on Build Plan sections or top-level headings.**\n\n## Exported Functions\n\n`readSpecFiles(projectRoot: string): Promise<Array<{ relativePath: string; content: string }>>` — reads all `.md` files from `specs/` directory, sorted alphabetically; throws `'No spec files found in specs/. Run \"are specify\" first.'` if directory missing or empty.\n\n`partitionSpec(specFiles: Array<{ relativePath: string; content: string }>): RebuildUnit[]` — partitions spec content into ordered `RebuildUnit` array using dual strategy: (1) extract from `## Build Plan` section with `### Phase N:` subsections, or (2) fall back to `## ` top-level headings; injects contextual sections (Architecture, Public API Surface, Data Structures, Behavioral Contracts, File Manifest) per-phase when `Defines:`/`Consumes:` metadata present; filters empty units with warning; throws if no units extracted.\n\n## Internal Extraction Strategy\n\n`extractFromBuildPlan(fullContent: string): RebuildUnit[]` — extracts phases from `## (?:\\d+\\.\\s*)?Build Plan` section, detects `### Phase (\\d+):` subsections, injects Architecture section unconditionally, performs targeted subsection injection (API, Data Structures, Behavioral Contracts, File Manifest) when `Defines:`/`Consumes:` format detected, gracefully degrades to full Public API Surface for older specs.\n\n`extractFromTopLevelHeadings(fullContent: string): RebuildUnit[]` — fallback parser extracting `## ` sections as units ordered by position.\n\n`extractSubsections(sectionContent: string): Map<string, string>` — parses section into `Map<heading, content>` keyed by `### ` headings; includes heading in each subsection value.\n\n`findRelevantSubsections(phaseContent: string, subsections: Map<string, string>): string | null` — matches subsections by extracting `Defines:`/`Consumes:` lists and file paths (`src/[\\w\\-./]+`, `[\\w-]+\\.(ts|js|py|rs|go)`), performs case-insensitive substring and fuzzy word matching (3+ char words) against subsection keys; returns null if no matches or falls back to all subsections if no structured references found.\n\n`extractManifestEntriesForPhase(manifestContent: string, phaseContent: string): string | null` — extracts `Defines:` symbols from phase, filters manifest lines containing any symbol, returns null if no `Defines:` or matches.\n\n`extractSection(fullContent: string, sectionName: string): string | null` — extracts section by regex `## (?:\\d+\\.\\s*)?${sectionName}` through next `## ` boundary; returns null if not found.\n\n## RebuildUnit Contract\n\n`RebuildUnit` type contains: `name: string`, `specContent: string` (with injected context prefix), `order: number` (phase number or position).\n\n## Error Handling & Validation\n\nBoth extraction strategies require ≥1 unit post-filtering; consistent error message specifies expected format: `\"Expected either a \\\"## Build Plan\\\" section with \\\"### Phase N:\\\" subsections, or top-level \\\"## \\\" headings.\"` Empty units logged as `[warn] Skipping empty spec section: \"{unit.name}\"`.\n\n## Regex Patterns\n\nBuild Plan section: `/^(## (?:\\d+\\.\\s*)?Build Plan)\\s*$/m`\nPhase extraction: `/^### Phase (\\d+):\\s*(.+)$/gm`\nTop-level heading: `/^## (.+)$/gm`\nSubsection heading: `/^### (.+)$/gm`\nDefines/Consumes: `/(?:\\*\\*Defines:\\*\\*|^Defines:)\\s*(.+)/m` and `/(?:\\*\\*Consumes:\\*\\*|^Consumes:)\\s*(.+)/m`\nFile path references: `/\\b(?:src\\/[\\w\\-./]+|[\\w-]+\\.(?:ts|js|py|rs|go))\\b/g`\n### types.ts\n**Purpose:** types.ts defines Zod-validated schemas and TypeScript types for rebuild checkpoint persistence, units, plans, and per...\n\n**types.ts defines Zod-validated schemas and TypeScript types for rebuild checkpoint persistence, units, plans, and per-module results.**\n\n## Exported Types & Schemas\n\n`RebuildCheckpointSchema` — Zod object schema validating checkpoint file structure: `version` (string), `createdAt`/`updatedAt` (ISO strings), `outputDir` (string), `specHashes` (record of string hashes), `modules` (record with `status` enum `['pending', 'done', 'failed']`, optional `completedAt`/`error`/`filesWritten`).\n\n`RebuildCheckpoint` — Type inferred from `RebuildCheckpointSchema`; persists to `.rebuild-checkpoint` file in output directory to track per-module completion status and spec file hashes for drift detection.\n\n`RebuildUnit` — Interface with `name` (derived from spec section heading), `specContent` (string), `order` (number from Build Plan phase numbering); represents single AI call producing all files for one logical module/phase.\n\n`RebuildPlan` — Interface with `specFiles` (array of `{relativePath, content}`), `units` (ordered `RebuildUnit[]`), `outputDir` (string); computed from spec files before execution.\n\n`RebuildResult` — Interface tracking single unit rebuild outcome: `unitName`, `success` (boolean), `filesWritten` (relative paths), `tokensIn`/`tokensOut`/`cacheReadTokens`/`cacheCreationTokens`, `durationMs`, `model`, optional `error`.\n\n## Checkpoint Persistence Contract\n\nCheckpoint file `.rebuild-checkpoint` stores version-stamped state for corruption/mismatch detection on disk read; `specHashes` enables drift detection across rebuilds.\n\n## Import Map (verified — use these exact paths)\n\ncheckpoint.ts:\n  ../change-detection/index.js → computeContentHashFromString\n  ../version.js → getVersion\n\norchestrator.ts:\n  ../ai/index.js → AIService (type)\n  ../ai/types.js → AIResponse (type)\n  ../orchestration/index.js → runPool, ProgressReporter, type ProgressLog, type ITraceWriter\n\n## Project Directory Structure\n\n<project-structure>\n./\n  .npmrc\n  CONTRIBUTING.md\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  pages.yml\n  publish.yml\nhooks/\n  are-check-update.js\n  are-context-loader.js\n  opencode-are-check-update.js\nscripts/\n  analyze-agents-compression.sh\n  analyze-sum-ratios.sh\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  codex.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/providers/\n  subprocess.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  rebuild.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/core/\n  index.ts\n  logger.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  claude-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  orchestrator.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/rebuild/\n  checkpoint.ts\n  index.ts\n  orchestrator.ts\n  output-parser.ts\n  prompts.ts\n  spec-reader.ts\n  types.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>",
      "systemPrompt": "You are generating an AGENTS.md file — a directory-level overview for AI coding assistants.\n\nCRITICAL: Output ONLY the raw markdown content. No code fences, no preamble, no explanations, no conversational text. Your entire response IS the AGENTS.md file content.\n\nOUTPUT FORMAT:\n- First line MUST be exactly: <!-- Generated by agents-reverse-engineer -->\n- Use a # heading with the directory name\n- Write a one-paragraph purpose statement for the directory\nusage\nADAPTIVE SECTIONS:\nAnalyze the directory contents and choose the most relevant sections. Do NOT use a fixed template. Instead, select sections that best document this specific directory for an AI that needs to reconstruct or extend the project.\n\nConsider these section types (choose what applies):\n- **Contents**: Group files by purpose/category under ## headings. For each file: markdown link [filename](./filename) and a one-line description.\n- **Subdirectories**: If subdirectories exist, list them with links [dirname/](./dirname/) and brief summaries.\n- **Architecture / Data Flow**: If files form a pipeline, request/response chain, or layered architecture, document it.\n- **Stack**: If this is a package root (has package.json, Cargo.toml, go.mod, etc.), document the technology stack, key scripts, and entry points.\n- **Structure**: If the directory layout follows a convention (feature-sliced, domain-driven, MVC, etc.), document it.\n- **Patterns**: If files share recurring design patterns (factory, strategy, middleware, barrel re-export), name and document them.\n- **Configuration**: If the directory contains config files, schemas, or environment definitions, document the config surface area.\n- **API Surface**: If the directory exports a public API (barrel index, route definitions, SDK), document the interface contract.\n- **File Relationships**: How files collaborate, depend on each other, or share state.\n- **Behavioral Contracts**: If files contain regex patterns, format specifications, magic constants, or template strings that define observable behavior, collect them in a dedicated section. Preserve verbatim patterns from file summaries — do NOT paraphrase regex into prose. This section is MANDATORY when file summaries contain behavioral artifacts.\n- **Reproduction-Critical Constants**: If file summaries reference annex files (via ## Annex References sections), list them with links. Example: \"Full prompt template text: [templates.annex.sum](./prompts/templates.annex.sum)\". Do NOT reproduce annex content in AGENTS.md — just link to it.\n- **Workflow & Conventions**: If file summaries contain contribution guidelines, PR conventions, commit standards, testing mandates, tool usage rules, AI agent instructions, or code conventions (naming, formatting, linting), collect them into actionable directives. Source from CONTRIBUTING.md, CI configs, PR templates, linter/formatter configs, README dev sections. Present as concrete rules (\"commits must use conventionalcommits format\", \"use camelCase for functions, PascalCase for types\") not vague descriptions (\"the project has guidelines\").\n\nChoose any relevant sections or create your own based on the directory contents. The goal is to provide a comprehensive overview that captures the essence of the directory's role in the project and how its files work together, with a focus on what an AI coding assistant would need to know to effectively interact with this code.\n\nSCOPE:\n- AGENTS.md is a NAVIGATIONAL INDEX — help an AI find the right file quickly\n- Focus on: what each file does, how files relate, directory-level patterns\n- Do NOT reproduce full architecture sections — those belong in the root CLAUDE.md\n\nPATH ACCURACY (MANDATORY):\n- When referencing files or modules outside this directory, use ONLY paths from the \"Import Map\" section\n- Do NOT invent, rename, or guess module paths — if a path isn't in the Import Map, don't reference it\n- Use the exact directory names from \"Project Directory Structure\" — do NOT rename directories\n  (e.g., if the directory is called \"cli\", write \"src/cli/\", NOT \"src/commands/\")\n- Cross-module references must use the specifier format from actual import statements\n  (e.g., \"../generation/writers/sum.js\", NOT \"../fs/sum-file.js\")\n- If you are unsure about a path, omit the cross-reference rather than guessing\n\nCONSISTENCY (MANDATORY):\n- Do not contradict yourself within the same document\n- If you describe a technique (e.g., \"regex-based\"), do not call it something else later (e.g., \"AST-based\")\n- When stating version numbers, engines, or config fields, use ONLY values present in the file summaries\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this directory\", \"this module\", \"provides\", \"responsible for\", \"is used to\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Per-file descriptions in Contents sections: 1-2 sentences maximum. Reference key symbols but do not reproduce full summaries.\n- Behavioral contracts (regex patterns, format specs, constants) belong in a separate Behavioral Contracts section, not in per-file descriptions.\n- Annex file references: link to .annex.sum files, do not inline their content. One line per annex reference.\n- Subdirectory descriptions: 1-2 sentences maximum. Capture the directory's role, not its full contents.\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- Key exported symbols from file summaries MUST appear in the directory overview\n- Preserve exact casing of identifiers\n\nUSER NOTES:\n- If \"User Notes\" are provided in the prompt, they contain user-defined instructions that will be automatically prepended to your output\n- Do NOT repeat or paraphrase user notes in your generated content — they are included separately\n- You may reference information from user notes for context",
      "response": "<!-- Generated by agents-reverse-engineer -->\n\n# src/rebuild\n\nThe rebuild module reconstructs source files from AI-friendly specification documents by orchestrating parallel AI calls grouped by dependency order, checkpointing progress to enable resumption, parsing multi-file responses, and accumulating built context across phases.\n\n## Contents\n\n### Core Orchestration\n- [orchestrator.ts](./orchestrator.ts) — `executeRebuild()` entry point; groups rebuild units by order, processes groups sequentially with concurrent units within each group via `runPool`, accumulates exported symbols as context (`builtContext`) for downstream groups, handles checkpoint resumption via `CheckpointManager.load()`.\n- [checkpoint.ts](./checkpoint.ts) — `CheckpointManager` persists per-module status and spec file content hashes to `.rebuild-checkpoint`; `load()` performs three-stage drift detection (file exists, schema valid, hashes match); `markDone()`/`markFailed()` update state; silent write failures allow rebuild to continue.\n\n### Specification & Parsing\n- [spec-reader.ts](./spec-reader.ts) — `readSpecFiles()` reads `.md` files from `specs/`; `partitionSpec()` extracts `RebuildUnit[]` from `## Build Plan` sections with `### Phase N:` subsections or falls back to `## ` top-level headings; injects contextual subsections (Architecture, API Surface, Behavioral Contracts, File Manifest) when `Defines:`/`Consumes:` metadata present.\n- [output-parser.ts](./output-parser.ts) — `parseModuleOutput()` parses AI responses into `Map<filePath, content>` using `===FILE: path===` / `===END_FILE===` delimiters (primary) or markdown fenced-block format (fallback); state machine with regex patterns for column-0 delimiter detection.\n- [prompts.ts](./prompts.ts) — `REBUILD_SYSTEM_PROMPT` instructs AI to generate compilable source using delimiter format; `buildRebuildPrompt()` assembles system + user prompt combining full spec, phase content, and previously-built context (`builtContext`); enforces strict naming, no tests/stubs/features.\n\n### Types & Exports\n- [types.ts](./types.ts) — `RebuildCheckpointSchema` validates checkpoint structure; `RebuildUnit` interface (name, specContent, order); `RebuildPlan` (specFiles, units, outputDir); `RebuildResult` (unitName, success, filesWritten, token counts).\n- [index.ts](./index.ts) — Barrel export of types (`RebuildCheckpoint`, `RebuildUnit`, `RebuildPlan`, `RebuildResult`), schemas (`RebuildCheckpointSchema`), and functions (`readSpecFiles`, `partitionSpec`, `parseModuleOutput`, `buildRebuildPrompt`, `executeRebuild`, `CheckpointManager`, `REBUILD_SYSTEM_PROMPT`).\n\n## Architecture & Data Flow\n\n```\nreadSpecFiles() → specFiles[]\n                      ↓\npartitionSpec() → RebuildUnit[] (with injected context)\n                      ↓\nCheckpointManager.load() → { manager, isResume }\n                      ↓\nGroup units by order field (ascending)\n                      ↓\nFor each group (sequential):\n  - Filter pending units (skip isDone())\n  - buildRebuildPrompt(unit, fullSpec, builtContext)\n  - aiService.call() → responseText\n  - parseModuleOutput() → Map<filePath, content>\n  - Write files to outputDir\n  - checkpoint.markDone(name, filesWritten)\n  - Extract file contents → accumulate to builtContext\n                      ↓\ncheckpoint.flush() → .rebuild-checkpoint file\n```\n\n## Behavioral Contracts\n\n**File delimiter format** (from prompts.ts, column 0, no markdown):\n```\n===FILE: relative/path.ext===\n[file content]\n===END_FILE===\n```\n\n**Strict naming compliance**: exported names from spec must match exactly (no synonyms).\n\n**Import requirements**: when prior phase exports appear in \"Already Built\" context, code must import/use them rather than redefine; signatures must match exactly.\n\n**Code quality**: production-ready only (no tests, comments, stubs, placeholders); must compile; must follow spec architecture; forbidden: feature invention, internal explanations.\n\n**Checkpoint schema**: version-stamped `.rebuild-checkpoint` file in output directory; `specHashes` (Map of relativePath → contentHash) enables drift detection; `modules` (Map of unitName → {status, completedAt?, error?, filesWritten?}).\n\n**Resume logic**: checkpoint valid only if JSON parses, schema passes, and ALL spec file content hashes match current files (file count + exact content). Any mismatch triggers fresh checkpoint.\n\n**Built context truncation**: when `builtContext` exceeds 100,000 chars, splits on section markers (`// === [filePath] ===`), keeps recent half-groups in full, truncates older sections to 20-line heads (imports/declarations) with ellipsis.\n\n## Integration Points\n\n- **AI Service** (`../ai/index.js`): `AIService.call({ prompt, systemPrompt, taskLabel })` returns `AIResponse` with token counts.\n- **Orchestration** (`../orchestration/index.js`): `runPool()` dispatches concurrent tasks with concurrency limit; `ProgressReporter` tracks token counts; `ITraceWriter` emits phase events.\n- **Change Detection** (`../change-detection/index.js`): `computeContentHashFromString()` computes spec file hashes for drift detection.\n- **Version** (`../version.js`): `getVersion()` stamps checkpoint creation timestamp.\n\n## Configuration Surface\n\n`RebuildExecutionOptions`:\n- `outputDir` (string, absolute path) — destination for generated files\n- `concurrency` (number) — max concurrent AI calls per order group\n- `failFast?` (boolean) — stop on first error\n- `force?` (boolean) — wipe output directory before rebuild\n- `debug?` (boolean) — verbose logging\n- `tracer?` (ITraceWriter) — emit phase events\n- `progressLog?` (ProgressLog) — track progress callbacks\n\n## Reproduction-Critical Constants\n\n- `BUILT_CONTEXT_LIMIT` (100,000 chars): max length before truncation\n- `TRUNCATED_HEAD_LINES` (20 lines): lines retained when truncating old sections\n- Checkpoint file: `.rebuild-checkpoint` in output directory",
      "model": "claude-haiku-4-5-20251001",
      "inputTokens": 3,
      "outputTokens": 1566,
      "cacheReadTokens": 17512,
      "cacheCreationTokens": 5906,
      "latencyMs": 40350,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": []
    },
    {
      "timestamp": "2026-02-14T13:49:24.368Z",
      "prompt": "Generate AGENTS.md for directory: \"src/ai\" (ai)\n\n## File Summaries (6 files)\n\n### index.ts\n**Purpose:** src/ai/index.ts is a barrel export defining the public API for the AI service layer, enforcing strict module boundaries.\n\n**src/ai/index.ts is a barrel export defining the public API for the AI service layer, enforcing strict module boundaries.**\n\n## Exported Symbols\n\n**Types:** `AIProvider`, `AIBackend`, `AIResponse`, `AICallOptions`, `SubprocessResult`, `RetryOptions`, `TelemetryEntry`, `RunLog`, `FileRead`\n\n**Classes:** `AIService` (service orchestrator), `BackendRegistry` (backend registry), `SubprocessProvider` (subprocess implementation)\n\n**Functions:** `createBackendRegistry()`, `resolveBackend()`, `detectBackend()`, `getInstallInstructions()`, `withRetry()`, `runSubprocess()`, `isCommandOnPath()`\n\n**Error:** `AIServiceError`\n\n**Constants:** `DEFAULT_RETRY_OPTIONS`\n\n**Type:** `AIServiceOptions`, `SubprocessProviderOptions`\n\n## Module Boundaries\n\nEnforces encapsulation: `src/ai/backends/` and `src/ai/telemetry/` are internal; all consumption must route through this barrel. Re-exports from `types.js`, `service.js`, `registry.js`, `retry.js`, `subprocess.js`, `providers/subprocess.js`, `backends/claude.js`.\n\n## Core Architecture Pattern\n\nComposition: `BackendRegistry` → `resolveBackend()` → `AIService` initialization with `AIServiceOptions` (timeout, retries, telemetry config) → `service.call()` for requests. `SubprocessProvider` bridges subprocess communication; `withRetry()` wraps transient failures.\n### registry.ts\n**Purpose:** registry.ts manages backend registration, runtime selection, and auto-detection for AI CLI providers (Claude, Codex, ...\n\n**registry.ts manages backend registration, runtime selection, and auto-detection for AI CLI providers (Claude, Codex, Gemini, OpenCode) with error handling and install instructions.**\n\n## Exports\n\n`BackendRegistry` class: stores `AIBackend` instances in insertion-order Map with `register(backend)`, `get(name)`, `getAll()` methods.\n\n`createBackendRegistry()` → `BackendRegistry`: factory populating registry in priority order: Claude > Codex > Gemini > OpenCode.\n\n`detectBackend(registry: BackendRegistry)` → `Promise<AIBackend | null>`: iterates registered backends, returns first where `isAvailable()` succeeds.\n\n`getInstallInstructions(registry)` → `string`: concatenates `backend.getInstallInstructions()` for all backends with `\\n\\n` separator.\n\n`resolveBackend(registry, requested: string | 'auto')` → `Promise<AIBackend>`: selects backend by name or auto-detection; throws `AIServiceError` with code `'CLI_NOT_FOUND'` if unavailable.\n\n## Key Behaviors\n\n`resolveBackend` with `requested='auto'` scans PATH in registration priority order; throws with formatted install instructions if all backends fail availability check. Explicit name selection validates registry membership and CLI presence on PATH. Error messages include available backend names or install command help.\n### retry.ts\n**Purpose:** retry.ts exports withRetry<T>() and DEFAULT_RETRY_OPTIONS for exponential backoff retry logic on transient async fail...\n\n**retry.ts exports withRetry<T>() and DEFAULT_RETRY_OPTIONS for exponential backoff retry logic on transient async failures.**\n\n## Exports\n\n`withRetry<T>(fn: () => Promise<T>, options: RetryOptions): Promise<T>` — executes async function with exponential backoff retry; returns on success, throws after exhausting maxRetries or on permanent errors (isRetryable returns false).\n\n`DEFAULT_RETRY_OPTIONS` — const object: {maxRetries: 3, baseDelayMs: 1_000, maxDelayMs: 8_000, multiplier: 2}; spread and augment with caller-specific isRetryable predicate and onRetry callback.\n\n## Key Behaviors\n\n**Delay formula:** `min(baseDelayMs × multiplier^attempt, maxDelayMs) + jitter[0-500ms]` — prevents thundering herd on rate limits.\n\n**Retry predicate:** error classification via options.isRetryable(error); permanent errors throw immediately without retry.\n\n**Callback hook:** options.onRetry?(attemptNumber, error) called before each wait.\n\n## Integration Notes\n\nDepends on `RetryOptions` type from './types.js' (defines isRetryable signature and onRetry callback shape). Loop exhaustiveness requires unreachable throw statement for TypeScript.\n### service.ts\n**Purpose:** AIService orchestrates AI calls with retry logic, telemetry recording, and subprocess management.\n\n**AIService orchestrates AI calls with retry logic, telemetry recording, and subprocess management.**\n\n## Exports\n\n`AIService` class: main orchestrator wrapping any `AIProvider` with configurable retry (rate-limit only), timeout, and telemetry; constructor accepts `AIProvider | AIBackend`, options `AIServiceOptions`, optional `Logger`; methods: `call(AICallOptions): Promise<AIResponse>`, `finalize(projectRoot: string): Promise<{ logPath, summary }>`, `setTracer(ITraceWriter): void`, `setDebug(boolean): void`, `setSubprocessLogDir(string): void`, `addFilesReadToLastEntry(FileRead[]): void`, `getSummary(): RunLog['summary']`.\n\n`AIServiceOptions` interface: `timeoutMs` (subprocess timeout), `maxRetries` (transient error retries), `model?` (default model ID applied unless overridden per-call), `command` (triggering command: \"generate\"|\"update\"|\"specify\"|\"rebuild\"), `telemetry.keepRuns` (max recent logs retained).\n\n`isAIBackend(obj: AIProvider | AIBackend): obj is AIBackend` type guard: checks for `buildArgs`, `parseResponse`, `cliCommand` properties to distinguish backend from provider.\n\n## Integration & Dependencies\n\n- Wraps `AIProvider` implementations; auto-wraps `AIBackend` in `SubprocessProvider`\n- Delegates `call()` to provider with `withRetry` using `DEFAULT_RETRY_OPTIONS`\n- Records telemetry via `TelemetryLogger`; writes run logs with `writeRunLog`; cleans old logs via `cleanupOldLogs`\n- Accepts `ITraceWriter` for retry event tracing; forwards to subprocess provider if applicable\n- Imports `Logger` for debug output (defaults to `nullLogger`)\n\n## Retry & Error Handling\n\nRetries only on `RATE_LIMIT` errors (code explicitly excludes timeout retries to prevent resource exhaustion). `onRetry` callback logs warn message with attempt count and emits trace event. `AIServiceError` with code `RATE_LIMIT` triggers retry; other `AIServiceError` codes (timeout, parse, subprocess failure) throw immediately. Failed calls recorded as error entries in telemetry with `exitCode: 1`.\n\n## Telemetry & State\n\n`TelemetryLogger` initialized with ISO timestamp, backend/provider name (\"custom\" for non-backend), model, command. Each `call()` increments `callCount`; successful calls record response metadata (model, tokens, latency); failed calls record error message. `finalize()` writes run log to disk, returns `logPath` and `summary`. `getSummary()` exposes current statistics without finalization. `addFilesReadToLastEntry()` attaches file metadata to most recent entry post-call.\n\n## Configuration & Options Merging\n\nService-level `options.model` serves as default; per-call `options.model` wins. `options.command` identifies triggering operation. Subprocess timeout forwarded to `SubprocessProvider`. Subprocess log directory (if set) creates files on first write with queued I/O (`logWriteQueue: Promise<void>`) to prevent concurrent mkdir interleaving.\n### subprocess.ts\n**Purpose:** src/ai/subprocess.ts centralizes spawning of AI CLI processes with guaranteed timeout enforcement, stdin piping, zomb...\n\n**src/ai/subprocess.ts centralizes spawning of AI CLI processes with guaranteed timeout enforcement, stdin piping, zombie prevention, and exit code extraction.**\n\n## Exports\n\n`runSubprocess(command: string, args: string[], options: SubprocessOptions): Promise<SubprocessResult>` — spawns CLI subprocess with timeout-enforced execution, stdin piping, and always-resolving result; escalates SIGTERM to SIGKILL after grace period.\n\n`getActiveSubprocessCount(): number` — returns count of actively running subprocesses.\n\n`getActiveSubprocesses(): Array<{ pid: number; command: string; spawnedAt: number; runningMs: number }>` — returns details of all active subprocesses with PID, command string, spawn timestamp, and elapsed duration.\n\n## Interface & Types\n\n`SubprocessOptions` — configuration with `timeoutMs: number` (kill threshold), `input?: string` (stdin), `env?: NodeJS.ProcessEnv` (environment overrides), `onSpawn?: (pid: number | undefined) => void` (synchronous spawn callback for trace events).\n\n## Behavioral Contracts\n\n**Timeout escalation**: SIGTERM sent by `execFile` when `timeoutMs` exceeded; if process hasn't exited within `SIGKILL_GRACE_MS` (5000ms), escalates to SIGKILL via `child.kill('SIGKILL')` and negative PID process group kill `-child.pid`.\n\n**Exit code extraction**: prioritizes `error.code` (number) from ExecFileException, falls back to `child.exitCode`, defaults to 1 for unknown failures or 0 if no error.\n\n**Timeout detection**: checks `error.killed === true` flag set by execFile.\n\n**Signal handling**: attempts process group kill first with `process.kill(-child.pid, 'SIGKILL')`, falls back to single process, silently catches ENOENT when process already dead.\n\n**Stdin closure**: unconditionally calls `child.stdin.end()` to signal EOF (blocking pitfall documented in RESEARCH.md).\n\n**Buffer**: 10MB maxBuffer for large AI responses.\n\n## State Tracking\n\n`activeSubprocesses: Map<number, { command: string; spawnedAt: number }>` — tracks running processes by PID; used for concurrency debugging; entries deleted on process exit.\n### types.ts\n**Purpose:** src/ai/types.ts defines the contract for AI backends, providers, and responses across the service layer.\n\n**src/ai/types.ts defines the contract for AI backends, providers, and responses across the service layer.**\n\n## Exported Interfaces & Types\n\n**SubprocessResult** — result of CLI process execution with `stdout`, `stderr`, `exitCode`, `signal`, `durationMs`, `timedOut`, `childPid?`.\n\n**AICallOptions** — input for AI calls: `prompt` (required), `systemPrompt?`, `model?`, `timeoutMs?`, `maxTurns?`, `taskLabel?`.\n\n**AIResponse** — normalized backend output: `text`, `model`, `inputTokens`, `outputTokens`, `cacheReadTokens`, `cacheCreationTokens`, `durationMs`, `exitCode`, `raw`.\n\n**AIProvider** — injectable interface with `call(options: AICallOptions): Promise<AIResponse>`.\n\n**AIBackend** — CLI adapter contract with `name`, `cliCommand` properties; `isAvailable()`, `buildArgs(options)`, `parseResponse(stdout, durationMs, exitCode)`, `getInstallInstructions()`, optional `composeStdinInput?(options)`, optional `ensureProjectConfig?(projectRoot)`.\n\n**RetryOptions** — exponential backoff config: `maxRetries`, `baseDelayMs`, `maxDelayMs`, `multiplier`, `isRetryable(error)`, `onRetry?(attempt, error)`.\n\n**FileRead** — telemetry record: `path`, `sizeBytes`.\n\n**TelemetryEntry** — per-call log with `timestamp`, `prompt`, `systemPrompt?`, `response`, `model`, token counts (input/output/cache), `latencyMs`, `exitCode`, `error?`, `retryCount`, `thinking`, `filesRead[]`.\n\n**RunLog** — per-run aggregation: `runId`, `startTime`, `endTime`, `backend`, `model`, `command`, `entries[]`, `summary` (totalCalls, token totals, errorCount, fileRead stats).\n\n## Error Handling\n\n**AIServiceErrorCode** — union type: `'CLI_NOT_FOUND' | 'TIMEOUT' | 'PARSE_ERROR' | 'SUBPROCESS_ERROR' | 'RATE_LIMIT'`.\n\n**AIServiceError** — typed error class extending `Error` with `code: AIServiceErrorCode` property for pattern matching on error type.\n\n## Design Patterns\n\nDependency injection via `AIProvider` interface enables swapping backends (subprocess, HTTP API, mock) without changing caller code. `AIBackend` implements Strategy pattern for multiple CLI toolchains (Claude, Gemini, OpenCode); `AIService` wraps any provider with retry/telemetry cross-cutting concerns.\n\n## Import Map (verified — use these exact paths)\n\nservice.ts:\n  ../orchestration/trace.js → ITraceWriter (type)\n  ../core/logger.js → Logger (type)\n  ../core/logger.js → nullLogger\n\n## Project Directory Structure\n\n<project-structure>\n./\n  .npmrc\n  CONTRIBUTING.md\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  pages.yml\n  publish.yml\nhooks/\n  are-check-update.js\n  are-context-loader.js\n  opencode-are-check-update.js\nscripts/\n  analyze-agents-compression.sh\n  analyze-sum-ratios.sh\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  codex.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/providers/\n  subprocess.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  rebuild.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/core/\n  index.ts\n  logger.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  claude-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  orchestrator.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/rebuild/\n  checkpoint.ts\n  index.ts\n  orchestrator.ts\n  output-parser.ts\n  prompts.ts\n  spec-reader.ts\n  types.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\n\n## Subdirectories\n\n### backends/\n<!-- Generated by agents-reverse-engineer v0.9.10 -->\n\n# src/ai/backends\n\nThe backends directory contains multiple AIBackend implementations that adapt different CLI tools (Claude, Codex, Gemini, OpenCode) to a unified interface for AIResponse parsing and argument construction.\n\n## Contents\n\n### Backend Implementations\n\n[claude.ts](./claude.ts) — Implements `ClaudeBackend` with response parsing for legacy JSON and NDJSON formats, token extraction from `modelUsage` map, and PATH detection via `isCommandOnPath()`; supports `--model`, `--system-prompt`, `--max-turns` options and requires `--allowedTools Read Write` for root execution.\n\n[codex.ts](./codex.ts) — Implements `CodexBackend` with polymorphic JSONL event stream parsing, text extraction from `agent_message` items with fallback to recursive `collectText()`, token normalization across field naming variants, and usage aggregation across turns.\n\n[gemini.ts](./gemini.ts) — Implements `GeminiBackend` as a stub that throws `AIServiceError('SUBPROCESS_ERROR')` from `parseResponse()`, deferring full implementation until Gemini CLI JSON output stabilizes; imports `isCommandOnPath()` from `claude.ts`.\n\n[opencode.ts](./opencode.ts) — Implements `OpenCodeBackend` with NDJSON streaming parsing, cost calculation from tokens using Anthropic Claude Sonnet pricing constants, model alias mapping for short names, agent config injection via `.opencode/agents/are-summarizer.md`, and handling of \"MAXIMUM STEPS REACHED\" markers.\n\n## Response Parsing Architecture\n\nEach backend parses distinct CLI output formats into a unified `AIResponse` type:\n\n- **Claude**: `ClaudeResponseSchema` (Zod) validates JSON object with `type: 'result'`, `subtype`, `is_error`, token counts in `usage` and `modelUsage` map (first key = model name), cost in `total_cost_usd`.\n- **Codex**: Multi-stage JSONL fallback chain—preferred path extracts `agent_message` items via `extractAssistantTextFromItem()`; fallback recursively collects text with `shouldSkipTextObject()` filter for reasoning fields; final fallback returns raw stdout.\n- **OpenCode**: NDJSON schema validation via `OpenCodeStepFinishSchema` and `OpenCodeTextSchema`; aggregates token counts across `step_finish` events, calculates cost via `calculateCostFromTokens()` if unprovided, concatenates text chunks from `text` events.\n\n## Cost & Token Handling\n\n**Claude**: extracts `input_tokens`, `output_tokens`, `cache_read_tokens`, `cache_creation_tokens` from `usage` object and `total_cost_usd`.\n\n**Codex**: normalizes field names (`inputTokens`, `input_tokens`, `cached_input_tokens`, etc.) and isolates non-cached input by subtracting `cacheRead` from raw input; returns `{ inputTokens, outputTokens, cacheReadTokens, cacheCreationTokens }`.\n\n**OpenCode**: pricing constants `INPUT_COST_PER_MTOK = 15`, `OUTPUT_COST_PER_MTOK = 75`, `CACHE_WRITE_COST_PER_MTOK = 18.75`, `CACHE_READ_COST_PER_MTOK = 1.50` (USD per million tokens) used by `calculateCostFromTokens()` when OpenCode provides zero cost.\n\n## CLI Argument Construction\n\n- **Claude**: `buildArgs()` → `['-p', '--output-format', 'json', '--no-session-persistence', '--allowedTools', 'Read', 'Write']` plus `--model`, `--system-prompt`, `--max-turns` if present.\n- **Codex**: `buildArgs()` → `['-a', 'never', 'exec', '--json', '--skip-git-repo-check', '--ephemeral', '--color', 'never']` plus optional `--model` and stdin flag `'-'`.\n- **OpenCode**: `buildArgs()` → `['run', '--format', 'json', '--agent', 'are-summarizer']` plus resolved model via `MODEL_ALIASES` (e.g., `'sonnet'` → `'anthropic/claude-sonnet-4-5'`).\n\n## OpenCode Agent Configuration\n\n`OPENCODE_AGENT_NAME = 'are-summarizer'` and `OPENCODE_AGENT_CONTENT` markdown disables all tools (`tools: {\"*\": false}`) and enforces single-turn execution with `steps: 5`. `ensureProjectConfig(projectRoot)` writes agent file to `.opencode/agents/are-summarizer.md` with parent directory creation; `composeStdinInput()` wraps system prompt in `<system-instructions>` XML tags for agent to follow exactly without output preamble.\n\n## Error Handling\n\nAll backends throw `AIServiceError`:\n- **Claude**: `PARSE_ERROR` if result JSON missing or schema validation fails (first 200 chars logged).\n- **Codex**: `PARSE_ERROR` if all extraction paths fail (malformed JSON, no text found).\n- **Gemini**: `SUBPROCESS_ERROR` unconditionally from `parseResponse()`.\n- **OpenCode**: `PARSE_ERROR` if no text found or \"MAXIMUM STEPS REACHED\" marker leaves <100 chars content; throws on malformed NDJSON without validation.\n\n## Behavioral Contracts\n\nClaude's `isCommandOnPath()` detects executables cross-platform via `fs.stat()` instead of execute-bit checks; accounts for Windows `PATHEXT` extensions.\n\nCodex's `extractUsageFromTurnCompleted()` deduplicates polymorphic token field names and normalizes cache terminology (`cached_input_tokens` → `cacheReadTokens`, etc.).\n\nOpenCode filters NDJSON events by type: `step_finish`, `text` are processed; `step_start`, `tool_use`, `tool_result`, `error` are skipped.\n### providers/\n<!-- Generated by agents-reverse-engineer v0.9.10 -->\n\n# src/ai/providers\n\nThis directory contains provider implementations that wrap AI backends into a consistent interface. Currently hosts `SubprocessProvider`, which adapts CLI-based backends to the `AIProvider` contract with subprocess lifecycle management, timeout enforcement, and rate-limit detection.\n\n## Contents\n\n- [subprocess.ts](./subprocess.ts) – `SubprocessProvider` class wrapping `AIBackend` instances; spawns subprocess with `call(options: AICallOptions)`, detects rate limits via `isRateLimitStderr()`, enforces timeouts, and writes structured logs asynchronously via `setSubprocessLogDir()`.\n\n## Architecture\n\n`SubprocessProvider` implements `AIProvider` by delegating to `AIBackend` (injected via constructor). The call flow is: `call(AICallOptions)` → build args via `backend.buildArgs()` → invoke `runSubprocess()` with timeout enforcement → parse `SubprocessResult` → throw on error or return `AIResponse`. Rate-limit detection intercepts stderr before error propagation.\n\n## Rate-Limit Detection & Error Handling\n\nRate limits are identified by `RATE_LIMIT_PATTERNS` (case-insensitive substring matches: `'rate limit'`, `'429'`, `'too many requests'`, `'overloaded'`). `isRateLimitStderr()` returns true if any pattern matches; matching triggers `AIServiceError('RATE_LIMIT', ...)`. Non-rate-limit errors throw `AIServiceError` with `'TIMEOUT'`, `'SUBPROCESS_ERROR'`, or `'PARSE_ERROR'` codes. Stderr/stdout truncated to 200–500 characters in error messages.\n\n## Subprocess Lifecycle & Tracing\n\n`call()` emits two structured trace events via optional `tracer: ITraceWriter`:\n- `subprocess:spawn` with `{type, childPid, command, taskLabel}`\n- `subprocess:exit` with `{type, childPid, command, taskLabel, exitCode, signal, durationMs, timedOut}`\n\nDebug mode (`debug: true`) logs heap snapshots (`heapUsed`, `rss`) before spawn and exit code/duration after exit. `activeCount` tracks concurrent subprocess count for memory profiling.\n\n## Logging & File I/O\n\n`setSubprocessLogDir(dir: string)` enables per-subprocess log files. Logs are queued and written serially via `logWriteQueue: Promise<void>` (chained promises prevent concurrent writes). Log format: metadata section (`task`, `pid`, `command`, `exit`, `signal`, `duration`, `timed_out`), then `--- stdout ---` and `--- stderr ---` sections. Task labels are sanitized for filenames (replace `/` with `--`, strip non-alphanumeric except `.`, `_`, `-`). Write failures are silently ignored.\n\n## Configuration\n\n`SubprocessProviderOptions` defines: `timeoutMs: number` (required; passed to `runSubprocess`), `debug?: boolean`, `logger?: Logger`, `tracer?: ITraceWriter | null`. Timeout enforcement is critical for preventing hung subprocesses.\n### telemetry/\n<!-- Generated by agents-reverse-engineer v0.9.10 -->\n\n# src/ai/telemetry\n\nAccumulates and persists telemetry data from AI service calls during CLI execution, providing cleanup and storage for run logs with aggregated metrics.\n\n## Contents\n\n**Telemetry accumulation:**\n- [logger.ts](./logger.ts) — `TelemetryLogger` class accumulates `TelemetryEntry` objects and computes aggregate `RunLog` summaries (token counts, durations, error counts, file tracking).\n\n**Telemetry persistence:**\n- [run-log.ts](./run-log.ts) — `writeRunLog()` serializes completed `RunLog` to disk as pretty-printed JSON in `.agents-reverse-engineer/logs/` with sanitized filenames.\n\n**Telemetry maintenance:**\n- [cleanup.ts](./cleanup.ts) — `cleanupOldLogs()` removes old `run-*.json` log files, retaining only the N most recent by lexicographic (ISO timestamp) ordering.\n\n## Data Flow\n\n1. During CLI execution: `TelemetryLogger` instance accumulates calls via `addEntry(entry: TelemetryEntry)`\n2. File reads deferred: `setFilesReadOnLastEntry(filesRead: FileRead[])` patches final entry after command completion\n3. Run completion: `toRunLog()` snapshots current state with `endTime` and aggregates via `getSummary()`\n4. Persistence: `writeRunLog()` writes `RunLog` JSON to disk with `startTime`-derived filename\n5. Maintenance: `cleanupOldLogs()` prunes logs directory to retain only `keepCount` most recent runs\n\n## Behavioral Contracts\n\n**Filename sanitization** (`run-log.ts`): Replaces non-alphanumeric characters in command, backend, model with `-`, and `:` / `.` in ISO timestamp with `-`. Pattern: `run-{safeCommand}-{safeBackend}-{safeModel}-{safeTimestamp}.json`. Example: `run-generate-claude-sonnet-2026-02-07T12-00-00-000Z.json`.\n\n**Log retention** (`cleanup.ts`): Filters entries by glob `run-*.json`, sorts lexicographically (newest first via reverse), slices to `keepCount`, deletes remainder. ISO timestamp filenames guarantee correct chronological ordering.\n\n**Aggregation** (`logger.ts`): `getSummary()` computes fresh aggregates on each call (not cached): `totalCalls`, `totalInputTokens`, `totalOutputTokens`, `totalCacheReadTokens`, `totalCacheCreationTokens`, `totalDurationMs`, `errorCount`, `totalFilesRead`, `uniqueFilesRead`.\n\n## Configuration\n\n- `LOGS_DIR` constant: `.agents-reverse-engineer/logs` — relative path from project root for all telemetry operations.\n- Directory auto-created by `writeRunLog()` if missing; `cleanupOldLogs()` returns 0 if directory absent (ENOENT handled gracefully).",
      "systemPrompt": "You are generating an AGENTS.md file — a directory-level overview for AI coding assistants.\n\nCRITICAL: Output ONLY the raw markdown content. No code fences, no preamble, no explanations, no conversational text. Your entire response IS the AGENTS.md file content.\n\nOUTPUT FORMAT:\n- First line MUST be exactly: <!-- Generated by agents-reverse-engineer -->\n- Use a # heading with the directory name\n- Write a one-paragraph purpose statement for the directory\nusage\nADAPTIVE SECTIONS:\nAnalyze the directory contents and choose the most relevant sections. Do NOT use a fixed template. Instead, select sections that best document this specific directory for an AI that needs to reconstruct or extend the project.\n\nConsider these section types (choose what applies):\n- **Contents**: Group files by purpose/category under ## headings. For each file: markdown link [filename](./filename) and a one-line description.\n- **Subdirectories**: If subdirectories exist, list them with links [dirname/](./dirname/) and brief summaries.\n- **Architecture / Data Flow**: If files form a pipeline, request/response chain, or layered architecture, document it.\n- **Stack**: If this is a package root (has package.json, Cargo.toml, go.mod, etc.), document the technology stack, key scripts, and entry points.\n- **Structure**: If the directory layout follows a convention (feature-sliced, domain-driven, MVC, etc.), document it.\n- **Patterns**: If files share recurring design patterns (factory, strategy, middleware, barrel re-export), name and document them.\n- **Configuration**: If the directory contains config files, schemas, or environment definitions, document the config surface area.\n- **API Surface**: If the directory exports a public API (barrel index, route definitions, SDK), document the interface contract.\n- **File Relationships**: How files collaborate, depend on each other, or share state.\n- **Behavioral Contracts**: If files contain regex patterns, format specifications, magic constants, or template strings that define observable behavior, collect them in a dedicated section. Preserve verbatim patterns from file summaries — do NOT paraphrase regex into prose. This section is MANDATORY when file summaries contain behavioral artifacts.\n- **Reproduction-Critical Constants**: If file summaries reference annex files (via ## Annex References sections), list them with links. Example: \"Full prompt template text: [templates.annex.sum](./prompts/templates.annex.sum)\". Do NOT reproduce annex content in AGENTS.md — just link to it.\n- **Workflow & Conventions**: If file summaries contain contribution guidelines, PR conventions, commit standards, testing mandates, tool usage rules, AI agent instructions, or code conventions (naming, formatting, linting), collect them into actionable directives. Source from CONTRIBUTING.md, CI configs, PR templates, linter/formatter configs, README dev sections. Present as concrete rules (\"commits must use conventionalcommits format\", \"use camelCase for functions, PascalCase for types\") not vague descriptions (\"the project has guidelines\").\n\nChoose any relevant sections or create your own based on the directory contents. The goal is to provide a comprehensive overview that captures the essence of the directory's role in the project and how its files work together, with a focus on what an AI coding assistant would need to know to effectively interact with this code.\n\nSCOPE:\n- AGENTS.md is a NAVIGATIONAL INDEX — help an AI find the right file quickly\n- Focus on: what each file does, how files relate, directory-level patterns\n- Do NOT reproduce full architecture sections — those belong in the root CLAUDE.md\n\nPATH ACCURACY (MANDATORY):\n- When referencing files or modules outside this directory, use ONLY paths from the \"Import Map\" section\n- Do NOT invent, rename, or guess module paths — if a path isn't in the Import Map, don't reference it\n- Use the exact directory names from \"Project Directory Structure\" — do NOT rename directories\n  (e.g., if the directory is called \"cli\", write \"src/cli/\", NOT \"src/commands/\")\n- Cross-module references must use the specifier format from actual import statements\n  (e.g., \"../generation/writers/sum.js\", NOT \"../fs/sum-file.js\")\n- If you are unsure about a path, omit the cross-reference rather than guessing\n\nCONSISTENCY (MANDATORY):\n- Do not contradict yourself within the same document\n- If you describe a technique (e.g., \"regex-based\"), do not call it something else later (e.g., \"AST-based\")\n- When stating version numbers, engines, or config fields, use ONLY values present in the file summaries\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this directory\", \"this module\", \"provides\", \"responsible for\", \"is used to\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Per-file descriptions in Contents sections: 1-2 sentences maximum. Reference key symbols but do not reproduce full summaries.\n- Behavioral contracts (regex patterns, format specs, constants) belong in a separate Behavioral Contracts section, not in per-file descriptions.\n- Annex file references: link to .annex.sum files, do not inline their content. One line per annex reference.\n- Subdirectory descriptions: 1-2 sentences maximum. Capture the directory's role, not its full contents.\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- Key exported symbols from file summaries MUST appear in the directory overview\n- Preserve exact casing of identifiers\n\nUSER NOTES:\n- If \"User Notes\" are provided in the prompt, they contain user-defined instructions that will be automatically prepended to your output\n- Do NOT repeat or paraphrase user notes in your generated content — they are included separately\n- You may reference information from user notes for context",
      "response": "<!-- Generated by agents-reverse-engineer -->\n\n# src/ai\n\nThe ai directory provides a unified orchestration layer for AI backend selection, provider execution, retry/timeout management, and telemetry collection. It exports a public API (`AIService`, `AIProvider`, `AIBackend`, `AIResponse`) with strict encapsulation of internal backends and telemetry implementations.\n\n## Contents\n\n### Core Service & Registry\n\n[index.ts](./index.ts) — Barrel export enforcing module boundaries; re-exports `AIService`, `AIProvider`, `AIBackend`, `AIResponse`, `BackendRegistry`, `SubprocessProvider`, `createBackendRegistry()`, `resolveBackend()`, `detectBackend()`, `withRetry()`, `runSubprocess()`, `isCommandOnPath()`, error type `AIServiceError`, and constants `DEFAULT_RETRY_OPTIONS`.\n\n[service.ts](./service.ts) — `AIService` orchestrator wrapping any `AIProvider` with retry (rate-limit only), timeout enforcement, and telemetry; accepts `AIServiceOptions` (timeoutMs, maxRetries, model, command, telemetry config); methods include `call(AICallOptions): Promise<AIResponse>`, `finalize(projectRoot)`, `setTracer()`, `setDebug()`, `setSubprocessLogDir()`, `addFilesReadToLastEntry()`, `getSummary()`.\n\n[registry.ts](./registry.ts) — `BackendRegistry` stores `AIBackend` instances with registration and lookup; `createBackendRegistry()` initializes with Claude, Codex, Gemini, OpenCode in priority order; `detectBackend()` and `resolveBackend()` with auto-detection support; `getInstallInstructions()` aggregates install guidance across all backends.\n\n[types.ts](./types.ts) — Defines contract types: `AIProvider` (injectable interface), `AIBackend` (CLI adapter with buildArgs/parseResponse/isAvailable), `AICallOptions`, `AIResponse`, `RetryOptions`, `FileRead`, `TelemetryEntry`, `RunLog`; error type `AIServiceError` with codes `CLI_NOT_FOUND | TIMEOUT | PARSE_ERROR | SUBPROCESS_ERROR | RATE_LIMIT`.\n\n### Retry & Subprocess\n\n[retry.ts](./retry.ts) — `withRetry<T>()` implements exponential backoff with formula `min(baseDelayMs × multiplier^attempt, maxDelayMs) + jitter[0–500ms]`; `DEFAULT_RETRY_OPTIONS` (maxRetries: 3, baseDelayMs: 1_000, maxDelayMs: 8_000, multiplier: 2); accepts `isRetryable(error)` predicate and `onRetry(attempt, error)` callback.\n\n[subprocess.ts](./subprocess.ts) — `runSubprocess(command, args, options)` spawns CLI processes with timeout enforcement (SIGTERM → SIGKILL after 5s grace), stdin piping, exit code extraction; `getActiveSubprocessCount()` and `getActiveSubprocesses()` for concurrency tracking; `SubprocessOptions` includes timeoutMs, input, env, onSpawn callback; 10MB maxBuffer for large outputs.\n\n## Subdirectories\n\n[backends/](./backends/) — Multiple `AIBackend` implementations (Claude, Codex, Gemini, OpenCode) adapting distinct CLI tools to unified response parsing and argument construction; handles polymorphic JSON/NDJSON output formats, token counting, cost calculation, and CLI argument building.\n\n[providers/](./providers/) — `SubprocessProvider` wrapping `AIBackend` instances; implements `AIProvider` contract with subprocess lifecycle, rate-limit detection via `RATE_LIMIT_PATTERNS`, structured trace events (subprocess:spawn, subprocess:exit), debug heap snapshots, and async log file I/O.\n\n[telemetry/](./telemetry/) — `TelemetryLogger` accumulates `TelemetryEntry` objects and computes `RunLog` summaries; `writeRunLog()` persists to `.agents-reverse-engineer/logs/run-*.json` with sanitized filenames; `cleanupOldLogs()` retains N most recent by lexicographic ordering.\n\n## Architecture & Data Flow\n\n**Selection & Initialization**\n1. `createBackendRegistry()` populates backend map (Claude > Codex > Gemini > OpenCode priority)\n2. `resolveBackend(registry, 'auto' | name)` detects available CLI or throws with install instructions\n3. `SubprocessProvider` wraps selected backend\n4. `AIService` wraps provider with retry/telemetry/timeout config\n\n**Call Execution**\n1. `AIService.call(AICallOptions)` → `withRetry()` wrapper\n2. `SubprocessProvider.call()` → `backend.buildArgs()` + `runSubprocess()`\n3. Exit code checked; rate limits detected via `isRateLimitStderr()`; response parsed via `backend.parseResponse()`\n4. `TelemetryLogger.addEntry()` records call metadata, tokens, latency\n5. Failed calls recorded with `exitCode: 1` and error message\n\n**Finalization & Cleanup**\n1. `AIService.finalize(projectRoot)` → `TelemetryLogger.toRunLog()` snapshots metrics\n2. `writeRunLog()` persists JSON to disk with sanitized `run-{command}-{backend}-{model}-{timestamp}.json`\n3. `cleanupOldLogs()` prunes retention to `keepRuns` count (most recent by timestamp)\n\n## Behavioral Contracts\n\n**Subprocess timeout escalation** (`subprocess.ts`): SIGTERM sent by `execFile` at `timeoutMs`; if unresponsive after `SIGKILL_GRACE_MS` (5000ms), escalates to SIGKILL via process group kill (`-child.pid`).\n\n**Retry predicate** (`retry.ts`): Permanent errors (non-retryable per `isRetryable`) throw immediately; only transient failures proceed through backoff loop with up to 3 attempts.\n\n**Rate-limit detection** (`providers/subprocess.ts`): Case-insensitive stderr substring matching against patterns `'rate limit'`, `'429'`, `'too many requests'`, `'overloaded'` triggers `AIServiceError('RATE_LIMIT')` for retry eligibility.\n\n**Telemetry aggregation** (`telemetry/logger.ts`): Per-call totals summed fresh on each `getSummary()` (not cached); includes input/output/cache tokens, duration, file reads, error count.\n\n**Log filename sanitization** (`telemetry/run-log.ts`): Pattern `run-{safeCommand}-{safeBackend}-{safeModel}-{safeTimestamp}.json`; replaces non-alphanumeric with `-`, `:/.` in timestamp with `-`.\n\n## Error Handling & Recovery\n\n`AIService.call()` retries only `RATE_LIMIT` errors (excludes timeout to prevent resource exhaustion). `onRetry` callback logs warn message with attempt count and emits trace event. Other `AIServiceError` codes (`TIMEOUT`, `PARSE_ERROR`, `SUBPROCESS_ERROR`, `CLI_NOT_FOUND`) throw immediately without retry. Failed calls persisted in telemetry with error message and `exitCode: 1`. `AIServiceError.code` enables pattern matching on error type for caller-specific handling.\n\n## Configuration & Composition\n\n**Service-level defaults** (`service.ts`): `AIServiceOptions.model` applies unless overridden per-call; `command` identifies triggering operation (generate|update|specify|rebuild) for telemetry labeling; `telemetry.keepRuns` controls log retention count.\n\n**Subprocess options** (`subprocess.ts`): `timeoutMs` forwarded to `execFile` timeout; `input` piped to stdin; `env` overrides (merge with process.env); `onSpawn(pid)` callback for trace correlation.\n\n**Retry options merging** (`retry.ts`): Caller spreads `DEFAULT_RETRY_OPTIONS` and augments with `isRetryable(error)` and `onRetry(attempt, error)` predicates specific to error type (rate-limit vs. timeout).\n\n## Annex References\n\nFull backend response parsing schemas: [backends AGENTS.md](./backends/)\n\nSubprocess lifecycle tracing patterns: [providers AGENTS.md](./providers/)\n\nTelemetry log formats and retention policy: [telemetry AGENTS.md](./telemetry/)",
      "model": "claude-haiku-4-5-20251001",
      "inputTokens": 3,
      "outputTokens": 2025,
      "cacheReadTokens": 17512,
      "cacheCreationTokens": 8083,
      "latencyMs": 42704,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": []
    },
    {
      "timestamp": "2026-02-14T13:49:24.380Z",
      "prompt": "Generate AGENTS.md for directory: \"src/installer\" (installer)\n\n## File Summaries (7 files)\n\n### banner.ts\n**Purpose:** banner.ts exports styled console output functions and version constant for installer UI.\n\n**banner.ts exports styled console output functions and version constant for installer UI.**\n\n## Exports\n\n- `VERSION` — string constant from `getVersion()`, displayed in banner\n- `displayBanner()` — void; prints ASCII \"ARE\" art in green with version/tagline using `pc.green()` and `pc.dim()`\n- `showHelp()` — void; prints usage text covering `--runtime`, `-g/--global`, `-l/--local`, `-u/--uninstall`, `--force`, `-q/--quiet`, `-h/--help` flags and examples\n- `showSuccess(msg: string)` — void; prefixes msg with green `✓`\n- `showError(msg: string)` — void; prefixes msg with red `✗`\n- `showWarning(msg: string)` — void; prefixes msg with yellow `!`\n- `showInfo(msg: string)` — void; prefixes msg with cyan `>`\n- `showNextSteps(runtime: string, filesCreated: number)` — void; displays post-install steps referencing `/are-help`, `/are-init`, `/are-discover`, `/are-generate`, `/are-update`, `/are-specify`, `/are-clean` commands\n\n## Design\n\nUses picocolors (`pc`) for terminal styling: `green()`, `red()`, `yellow()`, `cyan()`, `bold()`, `dim()`. All messaging functions follow prefix pattern: colored symbol + space + text.\n### index.ts\n**Purpose:** src/installer/index.ts is the main entry point for the agents-reverse-engineer installer, handling interactive/non-in...\n\n**src/installer/index.ts is the main entry point for the agents-reverse-engineer installer, handling interactive/non-interactive workflows for installing or uninstalling CLI commands and session hooks across runtime environments (Claude, Codex, OpenCode, Gemini).**\n\n## Exported Functions\n\n`parseInstallerArgs(args: string[]): InstallerArgs` — parses command-line flags (`--runtime`, `-g`/`--global`, `-l`/`--local`, `--force`, `-q`/`--quiet`, `-h`/`--help`) into structured config; validates runtime against `['claude', 'codex', 'opencode', 'gemini', 'all']`.\n\n`runInstaller(args: InstallerArgs): Promise<InstallerResult[]>` — main orchestrator: displays banner, determines location and runtime from flags or interactive prompts, delegates to `runInstall()` or `runUninstall()`, returns per-runtime results.\n\n## Exported Types & Paths\n\nRe-exports from `./types.js`: `InstallerArgs`, `InstallerResult`, `Runtime`, `Location`, `RuntimePaths`.\nRe-exports from `./paths.js`: `getRuntimePaths`, `getAllRuntimes`, `resolveInstallPath`, `getSettingsPath`.\nRe-exports from `./banner.js`: display functions (`displayBanner`, `showHelp`, `showSuccess`, `showError`, `showWarning`, `showInfo`, `showNextSteps`) and `VERSION`.\nRe-exports from `./prompts.js`: `selectRuntime`, `selectLocation`, `confirmAction`, `isInteractive`.\n\n## Mode Determination & Workflows\n\n`determineLocation(args: InstallerArgs): Location | undefined` — returns `'global'` if `-g` only, `'local'` if `-l` only, `undefined` if both or neither (requires prompt).\n\n`determineRuntimes(runtime: Runtime | undefined): Array<Exclude<Runtime, 'all'>>` — expands `'all'` to `getAllRuntimes()`, returns empty array if undefined (requires prompt).\n\n`runInstall(runtime, location, force, quiet): Promise<InstallerResult[]>` — calls `installFiles()`, verifies via `verifyInstallation()`, displays results via `displayInstallResults()`.\n\n`runUninstall(runtime, location, quiet): InstallerResult[]` — calls `uninstallFiles()`, calls `deleteConfigFolder()`, displays results via `displayUninstallResults()`.\n\n## Interactive vs Non-Interactive Behavior\n\nNon-interactive mode (when `!isInteractive()`) requires `--runtime` and `-g`/`--local` flags; missing flags trigger error and `process.exit(1)`. Interactive mode prompts via `selectRuntime()` and `selectLocation()` for missing values. Help flag (`-h`/`--help`) calls `showHelp()` and returns empty array.\n\n## Result Formatting\n\n`displayInstallResults(results: InstallerResult[])` — aggregates `filesCreated.length`, `filesSkipped.length`, `hookRegistered` counts; displays per-result success/error, summary counts, next steps via `showNextSteps(primaryRuntime, totalCreated)`, GitHub docs link.\n\n`displayUninstallResults(results: InstallerResult[], configDeleted?: boolean)` — reuses `InstallerResult` fields where `filesCreated` tracks deleted files, `filesSkipped` tracks not-found, `hookRegistered` tracks unregistered hooks; displays aggregate deletions and config folder removal.\n\n## Coupling & Dependencies\n\nImports structured across: `types.js` (interfaces), `paths.js` (path resolution), `banner.js` (styled output), `prompts.js` (interactive selection), `operations.js` (install logic), `uninstall.js` (uninstall logic). All display output routed through `show*` functions for consistent styling.\n### operations.ts\n**Purpose:** operations.ts orchestrates installation of ARE command templates, hooks, and plugins to Claude/Gemini/Codex/OpenCode ...\n\n**operations.ts orchestrates installation of ARE command templates, hooks, and plugins to Claude/Gemini/Codex/OpenCode runtimes with settings.json registration and version tracking.**\n\n## Exports\n\n`installFiles(runtime: Runtime, location: Location, options: InstallOptions): InstallerResult[]` — routes to runtime-specific installer; expands 'all' to all supported runtimes.\n\n`registerHooks(basePath: string, runtime: Exclude<Runtime, 'all'>, dryRun: boolean): boolean` — registers hooks in settings.json for Claude/Gemini; returns true if any hook added.\n\n`registerPermissions(settingsPath: string, dryRun: boolean): boolean` — adds ARE command permissions to Claude settings.json to reduce friction.\n\n`verifyInstallation(files: string[]): { success: boolean; missing: string[] }` — checks file existence; returns missing files list.\n\n`getPackageVersion(): string` — reads version from package.json via import.meta.url path resolution; returns 'unknown' on failure.\n\n`writeVersionFile(basePath: string, dryRun: boolean): void` — writes ARE-VERSION file tracking installed version.\n\n`formatInstallResult(result: InstallerResult): string[]` — formats InstallerResult to human-readable status lines (created/skipped counts, hook registration status).\n\n## Key Types\n\n`InstallOptions { force: boolean; dryRun: boolean }` — controls overwrite behavior and preview mode.\n\n`HookDefinition { event: 'SessionStart' | 'PostToolUse'; filename: string; name: string; matcher?: string }` — metadata for session hooks; matcher applies tool-scoped filtering (PostToolUse only).\n\n`PluginDefinition { srcFilename: string; destFilename: string }` — OpenCode plugin mapping from bundled hooks/dist/ to .opencode/plugins/.\n\n`SettingsJson { hooks?: { SessionStart?: HookEvent[]; PostToolUse?: HookEvent[] }; permissions?: { allow?: string[] } }` — Claude/Codex settings structure.\n\n`GeminiSettingsJson { hooks?: { SessionStart?: GeminiHook[] } }` — Gemini settings (SessionStart only, no PostToolUse).\n\n## Installation Strategy\n\nRuntime-specific branches: Claude/Gemini install session hooks + register in settings.json; Codex generates rules files (`are.rules`) with prefix_rule entries; OpenCode installs plugins from bundled sources (currently empty ARE_PLUGINS array).\n\nCommand templates copied from `getTemplatesForRuntime()` (Claude/Codex/OpenCode/Gemini sources) to `basePath/commands/` hierarchy. Hooks bundled in `hooks/dist/` copied to `basePath/hooks/` (runtime-relative).\n\n`shouldWriteManagedFile(filePath: string, marker: string, force: boolean): boolean` — allows --force to overwrite only ARE-generated files (identified by marker); preserves user-authored files.\n\n## Hooks & Permissions\n\n`ARE_HOOKS` array: `are-check-update.js` (SessionStart), `are-context-loader.js` (PostToolUse, matcher='Read' for tool-scoped execution).\n\n`ARE_PERMISSIONS` array: bash command patterns (`'Bash(npx are init*)'`, `'Bash(npx are discover*)'`, etc.) + sleep and progress.log cleanup — registered in Claude settings.allow to bypass permission prompts.\n\nClaude hooks registered as nested `HookEvent { hooks: [{ type: 'command', command: '...' }] }` with optional matcher. Gemini uses flat `{ name, type, command }` format and skips PostToolUse.\n\n## Codex Rules\n\n`buildCodexRulesFileContent()` generates `are.rules` with `prefix_rule` entries for all CODEX_PREFIX_RULES (npx are subcommands + sleep + rm).\n\n`buildLocalContextRulesContent()` seeds Codex context-loading instructions (marker: `'<!-- Generated by agents-reverse-engineer installer: local-context-rules -->'`): loads AGENTS.override.md and AGENTS.md from file directory, parent dirs, up to repo root; nearest-directory rules override parent/root.\n\n## Path Resolution\n\n`getBundledHookPath(hookName: string): string` — resolves hook files: from `dist/installer/operations.js` navigate `../../hooks/dist/`.\n\n`getCodexRulesFilePath(location: Location, basePath: string): string` — local: `<repo-root>/rules/are.rules`; global: `~/.codex/rules/are.rules`.\n\n`getCodexContextRulesFilePath(location: Location, basePath: string): string` — local: `<repo-root>/AGENTS.override.md`; global: `~/.codex/AGENTS.override.md`.\n\n`ensureDir(filePath: string): void` — mkdir -p pattern; parent directory creation before file writes.\n\n## Behavioral Contracts\n\n**Codex prefix rules** (`CODEX_PREFIX_RULES`): `['npx', 'are', 'init']`, `['npx', 'are', 'discover']`, `['npx', 'are', 'generate']`, `['npx', 'are', 'update']`, `['npx', 'are', 'specify']`, `['npx', 'are', 'rebuild']`, `['npx', 'are', 'clean']`, `['rm', '-f', '.agents-reverse-engineer/progress.log']`, `['sleep']` — serialized as `prefix_rule = [...]` in are.rules.\n\n**Settings.json hook command format** (Claude): `node .claude/hooks/are-check-update.js`, `node .claude/hooks/are-context-loader.js`.\n\n**Gemini format** (simpler, SessionStart only): flat hook objects, name field required.\n\n**ARE-VERSION filename**: written to basePath root to track installation version.\n\n**Hook existence check**: command string match in settings.json; merges without duplicating.\n\n**Dry run behavior**: sets dryRun=true to skip writeFileSync calls; returns result with filesCreated populated for preview.\n### paths.ts\n**Purpose:** paths.ts resolves cross-platform installation paths for AI runtime config directories (Claude, Codex, OpenCode, Gemin...\n\n**paths.ts resolves cross-platform installation paths for AI runtime config directories (Claude, Codex, OpenCode, Gemini) with environment variable overrides and filesystem detection.**\n\n## Exports\n\n`getAllRuntimes()` → `Array<Exclude<Runtime, 'all'>>` returns concrete runtime identifiers: `['claude', 'codex', 'opencode', 'gemini']`.\n\n`getRuntimePaths(runtime: Exclude<Runtime, 'all'>) → RuntimePaths` returns `{global, local, settingsFile}` with env var fallbacks: `CLAUDE_CONFIG_DIR` (claude), `OPENCODE_CONFIG_DIR` / `XDG_CONFIG_HOME` (opencode), `GEMINI_CONFIG_DIR` (gemini); codex uses fixed `~/.agents` and `.agents`.\n\n`resolveInstallPath(runtime, location, projectRoot?) → string` joins project root with local path (`paths.local`) or returns global path.\n\n`resolveCodexConfigPath(location, projectRoot?) → string` resolves Codex CLI rules directory (`.codex` local, `~/.codex` global), separate from skill install paths.\n\n`getSettingsPath(runtime) → string` returns `RuntimePaths.settingsFile` for hook registration.\n\n`isRuntimeInstalledLocally(runtime, projectRoot) → Promise<boolean>` checks directory existence via `stat()` on local config path.\n\n`isRuntimeInstalledGlobally(runtime) → Promise<boolean>` checks directory existence via `stat()` on global config path.\n\n`getInstalledRuntimes(projectRoot) → Promise<Array<Exclude<Runtime, 'all'>>>` iterates `getAllRuntimes()` with `isRuntimeInstalledLocally()` checks.\n\n## Behavioral Contracts\n\n**Runtime config directory names:** claude=`.claude`, codex=`.agents`, opencode=`.opencode`, gemini=`.gemini`; codex additionally uses `.codex` for CLI rules.\n\n**Settings file location:** `<configPath>/settings.json` for all runtimes; used for hook registration.\n\n**Path resolution:** global paths via `os.homedir()` + env vars; local paths via `path.join(projectRoot || process.cwd(), localDirName)`.\n\n**Error handling:** filesystem checks suppress exceptions and return `false` on missing directories.\n### prompts.ts\n**Purpose:** src/installer/prompts.ts provides interactive terminal UI for installer selection prompts with arrow-key support in T...\n\n**src/installer/prompts.ts provides interactive terminal UI for installer selection prompts with arrow-key support in TTY and numbered fallback for CI.**\n\n## Exports\n\n`isInteractive()` → `boolean`: detects TTY mode via `process.stdin.isTTY`.\n\n`selectOption<T>(prompt: string, options: SelectOption<T>[])` → `Promise<T>`: routes to `arrowKeySelect` or `numberedSelect` based on TTY detection.\n\n`selectRuntime(mode: 'install' | 'uninstall')` → `Promise<Runtime>`: selects from `{ label, value }` pairs for 'claude' | 'codex' | 'opencode' | 'gemini' | 'all'.\n\n`selectLocation(mode: 'install' | 'uninstall')` → `Promise<Location>`: selects 'global' (home dirs: ~/.claude, ~/.agents, ~/.config/opencode) or 'local' (./ equivalents).\n\n`confirmAction(message: string)` → `Promise<boolean>`: yes/no confirmation.\n\n## Terminal Control & Raw Mode\n\n`rawModeActive` flag tracks state. `cleanupRawMode()` calls `process.stdin.setRawMode(false)` + `pause()` with try-catch swallowing errors. Global handlers registered: `process.on('exit')` and `process.on('SIGINT')` both call `cleanupRawMode()` before exit.\n\n## Arrow Key Handler (Interactive)\n\n`arrowKeySelect` uses `readline.emitKeypressEvents()` + `setRawMode(true)` to capture keypresses. Navigation: `key.name === 'up'` (decrement bounded to 0) / `'down'` (increment bounded to length-1) / `'return'` (resolve). Renders via ANSI escape codes: `\\x1b[${n}A` (cursor up), `\\x1b[2K` (clear line), `\\x1b[1B` (cursor down). Selected option prefixed `pc.cyan('> ')` and colored; unselected plain. Ctrl+C invokes `cleanupRawMode()` + `process.exit(0)`. Re-render clears previous output before redrawing.\n\n## Numbered Fallback (Non-TTY)\n\n`numberedSelect` creates `readline.createInterface`, lists options as `1. Label`, `2. Label`, reads input via `rl.question('Enter number: ')`, validates `1 ≤ num ≤ options.length` (1-indexed), rejects on parse/range error with message `Invalid selection: {answer}. Expected 1-{options.length}`.\n\n## Types & Dependencies\n\n`SelectOption<T>` interface: `{ label: string; value: T }`.\nImports: `node:readline`, `picocolors` (pc.*), `Runtime` | `Location` from ./types.js.\n### types.ts\n**Purpose:** src/installer/types.ts defines type signatures for npx installation workflows across AI coding assistant runtimes (Cl...\n\n**src/installer/types.ts defines type signatures for npx installation workflows across AI coding assistant runtimes (Claude Code, Codex, OpenCode, Gemini).**\n\n## Exported Types\n\n`Runtime` union: 'claude' | 'codex' | 'opencode' | 'gemini' | 'all' — target runtime for installation.\n\n`Location` union: 'global' | 'local' — installation scope (user-level or project-level).\n\n`InstallerArgs` interface — parsed CLI arguments with properties: `runtime?: Runtime`, `global: boolean`, `local: boolean`, `uninstall: boolean`, `force: boolean`, `help: boolean`, `quiet: boolean`.\n\n`InstallerResult` interface — operation outcome per runtime/location with `success: boolean`, `runtime: Exclude<Runtime, 'all'>`, `location: Location`, `filesCreated: string[]`, `filesSkipped: string[]`, `errors: string[]`, `hookRegistered?: boolean`, `versionWritten?: boolean`.\n\n`RuntimePaths` interface — resolved paths with `global: string`, `local: string`, `settingsFile: string`.\n\n## Installation Model\n\nGlobal paths: ~/.claude, ~/.agents, ~/.config/opencode, ~/.gemini. Local paths: .claude, .agents, .opencode, .gemini. Hook registration supported for Claude/Gemini only via settings.json. Force flag (`force: boolean`) overrides existing file skipping.\n### uninstall.ts\n**Purpose:** uninstall.ts reverses installation of ARE (agents-reverse-engineer) by removing command templates, hooks, plugins, pe...\n\n**uninstall.ts reverses installation of ARE (agents-reverse-engineer) by removing command templates, hooks, plugins, permissions, and configuration across multiple AI runtimes (Claude, Codex, OpenCode, Gemini).**\n\n## Exports\n\n`uninstallFiles(runtime: Runtime, location: Location, dryRun?: boolean): InstallerResult[]` — Uninstalls files for specified runtime or 'all' runtimes, returns array of results.\n\n`unregisterHooks(basePath: string, runtime: Exclude<Runtime, 'all'>, dryRun: boolean): boolean` — Removes ARE hook entries from settings.json for Claude or Gemini, returns true if any hook was removed.\n\n`unregisterPermissions(basePath: string, dryRun: boolean): boolean` — Removes ARE Bash permissions from Claude Code settings.json, returns true if any removed.\n\n`deleteConfigFolder(location: Location, dryRun: boolean): boolean` — Deletes .agents-reverse-engineer folder for local installations, returns true if successful.\n\n## Hook & Plugin Registration Constants\n\n`ARE_HOOKS: HookDefinition[]` — Current and legacy hook definitions for ARE (PostToolUse: are-context-loader.js; legacy SessionStart: are-check-update.js; SessionEnd: are-session-end.js); used for cleanup across Claude/Gemini.\n\n`ARE_PLUGIN_FILENAMES: string[]` — OpenCode plugins to remove (are-check-update.js).\n\n`OPENCODE_AGENT_FILENAME = 'are-summarizer.md'` — Agent file created by OpenCode backend ensureProjectConfig(), matches OPENCODE_AGENT_NAME in src/ai/backends/opencode.ts.\n\n`ARE_PERMISSIONS: string[]` — Eight Bash command patterns for Claude permission removal (npx are init/discover/generate/update/specify/rebuild/clean, rm -f .agents-reverse-engineer/progress.log, sleep).\n\n## Configuration & File Path Handling\n\n`CODEX_RULES_FILENAME = 'are.rules'` — Codex rules file installed by ARE.\n\n`LOCAL_CONTEXT_RULES_FILENAME = 'AGENTS.override.md'` — Context rules file for Codex.\n\n`LOCAL_CONTEXT_RULES_MARKER = '<!-- Generated by agents-reverse-engineer installer: local-context-rules -->'` — Marker used to identify ARE-managed context rules files; only files with this marker are deleted during uninstall.\n\n`CONFIG_DIR = '.agents-reverse-engineer'` — Configuration directory name (matches config/loader.ts); deleted for local installs only.\n\n`getCodexRulesFilePath(location: Location, basePath: string): string` — Resolves Codex rules path (local → project root/rules/are.rules; global → Codex home).\n\n`getCodexContextRulesFilePath(location: Location, basePath: string): string` — Resolves Codex context rules path (local → repository root/AGENTS.override.md; global → Codex config/AGENTS.override.md).\n\n## Runtime-Specific Deletion Logic\n\n`getTemplatesForRuntime(runtime: Exclude<Runtime, 'all'>): Template[]` — Dispatches getClaudeTemplates(), getCodexTemplates(), getOpenCodeTemplates(), or getGeminiTemplates() based on runtime.\n\n`uninstallFilesForRuntime(runtime: Exclude<Runtime, 'all'>, location: Location, dryRun: boolean): InstallerResult` — Core uninstall function: removes command templates, hook files (Claude/Gemini), plugins (OpenCode), Codex rules/context-rules, ARE-VERSION; cleans empty directories; unregisters hooks/permissions; returns InstallerResult with filesCreated (actually deleted), filesSkipped, errors, hookRegistered (true if unregistered).\n\n## Hook Unregistration\n\n`unregisterClaudeHooks(basePath: string, dryRun: boolean): boolean` — Removes ARE hook entries from Claude's settings.json across SessionStart/SessionEnd/PostToolUse events using hook pattern matching, preserves JSONC comments via jsonc-parser.\n\n`unregisterGeminiHooks(basePath: string, dryRun: boolean): boolean` — Removes ARE hooks from Gemini's settings.json across SessionStart/SessionEnd events.\n\n`getHookPatterns(runtimeDir: string): string[]` — Generates hook command patterns for matching: \"node {runtimeDir}/hooks/{filename}\" (current) and \"node hooks/{filename}\" (legacy).\n\n## Directory & File Cleanup Utilities\n\n`cleanupAreSkillDirs(skillsDir: string): void` — Removes empty are-* skill subdirectories from Claude/Codex skills folder.\n\n`cleanupEmptyDirs(dirPath: string): void` — Recursively removes empty directories up the tree, stops at runtime roots (.claude, .codex, .opencode, .gemini, .agents, .config).\n\n`cleanupLegacyGeminiFiles(commandsDir: string): void` — Removes legacy are-*.md files (pre-TOML format) and .toml files in are/ subdirectory (pre-flat structure) from Gemini installations.\n\n`cleanupOpenCodeInfrastructure(basePath: string): void` — Removes OpenCode plugin infrastructure (package.json, bun.lock, .gitignore, node_modules/) only if no other commands/plugins/agents exist; only for local installs.\n\n`dirHasContent(dirPath: string): boolean` — Returns true if directory exists and is non-empty; used to guard OpenCode infrastructure cleanup.\n\n## Type Definitions\n\n`SessionHook` — { type: 'command'; command: string } — Hook configuration for Claude/Gemini.\n\n`HookEvent` — { hooks: SessionHook[] } — Event wrapper for hook arrays.\n\n`SettingsJson` — Partial schema: hooks (SessionStart/SessionEnd/PostToolUse arrays), permissions (allow/deny strings).\n\n`HookDefinition` — { event: 'SessionStart'|'SessionEnd'|'PostToolUse'; filename: string } — Hook metadata.\n\n`GeminiHook` — { name: string; type: 'command'; command: string } — Gemini-specific hook format.\n\n`GeminiSettingsJson` — Partial schema: hooks (SessionStart/SessionEnd arrays only; no PostToolUse).\n\n## Key Dependencies\n\n`jsonc-parser` — parse(), modify(), applyEdits() for reading/writing settings.json while preserving JSONC comments.\n\n`./paths.js` — resolveInstallPath(), getAllRuntimes(), resolveCodexConfigPath() for path resolution.\n\n`../integration/templates.js` — getClaudeTemplates(), getCodexTemplates(), getOpenCodeTemplates(), getGeminiTemplates() for template metadata.\n\n`node:fs` — existsSync, unlinkSync, readFileSync, writeFileSync, readdirSync, rmdirSync, rmSync.\n\n## Import Map (verified — use these exact paths)\n\nbanner.ts:\n  ../version.js → getVersion\n\noperations.ts:\n  ../integration/templates.js → getClaudeTemplates, getCodexTemplates, getOpenCodeTemplates, getGeminiTemplates\n\nuninstall.ts:\n  ../integration/templates.js → getClaudeTemplates, getCodexTemplates, getOpenCodeTemplates, getGeminiTemplates\n\n## Project Directory Structure\n\n<project-structure>\n./\n  .npmrc\n  CONTRIBUTING.md\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  pages.yml\n  publish.yml\nhooks/\n  are-check-update.js\n  are-context-loader.js\n  opencode-are-check-update.js\nscripts/\n  analyze-agents-compression.sh\n  analyze-sum-ratios.sh\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  codex.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/providers/\n  subprocess.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  rebuild.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/core/\n  index.ts\n  logger.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  claude-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  orchestrator.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/rebuild/\n  checkpoint.ts\n  index.ts\n  orchestrator.ts\n  output-parser.ts\n  prompts.ts\n  spec-reader.ts\n  types.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>",
      "systemPrompt": "You are generating an AGENTS.md file — a directory-level overview for AI coding assistants.\n\nCRITICAL: Output ONLY the raw markdown content. No code fences, no preamble, no explanations, no conversational text. Your entire response IS the AGENTS.md file content.\n\nOUTPUT FORMAT:\n- First line MUST be exactly: <!-- Generated by agents-reverse-engineer -->\n- Use a # heading with the directory name\n- Write a one-paragraph purpose statement for the directory\nusage\nADAPTIVE SECTIONS:\nAnalyze the directory contents and choose the most relevant sections. Do NOT use a fixed template. Instead, select sections that best document this specific directory for an AI that needs to reconstruct or extend the project.\n\nConsider these section types (choose what applies):\n- **Contents**: Group files by purpose/category under ## headings. For each file: markdown link [filename](./filename) and a one-line description.\n- **Subdirectories**: If subdirectories exist, list them with links [dirname/](./dirname/) and brief summaries.\n- **Architecture / Data Flow**: If files form a pipeline, request/response chain, or layered architecture, document it.\n- **Stack**: If this is a package root (has package.json, Cargo.toml, go.mod, etc.), document the technology stack, key scripts, and entry points.\n- **Structure**: If the directory layout follows a convention (feature-sliced, domain-driven, MVC, etc.), document it.\n- **Patterns**: If files share recurring design patterns (factory, strategy, middleware, barrel re-export), name and document them.\n- **Configuration**: If the directory contains config files, schemas, or environment definitions, document the config surface area.\n- **API Surface**: If the directory exports a public API (barrel index, route definitions, SDK), document the interface contract.\n- **File Relationships**: How files collaborate, depend on each other, or share state.\n- **Behavioral Contracts**: If files contain regex patterns, format specifications, magic constants, or template strings that define observable behavior, collect them in a dedicated section. Preserve verbatim patterns from file summaries — do NOT paraphrase regex into prose. This section is MANDATORY when file summaries contain behavioral artifacts.\n- **Reproduction-Critical Constants**: If file summaries reference annex files (via ## Annex References sections), list them with links. Example: \"Full prompt template text: [templates.annex.sum](./prompts/templates.annex.sum)\". Do NOT reproduce annex content in AGENTS.md — just link to it.\n- **Workflow & Conventions**: If file summaries contain contribution guidelines, PR conventions, commit standards, testing mandates, tool usage rules, AI agent instructions, or code conventions (naming, formatting, linting), collect them into actionable directives. Source from CONTRIBUTING.md, CI configs, PR templates, linter/formatter configs, README dev sections. Present as concrete rules (\"commits must use conventionalcommits format\", \"use camelCase for functions, PascalCase for types\") not vague descriptions (\"the project has guidelines\").\n\nChoose any relevant sections or create your own based on the directory contents. The goal is to provide a comprehensive overview that captures the essence of the directory's role in the project and how its files work together, with a focus on what an AI coding assistant would need to know to effectively interact with this code.\n\nSCOPE:\n- AGENTS.md is a NAVIGATIONAL INDEX — help an AI find the right file quickly\n- Focus on: what each file does, how files relate, directory-level patterns\n- Do NOT reproduce full architecture sections — those belong in the root CLAUDE.md\n\nPATH ACCURACY (MANDATORY):\n- When referencing files or modules outside this directory, use ONLY paths from the \"Import Map\" section\n- Do NOT invent, rename, or guess module paths — if a path isn't in the Import Map, don't reference it\n- Use the exact directory names from \"Project Directory Structure\" — do NOT rename directories\n  (e.g., if the directory is called \"cli\", write \"src/cli/\", NOT \"src/commands/\")\n- Cross-module references must use the specifier format from actual import statements\n  (e.g., \"../generation/writers/sum.js\", NOT \"../fs/sum-file.js\")\n- If you are unsure about a path, omit the cross-reference rather than guessing\n\nCONSISTENCY (MANDATORY):\n- Do not contradict yourself within the same document\n- If you describe a technique (e.g., \"regex-based\"), do not call it something else later (e.g., \"AST-based\")\n- When stating version numbers, engines, or config fields, use ONLY values present in the file summaries\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this directory\", \"this module\", \"provides\", \"responsible for\", \"is used to\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Per-file descriptions in Contents sections: 1-2 sentences maximum. Reference key symbols but do not reproduce full summaries.\n- Behavioral contracts (regex patterns, format specs, constants) belong in a separate Behavioral Contracts section, not in per-file descriptions.\n- Annex file references: link to .annex.sum files, do not inline their content. One line per annex reference.\n- Subdirectory descriptions: 1-2 sentences maximum. Capture the directory's role, not its full contents.\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- Key exported symbols from file summaries MUST appear in the directory overview\n- Preserve exact casing of identifiers\n\nUSER NOTES:\n- If \"User Notes\" are provided in the prompt, they contain user-defined instructions that will be automatically prepended to your output\n- Do NOT repeat or paraphrase user notes in your generated content — they are included separately\n- You may reference information from user notes for context",
      "response": "<!-- Generated by agents-reverse-engineer -->\n\n# src/installer\n\nThe installer directory handles interactive and automated installation of ARE commands, hooks, and plugins across multiple AI coding runtimes (Claude Code, Codex, OpenCode, Gemini), managing both global user-level and local project-level configurations with cross-platform path resolution and settings.json registration.\n\n## Contents\n\n### Entry Point & Orchestration\n- [index.ts](./index.ts) — Main installer entry point; exports `parseInstallerArgs()`, `runInstaller()`, and mode-determination functions (`determineLocation()`, `determineRuntimes()`); orchestrates install/uninstall workflows with interactive prompts or CLI flags.\n\n### User Interface & Prompts\n- [banner.ts](./banner.ts) — Styled console output via picocolors; exports `displayBanner()`, `showHelp()`, `showSuccess()`, `showError()`, `showWarning()`, `showInfo()`, `showNextSteps()`, and `VERSION` constant.\n- [prompts.ts](./prompts.ts) — Interactive terminal selection with arrow-key (TTY) and numbered (CI) fallback; exports `isInteractive()`, `selectRuntime()`, `selectLocation()`, `confirmAction()`, and raw mode lifecycle management.\n\n### Installation & Uninstallation\n- [operations.ts](./operations.ts) — Command template, hook, and plugin installation logic; exports `installFiles()`, `registerHooks()`, `registerPermissions()`, `verifyInstallation()`, `getPackageVersion()`, `writeVersionFile()`; routes to runtime-specific installers and manages settings.json registration.\n- [uninstall.ts](./uninstall.ts) — Reverses installation across all runtimes; exports `uninstallFiles()`, `unregisterHooks()`, `unregisterPermissions()`, `deleteConfigFolder()`; handles hook removal, Codex rules cleanup, legacy file cleanup, and empty directory recursion.\n\n### Path Resolution & Type Definitions\n- [paths.ts](./paths.ts) — Cross-platform config directory resolution; exports `getRuntimePaths()`, `resolveInstallPath()`, `resolveCodexConfigPath()`, `getSettingsPath()`, `getAllRuntimes()`, and installation detection functions (`isRuntimeInstalledLocally()`, `isRuntimeInstalledGlobally()`, `getInstalledRuntimes()`).\n- [types.ts](./types.ts) — Type signatures for CLI args, installation results, and runtime paths; exports `Runtime`, `Location`, `InstallerArgs`, `InstallerResult`, `RuntimePaths`.\n\n## Workflow & Conventions\n\n**CLI Flags & Modes:**\n- `--runtime {claude|codex|opencode|gemini|all}` specifies target runtime(s); omitted flags trigger interactive prompts.\n- `-g/--global`, `-l/--local` specify installation scope; both or neither requires prompt.\n- `--force` overwrites existing files (only those marked ARE-generated); preserves user files.\n- `-q/--quiet` suppresses display messages; `--help` calls `showHelp()` and exits.\n- Non-interactive mode (no TTY, e.g., CI) requires `--runtime` and `-g`/`--local` flags; missing flags trigger `process.exit(1)`.\n\n**Result Reporting:**\n- `displayInstallResults()` aggregates per-runtime results (`filesCreated`, `filesSkipped`, `hookRegistered`), displays summary counts and post-install next steps referencing `/are-*` commands.\n- `displayUninstallResults()` reuses `InstallerResult` fields (filesCreated tracks deletions); reports aggregate file removals and config folder deletion status.\n\n**Hook Registration & Permissions:**\n- Claude/Gemini: `registerHooks()` merges hook entries into settings.json without duplicating; Claude supports SessionStart, PostToolUse, SessionEnd events; Gemini supports SessionStart, SessionEnd only.\n- Claude: `registerPermissions()` adds Bash command patterns (`npx are init`, `npx are discover`, etc.) to settings.allow to reduce friction.\n- Codex: Generates `are.rules` prefix rules file and `AGENTS.override.md` context-loading instructions.\n\n**File Management:**\n- `shouldWriteManagedFile()` checks for ARE-generation marker before overwriting; `--force` flag bypasses for ARE-marked files.\n- `writeVersionFile()` writes ARE-VERSION tracking installed version.\n- `ensureDir()` creates parent directories before file writes.\n\n## Behavioral Contracts\n\n**Runtime Config Directory Naming:**\n- Claude: `.claude`\n- Codex: `.agents` (skills), `.codex` (CLI rules)\n- OpenCode: `.opencode`\n- Gemini: `.gemini`\n\n**Settings.json Hook Command Format (Claude):**\n```\nnode .claude/hooks/are-check-update.js\nnode .claude/hooks/are-context-loader.js\n```\n\n**Gemini Hook Format (Simplified):**\nFlat hook objects with `name`, `type`, `command` fields; SessionStart/SessionEnd only (no PostToolUse).\n\n**Codex Prefix Rules** (`CODEX_PREFIX_RULES`):\n```\n['npx', 'are', 'init']\n['npx', 'are', 'discover']\n['npx', 'are', 'generate']\n['npx', 'are', 'update']\n['npx', 'are', 'specify']\n['npx', 'are', 'rebuild']\n['npx', 'are', 'clean']\n['rm', '-f', '.agents-reverse-engineer/progress.log']\n['sleep']\n```\n\n**ARE-Specific Markers & Files:**\n- Hook existence check: command string match in settings.json; no duplicates on re-registration.\n- ARE-VERSION filename: written to config root (`basePath/ARE-VERSION`).\n- ARE-generated marker: `'<!-- Generated by agents-reverse-engineer -->'` in file content; identifies files eligible for `--force` overwrite.\n- Codex context-rules marker: `'<!-- Generated by agents-reverse-engineer installer: local-context-rules -->'`; only marked files deleted on uninstall.\n\n**Dry Run Behavior:**\n- `dryRun=true` skips `writeFileSync` calls; returns `InstallerResult` with `filesCreated` populated for preview.\n- Verifies installations via file existence checks; missing files added to `filesSkipped` list.\n\n## Installation Architecture\n\n**Runtime-Specific Routing:**\n- [operations.ts](./operations.ts) dispatches `installFiles()` to `installFilesForRuntime()` per runtime or expands 'all' to concrete runtimes.\n- Command templates sourced from `../integration/templates.js` (`getClaudeTemplates()`, `getCodexTemplates()`, `getOpenCodeTemplates()`, `getGeminiTemplates()`).\n- Hooks bundled in `dist/installer/operations.js` context; resolved via `getBundledHookPath()`.\n\n**Claude/Gemini:** Install session hooks + register in settings.json.\n**Codex:** Generate rules files (`are.rules`) + context-loading instructions (`AGENTS.override.md`).\n**OpenCode:** Install plugins from bundled sources (currently empty `ARE_PLUGINS` array); infrastructure setup (package.json, bun.lock, node_modules/).\n\n**Uninstall Logic:**\n- [uninstall.ts](./uninstall.ts) reverses all installation steps: removes templates, hook files, plugins, Codex rules/context-rules, ARE-VERSION; unregisters hooks/permissions in settings.json.\n- Hook pattern matching: `node {runtimeDir}/hooks/{filename}` (current) and `node hooks/{filename}` (legacy).\n- Cleanup utilities: `cleanupAreSkillDirs()`, `cleanupEmptyDirs()` (recursive), `cleanupLegacyGeminiFiles()`, `cleanupOpenCodeInfrastructure()`.",
      "model": "claude-haiku-4-5-20251001",
      "inputTokens": 3,
      "outputTokens": 1922,
      "cacheReadTokens": 17512,
      "cacheCreationTokens": 8301,
      "latencyMs": 42904,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": []
    },
    {
      "timestamp": "2026-02-14T13:49:49.286Z",
      "prompt": "Generate AGENTS.md for directory: \"src/update\" (update)\n\n## File Summaries (3 files)\n\n### index.ts\n**Purpose:** src/update/index.ts re-exports orchestration and cleanup utilities for incremental documentation updates.\n\n**src/update/index.ts re-exports orchestration and cleanup utilities for incremental documentation updates.**\n\n## Exports\n\n**Orchestrator**: `DocumentationOrchestrator` (aliased `UpdateOrchestrator`), `createUpdateOrchestrator()` — re-exported from `../orchestration/orchestrator.js` for state management and change detection.\n\n**Cleanup**: `cleanupOrphans()`, `cleanupEmptyDirectoryDocs()`, `getAffectedDirectories()` — orphan removal and affected-path detection from `./orphan-cleaner.js`.\n\n**Types**: `UpdatePlanOptions`, `UpdateResult`, `UpdateProgress`, `CleanupResult` — re-exported from `./types.js` for plan configuration, execution results, progress tracking, and cleanup outcome shapes.\n### orphan-cleaner.ts\n**Purpose:** orphan-cleaner.ts manages cleanup of stale .sum/.annex.sum files and AGENTS.md when source files are deleted or renamed.\n\n**orphan-cleaner.ts manages cleanup of stale .sum/.annex.sum files and AGENTS.md when source files are deleted or renamed.**\n\n## Exports\n\n`cleanupOrphans(projectRoot, changes, dryRun?, variant?)` → `Promise<CleanupResult>`: Deletes .sum/.annex.sum files for deleted/renamed sources; removes AGENTS.md from directories with no source files remaining.\n\n`cleanupEmptyDirectoryDocs(dirPath, dryRun?, agentsFilename?)` → `Promise<boolean>`: Deletes AGENTS.md/.variant.md if directory contains no source files.\n\n`getAffectedDirectories(changes)` → `Set<string>`: Returns all parent directory paths (up to root) for non-deleted file changes; used to identify which AGENTS.md files need regeneration.\n\n## Key Dependencies\n\n`getSumPath`, `getAnnexPath` from `../generation/writers/sum.js`: Compute paths for .sum and .annex.sum documentation files.\n\n`FileChange` type: Tracks file status (`deleted`, `renamed`) and paths.\n\n## Design Patterns\n\nCleanup operates in two phases: (1) delete .sum/.annex.sum for deleted/renamed sources, (2) prune empty directory documentation. Supports dry-run mode via `dryRun` parameter. Variant support allows per-variant .sum/.md files via optional `variant` parameter (e.g., AGENTS.v2.md).\n\n## Behavioral Contracts\n\n**GENERATED_FILES Set**: `['AGENTS.md', 'CLAUDE.md']` — constant file names ignored as source files when determining if directory is empty.\n\n**Source file detection** in `cleanupEmptyDirectoryDocs`: Counts as source if: not hidden (no leading dot), not `.sum` file, not in GENERATED_FILES, not AGENTS.*.md pattern. Deletes AGENTS.md only if zero source files remain.\n\n**Dry-run semantics**: `dryRun=true` skips actual `unlink()` calls but returns `true` to indicate what would be deleted.\n### types.ts\n**Purpose:** update/types.ts defines type contracts for incremental documentation updates, tracking file changes, cleanup operatio...\n\n**update/types.ts defines type contracts for incremental documentation updates, tracking file changes, cleanup operations, and progress callbacks.**\n\n## Exports\n\n**CleanupResult** — object shape: `deletedSumFiles: string[]`, `deletedAgentsMd: string[]`; tracks orphaned documentation artifacts removed during updates.\n\n**UpdatePlanOptions** — object shape: `includeUncommitted?: boolean`, `dryRun?: boolean`, `variant?: string`; configures update scope (uncommitted changes, dry-run mode, variant-specific .sum files).\n\n**UpdateResult** — object shape: `analyzedFiles: string[]`, `skippedFiles: string[]`, `cleanup: CleanupResult`, `regeneratedDirs: string[]`, `baseCommit: string`, `currentCommit: string`, `dryRun: boolean`; aggregates update operation outcomes including file tallies, deletions, git state.\n\n**UpdateProgress** — callback interface: `onFileStart?(path, status: 'analyzing'|'skipping')`, `onFileDone?(path, status: 'analyzed'|'skipped'|'error')`, `onCleanup?(path, type: 'sum'|'agents-md')`, `onDirRegenerate?(path)`; enables progress instrumentation during incremental updates.\n\n## Dependencies\n\nImports `FileChange` from `change-detection/types.js`; consumer workflows likely use this to correlate detected changes with update operations.\n\n## Import Map (verified — use these exact paths)\n\norphan-cleaner.ts:\n  ../change-detection/types.js → FileChange (type)\n  ../generation/writers/sum.js → getSumPath, getAnnexPath\n\ntypes.ts:\n  ../change-detection/types.js → FileChange (type)\n\n## Project Directory Structure\n\n<project-structure>\n./\n  .npmrc\n  CONTRIBUTING.md\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  pages.yml\n  publish.yml\nhooks/\n  are-check-update.js\n  are-context-loader.js\n  opencode-are-check-update.js\nscripts/\n  analyze-agents-compression.sh\n  analyze-sum-ratios.sh\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  codex.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/providers/\n  subprocess.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  rebuild.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/core/\n  index.ts\n  logger.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  claude-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  orchestrator.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/rebuild/\n  checkpoint.ts\n  index.ts\n  orchestrator.ts\n  output-parser.ts\n  prompts.ts\n  spec-reader.ts\n  types.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>",
      "systemPrompt": "You are generating an AGENTS.md file — a directory-level overview for AI coding assistants.\n\nCRITICAL: Output ONLY the raw markdown content. No code fences, no preamble, no explanations, no conversational text. Your entire response IS the AGENTS.md file content.\n\nOUTPUT FORMAT:\n- First line MUST be exactly: <!-- Generated by agents-reverse-engineer -->\n- Use a # heading with the directory name\n- Write a one-paragraph purpose statement for the directory\nusage\nADAPTIVE SECTIONS:\nAnalyze the directory contents and choose the most relevant sections. Do NOT use a fixed template. Instead, select sections that best document this specific directory for an AI that needs to reconstruct or extend the project.\n\nConsider these section types (choose what applies):\n- **Contents**: Group files by purpose/category under ## headings. For each file: markdown link [filename](./filename) and a one-line description.\n- **Subdirectories**: If subdirectories exist, list them with links [dirname/](./dirname/) and brief summaries.\n- **Architecture / Data Flow**: If files form a pipeline, request/response chain, or layered architecture, document it.\n- **Stack**: If this is a package root (has package.json, Cargo.toml, go.mod, etc.), document the technology stack, key scripts, and entry points.\n- **Structure**: If the directory layout follows a convention (feature-sliced, domain-driven, MVC, etc.), document it.\n- **Patterns**: If files share recurring design patterns (factory, strategy, middleware, barrel re-export), name and document them.\n- **Configuration**: If the directory contains config files, schemas, or environment definitions, document the config surface area.\n- **API Surface**: If the directory exports a public API (barrel index, route definitions, SDK), document the interface contract.\n- **File Relationships**: How files collaborate, depend on each other, or share state.\n- **Behavioral Contracts**: If files contain regex patterns, format specifications, magic constants, or template strings that define observable behavior, collect them in a dedicated section. Preserve verbatim patterns from file summaries — do NOT paraphrase regex into prose. This section is MANDATORY when file summaries contain behavioral artifacts.\n- **Reproduction-Critical Constants**: If file summaries reference annex files (via ## Annex References sections), list them with links. Example: \"Full prompt template text: [templates.annex.sum](./prompts/templates.annex.sum)\". Do NOT reproduce annex content in AGENTS.md — just link to it.\n- **Workflow & Conventions**: If file summaries contain contribution guidelines, PR conventions, commit standards, testing mandates, tool usage rules, AI agent instructions, or code conventions (naming, formatting, linting), collect them into actionable directives. Source from CONTRIBUTING.md, CI configs, PR templates, linter/formatter configs, README dev sections. Present as concrete rules (\"commits must use conventionalcommits format\", \"use camelCase for functions, PascalCase for types\") not vague descriptions (\"the project has guidelines\").\n\nChoose any relevant sections or create your own based on the directory contents. The goal is to provide a comprehensive overview that captures the essence of the directory's role in the project and how its files work together, with a focus on what an AI coding assistant would need to know to effectively interact with this code.\n\nSCOPE:\n- AGENTS.md is a NAVIGATIONAL INDEX — help an AI find the right file quickly\n- Focus on: what each file does, how files relate, directory-level patterns\n- Do NOT reproduce full architecture sections — those belong in the root CLAUDE.md\n\nPATH ACCURACY (MANDATORY):\n- When referencing files or modules outside this directory, use ONLY paths from the \"Import Map\" section\n- Do NOT invent, rename, or guess module paths — if a path isn't in the Import Map, don't reference it\n- Use the exact directory names from \"Project Directory Structure\" — do NOT rename directories\n  (e.g., if the directory is called \"cli\", write \"src/cli/\", NOT \"src/commands/\")\n- Cross-module references must use the specifier format from actual import statements\n  (e.g., \"../generation/writers/sum.js\", NOT \"../fs/sum-file.js\")\n- If you are unsure about a path, omit the cross-reference rather than guessing\n\nCONSISTENCY (MANDATORY):\n- Do not contradict yourself within the same document\n- If you describe a technique (e.g., \"regex-based\"), do not call it something else later (e.g., \"AST-based\")\n- When stating version numbers, engines, or config fields, use ONLY values present in the file summaries\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this directory\", \"this module\", \"provides\", \"responsible for\", \"is used to\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Per-file descriptions in Contents sections: 1-2 sentences maximum. Reference key symbols but do not reproduce full summaries.\n- Behavioral contracts (regex patterns, format specs, constants) belong in a separate Behavioral Contracts section, not in per-file descriptions.\n- Annex file references: link to .annex.sum files, do not inline their content. One line per annex reference.\n- Subdirectory descriptions: 1-2 sentences maximum. Capture the directory's role, not its full contents.\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- Key exported symbols from file summaries MUST appear in the directory overview\n- Preserve exact casing of identifiers\n\nUSER NOTES:\n- If \"User Notes\" are provided in the prompt, they contain user-defined instructions that will be automatically prepended to your output\n- Do NOT repeat or paraphrase user notes in your generated content — they are included separately\n- You may reference information from user notes for context",
      "response": "<!-- Generated by agents-reverse-engineer -->\n\n# src/update\n\nThe update directory orchestrates incremental documentation regeneration by detecting file changes, managing affected directories, and cleaning up orphaned .sum and AGENTS.md files from deleted or renamed sources.\n\n## Contents\n\n**Orchestration & Types**\n- [index.ts](./index.ts) — Re-exports `DocumentationOrchestrator` / `UpdateOrchestrator`, `createUpdateOrchestrator()`, cleanup utilities, and type contracts for plan configuration and execution results.\n- [types.ts](./types.ts) — Defines `CleanupResult`, `UpdatePlanOptions`, `UpdateResult`, `UpdateProgress` callback interface for progress instrumentation during incremental updates.\n\n**Cleanup & Detection**\n- [orphan-cleaner.ts](./orphan-cleaner.ts) — Implements `cleanupOrphans()`, `cleanupEmptyDirectoryDocs()`, `getAffectedDirectories()` to remove stale .sum/.annex.sum files and AGENTS.md when sources are deleted/renamed.\n\n## Update Workflow\n\nIncremental updates follow a three-phase pattern:\n\n1. **Change Detection**: Correlates file deletions/renames with their documentation via `FileChange` from `../change-detection/types.js`.\n2. **Orphan Cleanup**: `cleanupOrphans()` deletes .sum/.annex.sum for deleted/renamed sources; `cleanupEmptyDirectoryDocs()` removes AGENTS.md if no source files remain in a directory.\n3. **Affected Path Computation**: `getAffectedDirectories()` identifies all parent directory paths (up to root) requiring AGENTS.md regeneration.\n\nProgress is tracked via `UpdateProgress` callbacks: `onFileStart`, `onFileDone`, `onCleanup` (for .sum/.agents-md deletions), `onDirRegenerate` (for AGENTS.md regeneration).\n\n## Cleanup Semantics\n\n**GENERATED_FILES Set**: `['AGENTS.md', 'CLAUDE.md']` — constant excluded from source file counting; directory is \"empty\" only when no non-generated source files remain.\n\n**Source Detection**: A file counts as a source if not hidden (no leading dot), not a `.sum` file, not in GENERATED_FILES, and does not match AGENTS.*.md pattern.\n\n**Dry-run Mode**: When `dryRun=true`, `cleanupEmptyDirectoryDocs()` skips filesystem mutations but returns `true` to indicate what would be deleted; `cleanupOrphans()` tracks all would-be deletions in `CleanupResult` without writing to disk.\n\n**Variant Support**: Both cleanup and orchestration accept optional `variant` parameter (e.g., \"v2\") to handle per-variant .sum and AGENTS.md files simultaneously.\n\n## Type Contracts\n\n**UpdateResult** aggregates outcomes: `analyzedFiles[]`, `skippedFiles[]`, `cleanup: CleanupResult`, `regeneratedDirs[]`, git state via `baseCommit`, `currentCommit`, and `dryRun` flag.\n\n**UpdatePlanOptions** configures scope: `includeUncommitted` (track uncommitted changes), `dryRun`, `variant`.\n\n**CleanupResult** tracks deletion artifacts: `deletedSumFiles[]`, `deletedAgentsMd[]`.",
      "model": "claude-haiku-4-5-20251001",
      "inputTokens": 3,
      "outputTokens": 786,
      "cacheReadTokens": 0,
      "cacheCreationTokens": 20593,
      "latencyMs": 18446,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": []
    },
    {
      "timestamp": "2026-02-14T13:49:42.308Z",
      "prompt": "Generate AGENTS.md for directory: \"src/specify\" (specify)\n\n## File Summaries (3 files)\n\n### index.ts\n**Purpose:** `index.ts` re-exports specification generation and writing utilities from `prompts.js` and `writer.js` modules.\n\n**`index.ts` re-exports specification generation and writing utilities from `prompts.js` and `writer.js` modules.**\n\n## Exported Functions\n- `buildSpecPrompt()` — generates specification prompt from `prompts.js`\n- `writeSpec()` — writes specification file from `writer.js`\n\n## Exported Types\n- `SpecPrompt` — specification prompt type from `prompts.js`\n- `WriteSpecOptions` — options type for `writeSpec()` from `writer.js`\n\n## Exported Errors\n- `SpecExistsError` — error class from `writer.js` for existing specification conflicts\n### prompts.ts\n**Purpose:** prompts.ts constructs system + user prompt pairs for AI-driven specification generation from AGENTS.md documentation.\n\n**prompts.ts constructs system + user prompt pairs for AI-driven specification generation from AGENTS.md documentation.**\n\n## Exports\n\n`SpecPrompt` — interface with `system: string` and `user: string` fields for prompt pairs.\n\n`SPEC_SYSTEM_PROMPT` — system prompt defining specification synthesis rules for AI agents.\n\n`buildSpecPrompt(docs: AgentsDocs, annexFiles?: AgentsDocs): SpecPrompt` — assembles user prompt by injecting AGENTS.md sections and optional annex files with structured output requirements.\n\n## Specification Schema (SPEC_SYSTEM_PROMPT)\n\nMandates 12-section specification structure for AI reconstruction:\n1. Project Overview (purpose, value proposition, tech stack with versions)\n2. Architecture (module boundaries, data flow, design decisions)\n3. Public API Surface (exported interfaces, full type signatures)\n4. Data Structures & State (types, schemas, config objects, state patterns)\n5. Configuration (options, types, defaults, validation, environment variables)\n6. Dependencies (exact versions, rationale)\n7. Behavioral Contracts (error handling, retry logic, concurrency, lifecycle; verbatim regex patterns in backticks, format strings, magic constants, environment variable names, file format specs)\n8. Test Contracts (per-module scenarios, edge cases, error conditions)\n9. Build Plan (phased implementation with explicit \"Defines:\" and \"Consumes:\" lists cross-referencing Public API Surface section)\n10. Prompt Templates & System Instructions (full verbatim text from annex files, placeholder syntax preserved)\n11. IDE Integration & Installer (command templates, platform configs, permission lists — all verbatim)\n12. File Manifest (exhaustive source file list with relative path, module, public exports)\n\n## Prompts Composition Rules\n\n`SPEC_SYSTEM_PROMPT` enforces:\n- Conceptual grouping by concern, not directory structure\n- No folder-mirroring or exact file path prescription\n- Module boundaries and interfaces described, not file paths\n- Full type signatures for all public APIs\n- Exact function, type, constant names from documentation\n- All external dependency versions included\n- Build Plan phases must cross-reference Public API Surface; each phase lists \"Defines:\" and \"Consumes:\" with exact names\n- Behavioral Contracts must specify exact error types/codes and when thrown\n- Regex patterns, format strings, magic constants reproduced verbatim in backticks\n- Sections 10–11 require verbatim annex reproduction (no paraphrasing/summarization)\n- File Manifest entries must align with Build Plan phases (each file listed in manifest, each phase references which files it produces)\n\n`buildSpecPrompt()` constructs user prompt with:\n- Header: \"Generate a comprehensive project specification from the following documentation\"\n- Section: \"## AGENTS.md Files (N directories)\" with docs mapped as `### {relativePath}\\n\\n{content}`\n- Optional section: \"## Annex Files (M reproduction-critical source files)\" with annexFiles mapped identically\n- Section: \"## Output Requirements\" listing 12-section mandate and verbatim reproduction requirements\n- Footer: \"Output ONLY the markdown content. No preamble.\"\n### writer.ts\n**Purpose:** writer.ts exports spec file writing utilities with overwrite protection and multi-file splitting on markdown headings.\n\n**writer.ts exports spec file writing utilities with overwrite protection and multi-file splitting on markdown headings.**\n\n## Exports\n\n**WriteSpecOptions** interface: `{ outputPath: string, force: boolean, multiFile: boolean }` — controls spec output path, overwrite behavior, and heading-based file splitting.\n\n**SpecExistsError** class: extends Error with `paths: string[]` property — thrown when target files exist and `force=false`; message lists conflicting paths with instructions to use `--force`.\n\n**writeSpec**(content: string, options: WriteSpecOptions): Promise<string[]> — writes AI-generated markdown to disk; single-file mode writes directly to `outputPath`; multi-file mode splits on `^# ` regex and writes sections to separate files in `outputPath` directory; throws SpecExistsError on conflicts unless `force=true`; returns array of written absolute paths.\n\n## Internal Utilities\n\n**fileExists**(filePath: string): Promise<boolean> — checks existence via `fs.access()` with `constants.F_OK`.\n\n**slugify**(heading: string): string — converts heading to filename-safe slug via lowercase, whitespace→hyphens (`/\\s+/g`), strip non-alphanumeric (`/[^a-z0-9-]/g`), collapse hyphens (`/-+/g`), trim edges (`/^-|-$/g`).\n\n**splitByHeadings**(content: string): Array<{ filename, content }> — splits markdown on `^(?=# )/m` regex; preamble (pre-first-heading content) → `00-preamble.md`; each section named `[slug].md` from heading text or `00-preamble.md` if slug empty.\n\n## Behavior & Design\n\nSingle-file mode writes content as-is; multi-file mode requires heading-based splitting and directory structure. Conflict detection checks all target paths before any writes (atomic validation). Output uses UTF-8 encoding. Parent directories created recursively via `mkdir(..., { recursive: true })`.\n\n## Import Map (verified — use these exact paths)\n\nprompts.ts:\n  ../generation/collector.js → AgentsDocs (type)\n\n## Project Directory Structure\n\n<project-structure>\n./\n  .npmrc\n  CONTRIBUTING.md\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  pages.yml\n  publish.yml\nhooks/\n  are-check-update.js\n  are-context-loader.js\n  opencode-are-check-update.js\nscripts/\n  analyze-agents-compression.sh\n  analyze-sum-ratios.sh\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  codex.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/providers/\n  subprocess.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  rebuild.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/core/\n  index.ts\n  logger.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  claude-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  orchestrator.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/rebuild/\n  checkpoint.ts\n  index.ts\n  orchestrator.ts\n  output-parser.ts\n  prompts.ts\n  spec-reader.ts\n  types.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>",
      "systemPrompt": "You are generating an AGENTS.md file — a directory-level overview for AI coding assistants.\n\nCRITICAL: Output ONLY the raw markdown content. No code fences, no preamble, no explanations, no conversational text. Your entire response IS the AGENTS.md file content.\n\nOUTPUT FORMAT:\n- First line MUST be exactly: <!-- Generated by agents-reverse-engineer -->\n- Use a # heading with the directory name\n- Write a one-paragraph purpose statement for the directory\nusage\nADAPTIVE SECTIONS:\nAnalyze the directory contents and choose the most relevant sections. Do NOT use a fixed template. Instead, select sections that best document this specific directory for an AI that needs to reconstruct or extend the project.\n\nConsider these section types (choose what applies):\n- **Contents**: Group files by purpose/category under ## headings. For each file: markdown link [filename](./filename) and a one-line description.\n- **Subdirectories**: If subdirectories exist, list them with links [dirname/](./dirname/) and brief summaries.\n- **Architecture / Data Flow**: If files form a pipeline, request/response chain, or layered architecture, document it.\n- **Stack**: If this is a package root (has package.json, Cargo.toml, go.mod, etc.), document the technology stack, key scripts, and entry points.\n- **Structure**: If the directory layout follows a convention (feature-sliced, domain-driven, MVC, etc.), document it.\n- **Patterns**: If files share recurring design patterns (factory, strategy, middleware, barrel re-export), name and document them.\n- **Configuration**: If the directory contains config files, schemas, or environment definitions, document the config surface area.\n- **API Surface**: If the directory exports a public API (barrel index, route definitions, SDK), document the interface contract.\n- **File Relationships**: How files collaborate, depend on each other, or share state.\n- **Behavioral Contracts**: If files contain regex patterns, format specifications, magic constants, or template strings that define observable behavior, collect them in a dedicated section. Preserve verbatim patterns from file summaries — do NOT paraphrase regex into prose. This section is MANDATORY when file summaries contain behavioral artifacts.\n- **Reproduction-Critical Constants**: If file summaries reference annex files (via ## Annex References sections), list them with links. Example: \"Full prompt template text: [templates.annex.sum](./prompts/templates.annex.sum)\". Do NOT reproduce annex content in AGENTS.md — just link to it.\n- **Workflow & Conventions**: If file summaries contain contribution guidelines, PR conventions, commit standards, testing mandates, tool usage rules, AI agent instructions, or code conventions (naming, formatting, linting), collect them into actionable directives. Source from CONTRIBUTING.md, CI configs, PR templates, linter/formatter configs, README dev sections. Present as concrete rules (\"commits must use conventionalcommits format\", \"use camelCase for functions, PascalCase for types\") not vague descriptions (\"the project has guidelines\").\n\nChoose any relevant sections or create your own based on the directory contents. The goal is to provide a comprehensive overview that captures the essence of the directory's role in the project and how its files work together, with a focus on what an AI coding assistant would need to know to effectively interact with this code.\n\nSCOPE:\n- AGENTS.md is a NAVIGATIONAL INDEX — help an AI find the right file quickly\n- Focus on: what each file does, how files relate, directory-level patterns\n- Do NOT reproduce full architecture sections — those belong in the root CLAUDE.md\n\nPATH ACCURACY (MANDATORY):\n- When referencing files or modules outside this directory, use ONLY paths from the \"Import Map\" section\n- Do NOT invent, rename, or guess module paths — if a path isn't in the Import Map, don't reference it\n- Use the exact directory names from \"Project Directory Structure\" — do NOT rename directories\n  (e.g., if the directory is called \"cli\", write \"src/cli/\", NOT \"src/commands/\")\n- Cross-module references must use the specifier format from actual import statements\n  (e.g., \"../generation/writers/sum.js\", NOT \"../fs/sum-file.js\")\n- If you are unsure about a path, omit the cross-reference rather than guessing\n\nCONSISTENCY (MANDATORY):\n- Do not contradict yourself within the same document\n- If you describe a technique (e.g., \"regex-based\"), do not call it something else later (e.g., \"AST-based\")\n- When stating version numbers, engines, or config fields, use ONLY values present in the file summaries\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this directory\", \"this module\", \"provides\", \"responsible for\", \"is used to\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Per-file descriptions in Contents sections: 1-2 sentences maximum. Reference key symbols but do not reproduce full summaries.\n- Behavioral contracts (regex patterns, format specs, constants) belong in a separate Behavioral Contracts section, not in per-file descriptions.\n- Annex file references: link to .annex.sum files, do not inline their content. One line per annex reference.\n- Subdirectory descriptions: 1-2 sentences maximum. Capture the directory's role, not its full contents.\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- Key exported symbols from file summaries MUST appear in the directory overview\n- Preserve exact casing of identifiers\n\nUSER NOTES:\n- If \"User Notes\" are provided in the prompt, they contain user-defined instructions that will be automatically prepended to your output\n- Do NOT repeat or paraphrase user notes in your generated content — they are included separately\n- You may reference information from user notes for context",
      "response": "<!-- Generated by agents-reverse-engineer -->\n\n# src/specify\n\nThe `specify` directory exports utilities for generating AI-readable project specifications from AGENTS.md documentation. It provides prompt construction for specification synthesis and disk writing with conflict detection and optional multi-file splitting.\n\n## Contents\n\n### Core Exports\n- [index.ts](./index.ts) — Barrel re-export of `buildSpecPrompt()`, `writeSpec()`, `SpecPrompt`, `WriteSpecOptions`, and `SpecExistsError`.\n- [prompts.ts](./prompts.ts) — Constructs `SpecPrompt` (system + user pair) for AI specification generation; enforces 12-section specification schema via `SPEC_SYSTEM_PROMPT`.\n- [writer.ts](./writer.ts) — Writes specification markdown to disk via `writeSpec()`; supports single-file and multi-file (heading-split) modes; enforces overwrite protection via `SpecExistsError`.\n\n## Architecture\n\n`buildSpecPrompt()` ingests `AgentsDocs` and optional annex files (both from `../generation/collector.js`), injects them into the user prompt template, and returns structured `SpecPrompt`. `writeSpec()` accepts AI-generated markdown content and `WriteSpecOptions`, validates output paths, splits on `^# ` regex if multi-file mode, creates parent directories, and returns written absolute paths or throws `SpecExistsError` on conflicts (unless `force=true`).\n\n## Specification Schema (SPEC_SYSTEM_PROMPT)\n\nMandates 12-section AI reconstruction structure:\n\n1. **Project Overview** — purpose, value proposition, tech stack with versions\n2. **Architecture** — module boundaries, data flow, design decisions\n3. **Public API Surface** — exported interfaces with full type signatures\n4. **Data Structures & State** — types, schemas, config objects, state patterns\n5. **Configuration** — options, types, defaults, validation, environment variables\n6. **Dependencies** — exact versions and rationale\n7. **Behavioral Contracts** — error handling, retry logic, concurrency, lifecycle; verbatim regex patterns in backticks, format strings, magic constants, environment variable names\n8. **Test Contracts** — per-module scenarios, edge cases, error conditions\n9. **Build Plan** — phased implementation with explicit \"Defines:\" and \"Consumes:\" lists cross-referencing Public API Surface\n10. **Prompt Templates & System Instructions** — full verbatim text from annex files\n11. **IDE Integration & Installer** — command templates, platform configs, permission lists (verbatim)\n12. **File Manifest** — exhaustive source file list with relative path, module, public exports\n\n## Behavioral Contracts\n\n**`prompts.ts` enforcement rules:**\n- Conceptual grouping by concern, not directory structure\n- Module boundaries and interfaces described, not file paths\n- Full type signatures for all public APIs\n- All external dependency versions included\n- Build Plan phases must cross-reference Public API Surface; each phase lists \"Defines:\" and \"Consumes:\" with exact names\n- Regex patterns, format strings, magic constants reproduced verbatim in backticks\n- Sections 10–11 require verbatim annex reproduction\n- File Manifest entries must align with Build Plan phases\n\n**`writer.ts` file handling:**\n- `fileExists()` checks via `fs.access(..., constants.F_OK)`\n- `slugify()` converts headings via lowercase, whitespace→hyphens (`/\\s+/g`), strip non-alphanumeric (`/[^a-z0-9-]/g`), collapse hyphens (`/-+/g`), trim edges (`/^-|-$/g`)\n- `splitByHeadings()` splits on `^(?=# )/m` regex; preamble → `00-preamble.md`; each section named `[slug].md`\n- Conflict detection checks all target paths before writes (atomic validation)\n- Parent directories created via `mkdir(..., { recursive: true })`; UTF-8 encoding used\n\n## API Surface\n\n**`buildSpecPrompt(docs: AgentsDocs, annexFiles?: AgentsDocs): SpecPrompt`** — assembles user prompt with AGENTS.md sections and optional annex files, returns `{ system: string, user: string }`.\n\n**`writeSpec(content: string, options: WriteSpecOptions): Promise<string[]>`** — writes markdown content to disk; single-file mode writes directly to `outputPath`; multi-file mode splits sections and writes to `outputPath` directory; throws `SpecExistsError` on conflicts unless `force=true`; returns array of written absolute paths.\n\n**`WriteSpecOptions`** — `{ outputPath: string, force: boolean, multiFile: boolean }`.\n\n**`SpecExistsError`** — extends Error; `paths: string[]` property lists conflicting file paths; message includes `--force` usage instruction.",
      "model": "claude-haiku-4-5-20251001",
      "inputTokens": 3,
      "outputTokens": 1151,
      "cacheReadTokens": 0,
      "cacheCreationTokens": 20886,
      "latencyMs": 29534,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": []
    },
    {
      "timestamp": "2026-02-14T13:49:24.409Z",
      "prompt": "Generate AGENTS.md for directory: \"src/cli\" (cli)\n\n## File Summaries (8 files)\n\n### clean.ts\n**Purpose:** `clean.ts` implements the `are clean` command to delete generated documentation artifacts and restore user-authored f...\n\n**`clean.ts` implements the `are clean` command to delete generated documentation artifacts and restore user-authored files.**\n\n## Exported Symbols\n\n`cleanCommand(targetPath: string, options: CleanOptions): Promise<void>` — executes artifact cleanup, finding and deleting `.sum`, generated `AGENTS.md`, generated `CLAUDE.md`, and `.agents-reverse-engineer/GENERATION-PLAN.md` files; restores `AGENTS.local.md` → `AGENTS.md` and `CLAUDE.local.md` → `CLAUDE.md`.\n\n`CleanOptions` interface with `dryRun: boolean` (default `false`) — controls preview-only mode.\n\n## Deletion & Preservation Logic\n\nDeletes all `*.sum` files (includes `.annex.sum`) via glob `**/*.sum`. Filters `AGENTS.md`, `AGENTS.*.md`, and `CLAUDE.md` by checking `content.includes(GENERATED_MARKER_PREFIX)` — only removes ARE-generated files; preserves user-authored files without the marker. Always deletes `.agents-reverse-engineer/GENERATION-PLAN.md` if present. Silently skips unreadable files.\n\n## File Restoration\n\nRenames `.AGENTS.local.md` → `AGENTS.md` and `.CLAUDE.local.md` → `CLAUDE.md` in their original directories, restoring user-defined versions before generation overwrites them.\n\n## Glob Patterns & Ignores\n\nAll globs use `{ cwd: resolvedPath, absolute: true, onlyFiles: true, dot: true, ignore: ['**/node_modules/**', '**/.git/**'] }` to exclude dependencies and VCS.\n\n## Error Handling\n\nVerifies directory accessibility with error codes: `ENOENT` (not found), `EACCES`/`EPERM` (permission denied) exit with code 1. Suppresses individual file deletion/restoration errors via try-catch logging without halting. Read failures during marker filtering skip silently.\n\n## Output & Dry Run\n\nLogs relative paths for all operations. `dryRun: true` prints files without deletion/restoration, showing preview with `pc.yellow('Dry run — no files were changed.')`. Final summary: `Deleted X file(s), restored Y local file(s).`\n\n## Dependencies\n\n`findProjectRoot()` resolves target path; `createLogger({ colors: true })` produces colored output via `picocolors` (`pc`); `fg.glob()` from `fast-glob` finds artifacts; `GENERATED_MARKER_PREFIX` from `agents-md.js` identifies ARE-generated files.\n### discover.ts\n**Purpose:** discover.ts implements the `are discover` command for walking directory trees with gitignore/vendor/binary/custom fil...\n\n**discover.ts implements the `are discover` command for walking directory trees with gitignore/vendor/binary/custom filters to identify files suitable for analysis.**\n\n## Exports\n\n**discoverCommand**(targetPath: string, options: DiscoverOptions): Promise<void> — Entry point; resolves targetPath to project root via findProjectRoot, loads config, calls discoverFiles to filter candidates, emits discovery:start and discovery:end trace events, logs results (included/excluded files), and generates GENERATION-PLAN.md via orchestrator.createPlan and buildExecutionPlan.\n\n**DiscoverOptions** interface — tracer?: ITraceWriter; debug?: boolean; showExcluded?: boolean — controls trace emission, debug console output, and exclusion reporting.\n\n## Integration & Dependencies\n\ndiscoverCommand orchestrates **discoverFiles** (discovery/run.js) for the walk+filter pipeline, **loadConfig** (config/loader.js) for filter rules, **createLogger** (output/logger.js) for formatted output, **createOrchestrator** (orchestration/orchestrator.js) for plan generation, **buildExecutionPlan** and **formatExecutionPlanAsMarkdown** (generation/executor.js) for post-order execution plan markdown, and **ProgressLog** (orchestration/index.js) for tail -f monitoring.\n\n## Behavioral Contracts\n\n**Trace events**: discovery:start (targetPath), discovery:end (filesIncluded, filesExcluded, durationMs) — emitted via options.tracer?.emit().\n\n**Error handling**: ENOENT → \"Directory not found\"; EACCES/EPERM → \"Permission denied\" — both exit(1).\n\n**Output paths**: Relative paths computed via `path.relative(resolvedPath, absPath)`. Excluded files formatted as `- ${rel} (${reason}: ${filter})`. Results written to `.agents-reverse-engineer/GENERATION-PLAN.md`.\n\n**Timing**: Discovery duration measured with `process.hrtime.bigint()` and converted to milliseconds via division by 1_000_000.\n\n**ProgressLog format**: Timestamped header `=== ARE Discover (${ISO})`, status lines, file listings with + (included) and - (excluded) prefixes.\n### generate.ts\n**Purpose:** generate.ts executes the documentation generation pipeline: discovers files, creates an analysis plan, resolves an AI...\n\n**generate.ts executes the documentation generation pipeline: discovers files, creates an analysis plan, resolves an AI backend, and runs concurrent AI-driven .sum file and AGENTS.md generation.**\n\n## Exports\n\n`generateCommand(targetPath: string, options: GenerateOptions): Promise<void>` — Main CLI handler; discovers files in targetPath, builds execution plan via createOrchestrator and buildExecutionPlan, resolves AI backend via resolveBackend, provisions backend config with ensureProjectConfig, creates AIService with configurable timeouts/retries, executes two-phase pipeline via CommandRunner.executeGenerate, writes telemetry/traces, exits with code 0 (success), 1 (partial failure), or 2 (total failure).\n\n`interface GenerateOptions` — Command configuration: force (skip nothing), dryRun (plan without AI calls), concurrency (default from config.ai.concurrency), failFast, debug, trace (writes to .agents-reverse-engineer/traces/), model (override config.ai.model), backend (override config.ai.backend), eval (namespace output by backend.model for comparison).\n\n`formatPlan(plan: GenerationPlan): string` — Formats plan summary showing file counts, skipped files/dirs (existing .sum, AGENTS.md), task counts, and complexity metrics (fileCount, directoryDepth).\n\n## Integration Points\n\n- **Discovery**: discoverFiles(absolutePath, config, {tracer, debug}) filters source files, returns included/excluded lists mapped to discoveryResult.\n- **Planning**: createOrchestrator creates GenerationPlan via orchestrator.createPlan(discoveryResult, {force}); plan.files/tasks/skipped{Files,Dirs} drive execution.\n- **Execution**: buildExecutionPlan(plan, absolutePath, variant) creates executionPlan with fileTasks, directoryTasks, directoryFileMap; CommandRunner.executeGenerate processes concurrent tasks and returns RunSummary (filesProcessed, filesFailed).\n- **Backend**: createBackendRegistry() → resolveBackend(registry, backendName) resolves AIServiceError if CLI_NOT_FOUND (code='CLI_NOT_FOUND'); backend.ensureProjectConfig?(path) provisions config; getInstallInstructions(registry) shows setup help.\n- **AI Service**: AIService(backend, {timeoutMs, maxRetries, model, command: 'generate', telemetry: {keepRuns}}, consoleLogger); setDebug(true), setSubprocessLogDir(logDir) enable logging.\n- **Tracing**: createTraceWriter(absolutePath, enabled) → tracer.filePath, used in loadConfig/discoverFiles/createOrchestrator; cleanupOldTraces(absolutePath) removes old .agents-reverse-engineer/traces/; ProgressLog.create(absolutePath) logs progress to .agents-reverse-engineer/progress.log.\n- **Config**: loadConfig(absolutePath, {tracer, debug}) resolves config.ai.{backend, model, concurrency, timeoutMs, maxRetries, telemetry.keepRuns}.\n\n## Behavioral Contracts\n\n- **Dry-run**: When options.dryRun=true, computes dryRunVariant = `${backend}.${model}` (if eval), prints execution plan summary (file counts, directories, estimated AI calls), then returns without AI calls.\n- **Eval mode**: When options.eval=true, variant = `${backend.name}.${effectiveModel}`; outputs prefixed with variant (e.g., *.{variant}.sum, AGENTS.{variant}.md); logs \"[eval] Variant: {variant}\" and \"[eval] Output files: *.{variant}.sum, AGENTS.{variant}.md\".\n- **Subprocess logs**: If options.trace=true, setSubprocessLogDir(logDir) where logDir = `.agents-reverse-engineer/subprocess-logs/{ISO-timestamp}` (colons/periods replaced with hyphens).\n- **Progress log**: ProgressLog.write entries: \"=== ARE Generate ({timestamp}) ===\", \"Project: {absolutePath}\", \"Files: {count} | Directories: {count} | Skipped: {count}\".\n- **Exit codes**: 0 (all succeeded), 1 (summary.filesFailed > 0 && filesProcessed > 0), 2 (filesProcessed === 0 && filesFailed > 0).\n- **Error handling**: AIServiceError with code='CLI_NOT_FOUND' → exit(2) after printing error and install instructions.\n- **Skip conditions**: plan.skippedFiles (existing .sum files), plan.skippedDirs (existing AGENTS.md); early return if plan.files.length === 0 && plan.tasks.length === 0.\n### index.ts\n**Purpose:** CLI entry point orchestrating agents-reverse-engineer commands (install, discover, generate, update, specify, rebuild...\n\n**CLI entry point orchestrating agents-reverse-engineer commands (install, discover, generate, update, specify, rebuild, clean) with argument parsing and command routing.**\n\n## Exported Symbols\n\n`main()` — async entry point dispatching to command handlers based CLI args.\n`parseArgs(args: string[])` — returns `{command, positional, flags, values}` parsing POSIX-style flags (--long, -h/-g/-l/-V) and positional args; stops at first non-flag for command name.\n`showVersion()`, `showVersionBanner()`, `showHelp()`, `showUnknownCommand(command)` — output routines; first two exit(0), last two exit(1).\n`hasInstallerFlags(flags, values)` — detects installer-mode activation (global/local/force/runtime presence).\n\n## Key Dependencies & Integration\n\nImports command handlers: `initCommand`, `discoverCommand`, `generateCommand`, `updateCommand`, `cleanCommand`, `specifyCommand`, `rebuildCommand` from sibling modules; `runInstaller`, `parseInstallerArgs` from `../installer/`; `getVersion` from `../version.js`.\n\n## Command Routing Logic\n\nHandles 8 explicit commands (install, uninstall, init, clean, discover, generate, update, specify, rebuild); auto-detects installer mode via `hasInstallerFlags()` for direct flag invocation (e.g., `npx agents-reverse-engineer --runtime claude -g`); empty args launches interactive installer; unknown commands call `showUnknownCommand()`.\n\n## Options Structures\n\n`GenerateOptions` — force, dryRun, concurrency, failFast, debug, trace, model, backend, eval.\n`UpdateOptions` — uncommitted, dryRun, concurrency, failFast, debug, trace, model, backend, eval.\n`CleanOptions` — dryRun.\n`SpecifyOptions` — output, force, dryRun, multiFile, debug, trace, model, backend.\n`RebuildOptions` — output, force, dryRun, concurrency, failFast, debug, trace, model, backend.\n\n## Behavioral Contracts\n\nShort flag mapping (parseArgs): `-h` → help, `-g` → global, `-l` → local, `-V` → version.\nLong flags with values parsed via map; next arg taken as value if present and non-flag.\nVersion string sourced from `getVersion()` output as `agents-reverse-engineer v${VERSION}`.\nExit code 0 on help/version, 1 on error or unknown command.\nDefault path: `.` (current directory) if positional arg omitted.\n\n## Workflow & Convention Rules\n\nInstaller re-parses args via `parseInstallerArgs()` for install/uninstall commands; all other commands use extracted flags/values directly. `--dry-run` supported on generate, update, specify, rebuild. `--force` overrides existing on init, generate, specify, rebuild. `--concurrency` parsed as integer for generate, update, rebuild. Model/backend optional for analysis commands. Help flag anywhere exits before command dispatch unless installer-mode active.\n### init.ts\n**Purpose:** init.ts initializes ARE project configuration by creating `.agents-reverse/config.yaml`, updating `.gitignore` and VS...\n\n**init.ts initializes ARE project configuration by creating `.agents-reverse/config.yaml`, updating `.gitignore` and VS Code settings to exclude `*.sum` artifacts.**\n\n## Exports\n\n`initCommand(root: string, options?: { force?: boolean }): Promise<void>` — main CLI entry point; creates default config at `[root]/.agents-reverse/config.yaml`, optionally updating `.gitignore` and `.vscode/settings.json` to hide generated files; warns if config exists unless `force: true`.\n\n## Key Functions\n\n`ensureGitignoreEntry(root: string): Promise<boolean>` — appends `*.sum` to `.gitignore` under `# agents-reverse-engineer` section (creates section if missing); idempotent, returns true if modified.\n\n`ensureVscodeExclude(root: string): Promise<boolean>` — sets `files.exclude[\"**/*.sum\"] = true` in `.vscode/settings.json` using `jsonc-parser` to preserve comments/formatting; creates `.vscode` dir if needed; returns false on parse errors (data-loss safety).\n\n## Constants & Patterns\n\n`GITIGNORE_SECTION = '# agents-reverse-engineer'` — marker for ARE-managed gitignore block.\n\n`SUM_PATTERN = '*.sum'` — glob pattern excluded from version control and IDE file tree.\n\n## Dependencies\n\n`jsonc-parser` (`parse`, `modify`, `applyEdits`) — JSONC parsing/editing for `.vscode/settings.json`; preserves formatting and comments via surgical edits.\n\n`config/loader.js` (`configExists`, `writeDefaultConfig`, `CONFIG_DIR`, `CONFIG_FILE`) — config file I/O and paths.\n\n`output/logger.js` (`createLogger`) — status/warning/error logging to console.\n\n## Configuration Options\n\nConfig file edits documented in info logs: `exclude.patterns` (glob exclusions), `ai.concurrency` (1–20, default auto), `ai.timeoutMs` (subprocess timeout, default 300000ms), `ai.backend` (claude/codex/gemini/opencode/auto).\n\n## Error Handling\n\nEACCES/EPERM — permission denied, exits code 1 with helpful message. Parse errors in `.vscode/settings.json` — skips modification to prevent data loss. `.gitignore` missing or unreadable — creates new file.\n### rebuild.ts\n**Purpose:** rebuild.ts exports the rebuildCommand CLI handler that reconstructs projects from specification files via AI-driven c...\n\n**rebuild.ts exports the rebuildCommand CLI handler that reconstructs projects from specification files via AI-driven code generation with checkpoint-based session continuity.**\n\n## Exported Interface & Function\n\n`RebuildOptions` — configuration object with: `output` (string, custom output dir), `force` (boolean, wipe output), `dryRun` (boolean, show plan only), `concurrency` (number, worker pool override), `failFast` (boolean, stop on first failure), `debug` (boolean, verbose logging), `trace` (boolean, NDJSON tracing), `model` (string, override AI model), `backend` (string, override backend: \"claude\", \"codex\", \"opencode\", \"gemini\").\n\n`rebuildCommand(targetPath: string, options: RebuildOptions): Promise<void>` — main CLI entry point; reads spec files from `specs/` directory, partitions into rebuild units, resolves AI backend, executes checkpoint-resumable rebuild via `executeRebuild`, writes output to `rebuild/` (or `options.output`).\n\n## Workflow & Execution Model\n\n1. **Spec partition**: `readSpecFiles(absolutePath)` → `partitionSpec(specFiles)` creates ordered rebuild units.\n2. **Checkpoint resumption**: `CheckpointManager.load(outputDir, specFiles, unitNames)` enables resuming incomplete rebuilds; `--dry-run` displays checkpoint status without AI calls.\n3. **Backend resolution**: `resolveBackend(registry, backend)` loads CLI backend (throws `AIServiceError` with code `'CLI_NOT_FOUND'` if missing); `backend.ensureProjectConfig(absolutePath)` provisions backend resources.\n4. **Model selection**: CLI flag > config override > \"opus\" default (upgrades \"sonnet\" to \"opus\" for rebuild quality).\n5. **AI service**: `AIService` with `timeoutMs: Math.max(config.ai.timeoutMs, 900_000)` (15min minimum), `maxRetries`, `command: 'rebuild'`; subprocess logs written to `.agents-reverse-engineer/subprocess-logs/[ISO-timestamp]/` if `--trace` enabled.\n6. **Progress tracking**: `ProgressLog.create(absolutePath)` writes human-readable progress; `--trace` also invokes `createTraceWriter` for NDJSON output.\n7. **Execution**: `executeRebuild(aiService, absolutePath, {...})` runs rebuild; result contains `modulesProcessed`, `modulesSkipped`, `modulesFailed`.\n8. **Exit codes**: 0 (success), 1 (partial failure), 2 (all failed or no AI CLI).\n\n## Key Dependencies\n\n`loadConfig` — reads config with tracer/debug opts; `findProjectRoot` — resolves absolute path; `createBackendRegistry`, `resolveBackend`, `getInstallInstructions` — AI backend provisioning; `ProgressLog`, `createTraceWriter`, `cleanupOldTraces` — monitoring & tracing; `readSpecFiles`, `partitionSpec`, `CheckpointManager`, `executeRebuild` — rebuild orchestration.\n### specify.ts\n**Purpose:** specify.ts implements the CLI `specify` command, which synthesizes AGENTS.md documentation into a comprehensive proje...\n\n**specify.ts implements the CLI `specify` command, which synthesizes AGENTS.md documentation into a comprehensive project specification via AI.**\n\n## Exported Interface & Command\n\n`SpecifyOptions` interface: properties `output` (string), `force` (boolean), `dryRun` (boolean), `multiFile` (boolean), `debug` (boolean), `trace` (boolean), `model` (string), `backend` (string) — all optional.\n\n`specifyCommand(targetPath: string, options: SpecifyOptions): Promise<void>` — main entry point; resolves project root, checks for conflicts, loads config, collects AGENTS.md + annex files, optionally dry-runs or auto-generates docs, resolves AI backend, builds synthesis prompt, calls AIService, writes output files (single or multi-file), finalizes telemetry & tracing.\n\n## Workflow & Integration Points\n\n**Early conflict detection**: checks `specs/SPEC.md` (or `options.output`) for existence unless `--force` or `--dry-run` is set; scans output directory for `.md` files in multi-file mode; exits with code 1 if conflicts found.\n\n**Auto-generation trigger**: if `collectAgentsDocs()` returns empty array, automatically runs `generateCommand()` before proceeding; re-collects after generation; exits with code 1 if still empty.\n\n**Model resolution cascade**: `options.model` > `config.ai.model` (upgraded from 'sonnet' to 'opus') > default 'opus'. Specify command defaults to opus model for quality.\n\n**Backend resolution**: calls `resolveBackend(registry, options.backend ?? config.ai.backend)`; catches `AIServiceError` with code `'CLI_NOT_FOUND'`, logs install instructions via `getInstallInstructions()`, exits with code 2. Calls `backend.ensureProjectConfig?.(absolutePath)` if defined.\n\n**AIService configuration**: timeout fixed at `Math.max(config.ai.timeoutMs, 900_000)` ms (min 900s for spec generation); uses `config.ai.maxRetries`, config telemetry `keepRuns` setting.\n\n## Tracing, Debugging & Progress Monitoring\n\n**Tracer integration**: `createTraceWriter(absolutePath, options.trace)` writes to `.agents-reverse-engineer/subprocess-logs/{ISO-timestamp}/`; emits `phase:start` and `phase:end` events with `taskCount: 1, concurrency: 1`; `aiService.setTracer(tracer)` wires subprocess/retry events; `cleanupOldTraces()` called at finish.\n\n**Progress log**: `ProgressLog.create(absolutePath)` writes to monitoring-friendly log with startup metadata, AI request lifecycle events, output files written, token/duration summary.\n\n**Debug mode** (`--debug`): logs backend name, CLI command, effective model, prompt sizes (chars).\n\n## Input Collection & Prompt Building\n\n`collectAgentsDocs(absolutePath)` returns array of docs with `content` property; `collectAnnexFiles(absolutePath)` returns reproduction-critical content array. `buildSpecPrompt(docs, annexFiles?)` constructs `{ system: string, user: string }` prompt object.\n\n**Dry-run statistics**: calculates `totalChars / 4` as token estimate, displays file counts, token estimate (K), output path, mode (single/multi).\n\n## Output Writing\n\n`writeSpec(response.text, { outputPath, force, multiFile })` writes specification; returns array of written file paths. Throws `SpecExistsError` (caught, logged, exits code 1) if output exists and `force: false`. Multi-file mode splits into multiple `.md` files in output directory.\n\n## Dependencies & Logging\n\nUses `picocolors` (pc) for colored terminal output: `pc.red()`, `pc.green()`, `pc.bold()`, `pc.dim()`, `pc.cyan()`, `pc.yellow()`. Uses `consoleLogger` for AIService debug logging. Loads config via `loadConfig(absolutePath, { debug })`.\n### update.ts\n**Purpose:** update.ts implements the incremental documentation update command via git diff detection, concurrent AI analysis thro...\n\n**update.ts implements the incremental documentation update command via git diff detection, concurrent AI analysis through CommandRunner, and AGENTS.md regeneration for affected directories.**\n\n## Exports\n\n**updateCommand**(targetPath: string, options: UpdateOptions): Promise<void> — Main CLI entry point; detects changed files since last run via content-hash, resolves AI backend, analyzes files concurrently via CommandRunner orchestrator, regenerates AGENTS.md per affected directory, writes telemetry and progress logs, exits with 0 (success), 1 (partial failure), or 2 (total failure/no CLI).\n\n**UpdateOptions** interface — Configuration for update command:\n- `uncommitted?: boolean` — include staged + working directory changes\n- `dryRun?: boolean` — show plan without writing\n- `concurrency?: number` — concurrent AI calls\n- `failFast?: boolean` — stop on first file analysis failure\n- `debug?: boolean` — show AI prompts and backend details\n- `trace?: boolean` — enable concurrency tracing to .agents-reverse-engineer/traces/\n- `model?: string` — override AI model (e.g., \"sonnet\", \"opus\")\n- `backend?: string` — override AI backend (e.g., \"claude\", \"codex\", \"opencode\", \"gemini\")\n- `eval?: boolean` — namespace output by backend.model for A/B comparison\n\n## Workflow & Execution Flow\n\nupdateCommand executes seven ordered phases:\n\n1. **Git & plan preparation**: findProjectRoot resolves absolute path; createUpdateOrchestrator().preparePlan() detects changed files via content-hash, marks first-run/no-changes conditions, surfaces filesToAnalyze, filesToSkip, cleanup deletions (deletedSumFiles, deletedAgentsMd), affectedDirs.\n2. **First-run / no-changes bailout**: exits early if plan.isFirstRun or no file deltas.\n3. **Dry-run bailout**: returns without writing if dryRun=true.\n4. **Backend resolution**: createBackendRegistry() + resolveBackend() validates CLI availability (exits code 2 if CLI_NOT_FOUND); backend.ensureProjectConfig?() provisions backend-specific resources (e.g., OpenCode agent config).\n5. **AI service initialization**: new AIService(backend, {...}) with model from options.model or config; setSubprocessLogDir() if tracing enabled; setDebug() if debug=true.\n6. **Phase 1 – File analysis**: CommandRunner.executeUpdate(plan.fileTasks, absolutePath, config) runs file analysis concurrently (concurrency from options or config); emits tracer events (phase:start, task:start, task:done, phase:end).\n7. **Phase 2 – Directory AGENTS.md regeneration**: iterates plan.affectedDirs sequentially (concurrency=1); per directory: readFile agentsFilename (variant-aware: AGENTS.${variant}.md or AGENTS.md), check for GENERATED_MARKER_PREFIX to detect handwritten vs. generated, buildDirectoryPrompt(dirPath, absolutePath, debug, knownDirs, undefined, existingAgentsMd, undefined, variant), aiService.call(), writeAgentsMd(dirPath, absolutePath, response.text, variant), writeAgentsMdHub() if variant, writeClaudeMdPointer().\n8. **Telemetry & finalization**: aggregates summary (filesProcessed, filesFailed, dirsProcessed, dirsFailed, totalCalls, token counts, totalDurationMs); ProgressReporter.printSummary(summary); aiService.finalize(absolutePath); progressLog.finalize(); tracer.finalize(); cleanupOldTraces() if trace=true; orchestrator.recordRun(currentCommit, filesProcessed, filesSkipped); exit(0/1/2).\n\n## Variant (Eval) Mode\n\nWhen eval=true, outputs are namespace-prefixed: `*.${backend.name}.${model}.sum` files and `AGENTS.${variant}.md` for directory regeneration. Variant computed from options.backend/options.model (preliminary) or backend.name/effectiveModel (final after CLI resolution). Debug output prints `[eval] Variant: ${variant}` and `[eval] Output files: *.${variant}.sum, AGENTS.${variant}.md`.\n\n## Error Handling & Exit Codes\n\n- Exit 2: total failure (filesProcessed=0 && filesFailed>0 OR CLI not found via AIServiceError.code='CLI_NOT_FOUND')\n- Exit 1: partial failure (filesFailed>0)\n- Exit 0: all success or no files to process\n- Directory regen failures (phase 2) emit task:done with success=false and log as WARN; do not halt execution.\n\n## Key Dependencies\n\n- **createUpdateOrchestrator**(config, absolutePath, {tracer, debug}) → UpdatePlan with filesToAnalyze, filesToSkip, cleanup, affectedDirs, isFirstRun, currentCommit\n- **CommandRunner**(aiService, {concurrency, failFast, debug, tracer, progressLog, variant}) → runner.executeUpdate(fileTasks, absolutePath, config)\n- **AIService**(backend, {timeoutMs, maxRetries, model, command, telemetry}, consoleLogger) → aiService.call({prompt, systemPrompt}) → {text, inputTokens, outputTokens, model, cacheReadTokens, cacheCreationTokens}\n- **createBackendRegistry**() + **resolveBackend**(registry, backendName) → backend with name, cliCommand, ensureProjectConfig?()\n- **buildDirectoryPrompt**(dirPath, absolutePath, debug, knownDirs, undefined, existingAgentsMd, undefined, variant) → {user, system}\n- **writeAgentsMd, writeAgentsMdHub, writeClaudeMdPointer** — output writers for AGENTS.md, hub index, claude.md pointer\n- **ProgressLog.create**(absolutePath) → progressLog.write(), finalize()\n- **createTraceWriter**(absolutePath, enabled) → tracer.emit({type, ...}), finalize()\n- **cleanupOldTraces**(absolutePath)\n\n## Formatting & Display\n\n- **formatPlan**(plan): renders plan summary with status markers: `+` (added, green), `R` (renamed, blue), `M` (modified, yellow); displays file counts, affected directories, cleanup actions; hints \"are generate\" on first run.\n- **formatCleanup**(plan): formats deletedSumFiles and deletedAgentsMd arrays as red `-` prefixed lists.\n- Color codes via picocolors: yellow (warnings, hints), cyan (sections, eval output), red (errors, cleanup), green (success, added files), blue (renamed), dim (secondary info, debug).\n\n## Tracing & Debugging\n\nWhen trace=true: creates .agents-reverse-engineer/traces/${ISO}.json; emits phase/task lifecycle events with phase, taskLabel, durationMs, success, error, activeTasks; subprocess logs written to .agents-reverse-engineer/subprocess-logs/${ISO timestamp}/ via aiService.setSubprocessLogDir(). When debug=true: logs backend name, cliCommand, effectiveModel, variant; aiService.setDebug(true) enables verbose output.\n\n## Import Map (verified — use these exact paths)\n\nclean.ts:\n  ../config/loader.js → findProjectRoot\n  ../output/logger.js → createLogger\n  ../generation/writers/agents-md.js → GENERATED_MARKER_PREFIX\n\ndiscover.ts:\n  ../config/loader.js → loadConfig, findProjectRoot\n  ../discovery/run.js → discoverFiles\n  ../output/logger.js → createLogger\n  ../orchestration/orchestrator.js → createOrchestrator\n  ../generation/executor.js → buildExecutionPlan, formatExecutionPlanAsMarkdown\n  ../orchestration/index.js → ProgressLog\n  ../types/index.js → DiscoveryResult (type)\n  ../orchestration/trace.js → ITraceWriter (type)\n\ngenerate.ts:\n  ../config/loader.js → loadConfig, findProjectRoot\n  ../output/logger.js → createLogger\n  ../core/logger.js → consoleLogger\n  ../discovery/run.js → discoverFiles\n  ../orchestration/orchestrator.js → createOrchestrator, type GenerationPlan\n  ../generation/executor.js → buildExecutionPlan\n  ../ai/index.js → AIService, AIServiceError, createBackendRegistry, resolveBackend, getInstallInstructions\n  ../orchestration/index.js → CommandRunner, ProgressLog, createTraceWriter, cleanupOldTraces\n\nindex.ts:\n  ../installer/index.js → runInstaller, parseInstallerArgs\n  ../version.js → getVersion\n\ninit.ts:\n  ../config/loader.js → configExists, writeDefaultConfig, CONFIG_DIR, CONFIG_FILE\n  ../output/logger.js → createLogger\n\nrebuild.ts:\n  ../config/loader.js → loadConfig, findProjectRoot\n  ../core/logger.js → consoleLogger\n  ../ai/index.js → AIService, AIServiceError, createBackendRegistry, resolveBackend, getInstallInstructions\n  ../orchestration/index.js → ProgressLog, createTraceWriter, cleanupOldTraces\n  ../rebuild/index.js → readSpecFiles, partitionSpec, CheckpointManager, executeRebuild\n\nspecify.ts:\n  ../config/loader.js → loadConfig, findProjectRoot\n  ../core/logger.js → consoleLogger\n  ../generation/collector.js → collectAgentsDocs, collectAnnexFiles\n  ../specify/index.js → buildSpecPrompt, writeSpec, SpecExistsError\n  ../ai/index.js → AIService, AIServiceError, createBackendRegistry, resolveBackend, getInstallInstructions\n  ../orchestration/index.js → ProgressLog, createTraceWriter, cleanupOldTraces\n\nupdate.ts:\n  ../config/loader.js → loadConfig, findProjectRoot\n  ../output/logger.js → createLogger\n  ../core/logger.js → consoleLogger\n  ../orchestration/orchestrator.js → createUpdateOrchestrator, type UpdatePlan\n  ../generation/writers/agents-md.js → writeAgentsMd, writeAgentsMdHub, GENERATED_MARKER_PREFIX\n  ../generation/writers/claude-md.js → writeClaudeMdPointer\n  ../generation/prompts/index.js → buildDirectoryPrompt\n  ../ai/index.js → AIService, AIServiceError, createBackendRegistry, resolveBackend, getInstallInstructions\n  ../orchestration/index.js → CommandRunner, ProgressReporter, ProgressLog, createTraceWriter, cleanupOldTraces\n\n## Project Directory Structure\n\n<project-structure>\n./\n  .npmrc\n  CONTRIBUTING.md\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  pages.yml\n  publish.yml\nhooks/\n  are-check-update.js\n  are-context-loader.js\n  opencode-are-check-update.js\nscripts/\n  analyze-agents-compression.sh\n  analyze-sum-ratios.sh\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  codex.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/providers/\n  subprocess.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  rebuild.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/core/\n  index.ts\n  logger.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  claude-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  orchestrator.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/rebuild/\n  checkpoint.ts\n  index.ts\n  orchestrator.ts\n  output-parser.ts\n  prompts.ts\n  spec-reader.ts\n  types.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>",
      "systemPrompt": "You are generating an AGENTS.md file — a directory-level overview for AI coding assistants.\n\nCRITICAL: Output ONLY the raw markdown content. No code fences, no preamble, no explanations, no conversational text. Your entire response IS the AGENTS.md file content.\n\nOUTPUT FORMAT:\n- First line MUST be exactly: <!-- Generated by agents-reverse-engineer -->\n- Use a # heading with the directory name\n- Write a one-paragraph purpose statement for the directory\nusage\nADAPTIVE SECTIONS:\nAnalyze the directory contents and choose the most relevant sections. Do NOT use a fixed template. Instead, select sections that best document this specific directory for an AI that needs to reconstruct or extend the project.\n\nConsider these section types (choose what applies):\n- **Contents**: Group files by purpose/category under ## headings. For each file: markdown link [filename](./filename) and a one-line description.\n- **Subdirectories**: If subdirectories exist, list them with links [dirname/](./dirname/) and brief summaries.\n- **Architecture / Data Flow**: If files form a pipeline, request/response chain, or layered architecture, document it.\n- **Stack**: If this is a package root (has package.json, Cargo.toml, go.mod, etc.), document the technology stack, key scripts, and entry points.\n- **Structure**: If the directory layout follows a convention (feature-sliced, domain-driven, MVC, etc.), document it.\n- **Patterns**: If files share recurring design patterns (factory, strategy, middleware, barrel re-export), name and document them.\n- **Configuration**: If the directory contains config files, schemas, or environment definitions, document the config surface area.\n- **API Surface**: If the directory exports a public API (barrel index, route definitions, SDK), document the interface contract.\n- **File Relationships**: How files collaborate, depend on each other, or share state.\n- **Behavioral Contracts**: If files contain regex patterns, format specifications, magic constants, or template strings that define observable behavior, collect them in a dedicated section. Preserve verbatim patterns from file summaries — do NOT paraphrase regex into prose. This section is MANDATORY when file summaries contain behavioral artifacts.\n- **Reproduction-Critical Constants**: If file summaries reference annex files (via ## Annex References sections), list them with links. Example: \"Full prompt template text: [templates.annex.sum](./prompts/templates.annex.sum)\". Do NOT reproduce annex content in AGENTS.md — just link to it.\n- **Workflow & Conventions**: If file summaries contain contribution guidelines, PR conventions, commit standards, testing mandates, tool usage rules, AI agent instructions, or code conventions (naming, formatting, linting), collect them into actionable directives. Source from CONTRIBUTING.md, CI configs, PR templates, linter/formatter configs, README dev sections. Present as concrete rules (\"commits must use conventionalcommits format\", \"use camelCase for functions, PascalCase for types\") not vague descriptions (\"the project has guidelines\").\n\nChoose any relevant sections or create your own based on the directory contents. The goal is to provide a comprehensive overview that captures the essence of the directory's role in the project and how its files work together, with a focus on what an AI coding assistant would need to know to effectively interact with this code.\n\nSCOPE:\n- AGENTS.md is a NAVIGATIONAL INDEX — help an AI find the right file quickly\n- Focus on: what each file does, how files relate, directory-level patterns\n- Do NOT reproduce full architecture sections — those belong in the root CLAUDE.md\n\nPATH ACCURACY (MANDATORY):\n- When referencing files or modules outside this directory, use ONLY paths from the \"Import Map\" section\n- Do NOT invent, rename, or guess module paths — if a path isn't in the Import Map, don't reference it\n- Use the exact directory names from \"Project Directory Structure\" — do NOT rename directories\n  (e.g., if the directory is called \"cli\", write \"src/cli/\", NOT \"src/commands/\")\n- Cross-module references must use the specifier format from actual import statements\n  (e.g., \"../generation/writers/sum.js\", NOT \"../fs/sum-file.js\")\n- If you are unsure about a path, omit the cross-reference rather than guessing\n\nCONSISTENCY (MANDATORY):\n- Do not contradict yourself within the same document\n- If you describe a technique (e.g., \"regex-based\"), do not call it something else later (e.g., \"AST-based\")\n- When stating version numbers, engines, or config fields, use ONLY values present in the file summaries\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this directory\", \"this module\", \"provides\", \"responsible for\", \"is used to\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Per-file descriptions in Contents sections: 1-2 sentences maximum. Reference key symbols but do not reproduce full summaries.\n- Behavioral contracts (regex patterns, format specs, constants) belong in a separate Behavioral Contracts section, not in per-file descriptions.\n- Annex file references: link to .annex.sum files, do not inline their content. One line per annex reference.\n- Subdirectory descriptions: 1-2 sentences maximum. Capture the directory's role, not its full contents.\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- Key exported symbols from file summaries MUST appear in the directory overview\n- Preserve exact casing of identifiers\n\nUSER NOTES:\n- If \"User Notes\" are provided in the prompt, they contain user-defined instructions that will be automatically prepended to your output\n- Do NOT repeat or paraphrase user notes in your generated content — they are included separately\n- You may reference information from user notes for context",
      "response": "<!-- Generated by agents-reverse-engineer -->\n\n# src/cli\n\nThis directory exports the command-line interface for agents-reverse-engineer, providing eight CLI commands (install, init, discover, generate, update, specify, rebuild, clean) that coordinate with AI backends, file discovery, config management, and documentation generation pipelines.\n\n## Contents\n\n### Entry Point & Routing\n\n- [**index.ts**](./index.ts) — `main()` entry point parsing POSIX arguments via `parseArgs()`, routing to command handlers (`initCommand`, `discoverCommand`, `generateCommand`, `updateCommand`, `cleanCommand`, `specifyCommand`, `rebuildCommand`), detecting installer mode via `hasInstallerFlags()`, and outputting help/version. Handles 8 explicit commands plus auto-detected installer via flag combination (`--global`/`--local`/`--force`/`--runtime`).\n\n### Command Implementations\n\n- [**init.ts**](./init.ts) — `initCommand()` creates `.agents-reverse/config.yaml`, updates `.gitignore` to append `*.sum` under `# agents-reverse-engineer` section via `ensureGitignoreEntry()`, configures VS Code via `ensureVscodeExclude()` (using `jsonc-parser` for surgical edits to preserve comments/formatting).\n\n- [**discover.ts**](./discover.ts) — `discoverCommand()` orchestrates file discovery via `discoverFiles()`, loads filter rules via `loadConfig()`, emits trace events (`discovery:start`, `discovery:end`), calls `createOrchestrator().createPlan()` and `buildExecutionPlan()` to partition files, logs results (included/excluded), and writes `.agents-reverse-engineer/GENERATION-PLAN.md` via `formatExecutionPlanAsMarkdown()`.\n\n- [**generate.ts**](./generate.ts) — `generateCommand()` executes full documentation generation: discovers files, creates analysis plan, resolves AI backend via `resolveBackend()`, initializes `AIService` with configurable timeouts/retries/model, runs two-phase `CommandRunner.executeGenerate()` pipeline (file analysis + directory AGENTS.md generation), handles dry-run via `dryRunVariant`, supports eval mode (namespace output by `${backend}.${model}`), writes telemetry/traces, exits 0/1/2.\n\n- [**update.ts**](./update.ts) — `updateCommand()` detects changed files via content-hash (git diff + uncommitted flag), resolves backend, executes `CommandRunner.executeUpdate()` for concurrent file analysis, regenerates AGENTS.md per affected directory by `buildDirectoryPrompt()` + AI call + `writeAgentsMd()`, handles variant mode and cleanup of deleted `.sum`/`AGENTS.md` files, exits 0/1/2 based on success/failure counts.\n\n- [**specify.ts**](./specify.ts) — `specifyCommand()` synthesizes AGENTS.md into project specification: checks conflicts via spec file existence, auto-triggers `generateCommand()` if no docs exist, collects docs via `collectAgentsDocs()` + annex files via `collectAnnexFiles()`, builds prompt via `buildSpecPrompt()`, calls `AIService` (model upgraded to opus), writes output (single or multi-file) via `writeSpec()`, traces phase lifecycle with `createTraceWriter()`.\n\n- [**rebuild.ts**](./rebuild.ts) — `rebuildCommand()` reconstructs projects from specs: reads spec files via `readSpecFiles()`, partitions into units via `partitionSpec()`, loads checkpoint state via `CheckpointManager.load()` for resumable builds, resolves backend, executes `executeRebuild()` with 15min+ timeout (AI model upgraded to opus), writes progress via `ProgressLog`, exits 0/1/2.\n\n- [**clean.ts**](./clean.ts) — `cleanCommand()` deletes generated artifacts: removes all `*.sum` files via glob, filters `AGENTS.md`/`CLAUDE.md` by marker prefix (`GENERATED_MARKER_PREFIX`) to preserve user-authored files, deletes `.agents-reverse-engineer/GENERATION-PLAN.md`, restores `.AGENTS.local.md` → `AGENTS.md` and `.CLAUDE.local.md` → `CLAUDE.md` via rename, logs operations (dry-run mode available via `dryRun: true`).\n\n## Behavioral Contracts\n\n**CLI Argument Parsing** (`parseArgs`): POSIX-style flags (short `-h`, `-g`, `-l`, `-V`; long `--flag` or `--flag value`); stops at first non-flag positional arg (command name); returns `{command, positional, flags, values}` structure. Version string formatted as `agents-reverse-engineer v${VERSION}`.\n\n**Exit Codes**: 0 on help/version/success, 1 on partial failure (filesProcessed > 0 && filesFailed > 0) or conflict, 2 on total failure (filesProcessed === 0 && filesFailed > 0) or backend CLI not found (`AIServiceError` code `'CLI_NOT_FOUND'`).\n\n**Trace Events**: Commands emit `discovery:start(targetPath)`, `discovery:end(filesIncluded, filesExcluded, durationMs)`, `phase:start(phase, taskCount, concurrency)`, `phase:end(phase, durationMs, success)`, `task:start/done(taskLabel, durationMs, success)` via `ITraceWriter.emit()` interface; events written to `.agents-reverse-engineer/traces/${ISO}.json` when `--trace` enabled.\n\n**Model Cascade**: CLI flag (`--model`) > config file (`config.ai.model`) > command default (e.g., \"opus\" for specify/rebuild; varies for others). Sonnet auto-upgraded to opus for specify/rebuild.\n\n**Backend Provisioning**: `createBackendRegistry()` enumerates available backends; `resolveBackend(registry, backendName)` validates CLI availability (throws `AIServiceError` code `'CLI_NOT_FOUND'` if missing); `backend.ensureProjectConfig?.(path)` idempotent provisioning (e.g., OpenCode agent config).\n\n**Dry-Run & Eval Modes**: `--dry-run` prints execution plan (file counts, directories, estimated AI calls) without AI invocations or writes. `--eval` namespaces output (`*.${backend}.${model}.sum`, `AGENTS.${variant}.md`) for A/B comparison; logs `[eval] Variant: ${variant}`.\n\n**Subprocess Logging**: When `--trace` enabled, `setSubprocessLogDir(`.agents-reverse-engineer/subprocess-logs/${ISO-timestamp}/)` captures backend CLI stderr/stdout; ISO timestamp formatted with hyphens replacing colons/periods for filesystem safety.\n\n**Config Loading**: All commands call `loadConfig(absolutePath, {tracer, debug})` which reads `.agents-reverse/config.yaml` via `configExists()`, applies defaults, and surfaces `config.ai.{backend, model, concurrency, timeoutMs, maxRetries, telemetry.keepRuns}`.\n\n**Progress Logging**: `ProgressLog.create(absolutePath)` writes to `.agents-reverse-engineer/progress.log` with timestamped header, status lines, file counts; `finalize()` on command exit.\n\n**Error Messages**: Permission denied (EACCES/EPERM) exit 1 with helpful message; unknown commands trigger `showUnknownCommand(command)` then exit 1; install instructions printed on backend CLI not found via `getInstallInstructions(registry)`.\n\n## Workflow & Conventions\n\n**Installer Integration**: Commands `install` and `uninstall` delegate to `runInstaller(args)` which re-parses via `parseInstallerArgs()` for install-specific options (`--runtime`, `--global`, `--local`, `--force`). Empty args launches interactive installer. Other commands use extracted CLI flags/values directly.\n\n**File Path Resolution**: All commands resolve targetPath to project root via `findProjectRoot(targetPath)` before proceeding; default path is `.` (current directory) if positional arg omitted.\n\n**Conflict Detection**: `specify` checks `specs/SPEC.md` (or `options.output`) for existence unless `--force`; scans output directory in multi-file mode; exits code 1 on conflict. `init` warns if config exists unless `--force`.\n\n**Auto-Generation Trigger**: `specify` automatically runs `generateCommand()` if `collectAgentsDocs()` returns empty (no AGENTS.md found); re-collects after generation; exits code 1 if still empty.\n\n**Variant (Eval) Output**: When `--eval` flag set, variant computed as `${backend.name}.${effectiveModel}`; output files prefixed (e.g., `*.claude.opus.sum`); hub index written via `writeAgentsMdHub()` for variant aggregation.\n\n**Timeout Escalation**: Specify and rebuild commands enforce minimum timeouts (900_000ms = 15min) via `Math.max(config.ai.timeoutMs, 900_000)` to handle complex AI workloads.",
      "model": "claude-haiku-4-5-20251001",
      "inputTokens": 3,
      "outputTokens": 2182,
      "cacheReadTokens": 17512,
      "cacheCreationTokens": 10047,
      "latencyMs": 48183,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": []
    },
    {
      "timestamp": "2026-02-14T13:49:24.351Z",
      "prompt": "Generate AGENTS.md for directory: \"src/generation\" (generation)\n\n## File Summaries (4 files)\n\n### collector.ts\n**Purpose:** collector.ts recursively discovers and aggregates AGENTS.md and .annex.sum documentation files from a project tree.\n\n**collector.ts recursively discovers and aggregates AGENTS.md and .annex.sum documentation files from a project tree.**\n\n## Exports\n\n`AgentsDocs` type: array of `{ relativePath: string; content: string }` tuples representing collected documentation.\n\n`collectAgentsDocs(projectRoot: string): Promise<AgentsDocs>` — recursively walks `projectRoot`, locates all AGENTS.md files, reads content, and returns sorted by relative path; gracefully skips unreadable directories/files.\n\n`collectAnnexFiles(projectRoot: string): Promise<AgentsDocs>` — recursively walks `projectRoot`, locates all .annex.sum files, reads content, and returns sorted by relative path using identical skip logic.\n\n## Skip Directories\n\n`SKIP_DIRS` excludes: node_modules, .git, .agents-reverse-engineer, vendor, dist, build, __pycache__, .next, venv, .venv, target, .cargo, .gradle.\n\n## Integration\n\nBoth functions use shared recursion pattern with identical error handling: `readdir` failures trigger silent return; `readFile` failures skip individual files. Results always sorted alphabetically by `relativePath` using `localeCompare`.\n### complexity.ts\n**Purpose:** complexity.ts analyzes codebase structure to compute file counts, directory depths, and directory trees from file pat...\n\n**complexity.ts analyzes codebase structure to compute file counts, directory depths, and directory trees from file path lists.**\n\n## Exported Symbols\n\n- `ComplexityMetrics` interface: `{ fileCount: number; directoryDepth: number; files: string[]; directories: Set<string> }`\n- `analyzeComplexity(files: string[], projectRoot: string): ComplexityMetrics` — computes metrics from file list and root path\n\n## Internal Functions\n\n`calculateDirectoryDepth(files: string[], projectRoot: string): number` — measures maximum nesting depth by splitting relative paths on `path.sep`, subtracting 1 for the file itself.\n\n`extractDirectories(files: string[]): Set<string>` — traverses parent directories recursively via `path.dirname()` until reaching filesystem root (detects root when `path.dirname(dir) === dir`), accumulating all ancestors.\n\n## Dependencies\n\nUses `node:path` module for `relative()`, `dirname()`, and `sep` operations.\n### executor.ts\n**Purpose:** executor.ts builds execution plans from generation plans, transforming file/directory analysis tasks into ordered exe...\n\n**executor.ts builds execution plans from generation plans, transforming file/directory analysis tasks into ordered execution graphs with dependency tracking and post-order traversal for cascading AGENTS.md generation.**\n\n## Exported Types\n\n`ExecutionTask` — task object with id, type ('file'|'directory'), path, absolutePath, systemPrompt, userPrompt, dependencies (string[]), outputPath, metadata (directoryFiles?: string[], depth?: number, packageRoot?: string).\n\n`ExecutionPlan` — plan object with projectRoot, tasks, fileTasks, directoryTasks, directoryFileMap (Record<string, string[]>), projectStructure?, skippedFiles?, skippedDirs?.\n\n## Exported Functions\n\n`buildExecutionPlan(plan: GenerationPlan, projectRoot: string, variant?: string): ExecutionPlan` — transforms GenerationPlan into ExecutionPlan with post-order sorted directory tasks (deepest first); tracks all discovered files in directoryFileMap; creates file tasks with dependencies=[]; creates directory tasks with fileTaskIds as dependencies; skips directories not in filtered plan.\n\n`isDirectoryComplete(dirPath: string, expectedFiles: string[], projectRoot: string, variant?: string): Promise<{complete: boolean; missing: string[]}>` — checks if all files in expectedFiles have .sum artifacts via sumFileExists(); returns missing file list.\n\n`getReadyDirectories(executionPlan: ExecutionPlan, variant?: string): Promise<string[]>` — returns directories where all files have .sum files (isDirectoryComplete returns complete=true).\n\n`formatExecutionPlanAsMarkdown(plan: ExecutionPlan): string` — generates GENERATION-PLAN.md with header (date, projectRoot), Summary section (task counts, skipped counts, \"Post-order\" traversal note), Phase 1 (file tasks grouped by directory depth), Phase 2 (directory tasks grouped by depth), Skipped section.\n\n## Key Internals\n\n`getDirectoryDepth(dir: string): number` — calculates path segment count: '.' = 0, 'src' = 1, 'src/cli' = 2; determines post-order traversal order.\n\nPost-order traversal ensures child AGENTS.md generation completes before parents; fileTasks and directoryTasks sorted by depth descending; directoryFileMap includes all discovered files (plan.allDiscoveredFiles ?? plan.files) even if skipped, so directory prompts have complete context.\n\nDirectory task dependencies are fileTaskIds (`file:${f}` format) from directoryFileMap entries; only directories in plan.tasks (plannedDirs) become ExecutionTasks.\n\nVariant parameter (e.g., \"claude.haiku\") propagates through getSumPath() and sumFileExists() for conditional artifact naming (AGENTS.md vs AGENTS.{variant}.md).\n### types.ts\n**Purpose:** src/generation/types.ts defines type contracts for documentation metadata extraction during pipeline analysis.\n\n**src/generation/types.ts defines type contracts for documentation metadata extraction during pipeline analysis.**\n\n## Exports\n\n`SummaryMetadata` interface with three properties:\n- `purpose: string` — file's primary purpose\n- `criticalTodos?: string[]` — security/breaking issues only\n- `relatedFiles?: string[]` — tightly coupled sibling files\n\n## Import Map (verified — use these exact paths)\n\nexecutor.ts:\n  ../orchestration/orchestrator.js → GenerationPlan (type)\n\n## Project Directory Structure\n\n<project-structure>\n./\n  .npmrc\n  CONTRIBUTING.md\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  pages.yml\n  publish.yml\nhooks/\n  are-check-update.js\n  are-context-loader.js\n  opencode-are-check-update.js\nscripts/\n  analyze-agents-compression.sh\n  analyze-sum-ratios.sh\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  codex.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/providers/\n  subprocess.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  rebuild.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/core/\n  index.ts\n  logger.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  claude-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  orchestrator.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/rebuild/\n  checkpoint.ts\n  index.ts\n  orchestrator.ts\n  output-parser.ts\n  prompts.ts\n  spec-reader.ts\n  types.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\n\n## Subdirectories\n\n### prompts/\n<!-- Generated by agents-reverse-engineer v0.9.10 -->\n\n# src/generation/prompts\n\nThis directory provides prompt construction utilities for AI-driven documentation generation, handling both single-file summaries and directory-level AGENTS.md synthesis with language detection, compression rules, and incremental update workflows.\n\n## Contents\n\n**Prompt Construction**\n- [builder.ts](./builder.ts) — `detectLanguage()` maps file extensions to syntax highlighting codes; `buildFilePrompt()` assembles system/user prompts for single-file analysis with template substitution (`{{FILE_PATH}}`, `{{CONTENT}}`, `{{LANG}}`), optional context files, and aggressive compression injection when `compressionRatio < 0.5`; `buildDirectoryPrompt()` generates AGENTS.md prompts by reading `.sum` file summaries, extracting imports via `extractDirectoryImports()`, detecting `.annex.sum` files and manifest indicators (`package.json`, `Cargo.toml`, etc.), preserving user-authored content via `GENERATED_MARKER_PREFIX` detection, and returning update system prompts when `existingAgentsMd` is provided.\n\n**Type Definitions & Guidelines**\n- [types.ts](./types.ts) — `PromptContext` interface with `filePath`, `content`, `contextFiles`, `existingSum`, `sourceFileSize`, `compressionRatio`; `SUMMARY_GUIDELINES` constant specifying `targetLength` ({min: 300, max: 500}) and required documentation topics.\n\n**System Prompts**\n- [templates.ts](./templates.ts) — `FILE_SYSTEM_PROMPT` (19,318 chars) instructs AI to produce dense, identifier-rich summaries with density rules, anchor term preservation, behavioral contract extraction, and annex handling; `FILE_USER_PROMPT` template with substitution slots; `DIRECTORY_SYSTEM_PROMPT` orchestrates AGENTS.md generation with adaptive section selection and density enforcement; `DIRECTORY_UPDATE_SYSTEM_PROMPT` and `FILE_UPDATE_SYSTEM_PROMPT` preserve existing content while modifying only changed sections.\n\n**Public API**\n- [index.ts](./index.ts) — Barrel re-exporting `PromptContext`, `SUMMARY_GUIDELINES`, `buildFilePrompt()`, `buildDirectoryPrompt()`, and `detectLanguage()` for consumers in `src/generation/`.\n\n## Architecture\n\n`buildFilePrompt()` consumes `PromptContext` and optional debug/logger parameters, substituting templates and conditionally injecting compression rules. `buildDirectoryPrompt()` orchestrates a multi-step pipeline: reads parallel `.sum` files via `readSumFile(getSumPath())`, extracts directory imports from source files (`.ts|tsx|js|jsx|py|go|rs|java|kt`) via `extractDirectoryImports(dirPath, sourceFileNames)`, formats imports with `formatImportMap()`, detects user-authored `AGENTS.md` by checking absence of `GENERATED_MARKER_PREFIX`, scans for `.annex.sum` files and manifest files, collects child subdirectory `AGENTS.md`/`AGENTS.{variant}.md` via `readSumFile()`, and assembles user section with directory path, file summaries, import map, project structure, annex files, subdirectories, and manifest hints. Incremental updates (`FILE_UPDATE_SYSTEM_PROMPT`, `DIRECTORY_UPDATE_SYSTEM_PROMPT`) instruct the AI to preserve stable content and modify only affected entries.\n\n## Behavioral Contracts\n\n**Compression injection:** When `compressionRatio < 0.5` AND `sourceFileSize > 0`, calculate `targetSize = Math.round(sourceFileSize * ratio)` and `maxSize = Math.round(targetSize * 1.2)`, then append aggressive compression rules to user prompt.\n\n**Template substitution:** Patterns `{{FILE_PATH}}`, `{{CONTENT}}`, `{{LANG}}` replaced via `.replace(/\\{\\{...\\}\\}/g, ...)` in `FILE_USER_PROMPT`.\n\n**Generated marker detection:** `GENERATED_MARKER_PREFIX` substring presence distinguishes generated AGENTS.md from user-authored versions; absence signals user-maintained content requiring preservation.\n\n**Manifest file indicators:** Presence of `package.json`, `Cargo.toml`, `go.mod`, `pyproject.toml`, `pom.xml`, `build.gradle`, `Gemfile`, `composer.json`, `CMakeLists.txt`, or `Makefile` signals project root/manifest hints included in directory prompt.\n\n**Incremental update mode activation:** `FILE_UPDATE_SYSTEM_PROMPT` triggered when `context.existingSum` truthy; `DIRECTORY_UPDATE_SYSTEM_PROMPT` triggered when `existingAgentsMd` truthy; existing content appended under \"Existing Summary\"/\"Existing AGENTS.md\" headers with explicit instruction to preserve stable sections.\n\n## Reproduction-Critical Constants\n\nFull template text: [templates.annex.sum](./templates.annex.sum)\n### writers/\n<!-- Generated by agents-reverse-engineer v0.9.10 -->\n\n# src/generation/writers\n\nAggregates file writers for .sum documentation, AGENTS.md variants, and CLAUDE.md pointers. Handles marker-based user content preservation, version tracking, and annex file generation for reproduction-critical constants.\n\n## Contents\n\n### Sum File Generation\n[sum.ts](./sum.ts) — Serializes source documentation into YAML-frontmatter .sum files with SHA-256 change detection. Exports `readSumFile()`, `writeSumFile()`, `getSumPath()`, `sumFileExists()`, `writeAnnexFile()`, `getAnnexPath()`. Uses `SumFileContent` type with `summary`, `metadata`, `generatedAt`, `contentHash` fields. Frontmatter parsing extracts `generated_at`, `content_hash`, `purpose`, and optional `critical_todos`, `related_files` via regex-based YAML array extraction.\n\n### AGENTS.md Writers\n[agents-md.ts](./agents-md.ts) — Manages AGENTS.md file generation with marker-based user content preservation. Exports `GENERATED_MARKER_PREFIX`, `GENERATED_MARKER`, `isGeneratedAgentsMd()`, `writeAgentsMd()`, `writeAgentsMdHub()`. Detects user-authored content by checking for `GENERATED_MARKER_PREFIX` substring; renames existing AGENTS.md to AGENTS.local.md and prepends `@AGENTS.local.md` reference if user content detected. Supports variant suffix (e.g., AGENTS.claude.haiku.md) and hub mode with active variant references.\n\n### CLAUDE.md Pointer\n[claude-md.ts](./claude-md.ts) — Generates CLAUDE.md with @-references to AGENTS.md and optional CLAUDE.local.md. Exports `writeClaudeMdPointer()`. Detects and preserves user-authored CLAUDE.md by renaming to CLAUDE.local.md, then writes hub file with `@CLAUDE.local.md` and `@AGENTS.md` references using identical marker and preservation logic as agents-md.ts.\n\n### Barrel Export\n[index.ts](./index.ts) — Re-exports `writeSumFile()`, `readSumFile()`, `getSumPath()`, `sumFileExists()`, `getAnnexFile()`, `writeAnnexFile()`, `writeAgentsMd()`, `writeAgentsMdHub()`, `writeClaudeMdPointer()` from aggregated writer modules.\n\n## Behavioral Contracts\n\n**Marker Format**: `GENERATED_MARKER_PREFIX` = `'<!-- Generated by agents-reverse-engineer'`; `GENERATED_MARKER` = `${GENERATED_MARKER_PREFIX} v${version} -->`\n\n**User Content Detection**: Files lacking `GENERATED_MARKER_PREFIX` substring are treated as user-authored and preserved via rename to `.local.md` variant.\n\n**File Naming Patterns**: \n- Sum files: `foo.ts.sum` or `foo.ts.${variant}.sum`\n- Annex files: `foo.annex.sum` or `foo.annex.${variant}.sum`\n- AGENTS variants: `AGENTS.${variant}.md` or hub `AGENTS.md`\n- CLAUDE variants: `CLAUDE.md` and optional `CLAUDE.local.md`\n\n**YAML Frontmatter Arrays**: Inline format `[a, b, c]` for ≤3 short items; multi-line `- item` format otherwise. Parsing uses regex `/key:\\s*\\[([^\\]]*)\\]/` (inline) and `/key:\\s*\\n((?:\\s+-\\s+.+\\n?)+)/m` (multi-line).\n\n**Content Preservation Workflow**: (1) Check existing file for GENERATED_MARKER_PREFIX, (2) if absent, rename to `.local.md`, (3) check for pre-existing `.local.md` from prior runs, (4) prepend `@${localFile}` reference to generated content if local content exists. Marker stripping removes leading `GENERATED_MARKER_PREFIX` + content up to `-->` + leading newlines from LLM input.\n\n**Hub Reference Format**: @-references use exact identifiers: `@AGENTS.${variant}.md`, `@AGENTS.local.md`, `@CLAUDE.local.md`, `@AGENTS.md`.\n\n## Architecture\n\n**Layered Writer Stack**: sum.ts (lowest level, YAML serialization) → agents-md.ts, claude-md.ts (mid-level, variant/hub logic) → index.ts (export aggregation).\n\n**Marker-Based Preservation**: All writers (sum.ts via annex headers, agents-md.ts, claude-md.ts) use `GENERATED_MARKER` constant from agents-md.ts to identify AI-generated files and trigger user-content rename-and-reference workflows. Prevents accidental overwrites of user contributions.\n\n**Variant Support**: sum.ts and agents-md.ts support optional `variant` parameter for multi-model documentation (e.g., `file.ts.claude.haiku.sum`, `AGENTS.claude.haiku.md`); hub mode writes version-agnostic files that reference the active variant.",
      "systemPrompt": "You are generating an AGENTS.md file — a directory-level overview for AI coding assistants.\n\nCRITICAL: Output ONLY the raw markdown content. No code fences, no preamble, no explanations, no conversational text. Your entire response IS the AGENTS.md file content.\n\nOUTPUT FORMAT:\n- First line MUST be exactly: <!-- Generated by agents-reverse-engineer -->\n- Use a # heading with the directory name\n- Write a one-paragraph purpose statement for the directory\nusage\nADAPTIVE SECTIONS:\nAnalyze the directory contents and choose the most relevant sections. Do NOT use a fixed template. Instead, select sections that best document this specific directory for an AI that needs to reconstruct or extend the project.\n\nConsider these section types (choose what applies):\n- **Contents**: Group files by purpose/category under ## headings. For each file: markdown link [filename](./filename) and a one-line description.\n- **Subdirectories**: If subdirectories exist, list them with links [dirname/](./dirname/) and brief summaries.\n- **Architecture / Data Flow**: If files form a pipeline, request/response chain, or layered architecture, document it.\n- **Stack**: If this is a package root (has package.json, Cargo.toml, go.mod, etc.), document the technology stack, key scripts, and entry points.\n- **Structure**: If the directory layout follows a convention (feature-sliced, domain-driven, MVC, etc.), document it.\n- **Patterns**: If files share recurring design patterns (factory, strategy, middleware, barrel re-export), name and document them.\n- **Configuration**: If the directory contains config files, schemas, or environment definitions, document the config surface area.\n- **API Surface**: If the directory exports a public API (barrel index, route definitions, SDK), document the interface contract.\n- **File Relationships**: How files collaborate, depend on each other, or share state.\n- **Behavioral Contracts**: If files contain regex patterns, format specifications, magic constants, or template strings that define observable behavior, collect them in a dedicated section. Preserve verbatim patterns from file summaries — do NOT paraphrase regex into prose. This section is MANDATORY when file summaries contain behavioral artifacts.\n- **Reproduction-Critical Constants**: If file summaries reference annex files (via ## Annex References sections), list them with links. Example: \"Full prompt template text: [templates.annex.sum](./prompts/templates.annex.sum)\". Do NOT reproduce annex content in AGENTS.md — just link to it.\n- **Workflow & Conventions**: If file summaries contain contribution guidelines, PR conventions, commit standards, testing mandates, tool usage rules, AI agent instructions, or code conventions (naming, formatting, linting), collect them into actionable directives. Source from CONTRIBUTING.md, CI configs, PR templates, linter/formatter configs, README dev sections. Present as concrete rules (\"commits must use conventionalcommits format\", \"use camelCase for functions, PascalCase for types\") not vague descriptions (\"the project has guidelines\").\n\nChoose any relevant sections or create your own based on the directory contents. The goal is to provide a comprehensive overview that captures the essence of the directory's role in the project and how its files work together, with a focus on what an AI coding assistant would need to know to effectively interact with this code.\n\nSCOPE:\n- AGENTS.md is a NAVIGATIONAL INDEX — help an AI find the right file quickly\n- Focus on: what each file does, how files relate, directory-level patterns\n- Do NOT reproduce full architecture sections — those belong in the root CLAUDE.md\n\nPATH ACCURACY (MANDATORY):\n- When referencing files or modules outside this directory, use ONLY paths from the \"Import Map\" section\n- Do NOT invent, rename, or guess module paths — if a path isn't in the Import Map, don't reference it\n- Use the exact directory names from \"Project Directory Structure\" — do NOT rename directories\n  (e.g., if the directory is called \"cli\", write \"src/cli/\", NOT \"src/commands/\")\n- Cross-module references must use the specifier format from actual import statements\n  (e.g., \"../generation/writers/sum.js\", NOT \"../fs/sum-file.js\")\n- If you are unsure about a path, omit the cross-reference rather than guessing\n\nCONSISTENCY (MANDATORY):\n- Do not contradict yourself within the same document\n- If you describe a technique (e.g., \"regex-based\"), do not call it something else later (e.g., \"AST-based\")\n- When stating version numbers, engines, or config fields, use ONLY values present in the file summaries\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this directory\", \"this module\", \"provides\", \"responsible for\", \"is used to\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Per-file descriptions in Contents sections: 1-2 sentences maximum. Reference key symbols but do not reproduce full summaries.\n- Behavioral contracts (regex patterns, format specs, constants) belong in a separate Behavioral Contracts section, not in per-file descriptions.\n- Annex file references: link to .annex.sum files, do not inline their content. One line per annex reference.\n- Subdirectory descriptions: 1-2 sentences maximum. Capture the directory's role, not its full contents.\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- Key exported symbols from file summaries MUST appear in the directory overview\n- Preserve exact casing of identifiers\n\nUSER NOTES:\n- If \"User Notes\" are provided in the prompt, they contain user-defined instructions that will be automatically prepended to your output\n- Do NOT repeat or paraphrase user notes in your generated content — they are included separately\n- You may reference information from user notes for context",
      "response": "Based on the provided file summaries and project structure, here is the AGENTS.md for src/generation:\n\n<!-- Generated by agents-reverse-engineer -->\n\n# src/generation\n\nThis directory orchestrates documentation generation pipelines, combining file discovery, complexity analysis, and plan execution to produce .sum file summaries and AGENTS.md directory overviews with intelligent post-order traversal, variant support, and user content preservation.\n\n## Contents\n\n### Core Pipeline Files\n- [collector.ts](./collector.ts) — `collectAgentsDocs()` recursively walks project tree to locate and aggregate AGENTS.md files; `collectAnnexFiles()` collects .annex.sum files for reproduction-critical constants. Both functions skip directories (node_modules, .git, .agents-reverse-engineer, vendor, dist, build, __pycache__, .next, venv, .venv, target, .cargo, .gradle) and return sorted `AgentsDocs` tuples.\n\n- [complexity.ts](./complexity.ts) — `analyzeComplexity()` computes `ComplexityMetrics` (fileCount, directoryDepth, files Set, directories Set) from file lists; `calculateDirectoryDepth()` measures maximum nesting; `extractDirectories()` recursively traverses parent directories via `path.dirname()` until reaching filesystem root.\n\n- [executor.ts](./executor.ts) — `buildExecutionPlan()` transforms `GenerationPlan` into `ExecutionPlan` with post-order sorted directory tasks (deepest first) and file task dependencies; `isDirectoryComplete()` checks if all files have .sum artifacts; `getReadyDirectories()` filters directories with complete .sum coverage; `formatExecutionPlanAsMarkdown()` generates GENERATION-PLAN.md with task phase breakdown and skipped file/directory counts.\n\n- [types.ts](./types.ts) — Defines `SummaryMetadata` interface with `purpose`, optional `criticalTodos[]`, optional `relatedFiles[]` for pipeline type contracts.\n\n## Subdirectories\n\n### [prompts/](./prompts/)\nPrompt construction utilities for file summaries and AGENTS.md synthesis. `buildFilePrompt()` handles template substitution, compression injection (when ratio < 0.5), and language detection; `buildDirectoryPrompt()` orchestrates multi-step synthesis: reads .sum files, extracts directory imports, detects .annex.sum files and manifest indicators (package.json, Cargo.toml, etc.), preserves user-authored content via `GENERATED_MARKER_PREFIX` detection, and returns update prompts when existingAgentsMd provided.\n\n### [writers/](./writers/)\nFile writers for .sum serialization, AGENTS.md variants, and CLAUDE.md pointers. `readSumFile()`, `writeSumFile()` handle YAML-frontmatter .sum files with SHA-256 change detection; `writeAgentsMd()`, `writeAgentsMdHub()` manage variant suffix and marker-based user content preservation (rename to .local.md if user-authored); `writeClaudeMdPointer()` generates CLAUDE.md with @-references to AGENTS.md and optional CLAUDE.local.md.\n\n## Architecture & Data Flow\n\n**Generation Pipeline:** discovery → complexity analysis → execution plan building → parallel file/directory task execution → .sum + AGENTS.md output.\n\n1. **File Discovery & Collection** (collector.ts): `collectAgentsDocs()` and `collectAnnexFiles()` recursively aggregate project documentation.\n\n2. **Complexity Metrics** (complexity.ts): `analyzeComplexity()` computes fileCount, directoryDepth, directory ancestors from discovered file paths to inform traversal order.\n\n3. **Execution Plan** (executor.ts): `buildExecutionPlan()` consumes `GenerationPlan` type (from src/orchestration), creates `ExecutionTask` objects (type: 'file'|'directory', path, absolutePath, systemPrompt, userPrompt, dependencies, outputPath, metadata), post-order sorts directory tasks by depth descending (deepest first) to ensure child AGENTS.md generation completes before parents, tracks all discovered files in `directoryFileMap` for complete prompt context, and marks skipped files/directories.\n\n4. **Prompt Assembly** (prompts/): `buildFilePrompt()` and `buildDirectoryPrompt()` construct system/user prompts with template substitution, compression rules, import extraction, and preservation of existing summaries for incremental updates.\n\n5. **Artifact Writing** (writers/): `writeSumFile()` serializes documentation into YAML-frontmatter .sum files with content hash; `writeAgentsMd()` writes directory overviews with marker-based user content detection and .local.md preservation; `writeClaudeMdPointer()` generates hub files with @-references.\n\n**Variant Support:** Optional `variant` parameter (e.g., \"claude.haiku\") propagates through `getSumPath()`, `sumFileExists()`, `getAnnexPath()`, and file naming (AGENTS.${variant}.md, file.ts.${variant}.sum).\n\n**Post-Order Traversal:** Directory tasks sorted by `getDirectoryDepth()` descending ensures parent AGENTS.md synthesis waits for all child `.sum` file completion, enabling aggregated imports and nested directory references in parent directory prompts.\n\n## Behavioral Contracts\n\n**Directory Task Dependencies:** fileTaskIds are formatted as `file:${f}` and assigned to directoryTasks[].dependencies; only directories in plan.tasks (plannedDirs) become ExecutionTasks.\n\n**Complexity Depth Calculation:** directoryDepth = max(relative path segment count - 1 for file itself). Root '.' = 0, 'src' = 1, 'src/cli' = 2.\n\n**Skip Directory List (SKIP_DIRS):** node_modules, .git, .agents-reverse-engineer, vendor, dist, build, __pycache__, .next, venv, .venv, target, .cargo, .gradle. Graceful handling: readdir/readFile failures trigger silent skip.\n\n**Compression Ratio Trigger:** When sourceFileSize > 0 and compressionRatio < 0.5, targetSize = Math.round(sourceFileSize * ratio), maxSize = Math.round(targetSize * 1.2); aggressive compression rules injected into user prompt.\n\n**Template Substitution Patterns:** `{{FILE_PATH}}`, `{{CONTENT}}`, `{{LANG}}` replaced via `.replace(/\\{\\{...\\}\\}/g, ...)` in FILE_USER_PROMPT.\n\n**Generated Marker Detection:** `GENERATED_MARKER_PREFIX` = `'<!-- Generated by agents-reverse-engineer'`; files lacking this substring treated as user-authored and preserved via rename to `.local.md` variant.\n\n**Manifest File Indicators:** Presence of package.json, Cargo.toml, go.mod, pyproject.toml, pom.xml, build.gradle, Gemfile, composer.json, CMakeLists.txt, or Makefile signals project root/manifest hints in directory prompt.\n\n**Incremental Update Mode:** FILE_UPDATE_SYSTEM_PROMPT triggered when context.existingSum truthy; DIRECTORY_UPDATE_SYSTEM_PROMPT triggered when existingAgentsMd truthy; existing content appended under \"Existing Summary\"/\"Existing AGENTS.md\" headers with explicit preserve-stable-sections instruction.\n\n**YAML Frontmatter Arrays:** Inline format `[a, b, c]` for ≤3 short items; multi-line `- item` format otherwise. Parsing uses regex `/key:\\s*\\[([^\\]]*)\\]/` (inline) and `/key:\\s*\\n((?:\\s+-\\s+.+\\n?)+)/m` (multi-line).\n\n**Sum File Path Convention:** format `${dir}/${basename}.${ext}.sum` or `${dir}/${basename}.${ext}.${variant}.sum`; annex files follow `${dir}/${basename}.annex.sum` or `${dir}/${basename}.annex.${variant}.sum`.\n\n**Content Hash Tracking:** SHA-256 contentHash stored in sum file frontmatter; enables change detection for incremental update decisions.\n\n## Reproduction-Critical Constants\n\nFull prompt template text: [templates.annex.sum](./prompts/templates.annex.sum)",
      "model": "claude-haiku-4-5-20251001",
      "inputTokens": 17,
      "outputTokens": 2129,
      "cacheReadTokens": 64254,
      "cacheCreationTokens": 6040,
      "latencyMs": 49358,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": []
    },
    {
      "timestamp": "2026-02-14T13:49:24.304Z",
      "prompt": "Generate AGENTS.md for directory: \"src/quality\" (quality)\n\n## File Summaries (2 files)\n\n### index.ts\n**Purpose:** src/quality/index.ts is the public API barrel export for quality analysis modules (inconsistency detection, density v...\n\n**src/quality/index.ts is the public API barrel export for quality analysis modules (inconsistency detection, density validation, phantom path detection).**\n\n## Exported Types\n`InconsistencySeverity`, `CodeDocInconsistency`, `CodeCodeInconsistency`, `PhantomPathInconsistency`, `Inconsistency`, `InconsistencyReport` — inconsistency classification and report structures.\n\n## Exported Functions\n`extractExports()`—extracts code exports for comparison; `checkCodeVsDoc()` — detects code-documentation mismatches; `checkCodeVsCode()` — detects code-to-code inconsistencies; `buildInconsistencyReport()` — constructs inconsistency reports; `formatReportForCli()`, `formatReportAsMarkdown()` — report formatting; `checkPhantomPaths()` — detects phantom path references.\n\n## Module Organization\nRe-exports from `./inconsistency/code-vs-doc.js`, `./inconsistency/code-vs-code.js`, `./inconsistency/reporter.js`, `./phantom-paths/index.js`; types from `./types.js`. Barrel pattern centralizes quality analysis API.\n### types.ts\n**Purpose:** types.ts defines inconsistency detection types for quality analysis: CodeDocInconsistency, CodeCodeInconsistency, Pha...\n\n**types.ts defines inconsistency detection types for quality analysis: CodeDocInconsistency, CodeCodeInconsistency, PhantomPathInconsistency, and InconsistencyReport.**\n\n## Exported Types\n\n`InconsistencySeverity` — union type: `'info' | 'warning' | 'error'`.\n\n`CodeDocInconsistency` — interface tracking .sum/source mismatches; fields: type `'code-vs-doc'`, severity, filePath, sumPath, description, details (missingFromDoc[], missingFromCode[], purposeMismatch?).\n\n`CodeCodeInconsistency` — interface for cross-file pattern conflicts; fields: type `'code-vs-code'`, severity, files[], description, pattern string.\n\n`PhantomPathInconsistency` — interface for invalid doc references; fields: type `'phantom-path'`, severity, agentsMdPath, description, details (referencedPath, resolvedTo, context).\n\n`Inconsistency` — union: CodeDocInconsistency | CodeCodeInconsistency | PhantomPathInconsistency.\n\n`InconsistencyReport` — aggregated findings; fields: metadata (timestamp, projectRoot, filesChecked, durationMs), issues[], summary (total, codeVsDoc, codeVsCode, phantomPaths, errors, warnings, info counts).\n\n## Project Directory Structure\n\n<project-structure>\n./\n  .npmrc\n  CONTRIBUTING.md\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  pages.yml\n  publish.yml\nhooks/\n  are-check-update.js\n  are-context-loader.js\n  opencode-are-check-update.js\nscripts/\n  analyze-agents-compression.sh\n  analyze-sum-ratios.sh\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  codex.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/providers/\n  subprocess.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  rebuild.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/core/\n  index.ts\n  logger.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  claude-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  orchestrator.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/rebuild/\n  checkpoint.ts\n  index.ts\n  orchestrator.ts\n  output-parser.ts\n  prompts.ts\n  spec-reader.ts\n  types.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\n\n## Subdirectories\n\n### inconsistency/\n<!-- Generated by agents-reverse-engineer v0.9.10 -->\n\n# src/quality/inconsistency\n\nDetects and reports inconsistencies between code exports and documentation, and duplicate symbol exports across files, via heuristic analysis and structured reporting.\n\n## Contents\n\n**Export Detection & Comparison**\n\n[code-vs-doc.ts](./code-vs-doc.ts) — Compares TypeScript/JavaScript exports against .sum documentation; exports `extractExports(sourceContent: string): string[]` using regex `/^[ \\t]*export\\s+(?:default\\s+)?(?:function|class|const|let|var|type|interface|enum)\\s+(\\w+)/gm` and `checkCodeVsDoc(sourceContent: string, sumContent: SumFileContent, filePath: string): CodeDocInconsistency | null` for case-sensitive substring matching against `sumContent.summary`.\n\n[code-vs-code.ts](./code-vs-code.ts) — Detects duplicate symbol exports across co-located source files; exports `checkCodeVsCode(files: Array<{ path: string; content: string }>): CodeCodeInconsistency[]` via `Map<string, string[]>` heuristic tracking export names to file paths.\n\n**Reporting & Formatting**\n\n[reporter.ts](./reporter.ts) — Aggregates inconsistencies into typed reports and formats output; exports `buildInconsistencyReport(issues: Inconsistency[], metadata: {...}): InconsistencyReport`, `formatReportForCli(report: InconsistencyReport): string` with severity tags `[ERROR]`, `[WARN]`, `[INFO]`, and `formatReportAsMarkdown(report: InconsistencyReport): string` for GitHub Markdown tables.\n\n## Detection Pipeline\n\n`extractExports()` parses named/default exports from source via regex, matching function/class/const/let/var/type/interface/enum declarations and ignoring re-exports. `checkCodeVsDoc()` flags exports absent from `SumFileContent.summary` text using `.includes()`. `checkCodeVsCode()` builds export-to-paths map and flags entries with `paths.length > 1`. `buildInconsistencyReport()` aggregates results with counts (codeVsDoc, codeVsCode, phantomPaths) and metadata (timestamp ISO, projectRoot, filesChecked, durationMs). CLI and Markdown formatters render severity, type, description, and location (filePath for code-vs-doc; files array for code-vs-code).\n\n## Behavioral Contracts\n\n**Export Regex Pattern**\n```\n/^[ \\t]*export\\s+(?:default\\s+)?(?:function|class|const|let|var|type|interface|enum)\\s+(\\w+)/gm\n```\nMatches leading spaces/tabs, `export` keyword, optional `default`, keyword class (function/class/const/let/var/type/interface/enum), and identifier capture. Re-exports and comments ignored.\n\n**Inconsistency Objects**\n\ncode-vs-doc: `{type: 'code-vs-doc', severity: 'warning', filePath, description: 'Documentation out of sync: N exports undocumented', missingFromDoc: string[], missingFromCode: []}`\n\ncode-vs-code: `{type: 'code-vs-code', severity: 'warning', files: string[], description: 'Symbol \"[name]\" exported from [count] files', pattern: 'duplicate-export'}`\n\nphantom-path: `{type: 'phantom-path', severity: 'error'|'warning'|'info', agentsMdPath, filePath, description, details: {referencedPath, ...}}`\n\n**Report Summary Structure**\n\n`InconsistencyReport` contains `metadata` (timestamp ISO 8601, projectRoot, filesChecked, durationMs), `issues` (Inconsistency[] array), `summary` (total, codeVsDoc, codeVsCode, phantomPaths, errors, warnings, info counts).\n\n## Type Dependencies\n\nImports from `../types.js`: `CodeCodeInconsistency`, `CodeDocInconsistency`, `Inconsistency`, `InconsistencyReport`.\n\nImports from `../../generation/writers/sum.js`: `SumFileContent`.\n### phantom-paths/\n<!-- Generated by agents-reverse-engineer v0.9.10 -->\n\n# src/quality/phantom-paths\n\nDetects and reports invalid path references in AGENTS.md documentation by matching three classes of path patterns (markdown links, backtick-quoted paths, prose references) and verifying file existence via regex-based extraction and filesystem resolution.\n\n## Contents\n\n- [index.ts](./index.ts) — Re-exports `checkPhantomPaths` validator.\n- [validator.ts](./validator.ts) — Implements `checkPhantomPaths(agentsMdPath, content, projectRoot)` core validation logic.\n\n## Exports\n\n`checkPhantomPaths(agentsMdPath: string, content: string, projectRoot: string)` returns `PhantomPathInconsistency[]` array.\n\n## Path Detection Patterns\n\nThree `RegExp` instances in `PATH_PATTERNS` match:\n\n- Markdown links: `\\[(?:[^\\]]*)\\]\\((\\.[^)]+)\\)/g`\n- Backtick-quoted paths: `` /`((?:src\\/|\\.\\.?\\/)[^`]+\\.[a-z]{1,4})`/g ``\n- Prose references (from/in/by/via/see clauses): `/(?:from|in|by|via|see)\\s+`?(src\\/[\\w\\-./]+)`?/gi`\n\n## Validation Rules\n\n`SKIP_PATTERNS` — six RegExp instances reject node_modules, .git/, `https?:` URLs, template placeholders `{{`, template literals `${`, glob wildcards `*`, and brace expansion syntax `{a,b,c}`.\n\n## Resolution Strategy\n\nPaths resolved in sequence: (1) relative to AGENTS.md directory, (2) relative to `projectRoot` for src/ references, (3) with `.js`→`.ts` substitution for TypeScript import conventions. Deduplicates via `seen` Set.\n\n## Issue Structure\n\n`PhantomPathInconsistency` objects contain `type: 'phantom-path'`, `severity: 'warning'`, relative `agentsMdPath`, unresolved `rawPath`, absolute `resolvedTo` filesystem path, and 120-character context snippet.",
      "systemPrompt": "You are generating an AGENTS.md file — a directory-level overview for AI coding assistants.\n\nCRITICAL: Output ONLY the raw markdown content. No code fences, no preamble, no explanations, no conversational text. Your entire response IS the AGENTS.md file content.\n\nOUTPUT FORMAT:\n- First line MUST be exactly: <!-- Generated by agents-reverse-engineer -->\n- Use a # heading with the directory name\n- Write a one-paragraph purpose statement for the directory\nusage\nADAPTIVE SECTIONS:\nAnalyze the directory contents and choose the most relevant sections. Do NOT use a fixed template. Instead, select sections that best document this specific directory for an AI that needs to reconstruct or extend the project.\n\nConsider these section types (choose what applies):\n- **Contents**: Group files by purpose/category under ## headings. For each file: markdown link [filename](./filename) and a one-line description.\n- **Subdirectories**: If subdirectories exist, list them with links [dirname/](./dirname/) and brief summaries.\n- **Architecture / Data Flow**: If files form a pipeline, request/response chain, or layered architecture, document it.\n- **Stack**: If this is a package root (has package.json, Cargo.toml, go.mod, etc.), document the technology stack, key scripts, and entry points.\n- **Structure**: If the directory layout follows a convention (feature-sliced, domain-driven, MVC, etc.), document it.\n- **Patterns**: If files share recurring design patterns (factory, strategy, middleware, barrel re-export), name and document them.\n- **Configuration**: If the directory contains config files, schemas, or environment definitions, document the config surface area.\n- **API Surface**: If the directory exports a public API (barrel index, route definitions, SDK), document the interface contract.\n- **File Relationships**: How files collaborate, depend on each other, or share state.\n- **Behavioral Contracts**: If files contain regex patterns, format specifications, magic constants, or template strings that define observable behavior, collect them in a dedicated section. Preserve verbatim patterns from file summaries — do NOT paraphrase regex into prose. This section is MANDATORY when file summaries contain behavioral artifacts.\n- **Reproduction-Critical Constants**: If file summaries reference annex files (via ## Annex References sections), list them with links. Example: \"Full prompt template text: [templates.annex.sum](./prompts/templates.annex.sum)\". Do NOT reproduce annex content in AGENTS.md — just link to it.\n- **Workflow & Conventions**: If file summaries contain contribution guidelines, PR conventions, commit standards, testing mandates, tool usage rules, AI agent instructions, or code conventions (naming, formatting, linting), collect them into actionable directives. Source from CONTRIBUTING.md, CI configs, PR templates, linter/formatter configs, README dev sections. Present as concrete rules (\"commits must use conventionalcommits format\", \"use camelCase for functions, PascalCase for types\") not vague descriptions (\"the project has guidelines\").\n\nChoose any relevant sections or create your own based on the directory contents. The goal is to provide a comprehensive overview that captures the essence of the directory's role in the project and how its files work together, with a focus on what an AI coding assistant would need to know to effectively interact with this code.\n\nSCOPE:\n- AGENTS.md is a NAVIGATIONAL INDEX — help an AI find the right file quickly\n- Focus on: what each file does, how files relate, directory-level patterns\n- Do NOT reproduce full architecture sections — those belong in the root CLAUDE.md\n\nPATH ACCURACY (MANDATORY):\n- When referencing files or modules outside this directory, use ONLY paths from the \"Import Map\" section\n- Do NOT invent, rename, or guess module paths — if a path isn't in the Import Map, don't reference it\n- Use the exact directory names from \"Project Directory Structure\" — do NOT rename directories\n  (e.g., if the directory is called \"cli\", write \"src/cli/\", NOT \"src/commands/\")\n- Cross-module references must use the specifier format from actual import statements\n  (e.g., \"../generation/writers/sum.js\", NOT \"../fs/sum-file.js\")\n- If you are unsure about a path, omit the cross-reference rather than guessing\n\nCONSISTENCY (MANDATORY):\n- Do not contradict yourself within the same document\n- If you describe a technique (e.g., \"regex-based\"), do not call it something else later (e.g., \"AST-based\")\n- When stating version numbers, engines, or config fields, use ONLY values present in the file summaries\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this directory\", \"this module\", \"provides\", \"responsible for\", \"is used to\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Per-file descriptions in Contents sections: 1-2 sentences maximum. Reference key symbols but do not reproduce full summaries.\n- Behavioral contracts (regex patterns, format specs, constants) belong in a separate Behavioral Contracts section, not in per-file descriptions.\n- Annex file references: link to .annex.sum files, do not inline their content. One line per annex reference.\n- Subdirectory descriptions: 1-2 sentences maximum. Capture the directory's role, not its full contents.\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- Key exported symbols from file summaries MUST appear in the directory overview\n- Preserve exact casing of identifiers\n\nUSER NOTES:\n- If \"User Notes\" are provided in the prompt, they contain user-defined instructions that will be automatically prepended to your output\n- Do NOT repeat or paraphrase user notes in your generated content — they are included separately\n- You may reference information from user notes for context",
      "response": "<!-- Generated by agents-reverse-engineer -->\n\n# src/quality\n\nQuality analysis module providing inconsistency detection across code-vs-documentation, code-vs-code patterns, and phantom path references in AGENTS.md documentation. Exports types and validation functions for comprehensive documentation accuracy auditing.\n\n## Contents\n\n**API Barrel & Types**\n\n- [index.ts](./index.ts) — Public API re-exporting `extractExports()`, `checkCodeVsDoc()`, `checkCodeVsCode()`, `buildInconsistencyReport()`, `formatReportForCli()`, `formatReportAsMarkdown()`, `checkPhantomPaths()` and types `InconsistencySeverity`, `CodeDocInconsistency`, `CodeCodeInconsistency`, `PhantomPathInconsistency`, `Inconsistency`, `InconsistencyReport`.\n- [types.ts](./types.ts) — Defines `InconsistencySeverity` ('info' | 'warning' | 'error'), `CodeDocInconsistency`, `CodeCodeInconsistency`, `PhantomPathInconsistency`, `Inconsistency` union, and `InconsistencyReport` with metadata, issues array, and summary counts.\n\n## Subdirectories\n\n- [inconsistency/](./inconsistency/) — Detects and reports mismatches between code exports and documentation via `extractExports()` regex heuristic, `checkCodeVsDoc()` substring matching, and `checkCodeVsCode()` duplicate-export detection. Aggregates results with `buildInconsistencyReport()` and formats via `formatReportForCli()` (plain text with `[ERROR]`/`[WARN]`/`[INFO]` tags) and `formatReportAsMarkdown()` (GitHub Markdown table).\n- [phantom-paths/](./phantom-paths/) — Validates AGENTS.md path references via three `RegExp` patterns (markdown links, backtick paths, prose references) and resolves against AGENTS.md directory and projectRoot with `.js`→`.ts` fallback. Returns `PhantomPathInconsistency[]` with context snippets.\n\n## Validation Pipeline\n\n**Export Detection**: `extractExports(sourceContent)` parses TypeScript/JavaScript exports using regex matching `export` keyword, optional `default`, and declaration type (function/class/const/let/var/type/interface/enum), capturing identifier names.\n\n**Code-vs-Doc**: `checkCodeVsDoc(sourceContent, sumContent, filePath)` compares extracted exports against `SumFileContent.summary` text via `.includes()`, returning `CodeDocInconsistency` when exports are absent from documentation.\n\n**Code-vs-Code**: `checkCodeVsCode(files)` builds `Map<string, string[]>` tracking export names to file paths, flagging entries with `paths.length > 1` as duplicate-export issues.\n\n**Reporting**: `buildInconsistencyReport(issues, metadata)` aggregates inconsistencies by type (codeVsDoc, codeVsCode, phantomPaths) and severity (error, warning, info), attaching ISO 8601 timestamp, projectRoot, filesChecked, durationMs.\n\n**Phantom Paths**: `checkPhantomPaths(agentsMdPath, content, projectRoot)` extracts path strings, deduplicates via `Set`, skips node_modules/git/.http/templates/glob patterns, resolves relative to AGENTS.md dir and projectRoot, attempts `.js`→`.ts` substitution, and verifies `existsSync()`.\n\n## Behavioral Contracts\n\n**Export Regex Pattern**\n```\n/^[ \\t]*export\\s+(?:default\\s+)?(?:function|class|const|let|var|type|interface|enum)\\s+(\\w+)/gm\n```\n\n**Markdown Link Pattern**\n```\n/\\[(?:[^\\]]*)\\]\\((\\.[^)]+)\\)/g\n```\n\n**Backtick Path Pattern**\n```\n/`((?:src\\/|\\.\\.?\\/)[^`]+\\.[a-z]{1,4})`/g\n```\n\n**Prose Path Pattern**\n```\n/(?:from|in|by|via|see)\\s+`?(src\\/[\\w\\-./]+)`?/gi\n```\n\n**Skip Patterns**\n```\n/node_modules/ , /\\.git\\// , /^https?:/ , /\\{\\{/ , /\\$\\{/ , /\\*/ , /\\{[^}]*,[^}]*\\}/\n```\n\n**Inconsistency Report Structure**: metadata (timestamp ISO 8601, projectRoot, filesChecked, durationMs), issues (Inconsistency[]), summary (total, codeVsDoc, codeVsCode, phantomPaths, errors, warnings, info counts).\n\n**CLI Format Tags**: `[ERROR]` | `[WARN]` | `[INFO]` with file/doc/path location info.\n\n**Markdown Table**: Columns: Severity (`ERROR`/`WARN`/`INFO`), Type (code-vs-doc/code-vs-code/phantom-path), Description, Location (filePath or files list).",
      "model": "claude-haiku-4-5-20251001",
      "inputTokens": 35,
      "outputTokens": 2972,
      "cacheReadTokens": 181179,
      "cacheCreationTokens": 15481,
      "latencyMs": 53316,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": []
    },
    {
      "timestamp": "2026-02-14T13:49:24.426Z",
      "prompt": "Generate AGENTS.md for directory: \"src/orchestration\" (orchestration)\n\n## File Summaries (8 files)\n\n### index.ts\n**Purpose:** Barrel export aggregating orchestration module's public API: CommandRunner, ProgressReporter, runPool, PlanTracker, a...\n\n**Barrel export aggregating orchestration module's public API: CommandRunner, ProgressReporter, runPool, PlanTracker, and trace utilities.**\n\n## Exports\n\n**Types:** FileTaskResult, RunSummary, ProgressEvent, CommandRunOptions from types.js; PoolOptions, TaskResult from pool.js; ITraceWriter, TraceEvent, TraceEventPayload from trace.js.\n\n**Classes:** CommandRunner; ProgressReporter, ProgressLog; PlanTracker.\n\n**Functions:** runPool (concurrency pool); createTraceWriter, cleanupOldTraces (trace management).\n\n## Module Dependencies\n\nAggregates from five internal modules: types.js, pool.js, progress.js, plan-tracker.js, trace.js, runner.js. Re-exports form single import surface for orchestration subsystem.\n\n## Usage Pattern\n\nPrimary consumers import CommandRunner, ProgressReporter, and runPool from this barrel. CommandRunner constructor accepts aiService and CommandRunOptions (concurrency parameter); executeGenerate(plan) returns Promise<RunSummary>.\n### orchestrator.ts\n**Purpose:** DocumentationOrchestrator unifies generation and incremental update workflows for AI-friendly documentation, providin...\n\n**DocumentationOrchestrator unifies generation and incremental update workflows for AI-friendly documentation, providing two main execution paths (full project analysis vs. change-detection-driven updates) with shared task-building logic.**\n\n## Exported Types\n\n`PreparedFile` — file with absolute path, relative path, and content string for generation workflow.\n\n`AnalysisTask` — task object (type: 'file' | 'directory') with filePath, optional systemPrompt/userPrompt for file tasks, optional directoryInfo (sumFiles array, fileCount) for directory tasks.\n\n`GenerationPlan` — plan result containing files[], tasks[], ComplexityMetrics, optional projectStructure string, optional skippedFiles/skippedDirs arrays, optional allDiscoveredFiles[].\n\n`UpdatePlan` — plan result containing filesToAnalyze (FileChange[]), fileTasks (AnalysisTask[]), filesToSkip[], cleanup (CleanupResult), affectedDirs[], baseCommit, currentCommit, isFirstRun (boolean).\n\n## Exported Classes\n\n`DocumentationOrchestrator` — unified orchestrator with generation methods (prepareFiles, filterExistingFiles, filterExistingDirectories, createDirectoryTasks, createPlan), update methods (checkPrerequisites, preparePlan, recordFileAnalyzed, removeFileState, recordRun, getLastRun, isFirstRun), and shared method (createFileTasks). Constructor accepts Config, projectRoot string, and optional options: { tracer?: ITraceWriter; debug?: boolean; logger?: Logger }.\n\n## Exported Factory Functions\n\n`createOrchestrator(config: Config, projectRoot: string, options?: {...}): DocumentationOrchestrator` — factory for generation and update workflows.\n\n`createUpdateOrchestrator(config: Config, projectRoot: string, options?: {...}): DocumentationOrchestrator` — alias for update command compatibility.\n\n## Generation Workflow (are generate)\n\n`createPlan(discoveryResult: DiscoveryResult, options?: { force?: boolean }): Promise<GenerationPlan>` — orchestrates full project documentation generation. Calls prepareFiles() to load all discovered files, optionally filters by existing .sum artifacts (unless force=true), analyzes complexity via analyzeComplexity(), builds projectStructure string via buildProjectStructure(), creates fileTasks via createFileTasks(), filters directories via filterExistingDirectories() and createDirectoryTasks(), combines file and directory tasks. Emits tracer events: phase:start (plan-creation), plan:created (planType:'generate'), phase:end. Memory optimization: clears file.content after embedding into prompts.\n\n`prepareFiles(discoveryResult: DiscoveryResult): Promise<PreparedFile[]>` — reads file contents from disk, silently skips files with read errors, returns PreparedFile array with filePath, relativePath, content.\n\n`buildProjectStructure(files: PreparedFile[]): string` — groups files by directory (relative paths), returns newline-delimited listing with directory headers and indented filenames for bird's-eye context.\n\n`filterExistingFiles(files: PreparedFile[]): Promise<{ filesToProcess, skippedFiles[] }>` — checks sumFileExists() for each file; skipped files still appear in projectStructure for context but not in analysis tasks.\n\n`filterExistingDirectories(allFiles: PreparedFile[], processedFiles: PreparedFile[]): Promise<{ dirsToProcess: Set<string>, skippedDirs[] }>` — propagates dirty state from processed files up to ancestors via markDirtyWithAncestors(); checks isGeneratedAgentsMd() for unprocessed directories; returns only directories needing regeneration.\n\n`createDirectoryTasks(files: PreparedFile[]): AnalysisTask[]` — groups PreparedFile[] by directory, creates one directory task per directory with sumFiles array (constructed as relativePath.sum) and fileCount.\n\n## Update Workflow (are update)\n\n`preparePlan(options?: UpdatePlanOptions): Promise<UpdatePlan>` — change-detection-driven planning using frontmatter hashes. Calls checkPrerequisites() (throws if not git repo), getCurrentCommit(), discoverFiles(), iterates each file to read .sum via readSumFile(), computes currentHash via computeContentHash(), compares with stored content_hash from .sum frontmatter. Files with missing/mismatched hashes marked as 'added' or 'modified' status. Calls cleanupOrphans() for deleted files, getAffectedDirectories() for AGENTS.md regeneration. Emits tracer events: phase:start (update-plan-creation), plan:created (planType:'update'), phase:end. Sets isFirstRun=true if filesToSkip.length===0 and filesToAnalyze.length>0.\n\n`checkPrerequisites(): Promise<void>` — throws Error if not in git repository (required for change detection).\n\n`discoverFiles(): Promise<string[]>` — calls runDiscovery() from ../discovery/run.js, returns relative paths.\n\n`recordFileAnalyzed, removeFileState, recordRun, getLastRun` — no-op methods; frontmatter stores hashes directly in .sum files, not in database.\n\n`isFirstRun(): Promise<boolean>` — calls preparePlan({ dryRun: true }) and checks plan.isFirstRun.\n\n## Shared Task Creation\n\n`createFileTasks(files: PreparedFile[] | FileChange[], variant?: string): Promise<AnalysisTask[]>` — polymorphic: handles both PreparedFile (generation) and FileChange (update) arrays. For each file, reads content (cached for PreparedFile, loaded from disk for FileChange), optionally reads existing .sum via readSumFile(getSumPath(absolutePath, variant)) for incremental context, calls buildFilePrompt({ filePath, content, existingSum?: sumContent.summary, sourceFileSize: content.length, compressionRatio: config.generation.compressionRatio }, debug) to generate prompts, returns AnalysisTask[] with type:'file', filePath, systemPrompt, userPrompt.\n\n## Integration Points\n\n- Imports from ../generation/prompts (buildFilePrompt), ../generation/complexity (analyzeComplexity, ComplexityMetrics), ../generation/writers (sumFileExists, isGeneratedAgentsMd, readSumFile, getSumPath).\n- Imports from ../change-detection (isGitRepo, getCurrentCommit, computeContentHash, FileChange).\n- Imports from ../update (cleanupOrphans, getAffectedDirectories, UpdatePlanOptions, CleanupResult).\n- Imports from ../discovery/run (discoverFiles as runDiscovery).\n- Uses Config type for generation.compressionRatio setting.\n- Accepts optional ITraceWriter for event emission (phase:start, phase:end, plan:created).\n- Accepts optional Logger (defaults to nullLogger) for debug output.\n\n## State & Lifecycle\n\nConstructor stores config, projectRoot, tracer, debug flag, logger. close() is no-op (no database). Frontmatter-based hash storage in .sum files eliminates need for persistent state tracking between runs.\n\n## Debug Tracing\n\nWhen debug=true, logs via logger.debug() with [debug] prefix at: file preparation, .sum check, complexity analysis, AGENTS.md check, plan summary. Tracer emits structured events throughout plan creation (phase:start, plan:created, phase:end with durationMs, tasksCompleted, tasksFailed).\n### plan-tracker.ts\n**Purpose:** PlanTracker maintains in-memory markdown content and serializes disk writes to update GENERATION-PLAN.md checkboxes d...\n\n**PlanTracker maintains in-memory markdown content and serializes disk writes to update GENERATION-PLAN.md checkboxes during artifact generation.**\n\n## Exports\n\n`PlanTracker` class: Constructor takes `projectRoot: string, initialMarkdown: string`. Methods: `initialize(): Promise<void>` writes initial plan to disk; `markDone(itemPath: string): void` replaces `- [ ] \\`${itemPath}\\`` with `- [x] \\`${itemPath}\\`` in memory and queues serialized write; `flush(): Promise<void>` awaits completion of all queued writes.\n\n## Key Design\n\nUses Promise chain (`writeQueue`) to serialize concurrent `markDone()` calls and prevent file corruption. Disk writes are non-critical (caught silently); generation proceeds if file I/O fails. In-memory content updates are synchronous; disk serialization is asynchronous.\n\n## Path Convention\n\n`markDone()` expects exact markdown checkbox paths: files as `src/cli/init.ts`, directories as `src/cli/AGENTS.md` (caller appends `/AGENTS.md`), pointers as `CLAUDE.md`. Caller is responsible for formatting; `markDone()` performs literal string replacement on `- [ ] \\`${itemPath}\\``.\n\n## File Location\n\nPlan file location: `${projectRoot}/${CONFIG_DIR}/GENERATION-PLAN.md` where `CONFIG_DIR` imported from `../config/loader.js`.\n### pool.ts\n**Purpose:** pool.ts exports a shared-iterator-based concurrency limiter (`runPool`, `PoolOptions`, `TaskResult`) that executes as...\n\n**pool.ts exports a shared-iterator-based concurrency limiter (`runPool`, `PoolOptions`, `TaskResult`) that executes async task factories with bounded parallelism without batching idle periods.**\n\n## Exports\n\n`runPool<T>(tasks: Array<() => Promise<T>>, options: PoolOptions, onComplete?: (result: TaskResult<T>) => void): Promise<TaskResult<T>[]>` — executes task array through concurrency-limited pool using shared iterator; workers pull from same iterator so each task runs exactly once; returns sparse array indexed by original task position.\n\n`PoolOptions` — interface: `concurrency: number` (max concurrent workers), `failFast?: boolean` (abort on first error), `tracer?: ITraceWriter`, `phaseLabel?: string`, `taskLabels?: string[]`.\n\n`TaskResult<T>` — interface: `index: number`, `success: boolean`, `value?: T`, `error?: Error`.\n\n## Pattern & Architecture\n\nShared-iterator pattern: all workers iterate `tasks.entries()` concurrently, each pulling one `[index, task]` pair. Eliminates batch anti-pattern (Promise.all chunks) by keeping worker slots continuously filled as tasks complete. Requires zero external dependencies.\n\n## Concurrency Control\n\nEffective concurrency = `Math.min(options.concurrency, tasks.length)`. Abort flag `aborted` stops new task pulls on `failFast: true` or completion. `activeTasks` counter tracks in-flight work for tracing snapshots.\n\n## Error Handling\n\nCatch block normalizes non-Error throws via `err instanceof Error ? err : new Error(String(err))`. Result array indexed by task position remains sparse if aborted (results[index] assigned only on completion).\n\n## Tracing Events\n\nTracer emits: `worker:start` (workerId, phase), `task:pickup` (workerId, taskIndex, taskLabel, activeTasks), `task:done` (workerId, taskIndex, taskLabel, durationMs, success, error message, activeTasks), `worker:end` (workerId, phase, tasksExecuted). Task labels default to `task-${index}` if `taskLabels` not provided.\n### progress.ts\n**Purpose:** progress.ts streams build-log progress to stdout and optional .agents-reverse-engineer/progress.log with ETA calculat...\n\n**progress.ts streams build-log progress to stdout and optional .agents-reverse-engineer/progress.log with ETA calculation via moving averages.**\n\n## Exports\n\n**ProgressLog** — Plain-text progress log file writer that mirrors console output to `.agents-reverse-engineer/progress.log` without ANSI codes for `tail -f` monitoring. Serializes concurrent writes via promise-chain queue. `static create(projectRoot: string): ProgressLog` creates instance for project root; `write(line: string): void` appends lines; `finalize(): Promise<void>` flushes and closes file handle.\n\n**ProgressReporter** — Streaming progress reporter for file and directory analysis. Constructor: `new ProgressReporter(totalFiles: number, totalDirectories?: number, progressLog?: ProgressLog)`. Methods:\n- `onFileStart(filePath: string): void` — logs `[X/Y] ANALYZING path`\n- `onFileDone(filePath: string, durationMs: number, tokensIn: number, tokensOut: number, model: string, cacheReadTokens?: number, cacheCreationTokens?: number): void` — logs `[X/Y] DONE path Xs in/out tok model ~ETA`\n- `onFileError(filePath: string, error: string): void` — logs `[X/Y] FAIL path error`\n- `onDirectoryStart(dirPath: string): void` — logs `[dir X/Y] ANALYZING dirPath/AGENTS.md`\n- `onDirectoryDone(dirPath: string, durationMs: number, tokensIn: number, tokensOut: number, model: string, cacheReadTokens?: number, cacheCreationTokens?: number): void` — logs `[dir X/Y] DONE dirPath/AGENTS.md Xs in/out tok model ~ETA`\n- `printSummary(summary: RunSummary): void` — outputs end-of-run statistics\n\n## Key Design Patterns\n\n**Moving-average ETA**: `completionTimes` and `dirCompletionTimes` sliding windows (max 10 elements) track recent durations; ETA calculated when ≥2 completions exist; formatted as seconds if <60, else as `Xm Ys remaining`.\n\n**Token accounting**: Total input = `tokensIn + cacheReadTokens + cacheCreationTokens` (cache components tracked separately in summary).\n\n**Promise-chain serialization**: ProgressLog uses `writeQueue: Promise<void>` to serialize file writes from concurrent workers (identical pattern to TraceWriter).\n\n**ANSI stripping**: `stripAnsi(str)` removes escape sequences matching `/\\x1b\\[[0-9;]*m/g` for plain-text log output.\n\n**File initialization**: ProgressLog parent directory created and file opened (truncate mode `'w'`) on first `write()` call; write errors silently swallowed (non-critical telemetry).\n\n## Dependencies\n\n- `node:fs/promises` (open, mkdir, FileHandle)\n- `node:path` (join, dirname)\n- `picocolors` (pc — colored console output: cyan for ANALYZING, green/blue for DONE, red for FAIL, dim for metadata)\n- `./types.js` (RunSummary — aggregated run statistics)\n\n## Output Formats\n\nFile analysis: `[X/Y] ANALYZING path` | `[X/Y] DONE path Xs in/out tok model [~ETA]` | `[X/Y] FAIL path error`\n\nDirectory analysis: `[dir X/Y] ANALYZING dirPath/AGENTS.md` | `[dir X/Y] DONE dirPath/AGENTS.md Xs in/out tok model [~ETA]`\n\nSummary: `=== Run Summary ===` followed by lines for version, files/dirs processed/failed/skipped, total calls, token counts (input/output/cache), files read (total/unique), elapsed time, errors, retries.\n\n## Constants\n\n**PROGRESS_LOG_FILENAME** = `'progress.log'` — filename relative to `.agents-reverse-engineer/` directory.\n### runner.ts\n**Purpose:** CommandRunner orchestrates two-phase AI-driven documentation generation: concurrent file analysis producing .sum file...\n\n**CommandRunner orchestrates two-phase AI-driven documentation generation: concurrent file analysis producing .sum files, then post-order directory aggregation generating AGENTS.md and CLAUDE.md.**\n\n## Exports\n\n`CommandRunner` class with constructor `(aiService: AIService, options: CommandRunOptions)` holding AIService reference, execution options, tracer, and variant field.\n\n`executeGenerate(plan: ExecutionPlan, options?: { skippedFiles?: number; skippedDirs?: number }): Promise<RunSummary>` runs three sub-phases: pre-phase-1-cache (reads old .sum files via `readSumFile`, `getSumPath`), phase-1-files (concurrent file analysis calling `aiService.call()` with `userPrompt`/`systemPrompt`, writes outputs via `writeSumFile`, `writeAnnexFile`), post-phase-1-quality (inconsistency detection via `checkCodeVsDoc`, `checkCodeVsCode`, `buildInconsistencyReport`, `formatReportForCli`), phase-2-dirs (post-order by depth using `buildDirectoryPrompt`, `writeAgentsMd`, `writeAgentsMdHub`, `writeClaudeMdPointer`), post-phase-2 phantom validation via `checkPhantomPaths`.\n\n`executeUpdate(fileTasks: AnalysisTask[], projectRoot: string, config: Config): Promise<RunSummary>` runs Phase 1 only (file analysis) for changed files; does not regenerate AGENTS.md or root docs.\n\nHelper `stripPreamble(responseText: string): string` removes LLM preamble before `\\n---\\n` separator (if at offset <500) or text before opening bold marker `**[A-Z]` (if preceding text <300 chars and lacks `##`).\n\nHelper `extractPurpose(responseText: string): string` extracts single-line purpose by skipping lines matching `PREAMBLE_PREFIXES` (array: 'now i', 'perfect', 'based on', 'let me', 'here is', 'i\\'ll', 'i will', 'great', 'okay', 'sure', 'certainly', 'alright'), markdown headers (`#`), or `---`, stripping bold wrapper, truncating >120 chars to 117 + '...'.\n\n## Concurrency & Pooling\n\nRunner delegates task execution to `runPool(tasks, { concurrency, failFast, tracer, phaseLabel, taskLabels }, resultCallback)` with configurable parallelism: phase-1-files uses `options.concurrency`, pre-phase-1-cache throttles to 20 descriptors, post-phase-1-quality (directory checks) to 10, phase-2 per-depth to min(concurrency, dirCount).\n\nTracer emits `{ type: 'phase:start'|'phase:end', phase, taskCount?, concurrency?, durationMs?, tasksCompleted?, tasksFailed? }` for observability.\n\n## State Management & Caching\n\n`oldSumCache: Map<string, SumFileContent>` caches pre-existing .sum files for stale-doc detection via `checkCodeVsDoc(sourceContent, oldSum, filePath)`.\n\n`sourceContentCache: Map<string, string>` stores Phase 1 file reads to avoid redundant I/O in post-phase-1-quality; cleared after inconsistency detection.\n\n`updateSourceCache: Map<string, string>` analogous cache for executeUpdate.\n\n`planTracker: PlanTracker` tracks progress via `planTracker.markDone(path)` and `planTracker.flush()`, writing GENERATION-PLAN.md with checkboxes.\n\n## Phase 2 (Directory Aggregation) Ordering\n\nGroups directory tasks by `metadata.depth`, processes depth levels in descending order (highest depth first) ensuring post-order traversal (children before parents). Constructs `knownDirs: Set<string>` from plan for `buildDirectoryPrompt` filtering.\n\n## Output & Metrics\n\nReturns `RunSummary` containing: filesProcessed, filesFailed, filesSkipped, dirsProcessed, dirsFailed, dirsSkipped, totalCalls, totalInputTokens, totalOutputTokens, totalCacheReadTokens, totalCacheCreationTokens, totalDurationMs, errorCount, retryCount, totalFilesRead, uniqueFilesRead, inconsistenciesCodeVsDoc, inconsistenciesCodeVsCode, phantomPaths, inconsistencyReport.\n\n`ProgressReporter` instance logs via `onFileStart()`, `onFileDone(path, durationMs, tokensIn, tokensOut, model, cacheReadTokens, cacheCreationTokens)`, `onFileError()`, `onDirectoryStart()`, `onDirectoryDone()`, `printSummary(summary)`.\n\n## Annex & Variant Support\n\nDetects reproduction-critical constants by checking if cleaned text includes `'## Annex References'`, triggering `writeAnnexFile(absolutePath, sourceContent, variant)`.\n\nVariant mode (eval/A-B testing) supported via optional `variant: string` parameter; affects .sum path via `getSumPath(path, variant)`, AGENTS.md filename (`AGENTS.${variant}.md`), and triggers `writeAgentsMdHub(dirPath, variant)` after `writeAgentsMd()`.\n\n## Quality & Error Handling\n\nNon-throwing inconsistency detection and phantom validation wrap in try-catch, logging errors and continuing pipeline. Groups files by directory for quality checks to throttle I/O. Scales inconsistency detection to 10 concurrent directory checks.\n### trace.ts\n**Purpose:** trace.ts provides an append-only NDJSON tracing system for task/subprocess lifecycle debugging, with zero-overhead no...\n\n**trace.ts provides an append-only NDJSON tracing system for task/subprocess lifecycle debugging, with zero-overhead no-op mode when disabled.**\n\n## Exports\n\n`ITraceWriter` — interface for trace event emission with `emit(event: TraceEventPayload): void`, `finalize(): Promise<void>`, and read-only `filePath: string`.\n\n`TraceEvent` — discriminated union of 14 event types: `PhaseStartEvent`, `PhaseEndEvent`, `WorkerStartEvent`, `WorkerEndEvent`, `TaskPickupEvent`, `TaskDoneEvent`, `TaskStartEvent`, `SubprocessSpawnEvent`, `SubprocessExitEvent`, `RetryEvent`, `DiscoveryStartEvent`, `DiscoveryEndEvent`, `FilterAppliedEvent`, `PlanCreatedEvent`, `ConfigLoadedEvent`.\n\n`TraceEventPayload` — `TraceEvent` with auto-populated fields (`seq`, `ts`, `pid`, `elapsedMs`) omitted.\n\n`createTraceWriter(projectRoot: string, enabled: boolean): ITraceWriter` — factory returning `NullTraceWriter` (zero overhead) when disabled, else `TraceWriter` appending to `.agents-reverse-engineer/traces/trace-{ISO8601-timestamp}.ndjson`.\n\n`cleanupOldTraces(projectRoot: string, keepCount: number = 500): Promise<number>` — deletes oldest trace files beyond retention limit, returns count deleted, silently handles missing directory.\n\n## Architecture\n\n`TraceWriter` uses promise-chain serialization (`this.writeQueue`) to serialize concurrent writes from pool workers into correct NDJSON order; mirrors `PlanTracker` pattern. `NullTraceWriter` provides zero-cost abstraction (empty methods) when `--trace` flag absent. `TraceEventBase` auto-populates `seq` (monotonic per-run), `ts` (ISO 8601), `pid` (parent Node.js process), `elapsedMs` (high-resolution elapsed time since startup via `process.hrtime.bigint()`).\n\n## Behavioral Contracts\n\nTrace filename format: `trace-{ISO8601-timestamp}.ndjson` with colons and periods replaced by hyphens (e.g., `trace-2024-12-14T10-30-45-123Z.ndjson`). Each event serialized as single JSON line (`JSON.stringify(event) + '\\n'`). Directory created recursively if missing. Write errors silently ignored (trace loss acceptable). Timestamps use `new Date().toISOString()`. Cleanup sorts filenames lexicographically (ISO timestamps sort correctly) and keeps `keepCount` most recent entries.\n### types.ts\n**Purpose:** Defines shared type interfaces for orchestration subsystem: task results, run summaries, progress events, and command...\n\n**Defines shared type interfaces for orchestration subsystem: task results, run summaries, progress events, and command options.**\n\n## Exported Types\n\n**FileTaskResult** — per-file AI analysis outcome with path, success flag, token counts (tokensIn, tokensOut, cacheReadTokens, cacheCreationTokens), durationMs, model, optional error.\n\n**RunSummary** — aggregated command execution stats: version, filesProcessed/filesFailed/filesSkipped, totalCalls, totalInputTokens/totalOutputTokens/totalCacheReadTokens/totalCacheCreationTokens, totalDurationMs, errorCount, retryCount, totalFilesRead, uniqueFilesRead, inconsistenciesCodeVsDoc, inconsistenciesCodeVsCode, dirsProcessed/dirsFailed/dirsSkipped, phantomPaths, inconsistencyReport: InconsistencyReport.\n\n**ProgressEvent** — emitted task updates with type: 'start'|'done'|'error'|'dir-done', filePath, index, total, optional durationMs, tokensIn, tokensOut, model, error; event-specific fields populate selectively (start: filePath/index/total; done: adds durationMs/tokensIn/tokensOut/model; error: adds error; dir-done: filePath only).\n\n**CommandRunOptions** — execution control params: concurrency (number), failFast, debug, dryRun (bool flags), tracer: ITraceWriter, progressLog: ProgressLog, variant (eval model name string).\n\n## Dependencies\n\nImports InconsistencyReport from '../quality/index.js', ProgressLog and ITraceWriter from sibling orchestration modules (progress.js, trace.js).\n\n## Import Map (verified — use these exact paths)\n\norchestrator.ts:\n  ../config/schema.js → Config (type)\n  ../types/index.js → DiscoveryResult (type)\n  ../core/logger.js → Logger (type)\n  ../core/logger.js → nullLogger\n  ../generation/prompts/index.js → buildFilePrompt\n  ../generation/complexity.js → analyzeComplexity\n  ../generation/complexity.js → ComplexityMetrics (type)\n  ../generation/writers/sum.js → sumFileExists\n  ../generation/writers/agents-md.js → isGeneratedAgentsMd\n  ../change-detection/index.js → isGitRepo, getCurrentCommit, computeContentHash, type FileChange\n  ../update/orphan-cleaner.js → cleanupOrphans, getAffectedDirectories\n  ../generation/writers/sum.js → readSumFile, getSumPath\n  ../update/types.js → UpdatePlanOptions, CleanupResult (type)\n  ../discovery/run.js → discoverFiles\n\nplan-tracker.ts:\n  ../config/loader.js → CONFIG_DIR\n\nrunner.ts:\n  ../ai/index.js → AIService (type)\n  ../ai/types.js → AIResponse (type)\n  ../generation/executor.js → ExecutionPlan, ExecutionTask (type)\n  ../generation/writers/sum.js → writeSumFile, readSumFile, writeAnnexFile, getSumPath\n  ../generation/writers/sum.js → SumFileContent (type)\n  ../generation/writers/agents-md.js → writeAgentsMd, writeAgentsMdHub\n  ../generation/writers/claude-md.js → writeClaudeMdPointer\n  ../change-detection/index.js → computeContentHashFromString\n  ../generation/prompts/index.js → buildDirectoryPrompt\n  ../config/schema.js → Config (type)\n  ../quality/index.js → checkCodeVsDoc, checkCodeVsCode, checkPhantomPaths, buildInconsistencyReport, formatReportForCli\n  ../quality/index.js → Inconsistency (type)\n  ../generation/executor.js → formatExecutionPlanAsMarkdown\n  ../version.js → getVersion\n\ntypes.ts:\n  ../quality/index.js → InconsistencyReport (type)\n\n## Project Directory Structure\n\n<project-structure>\n./\n  .npmrc\n  CONTRIBUTING.md\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  pages.yml\n  publish.yml\nhooks/\n  are-check-update.js\n  are-context-loader.js\n  opencode-are-check-update.js\nscripts/\n  analyze-agents-compression.sh\n  analyze-sum-ratios.sh\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  codex.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/providers/\n  subprocess.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  rebuild.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/core/\n  index.ts\n  logger.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  claude-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  orchestrator.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/rebuild/\n  checkpoint.ts\n  index.ts\n  orchestrator.ts\n  output-parser.ts\n  prompts.ts\n  spec-reader.ts\n  types.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>",
      "systemPrompt": "You are generating an AGENTS.md file — a directory-level overview for AI coding assistants.\n\nCRITICAL: Output ONLY the raw markdown content. No code fences, no preamble, no explanations, no conversational text. Your entire response IS the AGENTS.md file content.\n\nOUTPUT FORMAT:\n- First line MUST be exactly: <!-- Generated by agents-reverse-engineer -->\n- Use a # heading with the directory name\n- Write a one-paragraph purpose statement for the directory\nusage\nADAPTIVE SECTIONS:\nAnalyze the directory contents and choose the most relevant sections. Do NOT use a fixed template. Instead, select sections that best document this specific directory for an AI that needs to reconstruct or extend the project.\n\nConsider these section types (choose what applies):\n- **Contents**: Group files by purpose/category under ## headings. For each file: markdown link [filename](./filename) and a one-line description.\n- **Subdirectories**: If subdirectories exist, list them with links [dirname/](./dirname/) and brief summaries.\n- **Architecture / Data Flow**: If files form a pipeline, request/response chain, or layered architecture, document it.\n- **Stack**: If this is a package root (has package.json, Cargo.toml, go.mod, etc.), document the technology stack, key scripts, and entry points.\n- **Structure**: If the directory layout follows a convention (feature-sliced, domain-driven, MVC, etc.), document it.\n- **Patterns**: If files share recurring design patterns (factory, strategy, middleware, barrel re-export), name and document them.\n- **Configuration**: If the directory contains config files, schemas, or environment definitions, document the config surface area.\n- **API Surface**: If the directory exports a public API (barrel index, route definitions, SDK), document the interface contract.\n- **File Relationships**: How files collaborate, depend on each other, or share state.\n- **Behavioral Contracts**: If files contain regex patterns, format specifications, magic constants, or template strings that define observable behavior, collect them in a dedicated section. Preserve verbatim patterns from file summaries — do NOT paraphrase regex into prose. This section is MANDATORY when file summaries contain behavioral artifacts.\n- **Reproduction-Critical Constants**: If file summaries reference annex files (via ## Annex References sections), list them with links. Example: \"Full prompt template text: [templates.annex.sum](./prompts/templates.annex.sum)\". Do NOT reproduce annex content in AGENTS.md — just link to it.\n- **Workflow & Conventions**: If file summaries contain contribution guidelines, PR conventions, commit standards, testing mandates, tool usage rules, AI agent instructions, or code conventions (naming, formatting, linting), collect them into actionable directives. Source from CONTRIBUTING.md, CI configs, PR templates, linter/formatter configs, README dev sections. Present as concrete rules (\"commits must use conventionalcommits format\", \"use camelCase for functions, PascalCase for types\") not vague descriptions (\"the project has guidelines\").\n\nChoose any relevant sections or create your own based on the directory contents. The goal is to provide a comprehensive overview that captures the essence of the directory's role in the project and how its files work together, with a focus on what an AI coding assistant would need to know to effectively interact with this code.\n\nSCOPE:\n- AGENTS.md is a NAVIGATIONAL INDEX — help an AI find the right file quickly\n- Focus on: what each file does, how files relate, directory-level patterns\n- Do NOT reproduce full architecture sections — those belong in the root CLAUDE.md\n\nPATH ACCURACY (MANDATORY):\n- When referencing files or modules outside this directory, use ONLY paths from the \"Import Map\" section\n- Do NOT invent, rename, or guess module paths — if a path isn't in the Import Map, don't reference it\n- Use the exact directory names from \"Project Directory Structure\" — do NOT rename directories\n  (e.g., if the directory is called \"cli\", write \"src/cli/\", NOT \"src/commands/\")\n- Cross-module references must use the specifier format from actual import statements\n  (e.g., \"../generation/writers/sum.js\", NOT \"../fs/sum-file.js\")\n- If you are unsure about a path, omit the cross-reference rather than guessing\n\nCONSISTENCY (MANDATORY):\n- Do not contradict yourself within the same document\n- If you describe a technique (e.g., \"regex-based\"), do not call it something else later (e.g., \"AST-based\")\n- When stating version numbers, engines, or config fields, use ONLY values present in the file summaries\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this directory\", \"this module\", \"provides\", \"responsible for\", \"is used to\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Per-file descriptions in Contents sections: 1-2 sentences maximum. Reference key symbols but do not reproduce full summaries.\n- Behavioral contracts (regex patterns, format specs, constants) belong in a separate Behavioral Contracts section, not in per-file descriptions.\n- Annex file references: link to .annex.sum files, do not inline their content. One line per annex reference.\n- Subdirectory descriptions: 1-2 sentences maximum. Capture the directory's role, not its full contents.\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- Key exported symbols from file summaries MUST appear in the directory overview\n- Preserve exact casing of identifiers\n\nUSER NOTES:\n- If \"User Notes\" are provided in the prompt, they contain user-defined instructions that will be automatically prepended to your output\n- Do NOT repeat or paraphrase user notes in your generated content — they are included separately\n- You may reference information from user notes for context",
      "response": "<!-- Generated by agents-reverse-engineer -->\n\n# src/orchestration\n\nThe orchestration module coordinates AI-driven documentation generation and incremental updates across the entire project. It manages task scheduling, progress reporting, plan tracking, concurrent execution, and lifecycle tracing for both full-project generation (are generate) and change-detection-driven updates (are update) workflows.\n\n## Contents\n\n**Core Exports**\n- [index.ts](./index.ts) — Barrel export aggregating CommandRunner, ProgressReporter, runPool, PlanTracker, trace utilities (createTraceWriter, cleanupOldTraces) and shared types (FileTaskResult, RunSummary, ProgressEvent, CommandRunOptions, PoolOptions, TaskResult, ITraceWriter, TraceEvent, TraceEventPayload).\n\n**Orchestration & Planning**\n- [orchestrator.ts](./orchestrator.ts) — DocumentationOrchestrator unifies generation (createPlan, prepareFiles, buildProjectStructure, filterExistingFiles, filterExistingDirectories, createDirectoryTasks) and update (preparePlan, checkPrerequisites, discoverFiles, recordFileAnalyzed, removeFileState, recordRun, getLastRun, isFirstRun) workflows; createFileTasks handles both PreparedFile and FileChange arrays polymorphically; emits tracer events (phase:start, phase:end, plan:created) throughout execution.\n- [plan-tracker.ts](./plan-tracker.ts) — PlanTracker maintains in-memory markdown and serializes disk writes to GENERATION-PLAN.md via promise-chain queue; markDone replaces checkbox strings (`- [ ] \\`path\\`` → `- [x] \\`path\\``); flush awaits completion; writes to `${projectRoot}/${CONFIG_DIR}/GENERATION-PLAN.md`.\n\n**Execution & Concurrency**\n- [runner.ts](./runner.ts) — CommandRunner orchestrates two-phase AI generation: Phase 1 (concurrent file analysis via runPool calling aiService.call, writing .sum and annex files) with post-phase-1 quality checks (checkCodeVsDoc, checkCodeVsCode, checkPhantomPaths); Phase 2 (post-order directory aggregation via buildDirectoryPrompt, writeAgentsMd, writeAgentsMdHub, writeClaudeMdPointer); executeGenerate handles full workflow, executeUpdate processes changed files only; caches old .sum files (oldSumCache) and Phase 1 source reads (sourceContentCache) for stale-doc detection and quality checks; returns RunSummary with token counts, duration, error metrics, inconsistency counts, and phantom paths.\n- [pool.ts](./pool.ts) — runPool executes async task factories with bounded concurrency using shared iterator; all workers pull from same iterator so tasks run exactly once; sparse result array indexed by original position; supports failFast abort and tracer events (worker:start, task:pickup, task:done, worker:end); no external dependencies.\n\n**Progress & Observability**\n- [progress.ts](./progress.ts) — ProgressLog streams analysis output to `.agents-reverse-engineer/progress.log` without ANSI codes for `tail -f` monitoring via promise-chain serialization; ProgressReporter logs file/directory analysis with ETA calculation via moving-average windows (onFileStart, onFileDone, onFileError, onDirectoryStart, onDirectoryDone, printSummary); formats `[X/Y] ANALYZING`, `[X/Y] DONE Xs in/out tok model ~ETA`, `[X/Y] FAIL` messages via picocolors.\n- [trace.ts](./trace.ts) — ITraceWriter appends NDJSON events to `.agents-reverse-engineer/traces/trace-{ISO8601-timestamp}.ndjson` with promise-chain serialization; TraceEvent discriminated union covers PhaseStartEvent, PhaseEndEvent, WorkerStartEvent, WorkerEndEvent, TaskPickupEvent, TaskDoneEvent, TaskStartEvent, SubprocessSpawnEvent, SubprocessExitEvent, RetryEvent, DiscoveryStartEvent, DiscoveryEndEvent, FilterAppliedEvent, PlanCreatedEvent, ConfigLoadedEvent; createTraceWriter returns NullTraceWriter (zero-cost) when disabled; cleanupOldTraces deletes oldest files beyond retention limit.\n\n**Type Definitions**\n- [types.ts](./types.ts) — FileTaskResult (path, success, tokensIn, tokensOut, cacheReadTokens, cacheCreationTokens, durationMs, model, optional error); RunSummary (version, filesProcessed/filesFailed/filesSkipped, dirsProcessed/dirsFailed/dirsSkipped, totalCalls, totalInputTokens, totalOutputTokens, totalCacheReadTokens, totalCacheCreationTokens, totalDurationMs, errorCount, retryCount, totalFilesRead, uniqueFilesRead, inconsistenciesCodeVsDoc, inconsistenciesCodeVsCode, phantomPaths, inconsistencyReport); ProgressEvent (type: 'start'|'done'|'error'|'dir-done' with event-specific fields); CommandRunOptions (concurrency, failFast, debug, dryRun, tracer, progressLog, variant).\n\n## Execution Workflows\n\n**Generation (are generate)**\n1. DocumentationOrchestrator.createPlan: prepareFiles → filterExistingFiles → analyzeComplexity → buildProjectStructure → createFileTasks → filterExistingDirectories → createDirectoryTasks; emits phase:start, plan:created, phase:end.\n2. CommandRunner.executeGenerate Phase 1: reads oldSumCache (readSumFile, getSumPath) → concurrent file analysis via runPool (aiService.call) → writes .sum and annex files (writeSumFile, writeAnnexFile) → post-phase-1-quality directory checks (checkCodeVsDoc, checkCodeVsCode, buildInconsistencyReport) via runPool (throttled 10 concurrent).\n3. Phase 2: post-order directory aggregation by depth (descending) → buildDirectoryPrompt → writeAgentsMd → writeAgentsMdHub (variant mode) → writeClaudeMdPointer; final validation checkPhantomPaths.\n\n**Update (are update)**\n1. DocumentationOrchestrator.preparePlan: checkPrerequisites (throws if not git repo) → discoverFiles → iterate each file, read .sum frontmatter hash (readSumFile) → computeContentHash → compare with stored content_hash → mark 'added'/'modified'; cleanupOrphans for deleted files; getAffectedDirectories for AGENTS.md regeneration; sets isFirstRun if all files added.\n2. CommandRunner.executeUpdate: Phase 1 only (file analysis) for changed files (filesToAnalyze); skips Phase 2 (AGENTS.md/CLAUDE.md regeneration).\n\n## Concurrency & Scaling\n\n- **Phase 1**: runPool with configurable concurrency; pre-phase-1-cache throttled to 20 concurrent file descriptor opens; post-phase-1-quality directory checks throttled to 10.\n- **Phase 2**: grouped by metadata.depth, each depth level processed with min(concurrency, dirCount) workers; post-order traversal ensures children process before parents.\n- **Serialization**: ProgressLog, PlanTracker, TraceWriter all use promise-chain writeQueue to serialize concurrent writes and maintain output/file order.\n\n## State & Caching\n\n- **CommandRunner**: oldSumCache (pre-existing .sum files for stale-doc detection), sourceContentCache (Phase 1 file reads to avoid I/O in post-phase-1-quality), planTracker (GENERATION-PLAN.md progress), progressReporter (moving-average ETA calculation).\n- **DocumentationOrchestrator**: frontmatter-based hash storage in .sum files (no persistent database); isFirstRun derived from plan metrics.\n- **ProgressReporter**: completionTimes and dirCompletionTimes sliding windows (max 10) for ETA estimation.\n\n## Behavioral Contracts\n\n- **Trace filename format**: `trace-{ISO8601-timestamp}.ndjson` with colons and periods replaced by hyphens (e.g., `trace-2024-12-14T10-30-45-123Z.ndjson`).\n- **NDJSON event serialization**: Each TraceEvent written as single JSON line (`JSON.stringify(event) + '\\n'`) with auto-populated seq, ts, pid, elapsedMs fields.\n- **Checkbox format**: PlanTracker.markDone performs literal string replacement on `- [ ] \\`${itemPath}\\`` (files: `src/cli/init.ts`, directories: `src/cli/AGENTS.md`, pointers: `CLAUDE.md`).\n- **Progress log format**: `[X/Y] ANALYZING path`, `[X/Y] DONE path Xs in/out tok model [~ETA]`, `[X/Y] FAIL path error`; directory: `[dir X/Y] ANALYZING dirPath/AGENTS.md`.\n- **ETA calculation**: moving-average window (≥2 completions required); formatted as seconds if <60, else as `Xm Ys remaining`.\n- **Token accounting**: total input = tokensIn + cacheReadTokens + cacheCreationTokens; tracked separately in RunSummary.\n- **Variant mode**: optional variant parameter affects .sum path (getSumPath), AGENTS.md filename (AGENTS.${variant}.md), triggers writeAgentsMdHub after writeAgentsMd.\n- **Preamble stripping**: stripPreamble removes text before `\\n---\\n` (if offset <500) or before first bold header `**[A-Z]` (if preceding text <300 chars and lacks `##`).\n- **Purpose extraction**: extractPurpose skips preamble lines (now i, perfect, based on, let me, here is, i'll, i will, great, okay, sure, certainly, alright), markdown headers, dashes; strips bold wrapper; truncates >120 chars to 117 + '...'.\n- **Content hash format**: computeContentHash and computeContentHashFromString produce hex digests; stored in .sum frontmatter as `content_hash` field for comparison with preparePlan.\n- **ANSI stripping**: ProgressLog.stripAnsi removes `/\\x1b\\[[0-9;]*m/g` escape sequences for plain-text output.\n- **Trace cleanup**: sorts by lexicographic filename (ISO timestamps sort correctly); keeps `keepCount` most recent entries; silently handles missing directory.\n\n## Integration Points\n\n- **AI Service**: CommandRunner.executeGenerate calls aiService.call(systemPrompt, userPrompt) → AIResponse with response, tokensIn, tokensOut, cacheReadTokens, cacheCreationTokens, model.\n- **File Analysis**: buildFilePrompt (../generation/prompts) generates prompts with existingSum (from readSumFile), sourceFileSize, compressionRatio; analyzeComplexity produces ComplexityMetrics.\n- **Directory Aggregation**: buildDirectoryPrompt accepts knownDirs Set for filtering; variant mode triggers writeAgentsMdHub.\n- **Quality Checks**: checkCodeVsDoc, checkCodeVsCode, checkPhantomPaths, buildInconsistencyReport, formatReportForCli from ../quality.\n- **Change Detection**: isGitRepo, getCurrentCommit, computeContentHash, FileChange from ../change-detection; computeContentHashFromString for .sum frontmatter comparison.\n- **Update Lifecycle**: cleanupOrphans, getAffectedDirectories from ../update; UpdatePlanOptions, CleanupResult types.\n- **Discovery**: discoverFiles as runDiscovery from ../discovery/run.\n- **Config**: Config.generation.compressionRatio setting; CONFIG_DIR from ../config/loader for GENERATION-PLAN.md path.\n- **Logging**: Logger type; nullLogger default; debug flag triggers logger.debug([debug] prefix) output.\n- **Version**: getVersion from ../version for RunSummary.version field.\n\n## Variant & Annex Support\n\n- **Variant Mode**: optional CommandRunOptions.variant parameter (eval model name string) routes .sum files via getSumPath(path, variant), renames AGENTS.md to AGENTS.${variant}.md, triggers writeAgentsMdHub after writeAgentsMd.\n- **Annex Detection**: stripPreamble checks if cleaned text includes `'## Annex References'` to trigger writeAnnexFile(absolutePath, sourceContent, variant) for reproduction-critical constants.",
      "model": "claude-haiku-4-5-20251001",
      "inputTokens": 3,
      "outputTokens": 3018,
      "cacheReadTokens": 17512,
      "cacheCreationTokens": 8967,
      "latencyMs": 58359,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": []
    },
    {
      "timestamp": "2026-02-14T13:50:22.800Z",
      "prompt": "Generate AGENTS.md for directory: \"scripts\" (scripts)\n\n## File Summaries (3 files)\n\n### analyze-agents-compression.sh\n**Purpose:** analyze-agents-compression.sh measures compression efficiency of AGENTS.md aggregation by computing input→output rati...\n\n**analyze-agents-compression.sh measures compression efficiency of AGENTS.md aggregation by computing input→output ratios across the documentation tree.**\n\n## Role\nAudit script for the Agents Reverse Engineer (ARE) pipeline. Validates that AGENTS.md files effectively compress their input sources (.sum files and nested AGENTS.md from subdirectories).\n\n## Output Metrics\nComputes per-directory ratio = (AGENTS.md size / input size) × 100%, then aggregates:\n- Individual compression ratios for each AGENTS.md\n- Overall aggregation ratio, compression factor, space saved\n- Mean, median, min, max ratios across all files\n- Interpretation: <50% = EXCELLENT (>2x), <70% = GOOD (~1.5x), <90% = ACCEPTABLE (<1.2x), ≥90% = MINIMAL\n\n## Input Calculation\nFor each AGENTS.md at depth N:\n- Sums all .sum files in same directory (maxdepth 1)\n- Sums AGENTS.md from immediate subdirectories (maxdepth 1)\n- Root AGENTS.md processed separately, excluding hidden subdirs (grep filter: `!= \"^\\./\\.\"`\n\n## File Discovery\n`find . -name \"AGENTS.md\" -type f | sort` locates all AGENTS.md candidates; root handled specially to avoid double-counting.\n\n## Size Retrieval\nPortable across macOS/Linux: `stat -f%z` (BSD) fallback to `stat -c%s` (GNU); `numfmt --to=iec` for human-readable bytes, fallback to raw bytes.\n\n## Ratio Computation\nUses `awk \"BEGIN {printf \"%.2f\", (output / input) * 100}\"` for 2-decimal percentage; median computed via sorted array with even-count averaging.\n### analyze-sum-ratios.sh\n**Purpose:** analyze-sum-ratios.sh measures compression efficiency of .sum documentation files against source files, reporting agg...\n\n**analyze-sum-ratios.sh measures compression efficiency of .sum documentation files against source files, reporting aggregate statistics and per-file ratios.**\n\n## Purpose\nAnalyzes all .sum files in the codebase to verify they meet the target 10% compression ratio (configured as 25% threshold for acceptance), calculating mean/median/min/max ratios and flagging outliers.\n\n## Execution Model\nBash script; finds .sum files via `find . -name \"*.sum\" -not -path \"*/node_modules/*\" -not -path \"*/.git/*\"`, pairs each with source file (removes .sum extension), computes `(sum_size / source_size) * 100` ratio using `awk`, accumulates totals.\n\n## Key Metrics & Thresholds\n- **Target ratio**: 25% (configured threshold; script aims for 10%)\n- **Outlier display threshold**: ratio > 50% or < 10% (shown regardless of file count limit)\n- **First 10 files**: always displayed\n- **Output columns**: filename, source bytes, .sum bytes, ratio percentage\n- **Pass criteria**: overall_ratio < 25% (PASS); < 30% (ACCEPTABLE); ≥ 30% (ABOVE TARGET)\n\n## Statistics Calculated\n- `file_count`: total .sum files processed\n- `total_source_size`, `total_sum_size`: cumulative byte counts\n- `overall_ratio`: `(total_sum_size / total_source_size) * 100`\n- `mean_ratio`: arithmetic mean of individual file ratios\n- `median`: middle value of sorted ratios array\n- `min_ratio`, `max_ratio`: extremes\n\n## Platform Compatibility\nUses conditional stat syntax: `stat -f%z` (BSD/macOS) fallback `stat -c%s` (GNU/Linux); `numfmt --to=iec-i --suffix=B` for human-readable sizes (graceful fallback to bytes).\n### build-hooks.js\n**Purpose:** build-hooks.js copies .js files from hooks/ to hooks/dist/ for npm bundling, invoked via npm run build:hooks.\n\n**build-hooks.js copies .js files from hooks/ to hooks/dist/ for npm bundling, invoked via npm run build:hooks.**\n\n## Execution Context\nEntry point script (shebang: `#!/usr/bin/env node`). Runs as npm lifecycle hook during `prepublishOnly`. Uses ES modules with `import.meta.url` to compute `__dirname` and `__filename` equivalents; resolves `projectRoot` via `join(__dirname, '..')`.\n\n## Directory Structure & Paths\n- `HOOKS_SRC = join(projectRoot, 'hooks')` — source directory containing hook implementations\n- `HOOKS_DIST = join(projectRoot, 'hooks', 'dist')` — destination directory for npm bundle\n- Creates `HOOKS_DIST` with `mkdirSync(HOOKS_DIST, { recursive: true })` if absent\n\n## Build Algorithm\nFilters `readdirSync(HOOKS_SRC)` by `.js` file extension, excludes `'dist'` folder. Iterates hookFiles, copies each via `copyFileSync(src, dest)`. Logs progress per file and summary count.\n\n## Console Output Format\n```\nBuilding hooks...\n  Copied: [filename] -> hooks/dist/[filename]\nDone. N hook(s) built.\n```\n\n## Project Directory Structure\n\n<project-structure>\n./\n  .npmrc\n  CONTRIBUTING.md\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  pages.yml\n  publish.yml\nhooks/\n  are-check-update.js\n  are-context-loader.js\n  opencode-are-check-update.js\nscripts/\n  analyze-agents-compression.sh\n  analyze-sum-ratios.sh\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  codex.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/providers/\n  subprocess.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  rebuild.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/core/\n  index.ts\n  logger.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  claude-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  orchestrator.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/rebuild/\n  checkpoint.ts\n  index.ts\n  orchestrator.ts\n  output-parser.ts\n  prompts.ts\n  spec-reader.ts\n  types.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>",
      "systemPrompt": "You are generating an AGENTS.md file — a directory-level overview for AI coding assistants.\n\nCRITICAL: Output ONLY the raw markdown content. No code fences, no preamble, no explanations, no conversational text. Your entire response IS the AGENTS.md file content.\n\nOUTPUT FORMAT:\n- First line MUST be exactly: <!-- Generated by agents-reverse-engineer -->\n- Use a # heading with the directory name\n- Write a one-paragraph purpose statement for the directory\nusage\nADAPTIVE SECTIONS:\nAnalyze the directory contents and choose the most relevant sections. Do NOT use a fixed template. Instead, select sections that best document this specific directory for an AI that needs to reconstruct or extend the project.\n\nConsider these section types (choose what applies):\n- **Contents**: Group files by purpose/category under ## headings. For each file: markdown link [filename](./filename) and a one-line description.\n- **Subdirectories**: If subdirectories exist, list them with links [dirname/](./dirname/) and brief summaries.\n- **Architecture / Data Flow**: If files form a pipeline, request/response chain, or layered architecture, document it.\n- **Stack**: If this is a package root (has package.json, Cargo.toml, go.mod, etc.), document the technology stack, key scripts, and entry points.\n- **Structure**: If the directory layout follows a convention (feature-sliced, domain-driven, MVC, etc.), document it.\n- **Patterns**: If files share recurring design patterns (factory, strategy, middleware, barrel re-export), name and document them.\n- **Configuration**: If the directory contains config files, schemas, or environment definitions, document the config surface area.\n- **API Surface**: If the directory exports a public API (barrel index, route definitions, SDK), document the interface contract.\n- **File Relationships**: How files collaborate, depend on each other, or share state.\n- **Behavioral Contracts**: If files contain regex patterns, format specifications, magic constants, or template strings that define observable behavior, collect them in a dedicated section. Preserve verbatim patterns from file summaries — do NOT paraphrase regex into prose. This section is MANDATORY when file summaries contain behavioral artifacts.\n- **Reproduction-Critical Constants**: If file summaries reference annex files (via ## Annex References sections), list them with links. Example: \"Full prompt template text: [templates.annex.sum](./prompts/templates.annex.sum)\". Do NOT reproduce annex content in AGENTS.md — just link to it.\n- **Workflow & Conventions**: If file summaries contain contribution guidelines, PR conventions, commit standards, testing mandates, tool usage rules, AI agent instructions, or code conventions (naming, formatting, linting), collect them into actionable directives. Source from CONTRIBUTING.md, CI configs, PR templates, linter/formatter configs, README dev sections. Present as concrete rules (\"commits must use conventionalcommits format\", \"use camelCase for functions, PascalCase for types\") not vague descriptions (\"the project has guidelines\").\n\nChoose any relevant sections or create your own based on the directory contents. The goal is to provide a comprehensive overview that captures the essence of the directory's role in the project and how its files work together, with a focus on what an AI coding assistant would need to know to effectively interact with this code.\n\nSCOPE:\n- AGENTS.md is a NAVIGATIONAL INDEX — help an AI find the right file quickly\n- Focus on: what each file does, how files relate, directory-level patterns\n- Do NOT reproduce full architecture sections — those belong in the root CLAUDE.md\n\nPATH ACCURACY (MANDATORY):\n- When referencing files or modules outside this directory, use ONLY paths from the \"Import Map\" section\n- Do NOT invent, rename, or guess module paths — if a path isn't in the Import Map, don't reference it\n- Use the exact directory names from \"Project Directory Structure\" — do NOT rename directories\n  (e.g., if the directory is called \"cli\", write \"src/cli/\", NOT \"src/commands/\")\n- Cross-module references must use the specifier format from actual import statements\n  (e.g., \"../generation/writers/sum.js\", NOT \"../fs/sum-file.js\")\n- If you are unsure about a path, omit the cross-reference rather than guessing\n\nCONSISTENCY (MANDATORY):\n- Do not contradict yourself within the same document\n- If you describe a technique (e.g., \"regex-based\"), do not call it something else later (e.g., \"AST-based\")\n- When stating version numbers, engines, or config fields, use ONLY values present in the file summaries\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this directory\", \"this module\", \"provides\", \"responsible for\", \"is used to\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Per-file descriptions in Contents sections: 1-2 sentences maximum. Reference key symbols but do not reproduce full summaries.\n- Behavioral contracts (regex patterns, format specs, constants) belong in a separate Behavioral Contracts section, not in per-file descriptions.\n- Annex file references: link to .annex.sum files, do not inline their content. One line per annex reference.\n- Subdirectory descriptions: 1-2 sentences maximum. Capture the directory's role, not its full contents.\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- Key exported symbols from file summaries MUST appear in the directory overview\n- Preserve exact casing of identifiers\n\nUSER NOTES:\n- If \"User Notes\" are provided in the prompt, they contain user-defined instructions that will be automatically prepended to your output\n- Do NOT repeat or paraphrase user notes in your generated content — they are included separately\n- You may reference information from user notes for context",
      "response": "<!-- Generated by agents-reverse-engineer -->\n\n# scripts\n\nThe scripts directory provides build automation and documentation auditing utilities for the agents-reverse-engineer project, including npm hook preparation, compression analysis for generated documentation, and diagnostic tools for the Agents Reverse Engineer pipeline.\n\n## Contents\n\n### Build & Distribution\n\n[build-hooks.js](./build-hooks.js) — Copies .js files from `hooks/` to `hooks/dist/` for npm bundling; invoked via `prepublishOnly` lifecycle hook. Resolves `projectRoot` via `import.meta.url`, creates `HOOKS_DIST` with `mkdirSync(..., { recursive: true })`, filters source files by `.js` extension excluding `dist`, logs per-file copies and summary count.\n\n### Documentation Auditing\n\n[analyze-agents-compression.sh](./analyze-agents-compression.sh) — Audits AGENTS.md compression efficiency across the documentation tree, computing per-directory ratios (AGENTS.md size / input size) and reporting aggregate statistics. Validates input calculation: sums `.sum` files in same directory plus AGENTS.md from immediate subdirectories (both maxdepth 1). Interprets compression: <50% = EXCELLENT (>2x), <70% = GOOD (~1.5x), <90% = ACCEPTABLE (<1.2x), ≥90% = MINIMAL. Uses portable stat syntax (`stat -f%z` on BSD, `stat -c%s` on GNU) and `awk` for 2-decimal percentage computation.\n\n[analyze-sum-ratios.sh](./analyze-sum-ratios.sh) — Measures .sum file compression against source files, validating target 25% threshold (script aims for 10%). Pairs each .sum file with source via extension removal, computes `(sum_size / source_size) * 100` ratio, reports mean/median/min/max. Flags outliers (ratio >50% or <10%) regardless of file count limit; displays first 10 files by default. Pass criteria: overall_ratio <25% (PASS), <30% (ACCEPTABLE), ≥30% (ABOVE TARGET). Platform-compatible stat and `numfmt --to=iec-i` for human-readable output.",
      "model": "claude-haiku-4-5-20251001",
      "inputTokens": 3,
      "outputTokens": 521,
      "cacheReadTokens": 17512,
      "cacheCreationTokens": 3253,
      "latencyMs": 12679,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": []
    },
    {
      "timestamp": "2026-02-14T13:50:22.809Z",
      "prompt": "Generate AGENTS.md for directory: \"hooks\" (hooks)\n\n## File Summaries (3 files)\n\n### are-check-update.js\n**Purpose:** are-check-update.js spawns background process to check for ARE (agents-reverse-engineer) npm package updates and cach...\n\n**are-check-update.js spawns background process to check for ARE (agents-reverse-engineer) npm package updates and caches result to ~/.claude/cache/are-update-check.json.**\n\n## Execution Model\nRuns as SessionStart hook invoked once per session. Spawns detached child process with `stdio: 'ignore'` and `detached: true` to avoid blocking CLI startup; parent process immediately unrefs child and exits.\n\n## Cache Location & Structure\nCache file: `~/.claude/cache/are-update-check.json`. Cache dir created recursively if missing. Result object contains: `update_available` (boolean), `installed` (version string), `latest` (version string or \"unknown\"), `checked` (Unix timestamp).\n\n## Version Detection Logic\nChecks ARE-VERSION file in project-local `.claude/` directory first, then falls back to global `~/.claude/ARE-VERSION`. Defaults to \"0.0.0\" if neither exists. Spawned child reads these paths via `fs.existsSync()` and `fs.readFileSync()`.\n\n## npm Lookup\nSpawned process runs `npm view agents-reverse-engineer version` with 10-second timeout. Sets `windowsHide: true` to suppress cmd window on Windows. Captures stdout, trims whitespace. Falls back to \"unknown\" on any error (network, timeout, npm not found).\n\n## Directory Structure\nParent creates `~/.claude/cache/` and `~/.claude/cache/npm-cache/` (both with `{ recursive: true }`). Exports none; hook-only entry point.\n### are-context-loader.js\n**Purpose:** are-context-loader.js auto-injects AGENTS.md documentation into Claude's context after Read tool calls, walking the f...\n\n**are-context-loader.js auto-injects AGENTS.md documentation into Claude's context after Read tool calls, walking the file's directory tree upward to collect generated documentation files.**\n\n## Exports & Hooks\n\n`main(data)` — processes PostToolUse hook data; extracts `file_path` from `data.tool_input`, walks ancestry from file directory toward project root, outputs collected AGENTS.md files as JSON to stdout with `additionalContext` for Claude's conversation.\n\n## Deduplication & Session Management\n\nUses session-scoped temp file (`~/.tmp/are-context-loader/{session_id}.json`) keyed by `data.session_id` to track which directories' AGENTS.md have been loaded, preventing redundant context injection across multiple Read calls in the same session.\n\n## Directory Traversal & Filtering\n\nWalks from `path.dirname(filePath)` upward, skipping project root (already loaded via CLAUDE.md), stopping at filesystem boundary; collects only files matching `AGENTS.md` with marker string `'Generated by agents-reverse-engineer'`.\n\n## Output Format\n\nOutputs JSON with structure `{suppressOutput: false, hookSpecificOutput: {hookEventName: 'PostToolUse', additionalContext: string}}` to stdout; reverses collected parts to ensure root-to-leaf ordering (parent context before child).\n\n## Input & Constraints\n\nReads stdin as JSON containing `tool_input.file_path`, `cwd` (project root), `session_id`; silently exits on malformed input or missing `file_path`; returns early if file outside project tree or at project root.\n### opencode-are-check-update.js\n**Purpose:** hooks/opencode-are-check-update.js exports AreCheckUpdate, an OpenCode session hook that spawns a background process ...\n\n**hooks/opencode-are-check-update.js exports AreCheckUpdate, an OpenCode session hook that spawns a background process to check for agents-reverse-engineer updates and cache the result.**\n\n## Exported Interface\n\n- `AreCheckUpdate(ctx)` — async function returning event handler; triggers on `session.created` event type to initiate background update check.\n\n## Cache & Version File Paths\n\n- Cache directory: `~/.config/opencode/cache/`\n- Cache file: `~/.config/opencode/cache/are-update-check.json`\n- npm cache directory: `~/.config/opencode/cache/npm-cache/`\n- Project version file: `.opencode/ARE-VERSION` (checked first)\n- Global version file: `~/.config/opencode/ARE-VERSION` (fallback)\n\n## Background Process Output\n\nSpawned child writes `are-update-check.json` with structure: `{update_available: boolean, installed: string, latest: string, checked: number (unix timestamp)}`. Installed version defaults to `'0.0.0'` if neither version file exists; latest defaults to `'unknown'` if npm registry query fails (10s timeout). Process spawned detached with `stdio: 'ignore'` and `child.unref()` to prevent parent blocking.\n\n## Integration\n\nEvent handler pattern: checks `event.type === 'session.created'` before executing. Uses `spawn(process.execPath)` with inline `-e` script to avoid blocking; inherits `process.env` in child. Filesystem operations use `fs.existsSync()`, `fs.readFileSync()`, `fs.writeFileSync()`, `mkdirSync({recursive: true})`.\n\n## Project Directory Structure\n\n<project-structure>\n./\n  .npmrc\n  CONTRIBUTING.md\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  pages.yml\n  publish.yml\nhooks/\n  are-check-update.js\n  are-context-loader.js\n  opencode-are-check-update.js\nscripts/\n  analyze-agents-compression.sh\n  analyze-sum-ratios.sh\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  codex.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/providers/\n  subprocess.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  rebuild.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/core/\n  index.ts\n  logger.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  claude-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  orchestrator.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/rebuild/\n  checkpoint.ts\n  index.ts\n  orchestrator.ts\n  output-parser.ts\n  prompts.ts\n  spec-reader.ts\n  types.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>",
      "systemPrompt": "You are generating an AGENTS.md file — a directory-level overview for AI coding assistants.\n\nCRITICAL: Output ONLY the raw markdown content. No code fences, no preamble, no explanations, no conversational text. Your entire response IS the AGENTS.md file content.\n\nOUTPUT FORMAT:\n- First line MUST be exactly: <!-- Generated by agents-reverse-engineer -->\n- Use a # heading with the directory name\n- Write a one-paragraph purpose statement for the directory\nusage\nADAPTIVE SECTIONS:\nAnalyze the directory contents and choose the most relevant sections. Do NOT use a fixed template. Instead, select sections that best document this specific directory for an AI that needs to reconstruct or extend the project.\n\nConsider these section types (choose what applies):\n- **Contents**: Group files by purpose/category under ## headings. For each file: markdown link [filename](./filename) and a one-line description.\n- **Subdirectories**: If subdirectories exist, list them with links [dirname/](./dirname/) and brief summaries.\n- **Architecture / Data Flow**: If files form a pipeline, request/response chain, or layered architecture, document it.\n- **Stack**: If this is a package root (has package.json, Cargo.toml, go.mod, etc.), document the technology stack, key scripts, and entry points.\n- **Structure**: If the directory layout follows a convention (feature-sliced, domain-driven, MVC, etc.), document it.\n- **Patterns**: If files share recurring design patterns (factory, strategy, middleware, barrel re-export), name and document them.\n- **Configuration**: If the directory contains config files, schemas, or environment definitions, document the config surface area.\n- **API Surface**: If the directory exports a public API (barrel index, route definitions, SDK), document the interface contract.\n- **File Relationships**: How files collaborate, depend on each other, or share state.\n- **Behavioral Contracts**: If files contain regex patterns, format specifications, magic constants, or template strings that define observable behavior, collect them in a dedicated section. Preserve verbatim patterns from file summaries — do NOT paraphrase regex into prose. This section is MANDATORY when file summaries contain behavioral artifacts.\n- **Reproduction-Critical Constants**: If file summaries reference annex files (via ## Annex References sections), list them with links. Example: \"Full prompt template text: [templates.annex.sum](./prompts/templates.annex.sum)\". Do NOT reproduce annex content in AGENTS.md — just link to it.\n- **Workflow & Conventions**: If file summaries contain contribution guidelines, PR conventions, commit standards, testing mandates, tool usage rules, AI agent instructions, or code conventions (naming, formatting, linting), collect them into actionable directives. Source from CONTRIBUTING.md, CI configs, PR templates, linter/formatter configs, README dev sections. Present as concrete rules (\"commits must use conventionalcommits format\", \"use camelCase for functions, PascalCase for types\") not vague descriptions (\"the project has guidelines\").\n\nChoose any relevant sections or create your own based on the directory contents. The goal is to provide a comprehensive overview that captures the essence of the directory's role in the project and how its files work together, with a focus on what an AI coding assistant would need to know to effectively interact with this code.\n\nSCOPE:\n- AGENTS.md is a NAVIGATIONAL INDEX — help an AI find the right file quickly\n- Focus on: what each file does, how files relate, directory-level patterns\n- Do NOT reproduce full architecture sections — those belong in the root CLAUDE.md\n\nPATH ACCURACY (MANDATORY):\n- When referencing files or modules outside this directory, use ONLY paths from the \"Import Map\" section\n- Do NOT invent, rename, or guess module paths — if a path isn't in the Import Map, don't reference it\n- Use the exact directory names from \"Project Directory Structure\" — do NOT rename directories\n  (e.g., if the directory is called \"cli\", write \"src/cli/\", NOT \"src/commands/\")\n- Cross-module references must use the specifier format from actual import statements\n  (e.g., \"../generation/writers/sum.js\", NOT \"../fs/sum-file.js\")\n- If you are unsure about a path, omit the cross-reference rather than guessing\n\nCONSISTENCY (MANDATORY):\n- Do not contradict yourself within the same document\n- If you describe a technique (e.g., \"regex-based\"), do not call it something else later (e.g., \"AST-based\")\n- When stating version numbers, engines, or config fields, use ONLY values present in the file summaries\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this directory\", \"this module\", \"provides\", \"responsible for\", \"is used to\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Per-file descriptions in Contents sections: 1-2 sentences maximum. Reference key symbols but do not reproduce full summaries.\n- Behavioral contracts (regex patterns, format specs, constants) belong in a separate Behavioral Contracts section, not in per-file descriptions.\n- Annex file references: link to .annex.sum files, do not inline their content. One line per annex reference.\n- Subdirectory descriptions: 1-2 sentences maximum. Capture the directory's role, not its full contents.\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- Key exported symbols from file summaries MUST appear in the directory overview\n- Preserve exact casing of identifiers\n\nUSER NOTES:\n- If \"User Notes\" are provided in the prompt, they contain user-defined instructions that will be automatically prepended to your output\n- Do NOT repeat or paraphrase user notes in your generated content — they are included separately\n- You may reference information from user notes for context",
      "response": "<!-- Generated by agents-reverse-engineer -->\n\n# hooks\n\nThis directory contains hook implementations that extend Claude and OpenCode sessions, automatically injecting generated documentation into context and checking for agents-reverse-engineer package updates.\n\n## Contents\n\n### SessionStart & Event-Driven Hooks\n\n[are-check-update.js](./are-check-update.js) — SessionStart hook that spawns a detached background process to check npm for ARE package updates, caching results (update_available, installed, latest, checked) to `~/.claude/cache/are-update-check.json`. Reads local `.claude/ARE-VERSION` then falls back to global `~/.claude/ARE-VERSION` (defaults to \"0.0.0\"), runs `npm view agents-reverse-engineer version` with 10-second timeout, suppresses Windows cmd window with `windowsHide: true`.\n\n[opencode-are-check-update.js](./opencode-are-check-update.js) — Exports `AreCheckUpdate(ctx)` async hook handler for OpenCode `session.created` event; mirrors are-check-update.js logic but writes to `~/.config/opencode/cache/are-update-check.json`, checks `.opencode/ARE-VERSION` then `~/.config/opencode/ARE-VERSION`. Spawns detached child via `spawn(process.execPath, ['-e', script])` with `child.unref()` to prevent blocking.\n\n### Context Injection\n\n[are-context-loader.js](./are-context-loader.js) — PostToolUse hook that auto-injects AGENTS.md files into Claude context after Read tool calls. `main(data)` extracts `file_path` from tool input, walks directory ancestry upward from file location toward project root, collects only AGENTS.md files marked with string `'Generated by agents-reverse-engineer'`. Uses session-scoped deduplication (temp file `~/.tmp/are-context-loader/{session_id}.json`) to prevent redundant context injection across multiple Read calls. Outputs JSON with `additionalContext` and `suppressOutput: false` to stdout; reverses collected parts to ensure root-to-leaf ordering. Silently exits on malformed input, missing `file_path`, or file outside project tree.\n\n## File Relationships\n\n**Version Detection Chain:** are-check-update.js and opencode-are-check-update.js both implement identical version lookup logic (local version file → global fallback → default \"0.0.0\"), then spawn background processes running `npm view` with identical error handling (10s timeout, \"unknown\" on failure).\n\n**Cache Directories:** are-check-update.js uses `~/.claude/cache/` and `~/.claude/cache/npm-cache/`, while opencode-are-check-update.js uses `~/.config/opencode/cache/` and `~/.config/opencode/cache/npm-cache/`. Both create directories recursively and write identically-structured result objects to `are-update-check.json`.\n\n**Context Injection Pipeline:** are-context-loader.js depends on output from src/generation/writers/agents-md.ts (which generates AGENTS.md files) and consumes them on PostToolUse events to extend Claude's context window without duplicating across the session.",
      "model": "claude-haiku-4-5-20251001",
      "inputTokens": 3,
      "outputTokens": 757,
      "cacheReadTokens": 0,
      "cacheCreationTokens": 20724,
      "latencyMs": 14451,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": []
    },
    {
      "timestamp": "2026-02-14T13:50:22.804Z",
      "prompt": "Generate AGENTS.md for directory: \"src\" (src)\n\n## File Summaries (1 files)\n\n### version.ts\n**Purpose:** src/version.ts exports getVersion() to retrieve package.json version string, falling back to 'unknown' on read/parse ...\n\n**src/version.ts exports getVersion() to retrieve package.json version string, falling back to 'unknown' on read/parse errors.**\n\n## Exports\n- `getVersion(): string` — reads package.json from parent directory, returns `version` field or 'unknown' if file missing/malformed\n\n## Key Details\n- Uses ESM introspection: `import.meta.url` → `fileURLToPath()` → `dirname()` to resolve __dirname\n- Path construction: `join(__dirname, '..', 'package.json')` assumes package.json at project root\n- Silent error handling: any fs/parse exception returns 'unknown' sentinel value\n- Return type always string (never null/undefined)\n\n## Project Directory Structure\n\n<project-structure>\n./\n  .npmrc\n  CONTRIBUTING.md\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  pages.yml\n  publish.yml\nhooks/\n  are-check-update.js\n  are-context-loader.js\n  opencode-are-check-update.js\nscripts/\n  analyze-agents-compression.sh\n  analyze-sum-ratios.sh\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  codex.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/providers/\n  subprocess.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  rebuild.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/core/\n  index.ts\n  logger.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  claude-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  orchestrator.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/rebuild/\n  checkpoint.ts\n  index.ts\n  orchestrator.ts\n  output-parser.ts\n  prompts.ts\n  spec-reader.ts\n  types.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\n\n## Subdirectories\n\n### ai/\n<!-- Generated by agents-reverse-engineer v0.9.10 -->\n\n# src/ai\n\nThe ai directory provides a unified orchestration layer for AI backend selection, provider execution, retry/timeout management, and telemetry collection. It exports a public API (`AIService`, `AIProvider`, `AIBackend`, `AIResponse`) with strict encapsulation of internal backends and telemetry implementations.\n\n## Contents\n\n### Core Service & Registry\n\n[index.ts](./index.ts) — Barrel export enforcing module boundaries; re-exports `AIService`, `AIProvider`, `AIBackend`, `AIResponse`, `BackendRegistry`, `SubprocessProvider`, `createBackendRegistry()`, `resolveBackend()`, `detectBackend()`, `withRetry()`, `runSubprocess()`, `isCommandOnPath()`, error type `AIServiceError`, and constants `DEFAULT_RETRY_OPTIONS`.\n\n[service.ts](./service.ts) — `AIService` orchestrator wrapping any `AIProvider` with retry (rate-limit only), timeout enforcement, and telemetry; accepts `AIServiceOptions` (timeoutMs, maxRetries, model, command, telemetry config); methods include `call(AICallOptions): Promise<AIResponse>`, `finalize(projectRoot)`, `setTracer()`, `setDebug()`, `setSubprocessLogDir()`, `addFilesReadToLastEntry()`, `getSummary()`.\n\n[registry.ts](./registry.ts) — `BackendRegistry` stores `AIBackend` instances with registration and lookup; `createBackendRegistry()` initializes with Claude, Codex, Gemini, OpenCode in priority order; `detectBackend()` and `resolveBackend()` with auto-detection support; `getInstallInstructions()` aggregates install guidance across all backends.\n\n[types.ts](./types.ts) — Defines contract types: `AIProvider` (injectable interface), `AIBackend` (CLI adapter with buildArgs/parseResponse/isAvailable), `AICallOptions`, `AIResponse`, `RetryOptions`, `FileRead`, `TelemetryEntry`, `RunLog`; error type `AIServiceError` with codes `CLI_NOT_FOUND | TIMEOUT | PARSE_ERROR | SUBPROCESS_ERROR | RATE_LIMIT`.\n\n### Retry & Subprocess\n\n[retry.ts](./retry.ts) — `withRetry<T>()` implements exponential backoff with formula `min(baseDelayMs × multiplier^attempt, maxDelayMs) + jitter[0–500ms]`; `DEFAULT_RETRY_OPTIONS` (maxRetries: 3, baseDelayMs: 1_000, maxDelayMs: 8_000, multiplier: 2); accepts `isRetryable(error)` predicate and `onRetry(attempt, error)` callback.\n\n[subprocess.ts](./subprocess.ts) — `runSubprocess(command, args, options)` spawns CLI processes with timeout enforcement (SIGTERM → SIGKILL after 5s grace), stdin piping, exit code extraction; `getActiveSubprocessCount()` and `getActiveSubprocesses()` for concurrency tracking; `SubprocessOptions` includes timeoutMs, input, env, onSpawn callback; 10MB maxBuffer for large outputs.\n\n## Subdirectories\n\n[backends/](./backends/) — Multiple `AIBackend` implementations (Claude, Codex, Gemini, OpenCode) adapting distinct CLI tools to unified response parsing and argument construction; handles polymorphic JSON/NDJSON output formats, token counting, cost calculation, and CLI argument building.\n\n[providers/](./providers/) — `SubprocessProvider` wrapping `AIBackend` instances; implements `AIProvider` contract with subprocess lifecycle, rate-limit detection via `RATE_LIMIT_PATTERNS`, structured trace events (subprocess:spawn, subprocess:exit), debug heap snapshots, and async log file I/O.\n\n[telemetry/](./telemetry/) — `TelemetryLogger` accumulates `TelemetryEntry` objects and computes `RunLog` summaries; `writeRunLog()` persists to `.agents-reverse-engineer/logs/run-*.json` with sanitized filenames; `cleanupOldLogs()` retains N most recent by lexicographic ordering.\n\n## Architecture & Data Flow\n\n**Selection & Initialization**\n1. `createBackendRegistry()` populates backend map (Claude > Codex > Gemini > OpenCode priority)\n2. `resolveBackend(registry, 'auto' | name)` detects available CLI or throws with install instructions\n3. `SubprocessProvider` wraps selected backend\n4. `AIService` wraps provider with retry/telemetry/timeout config\n\n**Call Execution**\n1. `AIService.call(AICallOptions)` → `withRetry()` wrapper\n2. `SubprocessProvider.call()` → `backend.buildArgs()` + `runSubprocess()`\n3. Exit code checked; rate limits detected via `isRateLimitStderr()`; response parsed via `backend.parseResponse()`\n4. `TelemetryLogger.addEntry()` records call metadata, tokens, latency\n5. Failed calls recorded with `exitCode: 1` and error message\n\n**Finalization & Cleanup**\n1. `AIService.finalize(projectRoot)` → `TelemetryLogger.toRunLog()` snapshots metrics\n2. `writeRunLog()` persists JSON to disk with sanitized `run-{command}-{backend}-{model}-{timestamp}.json`\n3. `cleanupOldLogs()` prunes retention to `keepRuns` count (most recent by timestamp)\n\n## Behavioral Contracts\n\n**Subprocess timeout escalation** (`subprocess.ts`): SIGTERM sent by `execFile` at `timeoutMs`; if unresponsive after `SIGKILL_GRACE_MS` (5000ms), escalates to SIGKILL via process group kill (`-child.pid`).\n\n**Retry predicate** (`retry.ts`): Permanent errors (non-retryable per `isRetryable`) throw immediately; only transient failures proceed through backoff loop with up to 3 attempts.\n\n**Rate-limit detection** (`providers/subprocess.ts`): Case-insensitive stderr substring matching against patterns `'rate limit'`, `'429'`, `'too many requests'`, `'overloaded'` triggers `AIServiceError('RATE_LIMIT')` for retry eligibility.\n\n**Telemetry aggregation** (`telemetry/logger.ts`): Per-call totals summed fresh on each `getSummary()` (not cached); includes input/output/cache tokens, duration, file reads, error count.\n\n**Log filename sanitization** (`telemetry/run-log.ts`): Pattern `run-{safeCommand}-{safeBackend}-{safeModel}-{safeTimestamp}.json`; replaces non-alphanumeric with `-`, `:/.` in timestamp with `-`.\n\n## Error Handling & Recovery\n\n`AIService.call()` retries only `RATE_LIMIT` errors (excludes timeout to prevent resource exhaustion). `onRetry` callback logs warn message with attempt count and emits trace event. Other `AIServiceError` codes (`TIMEOUT`, `PARSE_ERROR`, `SUBPROCESS_ERROR`, `CLI_NOT_FOUND`) throw immediately without retry. Failed calls persisted in telemetry with error message and `exitCode: 1`. `AIServiceError.code` enables pattern matching on error type for caller-specific handling.\n\n## Configuration & Composition\n\n**Service-level defaults** (`service.ts`): `AIServiceOptions.model` applies unless overridden per-call; `command` identifies triggering operation (generate|update|specify|rebuild) for telemetry labeling; `telemetry.keepRuns` controls log retention count.\n\n**Subprocess options** (`subprocess.ts`): `timeoutMs` forwarded to `execFile` timeout; `input` piped to stdin; `env` overrides (merge with process.env); `onSpawn(pid)` callback for trace correlation.\n\n**Retry options merging** (`retry.ts`): Caller spreads `DEFAULT_RETRY_OPTIONS` and augments with `isRetryable(error)` and `onRetry(attempt, error)` predicates specific to error type (rate-limit vs. timeout).\n\n## Annex References\n\nFull backend response parsing schemas: [backends AGENTS.md](./backends/)\n\nSubprocess lifecycle tracing patterns: [providers AGENTS.md](./providers/)\n\nTelemetry log formats and retention policy: [telemetry AGENTS.md](./telemetry/)\n### change-detection/\n<!-- Generated by agents-reverse-engineer v0.9.10 -->\n\n# src/change-detection\n\nThe change-detection directory provides git-based file modification tracking and integrity verification for the agents-reverse-engineer system. It detects added, modified, deleted, and renamed files between commits, supports uncommitted changes detection, and computes SHA-256 content hashes for cache validation and change verification in incremental documentation workflows.\n\n## Contents\n\n### Functions\n- [detector.ts](./detector.ts) — `isGitRepo()`, `getCurrentCommit()`, `getChangedFiles()`, `computeContentHash()`, `computeContentHashFromString()`; implements git diff parsing with rename detection (`-M` flag, 50% similarity threshold) and optional uncommitted change tracking (deduplicates across staged, modified, deleted, untracked).\n- [index.ts](./index.ts) — Barrel re-export of detector and types; public API surface for incremental update workflows.\n\n### Types\n- [types.ts](./types.ts) — `ChangeType` (added|modified|deleted|renamed), `FileChange` (path, status, oldPath?), `ChangeDetectionResult` (currentCommit, baseCommit, changes[], includesUncommitted), `ChangeDetectionOptions` (includeUncommitted?).\n\n## Data Flow\n\n`getChangedFiles(projectRoot, baseCommit, options)` → `git diff --name-status -M baseCommit HEAD` (parses status codes: A, M, D, R* with old/new paths) + optional `git status` (stage/working tree) → deduplicates and maps to `FileChange[]` → returns `ChangeDetectionResult`.\n\nUncommitted tracking merges `status.modified`, `status.deleted`, `status.not_added`, `status.staged`, filtering duplicates by path to avoid redundant entries.\n\n## Behavioral Contracts\n\n**Rename detection:** Status prefix `R` followed by percentage (R100, R95, etc.); old/new paths extracted from tab-separated parts[1] and parts[last].\n\n**Diff output format:** `STATUS\\tFILE` (or `STATUS\\tOLD\\tNEW` for renames); non-empty lines split on tab character.\n\n**Content hash algorithm:** SHA-256 hex digest via `crypto.createHash('sha256')` on file buffer or string.\n\n## Dependencies\n\n`simple-git` (checkIsRepo, revparse, diff, status); `node:crypto.createHash()`; `node:fs/promises.readFile()`.\n### cli/\n<!-- Generated by agents-reverse-engineer v0.9.10 -->\n\n# src/cli\n\nThis directory exports the command-line interface for agents-reverse-engineer, providing eight CLI commands (install, init, discover, generate, update, specify, rebuild, clean) that coordinate with AI backends, file discovery, config management, and documentation generation pipelines.\n\n## Contents\n\n### Entry Point & Routing\n\n- [**index.ts**](./index.ts) — `main()` entry point parsing POSIX arguments via `parseArgs()`, routing to command handlers (`initCommand`, `discoverCommand`, `generateCommand`, `updateCommand`, `cleanCommand`, `specifyCommand`, `rebuildCommand`), detecting installer mode via `hasInstallerFlags()`, and outputting help/version. Handles 8 explicit commands plus auto-detected installer via flag combination (`--global`/`--local`/`--force`/`--runtime`).\n\n### Command Implementations\n\n- [**init.ts**](./init.ts) — `initCommand()` creates `.agents-reverse/config.yaml`, updates `.gitignore` to append `*.sum` under `# agents-reverse-engineer` section via `ensureGitignoreEntry()`, configures VS Code via `ensureVscodeExclude()` (using `jsonc-parser` for surgical edits to preserve comments/formatting).\n\n- [**discover.ts**](./discover.ts) — `discoverCommand()` orchestrates file discovery via `discoverFiles()`, loads filter rules via `loadConfig()`, emits trace events (`discovery:start`, `discovery:end`), calls `createOrchestrator().createPlan()` and `buildExecutionPlan()` to partition files, logs results (included/excluded), and writes `.agents-reverse-engineer/GENERATION-PLAN.md` via `formatExecutionPlanAsMarkdown()`.\n\n- [**generate.ts**](./generate.ts) — `generateCommand()` executes full documentation generation: discovers files, creates analysis plan, resolves AI backend via `resolveBackend()`, initializes `AIService` with configurable timeouts/retries/model, runs two-phase `CommandRunner.executeGenerate()` pipeline (file analysis + directory AGENTS.md generation), handles dry-run via `dryRunVariant`, supports eval mode (namespace output by `${backend}.${model}`), writes telemetry/traces, exits 0/1/2.\n\n- [**update.ts**](./update.ts) — `updateCommand()` detects changed files via content-hash (git diff + uncommitted flag), resolves backend, executes `CommandRunner.executeUpdate()` for concurrent file analysis, regenerates AGENTS.md per affected directory by `buildDirectoryPrompt()` + AI call + `writeAgentsMd()`, handles variant mode and cleanup of deleted `.sum`/`AGENTS.md` files, exits 0/1/2 based on success/failure counts.\n\n- [**specify.ts**](./specify.ts) — `specifyCommand()` synthesizes AGENTS.md into project specification: checks conflicts via spec file existence, auto-triggers `generateCommand()` if no docs exist, collects docs via `collectAgentsDocs()` + annex files via `collectAnnexFiles()`, builds prompt via `buildSpecPrompt()`, calls `AIService` (model upgraded to opus), writes output (single or multi-file) via `writeSpec()`, traces phase lifecycle with `createTraceWriter()`.\n\n- [**rebuild.ts**](./rebuild.ts) — `rebuildCommand()` reconstructs projects from specs: reads spec files via `readSpecFiles()`, partitions into units via `partitionSpec()`, loads checkpoint state via `CheckpointManager.load()` for resumable builds, resolves backend, executes `executeRebuild()` with 15min+ timeout (AI model upgraded to opus), writes progress via `ProgressLog`, exits 0/1/2.\n\n- [**clean.ts**](./clean.ts) — `cleanCommand()` deletes generated artifacts: removes all `*.sum` files via glob, filters `AGENTS.md`/`CLAUDE.md` by marker prefix (`GENERATED_MARKER_PREFIX`) to preserve user-authored files, deletes `.agents-reverse-engineer/GENERATION-PLAN.md`, restores `.AGENTS.local.md` → `AGENTS.md` and `.CLAUDE.local.md` → `CLAUDE.md` via rename, logs operations (dry-run mode available via `dryRun: true`).\n\n## Behavioral Contracts\n\n**CLI Argument Parsing** (`parseArgs`): POSIX-style flags (short `-h`, `-g`, `-l`, `-V`; long `--flag` or `--flag value`); stops at first non-flag positional arg (command name); returns `{command, positional, flags, values}` structure. Version string formatted as `agents-reverse-engineer v${VERSION}`.\n\n**Exit Codes**: 0 on help/version/success, 1 on partial failure (filesProcessed > 0 && filesFailed > 0) or conflict, 2 on total failure (filesProcessed === 0 && filesFailed > 0) or backend CLI not found (`AIServiceError` code `'CLI_NOT_FOUND'`).\n\n**Trace Events**: Commands emit `discovery:start(targetPath)`, `discovery:end(filesIncluded, filesExcluded, durationMs)`, `phase:start(phase, taskCount, concurrency)`, `phase:end(phase, durationMs, success)`, `task:start/done(taskLabel, durationMs, success)` via `ITraceWriter.emit()` interface; events written to `.agents-reverse-engineer/traces/${ISO}.json` when `--trace` enabled.\n\n**Model Cascade**: CLI flag (`--model`) > config file (`config.ai.model`) > command default (e.g., \"opus\" for specify/rebuild; varies for others). Sonnet auto-upgraded to opus for specify/rebuild.\n\n**Backend Provisioning**: `createBackendRegistry()` enumerates available backends; `resolveBackend(registry, backendName)` validates CLI availability (throws `AIServiceError` code `'CLI_NOT_FOUND'` if missing); `backend.ensureProjectConfig?.(path)` idempotent provisioning (e.g., OpenCode agent config).\n\n**Dry-Run & Eval Modes**: `--dry-run` prints execution plan (file counts, directories, estimated AI calls) without AI invocations or writes. `--eval` namespaces output (`*.${backend}.${model}.sum`, `AGENTS.${variant}.md`) for A/B comparison; logs `[eval] Variant: ${variant}`.\n\n**Subprocess Logging**: When `--trace` enabled, `setSubprocessLogDir(`.agents-reverse-engineer/subprocess-logs/${ISO-timestamp}/)` captures backend CLI stderr/stdout; ISO timestamp formatted with hyphens replacing colons/periods for filesystem safety.\n\n**Config Loading**: All commands call `loadConfig(absolutePath, {tracer, debug})` which reads `.agents-reverse/config.yaml` via `configExists()`, applies defaults, and surfaces `config.ai.{backend, model, concurrency, timeoutMs, maxRetries, telemetry.keepRuns}`.\n\n**Progress Logging**: `ProgressLog.create(absolutePath)` writes to `.agents-reverse-engineer/progress.log` with timestamped header, status lines, file counts; `finalize()` on command exit.\n\n**Error Messages**: Permission denied (EACCES/EPERM) exit 1 with helpful message; unknown commands trigger `showUnknownCommand(command)` then exit 1; install instructions printed on backend CLI not found via `getInstallInstructions(registry)`.\n\n## Workflow & Conventions\n\n**Installer Integration**: Commands `install` and `uninstall` delegate to `runInstaller(args)` which re-parses via `parseInstallerArgs()` for install-specific options (`--runtime`, `--global`, `--local`, `--force`). Empty args launches interactive installer. Other commands use extracted CLI flags/values directly.\n\n**File Path Resolution**: All commands resolve targetPath to project root via `findProjectRoot(targetPath)` before proceeding; default path is `.` (current directory) if positional arg omitted.\n\n**Conflict Detection**: `specify` checks `specs/SPEC.md` (or `options.output`) for existence unless `--force`; scans output directory in multi-file mode; exits code 1 on conflict. `init` warns if config exists unless `--force`.\n\n**Auto-Generation Trigger**: `specify` automatically runs `generateCommand()` if `collectAgentsDocs()` returns empty (no AGENTS.md found); re-collects after generation; exits code 1 if still empty.\n\n**Variant (Eval) Output**: When `--eval` flag set, variant computed as `${backend.name}.${effectiveModel}`; output files prefixed (e.g., `*.claude.opus.sum`); hub index written via `writeAgentsMdHub()` for variant aggregation.\n\n**Timeout Escalation**: Specify and rebuild commands enforce minimum timeouts (900_000ms = 15min) via `Math.max(config.ai.timeoutMs, 900_000)` to handle complex AI workloads.\n### config/\n<!-- Generated by agents-reverse-engineer v0.9.10 -->\n\n# src/config\n\nConfiguration management for agents-reverse-engineer: loading, validating, and providing default values for the tool's runtime behavior, file exclusion patterns, binary detection, AI service settings, and concurrency limits.\n\n## Contents\n\n### Defaults\n[defaults.ts](./defaults.ts) exports `getDefaultConcurrency()` (cores × CONCURRENCY_MULTIPLIER clamped to MIN_CONCURRENCY=2, MAX_CONCURRENCY=20, with memory cap from totalMemGB × 0.5 ÷ 0.512 GB/subprocess), `DEFAULT_VENDOR_DIRS`, `DEFAULT_EXCLUDE_PATTERNS` (gitignore syntax), `DEFAULT_BINARY_EXTENSIONS`, size/compression limits, and a merged `DEFAULT_CONFIG` object.\n\n### Schema & Validation\n[schema.ts](./schema.ts) defines Zod validation schemas: `ConfigSchema` (root), `ExcludeSchema`, `OptionsSchema`, `OutputSchema`, `GenerationSchema` (compressionRatio 0.1–1.0), and `AISchema` (backend enum, model, timeoutMs=300s, maxRetries=3, concurrency 1–20, telemetry.keepRuns=50); exports `Config` type inferred from ConfigSchema.\n\n### Runtime Loading\n[loader.ts](./loader.ts) provides `loadConfig(root, options?)` to parse `.agents-reverse-engineer/config.yaml` via ConfigSchema with fallback to defaults, `findProjectRoot(startDir)` to locate `.agents-reverse-engineer/`, `configExists(root)`, and `writeDefaultConfig(root)` to scaffold config.yaml; throws `ConfigError` (extends Error with filePath and cause) on parse/validation failure.\n\n## Configuration Contracts\n\n- **compressionRatio**: 0.1–1.0; controls .sum file verbosity (0.1 = aggressive, 0.5 = verbose).\n- **backend**: enum `['claude', 'codex', 'gemini', 'opencode', 'auto']`; 'auto' detects from PATH.\n- **concurrency**: 1–20; auto-detected via `getDefaultConcurrency()` if omitted.\n- **timeoutMs**: default 300,000 (5 min for subprocess execution).\n- **maxRetries**: default 3 for transient errors.\n- **telemetry.keepRuns**: default 50 retained run logs.\n\n## Constants & Thresholds\n\n- CONCURRENCY_MULTIPLIER = 5; MIN_CONCURRENCY = 2; MAX_CONCURRENCY = 20.\n- SUBPROCESS_HEAP_GB = 0.512; MEMORY_FRACTION = 0.5.\n- DEFAULT_MAX_FILE_SIZE = 1048576 (1 MB); DEFAULT_COMPRESSION_RATIO = 0.25.\n- CONFIG_DIR = `.agents-reverse-engineer`; CONFIG_FILE = `config.yaml`.\n\n## Integration Points\n\n- Consumed by `src/cli/` commands (init, generate, update, discover, rebuild, specify).\n- Provides defaults to `src/discovery/` (file filters via exclude patterns/binary extensions/vendor dirs).\n- Supplies AI backend config to `src/ai/` (backend enum, concurrency, timeoutMs, maxRetries).\n- Trace events (`config:loaded`) emitted to `src/orchestration/trace.js` via ITraceWriter.\n### core/\n<!-- Generated by agents-reverse-engineer v0.9.10 -->\n\n# src/core\n\nThe core module serves as the unified public API for agents-reverse-engineer, aggregating 40+ exports across logging, AI services, file discovery, documentation generation, quality checking, and orchestration — enabling single-import access to the entire library.\n\n## Contents\n\n### [index.ts](./index.ts)\nRe-exports 40+ symbols from logging (`nullLogger`, `consoleLogger`, `Logger`), AI services (`AIService`, `withRetry`, `SubprocessProvider`), discovery (`discoverFiles`, `walkDirectory`, `applyFilters`), prompt generation (`buildFilePrompt`, `buildDirectoryPrompt`, `detectLanguage`), documentation writers (`writeSumFile`, `readSumFile`, `writeAgentsMd`), orchestration (`GenerationOrchestrator`, `buildExecutionPlan`), quality checks (`extractExports`, `checkCodeVsDoc`, `checkPhantomPaths`), change detection (`computeContentHash`, `getChangedFiles`), configuration (`loadConfig`, `findProjectRoot`, `DEFAULT_EXCLUDE_PATTERNS`), and imports (`extractImports`, `formatImportMap`). Module aggregates seven layers of functionality from sibling `../` paths (ai/, discovery/, generation/, orchestration/, quality/, change-detection/, config/, imports/) with no CLI-only dependencies in public API.\n\n### [logger.ts](./logger.ts)\nDefines `Logger` interface with `debug()`, `warn()`, `error()` methods, plus `nullLogger` (no-op) and `consoleLogger` (stderr) implementations. Enables dependency injection across core modules for decoupled output control.\n\n## API Layers\n\n**Logging**: `Logger` interface + `nullLogger`, `consoleLogger` implementations.\n\n**AI Services**: `AIService` orchestrator, `SubprocessProvider` for external LLM calls, `withRetry()` wrapper with `RetryOptions` and `DEFAULT_RETRY_OPTIONS`.\n\n**Discovery**: `discoverFiles()`, `walkDirectory()`, `applyFilters()` for file enumeration with `FileFilter` rules.\n\n**Prompts**: `buildFilePrompt()`, `buildDirectoryPrompt()`, `detectLanguage()` with `PromptContext`.\n\n**Writers**: `writeSumFile()`, `readSumFile()`, `writeAgentsMd()`, `writeAnnexFile()`, `writeClaudeMdPointer()` for documentation output.\n\n**Orchestration**: `GenerationOrchestrator`, `buildExecutionPlan()`, `formatExecutionPlanAsMarkdown()` for multi-file analysis pipelines.\n\n**Quality**: `extractExports()`, `checkCodeVsDoc()`, `checkCodeVsCode()`, `checkPhantomPaths()`, `buildInconsistencyReport()` for documentation validation.\n\n**Change Detection**: `computeContentHash()`, `isGitRepo()`, `getCurrentCommit()`, `getChangedFiles()` for differential analysis.\n\n**Configuration**: `loadConfig()`, `findProjectRoot()`, `getDefaultConcurrency()`, `DEFAULT_EXCLUDE_PATTERNS` for environment setup.\n\n**Imports**: `extractImports()`, `extractDirectoryImports()`, `formatImportMap()` for dependency analysis.\n\n## Design Pattern\n\nSingle aggregation point: consumers import from `@agents-reverse-engineer/core` (or `../core`) instead of navigating sibling module paths. Marked `@beta` until v1.0.0; API is stable but subject to refinement. Dependency injection (Logger parameter) decouples output behavior from library logic.\n### discovery/\n<!-- Generated by agents-reverse-engineer v0.9.10 -->\n\n# src/discovery\n\nThe discovery directory orchestrates file enumeration and exclusion, walking directory trees and filtering results through a composable chain of gitignore, vendor, binary, and custom pattern filters with trace instrumentation and bounded concurrency.\n\n## Contents\n\n**Core Pipeline**\n\n- [run.ts](./run.ts) — Entry point: `discoverFiles(root, config, options)` walks directory via `walkDirectory()`, instantiates filters in order (gitignore → vendor → binary → custom), chains them through `applyFilters()`, returns `FilterResult` with included/excluded files and filter attribution.\n\n- [walker.ts](./walker.ts) — Low-level traversal: `walkDirectory(options: WalkerOptions)` delegates to `fast-glob` with `absolute: true`, `onlyFiles: true`, `dot: true` (default), `followSymlinks: false` (default), `suppressErrors: true`, and hardcoded `.git/**` exclusion; separation of concerns keeps walker filter-agnostic.\n\n- [types.ts](./types.ts) — Interfaces: `FileFilter` (async-first `shouldExclude()` returning `Promise<boolean> | boolean`), `ExcludedFile` (path + reason + filter name), `FilterResult` (included/excluded arrays), `WalkerOptions` (cwd, followSymlinks, dot).\n\n## Subdirectories\n\n- [filters/](./filters/) — Filter chain factories (`createBinaryFilter`, `createVendorFilter`, `createGitignoreFilter`, `createCustomFilter`) and composition: `applyFilters(files, filters, options)` orchestrates ordered evaluation with short-circuit semantics, bounded concurrency (CONCURRENCY=30), and trace emission.\n\n## Filter Pipeline Architecture\n\n`discoverFiles()` constructs filter array in strict order: async `createGitignoreFilter(root)` → sync `createVendorFilter(vendorDirs)` → sync `createBinaryFilter(maxFileSize)` → sync `createCustomFilter(patterns, root)`. All filters passed to `applyFilters()` which evaluates per-file in order, stops at first exclusion, records reason and filter name, emits trace events (`filter:applied` with per-filter statistics).\n\nBinary filter receives `maxFileSize` and `additionalExtensions` config tuple for two-phase detection (extension Set lookup, then `isBinaryFile` content analysis). Custom filter receives patterns array and root path for gitignore-style pattern application via `ignore` library.\n\n## Configuration Surface\n\n`DiscoveryConfig` interface: `exclude.vendorDirs[]`, `exclude.binaryExtensions[]`, `exclude.patterns[]`, `options.maxFileSize`, `options.followSymlinks`.\n\n`DiscoverFilesOptions` interface: optional `tracer?: ITraceWriter`, `debug?: boolean` for pipeline instrumentation.\n\n## Key Design Patterns\n\n**Separation of Concerns:** Walker collects all files (no filtering); filters applied separately via composable chain.\n\n**Async-First Filters:** `FileFilter.shouldExclude()` returns `Promise<boolean> | boolean`, allowing sync filters (gitignore, vendor, binary extension check) and async filters (binary content I/O via `isBinaryFile`).\n\n**Trace Attribution:** Each `ExcludedFile` record includes `filter: string` (e.g., \"gitignore\", \"vendor\", \"binary\") for provenance tracking; `applyFilters()` emits `filter:applied` trace events with per-filter match counts.\n\n**Bounded Concurrency:** Worker pool via shared iterator prevents file descriptor exhaustion during binary content checks (`isBinaryFile` I/O); CONCURRENCY=30 configurable in `filters/index.ts`.\n\n## Behavioral Contracts\n\n**Path Normalization:**\n- Absolute paths → relative via `path.relative(normalizedRoot, absolutePath)` for pattern matching\n- Relative paths starting with `'..'` or empty treated as outside-root (never excluded)\n- OS-specific `path.sep` normalization in vendor filter for cross-platform matching\n\n**Binary Extension Detection:**\n- Case-insensitive via `path.extname().toLowerCase()`\n- 65 pre-computed binary extensions in `BINARY_EXTENSIONS` Set (O(1) lookup)\n\n**Vendor Directory Matching:**\n- Single-segment names ('node_modules') matched anywhere in hierarchy via `absolutePath.split(path.sep)` segment checking\n- Multi-segment patterns ('apps/vendor') matched via substring inclusion after normalization\n\n**Pattern Matching (gitignore, custom):**\n- Empty pattern arrays return `false` (pass all files)\n- Missing .gitignore silently handled (no error)\n\n## File Relationships\n\n- `run.ts` orchestrates: `walkDirectory()` → `applyFilters([gitignore, vendor, binary, custom])`\n- `walker.ts` produces file list; filters consume it sequentially\n- `filters/index.ts` chains four factories and exports `applyFilters()` orchestrator\n- All filters implement `FileFilter` interface from `types.ts`\n- Trace events routed to optional `ITraceWriter` (from `../orchestration/trace.js`)\n### generation/\n<!-- Generated by agents-reverse-engineer v0.9.10 -->\n\nBased on the provided file summaries and project structure, here is the AGENTS.md for src/generation:\n\n<!-- Generated by agents-reverse-engineer -->\n\n# src/generation\n\nThis directory orchestrates documentation generation pipelines, combining file discovery, complexity analysis, and plan execution to produce .sum file summaries and AGENTS.md directory overviews with intelligent post-order traversal, variant support, and user content preservation.\n\n## Contents\n\n### Core Pipeline Files\n- [collector.ts](./collector.ts) — `collectAgentsDocs()` recursively walks project tree to locate and aggregate AGENTS.md files; `collectAnnexFiles()` collects .annex.sum files for reproduction-critical constants. Both functions skip directories (node_modules, .git, .agents-reverse-engineer, vendor, dist, build, __pycache__, .next, venv, .venv, target, .cargo, .gradle) and return sorted `AgentsDocs` tuples.\n\n- [complexity.ts](./complexity.ts) — `analyzeComplexity()` computes `ComplexityMetrics` (fileCount, directoryDepth, files Set, directories Set) from file lists; `calculateDirectoryDepth()` measures maximum nesting; `extractDirectories()` recursively traverses parent directories via `path.dirname()` until reaching filesystem root.\n\n- [executor.ts](./executor.ts) — `buildExecutionPlan()` transforms `GenerationPlan` into `ExecutionPlan` with post-order sorted directory tasks (deepest first) and file task dependencies; `isDirectoryComplete()` checks if all files have .sum artifacts; `getReadyDirectories()` filters directories with complete .sum coverage; `formatExecutionPlanAsMarkdown()` generates GENERATION-PLAN.md with task phase breakdown and skipped file/directory counts.\n\n- [types.ts](./types.ts) — Defines `SummaryMetadata` interface with `purpose`, optional `criticalTodos[]`, optional `relatedFiles[]` for pipeline type contracts.\n\n## Subdirectories\n\n### [prompts/](./prompts/)\nPrompt construction utilities for file summaries and AGENTS.md synthesis. `buildFilePrompt()` handles template substitution, compression injection (when ratio < 0.5), and language detection; `buildDirectoryPrompt()` orchestrates multi-step synthesis: reads .sum files, extracts directory imports, detects .annex.sum files and manifest indicators (package.json, Cargo.toml, etc.), preserves user-authored content via `GENERATED_MARKER_PREFIX` detection, and returns update prompts when existingAgentsMd provided.\n\n### [writers/](./writers/)\nFile writers for .sum serialization, AGENTS.md variants, and CLAUDE.md pointers. `readSumFile()`, `writeSumFile()` handle YAML-frontmatter .sum files with SHA-256 change detection; `writeAgentsMd()`, `writeAgentsMdHub()` manage variant suffix and marker-based user content preservation (rename to .local.md if user-authored); `writeClaudeMdPointer()` generates CLAUDE.md with @-references to AGENTS.md and optional CLAUDE.local.md.\n\n## Architecture & Data Flow\n\n**Generation Pipeline:** discovery → complexity analysis → execution plan building → parallel file/directory task execution → .sum + AGENTS.md output.\n\n1. **File Discovery & Collection** (collector.ts): `collectAgentsDocs()` and `collectAnnexFiles()` recursively aggregate project documentation.\n\n2. **Complexity Metrics** (complexity.ts): `analyzeComplexity()` computes fileCount, directoryDepth, directory ancestors from discovered file paths to inform traversal order.\n\n3. **Execution Plan** (executor.ts): `buildExecutionPlan()` consumes `GenerationPlan` type (from src/orchestration), creates `ExecutionTask` objects (type: 'file'|'directory', path, absolutePath, systemPrompt, userPrompt, dependencies, outputPath, metadata), post-order sorts directory tasks by depth descending (deepest first) to ensure child AGENTS.md generation completes before parents, tracks all discovered files in `directoryFileMap` for complete prompt context, and marks skipped files/directories.\n\n4. **Prompt Assembly** (prompts/): `buildFilePrompt()` and `buildDirectoryPrompt()` construct system/user prompts with template substitution, compression rules, import extraction, and preservation of existing summaries for incremental updates.\n\n5. **Artifact Writing** (writers/): `writeSumFile()` serializes documentation into YAML-frontmatter .sum files with content hash; `writeAgentsMd()` writes directory overviews with marker-based user content detection and .local.md preservation; `writeClaudeMdPointer()` generates hub files with @-references.\n\n**Variant Support:** Optional `variant` parameter (e.g., \"claude.haiku\") propagates through `getSumPath()`, `sumFileExists()`, `getAnnexPath()`, and file naming (AGENTS.${variant}.md, file.ts.${variant}.sum).\n\n**Post-Order Traversal:** Directory tasks sorted by `getDirectoryDepth()` descending ensures parent AGENTS.md synthesis waits for all child `.sum` file completion, enabling aggregated imports and nested directory references in parent directory prompts.\n\n## Behavioral Contracts\n\n**Directory Task Dependencies:** fileTaskIds are formatted as `file:${f}` and assigned to directoryTasks[].dependencies; only directories in plan.tasks (plannedDirs) become ExecutionTasks.\n\n**Complexity Depth Calculation:** directoryDepth = max(relative path segment count - 1 for file itself). Root '.' = 0, 'src' = 1, 'src/cli' = 2.\n\n**Skip Directory List (SKIP_DIRS):** node_modules, .git, .agents-reverse-engineer, vendor, dist, build, __pycache__, .next, venv, .venv, target, .cargo, .gradle. Graceful handling: readdir/readFile failures trigger silent skip.\n\n**Compression Ratio Trigger:** When sourceFileSize > 0 and compressionRatio < 0.5, targetSize = Math.round(sourceFileSize * ratio), maxSize = Math.round(targetSize * 1.2); aggressive compression rules injected into user prompt.\n\n**Template Substitution Patterns:** `{{FILE_PATH}}`, `{{CONTENT}}`, `{{LANG}}` replaced via `.replace(/\\{\\{...\\}\\}/g, ...)` in FILE_USER_PROMPT.\n\n**Generated Marker Detection:** `GENERATED_MARKER_PREFIX` = `'<!-- Generated by agents-reverse-engineer'`; files lacking this substring treated as user-authored and preserved via rename to `.local.md` variant.\n\n**Manifest File Indicators:** Presence of package.json, Cargo.toml, go.mod, pyproject.toml, pom.xml, build.gradle, Gemfile, composer.json, CMakeLists.txt, or Makefile signals project root/manifest hints in directory prompt.\n\n**Incremental Update Mode:** FILE_UPDATE_SYSTEM_PROMPT triggered when context.existingSum truthy; DIRECTORY_UPDATE_SYSTEM_PROMPT triggered when existingAgentsMd truthy; existing content appended under \"Existing Summary\"/\"Existing AGENTS.md\" headers with explicit preserve-stable-sections instruction.\n\n**YAML Frontmatter Arrays:** Inline format `[a, b, c]` for ≤3 short items; multi-line `- item` format otherwise. Parsing uses regex `/key:\\s*\\[([^\\]]*)\\]/` (inline) and `/key:\\s*\\n((?:\\s+-\\s+.+\\n?)+)/m` (multi-line).\n\n**Sum File Path Convention:** format `${dir}/${basename}.${ext}.sum` or `${dir}/${basename}.${ext}.${variant}.sum`; annex files follow `${dir}/${basename}.annex.sum` or `${dir}/${basename}.annex.${variant}.sum`.\n\n**Content Hash Tracking:** SHA-256 contentHash stored in sum file frontmatter; enables change detection for incremental update decisions.\n\n## Reproduction-Critical Constants\n\nFull prompt template text: [templates.annex.sum](./prompts/templates.annex.sum)\n### imports/\n<!-- Generated by agents-reverse-engineer v0.9.10 -->\n\n# src/imports\n\nThe imports directory exports utilities for parsing TypeScript/JavaScript import statements from source files and formatting import maps for LLM consumption. It provides regex-based extraction across file trees with classification of internal vs. external imports.\n\n## Contents\n\n### Core Functions\n- [extractor.ts](./extractor.ts) — `extractImports()` parses individual files via IMPORT_REGEX; `extractDirectoryImports()` reads directories filtering to first 100 lines per file; `formatImportMap()` outputs structured text for LLM prompts with `specifier → symbols` grouping and `(type)` tags.\n- [index.ts](./index.ts) — Barrel export aggregating `extractImports`, `extractDirectoryImports`, `formatImportMap`, `ImportEntry`, and `FileImports`.\n- [types.ts](./types.ts) — `ImportEntry` (specifier, symbols[], typeOnly flag) and `FileImports` (fileName, externalImports[], internalImports[]) interfaces.\n\n## Behavioral Contracts\n\n**IMPORT_REGEX** (global, multiline):\n```\n/^import\\s+(type\\s+)?(?:\\{([^}]*)\\}|(\\*\\s+as\\s+\\w+)|(\\w+))\\s+from\\s+['\"]([^'\"]+)['\"]/gm\n```\nCaptures type keyword, named imports `{...}`, namespace `* as name`, default import, and module specifier from lines starting with `import`.\n\n**Symbol parsing**: named imports via `.split(',').map(s => s.trim().replace(/\\s+as\\s+\\w+/, ''))` strips aliases; namespace via `.replace('* as ', '')` isolates name; default used as-is.\n\n**Filter rules**: `extractDirectoryImports` skips specifiers not starting with `.` or `..` (excludes npm packages and `node:` builtins); classifies `./` as internal, `../` as external.\n\n**Performance**: reads only first 100 lines per file (imports clustered at top).\n\n**Format template**: `{fileName}:\\n  {specifier} → {symbols}{typeTag}\\n\\n` with `typeTag = ' (type)'` for type-only imports.\n\n## Type Surface\n\n- **ImportEntry**: `{ specifier: string; symbols: string[]; typeOnly: boolean }`\n- **FileImports**: `{ fileName: string; externalImports: ImportEntry[]; internalImports: ImportEntry[] }`\n### installer/\n<!-- Generated by agents-reverse-engineer v0.9.10 -->\n\n# src/installer\n\nThe installer directory handles interactive and automated installation of ARE commands, hooks, and plugins across multiple AI coding runtimes (Claude Code, Codex, OpenCode, Gemini), managing both global user-level and local project-level configurations with cross-platform path resolution and settings.json registration.\n\n## Contents\n\n### Entry Point & Orchestration\n- [index.ts](./index.ts) — Main installer entry point; exports `parseInstallerArgs()`, `runInstaller()`, and mode-determination functions (`determineLocation()`, `determineRuntimes()`); orchestrates install/uninstall workflows with interactive prompts or CLI flags.\n\n### User Interface & Prompts\n- [banner.ts](./banner.ts) — Styled console output via picocolors; exports `displayBanner()`, `showHelp()`, `showSuccess()`, `showError()`, `showWarning()`, `showInfo()`, `showNextSteps()`, and `VERSION` constant.\n- [prompts.ts](./prompts.ts) — Interactive terminal selection with arrow-key (TTY) and numbered (CI) fallback; exports `isInteractive()`, `selectRuntime()`, `selectLocation()`, `confirmAction()`, and raw mode lifecycle management.\n\n### Installation & Uninstallation\n- [operations.ts](./operations.ts) — Command template, hook, and plugin installation logic; exports `installFiles()`, `registerHooks()`, `registerPermissions()`, `verifyInstallation()`, `getPackageVersion()`, `writeVersionFile()`; routes to runtime-specific installers and manages settings.json registration.\n- [uninstall.ts](./uninstall.ts) — Reverses installation across all runtimes; exports `uninstallFiles()`, `unregisterHooks()`, `unregisterPermissions()`, `deleteConfigFolder()`; handles hook removal, Codex rules cleanup, legacy file cleanup, and empty directory recursion.\n\n### Path Resolution & Type Definitions\n- [paths.ts](./paths.ts) — Cross-platform config directory resolution; exports `getRuntimePaths()`, `resolveInstallPath()`, `resolveCodexConfigPath()`, `getSettingsPath()`, `getAllRuntimes()`, and installation detection functions (`isRuntimeInstalledLocally()`, `isRuntimeInstalledGlobally()`, `getInstalledRuntimes()`).\n- [types.ts](./types.ts) — Type signatures for CLI args, installation results, and runtime paths; exports `Runtime`, `Location`, `InstallerArgs`, `InstallerResult`, `RuntimePaths`.\n\n## Workflow & Conventions\n\n**CLI Flags & Modes:**\n- `--runtime {claude|codex|opencode|gemini|all}` specifies target runtime(s); omitted flags trigger interactive prompts.\n- `-g/--global`, `-l/--local` specify installation scope; both or neither requires prompt.\n- `--force` overwrites existing files (only those marked ARE-generated); preserves user files.\n- `-q/--quiet` suppresses display messages; `--help` calls `showHelp()` and exits.\n- Non-interactive mode (no TTY, e.g., CI) requires `--runtime` and `-g`/`--local` flags; missing flags trigger `process.exit(1)`.\n\n**Result Reporting:**\n- `displayInstallResults()` aggregates per-runtime results (`filesCreated`, `filesSkipped`, `hookRegistered`), displays summary counts and post-install next steps referencing `/are-*` commands.\n- `displayUninstallResults()` reuses `InstallerResult` fields (filesCreated tracks deletions); reports aggregate file removals and config folder deletion status.\n\n**Hook Registration & Permissions:**\n- Claude/Gemini: `registerHooks()` merges hook entries into settings.json without duplicating; Claude supports SessionStart, PostToolUse, SessionEnd events; Gemini supports SessionStart, SessionEnd only.\n- Claude: `registerPermissions()` adds Bash command patterns (`npx are init`, `npx are discover`, etc.) to settings.allow to reduce friction.\n- Codex: Generates `are.rules` prefix rules file and `AGENTS.override.md` context-loading instructions.\n\n**File Management:**\n- `shouldWriteManagedFile()` checks for ARE-generation marker before overwriting; `--force` flag bypasses for ARE-marked files.\n- `writeVersionFile()` writes ARE-VERSION tracking installed version.\n- `ensureDir()` creates parent directories before file writes.\n\n## Behavioral Contracts\n\n**Runtime Config Directory Naming:**\n- Claude: `.claude`\n- Codex: `.agents` (skills), `.codex` (CLI rules)\n- OpenCode: `.opencode`\n- Gemini: `.gemini`\n\n**Settings.json Hook Command Format (Claude):**\n```\nnode .claude/hooks/are-check-update.js\nnode .claude/hooks/are-context-loader.js\n```\n\n**Gemini Hook Format (Simplified):**\nFlat hook objects with `name`, `type`, `command` fields; SessionStart/SessionEnd only (no PostToolUse).\n\n**Codex Prefix Rules** (`CODEX_PREFIX_RULES`):\n```\n['npx', 'are', 'init']\n['npx', 'are', 'discover']\n['npx', 'are', 'generate']\n['npx', 'are', 'update']\n['npx', 'are', 'specify']\n['npx', 'are', 'rebuild']\n['npx', 'are', 'clean']\n['rm', '-f', '.agents-reverse-engineer/progress.log']\n['sleep']\n```\n\n**ARE-Specific Markers & Files:**\n- Hook existence check: command string match in settings.json; no duplicates on re-registration.\n- ARE-VERSION filename: written to config root (`basePath/ARE-VERSION`).\n- ARE-generated marker: `'<!-- Generated by agents-reverse-engineer -->'` in file content; identifies files eligible for `--force` overwrite.\n- Codex context-rules marker: `'<!-- Generated by agents-reverse-engineer installer: local-context-rules -->'`; only marked files deleted on uninstall.\n\n**Dry Run Behavior:**\n- `dryRun=true` skips `writeFileSync` calls; returns `InstallerResult` with `filesCreated` populated for preview.\n- Verifies installations via file existence checks; missing files added to `filesSkipped` list.\n\n## Installation Architecture\n\n**Runtime-Specific Routing:**\n- [operations.ts](./operations.ts) dispatches `installFiles()` to `installFilesForRuntime()` per runtime or expands 'all' to concrete runtimes.\n- Command templates sourced from `../integration/templates.js` (`getClaudeTemplates()`, `getCodexTemplates()`, `getOpenCodeTemplates()`, `getGeminiTemplates()`).\n- Hooks bundled in `dist/installer/operations.js` context; resolved via `getBundledHookPath()`.\n\n**Claude/Gemini:** Install session hooks + register in settings.json.\n**Codex:** Generate rules files (`are.rules`) + context-loading instructions (`AGENTS.override.md`).\n**OpenCode:** Install plugins from bundled sources (currently empty `ARE_PLUGINS` array); infrastructure setup (package.json, bun.lock, node_modules/).\n\n**Uninstall Logic:**\n- [uninstall.ts](./uninstall.ts) reverses all installation steps: removes templates, hook files, plugins, Codex rules/context-rules, ARE-VERSION; unregisters hooks/permissions in settings.json.\n- Hook pattern matching: `node {runtimeDir}/hooks/{filename}` (current) and `node hooks/{filename}` (legacy).\n- Cleanup utilities: `cleanupAreSkillDirs()`, `cleanupEmptyDirs()` (recursive), `cleanupLegacyGeminiFiles()`, `cleanupOpenCodeInfrastructure()`.\n### integration/\n<!-- Generated by agents-reverse-engineer v0.9.10 -->\n\n# src/integration\n\nDetects AI coding assistant environments (Claude Code, OpenCode, Aider, Gemini) and generates environment-specific integration command templates for ARE (agents-reverse-engineer) CLI execution.\n\n## Contents\n\n**Detection & Environment Discovery**\n\n- [detect.ts](./detect.ts) — `detectEnvironments(projectRoot)` scans for `.claude/`, `.opencode/`, `.aider/`, `.gemini/` config directories and returns `DetectedEnvironment[]` array; `hasEnvironment(projectRoot, type)` checks for specific `EnvironmentType`.\n\n**Template Generation**\n\n- [templates.ts](./templates.ts) — `getClaudeTemplates()`, `getOpenCodeTemplates()`, `getGeminiTemplates()` return `IntegrationTemplate[]` with platform-specific file paths and markdown/TOML frontmatter; `buildTemplate(platform, commandName, command)` assembles templates with placeholder substitution (`COMMAND_PREFIX`, `VERSION_FILE_PATH`, `BACKEND_FLAG`).\n\n**Integration Orchestration**\n\n- [generate.ts](./generate.ts) — `generateIntegrationFiles(projectRoot, options)` creates environment-specific command files and Claude session-end hooks; calls `detectEnvironments()`, fetches templates via `getTemplatesForEnvironment(type)`, writes files with `mkdirSync({ recursive: true })`, and copies bundled hook from `hooks/dist/are-session-end.js` for Claude environments.\n\n**Type System**\n\n- [types.ts](./types.ts) — Exports `EnvironmentType` (`'claude' | 'opencode' | 'aider' | 'gemini'`), `DetectedEnvironment`, `IntegrationTemplate` (`filename`, `path`, `content`), `IntegrationResult` (`filesCreated`, `filesSkipped` per environment).\n\n## Environment Detection\n\nClaude Code: `.claude/` directory OR `CLAUDE.md` file.\n\nOpenCode: `.opencode/` directory.\n\nAider: `.aider.conf.yml` file OR `.aider/` directory.\n\nGemini: `.gemini/` directory (inferred from template generation).\n\n## File Generation Workflow\n\n1. `detectEnvironments(projectRoot)` returns array of detected environments\n2. For each environment, `getTemplatesForEnvironment(type)` retrieves platform-specific templates\n3. Files are written to environment-specific config paths unless they exist (skip unless `force: true` in `GenerateOptions`)\n4. For Claude, additionally writes `.claude/hooks/are-session-end.js` from bundled hook (`readBundledHook('are-session-end.js')`)\n5. Returns `IntegrationResult[]` with `filesCreated` and `filesSkipped` per environment\n\n## Platform Configuration & Path Structure\n\n| Platform | Config Dir | Command Path | File Format |\n|----------|-----------|---|---|\n| Claude Code | `.claude/` | `.claude/skills/are-{command}/SKILL.md` | Markdown w/ YAML frontmatter |\n| OpenCode | `.opencode/` | `.opencode/commands/are-{command}.md` | Markdown w/ YAML frontmatter |\n| Gemini | `.gemini/` | `.gemini/commands/are-{command}.toml` | TOML (no YAML) |\n| Aider | `.aider/` | (detection only; no template generation) | N/A |\n\n## Behavioral Contracts — ARE Command Execution Workflows\n\n**generate / update / specify / rebuild:**\n1. Display version from `VERSION_FILE_PATH` (platform-specific: `.claude/ARE-VERSION`, `.opencode/ARE-VERSION`, `.gemini/ARE-VERSION`)\n2. Delete stale `.agents-reverse-engineer/progress.log`\n3. Spawn background task: `npx are {command} --backend {platform} $ARGUMENTS` with `run_in_background: true`\n4. Poll progress.log every 15 seconds; read last ~20 lines; continue until completion (tolerates missing log during startup)\n5. Summarize full background task output on completion\n\n**discover:**\n- Run ONLY `npx are discover $ARGUMENTS` (no extra flags)\n- Poll progress.log (~10s wait), read `GENERATION-PLAN.md` + `config.yaml`\n- Classify files: test/spec (`*.test.*`, `*.spec.*`, `__tests__/**`), CI/CD (`.github/workflows/*.yml`, `.gitlab-ci.yml`), tool configs (`.eslintrc*`, `tsconfig*.json`, `jest.config.*`), migrations (`migrations/`, `*.migration.*`), fixtures (`__snapshots__/**`, `*.fixture.*`), type declarations (`*.d.ts`), Docker/infra (`Dockerfile*`, `k8s/**`)\n- Present only categories with matches; skip if already in `config.yaml` exclude list\n- Suggest exclusions via `AskUserQuestion` with `multiSelect: true`; edit `config.yaml` with accepted glob patterns\n\n**clean:**\n- Run ONLY `npx are clean $ARGUMENTS` (no extra flags)\n\n## Command Registry\n\n`COMMANDS` object defines six commands: `generate`, `update`, `init`, `discover`, `clean`, `specify`, `rebuild`, `help` — each with `description`, `argumentHint`, and `content` (instruction template with placeholder substitution).\n\n## Placeholder Substitution\n\n- `COMMAND_PREFIX` → `/are-` (all platforms)\n- `VERSION_FILE_PATH` → platform-specific path\n- `BACKEND_FLAG` → `--backend {claude|opencode|gemini}`\n\n## Critical Dependencies\n\n- `detectEnvironments()` from [detect.ts](./detect.ts) — returns array of detected environments with config dirs\n- `getClaudeTemplates()`, `getOpenCodeTemplates()`, `getGeminiTemplates()` from [templates.ts](./templates.ts) — environment-specific template arrays\n- `readBundledHook('are-session-end.js')` — reads pre-built hook from `hooks/dist/`; validates existence before reading\n- `existsSync()` from `node:fs` — filesystem checks\n- `mkdirSync({ recursive: true })` — creates parent directories\n### orchestration/\n<!-- Generated by agents-reverse-engineer v0.9.10 -->\n\n# src/orchestration\n\nThe orchestration module coordinates AI-driven documentation generation and incremental updates across the entire project. It manages task scheduling, progress reporting, plan tracking, concurrent execution, and lifecycle tracing for both full-project generation (are generate) and change-detection-driven updates (are update) workflows.\n\n## Contents\n\n**Core Exports**\n- [index.ts](./index.ts) — Barrel export aggregating CommandRunner, ProgressReporter, runPool, PlanTracker, trace utilities (createTraceWriter, cleanupOldTraces) and shared types (FileTaskResult, RunSummary, ProgressEvent, CommandRunOptions, PoolOptions, TaskResult, ITraceWriter, TraceEvent, TraceEventPayload).\n\n**Orchestration & Planning**\n- [orchestrator.ts](./orchestrator.ts) — DocumentationOrchestrator unifies generation (createPlan, prepareFiles, buildProjectStructure, filterExistingFiles, filterExistingDirectories, createDirectoryTasks) and update (preparePlan, checkPrerequisites, discoverFiles, recordFileAnalyzed, removeFileState, recordRun, getLastRun, isFirstRun) workflows; createFileTasks handles both PreparedFile and FileChange arrays polymorphically; emits tracer events (phase:start, phase:end, plan:created) throughout execution.\n- [plan-tracker.ts](./plan-tracker.ts) — PlanTracker maintains in-memory markdown and serializes disk writes to GENERATION-PLAN.md via promise-chain queue; markDone replaces checkbox strings (`- [ ] \\`path\\`` → `- [x] \\`path\\``); flush awaits completion; writes to `${projectRoot}/${CONFIG_DIR}/GENERATION-PLAN.md`.\n\n**Execution & Concurrency**\n- [runner.ts](./runner.ts) — CommandRunner orchestrates two-phase AI generation: Phase 1 (concurrent file analysis via runPool calling aiService.call, writing .sum and annex files) with post-phase-1 quality checks (checkCodeVsDoc, checkCodeVsCode, checkPhantomPaths); Phase 2 (post-order directory aggregation via buildDirectoryPrompt, writeAgentsMd, writeAgentsMdHub, writeClaudeMdPointer); executeGenerate handles full workflow, executeUpdate processes changed files only; caches old .sum files (oldSumCache) and Phase 1 source reads (sourceContentCache) for stale-doc detection and quality checks; returns RunSummary with token counts, duration, error metrics, inconsistency counts, and phantom paths.\n- [pool.ts](./pool.ts) — runPool executes async task factories with bounded concurrency using shared iterator; all workers pull from same iterator so tasks run exactly once; sparse result array indexed by original position; supports failFast abort and tracer events (worker:start, task:pickup, task:done, worker:end); no external dependencies.\n\n**Progress & Observability**\n- [progress.ts](./progress.ts) — ProgressLog streams analysis output to `.agents-reverse-engineer/progress.log` without ANSI codes for `tail -f` monitoring via promise-chain serialization; ProgressReporter logs file/directory analysis with ETA calculation via moving-average windows (onFileStart, onFileDone, onFileError, onDirectoryStart, onDirectoryDone, printSummary); formats `[X/Y] ANALYZING`, `[X/Y] DONE Xs in/out tok model ~ETA`, `[X/Y] FAIL` messages via picocolors.\n- [trace.ts](./trace.ts) — ITraceWriter appends NDJSON events to `.agents-reverse-engineer/traces/trace-{ISO8601-timestamp}.ndjson` with promise-chain serialization; TraceEvent discriminated union covers PhaseStartEvent, PhaseEndEvent, WorkerStartEvent, WorkerEndEvent, TaskPickupEvent, TaskDoneEvent, TaskStartEvent, SubprocessSpawnEvent, SubprocessExitEvent, RetryEvent, DiscoveryStartEvent, DiscoveryEndEvent, FilterAppliedEvent, PlanCreatedEvent, ConfigLoadedEvent; createTraceWriter returns NullTraceWriter (zero-cost) when disabled; cleanupOldTraces deletes oldest files beyond retention limit.\n\n**Type Definitions**\n- [types.ts](./types.ts) — FileTaskResult (path, success, tokensIn, tokensOut, cacheReadTokens, cacheCreationTokens, durationMs, model, optional error); RunSummary (version, filesProcessed/filesFailed/filesSkipped, dirsProcessed/dirsFailed/dirsSkipped, totalCalls, totalInputTokens, totalOutputTokens, totalCacheReadTokens, totalCacheCreationTokens, totalDurationMs, errorCount, retryCount, totalFilesRead, uniqueFilesRead, inconsistenciesCodeVsDoc, inconsistenciesCodeVsCode, phantomPaths, inconsistencyReport); ProgressEvent (type: 'start'|'done'|'error'|'dir-done' with event-specific fields); CommandRunOptions (concurrency, failFast, debug, dryRun, tracer, progressLog, variant).\n\n## Execution Workflows\n\n**Generation (are generate)**\n1. DocumentationOrchestrator.createPlan: prepareFiles → filterExistingFiles → analyzeComplexity → buildProjectStructure → createFileTasks → filterExistingDirectories → createDirectoryTasks; emits phase:start, plan:created, phase:end.\n2. CommandRunner.executeGenerate Phase 1: reads oldSumCache (readSumFile, getSumPath) → concurrent file analysis via runPool (aiService.call) → writes .sum and annex files (writeSumFile, writeAnnexFile) → post-phase-1-quality directory checks (checkCodeVsDoc, checkCodeVsCode, buildInconsistencyReport) via runPool (throttled 10 concurrent).\n3. Phase 2: post-order directory aggregation by depth (descending) → buildDirectoryPrompt → writeAgentsMd → writeAgentsMdHub (variant mode) → writeClaudeMdPointer; final validation checkPhantomPaths.\n\n**Update (are update)**\n1. DocumentationOrchestrator.preparePlan: checkPrerequisites (throws if not git repo) → discoverFiles → iterate each file, read .sum frontmatter hash (readSumFile) → computeContentHash → compare with stored content_hash → mark 'added'/'modified'; cleanupOrphans for deleted files; getAffectedDirectories for AGENTS.md regeneration; sets isFirstRun if all files added.\n2. CommandRunner.executeUpdate: Phase 1 only (file analysis) for changed files (filesToAnalyze); skips Phase 2 (AGENTS.md/CLAUDE.md regeneration).\n\n## Concurrency & Scaling\n\n- **Phase 1**: runPool with configurable concurrency; pre-phase-1-cache throttled to 20 concurrent file descriptor opens; post-phase-1-quality directory checks throttled to 10.\n- **Phase 2**: grouped by metadata.depth, each depth level processed with min(concurrency, dirCount) workers; post-order traversal ensures children process before parents.\n- **Serialization**: ProgressLog, PlanTracker, TraceWriter all use promise-chain writeQueue to serialize concurrent writes and maintain output/file order.\n\n## State & Caching\n\n- **CommandRunner**: oldSumCache (pre-existing .sum files for stale-doc detection), sourceContentCache (Phase 1 file reads to avoid I/O in post-phase-1-quality), planTracker (GENERATION-PLAN.md progress), progressReporter (moving-average ETA calculation).\n- **DocumentationOrchestrator**: frontmatter-based hash storage in .sum files (no persistent database); isFirstRun derived from plan metrics.\n- **ProgressReporter**: completionTimes and dirCompletionTimes sliding windows (max 10) for ETA estimation.\n\n## Behavioral Contracts\n\n- **Trace filename format**: `trace-{ISO8601-timestamp}.ndjson` with colons and periods replaced by hyphens (e.g., `trace-2024-12-14T10-30-45-123Z.ndjson`).\n- **NDJSON event serialization**: Each TraceEvent written as single JSON line (`JSON.stringify(event) + '\\n'`) with auto-populated seq, ts, pid, elapsedMs fields.\n- **Checkbox format**: PlanTracker.markDone performs literal string replacement on `- [ ] \\`${itemPath}\\`` (files: `src/cli/init.ts`, directories: `src/cli/AGENTS.md`, pointers: `CLAUDE.md`).\n- **Progress log format**: `[X/Y] ANALYZING path`, `[X/Y] DONE path Xs in/out tok model [~ETA]`, `[X/Y] FAIL path error`; directory: `[dir X/Y] ANALYZING dirPath/AGENTS.md`.\n- **ETA calculation**: moving-average window (≥2 completions required); formatted as seconds if <60, else as `Xm Ys remaining`.\n- **Token accounting**: total input = tokensIn + cacheReadTokens + cacheCreationTokens; tracked separately in RunSummary.\n- **Variant mode**: optional variant parameter affects .sum path (getSumPath), AGENTS.md filename (AGENTS.${variant}.md), triggers writeAgentsMdHub after writeAgentsMd.\n- **Preamble stripping**: stripPreamble removes text before `\\n---\\n` (if offset <500) or before first bold header `**[A-Z]` (if preceding text <300 chars and lacks `##`).\n- **Purpose extraction**: extractPurpose skips preamble lines (now i, perfect, based on, let me, here is, i'll, i will, great, okay, sure, certainly, alright), markdown headers, dashes; strips bold wrapper; truncates >120 chars to 117 + '...'.\n- **Content hash format**: computeContentHash and computeContentHashFromString produce hex digests; stored in .sum frontmatter as `content_hash` field for comparison with preparePlan.\n- **ANSI stripping**: ProgressLog.stripAnsi removes `/\\x1b\\[[0-9;]*m/g` escape sequences for plain-text output.\n- **Trace cleanup**: sorts by lexicographic filename (ISO timestamps sort correctly); keeps `keepCount` most recent entries; silently handles missing directory.\n\n## Integration Points\n\n- **AI Service**: CommandRunner.executeGenerate calls aiService.call(systemPrompt, userPrompt) → AIResponse with response, tokensIn, tokensOut, cacheReadTokens, cacheCreationTokens, model.\n- **File Analysis**: buildFilePrompt (../generation/prompts) generates prompts with existingSum (from readSumFile), sourceFileSize, compressionRatio; analyzeComplexity produces ComplexityMetrics.\n- **Directory Aggregation**: buildDirectoryPrompt accepts knownDirs Set for filtering; variant mode triggers writeAgentsMdHub.\n- **Quality Checks**: checkCodeVsDoc, checkCodeVsCode, checkPhantomPaths, buildInconsistencyReport, formatReportForCli from ../quality.\n- **Change Detection**: isGitRepo, getCurrentCommit, computeContentHash, FileChange from ../change-detection; computeContentHashFromString for .sum frontmatter comparison.\n- **Update Lifecycle**: cleanupOrphans, getAffectedDirectories from ../update; UpdatePlanOptions, CleanupResult types.\n- **Discovery**: discoverFiles as runDiscovery from ../discovery/run.\n- **Config**: Config.generation.compressionRatio setting; CONFIG_DIR from ../config/loader for GENERATION-PLAN.md path.\n- **Logging**: Logger type; nullLogger default; debug flag triggers logger.debug([debug] prefix) output.\n- **Version**: getVersion from ../version for RunSummary.version field.\n\n## Variant & Annex Support\n\n- **Variant Mode**: optional CommandRunOptions.variant parameter (eval model name string) routes .sum files via getSumPath(path, variant), renames AGENTS.md to AGENTS.${variant}.md, triggers writeAgentsMdHub after writeAgentsMd.\n- **Annex Detection**: stripPreamble checks if cleaned text includes `'## Annex References'` to trigger writeAnnexFile(absolutePath, sourceContent, variant) for reproduction-critical constants.\n### output/\n<!-- Generated by agents-reverse-engineer v0.9.10 -->\n\n# src/output\n\nCLI output abstraction for colorized logging with format specifications. Exports `Logger` interface and factory functions (`createLogger`, `createSilentLogger`) that produce formatted output matching CONTEXT.md conventions.\n\n## Contents\n\n- [logger.ts](./logger.ts) — `Logger` interface with `info`, `file`, `excluded`, `summary`, `warn`, `error` methods; `createLogger(options)` and `createSilentLogger()` factories using `picocolors` for optional terminal colorization.\n\n## Logger Interface & Factories\n\n**Logger** contract (6 methods):\n- `info(message: string)` — plain text output\n- `file(path: string)` — green `\"  +\"` prefix + relative path\n- `excluded(path: string, reason: string, filter: string)` — dim `\"  -\"` prefix + path + dim suffix `\"(${reason}: ${filter})\"`\n- `summary(included: number, excluded: number)` — bold discovery count + dim exclusion count\n- `warn(message: string)` — yellow `\"Warning: \"` prefix\n- `error(message: string)` — red `\"Error: \"` prefix\n\n**createLogger(options: LoggerOptions)** — returns `Logger` with colored output; respects `options.colors` (default `true`) to toggle `picocolors` or identity functions.\n\n**createSilentLogger()** — returns no-op `Logger` for test/quiet modes.\n\n## Configuration\n\n**LoggerOptions** interface: single field `colors: boolean` controls terminal colorization behavior.\n### quality/\n<!-- Generated by agents-reverse-engineer v0.9.10 -->\n\n# src/quality\n\nQuality analysis module providing inconsistency detection across code-vs-documentation, code-vs-code patterns, and phantom path references in AGENTS.md documentation. Exports types and validation functions for comprehensive documentation accuracy auditing.\n\n## Contents\n\n**API Barrel & Types**\n\n- [index.ts](./index.ts) — Public API re-exporting `extractExports()`, `checkCodeVsDoc()`, `checkCodeVsCode()`, `buildInconsistencyReport()`, `formatReportForCli()`, `formatReportAsMarkdown()`, `checkPhantomPaths()` and types `InconsistencySeverity`, `CodeDocInconsistency`, `CodeCodeInconsistency`, `PhantomPathInconsistency`, `Inconsistency`, `InconsistencyReport`.\n- [types.ts](./types.ts) — Defines `InconsistencySeverity` ('info' | 'warning' | 'error'), `CodeDocInconsistency`, `CodeCodeInconsistency`, `PhantomPathInconsistency`, `Inconsistency` union, and `InconsistencyReport` with metadata, issues array, and summary counts.\n\n## Subdirectories\n\n- [inconsistency/](./inconsistency/) — Detects and reports mismatches between code exports and documentation via `extractExports()` regex heuristic, `checkCodeVsDoc()` substring matching, and `checkCodeVsCode()` duplicate-export detection. Aggregates results with `buildInconsistencyReport()` and formats via `formatReportForCli()` (plain text with `[ERROR]`/`[WARN]`/`[INFO]` tags) and `formatReportAsMarkdown()` (GitHub Markdown table).\n- [phantom-paths/](./phantom-paths/) — Validates AGENTS.md path references via three `RegExp` patterns (markdown links, backtick paths, prose references) and resolves against AGENTS.md directory and projectRoot with `.js`→`.ts` fallback. Returns `PhantomPathInconsistency[]` with context snippets.\n\n## Validation Pipeline\n\n**Export Detection**: `extractExports(sourceContent)` parses TypeScript/JavaScript exports using regex matching `export` keyword, optional `default`, and declaration type (function/class/const/let/var/type/interface/enum), capturing identifier names.\n\n**Code-vs-Doc**: `checkCodeVsDoc(sourceContent, sumContent, filePath)` compares extracted exports against `SumFileContent.summary` text via `.includes()`, returning `CodeDocInconsistency` when exports are absent from documentation.\n\n**Code-vs-Code**: `checkCodeVsCode(files)` builds `Map<string, string[]>` tracking export names to file paths, flagging entries with `paths.length > 1` as duplicate-export issues.\n\n**Reporting**: `buildInconsistencyReport(issues, metadata)` aggregates inconsistencies by type (codeVsDoc, codeVsCode, phantomPaths) and severity (error, warning, info), attaching ISO 8601 timestamp, projectRoot, filesChecked, durationMs.\n\n**Phantom Paths**: `checkPhantomPaths(agentsMdPath, content, projectRoot)` extracts path strings, deduplicates via `Set`, skips node_modules/git/.http/templates/glob patterns, resolves relative to AGENTS.md dir and projectRoot, attempts `.js`→`.ts` substitution, and verifies `existsSync()`.\n\n## Behavioral Contracts\n\n**Export Regex Pattern**\n```\n/^[ \\t]*export\\s+(?:default\\s+)?(?:function|class|const|let|var|type|interface|enum)\\s+(\\w+)/gm\n```\n\n**Markdown Link Pattern**\n```\n/\\[(?:[^\\]]*)\\]\\((\\.[^)]+)\\)/g\n```\n\n**Backtick Path Pattern**\n```\n/`((?:src\\/|\\.\\.?\\/)[^`]+\\.[a-z]{1,4})`/g\n```\n\n**Prose Path Pattern**\n```\n/(?:from|in|by|via|see)\\s+`?(src\\/[\\w\\-./]+)`?/gi\n```\n\n**Skip Patterns**\n```\n/node_modules/ , /\\.git\\// , /^https?:/ , /\\{\\{/ , /\\$\\{/ , /\\*/ , /\\{[^}]*,[^}]*\\}/\n```\n\n**Inconsistency Report Structure**: metadata (timestamp ISO 8601, projectRoot, filesChecked, durationMs), issues (Inconsistency[]), summary (total, codeVsDoc, codeVsCode, phantomPaths, errors, warnings, info counts).\n\n**CLI Format Tags**: `[ERROR]` | `[WARN]` | `[INFO]` with file/doc/path location info.\n\n**Markdown Table**: Columns: Severity (`ERROR`/`WARN`/`INFO`), Type (code-vs-doc/code-vs-code/phantom-path), Description, Location (filePath or files list).\n### rebuild/\n<!-- Generated by agents-reverse-engineer v0.9.10 -->\n\n# src/rebuild\n\nThe rebuild module reconstructs source files from AI-friendly specification documents by orchestrating parallel AI calls grouped by dependency order, checkpointing progress to enable resumption, parsing multi-file responses, and accumulating built context across phases.\n\n## Contents\n\n### Core Orchestration\n- [orchestrator.ts](./orchestrator.ts) — `executeRebuild()` entry point; groups rebuild units by order, processes groups sequentially with concurrent units within each group via `runPool`, accumulates exported symbols as context (`builtContext`) for downstream groups, handles checkpoint resumption via `CheckpointManager.load()`.\n- [checkpoint.ts](./checkpoint.ts) — `CheckpointManager` persists per-module status and spec file content hashes to `.rebuild-checkpoint`; `load()` performs three-stage drift detection (file exists, schema valid, hashes match); `markDone()`/`markFailed()` update state; silent write failures allow rebuild to continue.\n\n### Specification & Parsing\n- [spec-reader.ts](./spec-reader.ts) — `readSpecFiles()` reads `.md` files from `specs/`; `partitionSpec()` extracts `RebuildUnit[]` from `## Build Plan` sections with `### Phase N:` subsections or falls back to `## ` top-level headings; injects contextual subsections (Architecture, API Surface, Behavioral Contracts, File Manifest) when `Defines:`/`Consumes:` metadata present.\n- [output-parser.ts](./output-parser.ts) — `parseModuleOutput()` parses AI responses into `Map<filePath, content>` using `===FILE: path===` / `===END_FILE===` delimiters (primary) or markdown fenced-block format (fallback); state machine with regex patterns for column-0 delimiter detection.\n- [prompts.ts](./prompts.ts) — `REBUILD_SYSTEM_PROMPT` instructs AI to generate compilable source using delimiter format; `buildRebuildPrompt()` assembles system + user prompt combining full spec, phase content, and previously-built context (`builtContext`); enforces strict naming, no tests/stubs/features.\n\n### Types & Exports\n- [types.ts](./types.ts) — `RebuildCheckpointSchema` validates checkpoint structure; `RebuildUnit` interface (name, specContent, order); `RebuildPlan` (specFiles, units, outputDir); `RebuildResult` (unitName, success, filesWritten, token counts).\n- [index.ts](./index.ts) — Barrel export of types (`RebuildCheckpoint`, `RebuildUnit`, `RebuildPlan`, `RebuildResult`), schemas (`RebuildCheckpointSchema`), and functions (`readSpecFiles`, `partitionSpec`, `parseModuleOutput`, `buildRebuildPrompt`, `executeRebuild`, `CheckpointManager`, `REBUILD_SYSTEM_PROMPT`).\n\n## Architecture & Data Flow\n\n```\nreadSpecFiles() → specFiles[]\n                      ↓\npartitionSpec() → RebuildUnit[] (with injected context)\n                      ↓\nCheckpointManager.load() → { manager, isResume }\n                      ↓\nGroup units by order field (ascending)\n                      ↓\nFor each group (sequential):\n  - Filter pending units (skip isDone())\n  - buildRebuildPrompt(unit, fullSpec, builtContext)\n  - aiService.call() → responseText\n  - parseModuleOutput() → Map<filePath, content>\n  - Write files to outputDir\n  - checkpoint.markDone(name, filesWritten)\n  - Extract file contents → accumulate to builtContext\n                      ↓\ncheckpoint.flush() → .rebuild-checkpoint file\n```\n\n## Behavioral Contracts\n\n**File delimiter format** (from prompts.ts, column 0, no markdown):\n```\n===FILE: relative/path.ext===\n[file content]\n===END_FILE===\n```\n\n**Strict naming compliance**: exported names from spec must match exactly (no synonyms).\n\n**Import requirements**: when prior phase exports appear in \"Already Built\" context, code must import/use them rather than redefine; signatures must match exactly.\n\n**Code quality**: production-ready only (no tests, comments, stubs, placeholders); must compile; must follow spec architecture; forbidden: feature invention, internal explanations.\n\n**Checkpoint schema**: version-stamped `.rebuild-checkpoint` file in output directory; `specHashes` (Map of relativePath → contentHash) enables drift detection; `modules` (Map of unitName → {status, completedAt?, error?, filesWritten?}).\n\n**Resume logic**: checkpoint valid only if JSON parses, schema passes, and ALL spec file content hashes match current files (file count + exact content). Any mismatch triggers fresh checkpoint.\n\n**Built context truncation**: when `builtContext` exceeds 100,000 chars, splits on section markers (`// === [filePath] ===`), keeps recent half-groups in full, truncates older sections to 20-line heads (imports/declarations) with ellipsis.\n\n## Integration Points\n\n- **AI Service** (`../ai/index.js`): `AIService.call({ prompt, systemPrompt, taskLabel })` returns `AIResponse` with token counts.\n- **Orchestration** (`../orchestration/index.js`): `runPool()` dispatches concurrent tasks with concurrency limit; `ProgressReporter` tracks token counts; `ITraceWriter` emits phase events.\n- **Change Detection** (`../change-detection/index.js`): `computeContentHashFromString()` computes spec file hashes for drift detection.\n- **Version** (`../version.js`): `getVersion()` stamps checkpoint creation timestamp.\n\n## Configuration Surface\n\n`RebuildExecutionOptions`:\n- `outputDir` (string, absolute path) — destination for generated files\n- `concurrency` (number) — max concurrent AI calls per order group\n- `failFast?` (boolean) — stop on first error\n- `force?` (boolean) — wipe output directory before rebuild\n- `debug?` (boolean) — verbose logging\n- `tracer?` (ITraceWriter) — emit phase events\n- `progressLog?` (ProgressLog) — track progress callbacks\n\n## Reproduction-Critical Constants\n\n- `BUILT_CONTEXT_LIMIT` (100,000 chars): max length before truncation\n- `TRUNCATED_HEAD_LINES` (20 lines): lines retained when truncating old sections\n- Checkpoint file: `.rebuild-checkpoint` in output directory\n### specify/\n<!-- Generated by agents-reverse-engineer v0.9.10 -->\n\n# src/specify\n\nThe `specify` directory exports utilities for generating AI-readable project specifications from AGENTS.md documentation. It provides prompt construction for specification synthesis and disk writing with conflict detection and optional multi-file splitting.\n\n## Contents\n\n### Core Exports\n- [index.ts](./index.ts) — Barrel re-export of `buildSpecPrompt()`, `writeSpec()`, `SpecPrompt`, `WriteSpecOptions`, and `SpecExistsError`.\n- [prompts.ts](./prompts.ts) — Constructs `SpecPrompt` (system + user pair) for AI specification generation; enforces 12-section specification schema via `SPEC_SYSTEM_PROMPT`.\n- [writer.ts](./writer.ts) — Writes specification markdown to disk via `writeSpec()`; supports single-file and multi-file (heading-split) modes; enforces overwrite protection via `SpecExistsError`.\n\n## Architecture\n\n`buildSpecPrompt()` ingests `AgentsDocs` and optional annex files (both from `../generation/collector.js`), injects them into the user prompt template, and returns structured `SpecPrompt`. `writeSpec()` accepts AI-generated markdown content and `WriteSpecOptions`, validates output paths, splits on `^# ` regex if multi-file mode, creates parent directories, and returns written absolute paths or throws `SpecExistsError` on conflicts (unless `force=true`).\n\n## Specification Schema (SPEC_SYSTEM_PROMPT)\n\nMandates 12-section AI reconstruction structure:\n\n1. **Project Overview** — purpose, value proposition, tech stack with versions\n2. **Architecture** — module boundaries, data flow, design decisions\n3. **Public API Surface** — exported interfaces with full type signatures\n4. **Data Structures & State** — types, schemas, config objects, state patterns\n5. **Configuration** — options, types, defaults, validation, environment variables\n6. **Dependencies** — exact versions and rationale\n7. **Behavioral Contracts** — error handling, retry logic, concurrency, lifecycle; verbatim regex patterns in backticks, format strings, magic constants, environment variable names\n8. **Test Contracts** — per-module scenarios, edge cases, error conditions\n9. **Build Plan** — phased implementation with explicit \"Defines:\" and \"Consumes:\" lists cross-referencing Public API Surface\n10. **Prompt Templates & System Instructions** — full verbatim text from annex files\n11. **IDE Integration & Installer** — command templates, platform configs, permission lists (verbatim)\n12. **File Manifest** — exhaustive source file list with relative path, module, public exports\n\n## Behavioral Contracts\n\n**`prompts.ts` enforcement rules:**\n- Conceptual grouping by concern, not directory structure\n- Module boundaries and interfaces described, not file paths\n- Full type signatures for all public APIs\n- All external dependency versions included\n- Build Plan phases must cross-reference Public API Surface; each phase lists \"Defines:\" and \"Consumes:\" with exact names\n- Regex patterns, format strings, magic constants reproduced verbatim in backticks\n- Sections 10–11 require verbatim annex reproduction\n- File Manifest entries must align with Build Plan phases\n\n**`writer.ts` file handling:**\n- `fileExists()` checks via `fs.access(..., constants.F_OK)`\n- `slugify()` converts headings via lowercase, whitespace→hyphens (`/\\s+/g`), strip non-alphanumeric (`/[^a-z0-9-]/g`), collapse hyphens (`/-+/g`), trim edges (`/^-|-$/g`)\n- `splitByHeadings()` splits on `^(?=# )/m` regex; preamble → `00-preamble.md`; each section named `[slug].md`\n- Conflict detection checks all target paths before writes (atomic validation)\n- Parent directories created via `mkdir(..., { recursive: true })`; UTF-8 encoding used\n\n## API Surface\n\n**`buildSpecPrompt(docs: AgentsDocs, annexFiles?: AgentsDocs): SpecPrompt`** — assembles user prompt with AGENTS.md sections and optional annex files, returns `{ system: string, user: string }`.\n\n**`writeSpec(content: string, options: WriteSpecOptions): Promise<string[]>`** — writes markdown content to disk; single-file mode writes directly to `outputPath`; multi-file mode splits sections and writes to `outputPath` directory; throws `SpecExistsError` on conflicts unless `force=true`; returns array of written absolute paths.\n\n**`WriteSpecOptions`** — `{ outputPath: string, force: boolean, multiFile: boolean }`.\n\n**`SpecExistsError`** — extends Error; `paths: string[]` property lists conflicting file paths; message includes `--force` usage instruction.\n### types/\n<!-- Generated by agents-reverse-engineer v0.9.10 -->\n\n# src/types\n\nCentralized export point for file discovery metadata interfaces used across the codebase to represent discovered files, exclusion information, and discovery statistics.\n\n## Contents\n\n- [index.ts](./index.ts) — Exports `ExcludedFile`, `DiscoveryResult`, and `DiscoveryStats` interfaces for discovery metadata.\n\n## Exported Types\n\n**ExcludedFile**\n- `path: string` — absolute or relative file path\n- `reason: string` — exclusion cause: \"gitignore pattern\", \"binary file\", or \"vendor directory\"\n\n**DiscoveryResult**\n- `files: string[]` — analyzed file paths\n- `excluded: ExcludedFile[]` — excluded files with reasons\n\n**DiscoveryStats**\n- `totalFiles: number` — total files encountered\n- `includedFiles: number` — files included in analysis\n- `excludedFiles: number` — files excluded from analysis\n- `exclusionReasons: Record<string, number>` — exclusion reason breakdown by count\n\n## Type Relationships\n\n`ExcludedFile` and `DiscoveryResult` are consumed by src/discovery/run.ts and src/cli/discover.ts to represent discovery output. `DiscoveryStats` aggregates exclusion metadata across src/discovery/filters/ (binary.ts, custom.ts, gitignore.ts, vendor.ts) for discovery reporting.\n### update/\n<!-- Generated by agents-reverse-engineer v0.9.10 -->\n\n# src/update\n\nThe update directory orchestrates incremental documentation regeneration by detecting file changes, managing affected directories, and cleaning up orphaned .sum and AGENTS.md files from deleted or renamed sources.\n\n## Contents\n\n**Orchestration & Types**\n- [index.ts](./index.ts) — Re-exports `DocumentationOrchestrator` / `UpdateOrchestrator`, `createUpdateOrchestrator()`, cleanup utilities, and type contracts for plan configuration and execution results.\n- [types.ts](./types.ts) — Defines `CleanupResult`, `UpdatePlanOptions`, `UpdateResult`, `UpdateProgress` callback interface for progress instrumentation during incremental updates.\n\n**Cleanup & Detection**\n- [orphan-cleaner.ts](./orphan-cleaner.ts) — Implements `cleanupOrphans()`, `cleanupEmptyDirectoryDocs()`, `getAffectedDirectories()` to remove stale .sum/.annex.sum files and AGENTS.md when sources are deleted/renamed.\n\n## Update Workflow\n\nIncremental updates follow a three-phase pattern:\n\n1. **Change Detection**: Correlates file deletions/renames with their documentation via `FileChange` from `../change-detection/types.js`.\n2. **Orphan Cleanup**: `cleanupOrphans()` deletes .sum/.annex.sum for deleted/renamed sources; `cleanupEmptyDirectoryDocs()` removes AGENTS.md if no source files remain in a directory.\n3. **Affected Path Computation**: `getAffectedDirectories()` identifies all parent directory paths (up to root) requiring AGENTS.md regeneration.\n\nProgress is tracked via `UpdateProgress` callbacks: `onFileStart`, `onFileDone`, `onCleanup` (for .sum/.agents-md deletions), `onDirRegenerate` (for AGENTS.md regeneration).\n\n## Cleanup Semantics\n\n**GENERATED_FILES Set**: `['AGENTS.md', 'CLAUDE.md']` — constant excluded from source file counting; directory is \"empty\" only when no non-generated source files remain.\n\n**Source Detection**: A file counts as a source if not hidden (no leading dot), not a `.sum` file, not in GENERATED_FILES, and does not match AGENTS.*.md pattern.\n\n**Dry-run Mode**: When `dryRun=true`, `cleanupEmptyDirectoryDocs()` skips filesystem mutations but returns `true` to indicate what would be deleted; `cleanupOrphans()` tracks all would-be deletions in `CleanupResult` without writing to disk.\n\n**Variant Support**: Both cleanup and orchestration accept optional `variant` parameter (e.g., \"v2\") to handle per-variant .sum and AGENTS.md files simultaneously.\n\n## Type Contracts\n\n**UpdateResult** aggregates outcomes: `analyzedFiles[]`, `skippedFiles[]`, `cleanup: CleanupResult`, `regeneratedDirs[]`, git state via `baseCommit`, `currentCommit`, and `dryRun` flag.\n\n**UpdatePlanOptions** configures scope: `includeUncommitted` (track uncommitted changes), `dryRun`, `variant`.\n\n**CleanupResult** tracks deletion artifacts: `deletedSumFiles[]`, `deletedAgentsMd[]`.",
      "systemPrompt": "You are generating an AGENTS.md file — a directory-level overview for AI coding assistants.\n\nCRITICAL: Output ONLY the raw markdown content. No code fences, no preamble, no explanations, no conversational text. Your entire response IS the AGENTS.md file content.\n\nOUTPUT FORMAT:\n- First line MUST be exactly: <!-- Generated by agents-reverse-engineer -->\n- Use a # heading with the directory name\n- Write a one-paragraph purpose statement for the directory\nusage\nADAPTIVE SECTIONS:\nAnalyze the directory contents and choose the most relevant sections. Do NOT use a fixed template. Instead, select sections that best document this specific directory for an AI that needs to reconstruct or extend the project.\n\nConsider these section types (choose what applies):\n- **Contents**: Group files by purpose/category under ## headings. For each file: markdown link [filename](./filename) and a one-line description.\n- **Subdirectories**: If subdirectories exist, list them with links [dirname/](./dirname/) and brief summaries.\n- **Architecture / Data Flow**: If files form a pipeline, request/response chain, or layered architecture, document it.\n- **Stack**: If this is a package root (has package.json, Cargo.toml, go.mod, etc.), document the technology stack, key scripts, and entry points.\n- **Structure**: If the directory layout follows a convention (feature-sliced, domain-driven, MVC, etc.), document it.\n- **Patterns**: If files share recurring design patterns (factory, strategy, middleware, barrel re-export), name and document them.\n- **Configuration**: If the directory contains config files, schemas, or environment definitions, document the config surface area.\n- **API Surface**: If the directory exports a public API (barrel index, route definitions, SDK), document the interface contract.\n- **File Relationships**: How files collaborate, depend on each other, or share state.\n- **Behavioral Contracts**: If files contain regex patterns, format specifications, magic constants, or template strings that define observable behavior, collect them in a dedicated section. Preserve verbatim patterns from file summaries — do NOT paraphrase regex into prose. This section is MANDATORY when file summaries contain behavioral artifacts.\n- **Reproduction-Critical Constants**: If file summaries reference annex files (via ## Annex References sections), list them with links. Example: \"Full prompt template text: [templates.annex.sum](./prompts/templates.annex.sum)\". Do NOT reproduce annex content in AGENTS.md — just link to it.\n- **Workflow & Conventions**: If file summaries contain contribution guidelines, PR conventions, commit standards, testing mandates, tool usage rules, AI agent instructions, or code conventions (naming, formatting, linting), collect them into actionable directives. Source from CONTRIBUTING.md, CI configs, PR templates, linter/formatter configs, README dev sections. Present as concrete rules (\"commits must use conventionalcommits format\", \"use camelCase for functions, PascalCase for types\") not vague descriptions (\"the project has guidelines\").\n\nChoose any relevant sections or create your own based on the directory contents. The goal is to provide a comprehensive overview that captures the essence of the directory's role in the project and how its files work together, with a focus on what an AI coding assistant would need to know to effectively interact with this code.\n\nSCOPE:\n- AGENTS.md is a NAVIGATIONAL INDEX — help an AI find the right file quickly\n- Focus on: what each file does, how files relate, directory-level patterns\n- Do NOT reproduce full architecture sections — those belong in the root CLAUDE.md\n\nPATH ACCURACY (MANDATORY):\n- When referencing files or modules outside this directory, use ONLY paths from the \"Import Map\" section\n- Do NOT invent, rename, or guess module paths — if a path isn't in the Import Map, don't reference it\n- Use the exact directory names from \"Project Directory Structure\" — do NOT rename directories\n  (e.g., if the directory is called \"cli\", write \"src/cli/\", NOT \"src/commands/\")\n- Cross-module references must use the specifier format from actual import statements\n  (e.g., \"../generation/writers/sum.js\", NOT \"../fs/sum-file.js\")\n- If you are unsure about a path, omit the cross-reference rather than guessing\n\nCONSISTENCY (MANDATORY):\n- Do not contradict yourself within the same document\n- If you describe a technique (e.g., \"regex-based\"), do not call it something else later (e.g., \"AST-based\")\n- When stating version numbers, engines, or config fields, use ONLY values present in the file summaries\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this directory\", \"this module\", \"provides\", \"responsible for\", \"is used to\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Per-file descriptions in Contents sections: 1-2 sentences maximum. Reference key symbols but do not reproduce full summaries.\n- Behavioral contracts (regex patterns, format specs, constants) belong in a separate Behavioral Contracts section, not in per-file descriptions.\n- Annex file references: link to .annex.sum files, do not inline their content. One line per annex reference.\n- Subdirectory descriptions: 1-2 sentences maximum. Capture the directory's role, not its full contents.\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- Key exported symbols from file summaries MUST appear in the directory overview\n- Preserve exact casing of identifiers\n\nUSER NOTES:\n- If \"User Notes\" are provided in the prompt, they contain user-defined instructions that will be automatically prepended to your output\n- Do NOT repeat or paraphrase user notes in your generated content — they are included separately\n- You may reference information from user notes for context",
      "response": "<!-- Generated by agents-reverse-engineer -->\n\n# src\n\nThe src directory is the root of the agents-reverse-engineer library, exporting `getVersion()` for runtime version string retrieval and aggregating 11 feature modules (ai, change-detection, cli, config, core, discovery, generation, imports, installer, integration, orchestration, output, quality, rebuild, specify, types, update) that collectively provide file discovery, AI-powered documentation generation, incremental updates, specification synthesis, and project reconstruction with integrated telemetry and quality validation.\n\n## Contents\n\n### Version Module\n[version.ts](./version.ts) — Exports `getVersion(): string` via ESM introspection (`import.meta.url` → `fileURLToPath()` → `dirname()` to resolve `__dirname`), reads package.json from parent directory, returns `version` field or 'unknown' sentinel on fs/parse errors; silent error handling ensures return type always string.\n\n## Subdirectories\n\n### [ai/](./ai/)\nUnified AI backend orchestration layer providing `AIService` (orchestrator wrapping any provider with retry/timeout/telemetry), `BackendRegistry` (backend enumeration with Claude/Codex/Gemini/OpenCode priority order), `SubprocessProvider` (CLI subprocess adapter), and `TelemetryLogger` (call/token/duration tracking with `.agents-reverse-engineer/logs/run-*.json` persistence). Handles polymorphic JSON/NDJSON response parsing, rate-limit detection, exponential backoff retry (3 attempts, 1–8s delay with jitter), subprocess timeout escalation (SIGTERM → SIGKILL after 5s grace), and structured trace events (subprocess:spawn, subprocess:exit, retry attempts).\n\n### [change-detection/](./change-detection/)\nGit-based file modification tracking via `getChangedFiles()` (parses `git diff --name-status -M` with rename detection, optional uncommitted tracking), `computeContentHash()` (SHA-256 hex digest for cache validation), and `isGitRepo()` / `getCurrentCommit()` for repo state checks. Supports incremental updates by detecting added/modified/deleted/renamed files with deduplication across staged/working/untracked changes.\n\n### [cli/](./cli/)\nCommand-line interface routing eight commands (init, discover, generate, update, specify, rebuild, clean, install) plus auto-detected installer mode. Entry point `main()` parses POSIX arguments, resolves project root, loads config, orchestrates backend detection, manages dry-run/eval variants, handles trace/subprocess logging, and exits 0/1/2 based on success/failure. Embedded workflows: init creates config + .gitignore entry + VS Code exclude; discover builds GENERATION-PLAN.md; generate runs two-phase AI analysis; update processes changed files with hash-based change detection; specify synthesizes docs→spec with auto-generation fallback; rebuild reconstructs from specs with resumable checkpoints; clean removes artifacts with user-content preservation via marker prefix.\n\n### [config/](./config/)\nConfiguration management with schema validation, defaults, and runtime loading. `loadConfig()` reads `.agents-reverse-engineer/config.yaml` with Zod validation, applies defaults (concurrency auto-detect: cores × 5 clamped to 2–20 based on memory, backend enum, timeoutMs=300s, maxRetries=3, telemetry.keepRuns=50, compressionRatio 0.1–1.0). `findProjectRoot()` locates `.agents-reverse-engineer/` directory. `getDefaultConcurrency()` calculates based on CPU count and available memory (0.512 GB per subprocess).\n\n### [core/](./core/)\nUnified public API aggregating 40+ exports across logging (Logger interface + nullLogger/consoleLogger implementations), AI services (AIService, withRetry, SubprocessProvider), discovery (discoverFiles, walkDirectory, applyFilters), prompts (buildFilePrompt, buildDirectoryPrompt, detectLanguage), writers (writeSumFile, readSumFile, writeAgentsMd), orchestration (GenerationOrchestrator, buildExecutionPlan), quality checks (extractExports, checkCodeVsDoc, checkPhantomPaths), change detection (computeContentHash, getChangedFiles), config (loadConfig, findProjectRoot), imports (extractImports, formatImportMap). Dependency injection (Logger parameter) decouples output behavior; marked @beta until v1.0.0.\n\n### [discovery/](./discovery/)\nFile enumeration with composable filter chain: `walkDirectory()` (fast-glob with `.git/**` exclusion, symlink safe), then `applyFilters()` chains gitignore → vendor → binary → custom filters (bounded concurrency 30). `discoverFiles()` orchestrates pipeline, returns `FilterResult` with included/excluded files and filter attribution. Binary filter combines extension Set (O(1) lookup) + content analysis via `isBinaryFile`. Gitignore/custom filters use `ignore` library with pattern normalization. Trace events per-filter with match counts.\n\n### [generation/](./generation/)\nDocumentation generation pipeline combining file discovery, complexity analysis, and plan execution. `collectAgentsDocs()` recursively walks for AGENTS.md; `analyzeComplexity()` computes fileCount/directoryDepth/ancestors; `buildExecutionPlan()` creates post-order sorted directory tasks (deepest first) for parallel file analysis + aggregated directory synthesis. `buildFilePrompt()` handles compression injection (ratio < 0.5), language detection, template substitution. `buildDirectoryPrompt()` orchestrates multi-step synthesis: reads .sum files, extracts imports, detects manifest files (package.json, Cargo.toml, etc.), preserves user content via `GENERATED_MARKER_PREFIX` detection. Variant support (e.g., \"claude.haiku\") propagates through path/naming. YAML-frontmatter .sum files track SHA-256 contentHash for change detection.\n\n### [imports/](./imports/)\nImport statement extraction via IMPORT_REGEX (global multiline, captures type keyword, named/namespace/default imports, module specifier); `extractImports()` per-file, `extractDirectoryImports()` first 100 lines per file (performance optimization). `formatImportMap()` outputs structured text with specifier → symbols grouping and (type) tags. Internal vs. external classification: `.` / `..` specifiers classified internal; others (npm, node:) excluded.\n\n### [installer/](./installer/)\nInteractive and automated installation across Claude Code, Codex, OpenCode, Gemini runtimes. `parseInstallerArgs()` extracts CLI flags (--runtime, -g/--local, --force, -q/--quiet, --help). `runInstaller()` orchestrates mode-selection (location, runtime), template generation per environment, hook registration in settings.json, permission registration (Claude Bash allow-list, Codex rules file `are.rules`, Gemini simplified hooks), ARE-VERSION tracking. `uninstallFiles()` / `unregisterHooks()` reverses all steps: removes templates, hook files, plugins, Codex rules, unregisters in settings.json. Cross-platform path resolution via `getRuntimePaths()` (Claude .claude, Codex .agents/.codex, OpenCode .opencode, Gemini .gemini). Dry-run mode skips writes but returns `InstallerResult` for preview.\n\n### [integration/](./integration/)\nEnvironment detection and command template generation. `detectEnvironments()` scans for `.claude/`, `.opencode/`, `.aider/`, `.gemini/` config directories. `getClaudeTemplates()`, `getOpenCodeTemplates()`, `getGeminiTemplates()` return platform-specific IntegrationTemplate[] with markdown/TOML frontmatter. `generateIntegrationFiles()` creates command files per environment, copies bundled `are-session-end.js` hook for Claude. Placeholder substitution: COMMAND_PREFIX → `/are-`, VERSION_FILE_PATH (platform-specific), BACKEND_FLAG → `--backend {claude|opencode|gemini}`. Command lifecycle: display version, delete stale progress.log, spawn background ARE task with `run_in_background: true`, poll progress.log every 15s, summarize output on completion. `discover` command auto-classifies files (test/spec, CI/CD, tool configs, migrations, fixtures, type declarations, Docker/infra) with multi-select exclusion suggestions.\n\n### [orchestration/](./orchestration/)\nMulti-phase AI-driven documentation generation and incremental updates. `DocumentationOrchestrator` unifies generation (createPlan, prepareFiles, analyzeComplexity, buildProjectStructure, createFileTasks, createDirectoryTasks) and update (checkPrerequisites, discoverFiles, content-hash-based change detection, orphan cleanup, getAffectedDirectories) workflows. `CommandRunner` executes two-phase generation: Phase 1 concurrent file analysis (runPool with configurable concurrency, post-phase-1 quality checks: checkCodeVsDoc, checkCodeVsCode, checkPhantomPaths); Phase 2 post-order directory aggregation (depth descending ensures children complete before parents). `runPool()` executes async tasks with bounded concurrency, sparse result array, failFast abort. `ProgressReporter` streams to `.agents-reverse-engineer/progress.log` with moving-average ETA (format: `[X/Y] ANALYZING path`, `[X/Y] DONE Xs in/out tok model ~ETA`, `[X/Y] FAIL path error`). `PlanTracker` serializes checkbox updates to GENERATION-PLAN.md via promise-chain queue. `ITraceWriter` appends NDJSON trace events to `.agents-reverse-engineer/traces/trace-{ISO8601}.ndjson` with auto-populated seq, ts, pid, elapsedMs. Variant mode (eval) outputs to `.${backend}.${model}` suffixed paths, triggers writeAgentsMdHub aggregation. Subprocess logging (when --trace enabled) via `setSubprocessLogDir()` captures backend CLI stderr/stdout. Token accounting separates input/cache-read/cache-creation tokens; stored in RunSummary.\n\n### [output/](./output/)\nCLI output abstraction with colorized logging. `Logger` interface (6 methods: info, file, excluded, summary, warn, error) with `createLogger(options)` returning picocolors-formatted output (controlled by `colors` boolean flag) and `createSilentLogger()` no-op implementation. File method formats as green `\"  +\"` prefix; excluded as dim `\"  -\"` prefix with reason/filter suffix; summary as bold included count + dim exclusion count.\n\n### [quality/](./quality/)\nInconsistency detection across code-vs-documentation and phantom path references. `extractExports()` regex heuristic (pattern: `^[ \\t]*export\\s+(?:default\\s+)?(?:function|class|const|let|var|type|interface|enum)\\s+(\\w+)`, gm flags) parses TypeScript/JavaScript exports. `checkCodeVsDoc()` substring matching for extracted exports in sum content; `checkCodeVsCode()` detects duplicate exports across files. `checkPhantomPaths()` validates AGENTS.md path references via three RegExp patterns (markdown links, backtick paths, prose references) with `.js`→`.ts` fallback. `buildInconsistencyReport()` aggregates issues by type/severity with ISO 8601 timestamp, projectRoot, filesChecked, durationMs. `formatReportForCli()` outputs plain text with `[ERROR]`/`[WARN]`/`[INFO]` tags; `formatReportAsMarkdown()` generates GitHub table. Skip patterns (node_modules, .git, http:, {{, ${, *, glob patterns).\n\n### [rebuild/](./rebuild/)\nProject reconstruction from AI-readable specifications with resumable checkpoints. `readSpecFiles()` reads `.md` files from `specs/`; `partitionSpec()` extracts RebuildUnit[] from `## Build Plan` sections with `### Phase N:` subsections, injects contextual sections (Architecture, API Surface, Behavioral Contracts, File Manifest) when \"Defines:\"/\"Consumes:\" metadata present. `executeRebuild()` groups units by order field, processes sequentially, runs concurrent units within each group via runPool, accumulates exported symbols as context (builtContext) for downstream groups. `CheckpointManager` persists status and content hashes to `.rebuild-checkpoint` with three-stage drift detection (file exists, schema valid, hashes match). `parseModuleOutput()` extracts Map<filePath, content> from AI responses using `===FILE: path===` / `===END_FILE===` delimiters (primary) or markdown fence format (fallback). `buildRebuildPrompt()` combines system prompt, spec content, phase details, and built context. REBUILD_SYSTEM_PROMPT enforces compilable source, strict naming, no tests/stubs, production-ready code. Built context truncation at 100,000 chars splits on section markers, keeps recent half-groups in full, truncates older sections to 20-line heads with ellipsis. Checkpoint filename: `.rebuild-checkpoint` in output directory.\n\n### [specify/](./specify/)\nSpecification synthesis from AGENTS.md documentation with conflict detection and multi-file splitting. `buildSpecPrompt()` constructs system + user prompt from AgentsDocs (from ../generation/collector), injects optional annex files, returns SpecPrompt. SPEC_SYSTEM_PROMPT mandates 12-section schema: Project Overview, Architecture, Public API Surface, Data Structures & State, Configuration, Dependencies, Behavioral Contracts (verbatim regex/format strings), Test Contracts, Build Plan (with \"Defines:\"/\"Consumes:\" cross-references to API Surface), Prompt Templates & System Instructions (annex reproduction), IDE Integration & Installer (verbatim command templates), File Manifest (exhaustive source file list with module/exports). `writeSpec()` writes markdown to disk; single-file mode directly to outputPath; multi-file mode splits on `^# ` regex (preamble → 00-preamble.md, each section → [slug].md via lowercase/whitespace→hyphen slugification). Conflict detection checks all target paths before writes (atomic validation); throws SpecExistsError on conflicts unless force=true. Parent directories created via `mkdir(..., { recursive: true })` with UTF-8 encoding.\n\n### [types/](./types.ts)\nCentralized discovery metadata: `ExcludedFile` (path, reason: \"gitignore pattern\" | \"binary file\" | \"vendor directory\"), `DiscoveryResult` (files[], excluded[]), `DiscoveryStats` (totalFiles, includedFiles, excludedFiles, exclusionReasons breakdown).\n\n### [update/](./update/)\nIncremental documentation regeneration orchestrated by change detection. `cleanupOrphans()` deletes .sum/.annex.sum for deleted/renamed sources; `cleanupEmptyDirectoryDocs()` removes AGENTS.md if no source files remain (GENERATED_FILES=['AGENTS.md', 'CLAUDE.md'] excluded from count). `getAffectedDirectories()` identifies all parent paths (up to root) requiring AGENTS.md regeneration. Progress tracked via UpdateProgress callbacks: onFileStart, onFileDone, onCleanup (for .sum/.agents-md deletions), onDirRegenerate (for AGENTS.md updates). UpdateResult aggregates analyzedFiles[], skippedFiles[], CleanupResult (deletedSumFiles[], deletedAgentsMd[]), regeneratedDirs[], git state (baseCommit, currentCommit), dryRun flag. Variant support propagates through per-variant .sum and AGENTS.md files.\n\n## Module Dependencies (Import Map)\n\n```\nsrc/version → (none; standalone)\nsrc/ai → backend vendors (claude CLI, codex CLI, etc); simple-git; node:child_process; node:crypto; picocolors\nsrc/change-detection → simple-git; node:crypto; node:fs/promises\nsrc/cli → all sibling modules; node:process; node:path; jsonc-parser\nsrc/config → zod; yaml; node:fs; node:path; node:os\nsrc/core → all sibling modules (ai, discovery, generation, orchestration, quality, change-detection, config, imports, output)\nsrc/discovery → fast-glob; ignore; node:fs; node:path; identify-languages; binary-file\nsrc/generation → all analysis/writing deps; node:path; node:fs; compression metrics\nsrc/imports → node:fs; node:path\nsrc/installer → node:fs; node:path; picocolors; interactive TTY handling\nsrc/integration → node:fs; node:path; bundled hooks (hooks/dist/are-*.js)\nsrc/orchestration → all sibling modules; picocolors; promise-chain queue pattern\nsrc/output → picocolors; node:path\nsrc/quality → node:fs; node:path; node:crypto\nsrc/rebuild → node:fs; node:path; zod; promise-chain queue pattern\nsrc/specify → node:fs; node:path\nsrc/types → (types-only; no dependencies)\nsrc/update → node:fs; node:path\n```",
      "model": "claude-haiku-4-5-20251001",
      "inputTokens": 3,
      "outputTokens": 4091,
      "cacheReadTokens": 17512,
      "cacheCreationTokens": 24437,
      "latencyMs": 60955,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": []
    },
    {
      "timestamp": "2026-02-14T13:51:23.766Z",
      "prompt": "Generate AGENTS.md for directory: \".\" (agents-reverse-engineer)\n\n## File Summaries (6 files)\n\n### CONTRIBUTING.md\n**Purpose:** CONTRIBUTING.md defines contribution guidelines, development setup, project structure, and licensing terms for agents...\n\n**CONTRIBUTING.md defines contribution guidelines, development setup, project structure, and licensing terms for agents-reverse-engineer.**\n\n## Development Workflow\n\nFork from `main`, run `npm install`, `npm run build` (TypeScript → `dist/`), and `npm run dev` (`tsx watch`). PR submission required; `npm run build` must succeed.\n\n## Project Structure\n\n`src/cli/` (CLI entry point), `src/config/` (schema/loading), `src/discovery/` (file discovery), `src/generation/` (AI docs), `src/orchestration/` (concurrency/tracing), `src/ai/` (AI service/subprocess).\n\n## Issue Requirements\n\nBugs: steps to reproduce, expected vs actual behavior, environment (OS, Node version, AI runtime). Features: `enhancement` label, use case, proposed solution.\n\n## Licensing\n\nContributions licensed under MIT License.\n### LANGUAGES-MANIFEST.md\n**Purpose:** LANGUAGES-MANIFEST.md is a reference table mapping programming languages to their package manifest files, package man...\n\n**LANGUAGES-MANIFEST.md is a reference table mapping programming languages to their package manifest files, package managers, and support status.**\n\n## Content Structure\n\nReference table with 5 columns: Language, Manifest File(s), Package Manager, Supported (checkmark).\n\n## Language Coverage\n\nSupports 23 languages with manifest mappings:\n- **JavaScript/TypeScript**: `package.json` (npm, yarn, pnpm) — marked supported ✓\n- **Python**: `requirements.txt`, `pyproject.toml`, `setup.py`, `Pipfile` (pip, poetry, pipenv) — marked supported ✓\n- **Go**: `go.mod` (go modules) — marked supported ✓\n- **Rust**: `Cargo.toml` (cargo) — marked supported ✓\n- **Ruby**, **Java**, **Kotlin**, **C#/.NET**, **PHP**, **Swift**, **Elixir**, **Erlang**, **Scala**, **Clojure**, **Haskell**, **Dart/Flutter**, **Lua**, **R**, **Julia**, **Zig**, **Nim**, **OCaml**, **C/C++**: mapped to manifest files and package managers; support status not indicated (unmarked rows)\n\n## Usage Context\n\nReference for dependency analysis tools, package resolution, multi-language project scanning, and CI/CD integration. Marks JavaScript, Python, Go, Rust as currently supported; others listed for future implementation or informational purposes.\n### LICENSE\n**Purpose:** LICENSE establishes MIT licensing terms for GeoloeG-IsT software (2026), permitting unrestricted use, modification, d...\n\n**LICENSE establishes MIT licensing terms for GeoloeG-IsT software (2026), permitting unrestricted use, modification, distribution, and sublicensing under copyright and warranty disclaimers.**\n\n## License Terms\n\nMIT License grants permission to obtain, use, copy, modify, merge, publish, distribute, sublicense, and sell copies with conditions: (1) copyright notice and license text must be included in all copies; (2) software provided \"as-is\" without warranty of any kind; (3) authors/copyright holders not liable for claims, damages, or liability arising from use, contract, tort, or otherwise.\n\n## Copyright Attribution\n\nCopyright holder: GeoloeG-IsT. Year: 2026.\n### README.md\n**Purpose:** README.md documents the agents-reverse-engineer (ARE) CLI tool, which generates AI-friendly `.sum` files and `AGENTS....\n\n**README.md documents the agents-reverse-engineer (ARE) CLI tool, which generates AI-friendly `.sum` files and `AGENTS.md` documentation from codebases.**\n\n## Public Commands\n\nCLI: `are install`, `are init`, `are discover`, `are generate`, `are update`, `are specify`, `are rebuild`, `are clean`, `are uninstall`, `are --version`.\n\nAI Assistant: `/are-init`, `/are-discover`, `/are-generate`, `/are-update`, `/are-specify`, `/are-rebuild`, `/are-clean`.\n\n## Key Runtime Targets\n\nInstalls for Claude Code, Codex, OpenCode, Gemini CLI, or all runtimes. Supports global (`~/.claude/`, `~/.agents/`) or local (`./.claude/`, `./.agents/`) placement via `--runtime` and `-g`/`-l` flags.\n\n## Configuration Schema\n\n`.agents-reverse-engineer/config.yaml` controls exclusions (`patterns`, `vendorDirs`, `binaryExtensions`), discovery options (`followSymlinks`, `maxFileSize: 1048576`), output formatting (`colors: true`), and AI service settings (`backend: auto`, `model: sonnet`, `timeoutMs: 300000`, `maxRetries: 3`, `concurrency: 10`, `telemetry.keepRuns: 50`).\n\n## CLI Flags\n\nGeneral: `--model <name>`, `--backend <name>`, `--concurrency <n>`, `--dry-run`, `--force`, `--fail-fast`, `--debug`, `--trace`, `--eval`. Command-specific: `--show-excluded` (discover), `--uncommitted` (update).\n\n## Output Artifacts\n\n`.sum` files per source file with `file_type`, `generated_at`, purpose, public interface, dependencies; `AGENTS.md` per directory with file groupings; `CLAUDE.md` pointer files; root `AGENTS.md` project overview; optional `specs/SPEC.md` from `/are-specify`.\n\n## Workflow: Post-Order Traversal\n\n`/are-discover` scans codebase respecting `.gitignore`, generates `GENERATION-PLAN.md` with post-order traversal (deepest directories first). `/are-generate` executes plan creating `.sum` files then `AGENTS.md`. `/are-update` regenerates only changed files. `/are-specify --multi-file` splits spec into separate files; `--dry-run` previews without AI calls.\n\n## Requirements\n\nNode.js 18+. Supports Claude Code, Gemini CLI, OpenCode, or any `AGENTS.md`-compatible assistant.\n### package.json\n**Purpose:** package.json defines CLI entry points, dependencies, and publish configuration for agents-reverse-engineer, a documen...\n\n**package.json defines CLI entry points, dependencies, and publish configuration for agents-reverse-engineer, a documentation generation tool for AI agent codebases.**\n\n## Entry Points\nDual CLI commands: `agents-reverse-engineer` and `are` both resolve to `dist/cli/index.js`. ESM module with exports: default CLI at `.`, core library at `./core` with TypeScript definitions.\n\n## Dependencies\n**Runtime:** fast-glob (file globbing), ignore (gitignore parsing), isbinaryfile (binary detection), jsonc-parser (JSON with comments), picocolors (terminal colors), simple-git (git operations), yaml (YAML parsing), zod (schema validation).\n**Dev:** @types/node ≥22.10.7, marked (markdown parsing), tsx (TypeScript execution), typescript ≥5.7.3.\n\n## Build & Publish Scripts\n`build` → tsc compilation; `build:hooks` → node scripts/build-hooks.js; `prepublishOnly` runs build + build:hooks; `prepack` removes .sum artifacts before npm packaging; `dev` → tsx watch for live development.\n\n## Distribution & Publishing\nNode ≥18.0.0 required. Published files: dist/, hooks/dist/, README.md, LICENSE. Prepack removes LICENSE.sum and README.md.sum to prevent documentation artifact leakage in published package.\n### tsconfig.json\n**Purpose:** tsconfig.json defines TypeScript compilation target ES2022 with strict type checking, module resolution via NodeNext,...\n\n**tsconfig.json defines TypeScript compilation target ES2022 with strict type checking, module resolution via NodeNext, and output to dist/ from src/.**\n\n## Compiler Configuration\n\n- `target: \"ES2022\"` — output JavaScript for ES2022 standard\n- `module: \"NodeNext\"` — emit ES modules with Node.js interop\n- `moduleResolution: \"NodeNext\"` — resolve imports using Node.js algorithm\n- `lib: [\"ES2022\"]` — include ES2022 type definitions\n- `outDir: \"dist\"` — compiled output directory\n- `rootDir: \"src\"` — source root directory\n\n## Type Safety & Strictness\n\n- `strict: true` — enforce all strict type-checking options (noImplicitAny, strictNullChecks, strictFunctionTypes, etc.)\n- `isolatedModules: true` — each file transpiles independently; require explicit imports/exports\n- `skipLibCheck: true` — skip type checking of declaration files\n\n## Output & Source Maps\n\n- `declaration: true` — emit `.d.ts` type definition files\n- `declarationMap: true` — emit source maps for declarations\n- `sourceMap: true` — emit `.js.map` files for debugging\n- `resolveJsonModule: true` — allow importing JSON files as modules\n\n## Compatibility & File Handling\n\n- `esModuleInterop: true` — add helpers for CommonJS/ES module compatibility\n- `forceConsistentCasingInFileNames: true` — error on case-insensitive file name mismatches\n\n## File Inclusion/Exclusion\n\n- `include: [\"src/**/*\"]` — compile all files under src/\n- `exclude: [\"node_modules\", \"dist\"]` — skip dependencies and build output\n\n## Project Directory Structure\n\n<project-structure>\n./\n  .npmrc\n  CONTRIBUTING.md\n  LANGUAGES-MANIFEST.md\n  LICENSE\n  README.md\n  package.json\n  tsconfig.json\n.github/workflows/\n  pages.yml\n  publish.yml\nhooks/\n  are-check-update.js\n  are-context-loader.js\n  opencode-are-check-update.js\nscripts/\n  analyze-agents-compression.sh\n  analyze-sum-ratios.sh\n  build-hooks.js\nsrc/\n  version.ts\nsrc/ai/\n  index.ts\n  registry.ts\n  retry.ts\n  service.ts\n  subprocess.ts\n  types.ts\nsrc/ai/backends/\n  claude.ts\n  codex.ts\n  gemini.ts\n  opencode.ts\nsrc/ai/providers/\n  subprocess.ts\nsrc/ai/telemetry/\n  cleanup.ts\n  logger.ts\n  run-log.ts\nsrc/change-detection/\n  detector.ts\n  index.ts\n  types.ts\nsrc/cli/\n  clean.ts\n  discover.ts\n  generate.ts\n  index.ts\n  init.ts\n  rebuild.ts\n  specify.ts\n  update.ts\nsrc/config/\n  defaults.ts\n  loader.ts\n  schema.ts\nsrc/core/\n  index.ts\n  logger.ts\nsrc/discovery/\n  run.ts\n  types.ts\n  walker.ts\nsrc/discovery/filters/\n  binary.ts\n  custom.ts\n  gitignore.ts\n  index.ts\n  vendor.ts\nsrc/generation/\n  collector.ts\n  complexity.ts\n  executor.ts\n  types.ts\nsrc/generation/prompts/\n  builder.ts\n  index.ts\n  templates.ts\n  types.ts\nsrc/generation/writers/\n  agents-md.ts\n  claude-md.ts\n  index.ts\n  sum.ts\nsrc/imports/\n  extractor.ts\n  index.ts\n  types.ts\nsrc/installer/\n  banner.ts\n  index.ts\n  operations.ts\n  paths.ts\n  prompts.ts\n  types.ts\n  uninstall.ts\nsrc/integration/\n  detect.ts\n  generate.ts\n  templates.ts\n  types.ts\nsrc/orchestration/\n  index.ts\n  orchestrator.ts\n  plan-tracker.ts\n  pool.ts\n  progress.ts\n  runner.ts\n  trace.ts\n  types.ts\nsrc/output/\n  logger.ts\nsrc/quality/\n  index.ts\n  types.ts\nsrc/quality/inconsistency/\n  code-vs-code.ts\n  code-vs-doc.ts\n  reporter.ts\nsrc/quality/phantom-paths/\n  index.ts\n  validator.ts\nsrc/rebuild/\n  checkpoint.ts\n  index.ts\n  orchestrator.ts\n  output-parser.ts\n  prompts.ts\n  spec-reader.ts\n  types.ts\nsrc/specify/\n  index.ts\n  prompts.ts\n  writer.ts\nsrc/types/\n  index.ts\nsrc/update/\n  index.ts\n  orphan-cleaner.ts\n  types.ts\n</project-structure>\n\n## Subdirectories\n\n### hooks/\n<!-- Generated by agents-reverse-engineer v0.9.10 -->\n\n# hooks\n\nThis directory contains hook implementations that extend Claude and OpenCode sessions, automatically injecting generated documentation into context and checking for agents-reverse-engineer package updates.\n\n## Contents\n\n### SessionStart & Event-Driven Hooks\n\n[are-check-update.js](./are-check-update.js) — SessionStart hook that spawns a detached background process to check npm for ARE package updates, caching results (update_available, installed, latest, checked) to `~/.claude/cache/are-update-check.json`. Reads local `.claude/ARE-VERSION` then falls back to global `~/.claude/ARE-VERSION` (defaults to \"0.0.0\"), runs `npm view agents-reverse-engineer version` with 10-second timeout, suppresses Windows cmd window with `windowsHide: true`.\n\n[opencode-are-check-update.js](./opencode-are-check-update.js) — Exports `AreCheckUpdate(ctx)` async hook handler for OpenCode `session.created` event; mirrors are-check-update.js logic but writes to `~/.config/opencode/cache/are-update-check.json`, checks `.opencode/ARE-VERSION` then `~/.config/opencode/ARE-VERSION`. Spawns detached child via `spawn(process.execPath, ['-e', script])` with `child.unref()` to prevent blocking.\n\n### Context Injection\n\n[are-context-loader.js](./are-context-loader.js) — PostToolUse hook that auto-injects AGENTS.md files into Claude context after Read tool calls. `main(data)` extracts `file_path` from tool input, walks directory ancestry upward from file location toward project root, collects only AGENTS.md files marked with string `'Generated by agents-reverse-engineer'`. Uses session-scoped deduplication (temp file `~/.tmp/are-context-loader/{session_id}.json`) to prevent redundant context injection across multiple Read calls. Outputs JSON with `additionalContext` and `suppressOutput: false` to stdout; reverses collected parts to ensure root-to-leaf ordering. Silently exits on malformed input, missing `file_path`, or file outside project tree.\n\n## File Relationships\n\n**Version Detection Chain:** are-check-update.js and opencode-are-check-update.js both implement identical version lookup logic (local version file → global fallback → default \"0.0.0\"), then spawn background processes running `npm view` with identical error handling (10s timeout, \"unknown\" on failure).\n\n**Cache Directories:** are-check-update.js uses `~/.claude/cache/` and `~/.claude/cache/npm-cache/`, while opencode-are-check-update.js uses `~/.config/opencode/cache/` and `~/.config/opencode/cache/npm-cache/`. Both create directories recursively and write identically-structured result objects to `are-update-check.json`.\n\n**Context Injection Pipeline:** are-context-loader.js depends on output from src/generation/writers/agents-md.ts (which generates AGENTS.md files) and consumes them on PostToolUse events to extend Claude's context window without duplicating across the session.\n### scripts/\n<!-- Generated by agents-reverse-engineer v0.9.10 -->\n\n# scripts\n\nThe scripts directory provides build automation and documentation auditing utilities for the agents-reverse-engineer project, including npm hook preparation, compression analysis for generated documentation, and diagnostic tools for the Agents Reverse Engineer pipeline.\n\n## Contents\n\n### Build & Distribution\n\n[build-hooks.js](./build-hooks.js) — Copies .js files from `hooks/` to `hooks/dist/` for npm bundling; invoked via `prepublishOnly` lifecycle hook. Resolves `projectRoot` via `import.meta.url`, creates `HOOKS_DIST` with `mkdirSync(..., { recursive: true })`, filters source files by `.js` extension excluding `dist`, logs per-file copies and summary count.\n\n### Documentation Auditing\n\n[analyze-agents-compression.sh](./analyze-agents-compression.sh) — Audits AGENTS.md compression efficiency across the documentation tree, computing per-directory ratios (AGENTS.md size / input size) and reporting aggregate statistics. Validates input calculation: sums `.sum` files in same directory plus AGENTS.md from immediate subdirectories (both maxdepth 1). Interprets compression: <50% = EXCELLENT (>2x), <70% = GOOD (~1.5x), <90% = ACCEPTABLE (<1.2x), ≥90% = MINIMAL. Uses portable stat syntax (`stat -f%z` on BSD, `stat -c%s` on GNU) and `awk` for 2-decimal percentage computation.\n\n[analyze-sum-ratios.sh](./analyze-sum-ratios.sh) — Measures .sum file compression against source files, validating target 25% threshold (script aims for 10%). Pairs each .sum file with source via extension removal, computes `(sum_size / source_size) * 100` ratio, reports mean/median/min/max. Flags outliers (ratio >50% or <10%) regardless of file count limit; displays first 10 files by default. Pass criteria: overall_ratio <25% (PASS), <30% (ACCEPTABLE), ≥30% (ABOVE TARGET). Platform-compatible stat and `numfmt --to=iec-i` for human-readable output.\n### src/\n<!-- Generated by agents-reverse-engineer v0.9.10 -->\n\n# src\n\nThe src directory is the root of the agents-reverse-engineer library, exporting `getVersion()` for runtime version string retrieval and aggregating 11 feature modules (ai, change-detection, cli, config, core, discovery, generation, imports, installer, integration, orchestration, output, quality, rebuild, specify, types, update) that collectively provide file discovery, AI-powered documentation generation, incremental updates, specification synthesis, and project reconstruction with integrated telemetry and quality validation.\n\n## Contents\n\n### Version Module\n[version.ts](./version.ts) — Exports `getVersion(): string` via ESM introspection (`import.meta.url` → `fileURLToPath()` → `dirname()` to resolve `__dirname`), reads package.json from parent directory, returns `version` field or 'unknown' sentinel on fs/parse errors; silent error handling ensures return type always string.\n\n## Subdirectories\n\n### [ai/](./ai/)\nUnified AI backend orchestration layer providing `AIService` (orchestrator wrapping any provider with retry/timeout/telemetry), `BackendRegistry` (backend enumeration with Claude/Codex/Gemini/OpenCode priority order), `SubprocessProvider` (CLI subprocess adapter), and `TelemetryLogger` (call/token/duration tracking with `.agents-reverse-engineer/logs/run-*.json` persistence). Handles polymorphic JSON/NDJSON response parsing, rate-limit detection, exponential backoff retry (3 attempts, 1–8s delay with jitter), subprocess timeout escalation (SIGTERM → SIGKILL after 5s grace), and structured trace events (subprocess:spawn, subprocess:exit, retry attempts).\n\n### [change-detection/](./change-detection/)\nGit-based file modification tracking via `getChangedFiles()` (parses `git diff --name-status -M` with rename detection, optional uncommitted tracking), `computeContentHash()` (SHA-256 hex digest for cache validation), and `isGitRepo()` / `getCurrentCommit()` for repo state checks. Supports incremental updates by detecting added/modified/deleted/renamed files with deduplication across staged/working/untracked changes.\n\n### [cli/](./cli/)\nCommand-line interface routing eight commands (init, discover, generate, update, specify, rebuild, clean, install) plus auto-detected installer mode. Entry point `main()` parses POSIX arguments, resolves project root, loads config, orchestrates backend detection, manages dry-run/eval variants, handles trace/subprocess logging, and exits 0/1/2 based on success/failure. Embedded workflows: init creates config + .gitignore entry + VS Code exclude; discover builds GENERATION-PLAN.md; generate runs two-phase AI analysis; update processes changed files with hash-based change detection; specify synthesizes docs→spec with auto-generation fallback; rebuild reconstructs from specs with resumable checkpoints; clean removes artifacts with user-content preservation via marker prefix.\n\n### [config/](./config/)\nConfiguration management with schema validation, defaults, and runtime loading. `loadConfig()` reads `.agents-reverse-engineer/config.yaml` with Zod validation, applies defaults (concurrency auto-detect: cores × 5 clamped to 2–20 based on memory, backend enum, timeoutMs=300s, maxRetries=3, telemetry.keepRuns=50, compressionRatio 0.1–1.0). `findProjectRoot()` locates `.agents-reverse-engineer/` directory. `getDefaultConcurrency()` calculates based on CPU count and available memory (0.512 GB per subprocess).\n\n### [core/](./core/)\nUnified public API aggregating 40+ exports across logging (Logger interface + nullLogger/consoleLogger implementations), AI services (AIService, withRetry, SubprocessProvider), discovery (discoverFiles, walkDirectory, applyFilters), prompts (buildFilePrompt, buildDirectoryPrompt, detectLanguage), writers (writeSumFile, readSumFile, writeAgentsMd), orchestration (GenerationOrchestrator, buildExecutionPlan), quality checks (extractExports, checkCodeVsDoc, checkPhantomPaths), change detection (computeContentHash, getChangedFiles), config (loadConfig, findProjectRoot), imports (extractImports, formatImportMap). Dependency injection (Logger parameter) decouples output behavior; marked @beta until v1.0.0.\n\n### [discovery/](./discovery/)\nFile enumeration with composable filter chain: `walkDirectory()` (fast-glob with `.git/**` exclusion, symlink safe), then `applyFilters()` chains gitignore → vendor → binary → custom filters (bounded concurrency 30). `discoverFiles()` orchestrates pipeline, returns `FilterResult` with included/excluded files and filter attribution. Binary filter combines extension Set (O(1) lookup) + content analysis via `isBinaryFile`. Gitignore/custom filters use `ignore` library with pattern normalization. Trace events per-filter with match counts.\n\n### [generation/](./generation/)\nDocumentation generation pipeline combining file discovery, complexity analysis, and plan execution. `collectAgentsDocs()` recursively walks for AGENTS.md; `analyzeComplexity()` computes fileCount/directoryDepth/ancestors; `buildExecutionPlan()` creates post-order sorted directory tasks (deepest first) for parallel file analysis + aggregated directory synthesis. `buildFilePrompt()` handles compression injection (ratio < 0.5), language detection, template substitution. `buildDirectoryPrompt()` orchestrates multi-step synthesis: reads .sum files, extracts imports, detects manifest files (package.json, Cargo.toml, etc.), preserves user content via `GENERATED_MARKER_PREFIX` detection. Variant support (e.g., \"claude.haiku\") propagates through path/naming. YAML-frontmatter .sum files track SHA-256 contentHash for change detection.\n\n### [imports/](./imports/)\nImport statement extraction via IMPORT_REGEX (global multiline, captures type keyword, named/namespace/default imports, module specifier); `extractImports()` per-file, `extractDirectoryImports()` first 100 lines per file (performance optimization). `formatImportMap()` outputs structured text with specifier → symbols grouping and (type) tags. Internal vs. external classification: `.` / `..` specifiers classified internal; others (npm, node:) excluded.\n\n### [installer/](./installer/)\nInteractive and automated installation across Claude Code, Codex, OpenCode, Gemini runtimes. `parseInstallerArgs()` extracts CLI flags (--runtime, -g/--local, --force, -q/--quiet, --help). `runInstaller()` orchestrates mode-selection (location, runtime), template generation per environment, hook registration in settings.json, permission registration (Claude Bash allow-list, Codex rules file `are.rules`, Gemini simplified hooks), ARE-VERSION tracking. `uninstallFiles()` / `unregisterHooks()` reverses all steps: removes templates, hook files, plugins, Codex rules, unregisters in settings.json. Cross-platform path resolution via `getRuntimePaths()` (Claude .claude, Codex .agents/.codex, OpenCode .opencode, Gemini .gemini). Dry-run mode skips writes but returns `InstallerResult` for preview.\n\n### [integration/](./integration/)\nEnvironment detection and command template generation. `detectEnvironments()` scans for `.claude/`, `.opencode/`, `.aider/`, `.gemini/` config directories. `getClaudeTemplates()`, `getOpenCodeTemplates()`, `getGeminiTemplates()` return platform-specific IntegrationTemplate[] with markdown/TOML frontmatter. `generateIntegrationFiles()` creates command files per environment, copies bundled `are-session-end.js` hook for Claude. Placeholder substitution: COMMAND_PREFIX → `/are-`, VERSION_FILE_PATH (platform-specific), BACKEND_FLAG → `--backend {claude|opencode|gemini}`. Command lifecycle: display version, delete stale progress.log, spawn background ARE task with `run_in_background: true`, poll progress.log every 15s, summarize output on completion. `discover` command auto-classifies files (test/spec, CI/CD, tool configs, migrations, fixtures, type declarations, Docker/infra) with multi-select exclusion suggestions.\n\n### [orchestration/](./orchestration/)\nMulti-phase AI-driven documentation generation and incremental updates. `DocumentationOrchestrator` unifies generation (createPlan, prepareFiles, analyzeComplexity, buildProjectStructure, createFileTasks, createDirectoryTasks) and update (checkPrerequisites, discoverFiles, content-hash-based change detection, orphan cleanup, getAffectedDirectories) workflows. `CommandRunner` executes two-phase generation: Phase 1 concurrent file analysis (runPool with configurable concurrency, post-phase-1 quality checks: checkCodeVsDoc, checkCodeVsCode, checkPhantomPaths); Phase 2 post-order directory aggregation (depth descending ensures children complete before parents). `runPool()` executes async tasks with bounded concurrency, sparse result array, failFast abort. `ProgressReporter` streams to `.agents-reverse-engineer/progress.log` with moving-average ETA (format: `[X/Y] ANALYZING path`, `[X/Y] DONE Xs in/out tok model ~ETA`, `[X/Y] FAIL path error`). `PlanTracker` serializes checkbox updates to GENERATION-PLAN.md via promise-chain queue. `ITraceWriter` appends NDJSON trace events to `.agents-reverse-engineer/traces/trace-{ISO8601}.ndjson` with auto-populated seq, ts, pid, elapsedMs. Variant mode (eval) outputs to `.${backend}.${model}` suffixed paths, triggers writeAgentsMdHub aggregation. Subprocess logging (when --trace enabled) via `setSubprocessLogDir()` captures backend CLI stderr/stdout. Token accounting separates input/cache-read/cache-creation tokens; stored in RunSummary.\n\n### [output/](./output/)\nCLI output abstraction with colorized logging. `Logger` interface (6 methods: info, file, excluded, summary, warn, error) with `createLogger(options)` returning picocolors-formatted output (controlled by `colors` boolean flag) and `createSilentLogger()` no-op implementation. File method formats as green `\"  +\"` prefix; excluded as dim `\"  -\"` prefix with reason/filter suffix; summary as bold included count + dim exclusion count.\n\n### [quality/](./quality/)\nInconsistency detection across code-vs-documentation and phantom path references. `extractExports()` regex heuristic (pattern: `^[ \\t]*export\\s+(?:default\\s+)?(?:function|class|const|let|var|type|interface|enum)\\s+(\\w+)`, gm flags) parses TypeScript/JavaScript exports. `checkCodeVsDoc()` substring matching for extracted exports in sum content; `checkCodeVsCode()` detects duplicate exports across files. `checkPhantomPaths()` validates AGENTS.md path references via three RegExp patterns (markdown links, backtick paths, prose references) with `.js`→`.ts` fallback. `buildInconsistencyReport()` aggregates issues by type/severity with ISO 8601 timestamp, projectRoot, filesChecked, durationMs. `formatReportForCli()` outputs plain text with `[ERROR]`/`[WARN]`/`[INFO]` tags; `formatReportAsMarkdown()` generates GitHub table. Skip patterns (node_modules, .git, http:, {{, ${, *, glob patterns).\n\n### [rebuild/](./rebuild/)\nProject reconstruction from AI-readable specifications with resumable checkpoints. `readSpecFiles()` reads `.md` files from `specs/`; `partitionSpec()` extracts RebuildUnit[] from `## Build Plan` sections with `### Phase N:` subsections, injects contextual sections (Architecture, API Surface, Behavioral Contracts, File Manifest) when \"Defines:\"/\"Consumes:\" metadata present. `executeRebuild()` groups units by order field, processes sequentially, runs concurrent units within each group via runPool, accumulates exported symbols as context (builtContext) for downstream groups. `CheckpointManager` persists status and content hashes to `.rebuild-checkpoint` with three-stage drift detection (file exists, schema valid, hashes match). `parseModuleOutput()` extracts Map<filePath, content> from AI responses using `===FILE: path===` / `===END_FILE===` delimiters (primary) or markdown fence format (fallback). `buildRebuildPrompt()` combines system prompt, spec content, phase details, and built context. REBUILD_SYSTEM_PROMPT enforces compilable source, strict naming, no tests/stubs, production-ready code. Built context truncation at 100,000 chars splits on section markers, keeps recent half-groups in full, truncates older sections to 20-line heads with ellipsis. Checkpoint filename: `.rebuild-checkpoint` in output directory.\n\n### [specify/](./specify/)\nSpecification synthesis from AGENTS.md documentation with conflict detection and multi-file splitting. `buildSpecPrompt()` constructs system + user prompt from AgentsDocs (from ../generation/collector), injects optional annex files, returns SpecPrompt. SPEC_SYSTEM_PROMPT mandates 12-section schema: Project Overview, Architecture, Public API Surface, Data Structures & State, Configuration, Dependencies, Behavioral Contracts (verbatim regex/format strings), Test Contracts, Build Plan (with \"Defines:\"/\"Consumes:\" cross-references to API Surface), Prompt Templates & System Instructions (annex reproduction), IDE Integration & Installer (verbatim command templates), File Manifest (exhaustive source file list with module/exports). `writeSpec()` writes markdown to disk; single-file mode directly to outputPath; multi-file mode splits on `^# ` regex (preamble → 00-preamble.md, each section → [slug].md via lowercase/whitespace→hyphen slugification). Conflict detection checks all target paths before writes (atomic validation); throws SpecExistsError on conflicts unless force=true. Parent directories created via `mkdir(..., { recursive: true })` with UTF-8 encoding.\n\n### [types/](./types.ts)\nCentralized discovery metadata: `ExcludedFile` (path, reason: \"gitignore pattern\" | \"binary file\" | \"vendor directory\"), `DiscoveryResult` (files[], excluded[]), `DiscoveryStats` (totalFiles, includedFiles, excludedFiles, exclusionReasons breakdown).\n\n### [update/](./update/)\nIncremental documentation regeneration orchestrated by change detection. `cleanupOrphans()` deletes .sum/.annex.sum for deleted/renamed sources; `cleanupEmptyDirectoryDocs()` removes AGENTS.md if no source files remain (GENERATED_FILES=['AGENTS.md', 'CLAUDE.md'] excluded from count). `getAffectedDirectories()` identifies all parent paths (up to root) requiring AGENTS.md regeneration. Progress tracked via UpdateProgress callbacks: onFileStart, onFileDone, onCleanup (for .sum/.agents-md deletions), onDirRegenerate (for AGENTS.md updates). UpdateResult aggregates analyzedFiles[], skippedFiles[], CleanupResult (deletedSumFiles[], deletedAgentsMd[]), regeneratedDirs[], git state (baseCommit, currentCommit), dryRun flag. Variant support propagates through per-variant .sum and AGENTS.md files.\n\n## Module Dependencies (Import Map)\n\n```\nsrc/version → (none; standalone)\nsrc/ai → backend vendors (claude CLI, codex CLI, etc); simple-git; node:child_process; node:crypto; picocolors\nsrc/change-detection → simple-git; node:crypto; node:fs/promises\nsrc/cli → all sibling modules; node:process; node:path; jsonc-parser\nsrc/config → zod; yaml; node:fs; node:path; node:os\nsrc/core → all sibling modules (ai, discovery, generation, orchestration, quality, change-detection, config, imports, output)\nsrc/discovery → fast-glob; ignore; node:fs; node:path; identify-languages; binary-file\nsrc/generation → all analysis/writing deps; node:path; node:fs; compression metrics\nsrc/imports → node:fs; node:path\nsrc/installer → node:fs; node:path; picocolors; interactive TTY handling\nsrc/integration → node:fs; node:path; bundled hooks (hooks/dist/are-*.js)\nsrc/orchestration → all sibling modules; picocolors; promise-chain queue pattern\nsrc/output → picocolors; node:path\nsrc/quality → node:fs; node:path; node:crypto\nsrc/rebuild → node:fs; node:path; zod; promise-chain queue pattern\nsrc/specify → node:fs; node:path\nsrc/types → (types-only; no dependencies)\nsrc/update → node:fs; node:path\n```\n\n## Directory Hints\n\nContains manifest file(s): package.json — likely a package or project root.",
      "systemPrompt": "You are generating an AGENTS.md file — a directory-level overview for AI coding assistants.\n\nCRITICAL: Output ONLY the raw markdown content. No code fences, no preamble, no explanations, no conversational text. Your entire response IS the AGENTS.md file content.\n\nOUTPUT FORMAT:\n- First line MUST be exactly: <!-- Generated by agents-reverse-engineer -->\n- Use a # heading with the directory name\n- Write a one-paragraph purpose statement for the directory\nusage\nADAPTIVE SECTIONS:\nAnalyze the directory contents and choose the most relevant sections. Do NOT use a fixed template. Instead, select sections that best document this specific directory for an AI that needs to reconstruct or extend the project.\n\nConsider these section types (choose what applies):\n- **Contents**: Group files by purpose/category under ## headings. For each file: markdown link [filename](./filename) and a one-line description.\n- **Subdirectories**: If subdirectories exist, list them with links [dirname/](./dirname/) and brief summaries.\n- **Architecture / Data Flow**: If files form a pipeline, request/response chain, or layered architecture, document it.\n- **Stack**: If this is a package root (has package.json, Cargo.toml, go.mod, etc.), document the technology stack, key scripts, and entry points.\n- **Structure**: If the directory layout follows a convention (feature-sliced, domain-driven, MVC, etc.), document it.\n- **Patterns**: If files share recurring design patterns (factory, strategy, middleware, barrel re-export), name and document them.\n- **Configuration**: If the directory contains config files, schemas, or environment definitions, document the config surface area.\n- **API Surface**: If the directory exports a public API (barrel index, route definitions, SDK), document the interface contract.\n- **File Relationships**: How files collaborate, depend on each other, or share state.\n- **Behavioral Contracts**: If files contain regex patterns, format specifications, magic constants, or template strings that define observable behavior, collect them in a dedicated section. Preserve verbatim patterns from file summaries — do NOT paraphrase regex into prose. This section is MANDATORY when file summaries contain behavioral artifacts.\n- **Reproduction-Critical Constants**: If file summaries reference annex files (via ## Annex References sections), list them with links. Example: \"Full prompt template text: [templates.annex.sum](./prompts/templates.annex.sum)\". Do NOT reproduce annex content in AGENTS.md — just link to it.\n- **Workflow & Conventions**: If file summaries contain contribution guidelines, PR conventions, commit standards, testing mandates, tool usage rules, AI agent instructions, or code conventions (naming, formatting, linting), collect them into actionable directives. Source from CONTRIBUTING.md, CI configs, PR templates, linter/formatter configs, README dev sections. Present as concrete rules (\"commits must use conventionalcommits format\", \"use camelCase for functions, PascalCase for types\") not vague descriptions (\"the project has guidelines\").\n\nChoose any relevant sections or create your own based on the directory contents. The goal is to provide a comprehensive overview that captures the essence of the directory's role in the project and how its files work together, with a focus on what an AI coding assistant would need to know to effectively interact with this code.\n\nSCOPE:\n- AGENTS.md is a NAVIGATIONAL INDEX — help an AI find the right file quickly\n- Focus on: what each file does, how files relate, directory-level patterns\n- Do NOT reproduce full architecture sections — those belong in the root CLAUDE.md\n\nPATH ACCURACY (MANDATORY):\n- When referencing files or modules outside this directory, use ONLY paths from the \"Import Map\" section\n- Do NOT invent, rename, or guess module paths — if a path isn't in the Import Map, don't reference it\n- Use the exact directory names from \"Project Directory Structure\" — do NOT rename directories\n  (e.g., if the directory is called \"cli\", write \"src/cli/\", NOT \"src/commands/\")\n- Cross-module references must use the specifier format from actual import statements\n  (e.g., \"../generation/writers/sum.js\", NOT \"../fs/sum-file.js\")\n- If you are unsure about a path, omit the cross-reference rather than guessing\n\nCONSISTENCY (MANDATORY):\n- Do not contradict yourself within the same document\n- If you describe a technique (e.g., \"regex-based\"), do not call it something else later (e.g., \"AST-based\")\n- When stating version numbers, engines, or config fields, use ONLY values present in the file summaries\n\nDENSITY RULES (MANDATORY):\n- Every sentence must reference at least one specific identifier (function name, class name, type name, or constant)\n- Never use filler phrases: \"this directory\", \"this module\", \"provides\", \"responsible for\", \"is used to\"\n- Use technical shorthand: \"exports X, Y, Z\" not \"this module exports a function called X...\"\n- Per-file descriptions in Contents sections: 1-2 sentences maximum. Reference key symbols but do not reproduce full summaries.\n- Behavioral contracts (regex patterns, format specs, constants) belong in a separate Behavioral Contracts section, not in per-file descriptions.\n- Annex file references: link to .annex.sum files, do not inline their content. One line per annex reference.\n- Subdirectory descriptions: 1-2 sentences maximum. Capture the directory's role, not its full contents.\n\nANCHOR TERM PRESERVATION (MANDATORY):\n- Key exported symbols from file summaries MUST appear in the directory overview\n- Preserve exact casing of identifiers\n\nUSER NOTES:\n- If \"User Notes\" are provided in the prompt, they contain user-defined instructions that will be automatically prepended to your output\n- Do NOT repeat or paraphrase user notes in your generated content — they are included separately\n- You may reference information from user notes for context",
      "response": "<!-- Generated by agents-reverse-engineer -->\n\n# agents-reverse-engineer\n\nThe agents-reverse-engineer (ARE) project is a documentation generation system for AI-assisted coding, providing a CLI tool and library that discovers source code, generates AI-friendly `.sum` files and directory-level `AGENTS.md` documentation, synthesizes specifications from docs, and reconstructs projects from specifications with integrated telemetry and quality validation.\n\n## Public Commands & Entry Points\n\n**CLI Commands:** `are init`, `are discover`, `are generate`, `are update`, `are specify`, `are rebuild`, `are clean`, `are install` (interactive setup).\n\n**AI Assistant Slash Commands:** `/are-init`, `/are-discover`, `/are-generate`, `/are-update`, `/are-specify`, `/are-rebuild`, `/are-clean`.\n\n**Module Exports:** Dual entry points in `package.json` — `dist/cli/index.js` (both `agents-reverse-engineer` and `are` CLI commands) and `./core` library export aggregating 40+ symbols (AIService, discoverFiles, writeSumFile, buildExecutionPlan, checkCodeVsDoc, extractExports, etc.).\n\n## Core Workflow: Four-Phase Documentation Pipeline\n\n1. **Discovery** (`/are-discover`): `discoverFiles()` scans codebase respecting `.gitignore`, applies composable filter chain (gitignore → vendor → binary → custom), returns included/excluded files with filter attribution. Output: `GENERATION-PLAN.md` with post-order traversal (deepest directories first).\n\n2. **Generation** (`/are-generate`): `buildExecutionPlan()` creates two-phase pipeline. **Phase 1 (Concurrent):** `runPool()` analyzes each source file with bounded concurrency (default cores × 5, clamped 2–20), invokes `buildFilePrompt()` with complexity/compression metadata, executes via `AIService` with retry/timeout/telemetry, writes YAML-frontmatter `.sum` files (contentHash, tokens, model, generated_at). **Phase 2 (Post-Order):** Aggregates per-directory, reads collected `.sum` files via `collectAgentsDocs()`, invokes `buildDirectoryPrompt()` with import extraction/manifest detection/user-content preservation, writes `AGENTS.md` with file groupings, subdirectories, architecture, patterns, behavioral contracts.\n\n3. **Incremental Updates** (`/are-update`): Detects changed files via `getChangedFiles()` (git diff with rename detection), computes SHA-256 content hashes via `computeContentHash()`, skips unchanged files, regenerates `.sum` and affected AGENTS.md directories. Cleanup: `cleanupOrphans()` deletes `.sum` for deleted/renamed files, `cleanupEmptyDirectoryDocs()` removes empty AGENTS.md.\n\n4. **Specification & Rebuild:** (`/are-specify`, `/are-rebuild`): `buildSpecPrompt()` synthesizes docs→spec with 12-section schema (Architecture, API Surface, Behavioral Contracts, Build Plan, File Manifest, etc.). `writeSpec()` outputs single or multi-file (split on `^# ` regex) with conflict detection. `executeRebuild()` reads spec files, groups by phase order, executes via `runPool()` with resumable checkpoints (`.rebuild-checkpoint` with drift detection), parses `===FILE: path===` delimiters, accumulates built context.\n\n## Configuration & Runtime\n\n**Config Schema** (`.agents-reverse-engineer/config.yaml`): Exclusions (patterns, vendorDirs, binaryExtensions), discovery (followSymlinks, maxFileSize: 1048576), output (colors), AI service (backend: auto, model: sonnet, timeoutMs: 300000, maxRetries: 3, concurrency: auto-detect, telemetry.keepRuns: 50).\n\n**Backend Orchestration** (`src/ai/`): `AIService` wraps any backend (Claude CLI, Codex, Gemini, OpenCode) with exponential backoff retry (3 attempts, 1–8s delay, jitter), timeout escalation (SIGTERM → SIGKILL), JSON/NDJSON response parsing, rate-limit detection. `SubprocessProvider` forks backend CLI via `spawn()` with subprocess logging (optional `--trace`), token accounting (input/cache-read/cache-creation separate), structured telemetry. `TelemetryLogger` persists run summaries to `.agents-reverse-engineer/logs/run-*.json`.\n\n**Concurrency Control:** `getDefaultConcurrency()` calculates `cores × 5` clamped to memory budget (0.512 GB per subprocess). `runPool()` executes async tasks with configurable concurrency, sparse result array, failFast abort, progress stream to `progress.log`.\n\n## Quality & Validation\n\n**Code-vs-Documentation Consistency:** `checkCodeVsDoc()` extracts exports via `extractExports()` (regex: `^[ \\t]*export\\s+(?:default\\s+)?(?:function|class|const|let|var|type|interface|enum)\\s+(\\w+)`, multiline) and substring-matches against `.sum` content. `checkCodeVsCode()` detects duplicate exports across files.\n\n**Phantom Path Detection:** `checkPhantomPaths()` validates AGENTS.md path references (markdown links, backtick paths, prose) with `.js`→`.ts` fallback, flags unreachable files.\n\n**Inconsistency Reporting:** `buildInconsistencyReport()` aggregates issues by type/severity with ISO 8601 timestamp, file counts, duration. Output formats: plain text (`[ERROR]`/`[WARN]` tags) or GitHub markdown table.\n\n## Installation & Integration\n\n**Interactive Installer** (`src/installer/`): `runInstaller()` auto-detects Claude Code, Codex, OpenCode, Gemini environments, prompts for installation location (global `~/.claude/`, local `./.claude/`), registers hooks and permissions. `generateIntegrationFiles()` creates platform-specific command templates with placeholder substitution (COMMAND_PREFIX, VERSION_FILE_PATH, BACKEND_FLAG).\n\n**Hook Ecosystem** (`hooks/`):\n- **are-check-update.js** (SessionStart): Spawns background `npm view agents-reverse-engineer version` check, caches result (update_available, installed, latest, checked) to `~/.claude/cache/are-update-check.json`, detects local `.claude/ARE-VERSION` then global fallback.\n- **opencode-are-check-update.js** (`session.created` event): Mirrors are-check-update.js logic for OpenCode, writes to `~/.config/opencode/cache/are-update-check.json`.\n- **are-context-loader.js** (PostToolUse): Injects collected AGENTS.md files into context after Read tool calls, walks directory ancestry upward, deduplicates via session-scoped temp file (`~/.tmp/are-context-loader/{session_id}.json`), outputs JSON with `additionalContext` and `suppressOutput: false`.\n\n## Language Support\n\n**Languages Manifest** ([LANGUAGES-MANIFEST.md](./LANGUAGES-MANIFEST.md)): Reference table mapping 23 languages to manifest files and package managers. Supported (✓): JavaScript/TypeScript (package.json), Python (requirements.txt, pyproject.toml, setup.py, Pipfile), Go (go.mod), Rust (Cargo.toml). Others (Ruby, Java, Kotlin, C#/.NET, PHP, Swift, Elixir, Erlang, Scala, Clojure, Haskell, Dart, Lua, R, Julia, Zig, Nim, OCaml, C/C++) mapped for future implementation.\n\n## Development & Contribution\n\n**Build Pipeline:** `npm run build` compiles TypeScript (src/ → dist/) via tsc, `npm run dev` watches via tsx. Prepack removes `.sum` artifacts before npm publication (`LICENSE.sum`, `README.md.sum`).\n\n**Contribution Workflow** ([CONTRIBUTING.md](./CONTRIBUTING.md)): Fork `main`, run `npm install`, submit PR (build must succeed). Issue templates require steps to reproduce (bugs), environment info (OS, Node ≥18, AI runtime), and use case (features).\n\n**Type Safety:** TypeScript strict mode (noImplicitAny, strictNullChecks, strictFunctionTypes), isolatedModules, ES2022 target with NodeNext module resolution, declaration maps for debugging.\n\n## Contents\n\n### Documentation & Manifests\n\n[README.md](./README.md) — User-facing CLI documentation, public commands, configuration schema, flags, output artifacts, workflow stages, runtime requirements.\n\n[CONTRIBUTING.md](./CONTRIBUTING.md) — Development setup (npm install, npm run build/dev), fork workflow, PR requirements, issue templates (bugs/features), licensing (MIT).\n\n[LANGUAGES-MANIFEST.md](./LANGUAGES-MANIFEST.md) — Reference table for 23 programming languages with manifest files, package managers, and support status (✓ for JS/TS, Python, Go, Rust; unmarked for others).\n\n[LICENSE](./LICENSE) — MIT License terms (copyright GeoloeG-IsT 2026, unrestricted use/modification with copyright notice and liability disclaimer).\n\n### Configuration & Build\n\n[package.json](./package.json) — Dual CLI entry (`agents-reverse-engineer`, `are`), ESM with `./core` library export and TypeScript definitions, runtime deps (fast-glob, ignore, isbinaryfile, jsonc-parser, picocolors, simple-git, yaml, zod), dev deps (tsx, typescript, @types/node, marked), build script (`tsc`), prepack removes .sum artifacts.\n\n[tsconfig.json](./tsconfig.json) — Compilation target ES2022, module NodeNext, outDir dist/, rootDir src/, strict type checking, sourceMap + declarationMap, resolveJsonModule, esModuleInterop.\n\n[.npmrc](./.npmrc) — npm registry configuration.\n\n### Subdirectories\n\n[hooks/](./hooks/) — Session hooks for Claude/OpenCode integration, context injection, update checks. Contains are-check-update.js (SessionStart), are-context-loader.js (PostToolUse AGENTS.md injection), opencode-are-check-update.js (OpenCode event handler).\n\n[scripts/](./scripts/) — Build automation (build-hooks.js copies hooks/ → hooks/dist/) and documentation auditing (analyze-agents-compression.sh, analyze-sum-ratios.sh measure directory/file compression ratios).\n\n[src/](./src/) — Library root exporting `getVersion()`, aggregating 11 feature modules: ai/, change-detection/, cli/, config/, core/, discovery/, generation/, imports/, installer/, integration/, orchestration/, output/, quality/, rebuild/, specify/, types/, update/. Each module has specific role in discovery, generation, quality, rebuild, and integration pipelines.\n\n## Key Behavioral Artifacts\n\n**Import Extraction Regex:** Global multiline pattern captures `type?` keyword, named/namespace/default imports, module specifiers; classifies `.`/`..` as internal, excludes `node:` and npm packages.\n\n**Export Detection Regex:** Pattern `^[ \\t]*export\\s+(?:default\\s+)?(?:function|class|const|let|var|type|interface|enum)\\s+(\\w+)` with multiline flag; skips re-exports, matches function/class/type/const declarations.\n\n**Phantom Path Validation:** Three RegExp patterns validate markdown links (`[text](path)`), backtick paths (`` `path` ``), prose references; resolves `.js`→`.ts` fallback for TypeScript projects.\n\n**Rebuild Module Output Delimiter:** AI response parsing uses primary `===FILE: path===` / `===END_FILE===` markers or markdown fence fallback, extracts Map<filePath, content>.\n\n**GENERATION_PLAN.md Checkbox Format:** Post-order directory task tracking via `- [ ] ANALYZING path/to/dir/` and `- [x] DONE ...` markers, persisted by `PlanTracker` promise-chain queue.\n\n## Annex File References\n\nFull prompt templates: [src/generation/prompts/templates.annex.sum](./src/generation/prompts/templates.annex.sum)\n\nFull rebuild system prompt: [src/rebuild/prompts.annex.sum](./src/rebuild/prompts.annex.sum)\n\nFull specification schema: [src/specify/prompts.annex.sum](./src/specify/prompts.annex.sum)\n\nFull integration templates: [src/integration/templates.annex.sum](./src/integration/templates.annex.sum)\n\n## Workflow & Conventions\n\n**Commit Standards:** PRs required; `npm run build` must succeed before merge. Commits should reference issue/feature in commit message; conventional commit format recommended for clarity (type(scope): subject).\n\n**CLI Flag Conventions:** General flags `--model`, `--backend`, `--concurrency`, `--dry-run`, `--debug`, `--trace`, `--eval`; command-specific: `--show-excluded` (discover), `--uncommitted` (update), `--force`/`-f` (rebuild, specify), `--multi-file` (specify).\n\n**Path Resolution:** `findProjectRoot()` locates `.agents-reverse-engineer/` directory; relative imports use `import.meta.url` + `fileURLToPath()` for `__dirname` equivalent; cross-platform path handling via `node:path`.\n\n**Error Handling:** Most operations return tuple-like objects with status field or throw domain-specific errors (SpecExistsError, CheckpointDriftError). Silent fallbacks (e.g., unlink non-existent files) common for cleanup operations.\n\n**Logging & Telemetry:** Logger dependency injection enables colorized output (picocolors) or silent mode. `TelemetryLogger` tracks per-call metrics (input/cache tokens, duration, model) to JSON. Trace events (NDJSON to `.agents-reverse-engineer/traces/`) record subprocess lifecycle, retry attempts, quality checks.\n\n**Post-Order Traversal:** Generation always processes deepest directories first (Phase 1 files concurrently, Phase 2 aggregates directories in descending depth), ensuring child summaries available before parent AGENTS.md synthesis.\n\n**User Content Preservation:** GENERATED_MARKER_PREFIX detects manually-authored content in AGENTS.md (marker lines prevent overwrite); rebuild checkpoints preserve existing file content unless explicitly regenerated.",
      "model": "claude-haiku-4-5-20251001",
      "inputTokens": 3,
      "outputTokens": 3449,
      "cacheReadTokens": 17512,
      "cacheCreationTokens": 9629,
      "latencyMs": 44635,
      "exitCode": 0,
      "retryCount": 0,
      "thinking": "not supported",
      "filesRead": []
    }
  ],
  "summary": {
    "totalCalls": 140,
    "totalInputTokens": 466,
    "totalOutputTokens": 96858,
    "totalCacheReadTokens": 2263176,
    "totalCacheCreationTokens": 903053,
    "totalDurationMs": 3268215,
    "errorCount": 0,
    "totalFilesRead": 110,
    "uniqueFilesRead": 110
  }
}